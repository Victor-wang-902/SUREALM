INFO:root:Output: distilbert_bert_not_concat
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models.py:436: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of the model checkpoint at bert-base-uncased were not used when initializing RetrievalGenerationModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing RetrievalGenerationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RetrievalGenerationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:450: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9700.242769491793
INFO:root:current train perplexity2030.1748046875
INFO:root:current mean train loss 8117.542814168499
INFO:root:current train perplexity591.9998168945312
INFO:root:current mean train loss 7038.509101791126
INFO:root:current train perplexity256.1142883300781
INFO:root:current mean train loss 6321.436606653352
INFO:root:current train perplexity145.1980743408203
INFO:root:current mean train loss 5794.844393376597
INFO:root:current train perplexity95.93685150146484
INFO:root:current mean train loss 5389.499140820639
INFO:root:current train perplexity70.32491302490234
INFO:root:current mean train loss 5071.216600235269
INFO:root:current train perplexity55.081417083740234
INFO:root:current mean train loss 4824.362212714624
INFO:root:current train perplexity45.23953628540039
INFO:root:current mean train loss 4619.515834108211
INFO:root:current train perplexity38.38191223144531
INFO:root:current mean train loss 4445.205018250673
INFO:root:current train perplexity33.48512268066406
INFO:root:current mean train loss 4300.296331626052
INFO:root:current train perplexity29.815000534057617
INFO:root:current mean train loss 4174.842266423191
INFO:root:current train perplexity26.930028915405273
INFO:root:current mean train loss 4063.736944772722
INFO:root:current train perplexity24.683765411376953
INFO:root:current mean train loss 3967.4889154222883
INFO:root:current train perplexity22.846637725830078
INFO:root:current mean train loss 3881.102048989691
INFO:root:current train perplexity21.335779190063477
INFO:root:current mean train loss 3802.5637141377424
INFO:root:current train perplexity20.040056228637695
INFO:root:current mean train loss 3731.810159468805
INFO:root:current train perplexity18.931926727294922
INFO:root:current mean train loss 3666.414193323548
INFO:root:current train perplexity18.00010871887207
INFO:root:current mean train loss 3606.991612438907
INFO:root:current train perplexity17.180021286010742

100%|██████████| 1/1 [18:41<00:00, 1121.48s/it][A100%|██████████| 1/1 [18:41<00:00, 1121.48s/it]
INFO:root:final mean train loss: 3561.10447138076
INFO:root:final train perplexity: 16.58504295349121
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.33s/it][A100%|██████████| 1/1 [01:20<00:00, 80.33s/it]
INFO:root:eval mean loss: 2348.8498721291835
INFO:root:eval perplexity: 6.6832990646362305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.97s/it][A100%|██████████| 1/1 [01:19<00:00, 79.98s/it]
INFO:root:eval mean loss: 2685.9321345336048
INFO:root:eval perplexity: 8.99464225769043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/1
  0%|          | 1/200 [21:23<70:57:46, 1283.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2440.2694549560547
INFO:root:current train perplexity7.03192138671875
INFO:root:current mean train loss 2449.772225215517
INFO:root:current train perplexity6.993507385253906
INFO:root:current mean train loss 2437.3284341317635
INFO:root:current train perplexity6.951974868774414
INFO:root:current mean train loss 2433.440244650539
INFO:root:current train perplexity6.926337242126465
INFO:root:current mean train loss 2436.470485687256
INFO:root:current train perplexity6.905898571014404
INFO:root:current mean train loss 2425.43025810774
INFO:root:current train perplexity6.829555988311768
INFO:root:current mean train loss 2422.211808439973
INFO:root:current train perplexity6.8002495765686035
INFO:root:current mean train loss 2415.0824511650553
INFO:root:current train perplexity6.764073371887207
INFO:root:current mean train loss 2407.1423893348842
INFO:root:current train perplexity6.720174789428711
INFO:root:current mean train loss 2400.880851645657
INFO:root:current train perplexity6.678344249725342
INFO:root:current mean train loss 2393.7246856689453
INFO:root:current train perplexity6.637986660003662
INFO:root:current mean train loss 2387.1529533358885
INFO:root:current train perplexity6.6011271476745605
INFO:root:current mean train loss 2380.9666193911903
INFO:root:current train perplexity6.562087535858154
INFO:root:current mean train loss 2374.8056327100944
INFO:root:current train perplexity6.52603006362915
INFO:root:current mean train loss 2369.6406355173576
INFO:root:current train perplexity6.499686241149902
INFO:root:current mean train loss 2363.726975171736
INFO:root:current train perplexity6.466309547424316
INFO:root:current mean train loss 2358.6944876189277
INFO:root:current train perplexity6.4337477684021
INFO:root:current mean train loss 2353.1848520843578
INFO:root:current train perplexity6.403926372528076
INFO:root:current mean train loss 2349.545397922331
INFO:root:current train perplexity6.379202365875244
INFO:root:current mean train loss 2344.3802971252567
INFO:root:current train perplexity6.350311279296875

100%|██████████| 1/1 [18:25<00:00, 1105.75s/it][A100%|██████████| 1/1 [18:25<00:00, 1105.75s/it]
INFO:root:final mean train loss: 2339.971460662703
INFO:root:final train perplexity: 6.330897808074951
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.66s/it][A100%|██████████| 1/1 [01:20<00:00, 80.66s/it]
INFO:root:eval mean loss: 2103.442605742326
INFO:root:eval perplexity: 5.480195999145508
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.16s/it][A100%|██████████| 1/1 [01:18<00:00, 78.16s/it]
INFO:root:eval mean loss: 2477.6404834503824
INFO:root:eval perplexity: 7.58583402633667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/2
  1%|          | 2/200 [42:31<70:05:52, 1274.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2205.7034320253315
INFO:root:current train perplexity5.728884696960449
INFO:root:current mean train loss 2200.5239110960997
INFO:root:current train perplexity5.678314208984375
INFO:root:current mean train loss 2188.473606093247
INFO:root:current train perplexity5.642448902130127
INFO:root:current mean train loss 2187.1191259618995
INFO:root:current train perplexity5.623851776123047
INFO:root:current mean train loss 2186.704386299257
INFO:root:current train perplexity5.612670421600342
INFO:root:current mean train loss 2186.1444611683573
INFO:root:current train perplexity5.607208728790283
INFO:root:current mean train loss 2181.3443026911777
INFO:root:current train perplexity5.585538864135742
INFO:root:current mean train loss 2178.1131420265606
INFO:root:current train perplexity5.575013637542725
INFO:root:current mean train loss 2178.705024343722
INFO:root:current train perplexity5.57016658782959
INFO:root:current mean train loss 2180.1778267454615
INFO:root:current train perplexity5.569430351257324
INFO:root:current mean train loss 2178.0667000223107
INFO:root:current train perplexity5.555739879608154
INFO:root:current mean train loss 2173.539680177936
INFO:root:current train perplexity5.543788909912109
INFO:root:current mean train loss 2167.8967399999365
INFO:root:current train perplexity5.531749248504639
INFO:root:current mean train loss 2167.0746917381202
INFO:root:current train perplexity5.524312496185303
INFO:root:current mean train loss 2163.9763083075277
INFO:root:current train perplexity5.51158332824707
INFO:root:current mean train loss 2161.8040501544156
INFO:root:current train perplexity5.501336574554443
INFO:root:current mean train loss 2159.288731123579
INFO:root:current train perplexity5.490769863128662
INFO:root:current mean train loss 2156.55864517027
INFO:root:current train perplexity5.478977680206299
INFO:root:current mean train loss 2155.5065911975116
INFO:root:current train perplexity5.473616123199463
INFO:root:current mean train loss 2153.2537105459655
INFO:root:current train perplexity5.464570045471191

100%|██████████| 1/1 [18:11<00:00, 1091.20s/it][A100%|██████████| 1/1 [18:11<00:00, 1091.20s/it]
INFO:root:final mean train loss: 2152.1285711876144
INFO:root:final train perplexity: 5.459177017211914
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.05s/it][A100%|██████████| 1/1 [01:21<00:00, 81.05s/it]
INFO:root:eval mean loss: 2005.822161302499
INFO:root:eval perplexity: 5.064174175262451
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.26s/it][A100%|██████████| 1/1 [01:18<00:00, 78.26s/it]
INFO:root:eval mean loss: 2388.954630533854
INFO:root:eval perplexity: 7.05511474609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/3
  2%|▏         | 3/200 [1:03:25<69:12:46, 1264.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2060.4872509765623
INFO:root:current train perplexity5.119741439819336
INFO:root:current mean train loss 2051.695791015625
INFO:root:current train perplexity5.055324554443359
INFO:root:current mean train loss 2058.2449770507815
INFO:root:current train perplexity5.0712385177612305
INFO:root:current mean train loss 2052.161065499442
INFO:root:current train perplexity5.075805187225342
INFO:root:current mean train loss 2056.745030924479
INFO:root:current train perplexity5.080552101135254
INFO:root:current mean train loss 2055.7198737127133
INFO:root:current train perplexity5.069494247436523
INFO:root:current mean train loss 2060.2877815129204
INFO:root:current train perplexity5.080002307891846
INFO:root:current mean train loss 2062.4697770182293
INFO:root:current train perplexity5.080622673034668
INFO:root:current mean train loss 2058.4057113108915
INFO:root:current train perplexity5.067702293395996
INFO:root:current mean train loss 2058.2838369911597
INFO:root:current train perplexity5.070521354675293
INFO:root:current mean train loss 2055.111398228237
INFO:root:current train perplexity5.062442302703857
INFO:root:current mean train loss 2054.7357721212634
INFO:root:current train perplexity5.059316158294678
INFO:root:current mean train loss 2053.2107859375
INFO:root:current train perplexity5.054049491882324
INFO:root:current mean train loss 2053.5435572193287
INFO:root:current train perplexity5.051390171051025
INFO:root:current mean train loss 2051.994499343346
INFO:root:current train perplexity5.042201042175293
INFO:root:current mean train loss 2050.61504827684
INFO:root:current train perplexity5.0397047996521
INFO:root:current mean train loss 2049.828729876894
INFO:root:current train perplexity5.036248207092285
INFO:root:current mean train loss 2048.708748674665
INFO:root:current train perplexity5.031864643096924
INFO:root:current mean train loss 2047.6312990920608
INFO:root:current train perplexity5.025516986846924
INFO:root:current mean train loss 2048.003853102464
INFO:root:current train perplexity5.024468898773193

100%|██████████| 1/1 [18:21<00:00, 1101.64s/it][A100%|██████████| 1/1 [18:21<00:00, 1101.64s/it]
INFO:root:final mean train loss: 2046.3218166795693
INFO:root:final train perplexity: 5.022120952606201
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.51s/it][A100%|██████████| 1/1 [01:21<00:00, 81.51s/it]
INFO:root:eval mean loss: 1941.5603261095412
INFO:root:eval perplexity: 4.807705879211426
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.07s/it][A100%|██████████| 1/1 [01:19<00:00, 79.07s/it]
INFO:root:eval mean loss: 2340.825212627438
INFO:root:eval perplexity: 6.78280782699585
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/4
  2%|▏         | 4/200 [1:24:29<68:51:42, 1264.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1954.6151323460822
INFO:root:current train perplexity4.707826614379883
INFO:root:current mean train loss 1967.5507717475207
INFO:root:current train perplexity4.722445011138916
INFO:root:current mean train loss 1976.6071983080233
INFO:root:current train perplexity4.730832576751709
INFO:root:current mean train loss 1975.082655571462
INFO:root:current train perplexity4.734326362609863
INFO:root:current mean train loss 1970.0718345887146
INFO:root:current train perplexity4.728091716766357
INFO:root:current mean train loss 1973.970724008281
INFO:root:current train perplexity4.737147808074951
INFO:root:current mean train loss 1971.1734522143224
INFO:root:current train perplexity4.7323713302612305
INFO:root:current mean train loss 1971.343001821983
INFO:root:current train perplexity4.729008197784424
INFO:root:current mean train loss 1970.2395261700728
INFO:root:current train perplexity4.730851173400879
INFO:root:current mean train loss 1968.7667577165605
INFO:root:current train perplexity4.72475528717041
INFO:root:current mean train loss 1969.6096715381912
INFO:root:current train perplexity4.730635643005371
INFO:root:current mean train loss 1968.285755827576
INFO:root:current train perplexity4.731265068054199
INFO:root:current mean train loss 1970.1553408796617
INFO:root:current train perplexity4.7321391105651855
INFO:root:current mean train loss 1971.0717410887778
INFO:root:current train perplexity4.734842300415039
INFO:root:current mean train loss 1971.3294541268585
INFO:root:current train perplexity4.731818199157715
INFO:root:current mean train loss 1971.3631490526036
INFO:root:current train perplexity4.730337619781494
INFO:root:current mean train loss 1971.1893261484422
INFO:root:current train perplexity4.731269836425781
INFO:root:current mean train loss 1971.1677972159955
INFO:root:current train perplexity4.730305194854736
INFO:root:current mean train loss 1970.301898647772
INFO:root:current train perplexity4.729085922241211
INFO:root:current mean train loss 1970.0948843168132
INFO:root:current train perplexity4.7258477210998535

100%|██████████| 1/1 [18:05<00:00, 1085.09s/it][A100%|██████████| 1/1 [18:05<00:00, 1085.09s/it]
INFO:root:final mean train loss: 1968.8011460758735
INFO:root:final train perplexity: 4.724278926849365
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.18s/it][A100%|██████████| 1/1 [01:20<00:00, 80.18s/it]
INFO:root:eval mean loss: 1903.1862563545822
INFO:root:eval perplexity: 4.660791397094727
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.24s/it][A100%|██████████| 1/1 [01:18<00:00, 78.24s/it]
INFO:root:eval mean loss: 2309.6455515327184
INFO:root:eval perplexity: 6.61203670501709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/5
  2%|▎         | 5/200 [1:45:16<68:08:51, 1258.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1913.338899158296
INFO:root:current train perplexity4.511913776397705
INFO:root:current mean train loss 1898.2287518045177
INFO:root:current train perplexity4.474410057067871
INFO:root:current mean train loss 1903.7026006134463
INFO:root:current train perplexity4.472931385040283
INFO:root:current mean train loss 1907.460257212321
INFO:root:current train perplexity4.4811224937438965
INFO:root:current mean train loss 1910.7498928101595
INFO:root:current train perplexity4.495788097381592
INFO:root:current mean train loss 1909.0108684383026
INFO:root:current train perplexity4.49090576171875
INFO:root:current mean train loss 1909.6105389511376
INFO:root:current train perplexity4.492499828338623
INFO:root:current mean train loss 1910.8992219263193
INFO:root:current train perplexity4.501486778259277
INFO:root:current mean train loss 1915.9184644880338
INFO:root:current train perplexity4.5117974281311035
INFO:root:current mean train loss 1915.732335656639
INFO:root:current train perplexity4.5125837326049805
INFO:root:current mean train loss 1914.195547181302
INFO:root:current train perplexity4.506734371185303
INFO:root:current mean train loss 1913.8970653430836
INFO:root:current train perplexity4.511807918548584
INFO:root:current mean train loss 1913.909304704993
INFO:root:current train perplexity4.5140862464904785
INFO:root:current mean train loss 1911.6012607640614
INFO:root:current train perplexity4.5108513832092285
INFO:root:current mean train loss 1912.6647746864996
INFO:root:current train perplexity4.514405250549316
INFO:root:current mean train loss 1911.0950372098673
INFO:root:current train perplexity4.514089107513428
INFO:root:current mean train loss 1911.0032416771824
INFO:root:current train perplexity4.5134196281433105
INFO:root:current mean train loss 1909.678893221868
INFO:root:current train perplexity4.509527683258057
INFO:root:current mean train loss 1909.345034006042
INFO:root:current train perplexity4.505776882171631

100%|██████████| 1/1 [18:08<00:00, 1088.61s/it][A100%|██████████| 1/1 [18:08<00:00, 1088.62s/it]
INFO:root:final mean train loss: 1907.8896110715496
INFO:root:final train perplexity: 4.502696514129639
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.43s/it][A100%|██████████| 1/1 [01:20<00:00, 80.43s/it]
INFO:root:eval mean loss: 1867.3250122070312
INFO:root:eval perplexity: 4.527559280395508
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.23s/it][A100%|██████████| 1/1 [01:18<00:00, 78.23s/it]
INFO:root:eval mean loss: 2276.7958023395945
INFO:root:eval perplexity: 6.436765193939209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/6
  3%|▎         | 6/200 [2:06:06<67:39:00, 1255.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1877.8087158203125
INFO:root:current train perplexity4.1657915115356445
INFO:root:current mean train loss 1854.1531619836787
INFO:root:current train perplexity4.301884174346924
INFO:root:current mean train loss 1852.5116379771066
INFO:root:current train perplexity4.290266036987305
INFO:root:current mean train loss 1861.5163679661546
INFO:root:current train perplexity4.332698822021484
INFO:root:current mean train loss 1860.1815480829177
INFO:root:current train perplexity4.325940132141113
INFO:root:current mean train loss 1857.7473107983253
INFO:root:current train perplexity4.324118137359619
INFO:root:current mean train loss 1857.8685877541338
INFO:root:current train perplexity4.331323146820068
INFO:root:current mean train loss 1858.506929797555
INFO:root:current train perplexity4.3294291496276855
INFO:root:current mean train loss 1856.240375190192
INFO:root:current train perplexity4.322943210601807
INFO:root:current mean train loss 1856.6714486345468
INFO:root:current train perplexity4.319608211517334
INFO:root:current mean train loss 1855.5999775371113
INFO:root:current train perplexity4.319993019104004
INFO:root:current mean train loss 1857.2441534861773
INFO:root:current train perplexity4.3256072998046875
INFO:root:current mean train loss 1855.7012231018423
INFO:root:current train perplexity4.320592880249023
INFO:root:current mean train loss 1854.9530564116844
INFO:root:current train perplexity4.321229457855225
INFO:root:current mean train loss 1856.4085519968996
INFO:root:current train perplexity4.325272083282471
INFO:root:current mean train loss 1855.6640877110572
INFO:root:current train perplexity4.32664680480957
INFO:root:current mean train loss 1856.1650303704228
INFO:root:current train perplexity4.324682712554932
INFO:root:current mean train loss 1855.7929134918338
INFO:root:current train perplexity4.324097156524658
INFO:root:current mean train loss 1855.6089883875277
INFO:root:current train perplexity4.323331832885742
INFO:root:current mean train loss 1856.2573072663236
INFO:root:current train perplexity4.324521064758301

100%|██████████| 1/1 [18:15<00:00, 1095.42s/it][A100%|██████████| 1/1 [18:15<00:00, 1095.42s/it]
INFO:root:final mean train loss: 1856.5921801991735
INFO:root:final train perplexity: 4.324169635772705
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.63s/it][A100%|██████████| 1/1 [01:22<00:00, 82.63s/it]
INFO:root:eval mean loss: 1851.2392184210162
INFO:root:eval perplexity: 4.469039440155029
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.63s/it][A100%|██████████| 1/1 [01:18<00:00, 78.63s/it]
INFO:root:eval mean loss: 2268.572077757923
INFO:root:eval perplexity: 6.393619537353516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/7
  4%|▎         | 7/200 [2:27:05<67:22:16, 1256.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1833.0620863172744
INFO:root:current train perplexity4.219443321228027
INFO:root:current mean train loss 1808.3691954531912
INFO:root:current train perplexity4.181427001953125
INFO:root:current mean train loss 1814.4293588060852
INFO:root:current train perplexity4.191302299499512
INFO:root:current mean train loss 1818.9944649942265
INFO:root:current train perplexity4.1872687339782715
INFO:root:current mean train loss 1815.1587530254747
INFO:root:current train perplexity4.17251443862915
INFO:root:current mean train loss 1813.9644221596736
INFO:root:current train perplexity4.173736095428467
INFO:root:current mean train loss 1815.226210115797
INFO:root:current train perplexity4.175582408905029
INFO:root:current mean train loss 1815.8760005345252
INFO:root:current train perplexity4.177942752838135
INFO:root:current mean train loss 1816.2420528943499
INFO:root:current train perplexity4.179925918579102
INFO:root:current mean train loss 1814.5543395065274
INFO:root:current train perplexity4.1807098388671875
INFO:root:current mean train loss 1811.9119061243323
INFO:root:current train perplexity4.1744303703308105
INFO:root:current mean train loss 1810.8991685024528
INFO:root:current train perplexity4.172387599945068
INFO:root:current mean train loss 1811.6402944680701
INFO:root:current train perplexity4.175784111022949
INFO:root:current mean train loss 1812.032176919338
INFO:root:current train perplexity4.179360389709473
INFO:root:current mean train loss 1812.36601061478
INFO:root:current train perplexity4.178497791290283
INFO:root:current mean train loss 1812.6914531320765
INFO:root:current train perplexity4.178285121917725
INFO:root:current mean train loss 1812.3026660971059
INFO:root:current train perplexity4.175601959228516
INFO:root:current mean train loss 1813.3665813406078
INFO:root:current train perplexity4.177465438842773
INFO:root:current mean train loss 1813.9489941486825
INFO:root:current train perplexity4.178925037384033
INFO:root:current mean train loss 1814.4555730252869
INFO:root:current train perplexity4.179088115692139

100%|██████████| 1/1 [18:08<00:00, 1088.39s/it][A100%|██████████| 1/1 [18:08<00:00, 1088.39s/it]
INFO:root:final mean train loss: 1813.1304656782356
INFO:root:final train perplexity: 4.178464889526367
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.88s/it][A100%|██████████| 1/1 [01:19<00:00, 79.88s/it]
INFO:root:eval mean loss: 1821.0438422886193
INFO:root:eval perplexity: 4.361227035522461
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.06s/it][A100%|██████████| 1/1 [01:18<00:00, 78.06s/it]
INFO:root:eval mean loss: 2238.3742390084776
INFO:root:eval perplexity: 6.237654209136963
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/8
  4%|▍         | 8/200 [2:47:54<66:53:28, 1254.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1763.725830078125
INFO:root:current train perplexity4.027256965637207
INFO:root:current mean train loss 1762.4007514105904
INFO:root:current train perplexity4.026019096374512
INFO:root:current mean train loss 1774.10498046875
INFO:root:current train perplexity4.051490306854248
INFO:root:current mean train loss 1776.5318016849346
INFO:root:current train perplexity4.046989440917969
INFO:root:current mean train loss 1778.0819675489404
INFO:root:current train perplexity4.0647478103637695
INFO:root:current mean train loss 1778.7656033239632
INFO:root:current train perplexity4.066198825836182
INFO:root:current mean train loss 1779.444634288878
INFO:root:current train perplexity4.071600437164307
INFO:root:current mean train loss 1779.7970669908589
INFO:root:current train perplexity4.072854995727539
INFO:root:current mean train loss 1778.7707575084207
INFO:root:current train perplexity4.071631908416748
INFO:root:current mean train loss 1778.7424292906082
INFO:root:current train perplexity4.069573879241943
INFO:root:current mean train loss 1778.1651577124849
INFO:root:current train perplexity4.069761276245117
INFO:root:current mean train loss 1780.4370373158729
INFO:root:current train perplexity4.075013160705566
INFO:root:current mean train loss 1781.6248273224
INFO:root:current train perplexity4.076208591461182
INFO:root:current mean train loss 1782.2435643799743
INFO:root:current train perplexity4.080004692077637
INFO:root:current mean train loss 1782.862697949559
INFO:root:current train perplexity4.079253673553467
INFO:root:current mean train loss 1782.937942236487
INFO:root:current train perplexity4.07924222946167
INFO:root:current mean train loss 1783.418358479071
INFO:root:current train perplexity4.078042030334473
INFO:root:current mean train loss 1781.9565030056735
INFO:root:current train perplexity4.075941562652588
INFO:root:current mean train loss 1781.6825355633728
INFO:root:current train perplexity4.0743584632873535
INFO:root:current mean train loss 1780.632503949148
INFO:root:current train perplexity4.0699896812438965

100%|██████████| 1/1 [18:00<00:00, 1080.61s/it][A100%|██████████| 1/1 [18:00<00:00, 1080.61s/it]
INFO:root:final mean train loss: 1780.0760755360998
INFO:root:final train perplexity: 4.070944786071777
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.10s/it][A100%|██████████| 1/1 [01:20<00:00, 80.12s/it]
INFO:root:eval mean loss: 1827.2191465120789
INFO:root:eval perplexity: 4.38306188583374
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.70s/it][A100%|██████████| 1/1 [01:17<00:00, 77.70s/it]
INFO:root:eval mean loss: 2255.8965609936004
INFO:root:eval perplexity: 6.32768440246582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/9
  4%|▍         | 9/200 [3:08:35<66:19:28, 1250.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1727.158487173227
INFO:root:current train perplexity3.898474931716919
INFO:root:current mean train loss 1742.2643681576378
INFO:root:current train perplexity3.922691822052002
INFO:root:current mean train loss 1737.7329794263082
INFO:root:current train perplexity3.9366230964660645
INFO:root:current mean train loss 1736.6398103887384
INFO:root:current train perplexity3.9289512634277344
INFO:root:current mean train loss 1736.6845106276792
INFO:root:current train perplexity3.9369897842407227
INFO:root:current mean train loss 1737.12344647836
INFO:root:current train perplexity3.940800189971924
INFO:root:current mean train loss 1742.4653752800878
INFO:root:current train perplexity3.9466960430145264
INFO:root:current mean train loss 1741.5154222528986
INFO:root:current train perplexity3.9460840225219727
INFO:root:current mean train loss 1742.9452140700648
INFO:root:current train perplexity3.9493579864501953
INFO:root:current mean train loss 1742.9787947710822
INFO:root:current train perplexity3.9565696716308594
INFO:root:current mean train loss 1744.3237886029958
INFO:root:current train perplexity3.9592275619506836
INFO:root:current mean train loss 1744.610919210646
INFO:root:current train perplexity3.9563188552856445
INFO:root:current mean train loss 1744.7394453054799
INFO:root:current train perplexity3.9566650390625
INFO:root:current mean train loss 1744.9644478340826
INFO:root:current train perplexity3.958287477493286
INFO:root:current mean train loss 1744.0705980873634
INFO:root:current train perplexity3.956026315689087
INFO:root:current mean train loss 1744.1120321529427
INFO:root:current train perplexity3.9571027755737305
INFO:root:current mean train loss 1744.4486078811904
INFO:root:current train perplexity3.959113836288452
INFO:root:current mean train loss 1744.4273317241234
INFO:root:current train perplexity3.9572277069091797
INFO:root:current mean train loss 1744.3214345318195
INFO:root:current train perplexity3.9579646587371826
INFO:root:current mean train loss 1745.1225139430312
INFO:root:current train perplexity3.958089590072632

100%|██████████| 1/1 [17:54<00:00, 1074.59s/it][A100%|██████████| 1/1 [17:54<00:00, 1074.59s/it]
INFO:root:final mean train loss: 1744.2252431187555
INFO:root:final train perplexity: 3.957454204559326
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.83s/it][A100%|██████████| 1/1 [01:19<00:00, 79.84s/it]
INFO:root:eval mean loss: 1804.7002277780086
INFO:root:eval perplexity: 4.303959846496582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.61s/it][A100%|██████████| 1/1 [01:17<00:00, 77.61s/it]
INFO:root:eval mean loss: 2241.6714551127548
INFO:root:eval perplexity: 6.254496097564697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/10
  5%|▌         | 10/200 [3:29:10<65:43:27, 1245.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1706.0722355497055
INFO:root:current train perplexity3.810445547103882
INFO:root:current mean train loss 1701.0069276708118
INFO:root:current train perplexity3.8269588947296143
INFO:root:current mean train loss 1699.7142324908516
INFO:root:current train perplexity3.8405396938323975
INFO:root:current mean train loss 1703.8892888428395
INFO:root:current train perplexity3.841094970703125
INFO:root:current mean train loss 1705.4663728823793
INFO:root:current train perplexity3.8444857597351074
INFO:root:current mean train loss 1706.1884587561099
INFO:root:current train perplexity3.839769124984741
INFO:root:current mean train loss 1705.8914397144174
INFO:root:current train perplexity3.8359951972961426
INFO:root:current mean train loss 1705.4667419512964
INFO:root:current train perplexity3.8373444080352783
INFO:root:current mean train loss 1708.6032334164179
INFO:root:current train perplexity3.8447608947753906
INFO:root:current mean train loss 1709.570652508022
INFO:root:current train perplexity3.848348379135132
INFO:root:current mean train loss 1709.5618157667798
INFO:root:current train perplexity3.8479132652282715
INFO:root:current mean train loss 1709.2884524617061
INFO:root:current train perplexity3.8487672805786133
INFO:root:current mean train loss 1710.1027142319556
INFO:root:current train perplexity3.8486251831054688
INFO:root:current mean train loss 1710.6805214836616
INFO:root:current train perplexity3.8475942611694336
INFO:root:current mean train loss 1710.7765025534218
INFO:root:current train perplexity3.847297191619873
INFO:root:current mean train loss 1710.8323005982613
INFO:root:current train perplexity3.8506131172180176
INFO:root:current mean train loss 1711.6179291374888
INFO:root:current train perplexity3.852489709854126
INFO:root:current mean train loss 1710.5031167607715
INFO:root:current train perplexity3.8508963584899902
INFO:root:current mean train loss 1710.7366356193988
INFO:root:current train perplexity3.8525259494781494
INFO:root:current mean train loss 1711.8293597142426
INFO:root:current train perplexity3.855715751647949

100%|██████████| 1/1 [18:01<00:00, 1081.14s/it][A100%|██████████| 1/1 [18:01<00:00, 1081.14s/it]
INFO:root:final mean train loss: 1710.9322864649816
INFO:root:final train perplexity: 3.854895830154419
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.18s/it][A100%|██████████| 1/1 [01:20<00:00, 80.19s/it]
INFO:root:eval mean loss: 1779.3896722455397
INFO:root:eval perplexity: 4.216754913330078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.47s/it][A100%|██████████| 1/1 [01:20<00:00, 80.47s/it]
INFO:root:eval mean loss: 2211.1772612443206
INFO:root:eval perplexity: 6.1004438400268555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/11
  6%|▌         | 11/200 [3:49:54<65:21:50, 1245.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1658.3236438839936
INFO:root:current train perplexity3.720651626586914
INFO:root:current mean train loss 1665.6770892399613
INFO:root:current train perplexity3.731553554534912
INFO:root:current mean train loss 1671.2023682494264
INFO:root:current train perplexity3.735348701477051
INFO:root:current mean train loss 1672.9483797537848
INFO:root:current train perplexity3.739393711090088
INFO:root:current mean train loss 1673.2620251816486
INFO:root:current train perplexity3.744654417037964
INFO:root:current mean train loss 1671.33965481182
INFO:root:current train perplexity3.743025302886963
INFO:root:current mean train loss 1673.6122183841449
INFO:root:current train perplexity3.751112222671509
INFO:root:current mean train loss 1676.0813477494335
INFO:root:current train perplexity3.7553930282592773
INFO:root:current mean train loss 1677.1844399755748
INFO:root:current train perplexity3.756181001663208
INFO:root:current mean train loss 1679.556655110017
INFO:root:current train perplexity3.7612953186035156
INFO:root:current mean train loss 1679.8888398059823
INFO:root:current train perplexity3.7618165016174316
INFO:root:current mean train loss 1680.2724654662468
INFO:root:current train perplexity3.766458511352539
INFO:root:current mean train loss 1681.5731836203283
INFO:root:current train perplexity3.769198417663574
INFO:root:current mean train loss 1680.974809919085
INFO:root:current train perplexity3.767129421234131
INFO:root:current mean train loss 1681.432482384416
INFO:root:current train perplexity3.768650531768799
INFO:root:current mean train loss 1682.0917967980326
INFO:root:current train perplexity3.769890308380127
INFO:root:current mean train loss 1681.8216223303816
INFO:root:current train perplexity3.7690553665161133
INFO:root:current mean train loss 1682.167175976453
INFO:root:current train perplexity3.7699034214019775
INFO:root:current mean train loss 1682.9196671195652
INFO:root:current train perplexity3.7690823078155518

100%|██████████| 1/1 [18:12<00:00, 1092.66s/it][A100%|██████████| 1/1 [18:12<00:00, 1092.66s/it]
INFO:root:final mean train loss: 1682.6674411944891
INFO:root:final train perplexity: 3.769916534423828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.77s/it][A100%|██████████| 1/1 [01:20<00:00, 80.77s/it]
INFO:root:eval mean loss: 1777.1439135395888
INFO:root:eval perplexity: 4.209103584289551
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.74s/it][A100%|██████████| 1/1 [01:18<00:00, 78.74s/it]
INFO:root:eval mean loss: 2215.685886680657
INFO:root:eval perplexity: 6.122978687286377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/12
  6%|▌         | 12/200 [4:10:49<65:10:19, 1247.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1739.5397542317708
INFO:root:current train perplexity3.7692649364471436
INFO:root:current mean train loss 1644.685744794827
INFO:root:current train perplexity3.6572930812835693
INFO:root:current mean train loss 1646.1951639710976
INFO:root:current train perplexity3.652191638946533
INFO:root:current mean train loss 1652.295628915919
INFO:root:current train perplexity3.657503128051758
INFO:root:current mean train loss 1650.8602788655396
INFO:root:current train perplexity3.6612813472747803
INFO:root:current mean train loss 1647.5918085238568
INFO:root:current train perplexity3.6586382389068604
INFO:root:current mean train loss 1646.9095023741968
INFO:root:current train perplexity3.6571943759918213
INFO:root:current mean train loss 1646.328169452347
INFO:root:current train perplexity3.6587088108062744
INFO:root:current mean train loss 1647.0536241353227
INFO:root:current train perplexity3.6650612354278564
INFO:root:current mean train loss 1648.6150086192727
INFO:root:current train perplexity3.6687064170837402
INFO:root:current mean train loss 1650.8379229985824
INFO:root:current train perplexity3.671541452407837
INFO:root:current mean train loss 1651.6139699567586
INFO:root:current train perplexity3.6752207279205322
INFO:root:current mean train loss 1651.761592925239
INFO:root:current train perplexity3.6752500534057617
INFO:root:current mean train loss 1652.850539282845
INFO:root:current train perplexity3.6780004501342773
INFO:root:current mean train loss 1654.3156812236891
INFO:root:current train perplexity3.681058645248413
INFO:root:current mean train loss 1655.1080472518504
INFO:root:current train perplexity3.6835896968841553
INFO:root:current mean train loss 1654.0527020869074
INFO:root:current train perplexity3.6838228702545166
INFO:root:current mean train loss 1655.230617771832
INFO:root:current train perplexity3.6865272521972656
INFO:root:current mean train loss 1654.4598560640036
INFO:root:current train perplexity3.6862306594848633
INFO:root:current mean train loss 1654.493520394915
INFO:root:current train perplexity3.687067747116089

100%|██████████| 1/1 [18:08<00:00, 1088.04s/it][A100%|██████████| 1/1 [18:08<00:00, 1088.04s/it]
INFO:root:final mean train loss: 1654.1826498442326
INFO:root:final train perplexity: 3.6861703395843506
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:26<00:00, 86.88s/it][A100%|██████████| 1/1 [01:26<00:00, 86.88s/it]
INFO:root:eval mean loss: 1759.8576015174813
INFO:root:eval perplexity: 4.150668621063232
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.99s/it][A100%|██████████| 1/1 [01:18<00:00, 78.99s/it]
INFO:root:eval mean loss: 2199.2606737934952
INFO:root:eval perplexity: 6.041280269622803
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/13
  6%|▋         | 13/200 [4:31:45<64:57:30, 1250.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.4091430664062
INFO:root:current train perplexity3.6561944484710693
INFO:root:current mean train loss 1607.1172973632813
INFO:root:current train perplexity3.5671982765197754
INFO:root:current mean train loss 1616.8915272105824
INFO:root:current train perplexity3.58797287940979
INFO:root:current mean train loss 1623.8838287353515
INFO:root:current train perplexity3.599534034729004
INFO:root:current mean train loss 1622.9873968215215
INFO:root:current train perplexity3.5909266471862793
INFO:root:current mean train loss 1626.3031794621395
INFO:root:current train perplexity3.598275661468506
INFO:root:current mean train loss 1628.4488612021169
INFO:root:current train perplexity3.6006665229797363
INFO:root:current mean train loss 1630.2135233561198
INFO:root:current train perplexity3.6040196418762207
INFO:root:current mean train loss 1629.8427052567645
INFO:root:current train perplexity3.607088565826416
INFO:root:current mean train loss 1629.4439536716627
INFO:root:current train perplexity3.6095244884490967
INFO:root:current mean train loss 1630.2370549220666
INFO:root:current train perplexity3.610085964202881
INFO:root:current mean train loss 1631.0030031476701
INFO:root:current train perplexity3.6139087677001953
INFO:root:current mean train loss 1631.340214963819
INFO:root:current train perplexity3.613792896270752
INFO:root:current mean train loss 1631.300341796875
INFO:root:current train perplexity3.61569881439209
INFO:root:current mean train loss 1632.568760315801
INFO:root:current train perplexity3.6177139282226562
INFO:root:current mean train loss 1633.7548578362716
INFO:root:current train perplexity3.6199023723602295
INFO:root:current mean train loss 1632.4698504412615
INFO:root:current train perplexity3.618868589401245
INFO:root:current mean train loss 1631.2878598945085
INFO:root:current train perplexity3.618955612182617
INFO:root:current mean train loss 1632.3092523260432
INFO:root:current train perplexity3.6200525760650635
INFO:root:current mean train loss 1633.2799297332763
INFO:root:current train perplexity3.623821973800659

100%|██████████| 1/1 [18:02<00:00, 1082.43s/it][A100%|██████████| 1/1 [18:02<00:00, 1082.43s/it]
INFO:root:final mean train loss: 1633.1880092418862
INFO:root:final train perplexity: 3.625638008117676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.68s/it][A100%|██████████| 1/1 [01:20<00:00, 80.68s/it]
INFO:root:eval mean loss: 1776.0729751045822
INFO:root:eval perplexity: 4.205459117889404
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.97s/it][A100%|██████████| 1/1 [01:18<00:00, 78.97s/it]
INFO:root:eval mean loss: 2214.969165125637
INFO:root:eval perplexity: 6.1193928718566895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/14
  7%|▋         | 14/200 [4:52:30<64:31:14, 1248.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1600.771421690245
INFO:root:current train perplexity3.572307825088501
INFO:root:current mean train loss 1591.1932675995097
INFO:root:current train perplexity3.5294764041900635
INFO:root:current mean train loss 1592.0883119478508
INFO:root:current train perplexity3.535250425338745
INFO:root:current mean train loss 1592.1288863275454
INFO:root:current train perplexity3.538442611694336
INFO:root:current mean train loss 1597.0999230705627
INFO:root:current train perplexity3.544339418411255
INFO:root:current mean train loss 1595.6860947138327
INFO:root:current train perplexity3.542703628540039
INFO:root:current mean train loss 1600.361817747682
INFO:root:current train perplexity3.5477776527404785
INFO:root:current mean train loss 1601.932294371979
INFO:root:current train perplexity3.5498404502868652
INFO:root:current mean train loss 1605.53630067741
INFO:root:current train perplexity3.5563888549804688
INFO:root:current mean train loss 1605.7999026564169
INFO:root:current train perplexity3.554403066635132
INFO:root:current mean train loss 1606.2235365217425
INFO:root:current train perplexity3.5571417808532715
INFO:root:current mean train loss 1604.4220643861313
INFO:root:current train perplexity3.553316593170166
INFO:root:current mean train loss 1604.7275802131228
INFO:root:current train perplexity3.554464101791382
INFO:root:current mean train loss 1606.5034151383986
INFO:root:current train perplexity3.5564498901367188
INFO:root:current mean train loss 1608.1349790144398
INFO:root:current train perplexity3.561372995376587
INFO:root:current mean train loss 1610.7834813372997
INFO:root:current train perplexity3.563788652420044
INFO:root:current mean train loss 1611.4153782792694
INFO:root:current train perplexity3.5648436546325684
INFO:root:current mean train loss 1611.345886265607
INFO:root:current train perplexity3.5642802715301514
INFO:root:current mean train loss 1613.5780491795174
INFO:root:current train perplexity3.5671560764312744
INFO:root:current mean train loss 1613.2478448319325
INFO:root:current train perplexity3.567582368850708

100%|██████████| 1/1 [18:04<00:00, 1084.72s/it][A100%|██████████| 1/1 [18:04<00:00, 1084.72s/it]
INFO:root:final mean train loss: 1612.775941757379
INFO:root:final train perplexity: 3.567739248275757
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:22<00:00, 82.16s/it][A100%|██████████| 1/1 [01:22<00:00, 82.16s/it]
INFO:root:eval mean loss: 1755.3618449758976
INFO:root:eval perplexity: 4.1356048583984375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.40s/it][A100%|██████████| 1/1 [01:20<00:00, 80.40s/it]
INFO:root:eval mean loss: 2205.763480112062
INFO:root:eval perplexity: 6.073493003845215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/15
  8%|▊         | 15/200 [5:13:20<64:11:25, 1249.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1581.6534808123554
INFO:root:current train perplexity3.4853992462158203
INFO:root:current mean train loss 1574.9988189300934
INFO:root:current train perplexity3.4993410110473633
INFO:root:current mean train loss 1587.449427326833
INFO:root:current train perplexity3.507723331451416
INFO:root:current mean train loss 1586.4139245674435
INFO:root:current train perplexity3.504983425140381
INFO:root:current mean train loss 1587.584303532403
INFO:root:current train perplexity3.504824638366699
INFO:root:current mean train loss 1588.601675536228
INFO:root:current train perplexity3.5099880695343018
INFO:root:current mean train loss 1590.7276984631833
INFO:root:current train perplexity3.507211685180664
INFO:root:current mean train loss 1589.9714745640438
INFO:root:current train perplexity3.507575035095215
INFO:root:current mean train loss 1593.9678856449887
INFO:root:current train perplexity3.516185760498047
INFO:root:current mean train loss 1590.7774711944771
INFO:root:current train perplexity3.5105550289154053
INFO:root:current mean train loss 1591.6946809540664
INFO:root:current train perplexity3.510544538497925
INFO:root:current mean train loss 1593.494250319033
INFO:root:current train perplexity3.5120413303375244
INFO:root:current mean train loss 1593.856921717691
INFO:root:current train perplexity3.5130810737609863
INFO:root:current mean train loss 1593.9787081066227
INFO:root:current train perplexity3.5139644145965576
INFO:root:current mean train loss 1593.803342543737
INFO:root:current train perplexity3.5147147178649902
INFO:root:current mean train loss 1593.0683929168426
INFO:root:current train perplexity3.5159103870391846
INFO:root:current mean train loss 1592.255972146123
INFO:root:current train perplexity3.5157158374786377
INFO:root:current mean train loss 1592.9069188812848
INFO:root:current train perplexity3.5157158374786377
INFO:root:current mean train loss 1593.4147267758267
INFO:root:current train perplexity3.514817476272583
INFO:root:current mean train loss 1593.7454511628791
INFO:root:current train perplexity3.5137274265289307

100%|██████████| 1/1 [18:05<00:00, 1085.94s/it][A100%|██████████| 1/1 [18:05<00:00, 1085.94s/it]
INFO:root:final mean train loss: 1593.7025742677504
INFO:root:final train perplexity: 3.5144729614257812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.45s/it][A100%|██████████| 1/1 [01:16<00:00, 76.45s/it]
INFO:root:eval mean loss: 1760.0935132182237
INFO:root:eval perplexity: 4.151461124420166
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.32s/it][A100%|██████████| 1/1 [01:13<00:00, 73.32s/it]
INFO:root:eval mean loss: 2210.473068345523
INFO:root:eval perplexity: 6.096930980682373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/16
  8%|▊         | 16/200 [5:33:58<63:40:33, 1245.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1558.206388231734
INFO:root:current train perplexity3.423910617828369
INFO:root:current mean train loss 1561.4554957339637
INFO:root:current train perplexity3.4418702125549316
INFO:root:current mean train loss 1558.8942181914495
INFO:root:current train perplexity3.4344630241394043
INFO:root:current mean train loss 1563.7726813879297
INFO:root:current train perplexity3.4376723766326904
INFO:root:current mean train loss 1565.118536493581
INFO:root:current train perplexity3.4362916946411133
INFO:root:current mean train loss 1565.9161659147192
INFO:root:current train perplexity3.4384515285491943
INFO:root:current mean train loss 1564.8673150280015
INFO:root:current train perplexity3.435621976852417
INFO:root:current mean train loss 1564.9809734972844
INFO:root:current train perplexity3.4382236003875732
INFO:root:current mean train loss 1568.1882734857115
INFO:root:current train perplexity3.442795515060425
INFO:root:current mean train loss 1571.415096263316
INFO:root:current train perplexity3.449711799621582
INFO:root:current mean train loss 1569.371096713425
INFO:root:current train perplexity3.4454357624053955
INFO:root:current mean train loss 1570.4647330423384
INFO:root:current train perplexity3.448301076889038
INFO:root:current mean train loss 1571.711779986846
INFO:root:current train perplexity3.4496142864227295
INFO:root:current mean train loss 1570.875008547593
INFO:root:current train perplexity3.4485480785369873
INFO:root:current mean train loss 1572.031776703109
INFO:root:current train perplexity3.452144145965576
INFO:root:current mean train loss 1574.6454698937778
INFO:root:current train perplexity3.4566879272460938
INFO:root:current mean train loss 1575.9513179795033
INFO:root:current train perplexity3.459291696548462
INFO:root:current mean train loss 1575.3703046009273
INFO:root:current train perplexity3.4587831497192383
INFO:root:current mean train loss 1575.103138387936
INFO:root:current train perplexity3.4599268436431885
INFO:root:current mean train loss 1574.2465593637035
INFO:root:current train perplexity3.459080934524536

100%|██████████| 1/1 [17:40<00:00, 1060.94s/it][A100%|██████████| 1/1 [17:40<00:00, 1060.94s/it]
INFO:root:final mean train loss: 1573.8709098134927
INFO:root:final train perplexity: 3.459933280944824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.71s/it][A100%|██████████| 1/1 [01:14<00:00, 74.71s/it]
INFO:root:eval mean loss: 1754.4753011067708
INFO:root:eval perplexity: 4.132640838623047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.48s/it][A100%|██████████| 1/1 [01:14<00:00, 74.48s/it]
INFO:root:eval mean loss: 2206.2164046916555
INFO:root:eval perplexity: 6.075744152069092
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/17
  8%|▊         | 17/200 [5:54:10<62:49:14, 1235.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1552.2359216863458
INFO:root:current train perplexity3.3760366439819336
INFO:root:current mean train loss 1542.7066689349235
INFO:root:current train perplexity3.3562350273132324
INFO:root:current mean train loss 1537.768165588379
INFO:root:current train perplexity3.3622426986694336
INFO:root:current mean train loss 1543.4287688265142
INFO:root:current train perplexity3.383178234100342
INFO:root:current mean train loss 1547.0505606229187
INFO:root:current train perplexity3.384087324142456
INFO:root:current mean train loss 1547.332854394199
INFO:root:current train perplexity3.3869810104370117
INFO:root:current mean train loss 1550.2614974975586
INFO:root:current train perplexity3.390812635421753
INFO:root:current mean train loss 1548.3422503011482
INFO:root:current train perplexity3.3860514163970947
INFO:root:current mean train loss 1552.2952160534558
INFO:root:current train perplexity3.3915159702301025
INFO:root:current mean train loss 1552.9113428525113
INFO:root:current train perplexity3.3950376510620117
INFO:root:current mean train loss 1555.029566708733
INFO:root:current train perplexity3.398033380508423
INFO:root:current mean train loss 1553.076944575968
INFO:root:current train perplexity3.3951375484466553
INFO:root:current mean train loss 1553.3414120881455
INFO:root:current train perplexity3.398283004760742
INFO:root:current mean train loss 1554.3983865787386
INFO:root:current train perplexity3.4001433849334717
INFO:root:current mean train loss 1555.5322058893018
INFO:root:current train perplexity3.4044454097747803
INFO:root:current mean train loss 1555.2772851746988
INFO:root:current train perplexity3.405979871749878
INFO:root:current mean train loss 1555.3927733796468
INFO:root:current train perplexity3.4069299697875977
INFO:root:current mean train loss 1555.7643288563295
INFO:root:current train perplexity3.408194065093994
INFO:root:current mean train loss 1555.9763140920866
INFO:root:current train perplexity3.4108519554138184

100%|██████████| 1/1 [17:38<00:00, 1058.07s/it][A100%|██████████| 1/1 [17:38<00:00, 1058.07s/it]
INFO:root:final mean train loss: 1556.2237096373865
INFO:root:final train perplexity: 3.4121127128601074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.78s/it][A100%|██████████| 1/1 [01:18<00:00, 78.78s/it]
INFO:root:eval mean loss: 1766.9081195804245
INFO:root:eval perplexity: 4.174403667449951
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.47s/it][A100%|██████████| 1/1 [01:15<00:00, 75.48s/it]
INFO:root:eval mean loss: 2223.277372752521
INFO:root:eval perplexity: 6.161113262176514
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/18
  9%|▉         | 18/200 [6:14:25<62:09:21, 1229.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.3174072265624
INFO:root:current train perplexity3.4046759605407715
INFO:root:current mean train loss 1525.0707380022322
INFO:root:current train perplexity3.3192124366760254
INFO:root:current mean train loss 1530.7330060261052
INFO:root:current train perplexity3.3530631065368652
INFO:root:current mean train loss 1528.527400182505
INFO:root:current train perplexity3.3465421199798584
INFO:root:current mean train loss 1523.0005781008874
INFO:root:current train perplexity3.340534210205078
INFO:root:current mean train loss 1526.5115483350094
INFO:root:current train perplexity3.3444716930389404
INFO:root:current mean train loss 1528.5396189792098
INFO:root:current train perplexity3.3492963314056396
INFO:root:current mean train loss 1529.355952529366
INFO:root:current train perplexity3.3474748134613037
INFO:root:current mean train loss 1532.050370153581
INFO:root:current train perplexity3.3484885692596436
INFO:root:current mean train loss 1532.760909578945
INFO:root:current train perplexity3.3488240242004395
INFO:root:current mean train loss 1534.35663212259
INFO:root:current train perplexity3.352996587753296
INFO:root:current mean train loss 1535.3169898676117
INFO:root:current train perplexity3.35496187210083
INFO:root:current mean train loss 1535.7707837623184
INFO:root:current train perplexity3.3561553955078125
INFO:root:current mean train loss 1536.8457713160021
INFO:root:current train perplexity3.3583567142486572
INFO:root:current mean train loss 1537.5391158460297
INFO:root:current train perplexity3.36086368560791
INFO:root:current mean train loss 1537.2408112281976
INFO:root:current train perplexity3.3601033687591553
INFO:root:current mean train loss 1536.3729982750438
INFO:root:current train perplexity3.358818769454956
INFO:root:current mean train loss 1537.4319754055168
INFO:root:current train perplexity3.3617312908172607
INFO:root:current mean train loss 1537.9665974371321
INFO:root:current train perplexity3.363584041595459
INFO:root:current mean train loss 1538.4553371319307
INFO:root:current train perplexity3.3639774322509766

100%|██████████| 1/1 [17:35<00:00, 1055.13s/it][A100%|██████████| 1/1 [17:35<00:00, 1055.13s/it]
INFO:root:final mean train loss: 1538.9757094236559
INFO:root:final train perplexity: 3.3660120964050293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.72s/it][A100%|██████████| 1/1 [01:16<00:00, 76.72s/it]
INFO:root:eval mean loss: 1777.062138117797
INFO:root:eval perplexity: 4.20882511138916
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 73.00s/it][A100%|██████████| 1/1 [01:12<00:00, 73.00s/it]
INFO:root:eval mean loss: 2234.128221011331
INFO:root:eval perplexity: 6.216029644012451
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/19
 10%|▉         | 19/200 [6:34:32<61:28:40, 1222.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1529.720497824929
INFO:root:current train perplexity3.27217173576355
INFO:root:current mean train loss 1512.8996812163807
INFO:root:current train perplexity3.280968427658081
INFO:root:current mean train loss 1509.012638676274
INFO:root:current train perplexity3.2847940921783447
INFO:root:current mean train loss 1507.8369288474137
INFO:root:current train perplexity3.27726411819458
INFO:root:current mean train loss 1509.182978480913
INFO:root:current train perplexity3.2842507362365723
INFO:root:current mean train loss 1511.7658099762782
INFO:root:current train perplexity3.2908103466033936
INFO:root:current mean train loss 1516.1115112304688
INFO:root:current train perplexity3.3037288188934326
INFO:root:current mean train loss 1516.888238204153
INFO:root:current train perplexity3.3051083087921143
INFO:root:current mean train loss 1519.010453347162
INFO:root:current train perplexity3.3065035343170166
INFO:root:current mean train loss 1517.2531321229749
INFO:root:current train perplexity3.3051328659057617
INFO:root:current mean train loss 1518.2116546332252
INFO:root:current train perplexity3.308881998062134
INFO:root:current mean train loss 1519.13885084618
INFO:root:current train perplexity3.309664487838745
INFO:root:current mean train loss 1519.1816982637645
INFO:root:current train perplexity3.312861442565918
INFO:root:current mean train loss 1520.8421574533436
INFO:root:current train perplexity3.3162753582000732
INFO:root:current mean train loss 1522.1705608126483
INFO:root:current train perplexity3.3195340633392334
INFO:root:current mean train loss 1523.3795206117568
INFO:root:current train perplexity3.3230338096618652
INFO:root:current mean train loss 1524.160863159029
INFO:root:current train perplexity3.3255701065063477
INFO:root:current mean train loss 1524.7146501530062
INFO:root:current train perplexity3.328460454940796
INFO:root:current mean train loss 1523.872468682466
INFO:root:current train perplexity3.3267245292663574
INFO:root:current mean train loss 1525.7373352368343
INFO:root:current train perplexity3.3299520015716553

100%|██████████| 1/1 [17:22<00:00, 1042.97s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.97s/it]
INFO:root:final mean train loss: 1526.0254879180554
INFO:root:final train perplexity: 3.3318095207214355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.05s/it][A100%|██████████| 1/1 [01:17<00:00, 77.05s/it]
INFO:root:eval mean loss: 1747.0008943165449
INFO:root:eval perplexity: 4.107734680175781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.62s/it][A100%|██████████| 1/1 [01:13<00:00, 73.62s/it]
INFO:root:eval mean loss: 2208.174274850399
INFO:root:eval perplexity: 6.085479736328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/20
 10%|█         | 20/200 [6:54:28<60:44:08, 1214.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1507.8389516977163
INFO:root:current train perplexity3.273038148880005
INFO:root:current mean train loss 1503.9125344255845
INFO:root:current train perplexity3.260563373565674
INFO:root:current mean train loss 1501.2772804164488
INFO:root:current train perplexity3.26786470413208
INFO:root:current mean train loss 1499.5009668400858
INFO:root:current train perplexity3.2704081535339355
INFO:root:current mean train loss 1496.9161460372472
INFO:root:current train perplexity3.2695670127868652
INFO:root:current mean train loss 1501.6704076650192
INFO:root:current train perplexity3.274427890777588
INFO:root:current mean train loss 1507.4533609261907
INFO:root:current train perplexity3.2809715270996094
INFO:root:current mean train loss 1509.9836207739554
INFO:root:current train perplexity3.288891553878784
INFO:root:current mean train loss 1508.4249963044267
INFO:root:current train perplexity3.284630537033081
INFO:root:current mean train loss 1509.1649121821752
INFO:root:current train perplexity3.2875778675079346
INFO:root:current mean train loss 1510.7151347684446
INFO:root:current train perplexity3.2910802364349365
INFO:root:current mean train loss 1511.751693337083
INFO:root:current train perplexity3.2943265438079834
INFO:root:current mean train loss 1510.7000802373386
INFO:root:current train perplexity3.293025255203247
INFO:root:current mean train loss 1508.5313264876715
INFO:root:current train perplexity3.2903525829315186
INFO:root:current mean train loss 1511.1038544358605
INFO:root:current train perplexity3.29240345954895
INFO:root:current mean train loss 1511.3566928637965
INFO:root:current train perplexity3.2925939559936523
INFO:root:current mean train loss 1511.4885510112397
INFO:root:current train perplexity3.292788505554199
INFO:root:current mean train loss 1512.3661709259543
INFO:root:current train perplexity3.2955048084259033
INFO:root:current mean train loss 1511.7383139082892
INFO:root:current train perplexity3.2948484420776367
INFO:root:current mean train loss 1511.943619884001
INFO:root:current train perplexity3.2946760654449463

100%|██████████| 1/1 [17:29<00:00, 1049.99s/it][A100%|██████████| 1/1 [17:29<00:00, 1049.99s/it]
INFO:root:final mean train loss: 1512.0398905836328
INFO:root:final train perplexity: 3.295262098312378
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.87s/it][A100%|██████████| 1/1 [01:17<00:00, 77.87s/it]
INFO:root:eval mean loss: 1758.8283669762577
INFO:root:eval perplexity: 4.147215366363525
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.01s/it][A100%|██████████| 1/1 [01:12<00:00, 72.01s/it]
INFO:root:eval mean loss: 2224.3759705022717
INFO:root:eval perplexity: 6.16664981842041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/21
 10%|█         | 21/200 [7:14:30<60:12:41, 1210.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1482.649448939732
INFO:root:current train perplexity3.214073419570923
INFO:root:current mean train loss 1477.2794666779348
INFO:root:current train perplexity3.215419292449951
INFO:root:current mean train loss 1480.4409432411194
INFO:root:current train perplexity3.215684652328491
INFO:root:current mean train loss 1483.5572376036912
INFO:root:current train perplexity3.2256152629852295
INFO:root:current mean train loss 1487.3520205313698
INFO:root:current train perplexity3.22995924949646
INFO:root:current mean train loss 1490.3145565334842
INFO:root:current train perplexity3.2382030487060547
INFO:root:current mean train loss 1492.6033414515052
INFO:root:current train perplexity3.244640588760376
INFO:root:current mean train loss 1490.5800645616318
INFO:root:current train perplexity3.2431421279907227
INFO:root:current mean train loss 1494.844360494168
INFO:root:current train perplexity3.2497525215148926
INFO:root:current mean train loss 1495.5504613900284
INFO:root:current train perplexity3.2502148151397705
INFO:root:current mean train loss 1495.2939946723707
INFO:root:current train perplexity3.2506017684936523
INFO:root:current mean train loss 1494.0354858187243
INFO:root:current train perplexity3.247103452682495
INFO:root:current mean train loss 1495.3301014505373
INFO:root:current train perplexity3.2502925395965576
INFO:root:current mean train loss 1495.795038994083
INFO:root:current train perplexity3.2505569458007812
INFO:root:current mean train loss 1496.3255392221304
INFO:root:current train perplexity3.2504379749298096
INFO:root:current mean train loss 1497.270451876682
INFO:root:current train perplexity3.251203775405884
INFO:root:current mean train loss 1497.697175325403
INFO:root:current train perplexity3.253479242324829
INFO:root:current mean train loss 1497.1753321758436
INFO:root:current train perplexity3.2535715103149414
INFO:root:current mean train loss 1497.1256658619848
INFO:root:current train perplexity3.2536630630493164
INFO:root:current mean train loss 1496.515645345052
INFO:root:current train perplexity3.2534656524658203

100%|██████████| 1/1 [17:23<00:00, 1043.70s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.70s/it]
INFO:root:final mean train loss: 1496.0737480128948
INFO:root:final train perplexity: 3.2540283203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.13s/it][A100%|██████████| 1/1 [01:14<00:00, 74.13s/it]
INFO:root:eval mean loss: 1762.7110954988088
INFO:root:eval perplexity: 4.1602582931518555
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.13s/it][A100%|██████████| 1/1 [01:13<00:00, 73.13s/it]
INFO:root:eval mean loss: 2227.994343209774
INFO:root:eval perplexity: 6.1849260330200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/22
 11%|█         | 22/200 [7:34:24<59:36:48, 1205.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.814530046019
INFO:root:current train perplexity3.1767873764038086
INFO:root:current mean train loss 1467.594713155934
INFO:root:current train perplexity3.1714835166931152
INFO:root:current mean train loss 1472.4301257011218
INFO:root:current train perplexity3.18104887008667
INFO:root:current mean train loss 1473.4122438814302
INFO:root:current train perplexity3.19220232963562
INFO:root:current mean train loss 1474.7299864045158
INFO:root:current train perplexity3.1973769664764404
INFO:root:current mean train loss 1475.7524771964986
INFO:root:current train perplexity3.19897198677063
INFO:root:current mean train loss 1477.1864617675055
INFO:root:current train perplexity3.202791690826416
INFO:root:current mean train loss 1476.1828207432993
INFO:root:current train perplexity3.2019436359405518
INFO:root:current mean train loss 1474.7682719541988
INFO:root:current train perplexity3.200483560562134
INFO:root:current mean train loss 1477.8319012758543
INFO:root:current train perplexity3.2063167095184326
INFO:root:current mean train loss 1478.1504147432724
INFO:root:current train perplexity3.2073991298675537
INFO:root:current mean train loss 1476.453071093417
INFO:root:current train perplexity3.20481014251709
INFO:root:current mean train loss 1477.2267252284528
INFO:root:current train perplexity3.206590175628662
INFO:root:current mean train loss 1478.8817711237652
INFO:root:current train perplexity3.208747148513794
INFO:root:current mean train loss 1479.1584374867405
INFO:root:current train perplexity3.2104406356811523
INFO:root:current mean train loss 1479.952994393302
INFO:root:current train perplexity3.211979389190674
INFO:root:current mean train loss 1480.8133501142922
INFO:root:current train perplexity3.2142815589904785
INFO:root:current mean train loss 1480.171377286357
INFO:root:current train perplexity3.212630033493042
INFO:root:current mean train loss 1480.725214643015
INFO:root:current train perplexity3.214770793914795
INFO:root:current mean train loss 1480.518812377744
INFO:root:current train perplexity3.213719129562378

100%|██████████| 1/1 [17:42<00:00, 1062.39s/it][A100%|██████████| 1/1 [17:42<00:00, 1062.39s/it]
INFO:root:final mean train loss: 1480.4287430709862
INFO:root:final train perplexity: 3.214125156402588
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.21s/it][A100%|██████████| 1/1 [01:15<00:00, 75.21s/it]
INFO:root:eval mean loss: 1797.2674032441269
INFO:root:eval perplexity: 4.2781662940979
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.24s/it][A100%|██████████| 1/1 [01:17<00:00, 77.81s/it]
INFO:root:eval mean loss: 2268.231612401651
INFO:root:eval perplexity: 6.391839981079102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/23
 12%|█▏        | 23/200 [7:54:42<59:27:43, 1209.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.1216796875
INFO:root:current train perplexity3.157554864883423
INFO:root:current mean train loss 1457.3956555818256
INFO:root:current train perplexity3.1695632934570312
INFO:root:current mean train loss 1455.9729812095904
INFO:root:current train perplexity3.1639983654022217
INFO:root:current mean train loss 1462.1912556966147
INFO:root:current train perplexity3.1725385189056396
INFO:root:current mean train loss 1460.8492855149873
INFO:root:current train perplexity3.1662545204162598
INFO:root:current mean train loss 1464.905595372087
INFO:root:current train perplexity3.174085855484009
INFO:root:current mean train loss 1458.634445588485
INFO:root:current train perplexity3.1645376682281494
INFO:root:current mean train loss 1459.635860085789
INFO:root:current train perplexity3.1664459705352783
INFO:root:current mean train loss 1459.8816270463922
INFO:root:current train perplexity3.163316488265991
INFO:root:current mean train loss 1458.6359551323785
INFO:root:current train perplexity3.160240411758423
INFO:root:current mean train loss 1460.5245373647147
INFO:root:current train perplexity3.165407419204712
INFO:root:current mean train loss 1462.140944024094
INFO:root:current train perplexity3.16691517829895
INFO:root:current mean train loss 1461.0547821281493
INFO:root:current train perplexity3.16487455368042
INFO:root:current mean train loss 1463.4056663458296
INFO:root:current train perplexity3.1711044311523438
INFO:root:current mean train loss 1463.9551338349413
INFO:root:current train perplexity3.172802209854126
INFO:root:current mean train loss 1464.2025081226661
INFO:root:current train perplexity3.173407793045044
INFO:root:current mean train loss 1464.4027968547753
INFO:root:current train perplexity3.173125982284546
INFO:root:current mean train loss 1465.4215646413452
INFO:root:current train perplexity3.1745049953460693
INFO:root:current mean train loss 1465.6175557454428
INFO:root:current train perplexity3.1755902767181396

100%|██████████| 1/1 [18:01<00:00, 1081.22s/it][A100%|██████████| 1/1 [18:01<00:00, 1081.22s/it]
INFO:root:final mean train loss: 1473.2718536946368
INFO:root:final train perplexity: 3.1960344314575195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.86s/it][A100%|██████████| 1/1 [01:15<00:00, 75.86s/it]
INFO:root:eval mean loss: 3368.0096712585882
INFO:root:eval perplexity: 15.23885726928711
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.69s/it][A100%|██████████| 1/1 [01:14<00:00, 74.69s/it]
INFO:root:eval mean loss: 3814.282461179909
INFO:root:eval perplexity: 22.633358001708984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/24
 12%|█▏        | 24/200 [8:15:16<59:29:20, 1216.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5351.086704799107
INFO:root:current train perplexity64.73465728759766
INFO:root:current mean train loss 9138.985278548482
INFO:root:current train perplexity1398.5946044921875
INFO:root:current mean train loss 8366.585159080616
INFO:root:current train perplexity743.69384765625
INFO:root:current mean train loss 7794.625664825937
INFO:root:current train perplexity470.1170654296875
INFO:root:current mean train loss 7445.646008090832
INFO:root:current train perplexity358.08868408203125
INFO:root:current mean train loss 7258.965474567
INFO:root:current train perplexity306.6772155761719
INFO:root:current mean train loss 7137.453408959277
INFO:root:current train perplexity275.4570007324219
INFO:root:current mean train loss 7029.426766790797
INFO:root:current train perplexity253.5767364501953
INFO:root:current mean train loss 6955.764352564475
INFO:root:current train perplexity239.7417449951172
INFO:root:current mean train loss 6903.440170742317
INFO:root:current train perplexity230.67581176757812
INFO:root:current mean train loss 6881.851225503507
INFO:root:current train perplexity226.71153259277344
INFO:root:current mean train loss 6846.5300648042285
INFO:root:current train perplexity220.06161499023438
INFO:root:current mean train loss 6812.943531709559
INFO:root:current train perplexity214.72457885742188
INFO:root:current mean train loss 6787.265060506527
INFO:root:current train perplexity210.38735961914062
INFO:root:current mean train loss 6763.139488800418
INFO:root:current train perplexity206.7666015625
INFO:root:current mean train loss 6742.126486228331
INFO:root:current train perplexity203.10935974121094
INFO:root:current mean train loss 6725.938912886007
INFO:root:current train perplexity200.44903564453125
INFO:root:current mean train loss 6711.3306007318215
INFO:root:current train perplexity198.3894805908203
INFO:root:current mean train loss 6697.757796287009
INFO:root:current train perplexity196.3907012939453
INFO:root:current mean train loss 6683.455856251229
INFO:root:current train perplexity194.35812377929688

100%|██████████| 1/1 [17:26<00:00, 1046.20s/it][A100%|██████████| 1/1 [17:26<00:00, 1046.20s/it]
INFO:root:final mean train loss: 6673.3800043287865
INFO:root:final train perplexity: 193.06541442871094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.69s/it][A100%|██████████| 1/1 [01:16<00:00, 76.69s/it]
INFO:root:eval mean loss: 6151.779440588985
INFO:root:eval perplexity: 144.7774658203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.16s/it][A100%|██████████| 1/1 [01:12<00:00, 72.16s/it]
INFO:root:eval mean loss: 6291.22939176086
INFO:root:eval perplexity: 171.595458984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/25
 12%|█▎        | 25/200 [8:35:13<58:52:01, 1210.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6515.779541015625
INFO:root:current train perplexity162.43650817871094
INFO:root:current mean train loss 6496.765703755041
INFO:root:current train perplexity160.5861053466797
INFO:root:current mean train loss 6501.241481236049
INFO:root:current train perplexity161.2791290283203
INFO:root:current mean train loss 6482.909389166184
INFO:root:current train perplexity161.88340759277344
INFO:root:current mean train loss 6501.1990817088
INFO:root:current train perplexity162.9508514404297
INFO:root:current mean train loss 6495.419340410305
INFO:root:current train perplexity162.51800537109375
INFO:root:current mean train loss 6477.897385817308
INFO:root:current train perplexity161.48365783691406
INFO:root:current mean train loss 6460.885824466937
INFO:root:current train perplexity160.6282196044922
INFO:root:current mean train loss 6447.733922865785
INFO:root:current train perplexity159.82516479492188
INFO:root:current mean train loss 6448.810570654931
INFO:root:current train perplexity160.3560791015625
INFO:root:current mean train loss 6455.870217323303
INFO:root:current train perplexity161.1561279296875
INFO:root:current mean train loss 6459.42909843472
INFO:root:current train perplexity161.61268615722656
INFO:root:current mean train loss 6462.600474638098
INFO:root:current train perplexity162.03631591796875
INFO:root:current mean train loss 6465.798266454046
INFO:root:current train perplexity162.49391174316406
INFO:root:current mean train loss 6467.28022200338
INFO:root:current train perplexity162.9512481689453
INFO:root:current mean train loss 6471.345953673515
INFO:root:current train perplexity163.5373077392578
INFO:root:current mean train loss 6467.31064128406
INFO:root:current train perplexity163.4370574951172
INFO:root:current mean train loss 6465.434675389265
INFO:root:current train perplexity163.4307403564453
INFO:root:current mean train loss 6460.494285181949
INFO:root:current train perplexity163.0515899658203
INFO:root:current mean train loss 6463.060913847291
INFO:root:current train perplexity163.1470947265625

100%|██████████| 1/1 [17:30<00:00, 1050.06s/it][A100%|██████████| 1/1 [17:30<00:00, 1050.06s/it]
INFO:root:final mean train loss: 6461.491434640735
INFO:root:final train perplexity: 163.35423278808594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.01s/it][A100%|██████████| 1/1 [01:15<00:00, 75.01s/it]
INFO:root:eval mean loss: 6176.2026852005765
INFO:root:eval perplexity: 147.66554260253906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.21s/it][A100%|██████████| 1/1 [01:12<00:00, 72.21s/it]
INFO:root:eval mean loss: 6320.142789367243
INFO:root:eval perplexity: 175.7014923095703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/26
 13%|█▎        | 26/200 [8:55:13<58:21:55, 1207.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6594.264124428354
INFO:root:current train perplexity167.9801025390625
INFO:root:current mean train loss 6509.9755824745125
INFO:root:current train perplexity163.918212890625
INFO:root:current mean train loss 6513.295189315353
INFO:root:current train perplexity164.87332153320312
INFO:root:current mean train loss 6674.067021971225
INFO:root:current train perplexity187.05181884765625
INFO:root:current mean train loss 6645.849046910431
INFO:root:current train perplexity182.6460418701172
INFO:root:current mean train loss 6659.440911650878
INFO:root:current train perplexity186.99118041992188
INFO:root:current mean train loss 6752.327780689353
INFO:root:current train perplexity201.59103393554688
INFO:root:current mean train loss 6909.034192207532
INFO:root:current train perplexity230.01766967773438
INFO:root:current mean train loss 6926.760432149227
INFO:root:current train perplexity233.4522705078125
INFO:root:current mean train loss 6860.836404506509
INFO:root:current train perplexity223.41209411621094
INFO:root:current mean train loss 6823.048838444104
INFO:root:current train perplexity216.1085968017578
INFO:root:current mean train loss 6791.8590814321315
INFO:root:current train perplexity210.3578643798828
INFO:root:current mean train loss 6756.748360460944
INFO:root:current train perplexity205.41831970214844
INFO:root:current mean train loss 6731.891623045419
INFO:root:current train perplexity201.4554443359375
INFO:root:current mean train loss 6713.032940178262
INFO:root:current train perplexity198.3212890625
INFO:root:current mean train loss 6697.846100784292
INFO:root:current train perplexity196.18328857421875
INFO:root:current mean train loss 6684.794341352929
INFO:root:current train perplexity194.34124755859375
INFO:root:current mean train loss 6673.272895650847
INFO:root:current train perplexity192.56094360351562
INFO:root:current mean train loss 6661.062876355836
INFO:root:current train perplexity190.99264526367188
INFO:root:current mean train loss 6652.712901945276
INFO:root:current train perplexity189.5870361328125

100%|██████████| 1/1 [17:29<00:00, 1049.89s/it][A100%|██████████| 1/1 [17:29<00:00, 1049.89s/it]
INFO:root:final mean train loss: 6647.288588130949
INFO:root:final train perplexity: 189.13331604003906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.31s/it][A100%|██████████| 1/1 [01:14<00:00, 74.31s/it]
INFO:root:eval mean loss: 6246.6129297567595
INFO:root:eval perplexity: 156.3182830810547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.00s/it][A100%|██████████| 1/1 [01:18<00:00, 78.00s/it]
INFO:root:eval mean loss: 6367.412934431793
INFO:root:eval perplexity: 182.6268768310547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/27
 14%|█▎        | 27/200 [9:15:18<57:59:13, 1206.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6440.324698612608
INFO:root:current train perplexity164.5016326904297
INFO:root:current mean train loss 6470.77353021163
INFO:root:current train perplexity164.41944885253906
INFO:root:current mean train loss 6467.885738402374
INFO:root:current train perplexity164.64712524414062
INFO:root:current mean train loss 6460.906934684881
INFO:root:current train perplexity164.07797241210938
INFO:root:current mean train loss 6460.876720711654
INFO:root:current train perplexity163.79444885253906
INFO:root:current mean train loss 6454.088364030298
INFO:root:current train perplexity163.7618865966797
INFO:root:current mean train loss 6459.914692516385
INFO:root:current train perplexity163.55470275878906
INFO:root:current mean train loss 6458.705460118115
INFO:root:current train perplexity163.37791442871094
INFO:root:current mean train loss 6453.6089931390225
INFO:root:current train perplexity163.10003662109375
INFO:root:current mean train loss 6459.620632482222
INFO:root:current train perplexity163.1540069580078
INFO:root:current mean train loss 6457.996828017929
INFO:root:current train perplexity163.15151977539062
INFO:root:current mean train loss 6458.462314216996
INFO:root:current train perplexity163.20497131347656
INFO:root:current mean train loss 6460.406484437102
INFO:root:current train perplexity163.23345947265625
INFO:root:current mean train loss 6463.428557405767
INFO:root:current train perplexity163.3050079345703
INFO:root:current mean train loss 6462.660142854081
INFO:root:current train perplexity163.19517517089844
INFO:root:current mean train loss 6463.80626328827
INFO:root:current train perplexity163.22030639648438
INFO:root:current mean train loss 6464.034963057901
INFO:root:current train perplexity163.2431182861328
INFO:root:current mean train loss 6465.125147762016
INFO:root:current train perplexity163.43214416503906
INFO:root:current mean train loss 6465.673907753212
INFO:root:current train perplexity163.46908569335938
INFO:root:current mean train loss 6465.378817720968
INFO:root:current train perplexity163.5904998779297

100%|██████████| 1/1 [17:25<00:00, 1045.91s/it][A100%|██████████| 1/1 [17:25<00:00, 1045.92s/it]
INFO:root:final mean train loss: 6463.012764011677
INFO:root:final train perplexity: 163.55032348632812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.71s/it][A100%|██████████| 1/1 [01:20<00:00, 80.71s/it]
INFO:root:eval mean loss: 6192.803601853391
INFO:root:eval perplexity: 149.66151428222656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.36s/it][A100%|██████████| 1/1 [01:16<00:00, 76.36s/it]
INFO:root:eval mean loss: 6340.847445007757
INFO:root:eval perplexity: 178.7019805908203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/28
 14%|█▍        | 28/200 [9:35:23<57:37:55, 1206.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6468.973854166667
INFO:root:current train perplexity167.96409606933594
INFO:root:current mean train loss 6471.606157924107
INFO:root:current train perplexity167.82615661621094
INFO:root:current mean train loss 6455.640854048295
INFO:root:current train perplexity166.9542236328125
INFO:root:current mean train loss 6478.370557291667
INFO:root:current train perplexity168.1817626953125
INFO:root:current mean train loss 6498.285888157895
INFO:root:current train perplexity169.09799194335938
INFO:root:current mean train loss 6498.786959918479
INFO:root:current train perplexity169.1034698486328
INFO:root:current mean train loss 6479.252533998842
INFO:root:current train perplexity167.545654296875
INFO:root:current mean train loss 6461.426574470766
INFO:root:current train perplexity165.91339111328125
INFO:root:current mean train loss 6461.30586328125
INFO:root:current train perplexity165.75152587890625
INFO:root:current mean train loss 6470.015308994391
INFO:root:current train perplexity165.75753784179688
INFO:root:current mean train loss 6469.0390257085755
INFO:root:current train perplexity165.66073608398438
INFO:root:current mean train loss 6475.746806017287
INFO:root:current train perplexity166.04476928710938
INFO:root:current mean train loss 6484.129292662377
INFO:root:current train perplexity166.83517456054688
INFO:root:current mean train loss 6489.5050628551135
INFO:root:current train perplexity166.95986938476562
INFO:root:current mean train loss 6486.811906779661
INFO:root:current train perplexity166.6616668701172
INFO:root:current mean train loss 6485.46746186756
INFO:root:current train perplexity166.18704223632812
INFO:root:current mean train loss 6486.145971023788
INFO:root:current train perplexity165.99609375
INFO:root:current mean train loss 6490.856553147007
INFO:root:current train perplexity166.44549560546875
INFO:root:current mean train loss 6492.267737239584
INFO:root:current train perplexity166.93479919433594
INFO:root:current mean train loss 6496.8257332871835
INFO:root:current train perplexity167.74855041503906

100%|██████████| 1/1 [17:33<00:00, 1053.43s/it][A100%|██████████| 1/1 [17:33<00:00, 1053.43s/it]
INFO:root:final mean train loss: 6495.960401941897
INFO:root:final train perplexity: 167.8558349609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.90s/it][A100%|██████████| 1/1 [01:13<00:00, 73.91s/it]
INFO:root:eval mean loss: 6539.688422886193
INFO:root:eval perplexity: 198.12864685058594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.07s/it][A100%|██████████| 1/1 [01:12<00:00, 72.07s/it]
INFO:root:eval mean loss: 6624.5761164671985
INFO:root:eval perplexity: 225.3737030029297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/29
 14%|█▍        | 29/200 [9:55:25<57:14:11, 1204.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6534.9283022673235
INFO:root:current train perplexity181.22311401367188
INFO:root:current mean train loss 6593.132502237956
INFO:root:current train perplexity182.99395751953125
INFO:root:current mean train loss 6619.059006782427
INFO:root:current train perplexity184.34249877929688
INFO:root:current mean train loss 6620.0889493981185
INFO:root:current train perplexity183.59483337402344
INFO:root:current mean train loss 6619.468897873793
INFO:root:current train perplexity183.33615112304688
INFO:root:current mean train loss 6607.751542374895
INFO:root:current train perplexity182.4391326904297
INFO:root:current mean train loss 6604.7085969958
INFO:root:current train perplexity181.4131622314453
INFO:root:current mean train loss 6603.830292056306
INFO:root:current train perplexity180.78225708007812
INFO:root:current mean train loss 6599.514001957504
INFO:root:current train perplexity181.3251190185547
INFO:root:current mean train loss 6598.425444572203
INFO:root:current train perplexity181.86032104492188
INFO:root:current mean train loss 6598.759704366272
INFO:root:current train perplexity181.71279907226562
INFO:root:current mean train loss 6591.454136790845
INFO:root:current train perplexity180.9906463623047
INFO:root:current mean train loss 6595.075610653904
INFO:root:current train perplexity181.23521423339844
INFO:root:current mean train loss 6596.289297871206
INFO:root:current train perplexity181.77215576171875
INFO:root:current mean train loss 6602.123456612349
INFO:root:current train perplexity182.59535217285156
INFO:root:current mean train loss 6607.32621297405
INFO:root:current train perplexity183.34271240234375
INFO:root:current mean train loss 6613.037246740174
INFO:root:current train perplexity184.00711059570312
INFO:root:current mean train loss 6620.568592888968
INFO:root:current train perplexity184.97630310058594
INFO:root:current mean train loss 6623.86035078827
INFO:root:current train perplexity185.5944061279297

100%|██████████| 1/1 [17:21<00:00, 1041.60s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.60s/it]
INFO:root:final mean train loss: 6616.5063315279485
INFO:root:final train perplexity: 184.5969696044922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.21s/it][A100%|██████████| 1/1 [01:15<00:00, 75.21s/it]
INFO:root:eval mean loss: 6241.123997465093
INFO:root:eval perplexity: 155.62571716308594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.76s/it][A100%|██████████| 1/1 [01:11<00:00, 71.76s/it]
INFO:root:eval mean loss: 6380.186344227893
INFO:root:eval perplexity: 184.54478454589844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/30
 15%|█▌        | 30/200 [10:15:16<56:42:05, 1200.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6280.945475260417
INFO:root:current train perplexity158.49649047851562
INFO:root:current mean train loss 6424.13023222477
INFO:root:current train perplexity162.25643920898438
INFO:root:current mean train loss 6440.8232562051435
INFO:root:current train perplexity162.1121368408203
INFO:root:current mean train loss 6434.778445148159
INFO:root:current train perplexity160.2012481689453
INFO:root:current mean train loss 6415.272101591152
INFO:root:current train perplexity158.42494201660156
INFO:root:current mean train loss 6409.069559453278
INFO:root:current train perplexity157.20631408691406
INFO:root:current mean train loss 6408.431201252052
INFO:root:current train perplexity156.2201385498047
INFO:root:current mean train loss 6402.535968904266
INFO:root:current train perplexity155.58126831054688
INFO:root:current mean train loss 6395.447086367236
INFO:root:current train perplexity155.0087890625
INFO:root:current mean train loss 6388.618421900784
INFO:root:current train perplexity154.58554077148438
INFO:root:current mean train loss 6387.105509399777
INFO:root:current train perplexity154.19903564453125
INFO:root:current mean train loss 6383.031240313627
INFO:root:current train perplexity153.89186096191406
INFO:root:current mean train loss 6381.673880224488
INFO:root:current train perplexity153.57400512695312
INFO:root:current mean train loss 6381.408826811975
INFO:root:current train perplexity153.4929656982422
INFO:root:current mean train loss 6385.26930807532
INFO:root:current train perplexity153.4737091064453
INFO:root:current mean train loss 6382.790710166087
INFO:root:current train perplexity153.32302856445312
INFO:root:current mean train loss 6384.434901396928
INFO:root:current train perplexity153.222900390625
INFO:root:current mean train loss 6382.428271227234
INFO:root:current train perplexity153.11553955078125
INFO:root:current mean train loss 6384.47866219035
INFO:root:current train perplexity153.13006591796875
INFO:root:current mean train loss 6380.810424612853
INFO:root:current train perplexity152.9926300048828

100%|██████████| 1/1 [17:14<00:00, 1034.87s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.87s/it]
INFO:root:final mean train loss: 6377.8710111386235
INFO:root:final train perplexity: 152.92881774902344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.08s/it][A100%|██████████| 1/1 [01:14<00:00, 74.08s/it]
INFO:root:eval mean loss: 6127.143329593307
INFO:root:eval perplexity: 141.9214324951172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.29s/it][A100%|██████████| 1/1 [01:12<00:00, 72.29s/it]
INFO:root:eval mean loss: 6258.189559611868
INFO:root:eval perplexity: 167.02102661132812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/31
 16%|█▌        | 31/200 [10:34:59<56:07:35, 1195.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6418.667799729567
INFO:root:current train perplexity151.94064331054688
INFO:root:current mean train loss 6308.095420231894
INFO:root:current train perplexity149.13449096679688
INFO:root:current mean train loss 6356.512172462666
INFO:root:current train perplexity150.94927978515625
INFO:root:current mean train loss 6364.193350388228
INFO:root:current train perplexity150.83761596679688
INFO:root:current mean train loss 6353.770530736502
INFO:root:current train perplexity150.5723114013672
INFO:root:current mean train loss 6349.9730693396505
INFO:root:current train perplexity150.54165649414062
INFO:root:current mean train loss 6354.4205279365515
INFO:root:current train perplexity150.63693237304688
INFO:root:current mean train loss 6358.17972381844
INFO:root:current train perplexity151.0323028564453
INFO:root:current mean train loss 6364.244919155758
INFO:root:current train perplexity151.72793579101562
INFO:root:current mean train loss 6387.769723715071
INFO:root:current train perplexity153.8119659423828
INFO:root:current mean train loss 6404.216151544225
INFO:root:current train perplexity156.17813110351562
INFO:root:current mean train loss 6419.757571828513
INFO:root:current train perplexity158.3333282470703
INFO:root:current mean train loss 6436.448663957356
INFO:root:current train perplexity160.21461486816406
INFO:root:current mean train loss 6450.635888745523
INFO:root:current train perplexity161.94277954101562
INFO:root:current mean train loss 6459.027370800644
INFO:root:current train perplexity163.16934204101562
INFO:root:current mean train loss 6471.812653267837
INFO:root:current train perplexity164.4728546142578
INFO:root:current mean train loss 6484.514083580777
INFO:root:current train perplexity165.8255157470703
INFO:root:current mean train loss 6493.797225510121
INFO:root:current train perplexity166.88377380371094
INFO:root:current mean train loss 6496.783644877807
INFO:root:current train perplexity167.4995574951172
INFO:root:current mean train loss 6499.625603633259
INFO:root:current train perplexity168.2711944580078

100%|██████████| 1/1 [17:21<00:00, 1041.51s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.51s/it]
INFO:root:final mean train loss: 6503.101053904477
INFO:root:final train perplexity: 168.80369567871094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.35s/it][A100%|██████████| 1/1 [01:15<00:00, 75.35s/it]
INFO:root:eval mean loss: 6436.4916957557625
INFO:root:eval perplexity: 182.26400756835938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.10s/it][A100%|██████████| 1/1 [01:12<00:00, 72.10s/it]
INFO:root:eval mean loss: 6531.713084552305
INFO:root:eval perplexity: 208.8914031982422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/32
 16%|█▌        | 32/200 [10:54:51<55:44:01, 1194.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6579.782533157703
INFO:root:current train perplexity187.71041870117188
INFO:root:current mean train loss 6609.237400295017
INFO:root:current train perplexity185.52017211914062
INFO:root:current mean train loss 6617.863444010417
INFO:root:current train perplexity185.91522216796875
INFO:root:current mean train loss 6619.430000683309
INFO:root:current train perplexity186.10202026367188
INFO:root:current mean train loss 6619.47566309255
INFO:root:current train perplexity186.4180145263672
INFO:root:current mean train loss 6613.194303565262
INFO:root:current train perplexity186.2984161376953
INFO:root:current mean train loss 6612.978725973221
INFO:root:current train perplexity186.23358154296875
INFO:root:current mean train loss 6619.858272259842
INFO:root:current train perplexity186.57293701171875
INFO:root:current mean train loss 6622.810470997368
INFO:root:current train perplexity186.64625549316406
INFO:root:current mean train loss 6623.81419785177
INFO:root:current train perplexity186.65292358398438
INFO:root:current mean train loss 6627.746015100671
INFO:root:current train perplexity186.8360137939453
INFO:root:current mean train loss 6629.7112920699365
INFO:root:current train perplexity186.8545379638672
INFO:root:current mean train loss 6631.424422076127
INFO:root:current train perplexity186.981201171875
INFO:root:current mean train loss 6629.548854665976
INFO:root:current train perplexity186.8416290283203
INFO:root:current mean train loss 6634.356704849381
INFO:root:current train perplexity187.18690490722656
INFO:root:current mean train loss 6635.755044201636
INFO:root:current train perplexity187.06654357910156
INFO:root:current mean train loss 6634.762025448874
INFO:root:current train perplexity187.2405548095703
INFO:root:current mean train loss 6634.141292569833
INFO:root:current train perplexity187.04254150390625
INFO:root:current mean train loss 6634.868662941281
INFO:root:current train perplexity186.9608612060547
INFO:root:current mean train loss 6634.625566436406
INFO:root:current train perplexity186.84678649902344

100%|██████████| 1/1 [17:16<00:00, 1036.54s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.54s/it]
INFO:root:final mean train loss: 6631.374225595295
INFO:root:final train perplexity: 186.77427673339844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.57s/it][A100%|██████████| 1/1 [01:14<00:00, 74.57s/it]
INFO:root:eval mean loss: 6449.51944467531
INFO:root:eval perplexity: 184.1946258544922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.67s/it][A100%|██████████| 1/1 [01:11<00:00, 71.67s/it]
INFO:root:eval mean loss: 6532.598132410793
INFO:root:eval perplexity: 209.04266357421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/33
 16%|█▋        | 33/200 [11:14:36<55:16:30, 1191.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6631.488313802083
INFO:root:current train perplexity184.44114685058594
INFO:root:current mean train loss 6589.869287109375
INFO:root:current train perplexity184.58738708496094
INFO:root:current mean train loss 6598.201438551682
INFO:root:current train perplexity184.57057189941406
INFO:root:current mean train loss 6596.072819010416
INFO:root:current train perplexity184.9375
INFO:root:current mean train loss 6608.705727751359
INFO:root:current train perplexity184.67153930664062
INFO:root:current mean train loss 6606.191573660714
INFO:root:current train perplexity184.1846923828125
INFO:root:current mean train loss 6610.809460079308
INFO:root:current train perplexity184.0907440185547
INFO:root:current mean train loss 6618.715307617187
INFO:root:current train perplexity184.44854736328125
INFO:root:current mean train loss 6619.083834484011
INFO:root:current train perplexity184.56423950195312
INFO:root:current mean train loss 6614.88587290446
INFO:root:current train perplexity184.53871154785156
INFO:root:current mean train loss 6611.4125271779185
INFO:root:current train perplexity184.7203369140625
INFO:root:current mean train loss 6610.505081492457
INFO:root:current train perplexity184.98944091796875
INFO:root:current mean train loss 6614.641290380085
INFO:root:current train perplexity185.36569213867188
INFO:root:current mean train loss 6616.426615277459
INFO:root:current train perplexity185.44021606445312
INFO:root:current mean train loss 6620.056662697988
INFO:root:current train perplexity185.81959533691406
INFO:root:current mean train loss 6622.9054740710135
INFO:root:current train perplexity186.1807403564453
INFO:root:current mean train loss 6623.993167003953
INFO:root:current train perplexity186.35536193847656
INFO:root:current mean train loss 6623.958489435369
INFO:root:current train perplexity186.48069763183594
INFO:root:current mean train loss 6628.040331768733
INFO:root:current train perplexity186.6195526123047
INFO:root:current mean train loss 6631.061179896763
INFO:root:current train perplexity186.55674743652344

100%|██████████| 1/1 [17:20<00:00, 1040.46s/it][A100%|██████████| 1/1 [17:20<00:00, 1040.46s/it]
INFO:root:final mean train loss: 6630.192189149765
INFO:root:final train perplexity: 186.60025024414062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.47s/it][A100%|██████████| 1/1 [01:14<00:00, 74.47s/it]
INFO:root:eval mean loss: 6491.239959067487
INFO:root:eval perplexity: 190.51547241210938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.51s/it][A100%|██████████| 1/1 [01:12<00:00, 72.51s/it]
INFO:root:eval mean loss: 6570.100988509807
INFO:root:eval perplexity: 215.553466796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/34
 17%|█▋        | 34/200 [11:34:25<54:55:05, 1191.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6589.088594510958
INFO:root:current train perplexity184.44212341308594
INFO:root:current mean train loss 6641.83688923464
INFO:root:current train perplexity186.4111785888672
INFO:root:current mean train loss 6650.290955691561
INFO:root:current train perplexity185.8995361328125
INFO:root:current mean train loss 6659.835178527022
INFO:root:current train perplexity186.5984649658203
INFO:root:current mean train loss 6645.275972058438
INFO:root:current train perplexity186.13299560546875
INFO:root:current mean train loss 6643.619677142006
INFO:root:current train perplexity185.95130920410156
INFO:root:current mean train loss 6647.341983676837
INFO:root:current train perplexity186.28463745117188
INFO:root:current mean train loss 6642.412933231781
INFO:root:current train perplexity186.1641387939453
INFO:root:current mean train loss 6638.16277470692
INFO:root:current train perplexity186.12295532226562
INFO:root:current mean train loss 6632.996966858847
INFO:root:current train perplexity185.76820373535156
INFO:root:current mean train loss 6631.9067827116705
INFO:root:current train perplexity185.79354858398438
INFO:root:current mean train loss 6626.822260646772
INFO:root:current train perplexity185.43898010253906
INFO:root:current mean train loss 6627.26302147061
INFO:root:current train perplexity185.495849609375
INFO:root:current mean train loss 6629.744788120688
INFO:root:current train perplexity185.71365356445312
INFO:root:current mean train loss 6629.920159899712
INFO:root:current train perplexity185.8103790283203
INFO:root:current mean train loss 6627.586227000931
INFO:root:current train perplexity185.95742797851562
INFO:root:current mean train loss 6631.813606712601
INFO:root:current train perplexity186.24476623535156
INFO:root:current mean train loss 6630.41499042672
INFO:root:current train perplexity186.39002990722656
INFO:root:current mean train loss 6630.687685739378
INFO:root:current train perplexity186.5009765625
INFO:root:current mean train loss 6631.994212990406
INFO:root:current train perplexity186.59295654296875

100%|██████████| 1/1 [17:18<00:00, 1038.91s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.91s/it]
INFO:root:final mean train loss: 6630.173790820608
INFO:root:final train perplexity: 186.5974884033203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.22s/it][A100%|██████████| 1/1 [01:14<00:00, 74.22s/it]
INFO:root:eval mean loss: 6477.451062790891
INFO:root:eval perplexity: 188.4027862548828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.66s/it][A100%|██████████| 1/1 [01:12<00:00, 72.66s/it]
INFO:root:eval mean loss: 6562.343123199246
INFO:root:eval perplexity: 214.1902618408203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/35
 18%|█▊        | 35/200 [11:54:14<54:32:56, 1190.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6596.894141663896
INFO:root:current train perplexity185.8362579345703
INFO:root:current mean train loss 6607.016075527545
INFO:root:current train perplexity186.11532592773438
INFO:root:current mean train loss 6605.7179577885845
INFO:root:current train perplexity185.37904357910156
INFO:root:current mean train loss 6632.289740393005
INFO:root:current train perplexity185.86717224121094
INFO:root:current mean train loss 6637.902418870192
INFO:root:current train perplexity186.0679473876953
INFO:root:current mean train loss 6636.340542468961
INFO:root:current train perplexity185.68380737304688
INFO:root:current mean train loss 6636.378748649136
INFO:root:current train perplexity185.70860290527344
INFO:root:current mean train loss 6638.068902388028
INFO:root:current train perplexity185.60183715820312
INFO:root:current mean train loss 6636.1254418562985
INFO:root:current train perplexity185.7758331298828
INFO:root:current mean train loss 6634.542204398264
INFO:root:current train perplexity185.8867645263672
INFO:root:current mean train loss 6634.797724805759
INFO:root:current train perplexity185.9119415283203
INFO:root:current mean train loss 6633.73945410647
INFO:root:current train perplexity185.6316680908203
INFO:root:current mean train loss 6629.0576500163015
INFO:root:current train perplexity185.53060913085938
INFO:root:current mean train loss 6625.752736336532
INFO:root:current train perplexity185.28147888183594
INFO:root:current mean train loss 6624.102027576451
INFO:root:current train perplexity185.18370056152344
INFO:root:current mean train loss 6622.966141646899
INFO:root:current train perplexity185.015380859375
INFO:root:current mean train loss 6623.215614508006
INFO:root:current train perplexity185.00840759277344
INFO:root:current mean train loss 6622.47119140625
INFO:root:current train perplexity184.9497528076172
INFO:root:current mean train loss 6619.440282481273
INFO:root:current train perplexity184.73866271972656

100%|██████████| 1/1 [17:15<00:00, 1035.87s/it][A100%|██████████| 1/1 [17:15<00:00, 1035.87s/it]
INFO:root:final mean train loss: 6617.214951846555
INFO:root:final train perplexity: 184.70016479492188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.44s/it][A100%|██████████| 1/1 [01:14<00:00, 74.45s/it]
INFO:root:eval mean loss: 6413.12825347684
INFO:root:eval perplexity: 178.8526153564453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.55s/it][A100%|██████████| 1/1 [01:11<00:00, 71.56s/it]
INFO:root:eval mean loss: 6498.458354111259
INFO:root:eval perplexity: 203.28677368164062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/36
 18%|█▊        | 36/200 [12:13:58<54:08:12, 1188.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6458.047052556818
INFO:root:current train perplexity180.46409606933594
INFO:root:current mean train loss 6543.465560775619
INFO:root:current train perplexity179.21551513671875
INFO:root:current mean train loss 6575.8946793542655
INFO:root:current train perplexity181.0342254638672
INFO:root:current mean train loss 6574.683224791499
INFO:root:current train perplexity181.81185913085938
INFO:root:current mean train loss 6586.594404605763
INFO:root:current train perplexity182.39834594726562
INFO:root:current mean train loss 6605.35603538558
INFO:root:current train perplexity183.4130096435547
INFO:root:current mean train loss 6608.530458041377
INFO:root:current train perplexity183.68109130859375
INFO:root:current mean train loss 6606.3174171501405
INFO:root:current train perplexity183.80813598632812
INFO:root:current mean train loss 6610.766081371378
INFO:root:current train perplexity183.9181671142578
INFO:root:current mean train loss 6606.355071050014
INFO:root:current train perplexity183.76229858398438
INFO:root:current mean train loss 6613.826989057863
INFO:root:current train perplexity184.30628967285156
INFO:root:current mean train loss 6621.2803780378035
INFO:root:current train perplexity184.77601623535156
INFO:root:current mean train loss 6624.156668526785
INFO:root:current train perplexity184.82659912109375
INFO:root:current mean train loss 6624.777589939097
INFO:root:current train perplexity185.0132598876953
INFO:root:current mean train loss 6624.983612298459
INFO:root:current train perplexity185.13011169433594
INFO:root:current mean train loss 6623.645458693539
INFO:root:current train perplexity185.1973876953125
INFO:root:current mean train loss 6620.954551654155
INFO:root:current train perplexity185.15623474121094
INFO:root:current mean train loss 6619.975858758584
INFO:root:current train perplexity185.07131958007812
INFO:root:current mean train loss 6620.988213575459
INFO:root:current train perplexity185.0677947998047
INFO:root:current mean train loss 6619.674753840839
INFO:root:current train perplexity185.02658081054688

100%|██████████| 1/1 [17:17<00:00, 1037.29s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.29s/it]
INFO:root:final mean train loss: 6619.0680246204065
INFO:root:final train perplexity: 184.97030639648438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.61s/it][A100%|██████████| 1/1 [01:14<00:00, 74.61s/it]
INFO:root:eval mean loss: 6382.339142495013
INFO:root:eval perplexity: 174.45391845703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.06s/it][A100%|██████████| 1/1 [01:12<00:00, 72.06s/it]
INFO:root:eval mean loss: 6481.978913868573
INFO:root:eval perplexity: 200.56529235839844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/37
 18%|█▊        | 37/200 [12:33:44<53:46:41, 1187.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6426.673304966518
INFO:root:current train perplexity177.9686279296875
INFO:root:current mean train loss 6631.508361816406
INFO:root:current train perplexity184.7721405029297
INFO:root:current mean train loss 6624.907050952576
INFO:root:current train perplexity184.37405395507812
INFO:root:current mean train loss 6608.128903272675
INFO:root:current train perplexity184.30435180664062
INFO:root:current mean train loss 6597.679355514384
INFO:root:current train perplexity183.40487670898438
INFO:root:current mean train loss 6599.851380319306
INFO:root:current train perplexity183.84275817871094
INFO:root:current mean train loss 6608.12908896671
INFO:root:current train perplexity183.9397430419922
INFO:root:current mean train loss 6606.918536846454
INFO:root:current train perplexity183.7271728515625
INFO:root:current mean train loss 6600.0261495838995
INFO:root:current train perplexity183.70675659179688
INFO:root:current mean train loss 6603.706406165814
INFO:root:current train perplexity183.97000122070312
INFO:root:current mean train loss 6609.33134062652
INFO:root:current train perplexity184.3390655517578
INFO:root:current mean train loss 6608.412412386414
INFO:root:current train perplexity184.2167510986328
INFO:root:current mean train loss 6611.630314233637
INFO:root:current train perplexity184.44334411621094
INFO:root:current mean train loss 6615.238600030003
INFO:root:current train perplexity184.66387939453125
INFO:root:current mean train loss 6620.465856557466
INFO:root:current train perplexity184.75839233398438
INFO:root:current mean train loss 6617.441429258017
INFO:root:current train perplexity184.3620147705078
INFO:root:current mean train loss 6613.373593642026
INFO:root:current train perplexity184.25057983398438
INFO:root:current mean train loss 6610.125290764703
INFO:root:current train perplexity183.662353515625
INFO:root:current mean train loss 6610.497933619393
INFO:root:current train perplexity183.69711303710938
INFO:root:current mean train loss 6618.211115033795
INFO:root:current train perplexity184.17459106445312

100%|██████████| 1/1 [17:22<00:00, 1042.14s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.14s/it]
INFO:root:final mean train loss: 6613.736304486573
INFO:root:final train perplexity: 184.194091796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.59s/it][A100%|██████████| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 6469.131616037788
INFO:root:eval perplexity: 187.13943481445312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.05s/it][A100%|██████████| 1/1 [01:12<00:00, 72.05s/it]
INFO:root:eval mean loss: 6556.681426785516
INFO:root:eval perplexity: 213.2007293701172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/38
 19%|█▉        | 38/200 [12:53:35<53:29:35, 1188.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6539.511284722223
INFO:root:current train perplexity185.62254333496094
INFO:root:current mean train loss 6628.303013873922
INFO:root:current train perplexity186.97085571289062
INFO:root:current mean train loss 6617.03802216199
INFO:root:current train perplexity187.21868896484375
INFO:root:current mean train loss 6615.180010190217
INFO:root:current train perplexity186.44886779785156
INFO:root:current mean train loss 6616.8743482268255
INFO:root:current train perplexity186.4420166015625
INFO:root:current mean train loss 6628.6547994911125
INFO:root:current train perplexity187.0687713623047
INFO:root:current mean train loss 6626.010437106347
INFO:root:current train perplexity187.1635284423828
INFO:root:current mean train loss 6634.0042359217705
INFO:root:current train perplexity187.58372497558594
INFO:root:current mean train loss 6635.483013590976
INFO:root:current train perplexity187.63275146484375
INFO:root:current mean train loss 6636.033786479002
INFO:root:current train perplexity187.85520935058594
INFO:root:current mean train loss 6637.734360515102
INFO:root:current train perplexity187.85601806640625
INFO:root:current mean train loss 6640.615015607942
INFO:root:current train perplexity187.84983825683594
INFO:root:current mean train loss 6641.0768931193525
INFO:root:current train perplexity188.03526306152344
INFO:root:current mean train loss 6639.287299605018
INFO:root:current train perplexity187.92213439941406
INFO:root:current mean train loss 6641.261968128244
INFO:root:current train perplexity188.06297302246094
INFO:root:current mean train loss 6642.96769316343
INFO:root:current train perplexity188.11203002929688
INFO:root:current mean train loss 6641.8154065349545
INFO:root:current train perplexity188.16127014160156
INFO:root:current mean train loss 6644.791360919592
INFO:root:current train perplexity188.35374450683594
INFO:root:current mean train loss 6644.025421589176
INFO:root:current train perplexity188.51646423339844
INFO:root:current mean train loss 6648.141583738352
INFO:root:current train perplexity188.89158630371094

100%|██████████| 1/1 [17:22<00:00, 1042.39s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.39s/it]
INFO:root:final mean train loss: 6646.173307956497
INFO:root:final train perplexity: 188.96690368652344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.61s/it][A100%|██████████| 1/1 [01:14<00:00, 74.61s/it]
INFO:root:eval mean loss: 6493.111548024712
INFO:root:eval perplexity: 190.8041229248047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.55s/it][A100%|██████████| 1/1 [01:12<00:00, 72.55s/it]
INFO:root:eval mean loss: 6575.004415309176
INFO:root:eval perplexity: 216.4195098876953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/39
 20%|█▉        | 39/200 [13:13:27<53:12:27, 1189.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6687.503788117439
INFO:root:current train perplexity193.81190490722656
INFO:root:current mean train loss 6696.577055000965
INFO:root:current train perplexity192.7340087890625
INFO:root:current mean train loss 6669.750106229127
INFO:root:current train perplexity191.75955200195312
INFO:root:current mean train loss 6657.067261416609
INFO:root:current train perplexity188.18434143066406
INFO:root:current mean train loss 6641.7948098451025
INFO:root:current train perplexity186.60986328125
INFO:root:current mean train loss 6634.492056306995
INFO:root:current train perplexity186.73037719726562
INFO:root:current mean train loss 6643.935465003068
INFO:root:current train perplexity187.4884033203125
INFO:root:current mean train loss 6643.333515317421
INFO:root:current train perplexity187.76995849609375
INFO:root:current mean train loss 6640.924827345562
INFO:root:current train perplexity187.91604614257812
INFO:root:current mean train loss 6647.2205214153455
INFO:root:current train perplexity188.2789306640625
INFO:root:current mean train loss 6649.68986876177
INFO:root:current train perplexity188.51124572753906
INFO:root:current mean train loss 6646.971746920719
INFO:root:current train perplexity188.2889862060547
INFO:root:current mean train loss 6642.4092737067895
INFO:root:current train perplexity187.50823974609375
INFO:root:current mean train loss 6644.32571585903
INFO:root:current train perplexity186.93438720703125
INFO:root:current mean train loss 6641.623221547431
INFO:root:current train perplexity186.6569366455078
INFO:root:current mean train loss 6638.019733189621
INFO:root:current train perplexity186.00193786621094
INFO:root:current mean train loss 6632.569773686635
INFO:root:current train perplexity185.351806640625
INFO:root:current mean train loss 6625.011193612107
INFO:root:current train perplexity184.772216796875
INFO:root:current mean train loss 6618.815816221637
INFO:root:current train perplexity184.22000122070312
INFO:root:current mean train loss 6610.9117328359935
INFO:root:current train perplexity183.58477783203125

100%|██████████| 1/1 [17:19<00:00, 1039.25s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.25s/it]
INFO:root:final mean train loss: 6608.969771869484
INFO:root:final train perplexity: 183.50311279296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.30s/it][A100%|██████████| 1/1 [01:14<00:00, 74.30s/it]
INFO:root:eval mean loss: 6326.661692084996
INFO:root:eval perplexity: 166.77281188964844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.30s/it][A100%|██████████| 1/1 [01:12<00:00, 72.30s/it]
INFO:root:eval mean loss: 6435.071025875443
INFO:root:eval perplexity: 193.01699829101562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/40
 20%|██        | 40/200 [13:33:15<52:51:22, 1189.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6572.8176362242875
INFO:root:current train perplexity175.63172912597656
INFO:root:current mean train loss 6577.33480817912
INFO:root:current train perplexity174.58749389648438
INFO:root:current mean train loss 6567.139118153562
INFO:root:current train perplexity174.79290771484375
INFO:root:current mean train loss 6556.441138275066
INFO:root:current train perplexity175.11618041992188
INFO:root:current mean train loss 6545.571223822417
INFO:root:current train perplexity175.5517578125
INFO:root:current mean train loss 6571.885129095153
INFO:root:current train perplexity177.2481689453125
INFO:root:current mean train loss 6566.851968801777
INFO:root:current train perplexity177.1609649658203
INFO:root:current mean train loss 6566.811313457758
INFO:root:current train perplexity176.6479949951172
INFO:root:current mean train loss 6562.437888847412
INFO:root:current train perplexity175.98524475097656
INFO:root:current mean train loss 6558.630541667997
INFO:root:current train perplexity175.48020935058594
INFO:root:current mean train loss 6548.108481703255
INFO:root:current train perplexity174.7554931640625
INFO:root:current mean train loss 6544.851172786127
INFO:root:current train perplexity174.5045928955078
INFO:root:current mean train loss 6543.524102158058
INFO:root:current train perplexity174.4126739501953
INFO:root:current mean train loss 6545.700748745127
INFO:root:current train perplexity174.20753479003906
INFO:root:current mean train loss 6543.019121212601
INFO:root:current train perplexity173.9829864501953
INFO:root:current mean train loss 6543.275001917254
INFO:root:current train perplexity173.94515991210938
INFO:root:current mean train loss 6540.664645587496
INFO:root:current train perplexity173.64706420898438
INFO:root:current mean train loss 6542.298242407076
INFO:root:current train perplexity173.59176635742188
INFO:root:current mean train loss 6542.31849762174
INFO:root:current train perplexity173.4513397216797
INFO:root:current mean train loss 6536.948082058884
INFO:root:current train perplexity173.1818389892578

100%|██████████| 1/1 [17:19<00:00, 1039.64s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.64s/it]
INFO:root:final mean train loss: 6535.604149922781
INFO:root:final train perplexity: 173.18679809570312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.55s/it][A100%|██████████| 1/1 [01:14<00:00, 74.55s/it]
INFO:root:eval mean loss: 6308.694240705341
INFO:root:eval perplexity: 164.36692810058594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.23s/it][A100%|██████████| 1/1 [01:12<00:00, 72.24s/it]
INFO:root:eval mean loss: 6419.222618157137
INFO:root:eval perplexity: 190.53128051757812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/41
 20%|██        | 41/200 [13:53:04<52:31:06, 1189.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6484.263605753581
INFO:root:current train perplexity170.7076873779297
INFO:root:current mean train loss 6500.547014508928
INFO:root:current train perplexity171.48191833496094
INFO:root:current mean train loss 6516.652053420608
INFO:root:current train perplexity171.3839874267578
INFO:root:current mean train loss 6519.8406612511835
INFO:root:current train perplexity171.18624877929688
INFO:root:current mean train loss 6524.413752402029
INFO:root:current train perplexity171.42051696777344
INFO:root:current mean train loss 6533.028903628356
INFO:root:current train perplexity171.67083740234375
INFO:root:current mean train loss 6538.958566950656
INFO:root:current train perplexity171.7427215576172
INFO:root:current mean train loss 6535.657298332483
INFO:root:current train perplexity171.74549865722656
INFO:root:current mean train loss 6529.572652544294
INFO:root:current train perplexity171.65052795410156
INFO:root:current mean train loss 6523.128814574705
INFO:root:current train perplexity171.39723205566406
INFO:root:current mean train loss 6518.057741930885
INFO:root:current train perplexity171.2045440673828
INFO:root:current mean train loss 6518.580671329562
INFO:root:current train perplexity171.10391235351562
INFO:root:current mean train loss 6518.413801028405
INFO:root:current train perplexity171.02552795410156
INFO:root:current mean train loss 6515.286877476384
INFO:root:current train perplexity170.9293975830078
INFO:root:current mean train loss 6521.8448545078545
INFO:root:current train perplexity171.13287353515625
INFO:root:current mean train loss 6522.039133172286
INFO:root:current train perplexity171.1064453125
INFO:root:current mean train loss 6518.151285999226
INFO:root:current train perplexity171.0205535888672
INFO:root:current mean train loss 6516.902908155275
INFO:root:current train perplexity170.86672973632812
INFO:root:current mean train loss 6519.472174149525
INFO:root:current train perplexity170.7768096923828

100%|██████████| 1/1 [17:28<00:00, 1048.77s/it][A100%|██████████| 1/1 [17:28<00:00, 1048.77s/it]
INFO:root:final mean train loss: 6517.722463695316
INFO:root:final train perplexity: 170.76158142089844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.24s/it][A100%|██████████| 1/1 [01:17<00:00, 77.24s/it]
INFO:root:eval mean loss: 6308.007828083444
INFO:root:eval perplexity: 164.27572631835938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.58s/it][A100%|██████████| 1/1 [01:12<00:00, 72.58s/it]
INFO:root:eval mean loss: 6437.647544914949
INFO:root:eval perplexity: 193.42405700683594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/42
 21%|██        | 42/200 [14:13:05<52:20:38, 1192.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6713.8537034254805
INFO:root:current train perplexity175.7550506591797
INFO:root:current mean train loss 6551.0573665652655
INFO:root:current train perplexity170.0673828125
INFO:root:current mean train loss 6532.581304559126
INFO:root:current train perplexity169.5772247314453
INFO:root:current mean train loss 6535.268086686302
INFO:root:current train perplexity169.38633728027344
INFO:root:current mean train loss 6541.669901776256
INFO:root:current train perplexity169.71231079101562
INFO:root:current mean train loss 6533.368042230141
INFO:root:current train perplexity169.64895629882812
INFO:root:current mean train loss 6519.969249432861
INFO:root:current train perplexity169.12936401367188
INFO:root:current mean train loss 6516.676505111544
INFO:root:current train perplexity169.00576782226562
INFO:root:current mean train loss 6517.425963229359
INFO:root:current train perplexity169.2705535888672
INFO:root:current mean train loss 6508.528503217415
INFO:root:current train perplexity169.189697265625
INFO:root:current mean train loss 6509.698311115652
INFO:root:current train perplexity169.25779724121094
INFO:root:current mean train loss 6512.010272770665
INFO:root:current train perplexity169.52984619140625
INFO:root:current mean train loss 6507.938501519991
INFO:root:current train perplexity169.575439453125
INFO:root:current mean train loss 6513.70669246537
INFO:root:current train perplexity169.9104461669922
INFO:root:current mean train loss 6518.017396704154
INFO:root:current train perplexity170.1245574951172
INFO:root:current mean train loss 6520.141946554341
INFO:root:current train perplexity170.42294311523438
INFO:root:current mean train loss 6516.594151401697
INFO:root:current train perplexity170.48837280273438
INFO:root:current mean train loss 6518.450300493925
INFO:root:current train perplexity170.67138671875
INFO:root:current mean train loss 6522.232987182415
INFO:root:current train perplexity170.8777313232422
INFO:root:current mean train loss 6521.40589112732
INFO:root:current train perplexity170.98768615722656

100%|██████████| 1/1 [17:24<00:00, 1044.43s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.43s/it]
INFO:root:final mean train loss: 6520.017805152389
INFO:root:final train perplexity: 171.07095336914062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.45s/it][A100%|██████████| 1/1 [01:14<00:00, 74.45s/it]
INFO:root:eval mean loss: 6360.734122201906
INFO:root:eval perplexity: 171.4322967529297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.44s/it][A100%|██████████| 1/1 [01:12<00:00, 72.44s/it]
INFO:root:eval mean loss: 6477.644384938774
INFO:root:eval perplexity: 199.85568237304688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/43
 22%|██▏       | 43/200 [14:32:59<52:01:37, 1192.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6605.455452473959
INFO:root:current train perplexity174.23020935058594
INFO:root:current mean train loss 6601.472900390625
INFO:root:current train perplexity174.93524169921875
INFO:root:current mean train loss 6589.701278023098
INFO:root:current train perplexity173.3865966796875
INFO:root:current mean train loss 6571.525778290719
INFO:root:current train perplexity172.77586364746094
INFO:root:current mean train loss 6552.337653297602
INFO:root:current train perplexity171.97036743164062
INFO:root:current mean train loss 6555.907186025944
INFO:root:current train perplexity171.7981719970703
INFO:root:current mean train loss 6555.029036458333
INFO:root:current train perplexity171.7416534423828
INFO:root:current mean train loss 6555.12088572881
INFO:root:current train perplexity171.74673461914062
INFO:root:current mean train loss 6544.610781602974
INFO:root:current train perplexity171.62802124023438
INFO:root:current mean train loss 6538.852278645833
INFO:root:current train perplexity171.5810546875
INFO:root:current mean train loss 6530.379685129703
INFO:root:current train perplexity171.4156951904297
INFO:root:current mean train loss 6529.720773990597
INFO:root:current train perplexity171.47251892089844
INFO:root:current mean train loss 6524.722079442962
INFO:root:current train perplexity171.24209594726562
INFO:root:current mean train loss 6522.178658805216
INFO:root:current train perplexity171.15042114257812
INFO:root:current mean train loss 6526.122853269777
INFO:root:current train perplexity171.24879455566406
INFO:root:current mean train loss 6523.810296351614
INFO:root:current train perplexity171.14512634277344
INFO:root:current mean train loss 6521.977039997124
INFO:root:current train perplexity171.0009002685547
INFO:root:current mean train loss 6520.548385002709
INFO:root:current train perplexity170.92034912109375
INFO:root:current mean train loss 6522.374520257002
INFO:root:current train perplexity171.0201416015625
INFO:root:current mean train loss 6520.502173990042
INFO:root:current train perplexity170.93600463867188

100%|██████████| 1/1 [17:22<00:00, 1042.75s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.75s/it]
INFO:root:final mean train loss: 6519.546803592253
INFO:root:final train perplexity: 171.00741577148438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.76s/it][A100%|██████████| 1/1 [01:15<00:00, 75.76s/it]
INFO:root:eval mean loss: 6321.845680615581
INFO:root:eval perplexity: 166.12445068359375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.03s/it][A100%|██████████| 1/1 [01:15<00:00, 75.03s/it]
INFO:root:eval mean loss: 6436.922533833389
INFO:root:eval perplexity: 193.3094482421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/44
 22%|██▏       | 44/200 [14:52:55<51:44:06, 1193.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6620.710251828457
INFO:root:current train perplexity172.02081298828125
INFO:root:current mean train loss 6565.648241523172
INFO:root:current train perplexity172.58236694335938
INFO:root:current mean train loss 6522.131252767586
INFO:root:current train perplexity172.3917999267578
INFO:root:current mean train loss 6520.405286101855
INFO:root:current train perplexity171.73214721679688
INFO:root:current mean train loss 6521.046312438828
INFO:root:current train perplexity171.39486694335938
INFO:root:current mean train loss 6525.238541012054
INFO:root:current train perplexity171.1212921142578
INFO:root:current mean train loss 6519.4605194044625
INFO:root:current train perplexity170.89044189453125
INFO:root:current mean train loss 6520.637544317897
INFO:root:current train perplexity171.15638732910156
INFO:root:current mean train loss 6517.637494696355
INFO:root:current train perplexity171.23434448242188
INFO:root:current mean train loss 6520.086462905062
INFO:root:current train perplexity171.3543243408203
INFO:root:current mean train loss 6523.054972913682
INFO:root:current train perplexity171.5011444091797
INFO:root:current mean train loss 6521.674891530895
INFO:root:current train perplexity171.41725158691406
INFO:root:current mean train loss 6519.175890105002
INFO:root:current train perplexity171.27835083007812
INFO:root:current mean train loss 6518.449086439194
INFO:root:current train perplexity171.430908203125
INFO:root:current mean train loss 6520.265069904868
INFO:root:current train perplexity171.484375
INFO:root:current mean train loss 6523.21659139918
INFO:root:current train perplexity171.51095581054688
INFO:root:current mean train loss 6528.105952287777
INFO:root:current train perplexity171.54031372070312
INFO:root:current mean train loss 6530.371143220968
INFO:root:current train perplexity171.58612060546875
INFO:root:current mean train loss 6531.663171327236
INFO:root:current train perplexity171.66502380371094
INFO:root:current mean train loss 6525.5952629947515
INFO:root:current train perplexity171.54522705078125

100%|██████████| 1/1 [17:15<00:00, 1035.34s/it][A100%|██████████| 1/1 [17:15<00:00, 1035.34s/it]
INFO:root:final mean train loss: 6523.40879199268
INFO:root:final train perplexity: 171.52902221679688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 74.00s/it][A100%|██████████| 1/1 [01:13<00:00, 74.00s/it]
INFO:root:eval mean loss: 6328.8945866578015
INFO:root:eval perplexity: 167.07423400878906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.68s/it][A100%|██████████| 1/1 [01:11<00:00, 71.68s/it]
INFO:root:eval mean loss: 6443.839594414893
INFO:root:eval perplexity: 194.40603637695312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/45
 22%|██▎       | 45/200 [15:12:38<51:16:04, 1190.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6491.26286315918
INFO:root:current train perplexity172.04812622070312
INFO:root:current mean train loss 6518.944839105374
INFO:root:current train perplexity171.5813751220703
INFO:root:current mean train loss 6530.6241159150095
INFO:root:current train perplexity172.1658935546875
INFO:root:current mean train loss 6525.7926347334305
INFO:root:current train perplexity171.83323669433594
INFO:root:current mean train loss 6533.191544105267
INFO:root:current train perplexity172.03163146972656
INFO:root:current mean train loss 6535.940072134032
INFO:root:current train perplexity172.10269165039062
INFO:root:current mean train loss 6538.819010171545
INFO:root:current train perplexity172.10679626464844
INFO:root:current mean train loss 6538.509395579393
INFO:root:current train perplexity172.1980743408203
INFO:root:current mean train loss 6534.182187115704
INFO:root:current train perplexity172.3626251220703
INFO:root:current mean train loss 6532.407028514815
INFO:root:current train perplexity172.35826110839844
INFO:root:current mean train loss 6533.524409473391
INFO:root:current train perplexity172.58203125
INFO:root:current mean train loss 6533.916389386679
INFO:root:current train perplexity172.5446014404297
INFO:root:current mean train loss 6538.240942460072
INFO:root:current train perplexity172.67884826660156
INFO:root:current mean train loss 6541.236536467879
INFO:root:current train perplexity172.78468322753906
INFO:root:current mean train loss 6540.561153224257
INFO:root:current train perplexity172.8253936767578
INFO:root:current mean train loss 6537.613946236613
INFO:root:current train perplexity172.84121704101562
INFO:root:current mean train loss 6532.217531350942
INFO:root:current train perplexity172.71104431152344
INFO:root:current mean train loss 6531.09877647348
INFO:root:current train perplexity172.7669677734375
INFO:root:current mean train loss 6535.877103748239
INFO:root:current train perplexity173.04522705078125
INFO:root:current mean train loss 6536.461089155582
INFO:root:current train perplexity173.02517700195312

100%|██████████| 1/1 [17:15<00:00, 1035.70s/it][A100%|██████████| 1/1 [17:15<00:00, 1035.70s/it]
INFO:root:final mean train loss: 6534.462327488733
INFO:root:final train perplexity: 173.03079223632812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.11s/it][A100%|██████████| 1/1 [01:14<00:00, 74.11s/it]
INFO:root:eval mean loss: 6356.896349318484
INFO:root:eval perplexity: 170.9010467529297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.50s/it][A100%|██████████| 1/1 [01:12<00:00, 72.50s/it]
INFO:root:eval mean loss: 6470.448430054576
INFO:root:eval perplexity: 198.6830596923828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/46
 23%|██▎       | 46/200 [15:32:23<50:51:31, 1188.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6537.319378134645
INFO:root:current train perplexity173.6724853515625
INFO:root:current mean train loss 6531.900560579247
INFO:root:current train perplexity172.54278564453125
INFO:root:current mean train loss 6516.4010578847865
INFO:root:current train perplexity172.4867706298828
INFO:root:current mean train loss 6524.16817636565
INFO:root:current train perplexity172.9435272216797
INFO:root:current mean train loss 6539.196862615319
INFO:root:current train perplexity173.087890625
INFO:root:current mean train loss 6536.8832192609725
INFO:root:current train perplexity173.1923370361328
INFO:root:current mean train loss 6541.59235183783
INFO:root:current train perplexity173.52597045898438
INFO:root:current mean train loss 6543.791551421455
INFO:root:current train perplexity173.77395629882812
INFO:root:current mean train loss 6539.444930077681
INFO:root:current train perplexity173.57867431640625
INFO:root:current mean train loss 6534.038813133123
INFO:root:current train perplexity173.44464111328125
INFO:root:current mean train loss 6532.554348729475
INFO:root:current train perplexity173.42710876464844
INFO:root:current mean train loss 6540.68921497936
INFO:root:current train perplexity173.54202270507812
INFO:root:current mean train loss 6544.16463540142
INFO:root:current train perplexity173.65550231933594
INFO:root:current mean train loss 6544.105661092505
INFO:root:current train perplexity173.63604736328125
INFO:root:current mean train loss 6543.822145285597
INFO:root:current train perplexity173.5679168701172
INFO:root:current mean train loss 6544.37004121205
INFO:root:current train perplexity173.60260009765625
INFO:root:current mean train loss 6542.937685029837
INFO:root:current train perplexity173.64598083496094
INFO:root:current mean train loss 6541.743953372842
INFO:root:current train perplexity173.5100860595703
INFO:root:current mean train loss 6543.419217358619
INFO:root:current train perplexity173.48162841796875
INFO:root:current mean train loss 6538.956127153268
INFO:root:current train perplexity173.34799194335938

100%|██████████| 1/1 [17:17<00:00, 1037.41s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.41s/it]
INFO:root:final mean train loss: 6536.840936657881
INFO:root:final train perplexity: 173.35585021972656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.37s/it][A100%|██████████| 1/1 [01:14<00:00, 74.40s/it]
INFO:root:eval mean loss: 6337.8878909712985
INFO:root:eval perplexity: 168.2938232421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.88s/it][A100%|██████████| 1/1 [01:13<00:00, 73.88s/it]
INFO:root:eval mean loss: 6451.22620234929
INFO:root:eval perplexity: 195.58401489257812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/47
 24%|██▎       | 47/200 [15:52:11<50:31:03, 1188.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6554.745152064732
INFO:root:current train perplexity173.33229064941406
INFO:root:current mean train loss 6543.985534051452
INFO:root:current train perplexity172.86651611328125
INFO:root:current mean train loss 6515.738119035759
INFO:root:current train perplexity172.3259735107422
INFO:root:current mean train loss 6515.288992570273
INFO:root:current train perplexity172.36212158203125
INFO:root:current mean train loss 6515.558206458647
INFO:root:current train perplexity172.8977508544922
INFO:root:current mean train loss 6520.246612242631
INFO:root:current train perplexity172.9442596435547
INFO:root:current mean train loss 6531.915628777534
INFO:root:current train perplexity173.15029907226562
INFO:root:current mean train loss 6532.591568643288
INFO:root:current train perplexity173.53167724609375
INFO:root:current mean train loss 6526.867102676086
INFO:root:current train perplexity173.31393432617188
INFO:root:current mean train loss 6530.852375160477
INFO:root:current train perplexity173.32534790039062
INFO:root:current mean train loss 6531.512305310081
INFO:root:current train perplexity173.3915557861328
INFO:root:current mean train loss 6535.943301498592
INFO:root:current train perplexity173.4382781982422
INFO:root:current mean train loss 6538.245116058961
INFO:root:current train perplexity173.39263916015625
INFO:root:current mean train loss 6537.897562575443
INFO:root:current train perplexity173.46780395507812
INFO:root:current mean train loss 6539.322090586928
INFO:root:current train perplexity173.34542846679688
INFO:root:current mean train loss 6538.2988217082875
INFO:root:current train perplexity173.29815673828125
INFO:root:current mean train loss 6541.872279945616
INFO:root:current train perplexity173.60682678222656
INFO:root:current mean train loss 6544.10870802628
INFO:root:current train perplexity174.03060913085938
INFO:root:current mean train loss 6545.876264952006
INFO:root:current train perplexity174.0263671875

100%|██████████| 1/1 [17:20<00:00, 1040.73s/it][A100%|██████████| 1/1 [17:20<00:00, 1040.73s/it]
INFO:root:final mean train loss: 6542.031158770447
INFO:root:final train perplexity: 174.06687927246094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.34s/it][A100%|██████████| 1/1 [01:15<00:00, 75.34s/it]
INFO:root:eval mean loss: 6360.92581761137
INFO:root:eval perplexity: 171.4587860107422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.09s/it][A100%|██████████| 1/1 [01:13<00:00, 73.09s/it]
INFO:root:eval mean loss: 6471.565109361148
INFO:root:eval perplexity: 198.86456298828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/48
 24%|██▍       | 48/200 [16:12:02<50:13:21, 1189.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6477.036197916666
INFO:root:current train perplexity179.3073272705078
INFO:root:current mean train loss 6533.347927989131
INFO:root:current train perplexity176.93226623535156
INFO:root:current mean train loss 6537.192864280523
INFO:root:current train perplexity178.7023162841797
INFO:root:current mean train loss 6552.422614397321
INFO:root:current train perplexity178.1645050048828
INFO:root:current mean train loss 6549.861942300452
INFO:root:current train perplexity177.534912109375
INFO:root:current mean train loss 6563.571458775788
INFO:root:current train perplexity177.4247283935547
INFO:root:current mean train loss 6568.23284425813
INFO:root:current train perplexity177.5414276123047
INFO:root:current mean train loss 6568.006769695148
INFO:root:current train perplexity177.8065185546875
INFO:root:current mean train loss 6579.4738299223545
INFO:root:current train perplexity178.52618408203125
INFO:root:current mean train loss 6585.16767578125
INFO:root:current train perplexity178.84893798828125
INFO:root:current mean train loss 6584.669222887162
INFO:root:current train perplexity179.10821533203125
INFO:root:current mean train loss 6585.599936939462
INFO:root:current train perplexity179.41944885253906
INFO:root:current mean train loss 6582.654434719007
INFO:root:current train perplexity179.3296356201172
INFO:root:current mean train loss 6584.846031368821
INFO:root:current train perplexity179.40704345703125
INFO:root:current mean train loss 6585.086184573764
INFO:root:current train perplexity179.5712890625
INFO:root:current mean train loss 6585.407955922546
INFO:root:current train perplexity179.56854248046875
INFO:root:current mean train loss 6584.621258828367
INFO:root:current train perplexity179.6555633544922
INFO:root:current mean train loss 6588.321256605321
INFO:root:current train perplexity179.82183837890625
INFO:root:current mean train loss 6587.4477186639115
INFO:root:current train perplexity179.8471221923828
INFO:root:current mean train loss 6588.21695011627
INFO:root:current train perplexity180.03932189941406

100%|██████████| 1/1 [17:23<00:00, 1043.20s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.20s/it]
INFO:root:final mean train loss: 6585.48241301059
INFO:root:final train perplexity: 180.13525390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.69s/it][A100%|██████████| 1/1 [01:17<00:00, 77.69s/it]
INFO:root:eval mean loss: 6424.028552332668
INFO:root:eval perplexity: 180.43612670898438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.71s/it][A100%|██████████| 1/1 [01:12<00:00, 72.71s/it]
INFO:root:eval mean loss: 6529.378142661237
INFO:root:eval perplexity: 208.4927520751953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/49
 24%|██▍       | 49/200 [16:31:58<49:58:22, 1191.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6676.241546630859
INFO:root:current train perplexity187.41358947753906
INFO:root:current mean train loss 6641.431285511364
INFO:root:current train perplexity183.2107391357422
INFO:root:current mean train loss 6581.3063480771825
INFO:root:current train perplexity181.0013427734375
INFO:root:current mean train loss 6593.694684499717
INFO:root:current train perplexity181.70066833496094
INFO:root:current mean train loss 6600.209955286096
INFO:root:current train perplexity182.44834899902344
INFO:root:current mean train loss 6607.032655185327
INFO:root:current train perplexity183.150390625
INFO:root:current mean train loss 6612.24338193483
INFO:root:current train perplexity183.5531768798828
INFO:root:current mean train loss 6610.430766121286
INFO:root:current train perplexity183.30799865722656
INFO:root:current mean train loss 6601.512610802283
INFO:root:current train perplexity182.44931030273438
INFO:root:current mean train loss 6589.360875993328
INFO:root:current train perplexity181.48410034179688
INFO:root:current mean train loss 6585.791643955911
INFO:root:current train perplexity180.73130798339844
INFO:root:current mean train loss 6585.057237173559
INFO:root:current train perplexity180.11634826660156
INFO:root:current mean train loss 6580.664974856686
INFO:root:current train perplexity179.46112060546875
INFO:root:current mean train loss 6582.424359295819
INFO:root:current train perplexity179.15194702148438
INFO:root:current mean train loss 6578.836165273656
INFO:root:current train perplexity178.84144592285156
INFO:root:current mean train loss 6574.621664898825
INFO:root:current train perplexity178.42686462402344
INFO:root:current mean train loss 6570.883881812002
INFO:root:current train perplexity178.09307861328125
INFO:root:current mean train loss 6569.20177094986
INFO:root:current train perplexity177.80287170410156
INFO:root:current mean train loss 6568.249493061715
INFO:root:current train perplexity177.61764526367188
INFO:root:current mean train loss 6567.779212967456
INFO:root:current train perplexity177.53982543945312

100%|██████████| 1/1 [17:19<00:00, 1039.19s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.19s/it]
INFO:root:final mean train loss: 6567.026879968994
INFO:root:final train perplexity: 177.53228759765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.52s/it][A100%|██████████| 1/1 [01:14<00:00, 74.52s/it]
INFO:root:eval mean loss: 6356.102985787899
INFO:root:eval perplexity: 170.79147338867188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.79s/it][A100%|██████████| 1/1 [01:13<00:00, 73.79s/it]
INFO:root:eval mean loss: 6464.6316030515845
INFO:root:eval perplexity: 197.739990234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/50
 25%|██▌       | 50/200 [16:51:48<49:37:16, 1190.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6494.511469626913
INFO:root:current train perplexity173.15847778320312
INFO:root:current mean train loss 6552.542247797819
INFO:root:current train perplexity176.05482482910156
INFO:root:current mean train loss 6552.131035862199
INFO:root:current train perplexity176.09898376464844
INFO:root:current mean train loss 6556.418887949946
INFO:root:current train perplexity176.44200134277344
INFO:root:current mean train loss 6569.911819016216
INFO:root:current train perplexity176.63966369628906
INFO:root:current mean train loss 6562.38028837944
INFO:root:current train perplexity176.30996704101562
INFO:root:current mean train loss 6557.5071361289
INFO:root:current train perplexity176.23280334472656
INFO:root:current mean train loss 6554.85564606976
INFO:root:current train perplexity176.2389373779297
INFO:root:current mean train loss 6555.326671658753
INFO:root:current train perplexity176.23011779785156
INFO:root:current mean train loss 6559.921306453339
INFO:root:current train perplexity176.11871337890625
INFO:root:current mean train loss 6559.637488176984
INFO:root:current train perplexity176.0702667236328
INFO:root:current mean train loss 6562.060459757806
INFO:root:current train perplexity176.07815551757812
INFO:root:current mean train loss 6560.467652637735
INFO:root:current train perplexity175.82394409179688
INFO:root:current mean train loss 6559.635866701028
INFO:root:current train perplexity175.721435546875
INFO:root:current mean train loss 6557.438297627135
INFO:root:current train perplexity175.60861206054688
INFO:root:current mean train loss 6552.846439487169
INFO:root:current train perplexity175.4705047607422
INFO:root:current mean train loss 6551.585997017605
INFO:root:current train perplexity175.32806396484375
INFO:root:current mean train loss 6552.525247965355
INFO:root:current train perplexity175.42071533203125
INFO:root:current mean train loss 6553.171332318568
INFO:root:current train perplexity175.38693237304688
INFO:root:current mean train loss 6555.199115281474
INFO:root:current train perplexity175.450927734375

100%|██████████| 1/1 [17:18<00:00, 1038.81s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.81s/it]
INFO:root:final mean train loss: 6551.722863332467
INFO:root:final train perplexity: 175.40240478515625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.75s/it][A100%|██████████| 1/1 [01:14<00:00, 74.75s/it]
INFO:root:eval mean loss: 6355.266544423205
INFO:root:eval perplexity: 170.6759490966797
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.37s/it][A100%|██████████| 1/1 [01:13<00:00, 73.37s/it]
INFO:root:eval mean loss: 6463.36592350953
INFO:root:eval perplexity: 197.53549194335938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/51
 26%|██▌       | 51/200 [17:11:37<49:16:11, 1190.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6486.360299775095
INFO:root:current train perplexity172.09193420410156
INFO:root:current mean train loss 6513.106398249247
INFO:root:current train perplexity173.41677856445312
INFO:root:current mean train loss 6504.509391153665
INFO:root:current train perplexity173.3336181640625
INFO:root:current mean train loss 6508.326889621755
INFO:root:current train perplexity174.1336212158203
INFO:root:current mean train loss 6524.911077278366
INFO:root:current train perplexity174.6429443359375
INFO:root:current mean train loss 6535.382641687831
INFO:root:current train perplexity174.88186645507812
INFO:root:current mean train loss 6546.17552317943
INFO:root:current train perplexity175.50363159179688
INFO:root:current mean train loss 6548.588273728174
INFO:root:current train perplexity175.47409057617188
INFO:root:current mean train loss 6550.506447455074
INFO:root:current train perplexity175.4329376220703
INFO:root:current mean train loss 6552.401892873318
INFO:root:current train perplexity175.45285034179688
INFO:root:current mean train loss 6553.571232264306
INFO:root:current train perplexity175.5921173095703
INFO:root:current mean train loss 6555.5931000750425
INFO:root:current train perplexity175.65081787109375
INFO:root:current mean train loss 6552.294175182662
INFO:root:current train perplexity175.64378356933594
INFO:root:current mean train loss 6554.421130424712
INFO:root:current train perplexity175.64730834960938
INFO:root:current mean train loss 6558.692287221286
INFO:root:current train perplexity175.91392517089844
INFO:root:current mean train loss 6560.759801793982
INFO:root:current train perplexity176.02099609375
INFO:root:current mean train loss 6561.664042277067
INFO:root:current train perplexity176.0103302001953
INFO:root:current mean train loss 6560.560261537373
INFO:root:current train perplexity175.9984130859375
INFO:root:current mean train loss 6557.242365437433
INFO:root:current train perplexity175.8251190185547
INFO:root:current mean train loss 6556.398484440568
INFO:root:current train perplexity175.81271362304688

100%|██████████| 1/1 [17:21<00:00, 1041.86s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.86s/it]
INFO:root:final mean train loss: 6554.535204634904
INFO:root:final train perplexity: 175.79183959960938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.18s/it][A100%|██████████| 1/1 [01:14<00:00, 74.18s/it]
INFO:root:eval mean loss: 6367.505023063497
INFO:root:eval perplexity: 172.37362670898438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.18s/it][A100%|██████████| 1/1 [01:13<00:00, 73.18s/it]
INFO:root:eval mean loss: 6478.334405993739
INFO:root:eval perplexity: 199.96844482421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/52
 26%|██▌       | 52/200 [17:31:29<48:57:11, 1190.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6539.934511483434
INFO:root:current train perplexity173.9563446044922
INFO:root:current mean train loss 6572.041696016906
INFO:root:current train perplexity175.35841369628906
INFO:root:current mean train loss 6540.460043755521
INFO:root:current train perplexity174.48878479003906
INFO:root:current mean train loss 6530.558711039491
INFO:root:current train perplexity174.24874877929688
INFO:root:current mean train loss 6526.602527942223
INFO:root:current train perplexity174.25181579589844
INFO:root:current mean train loss 6527.230016482633
INFO:root:current train perplexity174.66360473632812
INFO:root:current mean train loss 6530.330542099424
INFO:root:current train perplexity174.54129028320312
INFO:root:current mean train loss 6545.177110148268
INFO:root:current train perplexity174.8535919189453
INFO:root:current mean train loss 6546.038328695675
INFO:root:current train perplexity174.9631805419922
INFO:root:current mean train loss 6548.073812925197
INFO:root:current train perplexity174.8416748046875
INFO:root:current mean train loss 6553.8142033486265
INFO:root:current train perplexity174.91213989257812
INFO:root:current mean train loss 6553.969622137178
INFO:root:current train perplexity174.8604278564453
INFO:root:current mean train loss 6552.938883019534
INFO:root:current train perplexity174.91246032714844
INFO:root:current mean train loss 6554.841594218863
INFO:root:current train perplexity175.01492309570312
INFO:root:current mean train loss 6554.359771749094
INFO:root:current train perplexity174.99839782714844
INFO:root:current mean train loss 6553.73604311434
INFO:root:current train perplexity174.8881072998047
INFO:root:current mean train loss 6551.636938665144
INFO:root:current train perplexity174.87142944335938
INFO:root:current mean train loss 6551.04710750184
INFO:root:current train perplexity174.8089141845703
INFO:root:current mean train loss 6552.722406534204
INFO:root:current train perplexity174.974609375
INFO:root:current mean train loss 6549.234054403839
INFO:root:current train perplexity175.05848693847656

100%|██████████| 1/1 [17:16<00:00, 1036.80s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.80s/it]
INFO:root:final mean train loss: 6549.234054403839
INFO:root:final train perplexity: 175.05848693847656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.67s/it][A100%|██████████| 1/1 [01:14<00:00, 74.67s/it]
INFO:root:eval mean loss: 6362.649872908355
INFO:root:eval perplexity: 171.69801330566406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.39s/it][A100%|██████████| 1/1 [01:13<00:00, 73.39s/it]
INFO:root:eval mean loss: 6474.979205625277
INFO:root:eval perplexity: 199.42063903808594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/53
 26%|██▋       | 53/200 [17:51:16<48:34:45, 1189.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6594.665834960938
INFO:root:current train perplexity177.64059448242188
INFO:root:current mean train loss 6569.281740722657
INFO:root:current train perplexity176.0609588623047
INFO:root:current mean train loss 6555.355994466146
INFO:root:current train perplexity176.08270263671875
INFO:root:current mean train loss 6570.581154785156
INFO:root:current train perplexity177.0201873779297
INFO:root:current mean train loss 6558.3500234375
INFO:root:current train perplexity176.73165893554688
INFO:root:current mean train loss 6555.4056290690105
INFO:root:current train perplexity176.80027770996094
INFO:root:current mean train loss 6548.437705775669
INFO:root:current train perplexity176.62425231933594
INFO:root:current mean train loss 6553.486135864257
INFO:root:current train perplexity176.42181396484375
INFO:root:current mean train loss 6545.164133572049
INFO:root:current train perplexity176.02610778808594
INFO:root:current mean train loss 6544.253268554688
INFO:root:current train perplexity175.84591674804688
INFO:root:current mean train loss 6547.0745458984375
INFO:root:current train perplexity175.79376220703125
INFO:root:current mean train loss 6544.988117675781
INFO:root:current train perplexity175.56398010253906
INFO:root:current mean train loss 6545.8457861328125
INFO:root:current train perplexity175.373046875
INFO:root:current mean train loss 6541.708186035156
INFO:root:current train perplexity175.1781463623047
INFO:root:current mean train loss 6542.379986653646
INFO:root:current train perplexity175.25726318359375
INFO:root:current mean train loss 6542.139753723144
INFO:root:current train perplexity175.2032928466797
INFO:root:current mean train loss 6547.138541187959
INFO:root:current train perplexity175.27256774902344
INFO:root:current mean train loss 6548.865501302083
INFO:root:current train perplexity175.28985595703125
INFO:root:current mean train loss 6550.730128752056
INFO:root:current train perplexity175.3009796142578

100%|██████████| 1/1 [17:17<00:00, 1037.66s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.66s/it]
INFO:root:final mean train loss: 6551.571974946222
INFO:root:final train perplexity: 175.38150024414062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.20s/it][A100%|██████████| 1/1 [01:16<00:00, 76.22s/it]
INFO:root:eval mean loss: 6354.8589940713655
INFO:root:eval perplexity: 170.61962890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.10s/it][A100%|██████████| 1/1 [01:12<00:00, 72.10s/it]
INFO:root:eval mean loss: 6470.310763311724
INFO:root:eval perplexity: 198.6605987548828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/54
 27%|██▋       | 54/200 [18:11:04<48:14:00, 1189.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6605.478774126838
INFO:root:current train perplexity176.5504913330078
INFO:root:current mean train loss 6585.2810621995195
INFO:root:current train perplexity176.1139373779297
INFO:root:current mean train loss 6571.684025777649
INFO:root:current train perplexity175.5519256591797
INFO:root:current mean train loss 6562.751441738959
INFO:root:current train perplexity175.57168579101562
INFO:root:current mean train loss 6544.128499934427
INFO:root:current train perplexity175.26170349121094
INFO:root:current mean train loss 6550.214085355718
INFO:root:current train perplexity175.7169952392578
INFO:root:current mean train loss 6544.704097605601
INFO:root:current train perplexity175.49334716796875
INFO:root:current mean train loss 6544.670077825357
INFO:root:current train perplexity175.16920471191406
INFO:root:current mean train loss 6542.38369164531
INFO:root:current train perplexity175.2855987548828
INFO:root:current mean train loss 6547.614901044507
INFO:root:current train perplexity175.454345703125
INFO:root:current mean train loss 6554.701838280482
INFO:root:current train perplexity175.50656127929688
INFO:root:current mean train loss 6555.026054197907
INFO:root:current train perplexity175.68809509277344
INFO:root:current mean train loss 6552.613227486904
INFO:root:current train perplexity175.75587463378906
INFO:root:current mean train loss 6553.46760882332
INFO:root:current train perplexity175.80374145507812
INFO:root:current mean train loss 6554.622255700864
INFO:root:current train perplexity175.84097290039062
INFO:root:current mean train loss 6555.8879283485085
INFO:root:current train perplexity175.7632598876953
INFO:root:current mean train loss 6552.922064031579
INFO:root:current train perplexity175.77381896972656
INFO:root:current mean train loss 6554.940211283307
INFO:root:current train perplexity175.8318328857422
INFO:root:current mean train loss 6555.880541199436
INFO:root:current train perplexity175.92189025878906
INFO:root:current mean train loss 6555.156694980356
INFO:root:current train perplexity175.8907012939453

100%|██████████| 1/1 [17:18<00:00, 1038.91s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.91s/it]
INFO:root:final mean train loss: 6555.080148917163
INFO:root:final train perplexity: 175.86746215820312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.65s/it][A100%|██████████| 1/1 [01:16<00:00, 76.65s/it]
INFO:root:eval mean loss: 6365.855203831449
INFO:root:eval perplexity: 172.14381408691406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.89s/it][A100%|██████████| 1/1 [01:12<00:00, 72.89s/it]
INFO:root:eval mean loss: 6480.512323041335
INFO:root:eval perplexity: 200.3249969482422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/55
 28%|██▊       | 55/200 [18:30:55<47:55:13, 1189.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6507.650175206802
INFO:root:current train perplexity175.29855346679688
INFO:root:current mean train loss 6536.898062179338
INFO:root:current train perplexity175.47711181640625
INFO:root:current mean train loss 6522.739318325988
INFO:root:current train perplexity175.63641357421875
INFO:root:current mean train loss 6515.520947850393
INFO:root:current train perplexity175.17446899414062
INFO:root:current mean train loss 6534.41011124712
INFO:root:current train perplexity175.70359802246094
INFO:root:current mean train loss 6540.950353500994
INFO:root:current train perplexity175.60292053222656
INFO:root:current mean train loss 6556.1898143298995
INFO:root:current train perplexity176.6172637939453
INFO:root:current mean train loss 6569.411404892924
INFO:root:current train perplexity178.00155639648438
INFO:root:current mean train loss 6588.63445122808
INFO:root:current train perplexity180.22271728515625
INFO:root:current mean train loss 6596.959611717077
INFO:root:current train perplexity181.58407592773438
INFO:root:current mean train loss 6603.663693219596
INFO:root:current train perplexity182.4579315185547
INFO:root:current mean train loss 6609.039616660466
INFO:root:current train perplexity183.28431701660156
INFO:root:current mean train loss 6611.2614647171295
INFO:root:current train perplexity183.6268768310547
INFO:root:current mean train loss 6614.20428128221
INFO:root:current train perplexity183.78070068359375
INFO:root:current mean train loss 6610.808850148732
INFO:root:current train perplexity183.6865234375
INFO:root:current mean train loss 6610.839253929168
INFO:root:current train perplexity184.13587951660156
INFO:root:current mean train loss 6613.898217863085
INFO:root:current train perplexity184.4090576171875
INFO:root:current mean train loss 6613.80561281268
INFO:root:current train perplexity184.46401977539062
INFO:root:current mean train loss 6616.375818683121
INFO:root:current train perplexity184.7096710205078
INFO:root:current mean train loss 6617.795812849422
INFO:root:current train perplexity184.89666748046875

100%|██████████| 1/1 [17:15<00:00, 1035.30s/it][A100%|██████████| 1/1 [17:15<00:00, 1035.30s/it]
INFO:root:final mean train loss: 6619.054203650955
INFO:root:final train perplexity: 184.96827697753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.64s/it][A100%|██████████| 1/1 [01:17<00:00, 77.75s/it]
INFO:root:eval mean loss: 6424.57951712101
INFO:root:eval perplexity: 180.51651000976562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.39s/it][A100%|██████████| 1/1 [01:12<00:00, 72.39s/it]
INFO:root:eval mean loss: 6533.978379702738
INFO:root:eval perplexity: 209.27883911132812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/56
 28%|██▊       | 56/200 [18:50:43<47:33:55, 1189.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6657.939137178309
INFO:root:current train perplexity186.82977294921875
INFO:root:current mean train loss 6659.7252108340235
INFO:root:current train perplexity186.71096801757812
INFO:root:current mean train loss 6643.650565705926
INFO:root:current train perplexity184.54258728027344
INFO:root:current mean train loss 6619.622916110221
INFO:root:current train perplexity182.59857177734375
INFO:root:current mean train loss 6618.366213102827
INFO:root:current train perplexity181.7345428466797
INFO:root:current mean train loss 6604.405410794295
INFO:root:current train perplexity180.399658203125
INFO:root:current mean train loss 6596.767893145161
INFO:root:current train perplexity179.87319946289062
INFO:root:current mean train loss 6598.213832728237
INFO:root:current train perplexity179.5753173828125
INFO:root:current mean train loss 6596.6799485669435
INFO:root:current train perplexity179.5625457763672
INFO:root:current mean train loss 6591.6854945041405
INFO:root:current train perplexity179.37582397460938
INFO:root:current mean train loss 6586.7558779584915
INFO:root:current train perplexity179.37274169921875
INFO:root:current mean train loss 6582.44650117425
INFO:root:current train perplexity179.01284790039062
INFO:root:current mean train loss 6580.465932722572
INFO:root:current train perplexity178.8537139892578
INFO:root:current mean train loss 6579.769681601591
INFO:root:current train perplexity178.58706665039062
INFO:root:current mean train loss 6577.510704834489
INFO:root:current train perplexity178.10105895996094
INFO:root:current mean train loss 6574.8951791434965
INFO:root:current train perplexity177.7879638671875
INFO:root:current mean train loss 6568.359539732075
INFO:root:current train perplexity177.40171813964844
INFO:root:current mean train loss 6565.128312560233
INFO:root:current train perplexity177.17422485351562
INFO:root:current mean train loss 6561.50223986823
INFO:root:current train perplexity176.99867248535156
INFO:root:current mean train loss 6562.72958478825
INFO:root:current train perplexity176.90821838378906

100%|██████████| 1/1 [17:18<00:00, 1038.02s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.02s/it]
INFO:root:final mean train loss: 6562.2736186048205
INFO:root:final train perplexity: 176.86798095703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.80s/it][A100%|██████████| 1/1 [01:15<00:00, 75.80s/it]
INFO:root:eval mean loss: 6357.896652329898
INFO:root:eval perplexity: 171.039306640625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.75s/it][A100%|██████████| 1/1 [01:16<00:00, 76.75s/it]
INFO:root:eval mean loss: 6471.584994701629
INFO:root:eval perplexity: 198.86778259277344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/57
 28%|██▊       | 57/200 [19:10:36<47:16:47, 1190.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6628.848424574908
INFO:root:current train perplexity175.46656799316406
INFO:root:current mean train loss 6560.7709059942335
INFO:root:current train perplexity174.10763549804688
INFO:root:current mean train loss 6575.0450439453125
INFO:root:current train perplexity174.00820922851562
INFO:root:current mean train loss 6573.364237909732
INFO:root:current train perplexity173.81796264648438
INFO:root:current mean train loss 6573.858848115317
INFO:root:current train perplexity173.9818115234375
INFO:root:current mean train loss 6561.4500491719855
INFO:root:current train perplexity173.70884704589844
INFO:root:current mean train loss 6543.918860521145
INFO:root:current train perplexity173.245361328125
INFO:root:current mean train loss 6542.986107508342
INFO:root:current train perplexity173.490478515625
INFO:root:current mean train loss 6548.55820953791
INFO:root:current train perplexity173.68408203125
INFO:root:current mean train loss 6551.0240554179045
INFO:root:current train perplexity173.6388702392578
INFO:root:current mean train loss 6549.556950144107
INFO:root:current train perplexity173.55609130859375
INFO:root:current mean train loss 6550.568358956951
INFO:root:current train perplexity173.74354553222656
INFO:root:current mean train loss 6547.595525603189
INFO:root:current train perplexity173.64151000976562
INFO:root:current mean train loss 6545.570846111454
INFO:root:current train perplexity173.6324005126953
INFO:root:current mean train loss 6540.1182269270475
INFO:root:current train perplexity173.55848693847656
INFO:root:current mean train loss 6540.107901748346
INFO:root:current train perplexity173.5674285888672
INFO:root:current mean train loss 6540.360765196437
INFO:root:current train perplexity173.56039428710938
INFO:root:current mean train loss 6538.335632048042
INFO:root:current train perplexity173.5302734375
INFO:root:current mean train loss 6540.534123749499
INFO:root:current train perplexity173.58729553222656
INFO:root:current mean train loss 6539.908124474006
INFO:root:current train perplexity173.49766540527344

100%|██████████| 1/1 [17:24<00:00, 1044.98s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.98s/it]
INFO:root:final mean train loss: 6537.574915468002
INFO:root:final train perplexity: 173.45614624023438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.73s/it][A100%|██████████| 1/1 [01:14<00:00, 74.73s/it]
INFO:root:eval mean loss: 6344.788648672983
INFO:root:eval perplexity: 169.23568725585938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.66s/it][A100%|██████████| 1/1 [01:15<00:00, 75.66s/it]
INFO:root:eval mean loss: 6460.902506510417
INFO:root:eval perplexity: 197.13792419433594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/58
 29%|██▉       | 58/200 [19:30:33<47:02:13, 1192.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6577.243169806985
INFO:root:current train perplexity172.2879180908203
INFO:root:current mean train loss 6540.880790751689
INFO:root:current train perplexity172.81253051757812
INFO:root:current mean train loss 6528.59435649671
INFO:root:current train perplexity172.80601501464844
INFO:root:current mean train loss 6527.425398234578
INFO:root:current train perplexity172.54542541503906
INFO:root:current mean train loss 6524.038455420426
INFO:root:current train perplexity172.3719024658203
INFO:root:current mean train loss 6517.311613581731
INFO:root:current train perplexity172.4434356689453
INFO:root:current mean train loss 6517.410949617929
INFO:root:current train perplexity172.4754180908203
INFO:root:current mean train loss 6526.2792284534235
INFO:root:current train perplexity172.8780517578125
INFO:root:current mean train loss 6529.133115951624
INFO:root:current train perplexity172.88877868652344
INFO:root:current mean train loss 6531.06724202887
INFO:root:current train perplexity173.0042266845703
INFO:root:current mean train loss 6542.492298657114
INFO:root:current train perplexity173.32022094726562
INFO:root:current mean train loss 6541.0366454048
INFO:root:current train perplexity173.29080200195312
INFO:root:current mean train loss 6542.916817014227
INFO:root:current train perplexity173.30657958984375
INFO:root:current mean train loss 6538.96550760097
INFO:root:current train perplexity173.2066192626953
INFO:root:current mean train loss 6537.305005787037
INFO:root:current train perplexity173.0670928955078
INFO:root:current mean train loss 6536.726945423403
INFO:root:current train perplexity172.95877075195312
INFO:root:current mean train loss 6535.501383125464
INFO:root:current train perplexity172.85646057128906
INFO:root:current mean train loss 6535.1461514684
INFO:root:current train perplexity172.68040466308594
INFO:root:current mean train loss 6535.303123963859
INFO:root:current train perplexity172.6736602783203

100%|██████████| 1/1 [17:18<00:00, 1038.26s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.26s/it]
INFO:root:final mean train loss: 6530.3472230019615
INFO:root:final train perplexity: 172.47024536132812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.29s/it][A100%|██████████| 1/1 [01:15<00:00, 75.29s/it]
INFO:root:eval mean loss: 6322.543109000997
INFO:root:eval perplexity: 166.21826171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.04s/it][A100%|██████████| 1/1 [01:14<00:00, 74.04s/it]
INFO:root:eval mean loss: 6445.907989285516
INFO:root:eval perplexity: 194.73512268066406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/59
 30%|██▉       | 59/200 [19:50:23<46:40:32, 1191.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6591.325439453125
INFO:root:current train perplexity175.85623168945312
INFO:root:current mean train loss 6498.338264016544
INFO:root:current train perplexity169.6882781982422
INFO:root:current mean train loss 6491.637717067605
INFO:root:current train perplexity170.39572143554688
INFO:root:current mean train loss 6497.716070920426
INFO:root:current train perplexity170.3518524169922
INFO:root:current mean train loss 6510.257708041822
INFO:root:current train perplexity170.6554412841797
INFO:root:current mean train loss 6510.736599500436
INFO:root:current train perplexity170.63095092773438
INFO:root:current mean train loss 6506.892796310475
INFO:root:current train perplexity170.57342529296875
INFO:root:current mean train loss 6505.922700626558
INFO:root:current train perplexity170.5589599609375
INFO:root:current mean train loss 6508.402329138093
INFO:root:current train perplexity170.51577758789062
INFO:root:current mean train loss 6508.50956912157
INFO:root:current train perplexity170.6310272216797
INFO:root:current mean train loss 6509.165184767185
INFO:root:current train perplexity170.96371459960938
INFO:root:current mean train loss 6513.252303606369
INFO:root:current train perplexity171.139404296875
INFO:root:current mean train loss 6520.341076233621
INFO:root:current train perplexity171.2340087890625
INFO:root:current mean train loss 6519.993396202357
INFO:root:current train perplexity171.2197265625
INFO:root:current mean train loss 6520.110034980719
INFO:root:current train perplexity171.266357421875
INFO:root:current mean train loss 6521.2479893345335
INFO:root:current train perplexity171.35391235351562
INFO:root:current mean train loss 6528.136953137192
INFO:root:current train perplexity171.51275634765625
INFO:root:current mean train loss 6528.545848232318
INFO:root:current train perplexity171.5089874267578
INFO:root:current mean train loss 6526.832833039244
INFO:root:current train perplexity171.4395751953125
INFO:root:current mean train loss 6524.554826898906
INFO:root:current train perplexity171.37393188476562

100%|██████████| 1/1 [17:14<00:00, 1034.52s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.52s/it]
INFO:root:final mean train loss: 6522.132661435679
INFO:root:final train perplexity: 171.35653686523438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.17s/it][A100%|██████████| 1/1 [01:18<00:00, 78.17s/it]
INFO:root:eval mean loss: 6322.477404005984
INFO:root:eval perplexity: 166.20938110351562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.32s/it][A100%|██████████| 1/1 [01:12<00:00, 72.32s/it]
INFO:root:eval mean loss: 6450.318199211824
INFO:root:eval perplexity: 195.4387664794922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/60
 30%|███       | 60/200 [20:10:11<46:17:38, 1190.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6505.899259868421
INFO:root:current train perplexity171.5594482421875
INFO:root:current mean train loss 6499.675215007878
INFO:root:current train perplexity170.58668518066406
INFO:root:current mean train loss 6530.409217590611
INFO:root:current train perplexity171.06369018554688
INFO:root:current mean train loss 6537.376198508523
INFO:root:current train perplexity171.04779052734375
INFO:root:current mean train loss 6529.8575360792065
INFO:root:current train perplexity171.1776885986328
INFO:root:current mean train loss 6539.761722513246
INFO:root:current train perplexity171.8784637451172
INFO:root:current mean train loss 6527.798283048516
INFO:root:current train perplexity170.7076873779297
INFO:root:current mean train loss 6524.810335671288
INFO:root:current train perplexity169.81228637695312
INFO:root:current mean train loss 6509.323785318415
INFO:root:current train perplexity169.25247192382812
INFO:root:current mean train loss 6516.528629008263
INFO:root:current train perplexity169.38011169433594
INFO:root:current mean train loss 6530.7255940835075
INFO:root:current train perplexity170.8704071044922
INFO:root:current mean train loss 6537.85407285034
INFO:root:current train perplexity172.39279174804688
INFO:root:current mean train loss 6552.011448773329
INFO:root:current train perplexity174.1907501220703
INFO:root:current mean train loss 6562.031391042575
INFO:root:current train perplexity175.5979766845703
INFO:root:current mean train loss 6565.745796789663
INFO:root:current train perplexity176.677490234375
INFO:root:current mean train loss 6568.9211446675445
INFO:root:current train perplexity177.3978271484375
INFO:root:current mean train loss 6571.366153936168
INFO:root:current train perplexity178.05120849609375
INFO:root:current mean train loss 6578.118133669193
INFO:root:current train perplexity178.87649536132812
INFO:root:current mean train loss 6581.650722409291
INFO:root:current train perplexity179.3422088623047
INFO:root:current mean train loss 6586.468312607885
INFO:root:current train perplexity179.8685760498047

100%|██████████| 1/1 [17:21<00:00, 1041.99s/it][A100%|██████████| 1/1 [17:21<00:00, 1042.00s/it]
INFO:root:final mean train loss: 6585.517074084799
INFO:root:final train perplexity: 180.14013671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.18s/it][A100%|██████████| 1/1 [01:14<00:00, 74.18s/it]
INFO:root:eval mean loss: 6477.846799160572
INFO:root:eval perplexity: 188.46307373046875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.31s/it][A100%|██████████| 1/1 [01:14<00:00, 74.31s/it]
INFO:root:eval mean loss: 6565.4493408203125
INFO:root:eval perplexity: 214.73501586914062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/61
 30%|███       | 61/200 [20:30:03<45:59:25, 1191.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6748.8411865234375
INFO:root:current train perplexity193.62124633789062
INFO:root:current mean train loss 6657.628694422105
INFO:root:current train perplexity190.34185791015625
INFO:root:current mean train loss 6634.602932170286
INFO:root:current train perplexity188.0408172607422
INFO:root:current mean train loss 6628.998270670573
INFO:root:current train perplexity187.2506561279297
INFO:root:current mean train loss 6630.945199388976
INFO:root:current train perplexity187.37570190429688
INFO:root:current mean train loss 6616.550321208897
INFO:root:current train perplexity184.70359802246094
INFO:root:current mean train loss 6594.868158688335
INFO:root:current train perplexity182.57594299316406
INFO:root:current mean train loss 6591.411597210428
INFO:root:current train perplexity181.11099243164062
INFO:root:current mean train loss 6588.883986477646
INFO:root:current train perplexity180.27127075195312
INFO:root:current mean train loss 6579.979005471254
INFO:root:current train perplexity179.1582794189453
INFO:root:current mean train loss 6573.59949955885
INFO:root:current train perplexity178.401611328125
INFO:root:current mean train loss 6571.01811927473
INFO:root:current train perplexity177.9726104736328
INFO:root:current mean train loss 6566.891411938714
INFO:root:current train perplexity177.43885803222656
INFO:root:current mean train loss 6567.89973751228
INFO:root:current train perplexity177.19271850585938
INFO:root:current mean train loss 6566.625497462026
INFO:root:current train perplexity176.82388305664062
INFO:root:current mean train loss 6562.682963371277
INFO:root:current train perplexity176.44461059570312
INFO:root:current mean train loss 6564.934927569625
INFO:root:current train perplexity176.27734375
INFO:root:current mean train loss 6562.93224985149
INFO:root:current train perplexity176.1334991455078
INFO:root:current mean train loss 6559.196515384582
INFO:root:current train perplexity175.9357452392578
INFO:root:current mean train loss 6555.33629513575
INFO:root:current train perplexity175.68507385253906

100%|██████████| 1/1 [17:16<00:00, 1036.46s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.46s/it]
INFO:root:final mean train loss: 6552.943471288176
INFO:root:final train perplexity: 175.57144165039062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.49s/it][A100%|██████████| 1/1 [01:13<00:00, 73.49s/it]
INFO:root:eval mean loss: 6304.566750817265
INFO:root:eval perplexity: 163.8192138671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.39s/it][A100%|██████████| 1/1 [01:14<00:00, 74.39s/it]
INFO:root:eval mean loss: 6420.378351306239
INFO:root:eval perplexity: 190.71144104003906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/62
 31%|███       | 62/200 [20:49:50<45:36:32, 1189.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6418.464456810141
INFO:root:current train perplexity167.4403839111328
INFO:root:current mean train loss 6418.955358966503
INFO:root:current train perplexity166.31443786621094
INFO:root:current mean train loss 6440.814194509634
INFO:root:current train perplexity166.3054656982422
INFO:root:current mean train loss 6456.504358567192
INFO:root:current train perplexity166.38201904296875
INFO:root:current mean train loss 6483.093356572503
INFO:root:current train perplexity166.865478515625
INFO:root:current mean train loss 6488.868068701967
INFO:root:current train perplexity166.98065185546875
INFO:root:current mean train loss 6498.521763286036
INFO:root:current train perplexity166.98797607421875
INFO:root:current mean train loss 6495.594465886455
INFO:root:current train perplexity166.91648864746094
INFO:root:current mean train loss 6502.238676797882
INFO:root:current train perplexity167.02349853515625
INFO:root:current mean train loss 6498.766692762985
INFO:root:current train perplexity166.9561309814453
INFO:root:current mean train loss 6492.094041670376
INFO:root:current train perplexity167.01904296875
INFO:root:current mean train loss 6494.324419483141
INFO:root:current train perplexity167.0927276611328
INFO:root:current mean train loss 6498.726647452364
INFO:root:current train perplexity167.30958557128906
INFO:root:current mean train loss 6501.304870831023
INFO:root:current train perplexity167.8079071044922
INFO:root:current mean train loss 6504.53715877441
INFO:root:current train perplexity168.30770874023438
INFO:root:current mean train loss 6506.146366785053
INFO:root:current train perplexity168.6006622314453
INFO:root:current mean train loss 6499.884443944249
INFO:root:current train perplexity168.33067321777344
INFO:root:current mean train loss 6497.831351890063
INFO:root:current train perplexity168.103271484375
INFO:root:current mean train loss 6500.387509539008
INFO:root:current train perplexity168.33419799804688
INFO:root:current mean train loss 6506.326959925435
INFO:root:current train perplexity168.965087890625

100%|██████████| 1/1 [17:18<00:00, 1038.58s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.58s/it]
INFO:root:final mean train loss: 6504.565389428305
INFO:root:final train perplexity: 168.99876403808594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.32s/it][A100%|██████████| 1/1 [01:15<00:00, 75.32s/it]
INFO:root:eval mean loss: 6367.946406804078
INFO:root:eval perplexity: 172.43511962890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.29s/it][A100%|██████████| 1/1 [01:13<00:00, 73.29s/it]
INFO:root:eval mean loss: 6473.473799035904
INFO:root:eval perplexity: 199.1751708984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/63
 32%|███▏      | 63/200 [21:09:40<45:16:31, 1189.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6524.499428013393
INFO:root:current train perplexity176.44561767578125
INFO:root:current mean train loss 6560.059105009192
INFO:root:current train perplexity176.34544372558594
INFO:root:current mean train loss 6571.275213396991
INFO:root:current train perplexity176.39463806152344
INFO:root:current mean train loss 6569.045334934544
INFO:root:current train perplexity177.2119903564453
INFO:root:current mean train loss 6564.108701795213
INFO:root:current train perplexity176.35166931152344
INFO:root:current mean train loss 6563.673602830318
INFO:root:current train perplexity175.95327758789062
INFO:root:current mean train loss 6558.764598151819
INFO:root:current train perplexity175.76124572753906
INFO:root:current mean train loss 6564.665240082183
INFO:root:current train perplexity175.82101440429688
INFO:root:current mean train loss 6560.155763963721
INFO:root:current train perplexity175.60986328125
INFO:root:current mean train loss 6563.417408988402
INFO:root:current train perplexity175.56524658203125
INFO:root:current mean train loss 6562.709494104118
INFO:root:current train perplexity175.5135040283203
INFO:root:current mean train loss 6560.003350777912
INFO:root:current train perplexity175.38943481445312
INFO:root:current mean train loss 6557.740906819021
INFO:root:current train perplexity175.3441162109375
INFO:root:current mean train loss 6550.284347556455
INFO:root:current train perplexity175.0480499267578
INFO:root:current mean train loss 6546.476334635417
INFO:root:current train perplexity174.97152709960938
INFO:root:current mean train loss 6553.681163851015
INFO:root:current train perplexity175.1541748046875
INFO:root:current mean train loss 6548.902245801366
INFO:root:current train perplexity174.99781799316406
INFO:root:current mean train loss 6551.332076767743
INFO:root:current train perplexity175.00840759277344
INFO:root:current mean train loss 6552.57224447485
INFO:root:current train perplexity175.1162567138672
INFO:root:current mean train loss 6550.506974738261
INFO:root:current train perplexity175.03561401367188

100%|██████████| 1/1 [17:19<00:00, 1039.06s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.06s/it]
INFO:root:final mean train loss: 6549.542605678541
INFO:root:final train perplexity: 175.10098266601562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.82s/it][A100%|██████████| 1/1 [01:16<00:00, 76.82s/it]
INFO:root:eval mean loss: 6360.365966796875
INFO:root:eval perplexity: 171.38121032714844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.29s/it][A100%|██████████| 1/1 [01:12<00:00, 72.30s/it]
INFO:root:eval mean loss: 6466.8089746786345
INFO:root:eval perplexity: 198.0925750732422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/64
 32%|███▏      | 64/200 [21:29:30<44:57:13, 1189.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6487.244073275862
INFO:root:current train perplexity173.794921875
INFO:root:current mean train loss 6508.534487800802
INFO:root:current train perplexity173.81564331054688
INFO:root:current mean train loss 6520.218714272104
INFO:root:current train perplexity174.2799072265625
INFO:root:current mean train loss 6527.588226239503
INFO:root:current train perplexity174.6556854248047
INFO:root:current mean train loss 6538.9555884641295
INFO:root:current train perplexity174.5080108642578
INFO:root:current mean train loss 6537.955833422061
INFO:root:current train perplexity174.34556579589844
INFO:root:current mean train loss 6537.7422621281385
INFO:root:current train perplexity174.35662841796875
INFO:root:current mean train loss 6541.375095546776
INFO:root:current train perplexity174.7173309326172
INFO:root:current mean train loss 6540.845872674747
INFO:root:current train perplexity174.7175750732422
INFO:root:current mean train loss 6546.687042390926
INFO:root:current train perplexity174.88511657714844
INFO:root:current mean train loss 6542.40025855997
INFO:root:current train perplexity174.84959411621094
INFO:root:current mean train loss 6544.626281378343
INFO:root:current train perplexity175.02919006347656
INFO:root:current mean train loss 6543.984923225646
INFO:root:current train perplexity174.77523803710938
INFO:root:current mean train loss 6545.545724177068
INFO:root:current train perplexity174.4034423828125
INFO:root:current mean train loss 6542.592257245083
INFO:root:current train perplexity174.01583862304688
INFO:root:current mean train loss 6539.435960083392
INFO:root:current train perplexity174.0044708251953
INFO:root:current mean train loss 6537.638751180905
INFO:root:current train perplexity174.19192504882812
INFO:root:current mean train loss 6542.961469226532
INFO:root:current train perplexity174.53514099121094
INFO:root:current mean train loss 6547.289273389888
INFO:root:current train perplexity174.857177734375

100%|██████████| 1/1 [17:17<00:00, 1037.87s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.87s/it]
INFO:root:final mean train loss: 6550.369846084294
INFO:root:final train perplexity: 175.21531677246094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.02s/it][A100%|██████████| 1/1 [01:15<00:00, 75.02s/it]
INFO:root:eval mean loss: 6404.991226520944
INFO:root:eval perplexity: 177.67947387695312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.29s/it][A100%|██████████| 1/1 [01:13<00:00, 73.29s/it]
INFO:root:eval mean loss: 6511.369074828236
INFO:root:eval perplexity: 205.44456481933594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/65
 32%|███▎      | 65/200 [21:49:19<44:36:31, 1189.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6517.931396484375
INFO:root:current train perplexity180.29698181152344
INFO:root:current mean train loss 6538.423715444712
INFO:root:current train perplexity173.22132873535156
INFO:root:current mean train loss 6535.256656422334
INFO:root:current train perplexity173.50743103027344
INFO:root:current mean train loss 6509.23222110146
INFO:root:current train perplexity173.283203125
INFO:root:current mean train loss 6531.216092251315
INFO:root:current train perplexity173.62570190429688
INFO:root:current mean train loss 6546.39154343378
INFO:root:current train perplexity173.627197265625
INFO:root:current mean train loss 6544.544702795168
INFO:root:current train perplexity173.02444458007812
INFO:root:current mean train loss 6523.714698097922
INFO:root:current train perplexity171.21115112304688
INFO:root:current mean train loss 6517.138063952698
INFO:root:current train perplexity170.2393035888672
INFO:root:current mean train loss 6506.489497632052
INFO:root:current train perplexity169.18113708496094
INFO:root:current mean train loss 6501.503500159518
INFO:root:current train perplexity168.64834594726562
INFO:root:current mean train loss 6496.818784851959
INFO:root:current train perplexity168.14134216308594
INFO:root:current mean train loss 6496.356377180233
INFO:root:current train perplexity167.7724609375
INFO:root:current mean train loss 6499.364467129386
INFO:root:current train perplexity167.45172119140625
INFO:root:current mean train loss 6495.303786057692
INFO:root:current train perplexity167.11773681640625
INFO:root:current mean train loss 6489.540815962122
INFO:root:current train perplexity166.92022705078125
INFO:root:current mean train loss 6486.208299441825
INFO:root:current train perplexity166.51019287109375
INFO:root:current mean train loss 6486.517110761903
INFO:root:current train perplexity166.39930725097656
INFO:root:current mean train loss 6488.163631329251
INFO:root:current train perplexity166.30380249023438
INFO:root:current mean train loss 6484.028650107504
INFO:root:current train perplexity166.09744262695312

100%|██████████| 1/1 [17:18<00:00, 1038.82s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.82s/it]
INFO:root:final mean train loss: 6481.832037775191
INFO:root:final train perplexity: 165.9957733154297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.57s/it][A100%|██████████| 1/1 [01:14<00:00, 74.57s/it]
INFO:root:eval mean loss: 6250.668235400044
INFO:root:eval perplexity: 156.83180236816406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.56s/it][A100%|██████████| 1/1 [01:14<00:00, 74.56s/it]
INFO:root:eval mean loss: 6375.579473833665
INFO:root:eval perplexity: 183.85073852539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/66
 33%|███▎      | 66/200 [22:09:09<44:17:07, 1189.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6371.136183965774
INFO:root:current train perplexity160.75457763671875
INFO:root:current mean train loss 6428.799764333678
INFO:root:current train perplexity162.51475524902344
INFO:root:current mean train loss 6436.755342371323
INFO:root:current train perplexity162.69207763671875
INFO:root:current mean train loss 6448.423756632107
INFO:root:current train perplexity162.8333282470703
INFO:root:current mean train loss 6439.479370407141
INFO:root:current train perplexity162.5063934326172
INFO:root:current mean train loss 6445.3783523647435
INFO:root:current train perplexity162.3650360107422
INFO:root:current mean train loss 6456.547932549567
INFO:root:current train perplexity162.5160675048828
INFO:root:current mean train loss 6459.459586430522
INFO:root:current train perplexity162.69944763183594
INFO:root:current mean train loss 6454.995026787074
INFO:root:current train perplexity162.50119018554688
INFO:root:current mean train loss 6459.251335483679
INFO:root:current train perplexity162.8146209716797
INFO:root:current mean train loss 6458.635589629499
INFO:root:current train perplexity163.09815979003906
INFO:root:current mean train loss 6456.611359922084
INFO:root:current train perplexity163.09396362304688
INFO:root:current mean train loss 6456.254499705671
INFO:root:current train perplexity162.9869842529297
INFO:root:current mean train loss 6455.972625570709
INFO:root:current train perplexity162.96926879882812
INFO:root:current mean train loss 6455.774649097247
INFO:root:current train perplexity162.91651916503906
INFO:root:current mean train loss 6454.697774451944
INFO:root:current train perplexity162.84994506835938
INFO:root:current mean train loss 6457.158515793684
INFO:root:current train perplexity162.86175537109375
INFO:root:current mean train loss 6457.22428366502
INFO:root:current train perplexity162.8121337890625
INFO:root:current mean train loss 6456.999222932969
INFO:root:current train perplexity162.61180114746094
INFO:root:current mean train loss 6457.671823655485
INFO:root:current train perplexity162.52615356445312

100%|██████████| 1/1 [17:21<00:00, 1041.03s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.03s/it]
INFO:root:final mean train loss: 6454.3323786856245
INFO:root:final train perplexity: 162.4345703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.06s/it][A100%|██████████| 1/1 [01:14<00:00, 74.06s/it]
INFO:root:eval mean loss: 6218.531895847185
INFO:root:eval perplexity: 152.8081817626953
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.21s/it][A100%|██████████| 1/1 [01:13<00:00, 73.21s/it]
INFO:root:eval mean loss: 6353.76334808566
INFO:root:eval perplexity: 180.59959411621094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/67
 34%|███▎      | 67/200 [22:29:00<43:57:54, 1190.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6377.459729646382
INFO:root:current train perplexity158.86241149902344
INFO:root:current mean train loss 6436.708251953125
INFO:root:current train perplexity160.99130249023438
INFO:root:current mean train loss 6467.898700105042
INFO:root:current train perplexity161.21046447753906
INFO:root:current mean train loss 6461.814349112426
INFO:root:current train perplexity160.72775268554688
INFO:root:current mean train loss 6445.954761522546
INFO:root:current train perplexity160.39447021484375
INFO:root:current mean train loss 6451.11225749303
INFO:root:current train perplexity160.2579345703125
INFO:root:current mean train loss 6450.206613379212
INFO:root:current train perplexity160.1556396484375
INFO:root:current mean train loss 6456.4432826261855
INFO:root:current train perplexity160.2854461669922
INFO:root:current mean train loss 6454.935172215282
INFO:root:current train perplexity160.44871520996094
INFO:root:current mean train loss 6452.754965580857
INFO:root:current train perplexity160.36158752441406
INFO:root:current mean train loss 6451.774389130991
INFO:root:current train perplexity160.4100799560547
INFO:root:current mean train loss 6447.571011454443
INFO:root:current train perplexity160.40342712402344
INFO:root:current mean train loss 6447.248952443457
INFO:root:current train perplexity160.4696807861328
INFO:root:current mean train loss 6450.709320843844
INFO:root:current train perplexity160.6915740966797
INFO:root:current mean train loss 6446.33987532869
INFO:root:current train perplexity160.67486572265625
INFO:root:current mean train loss 6457.984137526414
INFO:root:current train perplexity161.83224487304688
INFO:root:current mean train loss 6460.952830183055
INFO:root:current train perplexity162.72055053710938
INFO:root:current mean train loss 6468.055356990344
INFO:root:current train perplexity163.65501403808594
INFO:root:current mean train loss 6468.560471162184
INFO:root:current train perplexity164.10723876953125
INFO:root:current mean train loss 6475.756865415779
INFO:root:current train perplexity164.8876190185547

100%|██████████| 1/1 [17:22<00:00, 1042.17s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.17s/it]
INFO:root:final mean train loss: 6476.446837424751
INFO:root:final train perplexity: 165.29232788085938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.61s/it][A100%|██████████| 1/1 [01:16<00:00, 76.61s/it]
INFO:root:eval mean loss: 6498.57921584109
INFO:root:eval perplexity: 191.64976501464844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.29s/it][A100%|██████████| 1/1 [01:12<00:00, 72.29s/it]
INFO:root:eval mean loss: 6581.590796071587
INFO:root:eval perplexity: 217.5885772705078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/68
 34%|███▍      | 68/200 [22:48:53<43:40:19, 1191.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6518.305255681818
INFO:root:current train perplexity180.4763946533203
INFO:root:current mean train loss 6504.862081023185
INFO:root:current train perplexity173.1793670654297
INFO:root:current mean train loss 6553.147855392157
INFO:root:current train perplexity175.3758087158203
INFO:root:current mean train loss 6572.095356514084
INFO:root:current train perplexity176.60269165039062
INFO:root:current mean train loss 6578.802569110577
INFO:root:current train perplexity176.42416381835938
INFO:root:current mean train loss 6580.1472471494935
INFO:root:current train perplexity176.8756561279297
INFO:root:current mean train loss 6571.095518994513
INFO:root:current train perplexity176.01243591308594
INFO:root:current mean train loss 6567.450413907285
INFO:root:current train perplexity175.65953063964844
INFO:root:current mean train loss 6564.77587148209
INFO:root:current train perplexity176.0355224609375
INFO:root:current mean train loss 6558.683301803829
INFO:root:current train perplexity175.58030700683594
INFO:root:current mean train loss 6555.1855070719785
INFO:root:current train perplexity174.82725524902344
INFO:root:current mean train loss 6557.335454291802
INFO:root:current train perplexity174.3248748779297
INFO:root:current mean train loss 6552.629610853462
INFO:root:current train perplexity173.77105712890625
INFO:root:current mean train loss 6547.808077000692
INFO:root:current train perplexity173.2755126953125
INFO:root:current mean train loss 6535.784846166237
INFO:root:current train perplexity172.40248107910156
INFO:root:current mean train loss 6534.041380187399
INFO:root:current train perplexity171.97265625
INFO:root:current mean train loss 6531.377431375095
INFO:root:current train perplexity171.7301025390625
INFO:root:current mean train loss 6526.989572204416
INFO:root:current train perplexity171.355712890625
INFO:root:current mean train loss 6519.8529417958225
INFO:root:current train perplexity170.8228302001953
INFO:root:current mean train loss 6515.88318189538
INFO:root:current train perplexity170.39173889160156

100%|██████████| 1/1 [17:35<00:00, 1055.27s/it][A100%|██████████| 1/1 [17:35<00:00, 1055.27s/it]
INFO:root:final mean train loss: 6514.3394402962285
INFO:root:final train perplexity: 170.30653381347656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.12s/it][A100%|██████████| 1/1 [01:17<00:00, 77.12s/it]
INFO:root:eval mean loss: 6242.021636746454
INFO:root:eval perplexity: 155.73878479003906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.33s/it][A100%|██████████| 1/1 [01:16<00:00, 76.33s/it]
INFO:root:eval mean loss: 6368.0139861411235
INFO:root:eval perplexity: 182.7166748046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/69
 34%|███▍      | 69/200 [23:09:04<43:33:38, 1197.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6445.573961046007
INFO:root:current train perplexity163.14584350585938
INFO:root:current mean train loss 6502.813834256904
INFO:root:current train perplexity165.33978271484375
INFO:root:current mean train loss 6493.880534452551
INFO:root:current train perplexity166.09117126464844
INFO:root:current mean train loss 6487.507063014533
INFO:root:current train perplexity166.77304077148438
INFO:root:current mean train loss 6483.026532706568
INFO:root:current train perplexity166.67208862304688
INFO:root:current mean train loss 6496.974765590854
INFO:root:current train perplexity167.35769653320312
INFO:root:current mean train loss 6504.391964140393
INFO:root:current train perplexity167.46633911132812
INFO:root:current mean train loss 6498.446962030441
INFO:root:current train perplexity167.21299743652344
INFO:root:current mean train loss 6494.734485871201
INFO:root:current train perplexity166.65985107421875
INFO:root:current mean train loss 6487.5685367034785
INFO:root:current train perplexity166.14083862304688
INFO:root:current mean train loss 6487.413355129868
INFO:root:current train perplexity165.82180786132812
INFO:root:current mean train loss 6486.819217616788
INFO:root:current train perplexity165.71714782714844
INFO:root:current mean train loss 6482.359732765822
INFO:root:current train perplexity165.48838806152344
INFO:root:current mean train loss 6477.963909182535
INFO:root:current train perplexity165.14634704589844
INFO:root:current mean train loss 6475.320410355278
INFO:root:current train perplexity164.79527282714844
INFO:root:current mean train loss 6472.869619587906
INFO:root:current train perplexity164.58331298828125
INFO:root:current mean train loss 6472.182988947088
INFO:root:current train perplexity164.4072265625
INFO:root:current mean train loss 6471.345780831159
INFO:root:current train perplexity164.37594604492188
INFO:root:current mean train loss 6474.140354254307
INFO:root:current train perplexity164.48939514160156
INFO:root:current mean train loss 6472.651068820915
INFO:root:current train perplexity164.54415893554688

100%|██████████| 1/1 [17:34<00:00, 1055.00s/it][A100%|██████████| 1/1 [17:34<00:00, 1055.00s/it]
INFO:root:final mean train loss: 6470.747627662277
INFO:root:final train perplexity: 164.5509796142578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.46s/it][A100%|██████████| 1/1 [01:17<00:00, 77.46s/it]
INFO:root:eval mean loss: 6276.742554576685
INFO:root:eval perplexity: 160.17405700683594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.28s/it][A100%|██████████| 1/1 [01:15<00:00, 75.28s/it]
INFO:root:eval mean loss: 6391.429858917885
INFO:root:eval perplexity: 186.2494659423828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/70
 35%|███▌      | 70/200 [23:29:15<43:22:11, 1201.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6575.202252677317
INFO:root:current train perplexity170.19871520996094
INFO:root:current mean train loss 6539.2931392609125
INFO:root:current train perplexity171.7919464111328
INFO:root:current mean train loss 6519.930589722102
INFO:root:current train perplexity172.11802673339844
INFO:root:current mean train loss 6519.270627058564
INFO:root:current train perplexity171.95364379882812
INFO:root:current mean train loss 6506.97820907624
INFO:root:current train perplexity171.92100524902344
INFO:root:current mean train loss 6517.664882381419
INFO:root:current train perplexity171.93150329589844
INFO:root:current mean train loss 6515.293230961992
INFO:root:current train perplexity171.86297607421875
INFO:root:current mean train loss 6518.515609528477
INFO:root:current train perplexity171.8990478515625
INFO:root:current mean train loss 6523.178624156356
INFO:root:current train perplexity172.06370544433594
INFO:root:current mean train loss 6522.434279022371
INFO:root:current train perplexity172.2655029296875
INFO:root:current mean train loss 6518.562167305154
INFO:root:current train perplexity172.3259735107422
INFO:root:current mean train loss 6521.044208138404
INFO:root:current train perplexity172.5448455810547
INFO:root:current mean train loss 6521.836279183233
INFO:root:current train perplexity172.7127685546875
INFO:root:current mean train loss 6523.886319406947
INFO:root:current train perplexity172.71005249023438
INFO:root:current mean train loss 6525.5697569939975
INFO:root:current train perplexity172.6676483154297
INFO:root:current mean train loss 6531.0581948896715
INFO:root:current train perplexity172.9409637451172
INFO:root:current mean train loss 6532.699678410857
INFO:root:current train perplexity173.10556030273438
INFO:root:current mean train loss 6534.839432982375
INFO:root:current train perplexity173.16351318359375
INFO:root:current mean train loss 6533.426745405141
INFO:root:current train perplexity173.12355041503906

100%|██████████| 1/1 [17:41<00:00, 1061.22s/it][A100%|██████████| 1/1 [17:41<00:00, 1061.22s/it]
INFO:root:final mean train loss: 6534.904599249874
INFO:root:final train perplexity: 173.09127807617188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.29s/it][A100%|██████████| 1/1 [01:17<00:00, 77.29s/it]
INFO:root:eval mean loss: 6304.714161541445
INFO:root:eval perplexity: 163.8387451171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.18s/it][A100%|██████████| 1/1 [01:16<00:00, 76.18s/it]
INFO:root:eval mean loss: 6417.833503885472
INFO:root:eval perplexity: 190.31500244140625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/71
 36%|███▌      | 71/200 [23:49:32<43:12:33, 1205.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6445.85400390625
INFO:root:current train perplexity168.21592712402344
INFO:root:current mean train loss 6484.74846145342
INFO:root:current train perplexity169.67144775390625
INFO:root:current mean train loss 6488.102001005006
INFO:root:current train perplexity169.68739318847656
INFO:root:current mean train loss 6482.164044947406
INFO:root:current train perplexity169.3008270263672
INFO:root:current mean train loss 6493.1788119612065
INFO:root:current train perplexity169.3738250732422
INFO:root:current mean train loss 6489.938192857584
INFO:root:current train perplexity168.7021484375
INFO:root:current mean train loss 6486.753421191728
INFO:root:current train perplexity167.5968780517578
INFO:root:current mean train loss 6490.8139558527355
INFO:root:current train perplexity167.9480743408203
INFO:root:current mean train loss 6497.7191213603055
INFO:root:current train perplexity168.06983947753906
INFO:root:current mean train loss 6501.716719267384
INFO:root:current train perplexity167.97386169433594
INFO:root:current mean train loss 6495.166970345894
INFO:root:current train perplexity167.60678100585938
INFO:root:current mean train loss 6498.360686207335
INFO:root:current train perplexity167.52688598632812
INFO:root:current mean train loss 6493.1584756069915
INFO:root:current train perplexity167.40493774414062
INFO:root:current mean train loss 6495.487599301302
INFO:root:current train perplexity167.28045654296875
INFO:root:current mean train loss 6491.8966257195725
INFO:root:current train perplexity167.11766052246094
INFO:root:current mean train loss 6492.812885826485
INFO:root:current train perplexity167.22152709960938
INFO:root:current mean train loss 6495.847105337407
INFO:root:current train perplexity167.29656982421875
INFO:root:current mean train loss 6495.620438892145
INFO:root:current train perplexity167.27088928222656
INFO:root:current mean train loss 6496.657921133115
INFO:root:current train perplexity167.3023223876953
INFO:root:current mean train loss 6494.182498575633
INFO:root:current train perplexity167.269775390625

100%|██████████| 1/1 [17:18<00:00, 1038.50s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.50s/it]
INFO:root:final mean train loss: 6491.457656683371
INFO:root:final train perplexity: 167.26075744628906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.89s/it][A100%|██████████| 1/1 [01:13<00:00, 73.89s/it]
INFO:root:eval mean loss: 6252.9640039755095
INFO:root:eval perplexity: 157.12319946289062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.38s/it][A100%|██████████| 1/1 [01:13<00:00, 73.38s/it]
INFO:root:eval mean loss: 6375.121569045046
INFO:root:eval perplexity: 183.78184509277344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/72
 36%|███▌      | 72/200 [24:09:20<42:41:04, 1200.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6473.581882642663
INFO:root:current train perplexity163.23890686035156
INFO:root:current mean train loss 6413.61324949187
INFO:root:current train perplexity160.94378662109375
INFO:root:current mean train loss 6401.5381406775505
INFO:root:current train perplexity160.6648406982422
INFO:root:current mean train loss 6407.370455809791
INFO:root:current train perplexity160.51123046875
INFO:root:current mean train loss 6426.865985843307
INFO:root:current train perplexity160.92138671875
INFO:root:current mean train loss 6427.131061969706
INFO:root:current train perplexity160.88746643066406
INFO:root:current mean train loss 6420.571071961527
INFO:root:current train perplexity160.5220184326172
INFO:root:current mean train loss 6421.026131488806
INFO:root:current train perplexity160.35891723632812
INFO:root:current mean train loss 6423.033426203676
INFO:root:current train perplexity160.25006103515625
INFO:root:current mean train loss 6431.497417346628
INFO:root:current train perplexity160.27505493164062
INFO:root:current mean train loss 6433.389022215603
INFO:root:current train perplexity160.38804626464844
INFO:root:current mean train loss 6432.816876269619
INFO:root:current train perplexity160.27650451660156
INFO:root:current mean train loss 6434.4266947311935
INFO:root:current train perplexity160.41482543945312
INFO:root:current mean train loss 6438.087849658093
INFO:root:current train perplexity160.66445922851562
INFO:root:current mean train loss 6437.382195543087
INFO:root:current train perplexity160.7959747314453
INFO:root:current mean train loss 6442.0783673772985
INFO:root:current train perplexity161.15966796875
INFO:root:current mean train loss 6447.181335562038
INFO:root:current train perplexity161.54034423828125
INFO:root:current mean train loss 6450.365088145676
INFO:root:current train perplexity161.94703674316406
INFO:root:current mean train loss 6453.514781824259
INFO:root:current train perplexity162.31317138671875
INFO:root:current mean train loss 6457.680240022101
INFO:root:current train perplexity162.71450805664062

100%|██████████| 1/1 [17:17<00:00, 1037.86s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.86s/it]
INFO:root:final mean train loss: 6457.9976489614855
INFO:root:final train perplexity: 162.90478515625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.29s/it][A100%|██████████| 1/1 [01:14<00:00, 74.29s/it]
INFO:root:eval mean loss: 6302.988334926307
INFO:root:eval perplexity: 163.6101531982422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.74s/it][A100%|██████████| 1/1 [01:11<00:00, 71.74s/it]
INFO:root:eval mean loss: 6416.9453973431955
INFO:root:eval perplexity: 190.1769256591797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/73
 36%|███▋      | 73/200 [24:29:06<42:11:57, 1196.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6503.258239746094
INFO:root:current train perplexity168.60169982910156
INFO:root:current mean train loss 6511.35936453683
INFO:root:current train perplexity169.59881591796875
INFO:root:current mean train loss 6514.378493245443
INFO:root:current train perplexity169.21139526367188
INFO:root:current mean train loss 6529.093807444853
INFO:root:current train perplexity169.66497802734375
INFO:root:current mean train loss 6525.997514204545
INFO:root:current train perplexity169.41380310058594
INFO:root:current mean train loss 6527.8447482638885
INFO:root:current train perplexity169.84458923339844
INFO:root:current mean train loss 6523.797358703613
INFO:root:current train perplexity169.8959503173828
INFO:root:current mean train loss 6522.735732289907
INFO:root:current train perplexity169.9815216064453
INFO:root:current mean train loss 6521.24440452939
INFO:root:current train perplexity169.92520141601562
INFO:root:current mean train loss 6517.159777572308
INFO:root:current train perplexity169.7778778076172
INFO:root:current mean train loss 6516.299102313702
INFO:root:current train perplexity169.7941436767578
INFO:root:current mean train loss 6518.153110437226
INFO:root:current train perplexity169.69151306152344
INFO:root:current mean train loss 6523.9992833291335
INFO:root:current train perplexity169.7782745361328
INFO:root:current mean train loss 6523.630800708373
INFO:root:current train perplexity169.6754150390625
INFO:root:current mean train loss 6516.8978064643015
INFO:root:current train perplexity169.6699981689453
INFO:root:current mean train loss 6517.27215750558
INFO:root:current train perplexity169.7047882080078
INFO:root:current mean train loss 6516.589215236757
INFO:root:current train perplexity169.7410430908203
INFO:root:current mean train loss 6516.008925725126
INFO:root:current train perplexity169.5961456298828
INFO:root:current mean train loss 6513.149367622707
INFO:root:current train perplexity169.43666076660156
INFO:root:current mean train loss 6509.001337991302
INFO:root:current train perplexity169.26385498046875

100%|██████████| 1/1 [17:14<00:00, 1034.57s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.57s/it]
INFO:root:final mean train loss: 6506.099439843159
INFO:root:final train perplexity: 169.2033233642578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.87s/it][A100%|██████████| 1/1 [01:13<00:00, 73.87s/it]
INFO:root:eval mean loss: 6287.80919630984
INFO:root:eval perplexity: 161.61407470703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.19s/it][A100%|██████████| 1/1 [01:12<00:00, 72.19s/it]
INFO:root:eval mean loss: 6404.540144683621
INFO:root:eval perplexity: 188.2571258544922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/74
 37%|███▋      | 74/200 [24:48:49<41:43:43, 1192.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6330.461999725877
INFO:root:current train perplexity161.7645721435547
INFO:root:current mean train loss 6422.656763161824
INFO:root:current train perplexity165.52090454101562
INFO:root:current mean train loss 6441.515492005107
INFO:root:current train perplexity166.5980224609375
INFO:root:current mean train loss 6463.079945454744
INFO:root:current train perplexity166.79348754882812
INFO:root:current mean train loss 6489.70772681038
INFO:root:current train perplexity167.57833862304688
INFO:root:current mean train loss 6497.979314232215
INFO:root:current train perplexity167.53079223632812
INFO:root:current mean train loss 6480.016820805984
INFO:root:current train perplexity167.24154663085938
INFO:root:current mean train loss 6483.2678225881355
INFO:root:current train perplexity167.4679412841797
INFO:root:current mean train loss 6494.026311921128
INFO:root:current train perplexity167.99484252929688
INFO:root:current mean train loss 6497.508998253004
INFO:root:current train perplexity168.30201721191406
INFO:root:current mean train loss 6500.089026098332
INFO:root:current train perplexity168.5641632080078
INFO:root:current mean train loss 6502.366966359658
INFO:root:current train perplexity168.8889617919922
INFO:root:current mean train loss 6508.780524764444
INFO:root:current train perplexity169.20130920410156
INFO:root:current mean train loss 6510.57571453862
INFO:root:current train perplexity169.41258239746094
INFO:root:current mean train loss 6514.36992549438
INFO:root:current train perplexity169.68893432617188
INFO:root:current mean train loss 6511.938229129034
INFO:root:current train perplexity169.7035675048828
INFO:root:current mean train loss 6508.97026641238
INFO:root:current train perplexity169.80580139160156
INFO:root:current mean train loss 6509.049514275487
INFO:root:current train perplexity169.6988067626953
INFO:root:current mean train loss 6510.418700251582
INFO:root:current train perplexity169.6769561767578
INFO:root:current mean train loss 6511.272695721688
INFO:root:current train perplexity169.7245330810547

100%|██████████| 1/1 [17:14<00:00, 1034.25s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.25s/it]
INFO:root:final mean train loss: 6510.519783393217
INFO:root:final train perplexity: 169.7942352294922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.65s/it][A100%|██████████| 1/1 [01:14<00:00, 74.65s/it]
INFO:root:eval mean loss: 6294.209238904587
INFO:root:eval perplexity: 162.45269775390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.94s/it][A100%|██████████| 1/1 [01:11<00:00, 71.94s/it]
INFO:root:eval mean loss: 6409.150238253546
INFO:root:eval perplexity: 188.96826171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/75
 38%|███▊      | 75/200 [25:08:32<41:18:10, 1189.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6504.302714579814
INFO:root:current train perplexity169.63092041015625
INFO:root:current mean train loss 6517.310776984555
INFO:root:current train perplexity170.26292419433594
INFO:root:current mean train loss 6503.524987882071
INFO:root:current train perplexity169.65963745117188
INFO:root:current mean train loss 6507.029189818683
INFO:root:current train perplexity169.98703002929688
INFO:root:current mean train loss 6513.755963418052
INFO:root:current train perplexity170.36404418945312
INFO:root:current mean train loss 6530.85272535796
INFO:root:current train perplexity171.14593505859375
INFO:root:current mean train loss 6516.728488820243
INFO:root:current train perplexity170.85574340820312
INFO:root:current mean train loss 6523.063197724887
INFO:root:current train perplexity171.2876739501953
INFO:root:current mean train loss 6519.014691455413
INFO:root:current train perplexity171.2569580078125
INFO:root:current mean train loss 6520.5537189585475
INFO:root:current train perplexity171.40933227539062
INFO:root:current mean train loss 6523.1364955227245
INFO:root:current train perplexity171.4483184814453
INFO:root:current mean train loss 6527.397152330441
INFO:root:current train perplexity171.68482971191406
INFO:root:current mean train loss 6530.688639450672
INFO:root:current train perplexity171.79702758789062
INFO:root:current mean train loss 6526.728895517763
INFO:root:current train perplexity171.5812225341797
INFO:root:current mean train loss 6520.54314531303
INFO:root:current train perplexity171.3518829345703
INFO:root:current mean train loss 6520.630362407679
INFO:root:current train perplexity171.31993103027344
INFO:root:current mean train loss 6524.108547780391
INFO:root:current train perplexity171.36241149902344
INFO:root:current mean train loss 6523.421631134618
INFO:root:current train perplexity171.3647918701172
INFO:root:current mean train loss 6523.697857607391
INFO:root:current train perplexity171.3439483642578
INFO:root:current mean train loss 6522.15860953133
INFO:root:current train perplexity171.20167541503906

100%|██████████| 1/1 [17:13<00:00, 1033.67s/it][A100%|██████████| 1/1 [17:13<00:00, 1033.67s/it]
INFO:root:final mean train loss: 6521.15794913503
INFO:root:final train perplexity: 171.22470092773438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.91s/it][A100%|██████████| 1/1 [01:13<00:00, 73.91s/it]
INFO:root:eval mean loss: 6306.96290447695
INFO:root:eval perplexity: 164.13705444335938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.82s/it][A100%|██████████| 1/1 [01:11<00:00, 71.82s/it]
INFO:root:eval mean loss: 6421.325949378048
INFO:root:eval perplexity: 190.85926818847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/76
 38%|███▊      | 76/200 [25:28:14<40:53:27, 1187.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6519.592038332761
INFO:root:current train perplexity170.71337890625
INFO:root:current mean train loss 6511.657070619273
INFO:root:current train perplexity170.28184509277344
INFO:root:current mean train loss 6511.249947983784
INFO:root:current train perplexity170.32846069335938
INFO:root:current mean train loss 6508.752291550112
INFO:root:current train perplexity170.05838012695312
INFO:root:current mean train loss 6524.382884101324
INFO:root:current train perplexity170.2766571044922
INFO:root:current mean train loss 6521.058441730119
INFO:root:current train perplexity170.3766326904297
INFO:root:current mean train loss 6522.405014810963
INFO:root:current train perplexity170.1793212890625
INFO:root:current mean train loss 6515.850862486172
INFO:root:current train perplexity170.0883026123047
INFO:root:current mean train loss 6512.207012617494
INFO:root:current train perplexity169.9292449951172
INFO:root:current mean train loss 6513.816133778223
INFO:root:current train perplexity170.12115478515625
INFO:root:current mean train loss 6514.866449483702
INFO:root:current train perplexity170.13128662109375
INFO:root:current mean train loss 6516.347013817826
INFO:root:current train perplexity170.20504760742188
INFO:root:current mean train loss 6516.5595703125
INFO:root:current train perplexity170.28323364257812
INFO:root:current mean train loss 6514.7156949951695
INFO:root:current train perplexity170.30384826660156
INFO:root:current mean train loss 6515.471311593519
INFO:root:current train perplexity170.331787109375
INFO:root:current mean train loss 6514.319309543919
INFO:root:current train perplexity170.23695373535156
INFO:root:current mean train loss 6515.663377289234
INFO:root:current train perplexity170.1673126220703
INFO:root:current mean train loss 6515.924648197585
INFO:root:current train perplexity170.1298370361328
INFO:root:current mean train loss 6518.447079453249
INFO:root:current train perplexity170.14816284179688

100%|██████████| 1/1 [17:11<00:00, 1031.29s/it][A100%|██████████| 1/1 [17:11<00:00, 1031.29s/it]
INFO:root:final mean train loss: 6513.260502971547
INFO:root:final train perplexity: 170.16180419921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.45s/it][A100%|██████████| 1/1 [01:13<00:00, 73.45s/it]
INFO:root:eval mean loss: 6335.3161586463875
INFO:root:eval perplexity: 167.94415283203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 72.00s/it][A100%|██████████| 1/1 [01:11<00:00, 72.00s/it]
INFO:root:eval mean loss: 6446.211332280585
INFO:root:eval perplexity: 194.78350830078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/77
 38%|███▊      | 77/200 [25:47:53<40:28:41, 1184.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6584.5596923828125
INFO:root:current train perplexity170.23216247558594
INFO:root:current mean train loss 6531.397800021701
INFO:root:current train perplexity172.8738555908203
INFO:root:current mean train loss 6532.227111816406
INFO:root:current train perplexity171.84339904785156
INFO:root:current mean train loss 6510.045733563312
INFO:root:current train perplexity171.43344116210938
INFO:root:current mean train loss 6509.777802112056
INFO:root:current train perplexity170.6638946533203
INFO:root:current mean train loss 6520.151025967335
INFO:root:current train perplexity170.86436462402344
INFO:root:current mean train loss 6511.791319997687
INFO:root:current train perplexity171.17498779296875
INFO:root:current mean train loss 6512.047535007283
INFO:root:current train perplexity171.15899658203125
INFO:root:current mean train loss 6515.836065009089
INFO:root:current train perplexity171.2568817138672
INFO:root:current mean train loss 6513.9683767982515
INFO:root:current train perplexity170.9956817626953
INFO:root:current mean train loss 6519.132516043527
INFO:root:current train perplexity171.1703338623047
INFO:root:current mean train loss 6519.524638812894
INFO:root:current train perplexity171.0935516357422
INFO:root:current mean train loss 6523.631516210291
INFO:root:current train perplexity171.07699584960938
INFO:root:current mean train loss 6521.429050643875
INFO:root:current train perplexity171.12928771972656
INFO:root:current mean train loss 6521.606035405939
INFO:root:current train perplexity171.34925842285156
INFO:root:current mean train loss 6525.162660148479
INFO:root:current train perplexity171.68368530273438
INFO:root:current mean train loss 6525.887762724464
INFO:root:current train perplexity171.53237915039062
INFO:root:current mean train loss 6521.572552361589
INFO:root:current train perplexity171.35057067871094
INFO:root:current mean train loss 6523.121159916431
INFO:root:current train perplexity171.35317993164062
INFO:root:current mean train loss 6519.272936679033
INFO:root:current train perplexity171.12757873535156

100%|██████████| 1/1 [17:14<00:00, 1034.82s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.83s/it]
INFO:root:final mean train loss: 6520.455153964952
INFO:root:final train perplexity: 171.12994384765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.52s/it][A100%|██████████| 1/1 [01:14<00:00, 74.52s/it]
INFO:root:eval mean loss: 6300.703711976396
INFO:root:eval perplexity: 163.30819702148438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.57s/it][A100%|██████████| 1/1 [01:11<00:00, 71.57s/it]
INFO:root:eval mean loss: 6413.339822972074
INFO:root:eval perplexity: 189.61688232421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/78
 39%|███▉      | 78/200 [26:07:36<40:08:01, 1184.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6533.98884765625
INFO:root:current train perplexity171.32752990722656
INFO:root:current mean train loss 6516.7902265625
INFO:root:current train perplexity171.11093139648438
INFO:root:current mean train loss 6529.290162760417
INFO:root:current train perplexity170.9776611328125
INFO:root:current mean train loss 6527.216700721154
INFO:root:current train perplexity171.4944305419922
INFO:root:current mean train loss 6521.349549632353
INFO:root:current train perplexity171.4759521484375
INFO:root:current mean train loss 6525.742227492559
INFO:root:current train perplexity171.86093139648438
INFO:root:current mean train loss 6526.30420234375
INFO:root:current train perplexity171.76089477539062
INFO:root:current mean train loss 6525.219379040948
INFO:root:current train perplexity171.70169067382812
INFO:root:current mean train loss 6525.839562618371
INFO:root:current train perplexity171.4663848876953
INFO:root:current mean train loss 6524.347824113176
INFO:root:current train perplexity171.11753845214844
INFO:root:current mean train loss 6528.960611185214
INFO:root:current train perplexity171.06027221679688
INFO:root:current mean train loss 6533.773458333333
INFO:root:current train perplexity171.4015655517578
INFO:root:current mean train loss 6533.98829958546
INFO:root:current train perplexity171.46066284179688
INFO:root:current mean train loss 6531.389603847288
INFO:root:current train perplexity171.43360900878906
INFO:root:current mean train loss 6529.347360882675
INFO:root:current train perplexity171.3938751220703
INFO:root:current mean train loss 6527.470850089651
INFO:root:current train perplexity171.4065399169922
INFO:root:current mean train loss 6529.150620793269
INFO:root:current train perplexity171.49876403808594
INFO:root:current mean train loss 6528.750165874094
INFO:root:current train perplexity171.5265655517578
INFO:root:current mean train loss 6528.458074700343
INFO:root:current train perplexity171.5813751220703
INFO:root:current mean train loss 6524.481196986607
INFO:root:current train perplexity171.53614807128906

100%|██████████| 1/1 [17:12<00:00, 1032.73s/it][A100%|██████████| 1/1 [17:12<00:00, 1032.73s/it]
INFO:root:final mean train loss: 6523.5481692038575
INFO:root:final train perplexity: 171.5479278564453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.59s/it][A100%|██████████| 1/1 [01:13<00:00, 73.59s/it]
INFO:root:eval mean loss: 6307.664175047096
INFO:root:eval perplexity: 164.23007202148438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.51s/it][A100%|██████████| 1/1 [01:11<00:00, 71.51s/it]
INFO:root:eval mean loss: 6419.14773451352
INFO:root:eval perplexity: 190.5196533203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/79
 40%|███▉      | 79/200 [26:27:16<39:45:46, 1183.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6534.369524274553
INFO:root:current train perplexity170.6283416748047
INFO:root:current mean train loss 6534.58642578125
INFO:root:current train perplexity171.36143493652344
INFO:root:current mean train loss 6528.6033562274015
INFO:root:current train perplexity172.558837890625
INFO:root:current mean train loss 6532.951618752284
INFO:root:current train perplexity172.62557983398438
INFO:root:current mean train loss 6530.369557100184
INFO:root:current train perplexity172.3594207763672
INFO:root:current mean train loss 6538.512388109721
INFO:root:current train perplexity172.72850036621094
INFO:root:current mean train loss 6535.3184293467675
INFO:root:current train perplexity172.74374389648438
INFO:root:current mean train loss 6539.707093765794
INFO:root:current train perplexity173.09185791015625
INFO:root:current mean train loss 6548.95048468583
INFO:root:current train perplexity173.66644287109375
INFO:root:current mean train loss 6547.181577905221
INFO:root:current train perplexity173.6327362060547
INFO:root:current mean train loss 6540.310473304792
INFO:root:current train perplexity173.34864807128906
INFO:root:current mean train loss 6538.4925030442755
INFO:root:current train perplexity173.1599578857422
INFO:root:current mean train loss 6538.02095324137
INFO:root:current train perplexity173.0696563720703
INFO:root:current mean train loss 6541.709478841631
INFO:root:current train perplexity173.03887939453125
INFO:root:current mean train loss 6536.132505715803
INFO:root:current train perplexity172.55520629882812
INFO:root:current mean train loss 6535.804208085076
INFO:root:current train perplexity172.55636596679688
INFO:root:current mean train loss 6529.542200049006
INFO:root:current train perplexity172.39328002929688
INFO:root:current mean train loss 6531.15704212561
INFO:root:current train perplexity172.37823486328125
INFO:root:current mean train loss 6532.388662862209
INFO:root:current train perplexity172.2995147705078
INFO:root:current mean train loss 6531.683620401809
INFO:root:current train perplexity172.27313232421875

100%|██████████| 1/1 [17:16<00:00, 1036.48s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.48s/it]
INFO:root:final mean train loss: 6528.598741401523
INFO:root:final train perplexity: 172.23255920410156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.68s/it][A100%|██████████| 1/1 [01:13<00:00, 73.68s/it]
INFO:root:eval mean loss: 6319.826528562721
INFO:root:eval perplexity: 165.8534393310547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.37s/it][A100%|██████████| 1/1 [01:12<00:00, 72.37s/it]
INFO:root:eval mean loss: 6430.603362387799
INFO:root:eval perplexity: 192.3129119873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/80
 40%|████      | 80/200 [26:47:01<39:27:06, 1183.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6512.079258805615
INFO:root:current train perplexity173.32029724121094
INFO:root:current mean train loss 6487.543665855936
INFO:root:current train perplexity172.13050842285156
INFO:root:current mean train loss 6526.180831850265
INFO:root:current train perplexity173.83238220214844
INFO:root:current mean train loss 6530.580540564067
INFO:root:current train perplexity173.7740478515625
INFO:root:current mean train loss 6536.5850268927015
INFO:root:current train perplexity173.93287658691406
INFO:root:current mean train loss 6538.2536835098945
INFO:root:current train perplexity173.61990356445312
INFO:root:current mean train loss 6541.955911685556
INFO:root:current train perplexity173.9902801513672
INFO:root:current mean train loss 6537.940842056777
INFO:root:current train perplexity173.75714111328125
INFO:root:current mean train loss 6536.0499263314905
INFO:root:current train perplexity173.49957275390625
INFO:root:current mean train loss 6533.820038064553
INFO:root:current train perplexity173.70370483398438
INFO:root:current mean train loss 6528.834897308782
INFO:root:current train perplexity173.65542602539062
INFO:root:current mean train loss 6532.530585617315
INFO:root:current train perplexity173.69161987304688
INFO:root:current mean train loss 6527.604978141754
INFO:root:current train perplexity173.58688354492188
INFO:root:current mean train loss 6527.803921124793
INFO:root:current train perplexity173.3966064453125
INFO:root:current mean train loss 6532.573405171029
INFO:root:current train perplexity173.4400177001953
INFO:root:current mean train loss 6533.668188930705
INFO:root:current train perplexity173.54632568359375
INFO:root:current mean train loss 6537.545535832015
INFO:root:current train perplexity173.66900634765625
INFO:root:current mean train loss 6542.188353034839
INFO:root:current train perplexity173.84457397460938
INFO:root:current mean train loss 6542.416805437651
INFO:root:current train perplexity173.8085174560547
INFO:root:current mean train loss 6543.349596164737
INFO:root:current train perplexity173.826416015625

100%|██████████| 1/1 [17:17<00:00, 1037.29s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.29s/it]
INFO:root:final mean train loss: 6539.98370364406
INFO:root:final train perplexity: 173.78598022460938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.65s/it][A100%|██████████| 1/1 [01:13<00:00, 73.68s/it]
INFO:root:eval mean loss: 6349.735748074579
INFO:root:eval perplexity: 169.9141845703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.61s/it][A100%|██████████| 1/1 [01:11<00:00, 71.61s/it]
INFO:root:eval mean loss: 6458.758408999613
INFO:root:eval perplexity: 196.79257202148438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/81
 40%|████      | 81/200 [27:06:46<39:08:09, 1183.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6487.084954512747
INFO:root:current train perplexity174.82666015625
INFO:root:current mean train loss 6555.807508988814
INFO:root:current train perplexity175.2946319580078
INFO:root:current mean train loss 6527.139963343524
INFO:root:current train perplexity174.0360870361328
INFO:root:current mean train loss 6523.852929947224
INFO:root:current train perplexity173.9878692626953
INFO:root:current mean train loss 6520.892534015559
INFO:root:current train perplexity173.43563842773438
INFO:root:current mean train loss 6526.047804938422
INFO:root:current train perplexity173.64266967773438
INFO:root:current mean train loss 6530.629656007304
INFO:root:current train perplexity173.749267578125
INFO:root:current mean train loss 6532.380040119604
INFO:root:current train perplexity173.75704956054688
INFO:root:current mean train loss 6530.059632741153
INFO:root:current train perplexity173.78125
INFO:root:current mean train loss 6538.37155201396
INFO:root:current train perplexity173.95486450195312
INFO:root:current mean train loss 6540.093046620876
INFO:root:current train perplexity174.1557159423828
INFO:root:current mean train loss 6546.17022538996
INFO:root:current train perplexity174.42181396484375
INFO:root:current mean train loss 6545.856357299422
INFO:root:current train perplexity174.61770629882812
INFO:root:current mean train loss 6546.49460016295
INFO:root:current train perplexity174.6501922607422
INFO:root:current mean train loss 6547.074654101033
INFO:root:current train perplexity174.51907348632812
INFO:root:current mean train loss 6547.765367227157
INFO:root:current train perplexity174.64535522460938
INFO:root:current mean train loss 6553.666808645025
INFO:root:current train perplexity174.73507690429688
INFO:root:current mean train loss 6551.390072659329
INFO:root:current train perplexity174.6180419921875
INFO:root:current mean train loss 6547.38136457431
INFO:root:current train perplexity174.43362426757812
INFO:root:current mean train loss 6546.075381136134
INFO:root:current train perplexity174.33792114257812

100%|██████████| 1/1 [17:09<00:00, 1029.82s/it][A100%|██████████| 1/1 [17:09<00:00, 1029.82s/it]
INFO:root:final mean train loss: 6543.8242026216985
INFO:root:final train perplexity: 174.313232421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.00s/it][A100%|██████████| 1/1 [01:14<00:00, 74.00s/it]
INFO:root:eval mean loss: 6320.18006150266
INFO:root:eval perplexity: 165.90089416503906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.41s/it][A100%|██████████| 1/1 [01:11<00:00, 71.41s/it]
INFO:root:eval mean loss: 6426.964952834109
INFO:root:eval perplexity: 191.74154663085938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/82
 41%|████      | 82/200 [27:26:23<38:44:36, 1182.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6518.6694283434135
INFO:root:current train perplexity171.37818908691406
INFO:root:current mean train loss 6521.14261607432
INFO:root:current train perplexity170.4652099609375
INFO:root:current mean train loss 6503.136325458618
INFO:root:current train perplexity170.03616333007812
INFO:root:current mean train loss 6498.5727911796275
INFO:root:current train perplexity169.10751342773438
INFO:root:current mean train loss 6489.932668689782
INFO:root:current train perplexity168.48219299316406
INFO:root:current mean train loss 6486.13481008906
INFO:root:current train perplexity168.3709716796875
INFO:root:current mean train loss 6496.237559749278
INFO:root:current train perplexity168.55555725097656
INFO:root:current mean train loss 6500.610447002088
INFO:root:current train perplexity168.96983337402344
INFO:root:current mean train loss 6505.546992012528
INFO:root:current train perplexity169.13661193847656
INFO:root:current mean train loss 6506.181775357188
INFO:root:current train perplexity169.296142578125
INFO:root:current mean train loss 6511.843349725526
INFO:root:current train perplexity169.55401611328125
INFO:root:current mean train loss 6518.004412539946
INFO:root:current train perplexity169.76443481445312
INFO:root:current mean train loss 6520.771671681651
INFO:root:current train perplexity170.00851440429688
INFO:root:current mean train loss 6521.976769309718
INFO:root:current train perplexity170.2756805419922
INFO:root:current mean train loss 6523.505802468812
INFO:root:current train perplexity170.73471069335938
INFO:root:current mean train loss 6524.613044006002
INFO:root:current train perplexity171.0558624267578
INFO:root:current mean train loss 6526.288145927163
INFO:root:current train perplexity171.3944549560547
INFO:root:current mean train loss 6527.398420615763
INFO:root:current train perplexity171.6962127685547
INFO:root:current mean train loss 6530.301456022187
INFO:root:current train perplexity171.90921020507812

100%|██████████| 1/1 [17:13<00:00, 1033.84s/it][A100%|██████████| 1/1 [17:13<00:00, 1033.84s/it]
INFO:root:final mean train loss: 6526.91522558446
INFO:root:final train perplexity: 172.0040740966797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.37s/it][A100%|██████████| 1/1 [01:14<00:00, 74.37s/it]
INFO:root:eval mean loss: 6337.432296861148
INFO:root:eval perplexity: 168.2318878173828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.51s/it][A100%|██████████| 1/1 [01:11<00:00, 71.51s/it]
INFO:root:eval mean loss: 6447.870347476175
INFO:root:eval perplexity: 195.0480194091797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/83
 42%|████▏     | 83/200 [27:46:05<38:24:59, 1182.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6652.95634765625
INFO:root:current train perplexity173.90866088867188
INFO:root:current mean train loss 6544.865292080966
INFO:root:current train perplexity175.60467529296875
INFO:root:current mean train loss 6528.263527715774
INFO:root:current train perplexity174.9044647216797
INFO:root:current mean train loss 6533.005319115423
INFO:root:current train perplexity174.12722778320312
INFO:root:current mean train loss 6520.748387480945
INFO:root:current train perplexity173.4423370361328
INFO:root:current mean train loss 6512.725843481924
INFO:root:current train perplexity172.90673828125
INFO:root:current mean train loss 6523.576175877305
INFO:root:current train perplexity173.39999389648438
INFO:root:current mean train loss 6534.995891560299
INFO:root:current train perplexity173.97584533691406
INFO:root:current mean train loss 6543.150175419561
INFO:root:current train perplexity174.18585205078125
INFO:root:current mean train loss 6539.657123003949
INFO:root:current train perplexity174.21725463867188
INFO:root:current mean train loss 6543.775465075804
INFO:root:current train perplexity174.258056640625
INFO:root:current mean train loss 6550.708189928209
INFO:root:current train perplexity174.43836975097656
INFO:root:current mean train loss 6555.5491840457125
INFO:root:current train perplexity174.5995635986328
INFO:root:current mean train loss 6552.046274153149
INFO:root:current train perplexity174.44960021972656
INFO:root:current mean train loss 6547.989091589096
INFO:root:current train perplexity174.29519653320312
INFO:root:current mean train loss 6550.484674436051
INFO:root:current train perplexity174.4151611328125
INFO:root:current mean train loss 6551.17890625
INFO:root:current train perplexity174.36302185058594
INFO:root:current mean train loss 6549.115468521564
INFO:root:current train perplexity174.26370239257812
INFO:root:current mean train loss 6546.94467287854
INFO:root:current train perplexity174.0864715576172
INFO:root:current mean train loss 6543.119454300965
INFO:root:current train perplexity173.86264038085938

100%|██████████| 1/1 [17:23<00:00, 1043.15s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.15s/it]
INFO:root:final mean train loss: 6539.377454333515
INFO:root:final train perplexity: 173.702880859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.91s/it][A100%|██████████| 1/1 [01:16<00:00, 76.93s/it]
INFO:root:eval mean loss: 6301.9522852255095
INFO:root:eval perplexity: 163.47314453125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.04s/it][A100%|██████████| 1/1 [01:14<00:00, 74.05s/it]
INFO:root:eval mean loss: 6415.54037670379
INFO:root:eval perplexity: 189.9584197998047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/84
 42%|████▏     | 84/200 [28:06:02<38:13:38, 1186.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6609.50415943287
INFO:root:current train perplexity170.74578857421875
INFO:root:current mean train loss 6587.321888841043
INFO:root:current train perplexity172.66476440429688
INFO:root:current mean train loss 6548.33982654185
INFO:root:current train perplexity172.208740234375
INFO:root:current mean train loss 6554.792504360187
INFO:root:current train perplexity172.6567840576172
INFO:root:current mean train loss 6548.015223625951
INFO:root:current train perplexity172.87582397460938
INFO:root:current mean train loss 6543.183844839599
INFO:root:current train perplexity172.49334716796875
INFO:root:current mean train loss 6542.573797441936
INFO:root:current train perplexity172.4041290283203
INFO:root:current mean train loss 6536.41712517194
INFO:root:current train perplexity172.5226287841797
INFO:root:current mean train loss 6537.370996920345
INFO:root:current train perplexity173.19894409179688
INFO:root:current mean train loss 6544.408838891417
INFO:root:current train perplexity173.80694580078125
INFO:root:current mean train loss 6540.893309358265
INFO:root:current train perplexity173.87184143066406
INFO:root:current mean train loss 6539.053100910881
INFO:root:current train perplexity173.8810577392578
INFO:root:current mean train loss 6539.274944128209
INFO:root:current train perplexity173.9898681640625
INFO:root:current mean train loss 6545.513512180906
INFO:root:current train perplexity174.35952758789062
INFO:root:current mean train loss 6545.561830709093
INFO:root:current train perplexity174.37840270996094
INFO:root:current mean train loss 6546.924987273351
INFO:root:current train perplexity174.48770141601562
INFO:root:current mean train loss 6544.653055314133
INFO:root:current train perplexity174.69383239746094
INFO:root:current mean train loss 6548.04547433682
INFO:root:current train perplexity175.0272674560547
INFO:root:current mean train loss 6549.8218096820265
INFO:root:current train perplexity175.22134399414062
INFO:root:current mean train loss 6550.839177082657
INFO:root:current train perplexity175.2080535888672

100%|██████████| 1/1 [17:35<00:00, 1055.71s/it][A100%|██████████| 1/1 [17:35<00:00, 1055.71s/it]
INFO:root:final mean train loss: 6551.057121396125
INFO:root:final train perplexity: 175.3103485107422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.90s/it][A100%|██████████| 1/1 [01:14<00:00, 74.90s/it]
INFO:root:eval mean loss: 6377.401820838874
INFO:root:eval perplexity: 173.75880432128906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.35s/it][A100%|██████████| 1/1 [01:12<00:00, 72.35s/it]
INFO:root:eval mean loss: 6474.585601590204
INFO:root:eval perplexity: 199.3563690185547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/85
 42%|████▎     | 85/200 [28:26:07<38:04:47, 1192.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6529.018110795455
INFO:root:current train perplexity178.29563903808594
INFO:root:current mean train loss 6565.55268351237
INFO:root:current train perplexity178.84434509277344
INFO:root:current mean train loss 6559.022470943263
INFO:root:current train perplexity179.1654510498047
INFO:root:current mean train loss 6558.964346952216
INFO:root:current train perplexity178.96966552734375
INFO:root:current mean train loss 6573.218226527309
INFO:root:current train perplexity179.36932373046875
INFO:root:current mean train loss 6588.279463824104
INFO:root:current train perplexity179.70799255371094
INFO:root:current mean train loss 6586.851982543187
INFO:root:current train perplexity178.74705505371094
INFO:root:current mean train loss 6579.67004985194
INFO:root:current train perplexity178.13035583496094
INFO:root:current mean train loss 6583.887859615669
INFO:root:current train perplexity178.28543090820312
INFO:root:current mean train loss 6576.506237999867
INFO:root:current train perplexity177.84442138671875
INFO:root:current mean train loss 6575.922278627125
INFO:root:current train perplexity177.67608642578125
INFO:root:current mean train loss 6580.021126273628
INFO:root:current train perplexity178.0023193359375
INFO:root:current mean train loss 6578.317907597091
INFO:root:current train perplexity177.9938201904297
INFO:root:current mean train loss 6577.889952523367
INFO:root:current train perplexity177.955810546875
INFO:root:current mean train loss 6576.039122013504
INFO:root:current train perplexity178.0770263671875
INFO:root:current mean train loss 6575.072343104862
INFO:root:current train perplexity178.14088439941406
INFO:root:current mean train loss 6577.191412190161
INFO:root:current train perplexity178.3206329345703
INFO:root:current mean train loss 6577.493812771018
INFO:root:current train perplexity178.55419921875
INFO:root:current mean train loss 6578.73614409275
INFO:root:current train perplexity178.77792358398438
INFO:root:current mean train loss 6576.112352159288
INFO:root:current train perplexity178.7946319580078

100%|██████████| 1/1 [17:19<00:00, 1039.66s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.66s/it]
INFO:root:final mean train loss: 6576.683769560798
INFO:root:final train perplexity: 178.88954162597656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.22s/it][A100%|██████████| 1/1 [01:15<00:00, 75.22s/it]
INFO:root:eval mean loss: 6400.448243918994
INFO:root:eval perplexity: 177.0278778076172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.23s/it][A100%|██████████| 1/1 [01:12<00:00, 72.23s/it]
INFO:root:eval mean loss: 6489.026699634309
INFO:root:eval perplexity: 201.7247772216797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/86
 43%|████▎     | 86/200 [28:45:57<37:43:26, 1191.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6613.019171042521
INFO:root:current train perplexity179.49569702148438
INFO:root:current mean train loss 6614.010511694488
INFO:root:current train perplexity181.01446533203125
INFO:root:current mean train loss 6630.059757393438
INFO:root:current train perplexity181.4763946533203
INFO:root:current mean train loss 6622.383809351195
INFO:root:current train perplexity181.75750732421875
INFO:root:current mean train loss 6618.95230943262
INFO:root:current train perplexity181.6914825439453
INFO:root:current mean train loss 6616.507068328042
INFO:root:current train perplexity181.70387268066406
INFO:root:current mean train loss 6603.568077191282
INFO:root:current train perplexity181.07937622070312
INFO:root:current mean train loss 6601.160838303836
INFO:root:current train perplexity181.21714782714844
INFO:root:current mean train loss 6600.681327013465
INFO:root:current train perplexity181.27272033691406
INFO:root:current mean train loss 6603.86219849522
INFO:root:current train perplexity181.22483825683594
INFO:root:current mean train loss 6599.533856160903
INFO:root:current train perplexity181.08041381835938
INFO:root:current mean train loss 6597.0318001049745
INFO:root:current train perplexity180.917724609375
INFO:root:current mean train loss 6595.222539697536
INFO:root:current train perplexity180.79664611816406
INFO:root:current mean train loss 6598.585156106494
INFO:root:current train perplexity180.8116455078125
INFO:root:current mean train loss 6600.799253908924
INFO:root:current train perplexity180.89901733398438
INFO:root:current mean train loss 6598.619466875701
INFO:root:current train perplexity180.64231872558594
INFO:root:current mean train loss 6595.510802744958
INFO:root:current train perplexity180.48690795898438
INFO:root:current mean train loss 6590.806102711527
INFO:root:current train perplexity180.3828887939453
INFO:root:current mean train loss 6588.114027971353
INFO:root:current train perplexity180.17054748535156
INFO:root:current mean train loss 6586.047146903685
INFO:root:current train perplexity180.06173706054688

100%|██████████| 1/1 [17:21<00:00, 1041.66s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.66s/it]
INFO:root:final mean train loss: 6584.913740795788
INFO:root:final train perplexity: 180.05435180664062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.86s/it][A100%|██████████| 1/1 [01:14<00:00, 74.86s/it]
INFO:root:eval mean loss: 6395.450714760638
INFO:root:eval perplexity: 176.31375122070312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.21s/it][A100%|██████████| 1/1 [01:12<00:00, 72.21s/it]
INFO:root:eval mean loss: 6485.222899524878
INFO:root:eval perplexity: 201.0982208251953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/87
 44%|████▎     | 87/200 [29:05:48<37:23:26, 1191.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6560.95175405649
INFO:root:current train perplexity177.61061096191406
INFO:root:current mean train loss 6596.941998771067
INFO:root:current train perplexity178.69891357421875
INFO:root:current mean train loss 6608.0753621712
INFO:root:current train perplexity179.94981384277344
INFO:root:current mean train loss 6592.312847480572
INFO:root:current train perplexity179.4351043701172
INFO:root:current mean train loss 6591.710744434819
INFO:root:current train perplexity180.05255126953125
INFO:root:current mean train loss 6595.310647403493
INFO:root:current train perplexity180.09608459472656
INFO:root:current mean train loss 6592.458336214048
INFO:root:current train perplexity179.92604064941406
INFO:root:current mean train loss 6591.647691898297
INFO:root:current train perplexity179.991943359375
INFO:root:current mean train loss 6591.9258340822535
INFO:root:current train perplexity179.7913055419922
INFO:root:current mean train loss 6586.404853056301
INFO:root:current train perplexity179.6678924560547
INFO:root:current mean train loss 6588.598090177122
INFO:root:current train perplexity179.79208374023438
INFO:root:current mean train loss 6587.493942908399
INFO:root:current train perplexity179.61770629882812
INFO:root:current mean train loss 6593.715672452685
INFO:root:current train perplexity180.1477813720703
INFO:root:current mean train loss 6592.465250178588
INFO:root:current train perplexity180.3717041015625
INFO:root:current mean train loss 6595.156960948072
INFO:root:current train perplexity180.58995056152344
INFO:root:current mean train loss 6595.1154293161835
INFO:root:current train perplexity180.66928100585938
INFO:root:current mean train loss 6592.337168387776
INFO:root:current train perplexity180.61422729492188
INFO:root:current mean train loss 6592.235631404229
INFO:root:current train perplexity180.5709228515625
INFO:root:current mean train loss 6591.726383099541
INFO:root:current train perplexity180.6956329345703
INFO:root:current mean train loss 6590.773021794426
INFO:root:current train perplexity180.67478942871094

100%|██████████| 1/1 [17:19<00:00, 1039.20s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.20s/it]
INFO:root:final mean train loss: 6589.451617188485
INFO:root:final train perplexity: 180.699951171875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.87s/it][A100%|██████████| 1/1 [01:14<00:00, 74.87s/it]
INFO:root:eval mean loss: 6414.417175725842
INFO:root:eval perplexity: 179.03897094726562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.02s/it][A100%|██████████| 1/1 [01:12<00:00, 72.02s/it]
INFO:root:eval mean loss: 6499.632969200188
INFO:root:eval perplexity: 203.48219299316406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/88
 44%|████▍     | 88/200 [29:25:36<37:02:01, 1190.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6627.4273488898025
INFO:root:current train perplexity183.66050720214844
INFO:root:current mean train loss 6629.278049879807
INFO:root:current train perplexity183.59396362304688
INFO:root:current mean train loss 6607.998025357521
INFO:root:current train perplexity182.4082489013672
INFO:root:current mean train loss 6595.263615011867
INFO:root:current train perplexity181.96902465820312
INFO:root:current mean train loss 6592.967667889836
INFO:root:current train perplexity181.6915740966797
INFO:root:current mean train loss 6606.596956243435
INFO:root:current train perplexity181.76210021972656
INFO:root:current mean train loss 6606.694675977968
INFO:root:current train perplexity182.10546875
INFO:root:current mean train loss 6608.558151533019
INFO:root:current train perplexity182.28834533691406
INFO:root:current mean train loss 6607.955556586068
INFO:root:current train perplexity182.5338134765625
INFO:root:current mean train loss 6610.588747448179
INFO:root:current train perplexity182.7400360107422
INFO:root:current mean train loss 6608.481028824201
INFO:root:current train perplexity182.87008666992188
INFO:root:current mean train loss 6610.948585005884
INFO:root:current train perplexity183.0649871826172
INFO:root:current mean train loss 6610.078000196067
INFO:root:current train perplexity183.20672607421875
INFO:root:current mean train loss 6612.331421510977
INFO:root:current train perplexity183.2650909423828
INFO:root:current mean train loss 6614.810564511915
INFO:root:current train perplexity183.38031005859375
INFO:root:current mean train loss 6611.450983909679
INFO:root:current train perplexity183.31053161621094
INFO:root:current mean train loss 6607.6047298465155
INFO:root:current train perplexity183.22454833984375
INFO:root:current mean train loss 6609.9898002263235
INFO:root:current train perplexity183.36920166015625
INFO:root:current mean train loss 6607.893322528446
INFO:root:current train perplexity183.36456298828125

100%|██████████| 1/1 [17:23<00:00, 1043.81s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.81s/it]
INFO:root:final mean train loss: 6608.5600413573975
INFO:root:final train perplexity: 183.4438018798828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.18s/it][A100%|██████████| 1/1 [01:14<00:00, 74.18s/it]
INFO:root:eval mean loss: 6449.774796722629
INFO:root:eval perplexity: 184.23265075683594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.20s/it][A100%|██████████| 1/1 [01:12<00:00, 72.20s/it]
INFO:root:eval mean loss: 6540.785290440769
INFO:root:eval perplexity: 210.44705200195312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/89
 44%|████▍     | 89/200 [29:45:29<36:43:24, 1191.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6615.356038411458
INFO:root:current train perplexity181.47042846679688
INFO:root:current mean train loss 6585.0322265625
INFO:root:current train perplexity184.99606323242188
INFO:root:current mean train loss 6619.341013782429
INFO:root:current train perplexity185.82579040527344
INFO:root:current mean train loss 6633.764218061398
INFO:root:current train perplexity185.40687561035156
INFO:root:current mean train loss 6624.44314604824
INFO:root:current train perplexity185.40724182128906
INFO:root:current mean train loss 6622.083044052124
INFO:root:current train perplexity184.77574157714844
INFO:root:current mean train loss 6613.238428053513
INFO:root:current train perplexity183.7654571533203
INFO:root:current mean train loss 6614.733917579222
INFO:root:current train perplexity183.37417602539062
INFO:root:current mean train loss 6614.818777901785
INFO:root:current train perplexity183.17257690429688
INFO:root:current mean train loss 6614.733678985061
INFO:root:current train perplexity183.29751586914062
INFO:root:current mean train loss 6619.119643380991
INFO:root:current train perplexity183.61068725585938
INFO:root:current mean train loss 6613.317172921818
INFO:root:current train perplexity183.6371307373047
INFO:root:current mean train loss 6613.399676332379
INFO:root:current train perplexity183.43382263183594
INFO:root:current mean train loss 6607.752921499857
INFO:root:current train perplexity183.25057983398438
INFO:root:current mean train loss 6606.027344095808
INFO:root:current train perplexity183.0612335205078
INFO:root:current mean train loss 6605.9813235651245
INFO:root:current train perplexity183.20262145996094
INFO:root:current mean train loss 6605.845087018261
INFO:root:current train perplexity183.19239807128906
INFO:root:current mean train loss 6609.080531040085
INFO:root:current train perplexity183.42446899414062
INFO:root:current mean train loss 6608.82390616377
INFO:root:current train perplexity183.4451141357422
INFO:root:current mean train loss 6609.043294100582
INFO:root:current train perplexity183.23651123046875

100%|██████████| 1/1 [17:20<00:00, 1040.68s/it][A100%|██████████| 1/1 [17:20<00:00, 1040.68s/it]
INFO:root:final mean train loss: 6606.784122315069
INFO:root:final train perplexity: 183.1868896484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.12s/it][A100%|██████████| 1/1 [01:15<00:00, 75.12s/it]
INFO:root:eval mean loss: 6420.214042068374
INFO:root:eval perplexity: 179.88040161132812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.12s/it][A100%|██████████| 1/1 [01:12<00:00, 72.12s/it]
INFO:root:eval mean loss: 6506.137391435339
INFO:root:eval perplexity: 204.56752014160156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/90
 45%|████▌     | 90/200 [30:05:19<36:23:11, 1190.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6721.684705010776
INFO:root:current train perplexity178.88818359375
INFO:root:current mean train loss 6626.993247335271
INFO:root:current train perplexity178.2090301513672
INFO:root:current mean train loss 6615.529139089793
INFO:root:current train perplexity178.72430419921875
INFO:root:current mean train loss 6607.861102536094
INFO:root:current train perplexity179.73301696777344
INFO:root:current mean train loss 6605.058968212777
INFO:root:current train perplexity180.22193908691406
INFO:root:current mean train loss 6608.187084637878
INFO:root:current train perplexity180.61207580566406
INFO:root:current mean train loss 6612.427298104631
INFO:root:current train perplexity180.2852783203125
INFO:root:current mean train loss 6606.385680566273
INFO:root:current train perplexity180.12356567382812
INFO:root:current mean train loss 6604.164056609997
INFO:root:current train perplexity180.05332946777344
INFO:root:current mean train loss 6596.0106654500805
INFO:root:current train perplexity179.8660888671875
INFO:root:current mean train loss 6594.228960724915
INFO:root:current train perplexity179.90794372558594
INFO:root:current mean train loss 6589.359777215733
INFO:root:current train perplexity179.93136596679688
INFO:root:current mean train loss 6590.783990970174
INFO:root:current train perplexity180.05581665039062
INFO:root:current mean train loss 6593.097613263614
INFO:root:current train perplexity180.32508850097656
INFO:root:current mean train loss 6594.4999456705955
INFO:root:current train perplexity180.62689208984375
INFO:root:current mean train loss 6592.678285886916
INFO:root:current train perplexity180.80087280273438
INFO:root:current mean train loss 6593.9020817746705
INFO:root:current train perplexity180.99806213378906
INFO:root:current mean train loss 6597.421046136224
INFO:root:current train perplexity181.37596130371094
INFO:root:current mean train loss 6598.796482292663
INFO:root:current train perplexity181.56199645996094
INFO:root:current mean train loss 6598.335298102158
INFO:root:current train perplexity181.6249542236328

100%|██████████| 1/1 [17:21<00:00, 1041.33s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.33s/it]
INFO:root:final mean train loss: 6595.828966503364
INFO:root:final train perplexity: 181.61109924316406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.27s/it][A100%|██████████| 1/1 [01:14<00:00, 74.27s/it]
INFO:root:eval mean loss: 6419.383022010749
INFO:root:eval perplexity: 179.75950622558594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.76s/it][A100%|██████████| 1/1 [01:12<00:00, 72.76s/it]
INFO:root:eval mean loss: 6512.680973134143
INFO:root:eval perplexity: 205.66519165039062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/91
 46%|████▌     | 91/200 [30:25:10<36:03:16, 1190.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6615.153575067935
INFO:root:current train perplexity183.14881896972656
INFO:root:current mean train loss 6656.03846385381
INFO:root:current train perplexity184.64442443847656
INFO:root:current mean train loss 6613.901525978151
INFO:root:current train perplexity183.1975555419922
INFO:root:current mean train loss 6601.823016392702
INFO:root:current train perplexity182.55348205566406
INFO:root:current mean train loss 6589.755548451513
INFO:root:current train perplexity181.9666748046875
INFO:root:current mean train loss 6599.024475768372
INFO:root:current train perplexity181.8388214111328
INFO:root:current mean train loss 6593.460997968266
INFO:root:current train perplexity181.1500244140625
INFO:root:current mean train loss 6602.99657155873
INFO:root:current train perplexity181.39671325683594
INFO:root:current mean train loss 6600.893690321181
INFO:root:current train perplexity181.30099487304688
INFO:root:current mean train loss 6604.549263242436
INFO:root:current train perplexity181.3326416015625
INFO:root:current mean train loss 6597.564827971887
INFO:root:current train perplexity181.40338134765625
INFO:root:current mean train loss 6598.891370204106
INFO:root:current train perplexity181.5831298828125
INFO:root:current mean train loss 6605.135145747642
INFO:root:current train perplexity181.9080352783203
INFO:root:current mean train loss 6604.341648867014
INFO:root:current train perplexity182.0039825439453
INFO:root:current mean train loss 6595.058226019515
INFO:root:current train perplexity181.7664337158203
INFO:root:current mean train loss 6595.939171084148
INFO:root:current train perplexity181.8480987548828
INFO:root:current mean train loss 6598.406351453334
INFO:root:current train perplexity181.8744659423828
INFO:root:current mean train loss 6600.513374599531
INFO:root:current train perplexity181.8128204345703
INFO:root:current mean train loss 6600.452705490757
INFO:root:current train perplexity181.76947021484375
INFO:root:current mean train loss 6597.942665594087
INFO:root:current train perplexity181.6040802001953

100%|██████████| 1/1 [17:26<00:00, 1046.92s/it][A100%|██████████| 1/1 [17:26<00:00, 1046.92s/it]
INFO:root:final mean train loss: 6595.373354667014
INFO:root:final train perplexity: 181.5458221435547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.23s/it][A100%|██████████| 1/1 [01:14<00:00, 74.23s/it]
INFO:root:eval mean loss: 6399.193906527039
INFO:root:eval perplexity: 176.84825134277344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.79s/it][A100%|██████████| 1/1 [01:15<00:00, 75.79s/it]
INFO:root:eval mean loss: 6492.07600738309
INFO:root:eval perplexity: 202.22848510742188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/92
 46%|████▌     | 92/200 [30:45:09<35:48:04, 1193.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6485.6801525297615
INFO:root:current train perplexity178.41879272460938
INFO:root:current mean train loss 6525.134996285467
INFO:root:current train perplexity179.31407165527344
INFO:root:current mean train loss 6525.073039819986
INFO:root:current train perplexity178.10113525390625
INFO:root:current mean train loss 6559.522829502411
INFO:root:current train perplexity178.1382598876953
INFO:root:current mean train loss 6548.128730131277
INFO:root:current train perplexity177.3677978515625
INFO:root:current mean train loss 6552.051788167462
INFO:root:current train perplexity177.49630737304688
INFO:root:current mean train loss 6552.342373532947
INFO:root:current train perplexity177.16500854492188
INFO:root:current mean train loss 6547.262104639376
INFO:root:current train perplexity176.84100341796875
INFO:root:current mean train loss 6554.117113946625
INFO:root:current train perplexity176.84521484375
INFO:root:current mean train loss 6557.8476795739225
INFO:root:current train perplexity176.96153259277344
INFO:root:current mean train loss 6556.136769277693
INFO:root:current train perplexity176.87692260742188
INFO:root:current mean train loss 6553.410824225468
INFO:root:current train perplexity176.7432861328125
INFO:root:current mean train loss 6559.2534987690515
INFO:root:current train perplexity176.91041564941406
INFO:root:current mean train loss 6563.659825952632
INFO:root:current train perplexity177.03944396972656
INFO:root:current mean train loss 6564.178702927418
INFO:root:current train perplexity176.8870391845703
INFO:root:current mean train loss 6564.699789192458
INFO:root:current train perplexity176.85643005371094
INFO:root:current mean train loss 6562.068679415025
INFO:root:current train perplexity176.6618194580078
INFO:root:current mean train loss 6562.363849295856
INFO:root:current train perplexity176.70529174804688
INFO:root:current mean train loss 6561.898312219035
INFO:root:current train perplexity176.79908752441406
INFO:root:current mean train loss 6562.801781940509
INFO:root:current train perplexity176.76722717285156

100%|██████████| 1/1 [17:51<00:00, 1071.69s/it][A100%|██████████| 1/1 [17:51<00:00, 1071.69s/it]
INFO:root:final mean train loss: 6561.387470378097
INFO:root:final train perplexity: 176.744384765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:19<00:00, 79.55s/it][A100%|██████████| 1/1 [01:19<00:00, 79.55s/it]
INFO:root:eval mean loss: 6367.875995608932
INFO:root:eval perplexity: 172.4253387451172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.50s/it][A100%|██████████| 1/1 [01:16<00:00, 76.50s/it]
INFO:root:eval mean loss: 6480.132368371842
INFO:root:eval perplexity: 200.26272583007812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/93
 46%|████▋     | 93/200 [31:05:39<35:47:57, 1204.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6527.728631591797
INFO:root:current train perplexity175.7649383544922
INFO:root:current mean train loss 6542.767247178819
INFO:root:current train perplexity176.79470825195312
INFO:root:current mean train loss 6555.7141130719865
INFO:root:current train perplexity176.9573974609375
INFO:root:current mean train loss 6574.523380962171
INFO:root:current train perplexity177.4748992919922
INFO:root:current mean train loss 6576.212539672852
INFO:root:current train perplexity177.57521057128906
INFO:root:current mean train loss 6575.682485014817
INFO:root:current train perplexity177.61204528808594
INFO:root:current mean train loss 6575.600342514936
INFO:root:current train perplexity177.29550170898438
INFO:root:current mean train loss 6573.094357221555
INFO:root:current train perplexity177.33456420898438
INFO:root:current mean train loss 6568.375641424006
INFO:root:current train perplexity177.1091766357422
INFO:root:current mean train loss 6569.439229412468
INFO:root:current train perplexity177.04974365234375
INFO:root:current mean train loss 6566.043844943576
INFO:root:current train perplexity176.8345947265625
INFO:root:current mean train loss 6566.2978457693325
INFO:root:current train perplexity176.7469940185547
INFO:root:current mean train loss 6567.32915763855
INFO:root:current train perplexity176.74928283691406
INFO:root:current mean train loss 6565.5959561112995
INFO:root:current train perplexity176.52801513671875
INFO:root:current mean train loss 6566.234059926626
INFO:root:current train perplexity176.45773315429688
INFO:root:current mean train loss 6564.172724856606
INFO:root:current train perplexity176.371337890625
INFO:root:current mean train loss 6562.504976109096
INFO:root:current train perplexity176.25473022460938
INFO:root:current mean train loss 6563.418736832865
INFO:root:current train perplexity176.21617126464844
INFO:root:current mean train loss 6558.991753760804
INFO:root:current train perplexity176.02737426757812
INFO:root:current mean train loss 6556.737244022253
INFO:root:current train perplexity175.87098693847656

100%|██████████| 1/1 [17:54<00:00, 1074.56s/it][A100%|██████████| 1/1 [17:54<00:00, 1074.56s/it]
INFO:root:final mean train loss: 6555.216792689029
INFO:root:final train perplexity: 175.88650512695312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:20<00:00, 80.81s/it][A100%|██████████| 1/1 [01:20<00:00, 80.81s/it]
INFO:root:eval mean loss: 6349.324783216977
INFO:root:eval perplexity: 169.8577117919922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.51s/it][A100%|██████████| 1/1 [01:17<00:00, 77.51s/it]
INFO:root:eval mean loss: 6464.492920787622
INFO:root:eval perplexity: 197.71754455566406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/94
 47%|████▋     | 94/200 [31:26:15<35:44:15, 1213.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6566.49113543009
INFO:root:current train perplexity175.9007568359375
INFO:root:current mean train loss 6526.152586651332
INFO:root:current train perplexity174.90386962890625
INFO:root:current mean train loss 6533.368011166351
INFO:root:current train perplexity174.7117462158203
INFO:root:current mean train loss 6517.448986293687
INFO:root:current train perplexity174.33741760253906
INFO:root:current mean train loss 6516.782179404552
INFO:root:current train perplexity174.05044555664062
INFO:root:current mean train loss 6520.828808757328
INFO:root:current train perplexity174.3345947265625
INFO:root:current mean train loss 6521.854479577654
INFO:root:current train perplexity174.3409881591797
INFO:root:current mean train loss 6519.795926006705
INFO:root:current train perplexity174.1764678955078
INFO:root:current mean train loss 6523.241177187849
INFO:root:current train perplexity174.1066436767578
INFO:root:current mean train loss 6521.563444238967
INFO:root:current train perplexity173.9322967529297
INFO:root:current mean train loss 6526.888507630897
INFO:root:current train perplexity174.0888671875
INFO:root:current mean train loss 6528.098370927319
INFO:root:current train perplexity174.09152221679688
INFO:root:current mean train loss 6527.822689153455
INFO:root:current train perplexity173.97958374023438
INFO:root:current mean train loss 6525.498285598045
INFO:root:current train perplexity173.69178771972656
INFO:root:current mean train loss 6528.186905386294
INFO:root:current train perplexity173.3737030029297
INFO:root:current mean train loss 6532.0420961421805
INFO:root:current train perplexity173.21661376953125
INFO:root:current mean train loss 6530.861191164555
INFO:root:current train perplexity172.82992553710938
INFO:root:current mean train loss 6527.515083733306
INFO:root:current train perplexity172.3606414794922
INFO:root:current mean train loss 6525.8764671603185
INFO:root:current train perplexity171.8368377685547

100%|██████████| 1/1 [17:51<00:00, 1071.19s/it][A100%|██████████| 1/1 [17:51<00:00, 1071.19s/it]
INFO:root:final mean train loss: 6524.369422316251
INFO:root:final train perplexity: 171.65904235839844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.25s/it][A100%|██████████| 1/1 [01:14<00:00, 74.25s/it]
INFO:root:eval mean loss: 6332.397391677749
INFO:root:eval perplexity: 167.5482940673828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.79s/it][A100%|██████████| 1/1 [01:12<00:00, 72.79s/it]
INFO:root:eval mean loss: 6444.03975163453
INFO:root:eval perplexity: 194.43783569335938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/95
 48%|████▊     | 95/200 [31:46:35<35:27:36, 1215.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6482.067522321428
INFO:root:current train perplexity172.79241943359375
INFO:root:current mean train loss 6541.964745237116
INFO:root:current train perplexity170.42074584960938
INFO:root:current mean train loss 6518.483494268399
INFO:root:current train perplexity169.02076721191406
INFO:root:current mean train loss 6496.624037432823
INFO:root:current train perplexity167.5307159423828
INFO:root:current mean train loss 6484.785353213692
INFO:root:current train perplexity166.34234619140625
INFO:root:current mean train loss 6475.545431055448
INFO:root:current train perplexity165.7798309326172
INFO:root:current mean train loss 6472.645334448799
INFO:root:current train perplexity165.6624755859375
INFO:root:current mean train loss 6467.254516259629
INFO:root:current train perplexity165.1812286376953
INFO:root:current mean train loss 6462.055383330774
INFO:root:current train perplexity164.6568603515625
INFO:root:current mean train loss 6461.800953270309
INFO:root:current train perplexity164.4105224609375
INFO:root:current mean train loss 6470.540139704296
INFO:root:current train perplexity164.53451538085938
INFO:root:current mean train loss 6471.90293065179
INFO:root:current train perplexity164.563232421875
INFO:root:current mean train loss 6468.743377635271
INFO:root:current train perplexity164.27886962890625
INFO:root:current mean train loss 6475.813705467561
INFO:root:current train perplexity164.3282928466797
INFO:root:current mean train loss 6474.242520042322
INFO:root:current train perplexity164.3319091796875
INFO:root:current mean train loss 6477.57553394877
INFO:root:current train perplexity164.48524475097656
INFO:root:current mean train loss 6476.938845344931
INFO:root:current train perplexity164.52313232421875
INFO:root:current mean train loss 6475.764545596467
INFO:root:current train perplexity164.42996215820312
INFO:root:current mean train loss 6472.778309814722
INFO:root:current train perplexity164.3760986328125
INFO:root:current mean train loss 6472.501239071072
INFO:root:current train perplexity164.38841247558594

100%|██████████| 1/1 [17:22<00:00, 1042.53s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.53s/it]
INFO:root:final mean train loss: 6470.200872331812
INFO:root:final train perplexity: 164.4801483154297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.29s/it][A100%|██████████| 1/1 [01:14<00:00, 74.29s/it]
INFO:root:eval mean loss: 6293.026746384641
INFO:root:eval perplexity: 162.29745483398438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.87s/it][A100%|██████████| 1/1 [01:11<00:00, 71.87s/it]
INFO:root:eval mean loss: 6407.575089691379
INFO:root:eval perplexity: 188.72494506835938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/96
 48%|████▊     | 96/200 [32:06:26<34:54:28, 1208.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6430.995038432459
INFO:root:current train perplexity164.759521484375
INFO:root:current mean train loss 6475.22206733063
INFO:root:current train perplexity166.92564392089844
INFO:root:current mean train loss 6481.730358833875
INFO:root:current train perplexity166.02545166015625
INFO:root:current mean train loss 6472.479226656911
INFO:root:current train perplexity165.28231811523438
INFO:root:current mean train loss 6465.313012072216
INFO:root:current train perplexity165.21029663085938
INFO:root:current mean train loss 6468.738258261241
INFO:root:current train perplexity164.68930053710938
INFO:root:current mean train loss 6476.496894655062
INFO:root:current train perplexity164.63394165039062
INFO:root:current mean train loss 6482.681036786081
INFO:root:current train perplexity164.63409423828125
INFO:root:current mean train loss 6479.88303636902
INFO:root:current train perplexity164.38433837890625
INFO:root:current mean train loss 6474.312056823141
INFO:root:current train perplexity163.9273681640625
INFO:root:current mean train loss 6473.733283352783
INFO:root:current train perplexity163.68850708007812
INFO:root:current mean train loss 6472.156640711345
INFO:root:current train perplexity163.6256866455078
INFO:root:current mean train loss 6472.925594822553
INFO:root:current train perplexity163.68600463867188
INFO:root:current mean train loss 6471.354425420267
INFO:root:current train perplexity163.6157684326172
INFO:root:current mean train loss 6472.955170253537
INFO:root:current train perplexity163.66876220703125
INFO:root:current mean train loss 6471.769592165558
INFO:root:current train perplexity163.57957458496094
INFO:root:current mean train loss 6470.220897718999
INFO:root:current train perplexity163.5679473876953
INFO:root:current mean train loss 6466.940944484312
INFO:root:current train perplexity163.5904998779297
INFO:root:current mean train loss 6468.581521368105
INFO:root:current train perplexity163.70419311523438
INFO:root:current mean train loss 6468.190268612846
INFO:root:current train perplexity163.8152313232422

100%|██████████| 1/1 [17:14<00:00, 1034.58s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.58s/it]
INFO:root:final mean train loss: 6465.077641766539
INFO:root:final train perplexity: 163.81686401367188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.38s/it][A100%|██████████| 1/1 [01:14<00:00, 74.38s/it]
INFO:root:eval mean loss: 6240.384043592087
INFO:root:eval perplexity: 155.53268432617188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.01s/it][A100%|██████████| 1/1 [01:12<00:00, 72.01s/it]
INFO:root:eval mean loss: 6368.079568200077
INFO:root:eval perplexity: 182.72653198242188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/97
 48%|████▊     | 97/200 [32:26:10<34:21:24, 1200.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6501.609263102214
INFO:root:current train perplexity162.32028198242188
INFO:root:current mean train loss 6528.0684121621625
INFO:root:current train perplexity165.00631713867188
INFO:root:current mean train loss 6492.764522429436
INFO:root:current train perplexity164.1427764892578
INFO:root:current mean train loss 6492.771539096175
INFO:root:current train perplexity164.1772918701172
INFO:root:current mean train loss 6472.696159362793
INFO:root:current train perplexity163.4010467529297
INFO:root:current mean train loss 6469.623136868442
INFO:root:current train perplexity163.37184143066406
INFO:root:current mean train loss 6460.2427164713545
INFO:root:current train perplexity163.22552490234375
INFO:root:current mean train loss 6463.6676749979115
INFO:root:current train perplexity163.7932891845703
INFO:root:current mean train loss 6461.8477385898805
INFO:root:current train perplexity163.97177124023438
INFO:root:current mean train loss 6460.596417004549
INFO:root:current train perplexity163.98912048339844
INFO:root:current mean train loss 6460.460696154878
INFO:root:current train perplexity164.1602325439453
INFO:root:current mean train loss 6466.116170105618
INFO:root:current train perplexity164.39263916015625
INFO:root:current mean train loss 6471.557635576297
INFO:root:current train perplexity164.8404541015625
INFO:root:current mean train loss 6473.651782299008
INFO:root:current train perplexity165.17202758789062
INFO:root:current mean train loss 6473.648252708477
INFO:root:current train perplexity165.27626037597656
INFO:root:current mean train loss 6476.718222605782
INFO:root:current train perplexity165.4868927001953
INFO:root:current mean train loss 6477.873886849116
INFO:root:current train perplexity165.50677490234375
INFO:root:current mean train loss 6478.019494936177
INFO:root:current train perplexity165.6113739013672
INFO:root:current mean train loss 6481.59164256967
INFO:root:current train perplexity165.74473571777344
INFO:root:current mean train loss 6482.325868579152
INFO:root:current train perplexity165.7892303466797

100%|██████████| 1/1 [17:16<00:00, 1036.74s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.75s/it]
INFO:root:final mean train loss: 6479.753067455205
INFO:root:final train perplexity: 165.72386169433594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.41s/it][A100%|██████████| 1/1 [01:14<00:00, 74.41s/it]
INFO:root:eval mean loss: 6269.385179452017
INFO:root:eval perplexity: 159.22384643554688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.41s/it][A100%|██████████| 1/1 [01:11<00:00, 71.42s/it]
INFO:root:eval mean loss: 6388.176714525155
INFO:root:eval perplexity: 185.7545623779297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/98
 49%|████▉     | 98/200 [32:45:55<33:53:17, 1196.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6516.587184495193
INFO:root:current train perplexity167.49197387695312
INFO:root:current mean train loss 6500.044688091856
INFO:root:current train perplexity167.1560821533203
INFO:root:current mean train loss 6505.852581441627
INFO:root:current train perplexity167.28875732421875
INFO:root:current mean train loss 6518.68215432363
INFO:root:current train perplexity167.589599609375
INFO:root:current mean train loss 6522.864745043683
INFO:root:current train perplexity167.5085906982422
INFO:root:current mean train loss 6519.934080302821
INFO:root:current train perplexity167.35186767578125
INFO:root:current mean train loss 6514.535141564849
INFO:root:current train perplexity167.12307739257812
INFO:root:current mean train loss 6500.3963496987335
INFO:root:current train perplexity166.47105407714844
INFO:root:current mean train loss 6494.629479204299
INFO:root:current train perplexity166.23419189453125
INFO:root:current mean train loss 6497.6269586909
INFO:root:current train perplexity166.5158233642578
INFO:root:current mean train loss 6496.426025161385
INFO:root:current train perplexity166.40724182128906
INFO:root:current mean train loss 6491.05614102736
INFO:root:current train perplexity166.37034606933594
INFO:root:current mean train loss 6497.499128813612
INFO:root:current train perplexity166.90072631835938
INFO:root:current mean train loss 6494.413995964973
INFO:root:current train perplexity167.04627990722656
INFO:root:current mean train loss 6492.173113534556
INFO:root:current train perplexity166.84510803222656
INFO:root:current mean train loss 6492.9008698582265
INFO:root:current train perplexity166.862060546875
INFO:root:current mean train loss 6488.082969101914
INFO:root:current train perplexity166.42605590820312
INFO:root:current mean train loss 6485.295327992209
INFO:root:current train perplexity166.2414093017578
INFO:root:current mean train loss 6485.041239475117
INFO:root:current train perplexity166.09205627441406
INFO:root:current mean train loss 6481.512126769243
INFO:root:current train perplexity165.83880615234375

100%|██████████| 1/1 [17:18<00:00, 1038.83s/it][A100%|██████████| 1/1 [17:18<00:00, 1038.83s/it]
INFO:root:final mean train loss: 6480.884091314257
INFO:root:final train perplexity: 165.8717803955078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.94s/it][A100%|██████████| 1/1 [01:14<00:00, 74.98s/it]
INFO:root:eval mean loss: 6230.97792691711
INFO:root:eval perplexity: 154.3540496826172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.79s/it][A100%|██████████| 1/1 [01:11<00:00, 71.79s/it]
INFO:root:eval mean loss: 6359.887523894615
INFO:root:eval perplexity: 181.50634765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/99
 50%|████▉     | 99/200 [33:05:43<33:29:20, 1193.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6531.590736947409
INFO:root:current train perplexity162.91790771484375
INFO:root:current mean train loss 6454.530659769917
INFO:root:current train perplexity160.92605590820312
INFO:root:current mean train loss 6428.3672723431955
INFO:root:current train perplexity160.38185119628906
INFO:root:current mean train loss 6459.622200691263
INFO:root:current train perplexity161.90487670898438
INFO:root:current mean train loss 6465.55287012124
INFO:root:current train perplexity163.00393676757812
INFO:root:current mean train loss 6467.036251107442
INFO:root:current train perplexity162.99708557128906
INFO:root:current mean train loss 6467.978538535557
INFO:root:current train perplexity162.7125701904297
INFO:root:current mean train loss 6464.098953129996
INFO:root:current train perplexity162.7041015625
INFO:root:current mean train loss 6463.6305919828865
INFO:root:current train perplexity162.5501708984375
INFO:root:current mean train loss 6461.572048334871
INFO:root:current train perplexity162.43472290039062
INFO:root:current mean train loss 6462.250759498469
INFO:root:current train perplexity162.21336364746094
INFO:root:current mean train loss 6457.3788637009575
INFO:root:current train perplexity161.8979949951172
INFO:root:current mean train loss 6456.200763196543
INFO:root:current train perplexity162.11932373046875
INFO:root:current mean train loss 6456.320837526004
INFO:root:current train perplexity162.22805786132812
INFO:root:current mean train loss 6457.967262093033
INFO:root:current train perplexity162.38818359375
INFO:root:current mean train loss 6459.5753258706345
INFO:root:current train perplexity162.52769470214844
INFO:root:current mean train loss 6463.172535427969
INFO:root:current train perplexity163.04124450683594
INFO:root:current mean train loss 6464.991262450898
INFO:root:current train perplexity163.54891967773438
INFO:root:current mean train loss 6466.079411084036
INFO:root:current train perplexity163.73924255371094
INFO:root:current mean train loss 6464.6427978761985
INFO:root:current train perplexity163.52334594726562

100%|██████████| 1/1 [17:26<00:00, 1046.78s/it][A100%|██████████| 1/1 [17:26<00:00, 1046.78s/it]
INFO:root:final mean train loss: 6462.774191344199
INFO:root:final train perplexity: 163.51953125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.07s/it][A100%|██████████| 1/1 [01:15<00:00, 75.07s/it]
INFO:root:eval mean loss: 6231.400314439273
INFO:root:eval perplexity: 154.40675354003906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.39s/it][A100%|██████████| 1/1 [01:12<00:00, 72.39s/it]
INFO:root:eval mean loss: 6359.298257597795
INFO:root:eval perplexity: 181.4189453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/100
 50%|█████     | 100/200 [33:25:39<33:10:54, 1194.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6467.869120896465
INFO:root:current train perplexity165.9615020751953
INFO:root:current mean train loss 6500.726454538317
INFO:root:current train perplexity167.3860321044922
INFO:root:current mean train loss 6493.827775527801
INFO:root:current train perplexity167.63259887695312
INFO:root:current mean train loss 6480.571193609023
INFO:root:current train perplexity167.75326538085938
INFO:root:current mean train loss 6498.177019077217
INFO:root:current train perplexity168.27569580078125
INFO:root:current mean train loss 6505.604237042206
INFO:root:current train perplexity168.51161193847656
INFO:root:current mean train loss 6499.1517478931955
INFO:root:current train perplexity168.8771209716797
INFO:root:current mean train loss 6497.546929389276
INFO:root:current train perplexity169.2033233642578
INFO:root:current mean train loss 6501.294677462806
INFO:root:current train perplexity169.68617248535156
INFO:root:current mean train loss 6508.4681908470975
INFO:root:current train perplexity169.99278259277344
INFO:root:current mean train loss 6518.659000636232
INFO:root:current train perplexity170.3188018798828
INFO:root:current mean train loss 6520.348396205823
INFO:root:current train perplexity170.28428649902344
INFO:root:current mean train loss 6522.225079613525
INFO:root:current train perplexity170.41270446777344
INFO:root:current mean train loss 6523.049227405736
INFO:root:current train perplexity170.6548614501953
INFO:root:current mean train loss 6521.071657472169
INFO:root:current train perplexity170.7152557373047
INFO:root:current mean train loss 6519.87910779198
INFO:root:current train perplexity170.77232360839844
INFO:root:current mean train loss 6518.037964657519
INFO:root:current train perplexity170.671875
INFO:root:current mean train loss 6517.915038248246
INFO:root:current train perplexity170.81312561035156
INFO:root:current mean train loss 6519.116115801079
INFO:root:current train perplexity170.98800659179688

100%|██████████| 1/1 [17:21<00:00, 1041.65s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.65s/it]
INFO:root:final mean train loss: 6519.251305161253
INFO:root:final train perplexity: 170.96754455566406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.13s/it][A100%|██████████| 1/1 [01:15<00:00, 75.13s/it]
INFO:root:eval mean loss: 6291.007040253768
INFO:root:eval perplexity: 162.03253173828125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.24s/it][A100%|██████████| 1/1 [01:12<00:00, 72.24s/it]
INFO:root:eval mean loss: 6405.994186509585
INFO:root:eval perplexity: 188.48114013671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/101
 50%|█████     | 101/200 [33:45:31<32:49:26, 1193.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6457.000427246094
INFO:root:current train perplexity163.86741638183594
INFO:root:current mean train loss 6467.726432011045
INFO:root:current train perplexity169.19381713867188
INFO:root:current mean train loss 6498.765643084491
INFO:root:current train perplexity171.28219604492188
INFO:root:current mean train loss 6503.171320275415
INFO:root:current train perplexity171.45297241210938
INFO:root:current mean train loss 6531.517836350661
INFO:root:current train perplexity172.54171752929688
INFO:root:current mean train loss 6526.757094272348
INFO:root:current train perplexity171.8435516357422
INFO:root:current mean train loss 6539.845367035308
INFO:root:current train perplexity172.12615966796875
INFO:root:current mean train loss 6542.6588591676855
INFO:root:current train perplexity172.33114624023438
INFO:root:current mean train loss 6530.228516223384
INFO:root:current train perplexity171.69776916503906
INFO:root:current mean train loss 6528.125696173922
INFO:root:current train perplexity171.2541046142578
INFO:root:current mean train loss 6533.222221795029
INFO:root:current train perplexity171.37582397460938
INFO:root:current mean train loss 6535.078734038979
INFO:root:current train perplexity171.36331176757812
INFO:root:current mean train loss 6533.779570730109
INFO:root:current train perplexity171.3125
INFO:root:current mean train loss 6533.387140616095
INFO:root:current train perplexity171.53663635253906
INFO:root:current mean train loss 6526.770003669006
INFO:root:current train perplexity171.4860076904297
INFO:root:current mean train loss 6524.669983071199
INFO:root:current train perplexity171.43785095214844
INFO:root:current mean train loss 6523.967332292311
INFO:root:current train perplexity171.4224853515625
INFO:root:current mean train loss 6521.577842730187
INFO:root:current train perplexity171.2904510498047
INFO:root:current mean train loss 6521.213142025314
INFO:root:current train perplexity171.17279052734375
INFO:root:current mean train loss 6521.464166374445
INFO:root:current train perplexity171.16567993164062

100%|██████████| 1/1 [17:25<00:00, 1045.43s/it][A100%|██████████| 1/1 [17:25<00:00, 1045.43s/it]
INFO:root:final mean train loss: 6520.92245808119
INFO:root:final train perplexity: 171.19302368164062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.51s/it][A100%|██████████| 1/1 [01:15<00:00, 75.51s/it]
INFO:root:eval mean loss: 6304.79091519836
INFO:root:eval perplexity: 163.84890747070312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.74s/it][A100%|██████████| 1/1 [01:12<00:00, 72.74s/it]
INFO:root:eval mean loss: 6417.418316780253
INFO:root:eval perplexity: 190.25039672851562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/102
 51%|█████     | 102/200 [34:05:27<32:30:43, 1194.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6505.419907078598
INFO:root:current train perplexity173.11959838867188
INFO:root:current mean train loss 6492.409249441965
INFO:root:current train perplexity169.57852172851562
INFO:root:current mean train loss 6544.046944155713
INFO:root:current train perplexity171.167724609375
INFO:root:current mean train loss 6548.63866600976
INFO:root:current train perplexity171.6668243408203
INFO:root:current mean train loss 6542.668053325274
INFO:root:current train perplexity171.71896362304688
INFO:root:current mean train loss 6536.66679889042
INFO:root:current train perplexity171.45518493652344
INFO:root:current mean train loss 6526.03759765625
INFO:root:current train perplexity170.2275390625
INFO:root:current mean train loss 6504.368065473653
INFO:root:current train perplexity168.4553680419922
INFO:root:current mean train loss 6493.87292202037
INFO:root:current train perplexity167.58432006835938
INFO:root:current mean train loss 6491.791070052921
INFO:root:current train perplexity167.18540954589844
INFO:root:current mean train loss 6486.998510104066
INFO:root:current train perplexity167.1657257080078
INFO:root:current mean train loss 6488.448377078967
INFO:root:current train perplexity167.4193115234375
INFO:root:current mean train loss 6493.911945426551
INFO:root:current train perplexity167.7001495361328
INFO:root:current mean train loss 6493.337444468539
INFO:root:current train perplexity167.97386169433594
INFO:root:current mean train loss 6494.631285641464
INFO:root:current train perplexity168.31678771972656
INFO:root:current mean train loss 6500.679697373919
INFO:root:current train perplexity168.6666717529297
INFO:root:current mean train loss 6503.055687983198
INFO:root:current train perplexity168.74310302734375
INFO:root:current mean train loss 6500.975687087511
INFO:root:current train perplexity168.7093963623047
INFO:root:current mean train loss 6502.68602716474
INFO:root:current train perplexity168.69981384277344
INFO:root:current mean train loss 6503.003844614912
INFO:root:current train perplexity168.66908264160156

100%|██████████| 1/1 [17:26<00:00, 1046.85s/it][A100%|██████████| 1/1 [17:26<00:00, 1046.85s/it]
INFO:root:final mean train loss: 6502.516877959728
INFO:root:final train perplexity: 168.72604370117188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.58s/it][A100%|██████████| 1/1 [01:14<00:00, 74.58s/it]
INFO:root:eval mean loss: 6303.882104319038
INFO:root:eval perplexity: 163.72854614257812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.56s/it][A100%|██████████| 1/1 [01:12<00:00, 72.56s/it]
INFO:root:eval mean loss: 6419.468064328457
INFO:root:eval perplexity: 190.5695343017578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/103
 52%|█████▏    | 103/200 [34:25:23<32:11:50, 1194.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6507.845869140625
INFO:root:current train perplexity166.42454528808594
INFO:root:current mean train loss 6504.2576953125
INFO:root:current train perplexity167.2474365234375
INFO:root:current mean train loss 6499.326572265625
INFO:root:current train perplexity165.9768524169922
INFO:root:current mean train loss 6503.159089006696
INFO:root:current train perplexity166.23585510253906
INFO:root:current mean train loss 6488.754006076389
INFO:root:current train perplexity165.73501586914062
INFO:root:current mean train loss 6480.944025213068
INFO:root:current train perplexity165.5546112060547
INFO:root:current mean train loss 6473.994166917068
INFO:root:current train perplexity165.4763946533203
INFO:root:current mean train loss 6476.18930078125
INFO:root:current train perplexity165.4657440185547
INFO:root:current mean train loss 6482.9485897288605
INFO:root:current train perplexity165.6546630859375
INFO:root:current mean train loss 6477.674311780428
INFO:root:current train perplexity165.57986450195312
INFO:root:current mean train loss 6474.287743675595
INFO:root:current train perplexity165.65133666992188
INFO:root:current mean train loss 6477.492277513587
INFO:root:current train perplexity166.05641174316406
INFO:root:current mean train loss 6475.38237265625
INFO:root:current train perplexity166.22618103027344
INFO:root:current mean train loss 6477.012354962384
INFO:root:current train perplexity166.39891052246094
INFO:root:current mean train loss 6479.792833041487
INFO:root:current train perplexity166.6824188232422
INFO:root:current mean train loss 6485.528679435484
INFO:root:current train perplexity166.81138610839844
INFO:root:current mean train loss 6482.204469105113
INFO:root:current train perplexity166.60057067871094
INFO:root:current mean train loss 6481.151282366071
INFO:root:current train perplexity166.3966827392578
INFO:root:current mean train loss 6478.9401419974665
INFO:root:current train perplexity166.0016326904297
INFO:root:current mean train loss 6480.140768479568
INFO:root:current train perplexity165.78512573242188

100%|██████████| 1/1 [17:20<00:00, 1040.94s/it][A100%|██████████| 1/1 [17:20<00:00, 1040.94s/it]
INFO:root:final mean train loss: 6479.8828366308935
INFO:root:final train perplexity: 165.74070739746094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.27s/it][A100%|██████████| 1/1 [01:14<00:00, 74.27s/it]
INFO:root:eval mean loss: 6222.431521151928
INFO:root:eval perplexity: 153.2909393310547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.78s/it][A100%|██████████| 1/1 [01:12<00:00, 72.78s/it]
INFO:root:eval mean loss: 6353.759438372673
INFO:root:eval perplexity: 180.5990753173828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/104
 52%|█████▏    | 104/200 [34:45:14<31:49:45, 1193.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6405.759976970616
INFO:root:current train perplexity161.99290466308594
INFO:root:current mean train loss 6421.373728129678
INFO:root:current train perplexity161.5207061767578
INFO:root:current mean train loss 6428.184213702599
INFO:root:current train perplexity161.0990447998047
INFO:root:current mean train loss 6438.27305565608
INFO:root:current train perplexity160.9219970703125
INFO:root:current mean train loss 6431.7562378713865
INFO:root:current train perplexity160.8428192138672
INFO:root:current mean train loss 6431.782368654927
INFO:root:current train perplexity160.7764129638672
INFO:root:current mean train loss 6436.643700366613
INFO:root:current train perplexity160.66322326660156
INFO:root:current mean train loss 6445.037267891338
INFO:root:current train perplexity160.96397399902344
INFO:root:current mean train loss 6432.980460865412
INFO:root:current train perplexity160.71617126464844
INFO:root:current mean train loss 6433.6078546123645
INFO:root:current train perplexity160.8602294921875
INFO:root:current mean train loss 6433.053503177718
INFO:root:current train perplexity160.93519592285156
INFO:root:current mean train loss 6433.481602633623
INFO:root:current train perplexity161.03952026367188
INFO:root:current mean train loss 6437.420078340815
INFO:root:current train perplexity161.23406982421875
INFO:root:current mean train loss 6441.169732206131
INFO:root:current train perplexity161.41453552246094
INFO:root:current mean train loss 6444.631587636332
INFO:root:current train perplexity161.4698028564453
INFO:root:current mean train loss 6444.643804904375
INFO:root:current train perplexity161.56568908691406
INFO:root:current mean train loss 6446.055619247244
INFO:root:current train perplexity161.62071228027344
INFO:root:current mean train loss 6446.508002617431
INFO:root:current train perplexity161.729248046875
INFO:root:current mean train loss 6449.169731479312
INFO:root:current train perplexity161.71267700195312
INFO:root:current mean train loss 6450.626125504416
INFO:root:current train perplexity161.73843383789062

100%|██████████| 1/1 [17:29<00:00, 1049.31s/it][A100%|██████████| 1/1 [17:29<00:00, 1049.32s/it]
INFO:root:final mean train loss: 6448.63221439856
INFO:root:final train perplexity: 161.7058868408203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.60s/it][A100%|██████████| 1/1 [01:15<00:00, 75.60s/it]
INFO:root:eval mean loss: 6228.057700299202
INFO:root:eval perplexity: 153.98992919921875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.65s/it][A100%|██████████| 1/1 [01:12<00:00, 72.65s/it]
INFO:root:eval mean loss: 6359.471473639738
INFO:root:eval perplexity: 181.44464111328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/105
 52%|█████▎    | 105/200 [35:05:13<31:32:54, 1195.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6414.749157133557
INFO:root:current train perplexity162.09567260742188
INFO:root:current mean train loss 6441.451201065727
INFO:root:current train perplexity161.57286071777344
INFO:root:current mean train loss 6449.983744016836
INFO:root:current train perplexity161.46841430664062
INFO:root:current mean train loss 6448.444975535075
INFO:root:current train perplexity161.32420349121094
INFO:root:current mean train loss 6447.4271098996
INFO:root:current train perplexity161.38381958007812
INFO:root:current mean train loss 6451.378825984589
INFO:root:current train perplexity161.6446075439453
INFO:root:current mean train loss 6443.256528263204
INFO:root:current train perplexity161.17257690429688
INFO:root:current mean train loss 6442.919896962691
INFO:root:current train perplexity161.13230895996094
INFO:root:current mean train loss 6440.539129887231
INFO:root:current train perplexity161.1143341064453
INFO:root:current mean train loss 6445.791899890434
INFO:root:current train perplexity161.08314514160156
INFO:root:current mean train loss 6448.33799873155
INFO:root:current train perplexity161.13368225097656
INFO:root:current mean train loss 6445.408064558699
INFO:root:current train perplexity161.18463134765625
INFO:root:current mean train loss 6442.955628392109
INFO:root:current train perplexity161.09451293945312
INFO:root:current mean train loss 6440.825047134664
INFO:root:current train perplexity161.1197052001953
INFO:root:current mean train loss 6442.763367521795
INFO:root:current train perplexity161.19424438476562
INFO:root:current mean train loss 6443.001617739899
INFO:root:current train perplexity161.18080139160156
INFO:root:current mean train loss 6445.375874788914
INFO:root:current train perplexity161.23928833007812
INFO:root:current mean train loss 6446.341381124317
INFO:root:current train perplexity161.32635498046875
INFO:root:current mean train loss 6444.909479031897
INFO:root:current train perplexity161.37005615234375

100%|██████████| 1/1 [17:22<00:00, 1042.89s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.89s/it]
INFO:root:final mean train loss: 6446.634388271992
INFO:root:final train perplexity: 161.45132446289062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.87s/it][A100%|██████████| 1/1 [01:14<00:00, 74.87s/it]
INFO:root:eval mean loss: 6229.815848708999
INFO:root:eval perplexity: 154.20896911621094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.53s/it][A100%|██████████| 1/1 [01:13<00:00, 73.53s/it]
INFO:root:eval mean loss: 6358.853315637467
INFO:root:eval perplexity: 181.3529510498047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/106
 53%|█████▎    | 106/200 [35:25:07<31:12:07, 1194.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6272.470703125
INFO:root:current train perplexity151.71353149414062
INFO:root:current mean train loss 6404.128843401918
INFO:root:current train perplexity158.45750427246094
INFO:root:current mean train loss 6414.680950715174
INFO:root:current train perplexity159.329345703125
INFO:root:current mean train loss 6423.274389729548
INFO:root:current train perplexity160.5867156982422
INFO:root:current mean train loss 6423.894962301278
INFO:root:current train perplexity160.64047241210938
INFO:root:current mean train loss 6426.196799759855
INFO:root:current train perplexity160.64614868164062
INFO:root:current mean train loss 6425.932452260556
INFO:root:current train perplexity160.62844848632812
INFO:root:current mean train loss 6434.70567924728
INFO:root:current train perplexity161.0279998779297
INFO:root:current mean train loss 6441.581210132842
INFO:root:current train perplexity161.15843200683594
INFO:root:current mean train loss 6434.1636733924115
INFO:root:current train perplexity161.116943359375
INFO:root:current mean train loss 6427.843006602772
INFO:root:current train perplexity160.95130920410156
INFO:root:current mean train loss 6433.792192200982
INFO:root:current train perplexity161.2209930419922
INFO:root:current mean train loss 6434.191218011683
INFO:root:current train perplexity161.20401000976562
INFO:root:current mean train loss 6436.571437686155
INFO:root:current train perplexity161.36181640625
INFO:root:current mean train loss 6439.438980875825
INFO:root:current train perplexity161.4604949951172
INFO:root:current mean train loss 6442.979804479305
INFO:root:current train perplexity161.61846923828125
INFO:root:current mean train loss 6446.552451653752
INFO:root:current train perplexity161.7243194580078
INFO:root:current mean train loss 6447.85331181566
INFO:root:current train perplexity161.7803192138672
INFO:root:current mean train loss 6450.130450259839
INFO:root:current train perplexity161.8560028076172
INFO:root:current mean train loss 6451.046440401434
INFO:root:current train perplexity161.76280212402344

100%|██████████| 1/1 [17:29<00:00, 1049.66s/it][A100%|██████████| 1/1 [17:29<00:00, 1049.66s/it]
INFO:root:final mean train loss: 6449.106082733509
INFO:root:final train perplexity: 161.766357421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.23s/it][A100%|██████████| 1/1 [01:15<00:00, 75.23s/it]
INFO:root:eval mean loss: 6224.650075493129
INFO:root:eval perplexity: 153.566162109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.71s/it][A100%|██████████| 1/1 [01:12<00:00, 72.71s/it]
INFO:root:eval mean loss: 6355.075484471964
INFO:root:eval perplexity: 180.79354858398438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/107
 54%|█████▎    | 107/200 [35:45:07<30:54:30, 1196.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6336.896511501736
INFO:root:current train perplexity158.48976135253906
INFO:root:current mean train loss 6446.487548828125
INFO:root:current train perplexity161.89344787597656
INFO:root:current mean train loss 6458.244142864823
INFO:root:current train perplexity161.03106689453125
INFO:root:current mean train loss 6467.003758844339
INFO:root:current train perplexity161.81417846679688
INFO:root:current mean train loss 6469.345502205442
INFO:root:current train perplexity161.83169555664062
INFO:root:current mean train loss 6459.564087385376
INFO:root:current train perplexity161.51800537109375
INFO:root:current mean train loss 6450.5406656110945
INFO:root:current train perplexity161.55221557617188
INFO:root:current mean train loss 6448.21596992514
INFO:root:current train perplexity161.48658752441406
INFO:root:current mean train loss 6444.159990902926
INFO:root:current train perplexity161.4497833251953
INFO:root:current mean train loss 6448.081744557632
INFO:root:current train perplexity161.66510009765625
INFO:root:current mean train loss 6448.387279937684
INFO:root:current train perplexity161.6007537841797
INFO:root:current mean train loss 6445.110781756624
INFO:root:current train perplexity161.40159606933594
INFO:root:current mean train loss 6445.678752629823
INFO:root:current train perplexity161.44039916992188
INFO:root:current mean train loss 6443.80090943309
INFO:root:current train perplexity161.5177001953125
INFO:root:current mean train loss 6442.53112465841
INFO:root:current train perplexity161.55628967285156
INFO:root:current mean train loss 6441.352501428174
INFO:root:current train perplexity161.58009338378906
INFO:root:current mean train loss 6443.187337943738
INFO:root:current train perplexity161.62625122070312
INFO:root:current mean train loss 6446.935643508076
INFO:root:current train perplexity161.7360382080078
INFO:root:current mean train loss 6448.738549831545
INFO:root:current train perplexity161.86526489257812
INFO:root:current mean train loss 6454.040676526655
INFO:root:current train perplexity161.95228576660156

100%|██████████| 1/1 [17:21<00:00, 1041.91s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.91s/it]
INFO:root:final mean train loss: 6450.840660506887
INFO:root:final train perplexity: 161.98780822753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.33s/it][A100%|██████████| 1/1 [01:15<00:00, 75.36s/it]
INFO:root:eval mean loss: 6242.254306225066
INFO:root:eval perplexity: 155.7681884765625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.87s/it][A100%|██████████| 1/1 [01:12<00:00, 72.87s/it]
INFO:root:eval mean loss: 6369.2836775543
INFO:root:eval perplexity: 182.90663146972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/108
 54%|█████▍    | 108/200 [36:05:00<30:32:43, 1195.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6472.168987165179
INFO:root:current train perplexity163.60546875
INFO:root:current mean train loss 6415.570149739583
INFO:root:current train perplexity162.85491943359375
INFO:root:current mean train loss 6413.9878511469415
INFO:root:current train perplexity162.4851531982422
INFO:root:current mean train loss 6434.798886427238
INFO:root:current train perplexity162.5135955810547
INFO:root:current mean train loss 6437.265330908765
INFO:root:current train perplexity162.4989471435547
INFO:root:current mean train loss 6442.807168151285
INFO:root:current train perplexity162.6304931640625
INFO:root:current mean train loss 6449.16729976624
INFO:root:current train perplexity162.58738708496094
INFO:root:current mean train loss 6452.726938509779
INFO:root:current train perplexity162.76113891601562
INFO:root:current mean train loss 6456.607925944985
INFO:root:current train perplexity162.60250854492188
INFO:root:current mean train loss 6455.41300760361
INFO:root:current train perplexity162.628173828125
INFO:root:current mean train loss 6458.875031608544
INFO:root:current train perplexity162.68975830078125
INFO:root:current mean train loss 6455.742090273954
INFO:root:current train perplexity162.65647888183594
INFO:root:current mean train loss 6457.545704706477
INFO:root:current train perplexity162.58474731445312
INFO:root:current mean train loss 6457.933653002106
INFO:root:current train perplexity162.60203552246094
INFO:root:current mean train loss 6463.156794765353
INFO:root:current train perplexity162.76455688476562
INFO:root:current mean train loss 6459.553528030843
INFO:root:current train perplexity162.69091796875
INFO:root:current mean train loss 6460.171516927084
INFO:root:current train perplexity162.8112030029297
INFO:root:current mean train loss 6460.469149067903
INFO:root:current train perplexity162.88458251953125
INFO:root:current mean train loss 6461.563381567184
INFO:root:current train perplexity163.07965087890625
INFO:root:current mean train loss 6459.998368610707
INFO:root:current train perplexity163.107177734375

100%|██████████| 1/1 [17:24<00:00, 1044.90s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.90s/it]
INFO:root:final mean train loss: 6459.623905737834
INFO:root:final train perplexity: 163.11380004882812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.88s/it][A100%|██████████| 1/1 [01:14<00:00, 74.88s/it]
INFO:root:eval mean loss: 6252.40340688719
INFO:root:eval perplexity: 157.05197143554688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.35s/it][A100%|██████████| 1/1 [01:13<00:00, 73.35s/it]
INFO:root:eval mean loss: 6375.497188054078
INFO:root:eval perplexity: 183.83837890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/109
 55%|█████▍    | 109/200 [36:24:55<30:12:54, 1195.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6530.312509390024
INFO:root:current train perplexity164.8613739013672
INFO:root:current mean train loss 6479.454756887336
INFO:root:current train perplexity162.88023376464844
INFO:root:current mean train loss 6484.788609095982
INFO:root:current train perplexity162.7853546142578
INFO:root:current mean train loss 6474.21388799494
INFO:root:current train perplexity162.61451721191406
INFO:root:current mean train loss 6474.391269920146
INFO:root:current train perplexity162.49655151367188
INFO:root:current mean train loss 6471.86622155231
INFO:root:current train perplexity162.65345764160156
INFO:root:current mean train loss 6467.668454784557
INFO:root:current train perplexity162.48306274414062
INFO:root:current mean train loss 6454.867405668218
INFO:root:current train perplexity162.0232696533203
INFO:root:current mean train loss 6458.363998198173
INFO:root:current train perplexity161.95892333984375
INFO:root:current mean train loss 6455.591816365218
INFO:root:current train perplexity161.88587951660156
INFO:root:current mean train loss 6446.232917118435
INFO:root:current train perplexity161.6334228515625
INFO:root:current mean train loss 6445.158141665988
INFO:root:current train perplexity161.7004852294922
INFO:root:current mean train loss 6448.1217969218005
INFO:root:current train perplexity161.7892608642578
INFO:root:current mean train loss 6451.148269201877
INFO:root:current train perplexity161.8771514892578
INFO:root:current mean train loss 6455.000459697292
INFO:root:current train perplexity162.0784454345703
INFO:root:current mean train loss 6453.250989147068
INFO:root:current train perplexity161.93336486816406
INFO:root:current mean train loss 6455.137713342256
INFO:root:current train perplexity161.91946411132812
INFO:root:current mean train loss 6452.851691537796
INFO:root:current train perplexity161.7731475830078
INFO:root:current mean train loss 6454.41314974099
INFO:root:current train perplexity161.8548583984375
INFO:root:current mean train loss 6451.259861680328
INFO:root:current train perplexity161.83262634277344

100%|██████████| 1/1 [17:23<00:00, 1043.75s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.75s/it]
INFO:root:final mean train loss: 6449.8020523078985
INFO:root:final train perplexity: 161.85516357421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.98s/it][A100%|██████████| 1/1 [01:14<00:00, 74.98s/it]
INFO:root:eval mean loss: 6230.23710383422
INFO:root:eval perplexity: 154.26162719726562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.03s/it][A100%|██████████| 1/1 [01:13<00:00, 73.03s/it]
INFO:root:eval mean loss: 6357.947888097019
INFO:root:eval perplexity: 181.2187042236328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/110
 55%|█████▌    | 110/200 [36:44:49<29:52:25, 1194.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6458.293541949728
INFO:root:current train perplexity160.58717346191406
INFO:root:current mean train loss 6455.753663553994
INFO:root:current train perplexity160.92698669433594
INFO:root:current mean train loss 6462.42822265625
INFO:root:current train perplexity161.56768798828125
INFO:root:current mean train loss 6450.329518387957
INFO:root:current train perplexity161.7063446044922
INFO:root:current mean train loss 6437.622723089352
INFO:root:current train perplexity161.33973693847656
INFO:root:current mean train loss 6435.280080356162
INFO:root:current train perplexity161.4525604248047
INFO:root:current mean train loss 6438.910859112248
INFO:root:current train perplexity161.71298217773438
INFO:root:current mean train loss 6453.255281564938
INFO:root:current train perplexity162.10572814941406
INFO:root:current mean train loss 6448.634843165636
INFO:root:current train perplexity162.00201416015625
INFO:root:current mean train loss 6452.979550136255
INFO:root:current train perplexity162.24522399902344
INFO:root:current mean train loss 6457.439642682268
INFO:root:current train perplexity162.3826904296875
INFO:root:current mean train loss 6458.204157533148
INFO:root:current train perplexity162.2467041015625
INFO:root:current mean train loss 6453.299165958678
INFO:root:current train perplexity162.16270446777344
INFO:root:current mean train loss 6453.226381311632
INFO:root:current train perplexity162.15419006347656
INFO:root:current mean train loss 6454.116461227344
INFO:root:current train perplexity162.13494873046875
INFO:root:current mean train loss 6454.911720990679
INFO:root:current train perplexity162.29598999023438
INFO:root:current mean train loss 6451.326347995619
INFO:root:current train perplexity162.29962158203125
INFO:root:current mean train loss 6451.365748050187
INFO:root:current train perplexity162.23355102539062
INFO:root:current mean train loss 6454.844599071194
INFO:root:current train perplexity162.26890563964844
INFO:root:current mean train loss 6455.903826448625
INFO:root:current train perplexity162.35086059570312

100%|██████████| 1/1 [17:21<00:00, 1041.96s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.96s/it]
INFO:root:final mean train loss: 6453.852272883967
INFO:root:final train perplexity: 162.3729248046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.14s/it][A100%|██████████| 1/1 [01:15<00:00, 75.14s/it]
INFO:root:eval mean loss: 6252.496865996232
INFO:root:eval perplexity: 157.06387329101562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.16s/it][A100%|██████████| 1/1 [01:13<00:00, 73.16s/it]
INFO:root:eval mean loss: 6379.439797692265
INFO:root:eval perplexity: 184.43209838867188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/111
 56%|█████▌    | 111/200 [37:04:42<29:31:27, 1194.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6422.971549100654
INFO:root:current train perplexity162.2811279296875
INFO:root:current mean train loss 6435.510883946573
INFO:root:current train perplexity163.324951171875
INFO:root:current mean train loss 6458.29862837358
INFO:root:current train perplexity163.66493225097656
INFO:root:current mean train loss 6463.186719508986
INFO:root:current train perplexity163.63356018066406
INFO:root:current mean train loss 6454.046368634259
INFO:root:current train perplexity163.51251220703125
INFO:root:current mean train loss 6442.069239281143
INFO:root:current train perplexity163.2687225341797
INFO:root:current mean train loss 6446.834029217156
INFO:root:current train perplexity163.14195251464844
INFO:root:current mean train loss 6444.6536762732585
INFO:root:current train perplexity163.19509887695312
INFO:root:current mean train loss 6452.9418576070475
INFO:root:current train perplexity163.48179626464844
INFO:root:current mean train loss 6456.528624869264
INFO:root:current train perplexity163.58316040039062
INFO:root:current mean train loss 6453.5448094713975
INFO:root:current train perplexity163.5762176513672
INFO:root:current mean train loss 6448.65312887002
INFO:root:current train perplexity163.52755737304688
INFO:root:current mean train loss 6458.149054496136
INFO:root:current train perplexity163.7359619140625
INFO:root:current mean train loss 6454.7896244109625
INFO:root:current train perplexity163.66859436035156
INFO:root:current mean train loss 6460.507214141887
INFO:root:current train perplexity163.78858947753906
INFO:root:current mean train loss 6460.225437852203
INFO:root:current train perplexity163.729248046875
INFO:root:current mean train loss 6461.851615788108
INFO:root:current train perplexity163.831787109375
INFO:root:current mean train loss 6462.172032748198
INFO:root:current train perplexity163.9015655517578
INFO:root:current mean train loss 6464.812821551067
INFO:root:current train perplexity164.0038299560547

100%|██████████| 1/1 [17:22<00:00, 1042.00s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.00s/it]
INFO:root:final mean train loss: 6466.516096783598
INFO:root:final train perplexity: 164.0028839111328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.55s/it][A100%|██████████| 1/1 [01:14<00:00, 74.55s/it]
INFO:root:eval mean loss: 6255.860017384198
INFO:root:eval perplexity: 157.4916534423828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.06s/it][A100%|██████████| 1/1 [01:14<00:00, 74.06s/it]
INFO:root:eval mean loss: 6380.877955659907
INFO:root:eval perplexity: 184.6490936279297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/112
 56%|█████▌    | 112/200 [37:24:35<29:11:00, 1193.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5948.62939453125
INFO:root:current train perplexity144.97254943847656
INFO:root:current mean train loss 6492.31732118477
INFO:root:current train perplexity164.80508422851562
INFO:root:current mean train loss 6456.708385448737
INFO:root:current train perplexity164.035888671875
INFO:root:current mean train loss 6475.069173177083
INFO:root:current train perplexity164.10528564453125
INFO:root:current mean train loss 6465.943209134615
INFO:root:current train perplexity164.26828002929688
INFO:root:current mean train loss 6469.517323791625
INFO:root:current train perplexity164.3804168701172
INFO:root:current mean train loss 6472.835907539127
INFO:root:current train perplexity164.50344848632812
INFO:root:current mean train loss 6473.087676003512
INFO:root:current train perplexity164.434814453125
INFO:root:current mean train loss 6466.962210193221
INFO:root:current train perplexity164.0338592529297
INFO:root:current mean train loss 6462.835970484669
INFO:root:current train perplexity163.93798828125
INFO:root:current mean train loss 6468.21290279552
INFO:root:current train perplexity164.076416015625
INFO:root:current mean train loss 6472.51793050204
INFO:root:current train perplexity164.13345336914062
INFO:root:current mean train loss 6475.15278454255
INFO:root:current train perplexity164.28684997558594
INFO:root:current mean train loss 6471.572904924933
INFO:root:current train perplexity164.18519592285156
INFO:root:current mean train loss 6471.525749092347
INFO:root:current train perplexity164.2071990966797
INFO:root:current mean train loss 6468.203938802083
INFO:root:current train perplexity164.17103576660156
INFO:root:current mean train loss 6468.7479752929075
INFO:root:current train perplexity164.16078186035156
INFO:root:current mean train loss 6468.756838231247
INFO:root:current train perplexity164.21934509277344
INFO:root:current mean train loss 6471.555299002531
INFO:root:current train perplexity164.3242950439453
INFO:root:current mean train loss 6471.784300539034
INFO:root:current train perplexity164.38096618652344

100%|██████████| 1/1 [17:25<00:00, 1045.71s/it][A100%|██████████| 1/1 [17:25<00:00, 1045.72s/it]
INFO:root:final mean train loss: 6469.098391749795
INFO:root:final train perplexity: 164.337158203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.56s/it][A100%|██████████| 1/1 [01:15<00:00, 75.56s/it]
INFO:root:eval mean loss: 6252.540241647274
INFO:root:eval perplexity: 157.0694122314453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.08s/it][A100%|██████████| 1/1 [01:14<00:00, 74.08s/it]
INFO:root:eval mean loss: 6376.7860774046985
INFO:root:eval perplexity: 184.03221130371094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/113
 56%|█████▋    | 113/200 [37:44:33<28:52:48, 1195.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6434.9110107421875
INFO:root:current train perplexity165.1190185546875
INFO:root:current mean train loss 6489.051277669271
INFO:root:current train perplexity165.20880126953125
INFO:root:current mean train loss 6471.666510564631
INFO:root:current train perplexity165.64170837402344
INFO:root:current mean train loss 6484.150799560547
INFO:root:current train perplexity165.9469451904297
INFO:root:current mean train loss 6484.319760277158
INFO:root:current train perplexity165.6922607421875
INFO:root:current mean train loss 6475.534669846755
INFO:root:current train perplexity165.24267578125
INFO:root:current mean train loss 6466.836784116684
INFO:root:current train perplexity164.67649841308594
INFO:root:current mean train loss 6460.184503851997
INFO:root:current train perplexity164.2689971923828
INFO:root:current mean train loss 6464.8525134575075
INFO:root:current train perplexity164.21267700195312
INFO:root:current mean train loss 6470.195836340863
INFO:root:current train perplexity164.18011474609375
INFO:root:current mean train loss 6465.989455518536
INFO:root:current train perplexity164.02635192871094
INFO:root:current mean train loss 6467.765246146066
INFO:root:current train perplexity164.2666473388672
INFO:root:current mean train loss 6468.441482694032
INFO:root:current train perplexity164.35140991210938
INFO:root:current mean train loss 6467.198991625237
INFO:root:current train perplexity164.26820373535156
INFO:root:current mean train loss 6474.264216205435
INFO:root:current train perplexity164.4250946044922
INFO:root:current mean train loss 6475.973615144429
INFO:root:current train perplexity164.53489685058594
INFO:root:current mean train loss 6474.892248384453
INFO:root:current train perplexity164.4728546142578
INFO:root:current mean train loss 6472.957509595294
INFO:root:current train perplexity164.52987670898438
INFO:root:current mean train loss 6473.645604932177
INFO:root:current train perplexity164.5786895751953
INFO:root:current mean train loss 6473.467821248372
INFO:root:current train perplexity164.63975524902344

100%|██████████| 1/1 [17:36<00:00, 1056.62s/it][A100%|██████████| 1/1 [17:36<00:00, 1056.62s/it]
INFO:root:final mean train loss: 6471.232011157337
INFO:root:final train perplexity: 164.61392211914062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.67s/it][A100%|██████████| 1/1 [01:15<00:00, 75.67s/it]
INFO:root:eval mean loss: 6251.860434674202
INFO:root:eval perplexity: 156.9830780029297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.16s/it][A100%|██████████| 1/1 [01:13<00:00, 73.16s/it]
INFO:root:eval mean loss: 6377.318722988697
INFO:root:eval perplexity: 184.1123504638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/114
 57%|█████▋    | 114/200 [38:04:40<28:38:23, 1198.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6497.780444995777
INFO:root:current train perplexity164.97972106933594
INFO:root:current mean train loss 6530.601484089872
INFO:root:current train perplexity166.56617736816406
INFO:root:current mean train loss 6522.9828318664295
INFO:root:current train perplexity166.5568084716797
INFO:root:current mean train loss 6512.20252660191
INFO:root:current train perplexity165.86569213867188
INFO:root:current mean train loss 6514.461232479977
INFO:root:current train perplexity165.7456817626953
INFO:root:current mean train loss 6512.221825171671
INFO:root:current train perplexity166.46810913085938
INFO:root:current mean train loss 6506.863843884909
INFO:root:current train perplexity166.6119384765625
INFO:root:current mean train loss 6496.826839038119
INFO:root:current train perplexity166.6983184814453
INFO:root:current mean train loss 6493.896683887769
INFO:root:current train perplexity166.83517456054688
INFO:root:current mean train loss 6492.960836925527
INFO:root:current train perplexity167.0491485595703
INFO:root:current mean train loss 6491.724230333142
INFO:root:current train perplexity167.39329528808594
INFO:root:current mean train loss 6489.567377229689
INFO:root:current train perplexity167.65338134765625
INFO:root:current mean train loss 6493.8287127532585
INFO:root:current train perplexity168.03370666503906
INFO:root:current mean train loss 6496.2917821936935
INFO:root:current train perplexity168.140625
INFO:root:current mean train loss 6494.919795132546
INFO:root:current train perplexity168.2120819091797
INFO:root:current mean train loss 6498.899437253477
INFO:root:current train perplexity168.38076782226562
INFO:root:current mean train loss 6504.121785456914
INFO:root:current train perplexity168.7337646484375
INFO:root:current mean train loss 6503.818945762269
INFO:root:current train perplexity168.863037109375
INFO:root:current mean train loss 6504.008210408019
INFO:root:current train perplexity168.8747100830078
INFO:root:current mean train loss 6506.313360857237
INFO:root:current train perplexity169.00770568847656

100%|██████████| 1/1 [17:24<00:00, 1044.08s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.08s/it]
INFO:root:final mean train loss: 6504.707604112496
INFO:root:final train perplexity: 169.0177764892578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.19s/it][A100%|██████████| 1/1 [01:15<00:00, 75.19s/it]
INFO:root:eval mean loss: 6314.851271609043
INFO:root:eval perplexity: 165.1874542236328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.07s/it][A100%|██████████| 1/1 [01:13<00:00, 73.07s/it]
INFO:root:eval mean loss: 6418.291297858488
INFO:root:eval perplexity: 190.38624572753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/115
 57%|█████▊    | 115/200 [38:24:35<28:16:41, 1197.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6517.963387948495
INFO:root:current train perplexity170.6217498779297
INFO:root:current mean train loss 6564.904988078328
INFO:root:current train perplexity171.412353515625
INFO:root:current mean train loss 6540.803476408711
INFO:root:current train perplexity170.41636657714844
INFO:root:current mean train loss 6525.253816593839
INFO:root:current train perplexity170.15798950195312
INFO:root:current mean train loss 6524.19312276294
INFO:root:current train perplexity169.75933837890625
INFO:root:current mean train loss 6525.748345660819
INFO:root:current train perplexity170.09835815429688
INFO:root:current mean train loss 6521.689088780581
INFO:root:current train perplexity170.25392150878906
INFO:root:current mean train loss 6520.601846791073
INFO:root:current train perplexity170.0224609375
INFO:root:current mean train loss 6518.420510213883
INFO:root:current train perplexity169.87489318847656
INFO:root:current mean train loss 6515.27905426985
INFO:root:current train perplexity169.8971710205078
INFO:root:current mean train loss 6518.954610227407
INFO:root:current train perplexity170.0247344970703
INFO:root:current mean train loss 6516.169848675125
INFO:root:current train perplexity169.83358764648438
INFO:root:current mean train loss 6514.826415626246
INFO:root:current train perplexity169.5453643798828
INFO:root:current mean train loss 6514.066957639979
INFO:root:current train perplexity169.29290771484375
INFO:root:current mean train loss 6509.882578098134
INFO:root:current train perplexity169.07888793945312
INFO:root:current mean train loss 6504.5187755766365
INFO:root:current train perplexity168.7650604248047
INFO:root:current mean train loss 6500.075282104935
INFO:root:current train perplexity168.56568908691406
INFO:root:current mean train loss 6498.536485521932
INFO:root:current train perplexity168.40316772460938
INFO:root:current mean train loss 6500.552185256119
INFO:root:current train perplexity168.291259765625
INFO:root:current mean train loss 6501.1003797798585
INFO:root:current train perplexity168.14511108398438

100%|██████████| 1/1 [17:24<00:00, 1044.15s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.15s/it]
INFO:root:final mean train loss: 6497.531737788783
INFO:root:final train perplexity: 168.0639190673828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.61s/it][A100%|██████████| 1/1 [01:14<00:00, 74.61s/it]
INFO:root:eval mean loss: 6258.609506593529
INFO:root:eval perplexity: 157.84222412109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.09s/it][A100%|██████████| 1/1 [01:12<00:00, 72.09s/it]
INFO:root:eval mean loss: 6381.333552367299
INFO:root:eval perplexity: 184.71795654296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/116
 58%|█████▊    | 116/200 [38:44:28<27:54:49, 1196.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6527.803298305458
INFO:root:current train perplexity167.37588500976562
INFO:root:current mean train loss 6496.639459978071
INFO:root:current train perplexity165.7140655517578
INFO:root:current mean train loss 6506.753850394949
INFO:root:current train perplexity166.131103515625
INFO:root:current mean train loss 6503.039178318733
INFO:root:current train perplexity166.2949981689453
INFO:root:current mean train loss 6493.621869194533
INFO:root:current train perplexity165.78147888183594
INFO:root:current mean train loss 6479.1393328932245
INFO:root:current train perplexity165.32118225097656
INFO:root:current mean train loss 6485.4267745494135
INFO:root:current train perplexity165.5062255859375
INFO:root:current mean train loss 6478.313013613611
INFO:root:current train perplexity165.26222229003906
INFO:root:current mean train loss 6478.597988124282
INFO:root:current train perplexity165.38063049316406
INFO:root:current mean train loss 6487.276726232621
INFO:root:current train perplexity165.5773468017578
INFO:root:current mean train loss 6479.634523991888
INFO:root:current train perplexity165.401611328125
INFO:root:current mean train loss 6481.721464526847
INFO:root:current train perplexity165.3275604248047
INFO:root:current mean train loss 6479.796767047969
INFO:root:current train perplexity165.13516235351562
INFO:root:current mean train loss 6482.468948375387
INFO:root:current train perplexity165.1446990966797
INFO:root:current mean train loss 6482.767408836463
INFO:root:current train perplexity165.26608276367188
INFO:root:current mean train loss 6478.18266380888
INFO:root:current train perplexity165.1775360107422
INFO:root:current mean train loss 6477.034726118342
INFO:root:current train perplexity165.1122589111328
INFO:root:current mean train loss 6475.31777431977
INFO:root:current train perplexity165.13272094726562
INFO:root:current mean train loss 6478.326534367066
INFO:root:current train perplexity165.1890411376953
INFO:root:current mean train loss 6477.19432330313
INFO:root:current train perplexity165.1605224609375

100%|██████████| 1/1 [17:24<00:00, 1044.05s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.06s/it]
INFO:root:final mean train loss: 6475.583981789547
INFO:root:final train perplexity: 165.17990112304688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.73s/it][A100%|██████████| 1/1 [01:14<00:00, 74.73s/it]
INFO:root:eval mean loss: 6261.067865899268
INFO:root:eval perplexity: 158.15631103515625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.25s/it][A100%|██████████| 1/1 [01:12<00:00, 72.25s/it]
INFO:root:eval mean loss: 6387.360890922817
INFO:root:eval perplexity: 185.6307830810547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/117
 58%|█████▊    | 117/200 [39:04:22<27:33:39, 1195.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6486.409068714489
INFO:root:current train perplexity166.79212951660156
INFO:root:current mean train loss 6482.102141684674
INFO:root:current train perplexity165.9990234375
INFO:root:current mean train loss 6475.235343085395
INFO:root:current train perplexity166.03684997558594
INFO:root:current mean train loss 6475.48691959971
INFO:root:current train perplexity166.26036071777344
INFO:root:current mean train loss 6477.260890272797
INFO:root:current train perplexity166.38978576660156
INFO:root:current mean train loss 6471.698061988467
INFO:root:current train perplexity165.98745727539062
INFO:root:current mean train loss 6465.470631444177
INFO:root:current train perplexity165.6731414794922
INFO:root:current mean train loss 6460.354522550167
INFO:root:current train perplexity165.34498596191406
INFO:root:current mean train loss 6465.95980312588
INFO:root:current train perplexity165.5306854248047
INFO:root:current mean train loss 6475.580202172159
INFO:root:current train perplexity165.72512817382812
INFO:root:current mean train loss 6480.310558094698
INFO:root:current train perplexity165.9398956298828
INFO:root:current mean train loss 6480.630235460069
INFO:root:current train perplexity166.05213928222656
INFO:root:current mean train loss 6479.6577345569685
INFO:root:current train perplexity166.08413696289062
INFO:root:current mean train loss 6480.981565623874
INFO:root:current train perplexity166.08697509765625
INFO:root:current mean train loss 6488.332447010984
INFO:root:current train perplexity166.21588134765625
INFO:root:current mean train loss 6484.43128210111
INFO:root:current train perplexity166.04779052734375
INFO:root:current mean train loss 6483.660916441425
INFO:root:current train perplexity166.13973999023438
INFO:root:current mean train loss 6485.066566279537
INFO:root:current train perplexity166.16738891601562
INFO:root:current mean train loss 6485.644040899761
INFO:root:current train perplexity166.20550537109375

100%|██████████| 1/1 [17:16<00:00, 1036.66s/it][A100%|██████████| 1/1 [17:16<00:00, 1036.66s/it]
INFO:root:final mean train loss: 6484.219203685428
INFO:root:final train perplexity: 166.30856323242188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.59s/it][A100%|██████████| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 6275.630473251884
INFO:root:eval perplexity: 160.0301513671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.43s/it][A100%|██████████| 1/1 [01:12<00:00, 72.43s/it]
INFO:root:eval mean loss: 6399.057303787124
INFO:root:eval perplexity: 187.4149169921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/118
 59%|█████▉    | 118/200 [39:24:08<27:09:54, 1192.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6549.26435546875
INFO:root:current train perplexity173.6256103515625
INFO:root:current mean train loss 6531.912109375
INFO:root:current train perplexity167.6698455810547
INFO:root:current mean train loss 6496.217937785823
INFO:root:current train perplexity166.64060974121094
INFO:root:current mean train loss 6510.069681736681
INFO:root:current train perplexity167.2755126953125
INFO:root:current mean train loss 6500.903373360339
INFO:root:current train perplexity166.8890380859375
INFO:root:current mean train loss 6493.891858756188
INFO:root:current train perplexity166.8678741455078
INFO:root:current mean train loss 6488.861888236053
INFO:root:current train perplexity166.81393432617188
INFO:root:current mean train loss 6499.390675559619
INFO:root:current train perplexity167.2900390625
INFO:root:current mean train loss 6488.665492163238
INFO:root:current train perplexity167.0900115966797
INFO:root:current mean train loss 6490.597911451139
INFO:root:current train perplexity167.30686950683594
INFO:root:current mean train loss 6488.929042774409
INFO:root:current train perplexity167.3466796875
INFO:root:current mean train loss 6488.822908123586
INFO:root:current train perplexity167.3860321044922
INFO:root:current mean train loss 6494.721819080653
INFO:root:current train perplexity167.39974975585938
INFO:root:current mean train loss 6494.3299505358
INFO:root:current train perplexity167.5136260986328
INFO:root:current mean train loss 6497.245711813279
INFO:root:current train perplexity167.55397033691406
INFO:root:current mean train loss 6499.271877271076
INFO:root:current train perplexity167.6179656982422
INFO:root:current mean train loss 6499.922292092582
INFO:root:current train perplexity167.71334838867188
INFO:root:current mean train loss 6497.220586853922
INFO:root:current train perplexity167.71063232421875
INFO:root:current mean train loss 6497.529976411011
INFO:root:current train perplexity167.75238037109375
INFO:root:current mean train loss 6497.057510303888
INFO:root:current train perplexity167.74734497070312

100%|██████████| 1/1 [17:17<00:00, 1037.59s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.59s/it]
INFO:root:final mean train loss: 6494.2983066022125
INFO:root:final train perplexity: 167.63595581054688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.12s/it][A100%|██████████| 1/1 [01:14<00:00, 74.12s/it]
INFO:root:eval mean loss: 6282.633756164118
INFO:root:eval perplexity: 160.93902587890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.94s/it][A100%|██████████| 1/1 [01:11<00:00, 71.94s/it]
INFO:root:eval mean loss: 6405.900871114528
INFO:root:eval perplexity: 188.4667510986328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/119
 60%|█████▉    | 119/200 [39:43:54<26:47:21, 1190.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6542.822021484375
INFO:root:current train perplexity169.16815185546875
INFO:root:current mean train loss 6468.918108830687
INFO:root:current train perplexity167.39759826660156
INFO:root:current mean train loss 6488.741914766329
INFO:root:current train perplexity168.59286499023438
INFO:root:current mean train loss 6515.50982021545
INFO:root:current train perplexity169.06961059570312
INFO:root:current mean train loss 6522.899953254591
INFO:root:current train perplexity169.04075622558594
INFO:root:current mean train loss 6518.073269314236
INFO:root:current train perplexity169.25473022460938
INFO:root:current mean train loss 6506.751381631832
INFO:root:current train perplexity168.9014434814453
INFO:root:current mean train loss 6505.305292779389
INFO:root:current train perplexity168.69410705566406
INFO:root:current mean train loss 6513.166427872186
INFO:root:current train perplexity168.7657928466797
INFO:root:current mean train loss 6511.859533876763
INFO:root:current train perplexity168.43592834472656
INFO:root:current mean train loss 6504.239866491866
INFO:root:current train perplexity168.12010192871094
INFO:root:current mean train loss 6503.709840825535
INFO:root:current train perplexity167.91693115234375
INFO:root:current mean train loss 6500.0399571495245
INFO:root:current train perplexity167.76126098632812
INFO:root:current mean train loss 6498.000345342639
INFO:root:current train perplexity167.82615661621094
INFO:root:current mean train loss 6503.358434148647
INFO:root:current train perplexity168.3921661376953
INFO:root:current mean train loss 6506.320274643766
INFO:root:current train perplexity168.71043395996094
INFO:root:current mean train loss 6504.913094968596
INFO:root:current train perplexity168.9766845703125
INFO:root:current mean train loss 6509.402399893837
INFO:root:current train perplexity169.49476623535156
INFO:root:current mean train loss 6513.562039589908
INFO:root:current train perplexity169.9832305908203
INFO:root:current mean train loss 6515.836141755008
INFO:root:current train perplexity170.36671447753906

100%|██████████| 1/1 [17:21<00:00, 1041.95s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.95s/it]
INFO:root:final mean train loss: 6516.811942034638
INFO:root:final train perplexity: 170.638916015625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.13s/it][A100%|██████████| 1/1 [01:14<00:00, 74.13s/it]
INFO:root:eval mean loss: 6387.110891788564
INFO:root:eval perplexity: 175.1286163330078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.77s/it][A100%|██████████| 1/1 [01:11<00:00, 71.77s/it]
INFO:root:eval mean loss: 6486.761287608045
INFO:root:eval perplexity: 201.3514404296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/120
 60%|██████    | 120/200 [40:03:44<26:27:22, 1190.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6618.287071814904
INFO:root:current train perplexity176.50250244140625
INFO:root:current mean train loss 6578.010183649955
INFO:root:current train perplexity175.866455078125
INFO:root:current mean train loss 6556.482043916711
INFO:root:current train perplexity174.9958038330078
INFO:root:current mean train loss 6547.114953505255
INFO:root:current train perplexity173.76235961914062
INFO:root:current mean train loss 6549.113871858984
INFO:root:current train perplexity173.76402282714844
INFO:root:current mean train loss 6552.751908735795
INFO:root:current train perplexity174.210693359375
INFO:root:current mean train loss 6551.74863602186
INFO:root:current train perplexity174.30999755859375
INFO:root:current mean train loss 6553.000592676971
INFO:root:current train perplexity174.51107788085938
INFO:root:current mean train loss 6558.156223228918
INFO:root:current train perplexity174.91488647460938
INFO:root:current mean train loss 6548.630403333833
INFO:root:current train perplexity174.97203063964844
INFO:root:current mean train loss 6550.686149354849
INFO:root:current train perplexity175.39747619628906
INFO:root:current mean train loss 6554.958025817603
INFO:root:current train perplexity175.87283325195312
INFO:root:current mean train loss 6561.609870374924
INFO:root:current train perplexity176.08370971679688
INFO:root:current mean train loss 6558.181229287248
INFO:root:current train perplexity176.11351013183594
INFO:root:current mean train loss 6559.87754761336
INFO:root:current train perplexity176.2154083251953
INFO:root:current mean train loss 6563.851034242507
INFO:root:current train perplexity176.18373107910156
INFO:root:current mean train loss 6559.807142908214
INFO:root:current train perplexity175.93289184570312
INFO:root:current mean train loss 6560.362174685074
INFO:root:current train perplexity175.78848266601562
INFO:root:current mean train loss 6555.90438741546
INFO:root:current train perplexity175.43075561523438
INFO:root:current mean train loss 6553.001035740475
INFO:root:current train perplexity175.26930236816406

100%|██████████| 1/1 [17:22<00:00, 1042.32s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.32s/it]
INFO:root:final mean train loss: 6550.09279116632
INFO:root:final train perplexity: 175.17713928222656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.23s/it][A100%|██████████| 1/1 [01:14<00:00, 74.23s/it]
INFO:root:eval mean loss: 6313.142469040891
INFO:root:eval perplexity: 164.9593505859375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.34s/it][A100%|██████████| 1/1 [01:12<00:00, 72.34s/it]
INFO:root:eval mean loss: 6430.873639911624
INFO:root:eval perplexity: 192.35546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/121
 60%|██████    | 121/200 [40:23:35<26:07:50, 1190.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6560.296116420201
INFO:root:current train perplexity172.90872192382812
INFO:root:current mean train loss 6561.063063401442
INFO:root:current train perplexity173.2063751220703
INFO:root:current mean train loss 6548.341365814209
INFO:root:current train perplexity172.55101013183594
INFO:root:current mean train loss 6559.371352977967
INFO:root:current train perplexity172.412841796875
INFO:root:current mean train loss 6565.648224412349
INFO:root:current train perplexity172.16400146484375
INFO:root:current mean train loss 6562.054384519728
INFO:root:current train perplexity172.01858520507812
INFO:root:current mean train loss 6556.521253632336
INFO:root:current train perplexity172.7698516845703
INFO:root:current mean train loss 6552.187261026373
INFO:root:current train perplexity173.27420043945312
INFO:root:current mean train loss 6553.3874534535635
INFO:root:current train perplexity173.6300811767578
INFO:root:current mean train loss 6545.358781503334
INFO:root:current train perplexity173.9084930419922
INFO:root:current mean train loss 6542.741006099816
INFO:root:current train perplexity173.9127197265625
INFO:root:current mean train loss 6536.352822062879
INFO:root:current train perplexity173.20018005371094
INFO:root:current mean train loss 6530.467893564018
INFO:root:current train perplexity172.56944274902344
INFO:root:current mean train loss 6530.886195179987
INFO:root:current train perplexity172.4003448486328
INFO:root:current mean train loss 6525.763790927091
INFO:root:current train perplexity171.9342041015625
INFO:root:current mean train loss 6526.742372017593
INFO:root:current train perplexity171.8064422607422
INFO:root:current mean train loss 6521.66747693048
INFO:root:current train perplexity171.53500366210938
INFO:root:current mean train loss 6519.16557934181
INFO:root:current train perplexity171.222412109375
INFO:root:current mean train loss 6518.182679011904
INFO:root:current train perplexity171.06866455078125
INFO:root:current mean train loss 6519.153175026361
INFO:root:current train perplexity170.94073486328125

100%|██████████| 1/1 [17:25<00:00, 1045.03s/it][A100%|██████████| 1/1 [17:25<00:00, 1045.03s/it]
INFO:root:final mean train loss: 6518.960985392438
INFO:root:final train perplexity: 170.9284210205078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.44s/it][A100%|██████████| 1/1 [01:14<00:00, 74.44s/it]
INFO:root:eval mean loss: 6290.777454565603
INFO:root:eval perplexity: 162.0024871826172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.69s/it][A100%|██████████| 1/1 [01:12<00:00, 72.70s/it]
INFO:root:eval mean loss: 6405.50662123227
INFO:root:eval perplexity: 188.40602111816406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/122
 61%|██████    | 122/200 [40:43:30<25:49:27, 1191.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6477.750234107449
INFO:root:current train perplexity167.02540588378906
INFO:root:current mean train loss 6478.530180297146
INFO:root:current train perplexity166.96202087402344
INFO:root:current mean train loss 6488.27745106456
INFO:root:current train perplexity167.48646545410156
INFO:root:current mean train loss 6489.201734772956
INFO:root:current train perplexity167.8539276123047
INFO:root:current mean train loss 6514.623534123943
INFO:root:current train perplexity169.06622314453125
INFO:root:current mean train loss 6518.199384919012
INFO:root:current train perplexity169.23899841308594
INFO:root:current mean train loss 6510.6268863763
INFO:root:current train perplexity168.42291259765625
INFO:root:current mean train loss 6508.5653892605915
INFO:root:current train perplexity168.2014923095703
INFO:root:current mean train loss 6498.486617290414
INFO:root:current train perplexity167.8931427001953
INFO:root:current mean train loss 6500.468897036389
INFO:root:current train perplexity167.92140197753906
INFO:root:current mean train loss 6496.062010808627
INFO:root:current train perplexity167.88458251953125
INFO:root:current mean train loss 6495.450529574941
INFO:root:current train perplexity167.824462890625
INFO:root:current mean train loss 6489.588483620139
INFO:root:current train perplexity167.426025390625
INFO:root:current mean train loss 6486.450912975692
INFO:root:current train perplexity167.41323852539062
INFO:root:current mean train loss 6489.329382332506
INFO:root:current train perplexity167.49317932128906
INFO:root:current mean train loss 6493.567149381159
INFO:root:current train perplexity167.64378356933594
INFO:root:current mean train loss 6495.9754869970675
INFO:root:current train perplexity168.03274536132812
INFO:root:current mean train loss 6499.954254408576
INFO:root:current train perplexity168.28355407714844
INFO:root:current mean train loss 6502.98214475649
INFO:root:current train perplexity168.63475036621094
INFO:root:current mean train loss 6502.924574529587
INFO:root:current train perplexity168.6502685546875

100%|██████████| 1/1 [17:24<00:00, 1044.64s/it][A100%|██████████| 1/1 [17:24<00:00, 1044.64s/it]
INFO:root:final mean train loss: 6502.12729994504
INFO:root:final train perplexity: 168.67416381835938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.25s/it][A100%|██████████| 1/1 [01:15<00:00, 75.25s/it]
INFO:root:eval mean loss: 6289.095225232712
INFO:root:eval perplexity: 161.78224182128906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.12s/it][A100%|██████████| 1/1 [01:13<00:00, 73.12s/it]
INFO:root:eval mean loss: 6409.7135382036795
INFO:root:eval perplexity: 189.05540466308594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/123
 62%|██████▏   | 123/200 [41:03:25<25:30:53, 1192.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6482.13550889757
INFO:root:current train perplexity169.86378479003906
INFO:root:current mean train loss 6493.966609272204
INFO:root:current train perplexity169.683349609375
INFO:root:current mean train loss 6520.46877188847
INFO:root:current train perplexity171.46058654785156
INFO:root:current mean train loss 6532.5706943609775
INFO:root:current train perplexity173.03343200683594
INFO:root:current mean train loss 6544.7121263153695
INFO:root:current train perplexity174.40618896484375
INFO:root:current mean train loss 6559.316311076536
INFO:root:current train perplexity175.4257354736328
INFO:root:current mean train loss 6557.384929800724
INFO:root:current train perplexity176.05482482910156
INFO:root:current mean train loss 6555.921032560324
INFO:root:current train perplexity176.55393981933594
INFO:root:current mean train loss 6561.378002655372
INFO:root:current train perplexity177.05978393554688
INFO:root:current mean train loss 6567.867743351484
INFO:root:current train perplexity177.72158813476562
INFO:root:current mean train loss 6566.563675458716
INFO:root:current train perplexity177.9923858642578
INFO:root:current mean train loss 6574.165757533482
INFO:root:current train perplexity178.2203369140625
INFO:root:current mean train loss 6575.8208564226015
INFO:root:current train perplexity178.528564453125
INFO:root:current mean train loss 6574.249277062725
INFO:root:current train perplexity178.6939697265625
INFO:root:current mean train loss 6577.663126900692
INFO:root:current train perplexity179.00721740722656
INFO:root:current mean train loss 6576.890315141018
INFO:root:current train perplexity179.02120971679688
INFO:root:current mean train loss 6575.597311274963
INFO:root:current train perplexity179.0459747314453
INFO:root:current mean train loss 6577.490130444745
INFO:root:current train perplexity179.17723083496094
INFO:root:current mean train loss 6579.799505260004
INFO:root:current train perplexity179.17271423339844

100%|██████████| 1/1 [17:22<00:00, 1042.54s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.54s/it]
INFO:root:final mean train loss: 6579.172761071648
INFO:root:final train perplexity: 179.24107360839844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.93s/it][A100%|██████████| 1/1 [01:14<00:00, 74.93s/it]
INFO:root:eval mean loss: 6398.652958430297
INFO:root:eval perplexity: 176.77093505859375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.59s/it][A100%|██████████| 1/1 [01:12<00:00, 72.59s/it]
INFO:root:eval mean loss: 6493.747889309065
INFO:root:eval perplexity: 202.50523376464844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/124
 62%|██████▏   | 124/200 [41:23:18<25:10:50, 1192.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6528.113839285715
INFO:root:current train perplexity172.848876953125
INFO:root:current mean train loss 6630.3474098276865
INFO:root:current train perplexity179.2303924560547
INFO:root:current mean train loss 6579.737915628774
INFO:root:current train perplexity174.7364044189453
INFO:root:current mean train loss 6538.438121882635
INFO:root:current train perplexity172.12451171875
INFO:root:current mean train loss 6511.496183728117
INFO:root:current train perplexity170.52520751953125
INFO:root:current mean train loss 6512.540620762451
INFO:root:current train perplexity169.95567321777344
INFO:root:current mean train loss 6500.650426019358
INFO:root:current train perplexity169.37405395507812
INFO:root:current mean train loss 6493.383165416151
INFO:root:current train perplexity169.00746154785156
INFO:root:current mean train loss 6498.317431217085
INFO:root:current train perplexity168.92979431152344
INFO:root:current mean train loss 6494.779163903149
INFO:root:current train perplexity168.6033172607422
INFO:root:current mean train loss 6495.600185420804
INFO:root:current train perplexity168.4711151123047
INFO:root:current mean train loss 6500.43204906984
INFO:root:current train perplexity168.4619598388672
INFO:root:current mean train loss 6502.193766748006
INFO:root:current train perplexity168.40115356445312
INFO:root:current mean train loss 6502.865797374116
INFO:root:current train perplexity168.337646484375
INFO:root:current mean train loss 6501.058756163379
INFO:root:current train perplexity168.1468048095703
INFO:root:current mean train loss 6499.983784331972
INFO:root:current train perplexity168.06150817871094
INFO:root:current mean train loss 6500.528042292995
INFO:root:current train perplexity168.01182556152344
INFO:root:current mean train loss 6499.928054174905
INFO:root:current train perplexity167.84439086914062
INFO:root:current mean train loss 6496.641723970583
INFO:root:current train perplexity167.7117462158203
INFO:root:current mean train loss 6495.024026151596
INFO:root:current train perplexity167.6092529296875

100%|██████████| 1/1 [17:20<00:00, 1040.87s/it][A100%|██████████| 1/1 [17:20<00:00, 1040.87s/it]
INFO:root:final mean train loss: 6493.5041825241115
INFO:root:final train perplexity: 167.53086853027344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.58s/it][A100%|██████████| 1/1 [01:15<00:00, 75.58s/it]
INFO:root:eval mean loss: 6280.9752050088655
INFO:root:eval perplexity: 160.72329711914062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.81s/it][A100%|██████████| 1/1 [01:12<00:00, 72.81s/it]
INFO:root:eval mean loss: 6400.263731611536
INFO:root:eval perplexity: 187.60000610351562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/125
 62%|██████▎   | 125/200 [41:43:09<24:50:32, 1192.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6458.417724609375
INFO:root:current train perplexity168.6680450439453
INFO:root:current mean train loss 6469.865289503528
INFO:root:current train perplexity167.2700958251953
INFO:root:current mean train loss 6480.489327566965
INFO:root:current train perplexity166.95343017578125
INFO:root:current mean train loss 6489.664368429301
INFO:root:current train perplexity166.88990783691406
INFO:root:current mean train loss 6494.950918521521
INFO:root:current train perplexity166.64459228515625
INFO:root:current mean train loss 6490.837434957955
INFO:root:current train perplexity166.8368377685547
INFO:root:current mean train loss 6496.726811335637
INFO:root:current train perplexity167.06149291992188
INFO:root:current mean train loss 6487.148688384841
INFO:root:current train perplexity166.57872009277344
INFO:root:current mean train loss 6496.476186807873
INFO:root:current train perplexity166.5615692138672
INFO:root:current mean train loss 6493.713346671232
INFO:root:current train perplexity166.38311767578125
INFO:root:current mean train loss 6498.32882642746
INFO:root:current train perplexity166.48724365234375
INFO:root:current mean train loss 6497.026893697175
INFO:root:current train perplexity166.54354858398438
INFO:root:current mean train loss 6494.048491035412
INFO:root:current train perplexity166.5901641845703
INFO:root:current mean train loss 6499.039135152119
INFO:root:current train perplexity166.73074340820312
INFO:root:current mean train loss 6494.85685232784
INFO:root:current train perplexity166.77511596679688
INFO:root:current mean train loss 6492.457303264949
INFO:root:current train perplexity166.86300659179688
INFO:root:current mean train loss 6493.5103958205045
INFO:root:current train perplexity166.94554138183594
INFO:root:current mean train loss 6497.927849081442
INFO:root:current train perplexity167.109375
INFO:root:current mean train loss 6499.0353289152445
INFO:root:current train perplexity167.25270080566406
INFO:root:current mean train loss 6495.384757757683
INFO:root:current train perplexity167.15536499023438

100%|██████████| 1/1 [17:22<00:00, 1042.67s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.67s/it]
INFO:root:final mean train loss: 6490.446143907787
INFO:root:final train perplexity: 167.12745666503906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.10s/it][A100%|██████████| 1/1 [01:15<00:00, 75.10s/it]
INFO:root:eval mean loss: 6293.860567999224
INFO:root:eval perplexity: 162.40684509277344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.80s/it][A100%|██████████| 1/1 [01:12<00:00, 72.81s/it]
INFO:root:eval mean loss: 6411.743390888187
INFO:root:eval perplexity: 189.369384765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/126
 63%|██████▎   | 126/200 [42:03:02<24:30:49, 1192.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6421.673363662348
INFO:root:current train perplexity166.4481964111328
INFO:root:current mean train loss 6468.406153036348
INFO:root:current train perplexity167.0655517578125
INFO:root:current mean train loss 6491.174573716286
INFO:root:current train perplexity168.855224609375
INFO:root:current mean train loss 6514.82869633202
INFO:root:current train perplexity170.81011962890625
INFO:root:current mean train loss 6508.105465428359
INFO:root:current train perplexity171.41586303710938
INFO:root:current mean train loss 6520.1040995768835
INFO:root:current train perplexity171.88485717773438
INFO:root:current mean train loss 6526.565581275595
INFO:root:current train perplexity172.5174560546875
INFO:root:current mean train loss 6525.445702597841
INFO:root:current train perplexity172.50140380859375
INFO:root:current mean train loss 6531.041952126375
INFO:root:current train perplexity172.9407958984375
INFO:root:current mean train loss 6531.115186636557
INFO:root:current train perplexity173.2664337158203
INFO:root:current mean train loss 6540.492570244956
INFO:root:current train perplexity173.87425231933594
INFO:root:current mean train loss 6548.430517706507
INFO:root:current train perplexity174.3926239013672
INFO:root:current mean train loss 6554.591548603067
INFO:root:current train perplexity174.6715087890625
INFO:root:current mean train loss 6551.9254353385995
INFO:root:current train perplexity174.87835693359375
INFO:root:current mean train loss 6552.370383522727
INFO:root:current train perplexity174.64443969726562
INFO:root:current mean train loss 6553.576109453581
INFO:root:current train perplexity174.69924926757812
INFO:root:current mean train loss 6553.665183672303
INFO:root:current train perplexity174.9659423828125
INFO:root:current mean train loss 6552.491235057079
INFO:root:current train perplexity174.7936553955078
INFO:root:current mean train loss 6549.101223541044
INFO:root:current train perplexity174.4882049560547
INFO:root:current mean train loss 6545.82597741781
INFO:root:current train perplexity174.42431640625

100%|██████████| 1/1 [17:23<00:00, 1043.46s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.46s/it]
INFO:root:final mean train loss: 6545.262353417132
INFO:root:final train perplexity: 174.51100158691406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.98s/it][A100%|██████████| 1/1 [01:14<00:00, 74.98s/it]
INFO:root:eval mean loss: 6390.971125609486
INFO:root:eval perplexity: 175.67611694335938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.63s/it][A100%|██████████| 1/1 [01:12<00:00, 72.64s/it]
INFO:root:eval mean loss: 6499.350755623892
INFO:root:eval perplexity: 203.4352264404297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/127
 64%|██████▎   | 127/200 [42:22:56<24:11:17, 1192.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6601.078082906789
INFO:root:current train perplexity179.77536010742188
INFO:root:current mean train loss 6587.6904513202135
INFO:root:current train perplexity179.20449829101562
INFO:root:current mean train loss 6620.518749621488
INFO:root:current train perplexity180.4072265625
INFO:root:current mean train loss 6608.456957598638
INFO:root:current train perplexity180.37582397460938
INFO:root:current mean train loss 6617.082110142604
INFO:root:current train perplexity180.2607879638672
INFO:root:current mean train loss 6593.1095395105285
INFO:root:current train perplexity179.7428741455078
INFO:root:current mean train loss 6591.892839333207
INFO:root:current train perplexity179.62266540527344
INFO:root:current mean train loss 6594.803639434573
INFO:root:current train perplexity180.03614807128906
INFO:root:current mean train loss 6595.77119584517
INFO:root:current train perplexity180.0911865234375
INFO:root:current mean train loss 6595.100154231635
INFO:root:current train perplexity179.91343688964844
INFO:root:current mean train loss 6601.071960103083
INFO:root:current train perplexity179.94439697265625
INFO:root:current mean train loss 6597.444493638008
INFO:root:current train perplexity179.80184936523438
INFO:root:current mean train loss 6592.8061046024195
INFO:root:current train perplexity179.64707946777344
INFO:root:current mean train loss 6588.352790034748
INFO:root:current train perplexity179.5517578125
INFO:root:current mean train loss 6589.685254174168
INFO:root:current train perplexity179.68177795410156
INFO:root:current mean train loss 6587.887116771301
INFO:root:current train perplexity179.5341339111328
INFO:root:current mean train loss 6589.2073337016545
INFO:root:current train perplexity179.62147521972656
INFO:root:current mean train loss 6585.504754215106
INFO:root:current train perplexity179.6123046875
INFO:root:current mean train loss 6585.96995992835
INFO:root:current train perplexity179.76353454589844
INFO:root:current mean train loss 6583.254506252394
INFO:root:current train perplexity179.7809295654297

100%|██████████| 1/1 [17:27<00:00, 1047.71s/it][A100%|██████████| 1/1 [17:27<00:00, 1047.71s/it]
INFO:root:final mean train loss: 6583.033412054218
INFO:root:final train perplexity: 179.78762817382812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.46s/it][A100%|██████████| 1/1 [01:16<00:00, 76.46s/it]
INFO:root:eval mean loss: 6414.176056557513
INFO:root:eval perplexity: 179.00413513183594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.90s/it][A100%|██████████| 1/1 [01:13<00:00, 73.90s/it]
INFO:root:eval mean loss: 6505.100680303912
INFO:root:eval perplexity: 204.39414978027344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/128
 64%|██████▍   | 128/200 [42:42:56<23:54:08, 1195.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6567.097734375
INFO:root:current train perplexity179.38275146484375
INFO:root:current mean train loss 6561.492960379464
INFO:root:current train perplexity179.59886169433594
INFO:root:current mean train loss 6562.390969460227
INFO:root:current train perplexity180.0059356689453
INFO:root:current mean train loss 6570.460473958334
INFO:root:current train perplexity180.01760864257812
INFO:root:current mean train loss 6602.7594212582235
INFO:root:current train perplexity181.00013732910156
INFO:root:current mean train loss 6605.734114300271
INFO:root:current train perplexity181.0128173828125
INFO:root:current mean train loss 6611.65783347801
INFO:root:current train perplexity181.34042358398438
INFO:root:current mean train loss 6614.0688167842745
INFO:root:current train perplexity181.13905334472656
INFO:root:current mean train loss 6606.327749441964
INFO:root:current train perplexity180.71597290039062
INFO:root:current mean train loss 6602.031560496795
INFO:root:current train perplexity180.41152954101562
INFO:root:current mean train loss 6599.684442223837
INFO:root:current train perplexity180.34202575683594
INFO:root:current mean train loss 6593.757586436171
INFO:root:current train perplexity180.06036376953125
INFO:root:current mean train loss 6588.494011565564
INFO:root:current train perplexity179.83375549316406
INFO:root:current mean train loss 6589.0979364346595
INFO:root:current train perplexity179.75950622558594
INFO:root:current mean train loss 6589.955818988347
INFO:root:current train perplexity179.77081298828125
INFO:root:current mean train loss 6586.879465525793
INFO:root:current train perplexity179.68142700195312
INFO:root:current mean train loss 6581.632677530317
INFO:root:current train perplexity179.45709228515625
INFO:root:current mean train loss 6580.593857009243
INFO:root:current train perplexity179.29330444335938
INFO:root:current mean train loss 6579.740525520833
INFO:root:current train perplexity179.15809631347656
INFO:root:current mean train loss 6578.685533030063
INFO:root:current train perplexity179.0420379638672

100%|██████████| 1/1 [17:29<00:00, 1049.06s/it][A100%|██████████| 1/1 [17:29<00:00, 1049.06s/it]
INFO:root:final mean train loss: 6577.9357668847315
INFO:root:final train perplexity: 179.0662841796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.92s/it][A100%|██████████| 1/1 [01:13<00:00, 73.92s/it]
INFO:root:eval mean loss: 6353.718178607048
INFO:root:eval perplexity: 170.4622802734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.33s/it][A100%|██████████| 1/1 [01:11<00:00, 71.33s/it]
INFO:root:eval mean loss: 6447.092102483654
INFO:root:eval perplexity: 194.9239044189453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/129
 64%|██████▍   | 129/200 [43:02:53<23:34:46, 1195.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6614.1716255519705
INFO:root:current train perplexity176.2938995361328
INFO:root:current mean train loss 6612.3396733601885
INFO:root:current train perplexity176.0914306640625
INFO:root:current mean train loss 6589.4196944563355
INFO:root:current train perplexity175.96234130859375
INFO:root:current mean train loss 6581.339889837771
INFO:root:current train perplexity175.6540069580078
INFO:root:current mean train loss 6577.6837158203125
INFO:root:current train perplexity175.26754760742188
INFO:root:current mean train loss 6582.267235833246
INFO:root:current train perplexity175.5222930908203
INFO:root:current mean train loss 6578.907640754832
INFO:root:current train perplexity175.3131866455078
INFO:root:current mean train loss 6577.317140521425
INFO:root:current train perplexity175.21180725097656
INFO:root:current mean train loss 6571.052467243554
INFO:root:current train perplexity174.86509704589844
INFO:root:current mean train loss 6560.958206176758
INFO:root:current train perplexity174.60739135742188
INFO:root:current mean train loss 6557.231291942107
INFO:root:current train perplexity174.46282958984375
INFO:root:current mean train loss 6554.025905122693
INFO:root:current train perplexity174.3692626953125
INFO:root:current mean train loss 6553.47214529315
INFO:root:current train perplexity174.5171661376953
INFO:root:current mean train loss 6556.188354842964
INFO:root:current train perplexity174.739990234375
INFO:root:current mean train loss 6553.382303928242
INFO:root:current train perplexity174.78089904785156
INFO:root:current mean train loss 6549.722387572629
INFO:root:current train perplexity174.71224975585938
INFO:root:current mean train loss 6548.270376507554
INFO:root:current train perplexity174.747314453125
INFO:root:current mean train loss 6544.982526234218
INFO:root:current train perplexity174.7261505126953
INFO:root:current mean train loss 6550.270425227934
INFO:root:current train perplexity174.88711547851562

100%|██████████| 1/1 [17:14<00:00, 1034.27s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.27s/it]
INFO:root:final mean train loss: 6548.121926265838
INFO:root:final train perplexity: 174.90487670898438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.88s/it][A100%|██████████| 1/1 [01:13<00:00, 73.88s/it]
INFO:root:eval mean loss: 6374.568090993462
INFO:root:eval perplexity: 173.36105346679688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.35s/it][A100%|██████████| 1/1 [01:12<00:00, 72.35s/it]
INFO:root:eval mean loss: 6467.83181654477
INFO:root:eval perplexity: 198.2582244873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/130
 65%|██████▌   | 130/200 [43:22:35<23:10:21, 1191.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6754.863009982639
INFO:root:current train perplexity176.3758087158203
INFO:root:current mean train loss 6608.077941334575
INFO:root:current train perplexity177.8569793701172
INFO:root:current mean train loss 6566.867278614683
INFO:root:current train perplexity176.57127380371094
INFO:root:current mean train loss 6555.298971923038
INFO:root:current train perplexity176.1178741455078
INFO:root:current mean train loss 6543.908253266351
INFO:root:current train perplexity176.67115783691406
INFO:root:current mean train loss 6536.616398959357
INFO:root:current train perplexity176.2062530517578
INFO:root:current mean train loss 6549.849860330716
INFO:root:current train perplexity176.520263671875
INFO:root:current mean train loss 6557.139473510226
INFO:root:current train perplexity176.6498565673828
INFO:root:current mean train loss 6557.742246045465
INFO:root:current train perplexity176.43392944335938
INFO:root:current mean train loss 6566.3878478668175
INFO:root:current train perplexity176.74447631835938
INFO:root:current mean train loss 6567.231364496872
INFO:root:current train perplexity176.71978759765625
INFO:root:current mean train loss 6566.34277387779
INFO:root:current train perplexity176.8044891357422
INFO:root:current mean train loss 6568.446133167908
INFO:root:current train perplexity176.70443725585938
INFO:root:current mean train loss 6567.485609318302
INFO:root:current train perplexity176.68649291992188
INFO:root:current mean train loss 6566.948172532048
INFO:root:current train perplexity176.52801513671875
INFO:root:current mean train loss 6566.376965744595
INFO:root:current train perplexity176.45909118652344
INFO:root:current mean train loss 6565.553240864376
INFO:root:current train perplexity176.36032104492188
INFO:root:current mean train loss 6562.213146336948
INFO:root:current train perplexity176.25465393066406
INFO:root:current mean train loss 6558.95632730445
INFO:root:current train perplexity176.17373657226562
INFO:root:current mean train loss 6558.121105771592
INFO:root:current train perplexity176.04022216796875

100%|██████████| 1/1 [17:14<00:00, 1034.66s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.66s/it]
INFO:root:final mean train loss: 6555.686900051807
INFO:root:final train perplexity: 175.95167541503906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.08s/it][A100%|██████████| 1/1 [01:14<00:00, 74.08s/it]
INFO:root:eval mean loss: 6361.404044076906
INFO:root:eval perplexity: 171.52517700195312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.32s/it][A100%|██████████| 1/1 [01:12<00:00, 72.35s/it]
INFO:root:eval mean loss: 6454.103692237367
INFO:root:eval perplexity: 196.0447998046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/131
 66%|██████▌   | 131/200 [43:42:19<22:47:37, 1189.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6459.284592848558
INFO:root:current train perplexity171.67083740234375
INFO:root:current mean train loss 6514.6589277963785
INFO:root:current train perplexity172.99977111816406
INFO:root:current mean train loss 6519.9992913440265
INFO:root:current train perplexity173.12355041503906
INFO:root:current mean train loss 6553.174597991756
INFO:root:current train perplexity174.37391662597656
INFO:root:current mean train loss 6560.27812087368
INFO:root:current train perplexity174.70382690429688
INFO:root:current mean train loss 6567.946771774002
INFO:root:current train perplexity174.82833862304688
INFO:root:current mean train loss 6561.807603147464
INFO:root:current train perplexity174.5400390625
INFO:root:current mean train loss 6557.144424984935
INFO:root:current train perplexity174.3115692138672
INFO:root:current mean train loss 6556.009519710957
INFO:root:current train perplexity174.35354614257812
INFO:root:current mean train loss 6555.283473630703
INFO:root:current train perplexity174.30874633789062
INFO:root:current mean train loss 6549.727336325841
INFO:root:current train perplexity174.3850555419922
INFO:root:current mean train loss 6546.131118693106
INFO:root:current train perplexity174.30059814453125
INFO:root:current mean train loss 6541.722760995488
INFO:root:current train perplexity174.25123596191406
INFO:root:current mean train loss 6542.297333822351
INFO:root:current train perplexity174.14797973632812
INFO:root:current mean train loss 6543.896892189144
INFO:root:current train perplexity174.24301147460938
INFO:root:current mean train loss 6545.363249252539
INFO:root:current train perplexity174.3513946533203
INFO:root:current mean train loss 6543.38379206546
INFO:root:current train perplexity174.30259704589844
INFO:root:current mean train loss 6544.702491026488
INFO:root:current train perplexity174.3878936767578
INFO:root:current mean train loss 6546.1280740861175
INFO:root:current train perplexity174.37890625
INFO:root:current mean train loss 6547.082828319704
INFO:root:current train perplexity174.4353790283203

100%|██████████| 1/1 [17:17<00:00, 1037.82s/it][A100%|██████████| 1/1 [17:17<00:00, 1037.82s/it]
INFO:root:final mean train loss: 6544.496721399473
INFO:root:final train perplexity: 174.4055938720703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.05s/it][A100%|██████████| 1/1 [01:14<00:00, 74.05s/it]
INFO:root:eval mean loss: 6351.124875332447
INFO:root:eval perplexity: 170.1051788330078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:11<00:00, 71.62s/it][A100%|██████████| 1/1 [01:11<00:00, 71.62s/it]
INFO:root:eval mean loss: 6444.671772841866
INFO:root:eval perplexity: 194.53836059570312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/132
 66%|██████▌   | 132/200 [44:02:05<22:26:36, 1188.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6590.8076171875
INFO:root:current train perplexity174.56427001953125
INFO:root:current mean train loss 6527.3843558784965
INFO:root:current train perplexity173.63174438476562
INFO:root:current mean train loss 6547.071755240484
INFO:root:current train perplexity173.93553161621094
INFO:root:current mean train loss 6537.964571849945
INFO:root:current train perplexity173.4659881591797
INFO:root:current mean train loss 6537.887471562853
INFO:root:current train perplexity173.9123077392578
INFO:root:current mean train loss 6531.414913170465
INFO:root:current train perplexity173.77735900878906
INFO:root:current mean train loss 6532.653723543206
INFO:root:current train perplexity173.5946502685547
INFO:root:current mean train loss 6537.407727330081
INFO:root:current train perplexity173.80130004882812
INFO:root:current mean train loss 6543.023555081369
INFO:root:current train perplexity173.9020233154297
INFO:root:current mean train loss 6540.20516407907
INFO:root:current train perplexity173.7584686279297
INFO:root:current mean train loss 6544.68449072687
INFO:root:current train perplexity173.9511260986328
INFO:root:current mean train loss 6546.309408406469
INFO:root:current train perplexity174.10223388671875
INFO:root:current mean train loss 6549.337194146596
INFO:root:current train perplexity174.10215759277344
INFO:root:current mean train loss 6545.0035779417585
INFO:root:current train perplexity173.96083068847656
INFO:root:current mean train loss 6543.480547592364
INFO:root:current train perplexity173.93569946289062
INFO:root:current mean train loss 6541.92963876681
INFO:root:current train perplexity173.82525634765625
INFO:root:current mean train loss 6539.80105080027
INFO:root:current train perplexity173.688720703125
INFO:root:current mean train loss 6537.89185004527
INFO:root:current train perplexity173.5915985107422
INFO:root:current mean train loss 6538.585077245405
INFO:root:current train perplexity173.45184326171875
INFO:root:current mean train loss 6540.002974168087
INFO:root:current train perplexity173.48194885253906

100%|██████████| 1/1 [17:14<00:00, 1034.93s/it][A100%|██████████| 1/1 [17:14<00:00, 1034.93s/it]
INFO:root:final mean train loss: 6537.360899062934
INFO:root:final train perplexity: 173.42686462402344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.91s/it][A100%|██████████| 1/1 [01:13<00:00, 73.91s/it]
INFO:root:eval mean loss: 6332.399457349845
INFO:root:eval perplexity: 167.54852294921875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.66s/it][A100%|██████████| 1/1 [01:13<00:00, 73.66s/it]
INFO:root:eval mean loss: 6428.006326878324
INFO:root:eval perplexity: 191.9049072265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/133
 66%|██████▋   | 133/200 [44:21:50<22:05:43, 1187.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6549.826432291667
INFO:root:current train perplexity173.115966796875
INFO:root:current mean train loss 6588.745941162109
INFO:root:current train perplexity173.38478088378906
INFO:root:current mean train loss 6585.846195162259
INFO:root:current train perplexity173.8911590576172
INFO:root:current mean train loss 6565.326360405816
INFO:root:current train perplexity173.38800048828125
INFO:root:current mean train loss 6550.259764563519
INFO:root:current train perplexity173.22743225097656
INFO:root:current mean train loss 6537.123523821149
INFO:root:current train perplexity173.01609802246094
INFO:root:current mean train loss 6547.2175699869795
INFO:root:current train perplexity173.2862548828125
INFO:root:current mean train loss 6541.252923262747
INFO:root:current train perplexity173.11611938476562
INFO:root:current mean train loss 6535.865347928779
INFO:root:current train perplexity172.93618774414062
INFO:root:current mean train loss 6531.870101928711
INFO:root:current train perplexity172.99085998535156
INFO:root:current mean train loss 6538.80974858122
INFO:root:current train perplexity173.3229522705078
INFO:root:current mean train loss 6537.420713648303
INFO:root:current train perplexity173.3287353515625
INFO:root:current mean train loss 6540.603757440476
INFO:root:current train perplexity173.51852416992188
INFO:root:current mean train loss 6536.547498994715
INFO:root:current train perplexity173.39312744140625
INFO:root:current mean train loss 6534.672607087436
INFO:root:current train perplexity173.3152618408203
INFO:root:current mean train loss 6538.19446864984
INFO:root:current train perplexity173.34890747070312
INFO:root:current mean train loss 6538.06336566971
INFO:root:current train perplexity173.28543090820312
INFO:root:current mean train loss 6539.697682328658
INFO:root:current train perplexity173.3102264404297
INFO:root:current mean train loss 6537.603086672547
INFO:root:current train perplexity173.331298828125
INFO:root:current mean train loss 6539.0201171875
INFO:root:current train perplexity173.3867645263672

100%|██████████| 1/1 [17:19<00:00, 1039.66s/it][A100%|██████████| 1/1 [17:19<00:00, 1039.66s/it]
INFO:root:final mean train loss: 6536.799183563217
INFO:root:final train perplexity: 173.3500518798828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.46s/it][A100%|██████████| 1/1 [01:15<00:00, 75.46s/it]
INFO:root:eval mean loss: 6342.342323249113
INFO:root:eval perplexity: 168.9011993408203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.41s/it][A100%|██████████| 1/1 [01:12<00:00, 72.41s/it]
INFO:root:eval mean loss: 6440.830098037179
INFO:root:eval perplexity: 193.92820739746094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/134
 67%|██████▋   | 134/200 [44:41:39<21:46:49, 1188.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6491.696377840909
INFO:root:current train perplexity173.44497680664062
INFO:root:current mean train loss 6525.938123455156
INFO:root:current train perplexity174.1277313232422
INFO:root:current mean train loss 6531.733460133687
INFO:root:current train perplexity174.1609344482422
INFO:root:current mean train loss 6540.954850174072
INFO:root:current train perplexity174.51914978027344
INFO:root:current mean train loss 6550.0232880470385
INFO:root:current train perplexity174.64385986328125
INFO:root:current mean train loss 6539.6056845415405
INFO:root:current train perplexity174.1949920654297
INFO:root:current mean train loss 6537.299226250923
INFO:root:current train perplexity174.2071990966797
INFO:root:current mean train loss 6535.555653379384
INFO:root:current train perplexity173.95916748046875
INFO:root:current mean train loss 6529.1057231907425
INFO:root:current train perplexity173.64317321777344
INFO:root:current mean train loss 6526.909587005021
INFO:root:current train perplexity173.5501251220703
INFO:root:current mean train loss 6530.511589992456
INFO:root:current train perplexity173.37477111816406
INFO:root:current mean train loss 6538.771790536056
INFO:root:current train perplexity173.54408264160156
INFO:root:current mean train loss 6541.933177353539
INFO:root:current train perplexity173.64639282226562
INFO:root:current mean train loss 6537.5233488505355
INFO:root:current train perplexity173.5624542236328
INFO:root:current mean train loss 6542.573309958425
INFO:root:current train perplexity173.66651916503906
INFO:root:current mean train loss 6542.896095483909
INFO:root:current train perplexity173.7032928466797
INFO:root:current mean train loss 6542.970728456227
INFO:root:current train perplexity173.674560546875
INFO:root:current mean train loss 6538.499473249772
INFO:root:current train perplexity173.63539123535156
INFO:root:current mean train loss 6542.383927456546
INFO:root:current train perplexity173.6543426513672
INFO:root:current mean train loss 6541.0696350313765
INFO:root:current train perplexity173.6339874267578

100%|██████████| 1/1 [17:22<00:00, 1042.85s/it][A100%|██████████| 1/1 [17:22<00:00, 1042.85s/it]
INFO:root:final mean train loss: 6538.74597851267
INFO:root:final train perplexity: 173.61642456054688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.11s/it][A100%|██████████| 1/1 [01:15<00:00, 75.11s/it]
INFO:root:eval mean loss: 6335.069760153479
INFO:root:eval perplexity: 167.91075134277344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.37s/it][A100%|██████████| 1/1 [01:12<00:00, 72.37s/it]
INFO:root:eval mean loss: 6432.994079156971
INFO:root:eval perplexity: 192.68934631347656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/135
 68%|██████▊   | 135/200 [45:01:32<21:28:31, 1189.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6556.82876392121
INFO:root:current train perplexity174.61746215820312
INFO:root:current mean train loss 6561.508831850032
INFO:root:current train perplexity174.242431640625
INFO:root:current mean train loss 6572.7218640385845
INFO:root:current train perplexity174.60989379882812
INFO:root:current mean train loss 6566.858053914181
INFO:root:current train perplexity174.73007202148438
INFO:root:current mean train loss 6552.421547831794
INFO:root:current train perplexity174.64744567871094
INFO:root:current mean train loss 6556.257776331018
INFO:root:current train perplexity174.6356964111328
INFO:root:current mean train loss 6556.804988630223
INFO:root:current train perplexity174.76182556152344
INFO:root:current mean train loss 6563.209314610555
INFO:root:current train perplexity174.82041931152344
INFO:root:current mean train loss 6559.539429530201
INFO:root:current train perplexity174.7210693359375
INFO:root:current mean train loss 6556.716475611482
INFO:root:current train perplexity174.60597229003906
INFO:root:current mean train loss 6556.932315024423
INFO:root:current train perplexity174.5635986328125
INFO:root:current mean train loss 6553.90274983315
INFO:root:current train perplexity174.4674072265625
INFO:root:current mean train loss 6547.365532852951
INFO:root:current train perplexity174.29302978515625
INFO:root:current mean train loss 6546.751504424655
INFO:root:current train perplexity174.2645263671875
INFO:root:current mean train loss 6548.413029723059
INFO:root:current train perplexity174.25265502929688
INFO:root:current mean train loss 6548.636519026427
INFO:root:current train perplexity174.23353576660156
INFO:root:current mean train loss 6548.189102623229
INFO:root:current train perplexity174.17945861816406
INFO:root:current mean train loss 6545.564247905344
INFO:root:current train perplexity174.05874633789062
INFO:root:current mean train loss 6543.862300562632
INFO:root:current train perplexity173.9986572265625

100%|██████████| 1/1 [17:21<00:00, 1041.87s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.87s/it]
INFO:root:final mean train loss: 6540.494713856846
INFO:root:final train perplexity: 173.85601806640625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.04s/it][A100%|██████████| 1/1 [01:15<00:00, 75.04s/it]
INFO:root:eval mean loss: 6331.468230551862
INFO:root:eval perplexity: 167.42227172851562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.43s/it][A100%|██████████| 1/1 [01:12<00:00, 72.43s/it]
INFO:root:eval mean loss: 6428.836546120068
INFO:root:eval perplexity: 192.0353546142578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/136
 68%|██████▊   | 136/200 [45:21:24<21:09:26, 1190.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6391.740367542614
INFO:root:current train perplexity163.66493225097656
INFO:root:current mean train loss 6511.050143405124
INFO:root:current train perplexity173.2074432373047
INFO:root:current mean train loss 6508.21454522734
INFO:root:current train perplexity173.1302490234375
INFO:root:current mean train loss 6523.769157581391
INFO:root:current train perplexity173.68125915527344
INFO:root:current mean train loss 6524.908047492777
INFO:root:current train perplexity173.5409393310547
INFO:root:current mean train loss 6534.8186297929915
INFO:root:current train perplexity173.83810424804688
INFO:root:current mean train loss 6518.251662234043
INFO:root:current train perplexity173.67604064941406
INFO:root:current mean train loss 6524.885264207543
INFO:root:current train perplexity173.7518310546875
INFO:root:current mean train loss 6518.169785806489
INFO:root:current train perplexity173.49618530273438
INFO:root:current mean train loss 6531.652183490841
INFO:root:current train perplexity174.0108642578125
INFO:root:current mean train loss 6537.172818720635
INFO:root:current train perplexity174.11029052734375
INFO:root:current mean train loss 6542.337463433843
INFO:root:current train perplexity174.1722412109375
INFO:root:current mean train loss 6543.361613190932
INFO:root:current train perplexity174.32321166992188
INFO:root:current mean train loss 6540.186064579758
INFO:root:current train perplexity174.1619415283203
INFO:root:current mean train loss 6543.4229190429
INFO:root:current train perplexity174.27964782714844
INFO:root:current mean train loss 6547.4948124560515
INFO:root:current train perplexity174.45892333984375
INFO:root:current mean train loss 6547.676050092625
INFO:root:current train perplexity174.484130859375
INFO:root:current mean train loss 6548.493348987252
INFO:root:current train perplexity174.60406494140625
INFO:root:current mean train loss 6547.062298594095
INFO:root:current train perplexity174.50242614746094
INFO:root:current mean train loss 6549.317803638884
INFO:root:current train perplexity174.64569091796875

100%|██████████| 1/1 [17:21<00:00, 1041.16s/it][A100%|██████████| 1/1 [17:21<00:00, 1041.16s/it]
INFO:root:final mean train loss: 6546.1909109510925
INFO:root:final train perplexity: 174.63877868652344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.09s/it][A100%|██████████| 1/1 [01:15<00:00, 75.09s/it]
INFO:root:eval mean loss: 6351.316904920212
INFO:root:eval perplexity: 170.13162231445312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.01s/it][A100%|██████████| 1/1 [01:13<00:00, 73.01s/it]
INFO:root:eval mean loss: 6451.128343514517
INFO:root:eval perplexity: 195.56834411621094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/137
 68%|██████▊   | 137/200 [45:41:15<20:50:04, 1190.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6531.233921595982
INFO:root:current train perplexity173.88088989257812
INFO:root:current mean train loss 6591.5488929748535
INFO:root:current train perplexity175.93162536621094
INFO:root:current mean train loss 6570.721146432977
INFO:root:current train perplexity175.8979034423828
INFO:root:current mean train loss 6564.267427770103
INFO:root:current train perplexity175.63658142089844
INFO:root:current mean train loss 6560.582538925599
INFO:root:current train perplexity175.2747344970703
INFO:root:current mean train loss 6564.576768354936
INFO:root:current train perplexity175.2516632080078
INFO:root:current mean train loss 6560.6549305521
INFO:root:current train perplexity175.29537963867188
INFO:root:current mean train loss 6559.136395464887
INFO:root:current train perplexity175.43670654296875
INFO:root:current mean train loss 6558.982447822313
INFO:root:current train perplexity175.5130157470703
INFO:root:current mean train loss 6555.089437024347
INFO:root:current train perplexity175.36141967773438
INFO:root:current mean train loss 6553.880588635397
INFO:root:current train perplexity175.3105926513672
INFO:root:current mean train loss 6555.217383418523
INFO:root:current train perplexity175.21047973632812
INFO:root:current mean train loss 6557.107233799242
INFO:root:current train perplexity175.08636474609375
INFO:root:current mean train loss 6552.670538477151
INFO:root:current train perplexity174.97528076171875
INFO:root:current mean train loss 6551.91306815695
INFO:root:current train perplexity174.9025421142578
INFO:root:current mean train loss 6551.054998427786
INFO:root:current train perplexity174.80482482910156
INFO:root:current mean train loss 6548.231429116438
INFO:root:current train perplexity174.69508361816406
INFO:root:current mean train loss 6547.461017749928
INFO:root:current train perplexity174.6997528076172
INFO:root:current mean train loss 6547.361457674457
INFO:root:current train perplexity174.77215576171875
INFO:root:current mean train loss 6548.6237567569215
INFO:root:current train perplexity174.81399536132812

100%|██████████| 1/1 [17:23<00:00, 1043.40s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.40s/it]
INFO:root:final mean train loss: 6547.446575924657
INFO:root:final train perplexity: 174.81182861328125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.39s/it][A100%|██████████| 1/1 [01:14<00:00, 74.39s/it]
INFO:root:eval mean loss: 6350.202217697251
INFO:root:eval perplexity: 169.97828674316406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.21s/it][A100%|██████████| 1/1 [01:12<00:00, 72.21s/it]
INFO:root:eval mean loss: 6452.199480205563
INFO:root:eval perplexity: 195.73973083496094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/138
 69%|██████▉   | 138/200 [46:01:08<20:30:47, 1191.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6529.9120876736115
INFO:root:current train perplexity176.9231414794922
INFO:root:current mean train loss 6516.199821524784
INFO:root:current train perplexity174.3326873779297
INFO:root:current mean train loss 6536.109998804209
INFO:root:current train perplexity174.67384338378906
INFO:root:current mean train loss 6517.913926630435
INFO:root:current train perplexity173.9559326171875
INFO:root:current mean train loss 6512.351055565309
INFO:root:current train perplexity173.2210693359375
INFO:root:current mean train loss 6517.8855450831425
INFO:root:current train perplexity173.65774536132812
INFO:root:current mean train loss 6534.152380844234
INFO:root:current train perplexity173.89854431152344
INFO:root:current mean train loss 6538.242543388213
INFO:root:current train perplexity173.8595733642578
INFO:root:current mean train loss 6534.91754634338
INFO:root:current train perplexity173.82981872558594
INFO:root:current mean train loss 6528.393129443618
INFO:root:current train perplexity173.41868591308594
INFO:root:current mean train loss 6527.662325246711
INFO:root:current train perplexity173.2096710205078
INFO:root:current mean train loss 6525.755927179995
INFO:root:current train perplexity172.7734832763672
INFO:root:current mean train loss 6529.602992830698
INFO:root:current train perplexity172.5415496826172
INFO:root:current mean train loss 6529.942863106993
INFO:root:current train perplexity172.22903442382812
INFO:root:current mean train loss 6526.900096980428
INFO:root:current train perplexity172.01136779785156
INFO:root:current mean train loss 6524.9884983692355
INFO:root:current train perplexity171.7810516357422
INFO:root:current mean train loss 6524.362694422018
INFO:root:current train perplexity171.58154296875
INFO:root:current mean train loss 6521.012599055337
INFO:root:current train perplexity171.3650360107422
INFO:root:current mean train loss 6524.3004263528965
INFO:root:current train perplexity171.32467651367188
INFO:root:current mean train loss 6523.333205886488
INFO:root:current train perplexity171.1811981201172

100%|██████████| 1/1 [17:23<00:00, 1043.37s/it][A100%|██████████| 1/1 [17:23<00:00, 1043.37s/it]
INFO:root:final mean train loss: 6520.166787567369
INFO:root:final train perplexity: 171.0910186767578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.69s/it][A100%|██████████| 1/1 [01:14<00:00, 74.69s/it]
INFO:root:eval mean loss: 6287.604390029366
INFO:root:eval perplexity: 161.58718872070312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.38s/it][A100%|██████████| 1/1 [01:12<00:00, 72.38s/it]
INFO:root:eval mean loss: 6410.146506884419
INFO:root:eval perplexity: 189.122314453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat/139
 70%|██████▉   | 139/200 [46:21:01<20:11:26, 1191.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6495.003441595262
INFO:root:current train perplexity165.213134765625
INFO:root:current mean train loss 6454.591142819251
INFO:root:current train perplexity166.99729919433594
slurmstepd: error: *** JOB 29854915 ON gr040 CANCELLED AT 2023-02-07T10:42:37 ***
