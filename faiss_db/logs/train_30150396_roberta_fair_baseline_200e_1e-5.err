INFO:root:Output: roberta_fair_baseline
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Some weights of RobertaForCausalLMBaseline were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8.602389566826098
INFO:root:current train perplexity1.0067627429962158
INFO:root:current mean train loss 7.599161066601624
INFO:root:current train perplexity1.0059998035430908
INFO:root:current mean train loss 6.976559575186126
INFO:root:current train perplexity1.0055034160614014
INFO:root:current mean train loss 6.5274351832263156
INFO:root:current train perplexity1.00514554977417
INFO:root:current mean train loss 6.171200285932583
INFO:root:current train perplexity1.0048638582229614
INFO:root:current mean train loss 5.886919880550175
INFO:root:current train perplexity1.004640817642212
INFO:root:current mean train loss 5.652021392732219
INFO:root:current train perplexity1.0044543743133545
INFO:root:current mean train loss 5.455759925448402
INFO:root:current train perplexity1.0042973756790161
INFO:root:current mean train loss 5.2850994034789425
INFO:root:current train perplexity1.004164218902588
INFO:root:current mean train loss 5.1401000309276865
INFO:root:current train perplexity1.0040502548217773
INFO:root:current mean train loss 5.0139378401449965
INFO:root:current train perplexity1.0039564371109009
INFO:root:current mean train loss 4.899502184910014
INFO:root:current train perplexity1.003867506980896
INFO:root:current mean train loss 4.7986466343169765
INFO:root:current train perplexity1.0037897825241089
INFO:root:current mean train loss 4.708654549225813
INFO:root:current train perplexity1.0037200450897217
INFO:root:current mean train loss 4.626257498634903
INFO:root:current train perplexity1.0036534070968628
INFO:root:current mean train loss 4.5495835260125235
INFO:root:current train perplexity1.0035948753356934
INFO:root:current mean train loss 4.478059052438719
INFO:root:current train perplexity1.0035409927368164
INFO:root:current mean train loss 4.413409006337181
INFO:root:current train perplexity1.0034884214401245
INFO:root:current mean train loss 4.353417221905998
INFO:root:current train perplexity1.0034407377243042

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:45<00:00, 645.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:45<00:00, 645.23s/it]
INFO:root:final mean train loss: 4.307146895851562
INFO:root:final train perplexity: 1.003404974937439
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it]
INFO:root:eval mean loss: 3.0126092628384313
INFO:root:eval perplexity: 1.0024408102035522
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.48s/it]
INFO:root:eval mean loss: 3.318247631086525
INFO:root:eval perplexity: 1.0027318000793457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/1
  0%|          | 1/200 [12:02<39:57:18, 722.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3.248730853199959
INFO:root:current train perplexity1.002591848373413
INFO:root:current mean train loss 3.1995013232888847
INFO:root:current train perplexity1.0025227069854736
INFO:root:current mean train loss 3.1641349417191966
INFO:root:current train perplexity1.0025100708007812
INFO:root:current mean train loss 3.147624844991708
INFO:root:current train perplexity1.0024994611740112
INFO:root:current mean train loss 3.136478778261405
INFO:root:current train perplexity1.0024874210357666
INFO:root:current mean train loss 3.120429363823676
INFO:root:current train perplexity1.0024746656417847
INFO:root:current mean train loss 3.1085376511146494
INFO:root:current train perplexity1.002463698387146
INFO:root:current mean train loss 3.0966616512676857
INFO:root:current train perplexity1.0024505853652954
INFO:root:current mean train loss 3.0835247761478612
INFO:root:current train perplexity1.0024405717849731
INFO:root:current mean train loss 3.069807398267188
INFO:root:current train perplexity1.002429723739624
INFO:root:current mean train loss 3.0573074916216334
INFO:root:current train perplexity1.00242018699646
INFO:root:current mean train loss 3.0462296111609346
INFO:root:current train perplexity1.0024100542068481
INFO:root:current mean train loss 3.033461168212326
INFO:root:current train perplexity1.0024011135101318
INFO:root:current mean train loss 3.0206410777967387
INFO:root:current train perplexity1.002389907836914
INFO:root:current mean train loss 3.009714692157541
INFO:root:current train perplexity1.0023815631866455
INFO:root:current mean train loss 2.9997890670255494
INFO:root:current train perplexity1.0023739337921143
INFO:root:current mean train loss 2.9893394906331996
INFO:root:current train perplexity1.0023647546768188
INFO:root:current mean train loss 2.9785224142330233
INFO:root:current train perplexity1.0023562908172607
INFO:root:current mean train loss 2.96886252451048
INFO:root:current train perplexity1.002347469329834
INFO:root:current mean train loss 2.9589696441911206
INFO:root:current train perplexity1.0023380517959595

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:57<00:00, 657.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:57<00:00, 657.92s/it]
INFO:root:final mean train loss: 2.9522385210084363
INFO:root:final train perplexity: 1.0023325681686401
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.93s/it]
INFO:root:eval mean loss: 2.5924096437210733
INFO:root:eval perplexity: 1.0020999908447266
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.56s/it]
INFO:root:eval mean loss: 2.9517444355268005
INFO:root:eval perplexity: 1.0024296045303345
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/2
  1%|          | 2/200 [24:04<39:42:54, 722.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.7388691396424263
INFO:root:current train perplexity1.0021768808364868
INFO:root:current mean train loss 2.7528333179932787
INFO:root:current train perplexity1.0021860599517822
INFO:root:current mean train loss 2.738973494763026
INFO:root:current train perplexity1.0021675825119019
INFO:root:current mean train loss 2.7284289530447654
INFO:root:current train perplexity1.0021616220474243
INFO:root:current mean train loss 2.7210806748608243
INFO:root:current train perplexity1.0021593570709229
INFO:root:current mean train loss 2.7194416200913363
INFO:root:current train perplexity1.0021555423736572
INFO:root:current mean train loss 2.7118292269171884
INFO:root:current train perplexity1.0021482706069946
INFO:root:current mean train loss 2.7053305813267645
INFO:root:current train perplexity1.002140760421753
INFO:root:current mean train loss 2.701351526404629
INFO:root:current train perplexity1.0021376609802246
INFO:root:current mean train loss 2.6973802580838546
INFO:root:current train perplexity1.002132773399353
INFO:root:current mean train loss 2.6906398058168426
INFO:root:current train perplexity1.0021271705627441
INFO:root:current mean train loss 2.683655162038576
INFO:root:current train perplexity1.0021212100982666
INFO:root:current mean train loss 2.6777381048101967
INFO:root:current train perplexity1.0021162033081055
INFO:root:current mean train loss 2.6708560658860545
INFO:root:current train perplexity1.0021109580993652
INFO:root:current mean train loss 2.6665919793708626
INFO:root:current train perplexity1.002106785774231
INFO:root:current mean train loss 2.662260195824809
INFO:root:current train perplexity1.002103328704834
INFO:root:current mean train loss 2.6556876203580204
INFO:root:current train perplexity1.0020980834960938
INFO:root:current mean train loss 2.650670292744799
INFO:root:current train perplexity1.0020934343338013
INFO:root:current mean train loss 2.6457829847403453
INFO:root:current train perplexity1.0020896196365356
INFO:root:current mean train loss 2.640476142441246
INFO:root:current train perplexity1.0020850896835327

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.90s/it]
INFO:root:final mean train loss: 2.6383035553265146
INFO:root:final train perplexity: 1.0020842552185059
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.84s/it]
INFO:root:eval mean loss: 2.3933366520184998
INFO:root:eval perplexity: 1.0019385814666748
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.55s/it]
INFO:root:eval mean loss: 2.7796358850830836
INFO:root:eval perplexity: 1.0022878646850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/3
  2%|â–         | 3/200 [34:55<37:44:45, 689.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.539328293800354
INFO:root:current train perplexity1.0020238161087036
INFO:root:current mean train loss 2.5299090258280437
INFO:root:current train perplexity1.0020065307617188
INFO:root:current mean train loss 2.5270403394699095
INFO:root:current train perplexity1.00199556350708
INFO:root:current mean train loss 2.516478877748762
INFO:root:current train perplexity1.0019878149032593
INFO:root:current mean train loss 2.510532055430942
INFO:root:current train perplexity1.0019854307174683
INFO:root:current mean train loss 2.507314180460843
INFO:root:current train perplexity1.00198233127594
INFO:root:current mean train loss 2.504143766256479
INFO:root:current train perplexity1.0019806623458862
INFO:root:current mean train loss 2.501088285446167
INFO:root:current train perplexity1.001976490020752
INFO:root:current mean train loss 2.4977827473247753
INFO:root:current train perplexity1.0019742250442505
INFO:root:current mean train loss 2.4956580849697714
INFO:root:current train perplexity1.00197172164917
INFO:root:current mean train loss 2.4930343986692884
INFO:root:current train perplexity1.0019696950912476
INFO:root:current mean train loss 2.4902270897575045
INFO:root:current train perplexity1.0019681453704834
INFO:root:current mean train loss 2.4873616930007936
INFO:root:current train perplexity1.0019663572311401
INFO:root:current mean train loss 2.4857411057860763
INFO:root:current train perplexity1.001965045928955
INFO:root:current mean train loss 2.48226104982968
INFO:root:current train perplexity1.001961588859558
INFO:root:current mean train loss 2.4795568004731208
INFO:root:current train perplexity1.0019605159759521
INFO:root:current mean train loss 2.4770899989388204
INFO:root:current train perplexity1.0019587278366089
INFO:root:current mean train loss 2.4741161597115653
INFO:root:current train perplexity1.0019551515579224
INFO:root:current mean train loss 2.470744841163223
INFO:root:current train perplexity1.001951813697815
INFO:root:current mean train loss 2.4670034478260923
INFO:root:current train perplexity1.0019481182098389

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.40s/it]
INFO:root:final mean train loss: 2.466238948953795
INFO:root:final train perplexity: 1.0019482374191284
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.40s/it]
INFO:root:eval mean loss: 2.291085033129293
INFO:root:eval perplexity: 1.001855731010437
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.25s/it]
INFO:root:eval mean loss: 2.7005205137509827
INFO:root:eval perplexity: 1.0022226572036743
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/4
  2%|â–         | 4/200 [45:49<36:47:20, 675.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.396294330483052
INFO:root:current train perplexity1.0018693208694458
INFO:root:current mean train loss 2.39265710579421
INFO:root:current train perplexity1.0018842220306396
INFO:root:current mean train loss 2.38583627175749
INFO:root:current train perplexity1.0018830299377441
INFO:root:current mean train loss 2.3848715478133116
INFO:root:current train perplexity1.0018868446350098
INFO:root:current mean train loss 2.385875467059444
INFO:root:current train perplexity1.0018846988677979
INFO:root:current mean train loss 2.384411512438793
INFO:root:current train perplexity1.001883625984192
INFO:root:current mean train loss 2.3857006729989574
INFO:root:current train perplexity1.0018843412399292
INFO:root:current mean train loss 2.3824263554663827
INFO:root:current train perplexity1.0018802881240845
INFO:root:current mean train loss 2.3811643593864047
INFO:root:current train perplexity1.001878261566162
INFO:root:current mean train loss 2.377736922010641
INFO:root:current train perplexity1.001876711845398
INFO:root:current mean train loss 2.3749738438953947
INFO:root:current train perplexity1.0018742084503174
INFO:root:current mean train loss 2.3715686083044267
INFO:root:current train perplexity1.0018715858459473
INFO:root:current mean train loss 2.3681937807078612
INFO:root:current train perplexity1.0018693208694458
INFO:root:current mean train loss 2.3655832078915577
INFO:root:current train perplexity1.0018688440322876
INFO:root:current mean train loss 2.363215077711598
INFO:root:current train perplexity1.0018656253814697
INFO:root:current mean train loss 2.3608921631628643
INFO:root:current train perplexity1.0018646717071533
INFO:root:current mean train loss 2.3579544777918806
INFO:root:current train perplexity1.0018621683120728
INFO:root:current mean train loss 2.3561364814394534
INFO:root:current train perplexity1.001860499382019
INFO:root:current mean train loss 2.353999844386607
INFO:root:current train perplexity1.0018587112426758
INFO:root:current mean train loss 2.3523411677461867
INFO:root:current train perplexity1.0018577575683594

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.01s/it]
INFO:root:final mean train loss: 2.3525353668917925
INFO:root:final train perplexity: 1.0018583536148071
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.29s/it]
INFO:root:eval mean loss: 2.211233336452051
INFO:root:eval perplexity: 1.001791000366211
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.47s/it]
INFO:root:eval mean loss: 2.6261552073431353
INFO:root:eval perplexity: 1.0021613836288452
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/5
  2%|â–Ž         | 5/200 [56:08<35:29:25, 655.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.310084439459301
INFO:root:current train perplexity1.0018233060836792
INFO:root:current mean train loss 2.2999711853006612
INFO:root:current train perplexity1.001808524131775
INFO:root:current mean train loss 2.2985239049918214
INFO:root:current train perplexity1.001810073852539
INFO:root:current mean train loss 2.292668902936081
INFO:root:current train perplexity1.001808762550354
INFO:root:current mean train loss 2.289518450656213
INFO:root:current train perplexity1.0018064975738525
INFO:root:current mean train loss 2.2869848161935806
INFO:root:current train perplexity1.0018084049224854
INFO:root:current mean train loss 2.2853156582653873
INFO:root:current train perplexity1.0018080472946167
INFO:root:current mean train loss 2.28747928963632
INFO:root:current train perplexity1.0018105506896973
INFO:root:current mean train loss 2.2862204779866593
INFO:root:current train perplexity1.0018078088760376
INFO:root:current mean train loss 2.284462964873973
INFO:root:current train perplexity1.0018060207366943
INFO:root:current mean train loss 2.2830131565292824
INFO:root:current train perplexity1.0018037557601929
INFO:root:current mean train loss 2.28255620769955
INFO:root:current train perplexity1.0018041133880615
INFO:root:current mean train loss 2.280701554360048
INFO:root:current train perplexity1.0018020868301392
INFO:root:current mean train loss 2.2803981109506135
INFO:root:current train perplexity1.0018017292022705
INFO:root:current mean train loss 2.279230731356176
INFO:root:current train perplexity1.0018000602722168
INFO:root:current mean train loss 2.2770106551623104
INFO:root:current train perplexity1.0017986297607422
INFO:root:current mean train loss 2.275469785065096
INFO:root:current train perplexity1.0017979145050049
INFO:root:current mean train loss 2.2742473405572867
INFO:root:current train perplexity1.0017973184585571
INFO:root:current mean train loss 2.272809256667038
INFO:root:current train perplexity1.0017955303192139

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.18s/it]
INFO:root:final mean train loss: 2.270908230912848
INFO:root:final train perplexity: 1.0017938613891602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.21s/it]
INFO:root:eval mean loss: 2.162424915648521
INFO:root:eval perplexity: 1.00175142288208
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.29s/it]
INFO:root:eval mean loss: 2.5833709899415362
INFO:root:eval perplexity: 1.0021260976791382
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/6
  3%|â–Ž         | 6/200 [1:06:56<35:10:19, 652.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.1833269596099854
INFO:root:current train perplexity1.00179123878479
INFO:root:current mean train loss 2.230128219812223
INFO:root:current train perplexity1.0017629861831665
INFO:root:current mean train loss 2.2262751083468917
INFO:root:current train perplexity1.001761555671692
INFO:root:current mean train loss 2.225667022787455
INFO:root:current train perplexity1.001758337020874
INFO:root:current mean train loss 2.2242375787654125
INFO:root:current train perplexity1.00175940990448
INFO:root:current mean train loss 2.223455176857893
INFO:root:current train perplexity1.001760721206665
INFO:root:current mean train loss 2.222248041292594
INFO:root:current train perplexity1.001757025718689
INFO:root:current mean train loss 2.2212380322172027
INFO:root:current train perplexity1.001753807067871
INFO:root:current mean train loss 2.218684976764684
INFO:root:current train perplexity1.0017521381378174
INFO:root:current mean train loss 2.2180032498563964
INFO:root:current train perplexity1.0017521381378174
INFO:root:current mean train loss 2.2163803583377604
INFO:root:current train perplexity1.0017518997192383
INFO:root:current mean train loss 2.216425076059381
INFO:root:current train perplexity1.0017517805099487
INFO:root:current mean train loss 2.2147653323228313
INFO:root:current train perplexity1.0017499923706055
INFO:root:current mean train loss 2.2128162799112436
INFO:root:current train perplexity1.0017482042312622
INFO:root:current mean train loss 2.2117011779380813
INFO:root:current train perplexity1.0017473697662354
INFO:root:current mean train loss 2.210405736585842
INFO:root:current train perplexity1.0017457008361816
INFO:root:current mean train loss 2.2105141045673427
INFO:root:current train perplexity1.001746416091919
INFO:root:current mean train loss 2.2100552674393876
INFO:root:current train perplexity1.0017447471618652
INFO:root:current mean train loss 2.2098007830959236
INFO:root:current train perplexity1.0017448663711548
INFO:root:current mean train loss 2.20841983416907
INFO:root:current train perplexity1.0017441511154175

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.93s/it]
INFO:root:final mean train loss: 2.207875339839414
INFO:root:final train perplexity: 1.0017439126968384
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.55s/it]
INFO:root:eval mean loss: 2.117235275447791
INFO:root:eval perplexity: 1.001714825630188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.77s/it]
INFO:root:eval mean loss: 2.5403136589848403
INFO:root:eval perplexity: 1.0020906925201416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/7
  4%|â–Ž         | 7/200 [1:17:50<35:01:11, 653.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.17903561062283
INFO:root:current train perplexity1.001717209815979
INFO:root:current mean train loss 2.1750401531235646
INFO:root:current train perplexity1.0017255544662476
INFO:root:current mean train loss 2.1701388605144047
INFO:root:current train perplexity1.0017199516296387
INFO:root:current mean train loss 2.169402167482196
INFO:root:current train perplexity1.0017095804214478
INFO:root:current mean train loss 2.167611219951411
INFO:root:current train perplexity1.001704454421997
INFO:root:current mean train loss 2.1662129239686205
INFO:root:current train perplexity1.001709222793579
INFO:root:current mean train loss 2.168319197341462
INFO:root:current train perplexity1.0017093420028687
INFO:root:current mean train loss 2.1662858925489994
INFO:root:current train perplexity1.0017064809799194
INFO:root:current mean train loss 2.164301051632991
INFO:root:current train perplexity1.0017064809799194
INFO:root:current mean train loss 2.16426449001225
INFO:root:current train perplexity1.0017070770263672
INFO:root:current mean train loss 2.164018083532105
INFO:root:current train perplexity1.0017077922821045
INFO:root:current mean train loss 2.1634217488957646
INFO:root:current train perplexity1.0017058849334717
INFO:root:current mean train loss 2.1621873945438215
INFO:root:current train perplexity1.0017056465148926
INFO:root:current mean train loss 2.162620172695976
INFO:root:current train perplexity1.0017069578170776
INFO:root:current mean train loss 2.1618094134902415
INFO:root:current train perplexity1.0017057657241821
INFO:root:current mean train loss 2.159791624200517
INFO:root:current train perplexity1.0017038583755493
INFO:root:current mean train loss 2.1589304525566337
INFO:root:current train perplexity1.0017040967941284
INFO:root:current mean train loss 2.159150469219865
INFO:root:current train perplexity1.0017046928405762
INFO:root:current mean train loss 2.158233974239614
INFO:root:current train perplexity1.0017037391662598
INFO:root:current mean train loss 2.1576470105566
INFO:root:current train perplexity1.0017036199569702

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.13s/it]
INFO:root:final mean train loss: 2.1573709116640924
INFO:root:final train perplexity: 1.0017040967941284
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.03s/it]
INFO:root:eval mean loss: 2.0837065256233758
INFO:root:eval perplexity: 1.0016876459121704
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.39s/it]
INFO:root:eval mean loss: 2.514418198707256
INFO:root:eval perplexity: 1.002069354057312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/8
  4%|â–         | 8/200 [1:28:42<34:48:37, 652.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.125111062186105
INFO:root:current train perplexity1.0016803741455078
INFO:root:current mean train loss 2.1288728811122755
INFO:root:current train perplexity1.0016778707504272
INFO:root:current mean train loss 2.122759876859949
INFO:root:current train perplexity1.0016716718673706
INFO:root:current mean train loss 2.117037532934502
INFO:root:current train perplexity1.0016679763793945
INFO:root:current mean train loss 2.1194116573224124
INFO:root:current train perplexity1.0016703605651855
INFO:root:current mean train loss 2.1211273545416716
INFO:root:current train perplexity1.001672625541687
INFO:root:current mean train loss 2.1189395517814815
INFO:root:current train perplexity1.001671314239502
INFO:root:current mean train loss 2.1194193572414166
INFO:root:current train perplexity1.0016721487045288
INFO:root:current mean train loss 2.11894711148953
INFO:root:current train perplexity1.0016727447509766
INFO:root:current mean train loss 2.1191750340283235
INFO:root:current train perplexity1.0016732215881348
INFO:root:current mean train loss 2.1200563911078634
INFO:root:current train perplexity1.0016738176345825
INFO:root:current mean train loss 2.1195374537144462
INFO:root:current train perplexity1.0016734600067139
INFO:root:current mean train loss 2.1202370367552104
INFO:root:current train perplexity1.0016751289367676
INFO:root:current mean train loss 2.1184637602795378
INFO:root:current train perplexity1.0016738176345825
INFO:root:current mean train loss 2.1171942969232487
INFO:root:current train perplexity1.0016728639602661
INFO:root:current mean train loss 2.1171189168377107
INFO:root:current train perplexity1.0016725063323975
INFO:root:current mean train loss 2.1160473195055576
INFO:root:current train perplexity1.0016716718673706
INFO:root:current mean train loss 2.1149700311831166
INFO:root:current train perplexity1.0016705989837646
INFO:root:current mean train loss 2.1149744081237336
INFO:root:current train perplexity1.001670479774475
INFO:root:current mean train loss 2.1148458787637163
INFO:root:current train perplexity1.0016701221466064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.92s/it]
INFO:root:final mean train loss: 2.1144913422358016
INFO:root:final train perplexity: 1.0016701221466064
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it]
INFO:root:eval mean loss: 2.0572812049946885
INFO:root:eval perplexity: 1.0016661882400513
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it]
INFO:root:eval mean loss: 2.4897088539515826
INFO:root:eval perplexity: 1.0020489692687988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/9
  4%|â–         | 9/200 [1:39:33<34:36:25, 652.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.0700523257255554
INFO:root:current train perplexity1.0016381740570068
INFO:root:current mean train loss 2.0850555802646435
INFO:root:current train perplexity1.0016499757766724
INFO:root:current mean train loss 2.077015945362666
INFO:root:current train perplexity1.001643180847168
INFO:root:current mean train loss 2.08190850710327
INFO:root:current train perplexity1.0016448497772217
INFO:root:current mean train loss 2.083783061103483
INFO:root:current train perplexity1.0016474723815918
INFO:root:current mean train loss 2.08431495445362
INFO:root:current train perplexity1.0016484260559082
INFO:root:current mean train loss 2.0866130065698565
INFO:root:current train perplexity1.001646637916565
INFO:root:current mean train loss 2.085753766146112
INFO:root:current train perplexity1.0016472339630127
INFO:root:current mean train loss 2.0851621851674826
INFO:root:current train perplexity1.0016467571258545
INFO:root:current mean train loss 2.084996690895377
INFO:root:current train perplexity1.0016475915908813
INFO:root:current mean train loss 2.086318071804119
INFO:root:current train perplexity1.0016480684280396
INFO:root:current mean train loss 2.083763533271849
INFO:root:current train perplexity1.001645565032959
INFO:root:current mean train loss 2.082084182542734
INFO:root:current train perplexity1.001644492149353
INFO:root:current mean train loss 2.0814341494493935
INFO:root:current train perplexity1.0016449689865112
INFO:root:current mean train loss 2.081683859105938
INFO:root:current train perplexity1.0016447305679321
INFO:root:current mean train loss 2.080978506926409
INFO:root:current train perplexity1.0016437768936157
INFO:root:current mean train loss 2.080292828940306
INFO:root:current train perplexity1.0016429424285889
INFO:root:current mean train loss 2.07922982550375
INFO:root:current train perplexity1.001641869544983
INFO:root:current mean train loss 2.0787023542766696
INFO:root:current train perplexity1.0016413927078247
INFO:root:current mean train loss 2.078262789083309
INFO:root:current train perplexity1.0016409158706665

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.33s/it]
INFO:root:final mean train loss: 2.0785956814982827
INFO:root:final train perplexity: 1.0016417503356934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.22s/it]
INFO:root:eval mean loss: 2.031677301471115
INFO:root:eval perplexity: 1.0016454458236694
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it]
INFO:root:eval mean loss: 2.4655130002515535
INFO:root:eval perplexity: 1.0020290613174438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/10
  5%|â–Œ         | 10/200 [1:50:22<34:22:17, 651.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.035014598265938
INFO:root:current train perplexity1.0016214847564697
INFO:root:current mean train loss 2.045485791370008
INFO:root:current train perplexity1.0016223192214966
INFO:root:current mean train loss 2.05242091838312
INFO:root:current train perplexity1.001626968383789
INFO:root:current mean train loss 2.050005322549401
INFO:root:current train perplexity1.0016202926635742
INFO:root:current mean train loss 2.0488244236659394
INFO:root:current train perplexity1.0016182661056519
INFO:root:current mean train loss 2.0486142637440525
INFO:root:current train perplexity1.0016199350357056
INFO:root:current mean train loss 2.0495052677989896
INFO:root:current train perplexity1.0016193389892578
INFO:root:current mean train loss 2.050297993057261
INFO:root:current train perplexity1.0016199350357056
INFO:root:current mean train loss 2.050559048411212
INFO:root:current train perplexity1.0016205310821533
INFO:root:current mean train loss 2.049565369868795
INFO:root:current train perplexity1.0016194581985474
INFO:root:current mean train loss 2.0508417182151573
INFO:root:current train perplexity1.0016201734542847
INFO:root:current mean train loss 2.050486903704569
INFO:root:current train perplexity1.0016200542449951
INFO:root:current mean train loss 2.049722272641458
INFO:root:current train perplexity1.001619577407837
INFO:root:current mean train loss 2.0504691188051547
INFO:root:current train perplexity1.0016196966171265
INFO:root:current mean train loss 2.0493406187197722
INFO:root:current train perplexity1.0016189813613892
INFO:root:current mean train loss 2.048550511561358
INFO:root:current train perplexity1.0016191005706787
INFO:root:current mean train loss 2.0472260570726113
INFO:root:current train perplexity1.001617431640625
INFO:root:current mean train loss 2.047205600649573
INFO:root:current train perplexity1.0016160011291504
INFO:root:current mean train loss 2.0469189079258134
INFO:root:current train perplexity1.0016157627105713
INFO:root:current mean train loss 2.047163109418521
INFO:root:current train perplexity1.0016164779663086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.91s/it]
INFO:root:final mean train loss: 2.0471793793093
INFO:root:final train perplexity: 1.0016169548034668
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.04s/it]
INFO:root:eval mean loss: 2.0127116974363934
INFO:root:eval perplexity: 1.0016300678253174
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.03s/it]
INFO:root:eval mean loss: 2.4468508861589093
INFO:root:eval perplexity: 1.0020136833190918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/11
  6%|â–Œ         | 11/200 [2:01:19<34:17:03, 653.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2.0075026154518127
INFO:root:current train perplexity1.0015846490859985
INFO:root:current mean train loss 2.0140380487647107
INFO:root:current train perplexity1.0015884637832642
INFO:root:current mean train loss 2.014697907687901
INFO:root:current train perplexity1.001590371131897
INFO:root:current mean train loss 2.016147419578671
INFO:root:current train perplexity1.001587152481079
INFO:root:current mean train loss 2.015050534112954
INFO:root:current train perplexity1.0015876293182373
INFO:root:current mean train loss 2.016015535735433
INFO:root:current train perplexity1.0015878677368164
INFO:root:current mean train loss 2.0179064301638143
INFO:root:current train perplexity1.0015891790390015
INFO:root:current mean train loss 2.0185568111240104
INFO:root:current train perplexity1.0015910863876343
INFO:root:current mean train loss 2.018370402720389
INFO:root:current train perplexity1.001590371131897
INFO:root:current mean train loss 2.0183740138523962
INFO:root:current train perplexity1.001591444015503
INFO:root:current mean train loss 2.0175994451313826
INFO:root:current train perplexity1.0015907287597656
INFO:root:current mean train loss 2.0179653878356714
INFO:root:current train perplexity1.0015907287597656
INFO:root:current mean train loss 2.0184438841339216
INFO:root:current train perplexity1.0015908479690552
INFO:root:current mean train loss 2.018461871026742
INFO:root:current train perplexity1.0015915632247925
INFO:root:current mean train loss 2.01883962906867
INFO:root:current train perplexity1.0015926361083984
INFO:root:current mean train loss 2.019079899727772
INFO:root:current train perplexity1.001592755317688
INFO:root:current mean train loss 2.019265703034995
INFO:root:current train perplexity1.0015941858291626
INFO:root:current mean train loss 2.0187950281787157
INFO:root:current train perplexity1.0015935897827148
INFO:root:current mean train loss 2.01884510446328
INFO:root:current train perplexity1.0015934705734253

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.97s/it]
INFO:root:final mean train loss: 2.018822805122841
INFO:root:final train perplexity: 1.0015945434570312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.95s/it]
INFO:root:eval mean loss: 1.9968369374883936
INFO:root:eval perplexity: 1.001617193222046
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.82s/it]
INFO:root:eval mean loss: 2.433165433559012
INFO:root:eval perplexity: 1.0020023584365845
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/12
  6%|â–Œ         | 12/200 [2:12:10<34:04:04, 652.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.9643912712732952
INFO:root:current train perplexity1.001564383506775
INFO:root:current mean train loss 2.004165804501876
INFO:root:current train perplexity1.0015861988067627
INFO:root:current mean train loss 1.9927582693804662
INFO:root:current train perplexity1.0015736818313599
INFO:root:current mean train loss 1.9923088066648729
INFO:root:current train perplexity1.0015794038772583
INFO:root:current mean train loss 1.9890597209741105
INFO:root:current train perplexity1.0015738010406494
INFO:root:current mean train loss 1.9907620457958275
INFO:root:current train perplexity1.0015754699707031
INFO:root:current mean train loss 1.991325017231614
INFO:root:current train perplexity1.0015772581100464
INFO:root:current mean train loss 1.992401678735128
INFO:root:current train perplexity1.0015772581100464
INFO:root:current mean train loss 1.99236983899607
INFO:root:current train perplexity1.001575231552124
INFO:root:current mean train loss 1.9941468534543532
INFO:root:current train perplexity1.00157630443573
INFO:root:current mean train loss 1.9931716217714197
INFO:root:current train perplexity1.0015738010406494
INFO:root:current mean train loss 1.9928521621475843
INFO:root:current train perplexity1.0015736818313599
INFO:root:current mean train loss 1.9941363559597647
INFO:root:current train perplexity1.0015748739242554
INFO:root:current mean train loss 1.9943417075589722
INFO:root:current train perplexity1.0015746355056763
INFO:root:current mean train loss 1.9932716969328954
INFO:root:current train perplexity1.0015729665756226
INFO:root:current mean train loss 1.99374590265854
INFO:root:current train perplexity1.001574158668518
INFO:root:current mean train loss 1.9935001997522317
INFO:root:current train perplexity1.001574158668518
INFO:root:current mean train loss 1.9937573984939074
INFO:root:current train perplexity1.0015745162963867
INFO:root:current mean train loss 1.99343848830121
INFO:root:current train perplexity1.0015747547149658
INFO:root:current mean train loss 1.9940622103446042
INFO:root:current train perplexity1.0015747547149658

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.16s/it]
INFO:root:final mean train loss: 1.9937378381516557
INFO:root:final train perplexity: 1.0015746355056763
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.82s/it]
INFO:root:eval mean loss: 1.9822578683812568
INFO:root:eval perplexity: 1.0016053915023804
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.24s/it]
INFO:root:eval mean loss: 2.424923969921491
INFO:root:eval perplexity: 1.00199556350708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/13
  6%|â–‹         | 13/200 [2:22:58<33:49:23, 651.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.9372805893421172
INFO:root:current train perplexity1.0015130043029785
INFO:root:current mean train loss 1.9811823695898056
INFO:root:current train perplexity1.001556396484375
INFO:root:current mean train loss 1.9690578899600288
INFO:root:current train perplexity1.0015510320663452
INFO:root:current mean train loss 1.969792876392603
INFO:root:current train perplexity1.001554012298584
INFO:root:current mean train loss 1.9714256522201357
INFO:root:current train perplexity1.0015511512756348
INFO:root:current mean train loss 1.9721696308025947
INFO:root:current train perplexity1.0015541315078735
INFO:root:current mean train loss 1.970736599737598
INFO:root:current train perplexity1.0015558004379272
INFO:root:current mean train loss 1.9708463865849706
INFO:root:current train perplexity1.0015562772750854
INFO:root:current mean train loss 1.9716733995007305
INFO:root:current train perplexity1.0015578269958496
INFO:root:current mean train loss 1.9725448073252387
INFO:root:current train perplexity1.0015591382980347
INFO:root:current mean train loss 1.972998118984933
INFO:root:current train perplexity1.0015604496002197
INFO:root:current mean train loss 1.9733891817075866
INFO:root:current train perplexity1.0015604496002197
INFO:root:current mean train loss 1.973489164133541
INFO:root:current train perplexity1.0015597343444824
INFO:root:current mean train loss 1.973165693337267
INFO:root:current train perplexity1.0015586614608765
INFO:root:current mean train loss 1.972390268889951
INFO:root:current train perplexity1.0015586614608765
INFO:root:current mean train loss 1.9727556250597302
INFO:root:current train perplexity1.0015591382980347
INFO:root:current mean train loss 1.9730165504379038
INFO:root:current train perplexity1.0015590190887451
INFO:root:current mean train loss 1.9724426117054252
INFO:root:current train perplexity1.0015581846237183
INFO:root:current mean train loss 1.9719495126834283
INFO:root:current train perplexity1.0015579462051392
INFO:root:current mean train loss 1.97119883261621
INFO:root:current train perplexity1.0015571117401123

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:50<00:00, 590.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:50<00:00, 590.06s/it]
INFO:root:final mean train loss: 1.971382243490195
INFO:root:final train perplexity: 1.0015569925308228
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.04s/it]
INFO:root:eval mean loss: 1.9673162390154304
INFO:root:eval perplexity: 1.0015932321548462
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.89s/it]
INFO:root:eval mean loss: 2.4079013263925595
INFO:root:eval perplexity: 1.0019816160202026
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/14
  7%|â–‹         | 14/200 [2:33:53<33:41:20, 652.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.9407138502275623
INFO:root:current train perplexity1.0015109777450562
INFO:root:current mean train loss 1.940334921335652
INFO:root:current train perplexity1.0015183687210083
INFO:root:current mean train loss 1.9469173623539728
INFO:root:current train perplexity1.001527190208435
INFO:root:current mean train loss 1.9453229879413232
INFO:root:current train perplexity1.0015299320220947
INFO:root:current mean train loss 1.9492699456160347
INFO:root:current train perplexity1.0015370845794678
INFO:root:current mean train loss 1.949162100946437
INFO:root:current train perplexity1.0015355348587036
INFO:root:current mean train loss 1.9503322588967005
INFO:root:current train perplexity1.001536250114441
INFO:root:current mean train loss 1.9509668573433865
INFO:root:current train perplexity1.0015360116958618
INFO:root:current mean train loss 1.9508912923657908
INFO:root:current train perplexity1.0015385150909424
INFO:root:current mean train loss 1.951939653179816
INFO:root:current train perplexity1.0015392303466797
INFO:root:current mean train loss 1.952663400708882
INFO:root:current train perplexity1.0015394687652588
INFO:root:current mean train loss 1.9526672044551048
INFO:root:current train perplexity1.001539945602417
INFO:root:current mean train loss 1.9524012493721015
INFO:root:current train perplexity1.0015395879745483
INFO:root:current mean train loss 1.9529751031899862
INFO:root:current train perplexity1.0015408992767334
INFO:root:current mean train loss 1.9520147923553828
INFO:root:current train perplexity1.0015407800674438
INFO:root:current mean train loss 1.95223474634415
INFO:root:current train perplexity1.0015407800674438
INFO:root:current mean train loss 1.9520677871360383
INFO:root:current train perplexity1.0015405416488647
INFO:root:current mean train loss 1.9507247223961126
INFO:root:current train perplexity1.0015398263931274
INFO:root:current mean train loss 1.9507895806722169
INFO:root:current train perplexity1.001539945602417
INFO:root:current mean train loss 1.9514243000859628
INFO:root:current train perplexity1.001541018486023

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.54s/it]
INFO:root:final mean train loss: 1.951095154293847
INFO:root:final train perplexity: 1.001541018486023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.39s/it]
INFO:root:eval mean loss: 1.959043653298777
INFO:root:eval perplexity: 1.0015865564346313
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.14s/it]
INFO:root:eval mean loss: 2.407465539925487
INFO:root:eval perplexity: 1.001981258392334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/15
  8%|â–Š         | 15/200 [2:44:46<33:31:30, 652.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.9114644902723807
INFO:root:current train perplexity1.0015203952789307
INFO:root:current mean train loss 1.9225720120714855
INFO:root:current train perplexity1.001525640487671
INFO:root:current mean train loss 1.9284814032982653
INFO:root:current train perplexity1.001530408859253
INFO:root:current mean train loss 1.931145148088703
INFO:root:current train perplexity1.0015276670455933
INFO:root:current mean train loss 1.928126482448914
INFO:root:current train perplexity1.0015231370925903
INFO:root:current mean train loss 1.9290872013956202
INFO:root:current train perplexity1.001524806022644
INFO:root:current mean train loss 1.928693198100507
INFO:root:current train perplexity1.0015250444412231
INFO:root:current mean train loss 1.9293051426859686
INFO:root:current train perplexity1.001525640487671
INFO:root:current mean train loss 1.9305842577433976
INFO:root:current train perplexity1.0015268325805664
INFO:root:current mean train loss 1.931046010313294
INFO:root:current train perplexity1.0015240907669067
INFO:root:current mean train loss 1.9325373611594965
INFO:root:current train perplexity1.0015263557434082
INFO:root:current mean train loss 1.933821008771503
INFO:root:current train perplexity1.0015274286270142
INFO:root:current mean train loss 1.933157978730909
INFO:root:current train perplexity1.0015274286270142
INFO:root:current mean train loss 1.9326204202375876
INFO:root:current train perplexity1.0015259981155396
INFO:root:current mean train loss 1.9325504175898462
INFO:root:current train perplexity1.0015251636505127
INFO:root:current mean train loss 1.9326381417928669
INFO:root:current train perplexity1.0015252828598022
INFO:root:current mean train loss 1.9327438621451807
INFO:root:current train perplexity1.0015255212783813
INFO:root:current mean train loss 1.9327400684220761
INFO:root:current train perplexity1.001525640487671
INFO:root:current mean train loss 1.9323041421052836
INFO:root:current train perplexity1.001524806022644
INFO:root:current mean train loss 1.9316025051758257
INFO:root:current train perplexity1.0015249252319336

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:54<00:00, 594.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:54<00:00, 594.44s/it]
INFO:root:final mean train loss: 1.931415108136318
INFO:root:final train perplexity: 1.0015254020690918
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it]
INFO:root:eval mean loss: 1.9481109811904582
INFO:root:eval perplexity: 1.0015777349472046
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.94s/it]
INFO:root:eval mean loss: 2.3958483207310346
INFO:root:eval perplexity: 1.0019716024398804
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/16
  8%|â–Š         | 16/200 [2:55:44<33:25:43, 654.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.92610253582538
INFO:root:current train perplexity1.0015212297439575
INFO:root:current mean train loss 1.9256549467120254
INFO:root:current train perplexity1.0015236139297485
INFO:root:current mean train loss 1.9182194726493527
INFO:root:current train perplexity1.001516580581665
INFO:root:current mean train loss 1.917848999930842
INFO:root:current train perplexity1.001518726348877
INFO:root:current mean train loss 1.913233672737316
INFO:root:current train perplexity1.0015144348144531
INFO:root:current mean train loss 1.9131840559313051
INFO:root:current train perplexity1.0015137195587158
INFO:root:current mean train loss 1.9102454780643991
INFO:root:current train perplexity1.0015103816986084
INFO:root:current mean train loss 1.9120639958424945
INFO:root:current train perplexity1.0015122890472412
INFO:root:current mean train loss 1.91165421039174
INFO:root:current train perplexity1.001512885093689
INFO:root:current mean train loss 1.9106652701060631
INFO:root:current train perplexity1.001510500907898
INFO:root:current mean train loss 1.9118688378124788
INFO:root:current train perplexity1.0015110969543457
INFO:root:current mean train loss 1.9116476219398766
INFO:root:current train perplexity1.0015109777450562
INFO:root:current mean train loss 1.912392943698904
INFO:root:current train perplexity1.001511812210083
INFO:root:current mean train loss 1.9116711936494366
INFO:root:current train perplexity1.001511573791504
INFO:root:current mean train loss 1.912516111099015
INFO:root:current train perplexity1.001510739326477
INFO:root:current mean train loss 1.9133558431907036
INFO:root:current train perplexity1.0015122890472412
INFO:root:current mean train loss 1.9136216831663853
INFO:root:current train perplexity1.001511573791504
INFO:root:current mean train loss 1.913295047825706
INFO:root:current train perplexity1.001511573791504
INFO:root:current mean train loss 1.9141848854429897
INFO:root:current train perplexity1.001512050628662
INFO:root:current mean train loss 1.9142301408002245
INFO:root:current train perplexity1.0015113353729248

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.79s/it]
INFO:root:final mean train loss: 1.9141873486764613
INFO:root:final train perplexity: 1.001511812210083
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it]
INFO:root:eval mean loss: 1.939865157113853
INFO:root:eval perplexity: 1.0015710592269897
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.41s/it]
INFO:root:eval mean loss: 2.38933830269685
INFO:root:eval perplexity: 1.0019662380218506
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/17
  8%|â–Š         | 17/200 [3:06:34<33:11:05, 652.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8778827515515415
INFO:root:current train perplexity1.0014820098876953
INFO:root:current mean train loss 1.8844712186366954
INFO:root:current train perplexity1.0014898777008057
INFO:root:current mean train loss 1.887797173526552
INFO:root:current train perplexity1.0014913082122803
INFO:root:current mean train loss 1.8911673724651337
INFO:root:current train perplexity1.0014944076538086
INFO:root:current mean train loss 1.8929936167646626
INFO:root:current train perplexity1.0014961957931519
INFO:root:current mean train loss 1.8961458412968382
INFO:root:current train perplexity1.0015003681182861
INFO:root:current mean train loss 1.8959650327992994
INFO:root:current train perplexity1.001501441001892
INFO:root:current mean train loss 1.8957655004740972
INFO:root:current train perplexity1.0015016794204712
INFO:root:current mean train loss 1.895341150395505
INFO:root:current train perplexity1.0015004873275757
INFO:root:current mean train loss 1.8964074768035517
INFO:root:current train perplexity1.0014996528625488
INFO:root:current mean train loss 1.894687228343066
INFO:root:current train perplexity1.0014981031417847
INFO:root:current mean train loss 1.8945388156757612
INFO:root:current train perplexity1.0014979839324951
INFO:root:current mean train loss 1.8951317106159578
INFO:root:current train perplexity1.0014972686767578
INFO:root:current mean train loss 1.8937274134296505
INFO:root:current train perplexity1.0014959573745728
INFO:root:current mean train loss 1.8943502222017576
INFO:root:current train perplexity1.00149667263031
INFO:root:current mean train loss 1.8952538099336984
INFO:root:current train perplexity1.00149667263031
INFO:root:current mean train loss 1.8957494563683515
INFO:root:current train perplexity1.0014973878860474
INFO:root:current mean train loss 1.8966238752707538
INFO:root:current train perplexity1.001497507095337
INFO:root:current mean train loss 1.8967475756623988
INFO:root:current train perplexity1.0014972686767578

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.24s/it]
INFO:root:final mean train loss: 1.897170217783837
INFO:root:final train perplexity: 1.0014983415603638
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.47s/it]
INFO:root:eval mean loss: 1.9323326074485239
INFO:root:eval perplexity: 1.001564860343933
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.40s/it]
INFO:root:eval mean loss: 2.3832302004732986
INFO:root:eval perplexity: 1.0019612312316895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/18
  9%|â–‰         | 18/200 [3:17:23<32:56:54, 651.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8799817085266113
INFO:root:current train perplexity1.0014147758483887
INFO:root:current mean train loss 1.877214395432245
INFO:root:current train perplexity1.0014795064926147
INFO:root:current mean train loss 1.8755901505307453
INFO:root:current train perplexity1.001473069190979
INFO:root:current mean train loss 1.877194960000085
INFO:root:current train perplexity1.0014797449111938
INFO:root:current mean train loss 1.876313387317422
INFO:root:current train perplexity1.0014804601669312
INFO:root:current mean train loss 1.877795165364105
INFO:root:current train perplexity1.0014777183532715
INFO:root:current mean train loss 1.8792212624195193
INFO:root:current train perplexity1.0014801025390625
INFO:root:current mean train loss 1.8788451394290788
INFO:root:current train perplexity1.0014797449111938
INFO:root:current mean train loss 1.8796621599552794
INFO:root:current train perplexity1.0014814138412476
INFO:root:current mean train loss 1.8801119650266447
INFO:root:current train perplexity1.001482605934143
INFO:root:current mean train loss 1.880850160299842
INFO:root:current train perplexity1.0014827251434326
INFO:root:current mean train loss 1.8805257776743687
INFO:root:current train perplexity1.0014837980270386
INFO:root:current mean train loss 1.8820561629607964
INFO:root:current train perplexity1.001484990119934
INFO:root:current mean train loss 1.8828512013643637
INFO:root:current train perplexity1.0014853477478027
INFO:root:current mean train loss 1.8824951107391683
INFO:root:current train perplexity1.0014845132827759
INFO:root:current mean train loss 1.8823532656577735
INFO:root:current train perplexity1.0014841556549072
INFO:root:current mean train loss 1.8826104144069635
INFO:root:current train perplexity1.0014848709106445
INFO:root:current mean train loss 1.8828619413711458
INFO:root:current train perplexity1.0014851093292236
INFO:root:current mean train loss 1.8830888393513054
INFO:root:current train perplexity1.0014863014221191
INFO:root:current mean train loss 1.8822407144261157
INFO:root:current train perplexity1.0014865398406982

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.59s/it]
INFO:root:final mean train loss: 1.8821633472264685
INFO:root:final train perplexity: 1.0014865398406982
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.80s/it]
INFO:root:eval mean loss: 1.9259800175403028
INFO:root:eval perplexity: 1.0015597343444824
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.33s/it]
INFO:root:eval mean loss: 2.376733660697937
INFO:root:eval perplexity: 1.0019558668136597
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/19
 10%|â–‰         | 19/200 [3:28:14<32:45:09, 651.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8547488396817988
INFO:root:current train perplexity1.0014686584472656
INFO:root:current mean train loss 1.8642710281200097
INFO:root:current train perplexity1.0014736652374268
INFO:root:current mean train loss 1.868932848041122
INFO:root:current train perplexity1.0014783143997192
INFO:root:current mean train loss 1.8647124870963718
INFO:root:current train perplexity1.0014783143997192
INFO:root:current mean train loss 1.8645388588521152
INFO:root:current train perplexity1.001476526260376
INFO:root:current mean train loss 1.8650402122987184
INFO:root:current train perplexity1.001476526260376
INFO:root:current mean train loss 1.864360411450794
INFO:root:current train perplexity1.0014750957489014
INFO:root:current mean train loss 1.8642743543574685
INFO:root:current train perplexity1.0014724731445312
INFO:root:current mean train loss 1.8638376815475686
INFO:root:current train perplexity1.001471996307373
INFO:root:current mean train loss 1.8647632319601395
INFO:root:current train perplexity1.001471996307373
INFO:root:current mean train loss 1.8644509392475195
INFO:root:current train perplexity1.0014708042144775
INFO:root:current mean train loss 1.8649683638265002
INFO:root:current train perplexity1.0014723539352417
INFO:root:current mean train loss 1.8642997431481918
INFO:root:current train perplexity1.0014715194702148
INFO:root:current mean train loss 1.8642900120833277
INFO:root:current train perplexity1.0014728307724
INFO:root:current mean train loss 1.8652640935740894
INFO:root:current train perplexity1.0014725923538208
INFO:root:current mean train loss 1.865217919105299
INFO:root:current train perplexity1.0014727115631104
INFO:root:current mean train loss 1.8657133191934203
INFO:root:current train perplexity1.0014734268188477
INFO:root:current mean train loss 1.866353251169783
INFO:root:current train perplexity1.001473307609558
INFO:root:current mean train loss 1.8665411045731881
INFO:root:current train perplexity1.0014735460281372
INFO:root:current mean train loss 1.86681575321134
INFO:root:current train perplexity1.0014746189117432

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.89s/it]
INFO:root:final mean train loss: 1.8674953420295657
INFO:root:final train perplexity: 1.0014748573303223
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it]
INFO:root:eval mean loss: 1.9154014304174598
INFO:root:eval perplexity: 1.0015511512756348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.78s/it]
INFO:root:eval mean loss: 2.3712377015580524
INFO:root:eval perplexity: 1.0019513368606567
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/20
 10%|â–ˆ         | 20/200 [3:39:07<32:35:57, 651.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8591225972542396
INFO:root:current train perplexity1.001452922821045
INFO:root:current mean train loss 1.8512574691566632
INFO:root:current train perplexity1.001448631286621
INFO:root:current mean train loss 1.8501649426615887
INFO:root:current train perplexity1.001454472541809
INFO:root:current mean train loss 1.8512754855254407
INFO:root:current train perplexity1.0014592409133911
INFO:root:current mean train loss 1.8521532412270478
INFO:root:current train perplexity1.0014615058898926
INFO:root:current mean train loss 1.8516570026666646
INFO:root:current train perplexity1.0014612674713135
INFO:root:current mean train loss 1.850155641961732
INFO:root:current train perplexity1.0014585256576538
INFO:root:current mean train loss 1.8523217167034847
INFO:root:current train perplexity1.0014622211456299
INFO:root:current mean train loss 1.8530487143240328
INFO:root:current train perplexity1.0014630556106567
INFO:root:current mean train loss 1.85277815037769
INFO:root:current train perplexity1.0014631748199463
INFO:root:current mean train loss 1.8527879558926712
INFO:root:current train perplexity1.0014625787734985
INFO:root:current mean train loss 1.8515482821728286
INFO:root:current train perplexity1.0014617443084717
INFO:root:current mean train loss 1.8518771129428042
INFO:root:current train perplexity1.0014617443084717
INFO:root:current mean train loss 1.852613337810935
INFO:root:current train perplexity1.0014623403549194
INFO:root:current mean train loss 1.8531139558019367
INFO:root:current train perplexity1.001462697982788
INFO:root:current mean train loss 1.8528214504069676
INFO:root:current train perplexity1.0014618635177612
INFO:root:current mean train loss 1.8524249261875863
INFO:root:current train perplexity1.0014612674713135
INFO:root:current mean train loss 1.8527363132237702
INFO:root:current train perplexity1.0014623403549194
INFO:root:current mean train loss 1.853153319317339
INFO:root:current train perplexity1.0014629364013672
INFO:root:current mean train loss 1.8534831468317545
INFO:root:current train perplexity1.001463532447815

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.95s/it]
INFO:root:final mean train loss: 1.853476436103286
INFO:root:final train perplexity: 1.001463770866394
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.40s/it]
INFO:root:eval mean loss: 1.9101760154920266
INFO:root:eval perplexity: 1.0015469789505005
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it]
INFO:root:eval mean loss: 2.3647844022047435
INFO:root:eval perplexity: 1.0019460916519165
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/21
 10%|â–ˆ         | 21/200 [3:49:56<32:22:27, 651.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8441058205706733
INFO:root:current train perplexity1.0014599561691284
INFO:root:current mean train loss 1.8495077742980077
INFO:root:current train perplexity1.001458764076233
INFO:root:current mean train loss 1.8471123753115535
INFO:root:current train perplexity1.0014643669128418
INFO:root:current mean train loss 1.8465769100055267
INFO:root:current train perplexity1.0014638900756836
INFO:root:current mean train loss 1.842853662476205
INFO:root:current train perplexity1.0014606714248657
INFO:root:current mean train loss 1.840774327730961
INFO:root:current train perplexity1.0014570951461792
INFO:root:current mean train loss 1.8404506052412637
INFO:root:current train perplexity1.0014557838439941
INFO:root:current mean train loss 1.8404172337244427
INFO:root:current train perplexity1.0014557838439941
INFO:root:current mean train loss 1.8409248615536735
INFO:root:current train perplexity1.001456379890442
INFO:root:current mean train loss 1.8411084755195235
INFO:root:current train perplexity1.0014574527740479
INFO:root:current mean train loss 1.842313861982389
INFO:root:current train perplexity1.0014568567276
INFO:root:current mean train loss 1.840774374746534
INFO:root:current train perplexity1.0014550685882568
INFO:root:current mean train loss 1.8396866367121412
INFO:root:current train perplexity1.0014536380767822
INFO:root:current mean train loss 1.8398123219125742
INFO:root:current train perplexity1.0014533996582031
INFO:root:current mean train loss 1.8405950767817079
INFO:root:current train perplexity1.0014537572860718
INFO:root:current mean train loss 1.8407348252660511
INFO:root:current train perplexity1.0014533996582031
INFO:root:current mean train loss 1.8403518960095835
INFO:root:current train perplexity1.0014533996582031
INFO:root:current mean train loss 1.8405505039290035
INFO:root:current train perplexity1.0014537572860718
INFO:root:current mean train loss 1.8406249986126506
INFO:root:current train perplexity1.0014533996582031
INFO:root:current mean train loss 1.8406159431656446
INFO:root:current train perplexity1.0014535188674927

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.18s/it]
INFO:root:final mean train loss: 1.8404079444229273
INFO:root:final train perplexity: 1.0014535188674927
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.20s/it]
INFO:root:eval mean loss: 1.9064047209759976
INFO:root:eval perplexity: 1.0015438795089722
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.16s/it]
INFO:root:eval mean loss: 2.365198238098875
INFO:root:eval perplexity: 1.0019463300704956
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/22
 11%|â–ˆ         | 22/200 [4:00:46<32:10:20, 650.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8373993226926622
INFO:root:current train perplexity1.0014491081237793
INFO:root:current mean train loss 1.832592702325369
INFO:root:current train perplexity1.0014474391937256
INFO:root:current mean train loss 1.8270097617264633
INFO:root:current train perplexity1.0014394521713257
INFO:root:current mean train loss 1.8253723986986177
INFO:root:current train perplexity1.0014406442642212
INFO:root:current mean train loss 1.8240228271383563
INFO:root:current train perplexity1.0014392137527466
INFO:root:current mean train loss 1.8242568033527953
INFO:root:current train perplexity1.0014402866363525
INFO:root:current mean train loss 1.8239937875458603
INFO:root:current train perplexity1.0014402866363525
INFO:root:current mean train loss 1.8235126929634755
INFO:root:current train perplexity1.0014396905899048
INFO:root:current mean train loss 1.8245187305502875
INFO:root:current train perplexity1.0014405250549316
INFO:root:current mean train loss 1.826440073235797
INFO:root:current train perplexity1.001442313194275
INFO:root:current mean train loss 1.8263263996805084
INFO:root:current train perplexity1.001442790031433
INFO:root:current mean train loss 1.8270665642957045
INFO:root:current train perplexity1.0014444589614868
INFO:root:current mean train loss 1.8274279815047534
INFO:root:current train perplexity1.001443862915039
INFO:root:current mean train loss 1.82744482860718
INFO:root:current train perplexity1.0014437437057495
INFO:root:current mean train loss 1.8282394903659498
INFO:root:current train perplexity1.0014443397521973
INFO:root:current mean train loss 1.8280702037411136
INFO:root:current train perplexity1.0014443397521973
INFO:root:current mean train loss 1.8287362250751602
INFO:root:current train perplexity1.001444697380066
INFO:root:current mean train loss 1.8289180643062302
INFO:root:current train perplexity1.0014445781707764
INFO:root:current mean train loss 1.8289054282132915
INFO:root:current train perplexity1.0014444589614868
INFO:root:current mean train loss 1.8278171131590328
INFO:root:current train perplexity1.0014430284500122

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.88s/it]
INFO:root:final mean train loss: 1.8278267502003225
INFO:root:final train perplexity: 1.0014435052871704
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.26s/it]
INFO:root:eval mean loss: 1.9031744793797216
INFO:root:eval perplexity: 1.001541256904602
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.54s/it]
INFO:root:eval mean loss: 2.3657091057046933
INFO:root:eval perplexity: 1.0019468069076538
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/23
 12%|â–ˆâ–        | 23/200 [4:11:39<32:02:11, 651.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.8024910979800755
INFO:root:current train perplexity1.0014275312423706
INFO:root:current mean train loss 1.806396434181615
INFO:root:current train perplexity1.0014301538467407
INFO:root:current mean train loss 1.8087920419101058
INFO:root:current train perplexity1.0014336109161377
INFO:root:current mean train loss 1.8123274277418087
INFO:root:current train perplexity1.0014333724975586
INFO:root:current mean train loss 1.8110242004297217
INFO:root:current train perplexity1.001430630683899
INFO:root:current mean train loss 1.8112159112752495
INFO:root:current train perplexity1.001429557800293
INFO:root:current mean train loss 1.8119027547214341
INFO:root:current train perplexity1.0014296770095825
INFO:root:current mean train loss 1.8113499398472943
INFO:root:current train perplexity1.0014280080795288
INFO:root:current mean train loss 1.8117802073446552
INFO:root:current train perplexity1.0014278888702393
INFO:root:current mean train loss 1.8108176094112973
INFO:root:current train perplexity1.0014272928237915
INFO:root:current mean train loss 1.8117284304505095
INFO:root:current train perplexity1.0014286041259766
INFO:root:current mean train loss 1.8118760438526378
INFO:root:current train perplexity1.001429557800293
INFO:root:current mean train loss 1.8125512711761533
INFO:root:current train perplexity1.0014305114746094
INFO:root:current mean train loss 1.8128802463305083
INFO:root:current train perplexity1.0014311075210571
INFO:root:current mean train loss 1.8141674180158833
INFO:root:current train perplexity1.0014328956604004
INFO:root:current mean train loss 1.814498759440656
INFO:root:current train perplexity1.0014322996139526
INFO:root:current mean train loss 1.8152418940024968
INFO:root:current train perplexity1.0014331340789795
INFO:root:current mean train loss 1.8155624833186912
INFO:root:current train perplexity1.0014337301254272
INFO:root:current mean train loss 1.815289209254835
INFO:root:current train perplexity1.0014336109161377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.64s/it]
INFO:root:final mean train loss: 1.8160629206934904
INFO:root:final train perplexity: 1.0014342069625854
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.57s/it]
INFO:root:eval mean loss: 1.8990404601638198
INFO:root:eval perplexity: 1.0015379190444946
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.52s/it]
INFO:root:eval mean loss: 2.3623808588541992
INFO:root:eval perplexity: 1.0019440650939941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/24
 12%|â–ˆâ–        | 24/200 [4:22:29<31:49:45, 651.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.819287589618138
INFO:root:current train perplexity1.0014318227767944
INFO:root:current mean train loss 1.8056393505256867
INFO:root:current train perplexity1.0014249086380005
INFO:root:current mean train loss 1.8061286475923326
INFO:root:current train perplexity1.0014281272888184
INFO:root:current mean train loss 1.80483586236786
INFO:root:current train perplexity1.0014216899871826
INFO:root:current mean train loss 1.8056288096477124
INFO:root:current train perplexity1.0014231204986572
INFO:root:current mean train loss 1.804867188135783
INFO:root:current train perplexity1.0014244318008423
INFO:root:current mean train loss 1.8053656335324786
INFO:root:current train perplexity1.0014262199401855
INFO:root:current mean train loss 1.8054862423698501
INFO:root:current train perplexity1.0014252662658691
INFO:root:current mean train loss 1.8052750078423463
INFO:root:current train perplexity1.0014244318008423
INFO:root:current mean train loss 1.8040804406905253
INFO:root:current train perplexity1.0014255046844482
INFO:root:current mean train loss 1.8036964137362392
INFO:root:current train perplexity1.0014245510101318
INFO:root:current mean train loss 1.8029224468763605
INFO:root:current train perplexity1.0014227628707886
INFO:root:current mean train loss 1.8024133988027249
INFO:root:current train perplexity1.0014221668243408
INFO:root:current mean train loss 1.8031314416249107
INFO:root:current train perplexity1.00142240524292
INFO:root:current mean train loss 1.8040335668937992
INFO:root:current train perplexity1.0014230012893677
INFO:root:current mean train loss 1.8044597684901997
INFO:root:current train perplexity1.0014230012893677
INFO:root:current mean train loss 1.8048216924952214
INFO:root:current train perplexity1.0014240741729736
INFO:root:current mean train loss 1.8051580614560379
INFO:root:current train perplexity1.0014249086380005
INFO:root:current mean train loss 1.8054012192766773
INFO:root:current train perplexity1.0014256238937378
INFO:root:current mean train loss 1.8052995284940163
INFO:root:current train perplexity1.0014259815216064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.62s/it]
INFO:root:final mean train loss: 1.8045637444661111
INFO:root:final train perplexity: 1.0014251470565796
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.72s/it]
INFO:root:eval mean loss: 1.8915344726109335
INFO:root:eval perplexity: 1.0015318393707275
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 32.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 32.00s/it]
INFO:root:eval mean loss: 2.3544128291995814
INFO:root:eval perplexity: 1.0019375085830688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/25
 12%|â–ˆâ–Ž        | 25/200 [4:33:20<31:39:12, 651.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.771916891137759
INFO:root:current train perplexity1.0013964176177979
INFO:root:current mean train loss 1.7765528934617196
INFO:root:current train perplexity1.0014084577560425
INFO:root:current mean train loss 1.779555750212499
INFO:root:current train perplexity1.0014103651046753
INFO:root:current mean train loss 1.7846753107912747
INFO:root:current train perplexity1.0014104843139648
INFO:root:current mean train loss 1.7894696570792288
INFO:root:current train perplexity1.0014159679412842
INFO:root:current mean train loss 1.7902451612567174
INFO:root:current train perplexity1.0014164447784424
INFO:root:current mean train loss 1.7892815879522226
INFO:root:current train perplexity1.00141441822052
INFO:root:current mean train loss 1.7895078104174598
INFO:root:current train perplexity1.0014138221740723
INFO:root:current mean train loss 1.7894161514286857
INFO:root:current train perplexity1.001414179801941
INFO:root:current mean train loss 1.7900812796183996
INFO:root:current train perplexity1.0014147758483887
INFO:root:current mean train loss 1.7917941524647176
INFO:root:current train perplexity1.001415729522705
INFO:root:current mean train loss 1.7925946677494728
INFO:root:current train perplexity1.001416563987732
INFO:root:current mean train loss 1.7939192525117227
INFO:root:current train perplexity1.0014171600341797
INFO:root:current mean train loss 1.7929781762674857
INFO:root:current train perplexity1.0014163255691528
INFO:root:current mean train loss 1.7933924898002924
INFO:root:current train perplexity1.001415491104126
INFO:root:current mean train loss 1.794086741806641
INFO:root:current train perplexity1.0014159679412842
INFO:root:current mean train loss 1.7935930033182275
INFO:root:current train perplexity1.0014150142669678
INFO:root:current mean train loss 1.793772513517924
INFO:root:current train perplexity1.0014151334762573
INFO:root:current mean train loss 1.7937866018660236
INFO:root:current train perplexity1.001415729522705
INFO:root:current mean train loss 1.7933005168879106
INFO:root:current train perplexity1.0014158487319946

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.37s/it]
INFO:root:final mean train loss: 1.7932036706911934
INFO:root:final train perplexity: 1.0014162063598633
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.66s/it]
INFO:root:eval mean loss: 1.8923244751091544
INFO:root:eval perplexity: 1.0015324354171753
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.86s/it]
INFO:root:eval mean loss: 2.356576608005145
INFO:root:eval perplexity: 1.001939296722412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/26
 13%|â–ˆâ–Ž        | 26/200 [4:44:11<31:28:12, 651.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7885753439693917
INFO:root:current train perplexity1.0014225244522095
INFO:root:current mean train loss 1.7788292465480506
INFO:root:current train perplexity1.0014070272445679
INFO:root:current mean train loss 1.7796902790109153
INFO:root:current train perplexity1.001406192779541
INFO:root:current mean train loss 1.7761634944820683
INFO:root:current train perplexity1.001402497291565
INFO:root:current mean train loss 1.7776140896371162
INFO:root:current train perplexity1.0014039278030396
INFO:root:current mean train loss 1.7776135393519938
INFO:root:current train perplexity1.0014039278030396
INFO:root:current mean train loss 1.7784874318356447
INFO:root:current train perplexity1.0014052391052246
INFO:root:current mean train loss 1.7802113193249414
INFO:root:current train perplexity1.0014064311981201
INFO:root:current mean train loss 1.7812995879460174
INFO:root:current train perplexity1.0014070272445679
INFO:root:current mean train loss 1.780169491306755
INFO:root:current train perplexity1.0014058351516724
INFO:root:current mean train loss 1.7813185850779216
INFO:root:current train perplexity1.0014070272445679
INFO:root:current mean train loss 1.7818021255036804
INFO:root:current train perplexity1.0014077425003052
INFO:root:current mean train loss 1.7808782642258452
INFO:root:current train perplexity1.0014070272445679
INFO:root:current mean train loss 1.7816450226049116
INFO:root:current train perplexity1.0014078617095947
INFO:root:current mean train loss 1.7819368042605028
INFO:root:current train perplexity1.0014079809188843
INFO:root:current mean train loss 1.7824593706613698
INFO:root:current train perplexity1.0014078617095947
INFO:root:current mean train loss 1.7833820991876428
INFO:root:current train perplexity1.001408338546753
INFO:root:current mean train loss 1.7829918697056997
INFO:root:current train perplexity1.0014078617095947
INFO:root:current mean train loss 1.7825809212756636
INFO:root:current train perplexity1.001407504081726
INFO:root:current mean train loss 1.7830257891134649
INFO:root:current train perplexity1.0014078617095947

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.28s/it]
INFO:root:final mean train loss: 1.7829898011486036
INFO:root:final train perplexity: 1.0014081001281738
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.89s/it]
INFO:root:eval mean loss: 1.8870935051153737
INFO:root:eval perplexity: 1.001528263092041
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.06s/it]
INFO:root:eval mean loss: 2.3541390392797212
INFO:root:eval perplexity: 1.0019372701644897
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/27
 14%|â–ˆâ–Ž        | 27/200 [4:55:03<31:17:30, 651.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7738269690809578
INFO:root:current train perplexity1.001396894454956
INFO:root:current mean train loss 1.771216471738453
INFO:root:current train perplexity1.0013984441757202
INFO:root:current mean train loss 1.7678075335746588
INFO:root:current train perplexity1.0013889074325562
INFO:root:current mean train loss 1.7668542535611371
INFO:root:current train perplexity1.0013892650604248
INFO:root:current mean train loss 1.7670143262267632
INFO:root:current train perplexity1.0013909339904785
INFO:root:current mean train loss 1.7670065890930886
INFO:root:current train perplexity1.0013912916183472
INFO:root:current mean train loss 1.7675501497683193
INFO:root:current train perplexity1.0013914108276367
INFO:root:current mean train loss 1.7665523372413614
INFO:root:current train perplexity1.0013906955718994
INFO:root:current mean train loss 1.7673038417920643
INFO:root:current train perplexity1.0013933181762695
INFO:root:current mean train loss 1.7679258288819506
INFO:root:current train perplexity1.0013947486877441
INFO:root:current mean train loss 1.7685774872343571
INFO:root:current train perplexity1.0013960599899292
INFO:root:current mean train loss 1.7698409563519177
INFO:root:current train perplexity1.0013972520828247
INFO:root:current mean train loss 1.7708046829567805
INFO:root:current train perplexity1.0013988018035889
INFO:root:current mean train loss 1.7712746148432477
INFO:root:current train perplexity1.0013986825942993
INFO:root:current mean train loss 1.772703244689397
INFO:root:current train perplexity1.0013998746871948
INFO:root:current mean train loss 1.7719081525472071
INFO:root:current train perplexity1.0013986825942993
INFO:root:current mean train loss 1.772098397173266
INFO:root:current train perplexity1.0013983249664307
INFO:root:current mean train loss 1.7722962338634183
INFO:root:current train perplexity1.001399040222168
INFO:root:current mean train loss 1.7717844800877238
INFO:root:current train perplexity1.0013989210128784
INFO:root:current mean train loss 1.7719732126006067
INFO:root:current train perplexity1.001399278640747

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.37s/it]
INFO:root:final mean train loss: 1.771894323964102
INFO:root:final train perplexity: 1.0013993978500366
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 1.8845091267680445
INFO:root:eval perplexity: 1.001526117324829
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it]
INFO:root:eval mean loss: 2.3536471703373794
INFO:root:eval perplexity: 1.001936912536621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/28
 14%|â–ˆâ–        | 28/200 [5:05:51<31:04:31, 650.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7550073623657227
INFO:root:current train perplexity1.0013898611068726
INFO:root:current mean train loss 1.7552184282030379
INFO:root:current train perplexity1.0013856887817383
INFO:root:current mean train loss 1.7553132924166592
INFO:root:current train perplexity1.0013846158981323
INFO:root:current mean train loss 1.7591741180419922
INFO:root:current train perplexity1.0013867616653442
INFO:root:current mean train loss 1.7597014750932392
INFO:root:current train perplexity1.0013883113861084
INFO:root:current mean train loss 1.762142443449601
INFO:root:current train perplexity1.0013909339904785
INFO:root:current mean train loss 1.7621991461294668
INFO:root:current train perplexity1.0013914108276367
INFO:root:current mean train loss 1.7619195212087324
INFO:root:current train perplexity1.0013904571533203
INFO:root:current mean train loss 1.7631783753803798
INFO:root:current train perplexity1.0013904571533203
INFO:root:current mean train loss 1.7639320213366778
INFO:root:current train perplexity1.0013916492462158
INFO:root:current mean train loss 1.7645358531419622
INFO:root:current train perplexity1.001391887664795
INFO:root:current mean train loss 1.7634086950789107
INFO:root:current train perplexity1.0013917684555054
INFO:root:current mean train loss 1.762854723930359
INFO:root:current train perplexity1.0013914108276367
INFO:root:current mean train loss 1.7619261373173107
INFO:root:current train perplexity1.0013911724090576
INFO:root:current mean train loss 1.7618025964801594
INFO:root:current train perplexity1.0013915300369263
INFO:root:current mean train loss 1.7622925977858286
INFO:root:current train perplexity1.0013922452926636
INFO:root:current mean train loss 1.761682236087856
INFO:root:current train perplexity1.001391887664795
INFO:root:current mean train loss 1.761916498600597
INFO:root:current train perplexity1.0013917684555054
INFO:root:current mean train loss 1.7619275424321492
INFO:root:current train perplexity1.0013916492462158
INFO:root:current mean train loss 1.7620469586456877
INFO:root:current train perplexity1.0013912916183472

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.68s/it]
INFO:root:final mean train loss: 1.762015277366715
INFO:root:final train perplexity: 1.0013915300369263
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.44s/it]
INFO:root:eval mean loss: 1.8851724562915504
INFO:root:eval perplexity: 1.0015267133712769
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.05s/it]
INFO:root:eval mean loss: 2.355327348759834
INFO:root:eval perplexity: 1.0019382238388062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/29
 14%|â–ˆâ–        | 29/200 [5:16:45<30:56:02, 651.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7425899803638458
INFO:root:current train perplexity1.0013726949691772
INFO:root:current mean train loss 1.7520560162762802
INFO:root:current train perplexity1.0013909339904785
INFO:root:current mean train loss 1.7470800362221182
INFO:root:current train perplexity1.0013779401779175
INFO:root:current mean train loss 1.747853190315013
INFO:root:current train perplexity1.0013796091079712
INFO:root:current mean train loss 1.745718444750561
INFO:root:current train perplexity1.0013786554336548
INFO:root:current mean train loss 1.7480260796240858
INFO:root:current train perplexity1.0013827085494995
INFO:root:current mean train loss 1.7480106281407306
INFO:root:current train perplexity1.0013843774795532
INFO:root:current mean train loss 1.7482971368413982
INFO:root:current train perplexity1.0013840198516846
INFO:root:current mean train loss 1.749246168697896
INFO:root:current train perplexity1.001383900642395
INFO:root:current mean train loss 1.7488028184781153
INFO:root:current train perplexity1.0013824701309204
INFO:root:current mean train loss 1.7498304010965886
INFO:root:current train perplexity1.0013829469680786
INFO:root:current mean train loss 1.7488926878111475
INFO:root:current train perplexity1.0013821125030518
INFO:root:current mean train loss 1.7488100387548144
INFO:root:current train perplexity1.001381278038025
INFO:root:current mean train loss 1.749849199734885
INFO:root:current train perplexity1.0013819932937622
INFO:root:current mean train loss 1.7506606351114788
INFO:root:current train perplexity1.0013822317123413
INFO:root:current mean train loss 1.7505508647791703
INFO:root:current train perplexity1.0013824701309204
INFO:root:current mean train loss 1.7513889507621738
INFO:root:current train perplexity1.001383662223816
INFO:root:current mean train loss 1.7515552447044425
INFO:root:current train perplexity1.0013833045959473
INFO:root:current mean train loss 1.7513823170238518
INFO:root:current train perplexity1.0013830661773682

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.73s/it]
INFO:root:final mean train loss: 1.7523051593499657
INFO:root:final train perplexity: 1.001383900642395
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.99s/it]
INFO:root:eval mean loss: 1.8790766089520556
INFO:root:eval perplexity: 1.0015217065811157
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.14s/it]
INFO:root:eval mean loss: 2.3506275681739157
INFO:root:eval perplexity: 1.0019344091415405
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/30
 15%|â–ˆâ–Œ        | 30/200 [5:27:38<30:46:39, 651.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7471760643853083
INFO:root:current train perplexity1.0013537406921387
INFO:root:current mean train loss 1.7310762744431103
INFO:root:current train perplexity1.0013654232025146
INFO:root:current mean train loss 1.7324621671694889
INFO:root:current train perplexity1.0013630390167236
INFO:root:current mean train loss 1.7381007146681011
INFO:root:current train perplexity1.0013707876205444
INFO:root:current mean train loss 1.7383017738174402
INFO:root:current train perplexity1.0013713836669922
INFO:root:current mean train loss 1.7374930988360482
INFO:root:current train perplexity1.00137197971344
INFO:root:current mean train loss 1.7385376715307752
INFO:root:current train perplexity1.001373529434204
INFO:root:current mean train loss 1.7389418937591639
INFO:root:current train perplexity1.0013749599456787
INFO:root:current mean train loss 1.739124062034786
INFO:root:current train perplexity1.0013744831085205
INFO:root:current mean train loss 1.7382718943657787
INFO:root:current train perplexity1.0013740062713623
INFO:root:current mean train loss 1.740176489851756
INFO:root:current train perplexity1.0013753175735474
INFO:root:current mean train loss 1.7411945949705794
INFO:root:current train perplexity1.0013760328292847
INFO:root:current mean train loss 1.7407860596185878
INFO:root:current train perplexity1.0013763904571533
INFO:root:current mean train loss 1.741991685400308
INFO:root:current train perplexity1.0013771057128906
INFO:root:current mean train loss 1.7418780750717315
INFO:root:current train perplexity1.0013763904571533
INFO:root:current mean train loss 1.7423485281295221
INFO:root:current train perplexity1.0013771057128906
INFO:root:current mean train loss 1.7421151452956547
INFO:root:current train perplexity1.0013761520385742
INFO:root:current mean train loss 1.7419595057815052
INFO:root:current train perplexity1.0013757944107056
INFO:root:current mean train loss 1.7423282961874393
INFO:root:current train perplexity1.0013762712478638
INFO:root:current mean train loss 1.742400015755559
INFO:root:current train perplexity1.0013751983642578

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.39s/it]
INFO:root:final mean train loss: 1.7426645178056641
INFO:root:final train perplexity: 1.0013762712478638
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.80s/it]
INFO:root:eval mean loss: 1.8775414300303088
INFO:root:eval perplexity: 1.0015205144882202
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.83s/it]
INFO:root:eval mean loss: 2.3506484847542244
INFO:root:eval perplexity: 1.0019344091415405
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/31
 16%|â–ˆâ–Œ        | 31/200 [5:38:28<30:34:21, 651.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.708927489244021
INFO:root:current train perplexity1.0013468265533447
INFO:root:current mean train loss 1.730356406597864
INFO:root:current train perplexity1.0013736486434937
INFO:root:current mean train loss 1.726233724471742
INFO:root:current train perplexity1.001369833946228
INFO:root:current mean train loss 1.726866405068731
INFO:root:current train perplexity1.0013675689697266
INFO:root:current mean train loss 1.7304881553694675
INFO:root:current train perplexity1.0013736486434937
INFO:root:current mean train loss 1.7324352815124018
INFO:root:current train perplexity1.0013738870620728
INFO:root:current mean train loss 1.7326956678884098
INFO:root:current train perplexity1.001372218132019
INFO:root:current mean train loss 1.733324770592461
INFO:root:current train perplexity1.001371145248413
INFO:root:current mean train loss 1.732076943930933
INFO:root:current train perplexity1.0013682842254639
INFO:root:current mean train loss 1.7323387311550247
INFO:root:current train perplexity1.001368522644043
INFO:root:current mean train loss 1.7325040108976308
INFO:root:current train perplexity1.0013689994812012
INFO:root:current mean train loss 1.7320511176996927
INFO:root:current train perplexity1.0013684034347534
INFO:root:current mean train loss 1.7331004247774309
INFO:root:current train perplexity1.001369595527649
INFO:root:current mean train loss 1.7325462730401961
INFO:root:current train perplexity1.0013691186904907
INFO:root:current mean train loss 1.7318991220348514
INFO:root:current train perplexity1.0013680458068848
INFO:root:current mean train loss 1.7322287454830085
INFO:root:current train perplexity1.0013676881790161
INFO:root:current mean train loss 1.7332652254444794
INFO:root:current train perplexity1.0013682842254639
INFO:root:current mean train loss 1.7334287319233077
INFO:root:current train perplexity1.0013684034347534
INFO:root:current mean train loss 1.7335509472452144
INFO:root:current train perplexity1.0013684034347534
INFO:root:current mean train loss 1.7333508734886272
INFO:root:current train perplexity1.0013682842254639

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.30s/it]
INFO:root:final mean train loss: 1.7330571520045979
INFO:root:final train perplexity: 1.0013686418533325
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.10s/it]
INFO:root:eval mean loss: 1.8770687956336543
INFO:root:eval perplexity: 1.0015201568603516
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.75s/it]
INFO:root:eval mean loss: 2.353674794765229
INFO:root:eval perplexity: 1.001936912536621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/32
 16%|â–ˆâ–Œ        | 32/200 [5:49:18<30:22:35, 650.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7180970491364944
INFO:root:current train perplexity1.0013591051101685
INFO:root:current mean train loss 1.7122223177156248
INFO:root:current train perplexity1.0013577938079834
INFO:root:current mean train loss 1.7095457727526442
INFO:root:current train perplexity1.0013507604599
INFO:root:current mean train loss 1.7090947707957498
INFO:root:current train perplexity1.001351237297058
INFO:root:current mean train loss 1.7107606025635524
INFO:root:current train perplexity1.001349925994873
INFO:root:current mean train loss 1.7166203837807446
INFO:root:current train perplexity1.0013537406921387
INFO:root:current mean train loss 1.7202599145020148
INFO:root:current train perplexity1.001357913017273
INFO:root:current mean train loss 1.7191766518121774
INFO:root:current train perplexity1.001356840133667
INFO:root:current mean train loss 1.719977407975836
INFO:root:current train perplexity1.0013571977615356
INFO:root:current mean train loss 1.7207171930867313
INFO:root:current train perplexity1.001357913017273
INFO:root:current mean train loss 1.7218358225241832
INFO:root:current train perplexity1.0013587474822998
INFO:root:current mean train loss 1.7224420207900533
INFO:root:current train perplexity1.0013591051101685
INFO:root:current mean train loss 1.7228427077145319
INFO:root:current train perplexity1.001359224319458
INFO:root:current mean train loss 1.7227777461018565
INFO:root:current train perplexity1.0013598203659058
INFO:root:current mean train loss 1.7233215539883344
INFO:root:current train perplexity1.0013610124588013
INFO:root:current mean train loss 1.7234849114736333
INFO:root:current train perplexity1.0013614892959595
INFO:root:current mean train loss 1.724377926935601
INFO:root:current train perplexity1.0013618469238281
INFO:root:current mean train loss 1.7244618129128364
INFO:root:current train perplexity1.0013623237609863
INFO:root:current mean train loss 1.7246377951931966
INFO:root:current train perplexity1.001361608505249
INFO:root:current mean train loss 1.724789017325385
INFO:root:current train perplexity1.001361608505249

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.26s/it]
INFO:root:final mean train loss: 1.7244673977581106
INFO:root:final train perplexity: 1.0013618469238281
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.80s/it]
INFO:root:eval mean loss: 1.8770673803403868
INFO:root:eval perplexity: 1.0015201568603516
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.35816427959618
INFO:root:eval perplexity: 1.0019406080245972
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/33
 16%|â–ˆâ–‹        | 33/200 [6:00:04<30:07:30, 649.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.694843296209971
INFO:root:current train perplexity1.0013325214385986
INFO:root:current mean train loss 1.7028706073760986
INFO:root:current train perplexity1.001343846321106
INFO:root:current mean train loss 1.7019028567350827
INFO:root:current train perplexity1.0013407468795776
INFO:root:current mean train loss 1.7039418323172464
INFO:root:current train perplexity1.0013483762741089
INFO:root:current mean train loss 1.704964505330376
INFO:root:current train perplexity1.0013490915298462
INFO:root:current mean train loss 1.7075359427503176
INFO:root:current train perplexity1.0013517141342163
INFO:root:current mean train loss 1.7090874142719037
INFO:root:current train perplexity1.0013524293899536
INFO:root:current mean train loss 1.7106211925807753
INFO:root:current train perplexity1.0013536214828491
INFO:root:current mean train loss 1.7108724311340686
INFO:root:current train perplexity1.0013535022735596
INFO:root:current mean train loss 1.7115121082713207
INFO:root:current train perplexity1.0013550519943237
INFO:root:current mean train loss 1.711695481358834
INFO:root:current train perplexity1.0013549327850342
INFO:root:current mean train loss 1.7119568322239251
INFO:root:current train perplexity1.0013540983200073
INFO:root:current mean train loss 1.7126617632214984
INFO:root:current train perplexity1.0013538599014282
INFO:root:current mean train loss 1.7133821529500626
INFO:root:current train perplexity1.0013539791107178
INFO:root:current mean train loss 1.712375058050025
INFO:root:current train perplexity1.0013530254364014
INFO:root:current mean train loss 1.7136129821722323
INFO:root:current train perplexity1.00135338306427
INFO:root:current mean train loss 1.713641728478742
INFO:root:current train perplexity1.0013527870178223
INFO:root:current mean train loss 1.7138524973934346
INFO:root:current train perplexity1.001353144645691
INFO:root:current mean train loss 1.714750115268974
INFO:root:current train perplexity1.0013536214828491
INFO:root:current mean train loss 1.7150546403563753
INFO:root:current train perplexity1.0013542175292969

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.58s/it]
INFO:root:final mean train loss: 1.7149975546309277
INFO:root:final train perplexity: 1.0013543367385864
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.07s/it]
INFO:root:eval mean loss: 1.873577152160888
INFO:root:eval perplexity: 1.0015172958374023
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it]
INFO:root:eval mean loss: 2.3534375137471137
INFO:root:eval perplexity: 1.001936674118042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/34
 17%|â–ˆâ–‹        | 34/200 [6:10:59<30:01:24, 651.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7013077271449102
INFO:root:current train perplexity1.001340389251709
INFO:root:current mean train loss 1.6948268373133772
INFO:root:current train perplexity1.0013372898101807
INFO:root:current mean train loss 1.6983550197380974
INFO:root:current train perplexity1.0013397932052612
INFO:root:current mean train loss 1.6971152432717442
INFO:root:current train perplexity1.0013394355773926
INFO:root:current mean train loss 1.6982371659648743
INFO:root:current train perplexity1.0013396739959717
INFO:root:current mean train loss 1.6994528125848887
INFO:root:current train perplexity1.0013399124145508
INFO:root:current mean train loss 1.6996608821116663
INFO:root:current train perplexity1.0013402700424194
INFO:root:current mean train loss 1.7011706694059237
INFO:root:current train perplexity1.0013424158096313
INFO:root:current mean train loss 1.7025481946384078
INFO:root:current train perplexity1.0013443231582642
INFO:root:current mean train loss 1.7029716369439538
INFO:root:current train perplexity1.0013447999954224
INFO:root:current mean train loss 1.7041740326717594
INFO:root:current train perplexity1.0013443231582642
INFO:root:current mean train loss 1.7038033172652796
INFO:root:current train perplexity1.0013443231582642
INFO:root:current mean train loss 1.7040481219183399
INFO:root:current train perplexity1.0013450384140015
INFO:root:current mean train loss 1.7049661770019309
INFO:root:current train perplexity1.0013465881347656
INFO:root:current mean train loss 1.705570678978896
INFO:root:current train perplexity1.0013470649719238
INFO:root:current mean train loss 1.7054736271037583
INFO:root:current train perplexity1.0013459920883179
INFO:root:current mean train loss 1.7055675404798294
INFO:root:current train perplexity1.001346230506897
INFO:root:current mean train loss 1.7057299776968153
INFO:root:current train perplexity1.0013467073440552
INFO:root:current mean train loss 1.7061855135665145
INFO:root:current train perplexity1.0013469457626343
INFO:root:current mean train loss 1.7063483944069453
INFO:root:current train perplexity1.001347303390503

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.36s/it]
INFO:root:final mean train loss: 1.7063183293941342
INFO:root:final train perplexity: 1.001347541809082
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.90s/it]
INFO:root:eval mean loss: 1.8733982160581764
INFO:root:eval perplexity: 1.0015171766281128
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.04s/it]
INFO:root:eval mean loss: 2.3556686720104083
INFO:root:eval perplexity: 1.0019385814666748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/35
 18%|â–ˆâ–Š        | 35/200 [6:21:49<29:49:56, 650.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.7007124259116801
INFO:root:current train perplexity1.0013415813446045
INFO:root:current mean train loss 1.6915076731406535
INFO:root:current train perplexity1.0013339519500732
INFO:root:current mean train loss 1.6913825374882236
INFO:root:current train perplexity1.0013339519500732
INFO:root:current mean train loss 1.6930763936284834
INFO:root:current train perplexity1.0013368129730225
INFO:root:current mean train loss 1.6939383433897968
INFO:root:current train perplexity1.0013360977172852
INFO:root:current mean train loss 1.6939031682431898
INFO:root:current train perplexity1.0013365745544434
INFO:root:current mean train loss 1.6947231716999747
INFO:root:current train perplexity1.001338005065918
INFO:root:current mean train loss 1.693529663067921
INFO:root:current train perplexity1.0013370513916016
INFO:root:current mean train loss 1.6930439222845721
INFO:root:current train perplexity1.0013371706008911
INFO:root:current mean train loss 1.694442320517611
INFO:root:current train perplexity1.0013383626937866
INFO:root:current mean train loss 1.6942192352014225
INFO:root:current train perplexity1.001338243484497
INFO:root:current mean train loss 1.6952348619050517
INFO:root:current train perplexity1.0013399124145508
INFO:root:current mean train loss 1.6957516466631588
INFO:root:current train perplexity1.001340389251709
INFO:root:current mean train loss 1.6960904841956654
INFO:root:current train perplexity1.0013405084609985
INFO:root:current mean train loss 1.6963325292869424
INFO:root:current train perplexity1.0013405084609985
INFO:root:current mean train loss 1.6968165022815336
INFO:root:current train perplexity1.0013412237167358
INFO:root:current mean train loss 1.697011852630336
INFO:root:current train perplexity1.001340389251709
INFO:root:current mean train loss 1.6976727178131326
INFO:root:current train perplexity1.0013408660888672
INFO:root:current mean train loss 1.69790181898391
INFO:root:current train perplexity1.0013405084609985

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.07s/it]
INFO:root:final mean train loss: 1.6977470368973486
INFO:root:final train perplexity: 1.0013407468795776
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 31.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 31.00s/it]
INFO:root:eval mean loss: 1.8730058860271535
INFO:root:eval perplexity: 1.0015168190002441
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it]
INFO:root:eval mean loss: 2.3586601953134467
INFO:root:eval perplexity: 1.0019409656524658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/36
 18%|â–ˆâ–Š        | 36/200 [6:32:40<29:38:47, 650.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6807683056051081
INFO:root:current train perplexity1.0013152360916138
INFO:root:current mean train loss 1.6656626344801069
INFO:root:current train perplexity1.0013099908828735
INFO:root:current mean train loss 1.6736415200888828
INFO:root:current train perplexity1.001318097114563
INFO:root:current mean train loss 1.6785279079265534
INFO:root:current train perplexity1.0013231039047241
INFO:root:current mean train loss 1.6811481340088112
INFO:root:current train perplexity1.0013266801834106
INFO:root:current mean train loss 1.679501674879787
INFO:root:current train perplexity1.0013271570205688
INFO:root:current mean train loss 1.6827020844148926
INFO:root:current train perplexity1.001327633857727
INFO:root:current mean train loss 1.685262480868569
INFO:root:current train perplexity1.0013298988342285
INFO:root:current mean train loss 1.6845934782016438
INFO:root:current train perplexity1.0013298988342285
INFO:root:current mean train loss 1.6853938914025786
INFO:root:current train perplexity1.0013291835784912
INFO:root:current mean train loss 1.6855079113670675
INFO:root:current train perplexity1.0013301372528076
INFO:root:current mean train loss 1.6861577378426662
INFO:root:current train perplexity1.0013303756713867
INFO:root:current mean train loss 1.68611650492513
INFO:root:current train perplexity1.001331090927124
INFO:root:current mean train loss 1.6864985023901538
INFO:root:current train perplexity1.0013301372528076
INFO:root:current mean train loss 1.6868792026284567
INFO:root:current train perplexity1.001330852508545
INFO:root:current mean train loss 1.6873219092897356
INFO:root:current train perplexity1.001331090927124
INFO:root:current mean train loss 1.6884158206681597
INFO:root:current train perplexity1.001332402229309
INFO:root:current mean train loss 1.6889638498050996
INFO:root:current train perplexity1.0013329982757568
INFO:root:current mean train loss 1.6894022583764012
INFO:root:current train perplexity1.001333475112915
INFO:root:current mean train loss 1.689336778604816
INFO:root:current train perplexity1.0013335943222046

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.46s/it]
INFO:root:final mean train loss: 1.689358356380126
INFO:root:final train perplexity: 1.0013340711593628
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.34s/it]
INFO:root:eval mean loss: 1.8700482088623318
INFO:root:eval perplexity: 1.0015144348144531
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.80s/it]
INFO:root:eval mean loss: 2.355723166719396
INFO:root:eval perplexity: 1.0019385814666748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/37
 18%|â–ˆâ–Š        | 37/200 [6:43:28<29:26:12, 650.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6964913521494185
INFO:root:current train perplexity1.0013360977172852
INFO:root:current mean train loss 1.6806956455111504
INFO:root:current train perplexity1.0013213157653809
INFO:root:current mean train loss 1.6793055821929062
INFO:root:current train perplexity1.0013233423233032
INFO:root:current mean train loss 1.6772062887505788
INFO:root:current train perplexity1.0013203620910645
INFO:root:current mean train loss 1.677776934387528
INFO:root:current train perplexity1.0013229846954346
INFO:root:current mean train loss 1.678350814138398
INFO:root:current train perplexity1.0013229846954346
INFO:root:current mean train loss 1.678894415402868
INFO:root:current train perplexity1.0013240575790405
INFO:root:current mean train loss 1.679574768458094
INFO:root:current train perplexity1.0013254880905151
INFO:root:current mean train loss 1.6776514204515927
INFO:root:current train perplexity1.001323938369751
INFO:root:current mean train loss 1.6782825813981994
INFO:root:current train perplexity1.0013242959976196
INFO:root:current mean train loss 1.6797479423567478
INFO:root:current train perplexity1.0013258457183838
INFO:root:current mean train loss 1.6793330617196172
INFO:root:current train perplexity1.0013256072998047
INFO:root:current mean train loss 1.6802707264982528
INFO:root:current train perplexity1.0013271570205688
INFO:root:current mean train loss 1.6795993879976043
INFO:root:current train perplexity1.0013259649276733
INFO:root:current mean train loss 1.68012934138461
INFO:root:current train perplexity1.0013266801834106
INFO:root:current mean train loss 1.680421790293374
INFO:root:current train perplexity1.0013266801834106
INFO:root:current mean train loss 1.6802179216606319
INFO:root:current train perplexity1.0013266801834106
INFO:root:current mean train loss 1.6805753190484312
INFO:root:current train perplexity1.0013266801834106
INFO:root:current mean train loss 1.6808776286159615
INFO:root:current train perplexity1.0013275146484375
INFO:root:current mean train loss 1.6814369449229656
INFO:root:current train perplexity1.001327633857727

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.07s/it]
INFO:root:final mean train loss: 1.680982313129677
INFO:root:final train perplexity: 1.0013275146484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.19s/it]
INFO:root:eval mean loss: 1.8696538717188733
INFO:root:eval perplexity: 1.0015140771865845
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.75s/it]
INFO:root:eval mean loss: 2.355912410198374
INFO:root:eval perplexity: 1.0019387006759644
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/38
 19%|â–ˆâ–‰        | 38/200 [6:54:21<29:17:42, 651.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.678080177307129
INFO:root:current train perplexity1.0013461112976074
INFO:root:current mean train loss 1.6666954155625968
INFO:root:current train perplexity1.0013186931610107
INFO:root:current mean train loss 1.66925438423546
INFO:root:current train perplexity1.0013185739517212
INFO:root:current mean train loss 1.6672308807787688
INFO:root:current train perplexity1.0013164281845093
INFO:root:current mean train loss 1.6689335640896572
INFO:root:current train perplexity1.0013171434402466
INFO:root:current mean train loss 1.6699994266580005
INFO:root:current train perplexity1.0013186931610107
INFO:root:current mean train loss 1.6693078994750976
INFO:root:current train perplexity1.0013172626495361
INFO:root:current mean train loss 1.6694537471604827
INFO:root:current train perplexity1.0013179779052734
INFO:root:current mean train loss 1.6705026045353455
INFO:root:current train perplexity1.0013175010681152
INFO:root:current mean train loss 1.6712048848470051
INFO:root:current train perplexity1.0013175010681152
INFO:root:current mean train loss 1.6707331940317838
INFO:root:current train perplexity1.0013179779052734
INFO:root:current mean train loss 1.6711602110009005
INFO:root:current train perplexity1.0013196468353271
INFO:root:current mean train loss 1.6708014218203993
INFO:root:current train perplexity1.0013197660446167
INFO:root:current mean train loss 1.6710223737702494
INFO:root:current train perplexity1.001320242881775
INFO:root:current mean train loss 1.6719129938567798
INFO:root:current train perplexity1.0013200044631958
INFO:root:current mean train loss 1.6724363835498353
INFO:root:current train perplexity1.001320481300354
INFO:root:current mean train loss 1.6722880801893658
INFO:root:current train perplexity1.0013203620910645
INFO:root:current mean train loss 1.6726666559804135
INFO:root:current train perplexity1.0013198852539062
INFO:root:current mean train loss 1.6727586078773022
INFO:root:current train perplexity1.0013206005096436
INFO:root:current mean train loss 1.6727154644718514
INFO:root:current train perplexity1.0013203620910645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.89s/it]
INFO:root:final mean train loss: 1.673072380064956
INFO:root:final train perplexity: 1.0013213157653809
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.59s/it]
INFO:root:eval mean loss: 1.871192161073076
INFO:root:eval perplexity: 1.0015153884887695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.53s/it]
INFO:root:eval mean loss: 2.3585318081767848
INFO:root:eval perplexity: 1.0019408464431763
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/39
 20%|â–ˆâ–‰        | 39/200 [7:05:12<29:06:55, 651.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6685610317414807
INFO:root:current train perplexity1.0013166666030884
INFO:root:current mean train loss 1.6608105287139798
INFO:root:current train perplexity1.001311182975769
INFO:root:current mean train loss 1.6628615510372715
INFO:root:current train perplexity1.0013117790222168
INFO:root:current mean train loss 1.660649714219636
INFO:root:current train perplexity1.001308798789978
INFO:root:current mean train loss 1.6587979729041393
INFO:root:current train perplexity1.0013078451156616
INFO:root:current mean train loss 1.6594469504848495
INFO:root:current train perplexity1.0013076066970825
INFO:root:current mean train loss 1.6584644112342073
INFO:root:current train perplexity1.0013070106506348
INFO:root:current mean train loss 1.6577832220107551
INFO:root:current train perplexity1.0013056993484497
INFO:root:current mean train loss 1.6596830072369764
INFO:root:current train perplexity1.0013083219528198
INFO:root:current mean train loss 1.6598820721037422
INFO:root:current train perplexity1.0013083219528198
INFO:root:current mean train loss 1.660130499783209
INFO:root:current train perplexity1.0013082027435303
INFO:root:current mean train loss 1.6611874505697963
INFO:root:current train perplexity1.0013099908828735
INFO:root:current mean train loss 1.6616364455827632
INFO:root:current train perplexity1.0013110637664795
INFO:root:current mean train loss 1.662148782335189
INFO:root:current train perplexity1.0013121366500854
INFO:root:current mean train loss 1.663172613335047
INFO:root:current train perplexity1.0013136863708496
INFO:root:current mean train loss 1.6643530128096802
INFO:root:current train perplexity1.001314401626587
INFO:root:current mean train loss 1.6641447388451596
INFO:root:current train perplexity1.001314401626587
INFO:root:current mean train loss 1.6637948530215545
INFO:root:current train perplexity1.0013136863708496
INFO:root:current mean train loss 1.6640542652117836
INFO:root:current train perplexity1.0013138055801392
INFO:root:current mean train loss 1.6644581968874255
INFO:root:current train perplexity1.0013139247894287

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:46<00:00, 586.49s/it]
INFO:root:final mean train loss: 1.6643788030636895
INFO:root:final train perplexity: 1.001314401626587
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it]
INFO:root:eval mean loss: 1.8697615155936977
INFO:root:eval perplexity: 1.001514196395874
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it]
INFO:root:eval mean loss: 2.3601048795889454
INFO:root:eval perplexity: 1.0019421577453613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/40
 20%|â–ˆâ–ˆ        | 40/200 [7:16:03<28:55:54, 650.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6551649389387686
INFO:root:current train perplexity1.0013031959533691
INFO:root:current mean train loss 1.655858952239905
INFO:root:current train perplexity1.001305341720581
INFO:root:current mean train loss 1.6513969509405046
INFO:root:current train perplexity1.0013004541397095
INFO:root:current mean train loss 1.6520198753452553
INFO:root:current train perplexity1.0013009309768677
INFO:root:current mean train loss 1.6527582379621852
INFO:root:current train perplexity1.0013035535812378
INFO:root:current mean train loss 1.6523301576703324
INFO:root:current train perplexity1.0013049840927124
INFO:root:current mean train loss 1.65496932441953
INFO:root:current train perplexity1.0013054609298706
INFO:root:current mean train loss 1.654715278794432
INFO:root:current train perplexity1.001305103302002
INFO:root:current mean train loss 1.6560191872176864
INFO:root:current train perplexity1.001306414604187
INFO:root:current mean train loss 1.6551463846288492
INFO:root:current train perplexity1.0013065338134766
INFO:root:current mean train loss 1.6561686811455982
INFO:root:current train perplexity1.001307725906372
INFO:root:current mean train loss 1.6557836580114713
INFO:root:current train perplexity1.001307487487793
INFO:root:current mean train loss 1.656150089193826
INFO:root:current train perplexity1.0013072490692139
INFO:root:current mean train loss 1.6562957567439034
INFO:root:current train perplexity1.001307487487793
INFO:root:current mean train loss 1.6564060762333177
INFO:root:current train perplexity1.001307487487793
INFO:root:current mean train loss 1.6567052913059381
INFO:root:current train perplexity1.0013073682785034
INFO:root:current mean train loss 1.656562586069249
INFO:root:current train perplexity1.0013072490692139
INFO:root:current mean train loss 1.6572246184303494
INFO:root:current train perplexity1.0013079643249512
INFO:root:current mean train loss 1.657943504296953
INFO:root:current train perplexity1.001308798789978
INFO:root:current mean train loss 1.6577385198955525
INFO:root:current train perplexity1.0013086795806885

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.99s/it]
INFO:root:final mean train loss: 1.6577809367946945
INFO:root:final train perplexity: 1.0013091564178467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.46s/it]
INFO:root:eval mean loss: 1.8708989290480917
INFO:root:eval perplexity: 1.0015151500701904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.30s/it]
INFO:root:eval mean loss: 2.3600834734896394
INFO:root:eval perplexity: 1.0019421577453613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/41
 20%|â–ˆâ–ˆ        | 41/200 [7:26:49<28:40:58, 649.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6399188625315826
INFO:root:current train perplexity1.0012964010238647
INFO:root:current mean train loss 1.6417917481490545
INFO:root:current train perplexity1.001296877861023
INFO:root:current mean train loss 1.6419587425283484
INFO:root:current train perplexity1.0012980699539185
INFO:root:current mean train loss 1.643343182826283
INFO:root:current train perplexity1.001301646232605
INFO:root:current mean train loss 1.6449698128046528
INFO:root:current train perplexity1.0013025999069214
INFO:root:current mean train loss 1.6458927176942761
INFO:root:current train perplexity1.001301646232605
INFO:root:current mean train loss 1.644962172912455
INFO:root:current train perplexity1.0012993812561035
INFO:root:current mean train loss 1.6461359307394554
INFO:root:current train perplexity1.0012993812561035
INFO:root:current mean train loss 1.6468025431303042
INFO:root:current train perplexity1.0013021230697632
INFO:root:current mean train loss 1.6473743878454568
INFO:root:current train perplexity1.0013023614883423
INFO:root:current mean train loss 1.64732433202928
INFO:root:current train perplexity1.0013022422790527
INFO:root:current mean train loss 1.6487394877301411
INFO:root:current train perplexity1.0013036727905273
INFO:root:current mean train loss 1.6489543232284947
INFO:root:current train perplexity1.0013031959533691
INFO:root:current mean train loss 1.648513828615063
INFO:root:current train perplexity1.00130295753479
INFO:root:current mean train loss 1.6490073106027543
INFO:root:current train perplexity1.0013030767440796
INFO:root:current mean train loss 1.6495274563779807
INFO:root:current train perplexity1.00130295753479
INFO:root:current mean train loss 1.649477758919293
INFO:root:current train perplexity1.0013030767440796
INFO:root:current mean train loss 1.649777305816489
INFO:root:current train perplexity1.0013035535812378
INFO:root:current mean train loss 1.6499692345596064
INFO:root:current train perplexity1.0013033151626587

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.22s/it]
INFO:root:final mean train loss: 1.6497121716291088
INFO:root:final train perplexity: 1.0013028383255005
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.03s/it]
INFO:root:eval mean loss: 1.8690184308282027
INFO:root:eval perplexity: 1.0015136003494263
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.59s/it]
INFO:root:eval mean loss: 2.360611454814884
INFO:root:eval perplexity: 1.0019426345825195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/42
 21%|â–ˆâ–ˆ        | 42/200 [7:37:37<28:28:55, 648.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.646779858149015
INFO:root:current train perplexity1.0013000965118408
INFO:root:current mean train loss 1.6336023554337764
INFO:root:current train perplexity1.0012948513031006
INFO:root:current mean train loss 1.637387256667088
INFO:root:current train perplexity1.0012938976287842
INFO:root:current mean train loss 1.6371391501289587
INFO:root:current train perplexity1.0012916326522827
INFO:root:current mean train loss 1.6388899699827661
INFO:root:current train perplexity1.0012953281402588
INFO:root:current mean train loss 1.6378501760564586
INFO:root:current train perplexity1.0012907981872559
INFO:root:current mean train loss 1.6364035870861657
INFO:root:current train perplexity1.0012898445129395
INFO:root:current mean train loss 1.636330793816779
INFO:root:current train perplexity1.0012894868850708
INFO:root:current mean train loss 1.637481529774261
INFO:root:current train perplexity1.001291036605835
INFO:root:current mean train loss 1.6378276841663975
INFO:root:current train perplexity1.001292109489441
INFO:root:current mean train loss 1.6382525846259144
INFO:root:current train perplexity1.001293659210205
INFO:root:current mean train loss 1.6383278569121245
INFO:root:current train perplexity1.0012941360473633
INFO:root:current mean train loss 1.638758617300488
INFO:root:current train perplexity1.0012943744659424
INFO:root:current mean train loss 1.6391544542690315
INFO:root:current train perplexity1.001294493675232
INFO:root:current mean train loss 1.6394123774593972
INFO:root:current train perplexity1.0012950897216797
INFO:root:current mean train loss 1.6400838568757405
INFO:root:current train perplexity1.0012949705123901
INFO:root:current mean train loss 1.640464701418986
INFO:root:current train perplexity1.001295804977417
INFO:root:current mean train loss 1.641462054018799
INFO:root:current train perplexity1.0012965202331543
INFO:root:current mean train loss 1.6417791433776208
INFO:root:current train perplexity1.0012964010238647
INFO:root:current mean train loss 1.6421841398559187
INFO:root:current train perplexity1.0012966394424438

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.38s/it]
INFO:root:final mean train loss: 1.642453048665657
INFO:root:final train perplexity: 1.001297116279602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it]
INFO:root:eval mean loss: 1.8705119904896892
INFO:root:eval perplexity: 1.0015147924423218
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.79s/it]
INFO:root:eval mean loss: 2.3647490562276636
INFO:root:eval perplexity: 1.001945972442627
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/43
 22%|â–ˆâ–ˆâ–       | 43/200 [7:48:26<28:17:50, 648.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5997402906417846
INFO:root:current train perplexity1.0012736320495605
INFO:root:current mean train loss 1.62094183243238
INFO:root:current train perplexity1.0012835264205933
INFO:root:current mean train loss 1.627666843974072
INFO:root:current train perplexity1.0012867450714111
INFO:root:current mean train loss 1.6291861031994674
INFO:root:current train perplexity1.0012842416763306
INFO:root:current mean train loss 1.6307465747345327
INFO:root:current train perplexity1.0012857913970947
INFO:root:current mean train loss 1.629673458270307
INFO:root:current train perplexity1.0012853145599365
INFO:root:current mean train loss 1.6298529411119127
INFO:root:current train perplexity1.0012868642807007
INFO:root:current mean train loss 1.6290456657540309
INFO:root:current train perplexity1.0012872219085693
INFO:root:current mean train loss 1.6290038480816118
INFO:root:current train perplexity1.0012871026992798
INFO:root:current mean train loss 1.6297477235076248
INFO:root:current train perplexity1.0012882947921753
INFO:root:current mean train loss 1.6314379039319973
INFO:root:current train perplexity1.00128972530365
INFO:root:current mean train loss 1.6311743548486084
INFO:root:current train perplexity1.0012881755828857
INFO:root:current mean train loss 1.6309020800319145
INFO:root:current train perplexity1.0012872219085693
INFO:root:current mean train loss 1.6315036115789772
INFO:root:current train perplexity1.0012873411178589
INFO:root:current mean train loss 1.63194825849333
INFO:root:current train perplexity1.001287817955017
INFO:root:current mean train loss 1.6328916068170585
INFO:root:current train perplexity1.001288652420044
INFO:root:current mean train loss 1.633086292070845
INFO:root:current train perplexity1.0012887716293335
INFO:root:current mean train loss 1.6334524056814999
INFO:root:current train perplexity1.0012892484664917
INFO:root:current mean train loss 1.6340545926589132
INFO:root:current train perplexity1.0012900829315186
INFO:root:current mean train loss 1.634631561677073
INFO:root:current train perplexity1.0012904405593872

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.94s/it]
INFO:root:final mean train loss: 1.634469016897997
INFO:root:final train perplexity: 1.0012907981872559
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it]
INFO:root:eval mean loss: 1.8726931861106386
INFO:root:eval perplexity: 1.001516580581665
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.69s/it]
INFO:root:eval mean loss: 2.3682084726103656
INFO:root:eval perplexity: 1.0019488334655762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/44
 22%|â–ˆâ–ˆâ–       | 44/200 [7:59:11<28:04:04, 647.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.625628613411112
INFO:root:current train perplexity1.0012904405593872
INFO:root:current mean train loss 1.6159158036822365
INFO:root:current train perplexity1.0012791156768799
INFO:root:current mean train loss 1.6210533525297033
INFO:root:current train perplexity1.0012807846069336
INFO:root:current mean train loss 1.619166405125379
INFO:root:current train perplexity1.0012773275375366
INFO:root:current mean train loss 1.6197299311891764
INFO:root:current train perplexity1.0012792348861694
INFO:root:current mean train loss 1.6185896414071594
INFO:root:current train perplexity1.001276969909668
INFO:root:current mean train loss 1.6203577531360585
INFO:root:current train perplexity1.001279592514038
INFO:root:current mean train loss 1.6220311694036686
INFO:root:current train perplexity1.0012818574905396
INFO:root:current mean train loss 1.6228019601479613
INFO:root:current train perplexity1.0012811422348022
INFO:root:current mean train loss 1.6237342512469106
INFO:root:current train perplexity1.0012831687927246
INFO:root:current mean train loss 1.623910766845674
INFO:root:current train perplexity1.0012823343276978
INFO:root:current mean train loss 1.6237066334605528
INFO:root:current train perplexity1.00128173828125
INFO:root:current mean train loss 1.6251065161291274
INFO:root:current train perplexity1.0012836456298828
INFO:root:current mean train loss 1.6252879923860852
INFO:root:current train perplexity1.0012842416763306
INFO:root:current mean train loss 1.6259692104915626
INFO:root:current train perplexity1.0012849569320679
INFO:root:current mean train loss 1.626018194591298
INFO:root:current train perplexity1.0012845993041992
INFO:root:current mean train loss 1.626052143505724
INFO:root:current train perplexity1.0012842416763306
INFO:root:current mean train loss 1.626676710114591
INFO:root:current train perplexity1.0012842416763306
INFO:root:current mean train loss 1.626632618271601
INFO:root:current train perplexity1.0012837648391724
INFO:root:current mean train loss 1.6268023951089132
INFO:root:current train perplexity1.0012842416763306

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.92s/it]
INFO:root:final mean train loss: 1.6268450630113926
INFO:root:final train perplexity: 1.0012847185134888
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.40s/it]
INFO:root:eval mean loss: 1.8715692099104537
INFO:root:eval perplexity: 1.0015156269073486
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.87s/it]
INFO:root:eval mean loss: 2.368378995158148
INFO:root:eval perplexity: 1.0019489526748657
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [8:09:57<27:52:15, 647.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5992108676582575
INFO:root:current train perplexity1.0012705326080322
INFO:root:current mean train loss 1.6103335451789018
INFO:root:current train perplexity1.001276969909668
INFO:root:current mean train loss 1.6118943280342855
INFO:root:current train perplexity1.001271367073059
INFO:root:current mean train loss 1.6155275820375798
INFO:root:current train perplexity1.0012757778167725
INFO:root:current mean train loss 1.6161893459743466
INFO:root:current train perplexity1.001275897026062
INFO:root:current mean train loss 1.6172928573391963
INFO:root:current train perplexity1.0012764930725098
INFO:root:current mean train loss 1.6162951385759445
INFO:root:current train perplexity1.0012757778167725
INFO:root:current mean train loss 1.6174618665772582
INFO:root:current train perplexity1.0012791156768799
INFO:root:current mean train loss 1.6179308798853997
INFO:root:current train perplexity1.001278042793274
INFO:root:current mean train loss 1.617265052938857
INFO:root:current train perplexity1.001277208328247
INFO:root:current mean train loss 1.6187140737709247
INFO:root:current train perplexity1.0012778043746948
INFO:root:current mean train loss 1.6185567419963194
INFO:root:current train perplexity1.001278042793274
INFO:root:current mean train loss 1.6191376656105247
INFO:root:current train perplexity1.0012791156768799
INFO:root:current mean train loss 1.6181172923433467
INFO:root:current train perplexity1.0012776851654053
INFO:root:current mean train loss 1.6180227642339435
INFO:root:current train perplexity1.0012770891189575
INFO:root:current mean train loss 1.6185151041316255
INFO:root:current train perplexity1.0012775659561157
INFO:root:current mean train loss 1.6189162844362168
INFO:root:current train perplexity1.0012774467468262
INFO:root:current mean train loss 1.6192687667416337
INFO:root:current train perplexity1.0012775659561157
INFO:root:current mean train loss 1.6197394198486221
INFO:root:current train perplexity1.0012779235839844
INFO:root:current mean train loss 1.6201378092503596
INFO:root:current train perplexity1.0012791156768799

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.50s/it]
INFO:root:final mean train loss: 1.6199793420051962
INFO:root:final train perplexity: 1.001279354095459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.17s/it]
INFO:root:eval mean loss: 1.869910401655427
INFO:root:eval perplexity: 1.0015143156051636
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.73s/it]
INFO:root:eval mean loss: 2.367210253756097
INFO:root:eval perplexity: 1.0019479990005493
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [8:20:44<27:40:51, 647.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.6090501267233013
INFO:root:current train perplexity1.0012809038162231
INFO:root:current mean train loss 1.599955762288847
INFO:root:current train perplexity1.0012683868408203
INFO:root:current mean train loss 1.6031513294715474
INFO:root:current train perplexity1.0012701749801636
INFO:root:current mean train loss 1.6010250460131588
INFO:root:current train perplexity1.0012658834457397
INFO:root:current mean train loss 1.6026582571672054
INFO:root:current train perplexity1.001265287399292
INFO:root:current mean train loss 1.6034419113920806
INFO:root:current train perplexity1.0012668371200562
INFO:root:current mean train loss 1.602446493598333
INFO:root:current train perplexity1.0012657642364502
INFO:root:current mean train loss 1.602982007281881
INFO:root:current train perplexity1.0012661218643188
INFO:root:current mean train loss 1.6037955009436635
INFO:root:current train perplexity1.0012664794921875
INFO:root:current mean train loss 1.6051028366604103
INFO:root:current train perplexity1.0012680292129517
INFO:root:current mean train loss 1.6065683876552812
INFO:root:current train perplexity1.0012695789337158
INFO:root:current mean train loss 1.60803307879284
INFO:root:current train perplexity1.00127112865448
INFO:root:current mean train loss 1.608573337814754
INFO:root:current train perplexity1.0012707710266113
INFO:root:current mean train loss 1.6087635197042125
INFO:root:current train perplexity1.0012712478637695
INFO:root:current mean train loss 1.6095740042531272
INFO:root:current train perplexity1.0012720823287964
INFO:root:current mean train loss 1.610086389616424
INFO:root:current train perplexity1.0012714862823486
INFO:root:current mean train loss 1.6105280740143357
INFO:root:current train perplexity1.0012719631195068
INFO:root:current mean train loss 1.6112515615787752
INFO:root:current train perplexity1.0012727975845337
INFO:root:current mean train loss 1.6116816224347144
INFO:root:current train perplexity1.0012725591659546
INFO:root:current mean train loss 1.6124171284579798
INFO:root:current train perplexity1.0012730360031128

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.66s/it]
INFO:root:final mean train loss: 1.612371320142568
INFO:root:final train perplexity: 1.001273274421692
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.44s/it]
INFO:root:eval mean loss: 1.8711903940701315
INFO:root:eval perplexity: 1.0015153884887695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.85s/it]
INFO:root:eval mean loss: 2.3716241087473877
INFO:root:eval perplexity: 1.0019516944885254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [8:31:36<27:33:51, 648.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5970906274659293
INFO:root:current train perplexity1.00127112865448
INFO:root:current mean train loss 1.6074829571174853
INFO:root:current train perplexity1.0012813806533813
INFO:root:current mean train loss 1.6058892479678928
INFO:root:current train perplexity1.0012776851654053
INFO:root:current mean train loss 1.6051870500622083
INFO:root:current train perplexity1.001272201538086
INFO:root:current mean train loss 1.6027798142777867
INFO:root:current train perplexity1.0012718439102173
INFO:root:current mean train loss 1.6020909497969127
INFO:root:current train perplexity1.001268982887268
INFO:root:current mean train loss 1.602098262924861
INFO:root:current train perplexity1.0012664794921875
INFO:root:current mean train loss 1.6013493690275609
INFO:root:current train perplexity1.0012649297714233
INFO:root:current mean train loss 1.6025218702106008
INFO:root:current train perplexity1.0012660026550293
INFO:root:current mean train loss 1.6027618423253596
INFO:root:current train perplexity1.0012670755386353
INFO:root:current mean train loss 1.6025251238072504
INFO:root:current train perplexity1.0012668371200562
INFO:root:current mean train loss 1.6037655587188389
INFO:root:current train perplexity1.001266598701477
INFO:root:current mean train loss 1.6037891925024141
INFO:root:current train perplexity1.001266598701477
INFO:root:current mean train loss 1.6041119281996643
INFO:root:current train perplexity1.0012668371200562
INFO:root:current mean train loss 1.6039350987753975
INFO:root:current train perplexity1.0012668371200562
INFO:root:current mean train loss 1.6041395731354238
INFO:root:current train perplexity1.0012671947479248
INFO:root:current mean train loss 1.6042836029762655
INFO:root:current train perplexity1.001266598701477
INFO:root:current mean train loss 1.6048010224362501
INFO:root:current train perplexity1.001267433166504
INFO:root:current mean train loss 1.60577609170978
INFO:root:current train perplexity1.0012675523757935

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.94s/it]
INFO:root:final mean train loss: 1.6058111848460865
INFO:root:final train perplexity: 1.0012681484222412
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.74s/it]
INFO:root:eval mean loss: 1.871991589137003
INFO:root:eval perplexity: 1.0015159845352173
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.46s/it]
INFO:root:eval mean loss: 2.3750551993965257
INFO:root:eval perplexity: 1.0019545555114746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/48
 24%|â–ˆâ–ˆâ–       | 48/200 [8:42:24<27:22:44, 648.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5960736513137816
INFO:root:current train perplexity1.0012788772583008
INFO:root:current mean train loss 1.5921503751174264
INFO:root:current train perplexity1.0012637376785278
INFO:root:current mean train loss 1.5952507501424744
INFO:root:current train perplexity1.0012612342834473
INFO:root:current mean train loss 1.5964166838025289
INFO:root:current train perplexity1.0012624263763428
INFO:root:current mean train loss 1.5937572146036538
INFO:root:current train perplexity1.0012600421905518
INFO:root:current mean train loss 1.5939398980835109
INFO:root:current train perplexity1.0012603998184204
INFO:root:current mean train loss 1.5918722251566444
INFO:root:current train perplexity1.0012593269348145
INFO:root:current mean train loss 1.592830725149675
INFO:root:current train perplexity1.001259446144104
INFO:root:current mean train loss 1.594895164220611
INFO:root:current train perplexity1.0012602806091309
INFO:root:current mean train loss 1.5957064248173614
INFO:root:current train perplexity1.0012621879577637
INFO:root:current mean train loss 1.5961950136522942
INFO:root:current train perplexity1.0012632608413696
INFO:root:current mean train loss 1.5966664808213444
INFO:root:current train perplexity1.0012636184692383
INFO:root:current mean train loss 1.596230654951967
INFO:root:current train perplexity1.0012625455856323
INFO:root:current mean train loss 1.5965952627559126
INFO:root:current train perplexity1.0012633800506592
INFO:root:current mean train loss 1.5971926220735475
INFO:root:current train perplexity1.0012632608413696
INFO:root:current mean train loss 1.5981560375037367
INFO:root:current train perplexity1.0012633800506592
INFO:root:current mean train loss 1.598268424025261
INFO:root:current train perplexity1.0012637376785278
INFO:root:current mean train loss 1.598729984544804
INFO:root:current train perplexity1.0012637376785278
INFO:root:current mean train loss 1.5989680229796523
INFO:root:current train perplexity1.0012634992599487
INFO:root:current mean train loss 1.5990547008987506
INFO:root:current train perplexity1.0012626647949219

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.44s/it]
INFO:root:final mean train loss: 1.5991474307070341
INFO:root:final train perplexity: 1.0012627840042114
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.33s/it]
INFO:root:eval mean loss: 1.874648908774058
INFO:root:eval perplexity: 1.0015181303024292
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.74s/it]
INFO:root:eval mean loss: 2.3772049728014792
INFO:root:eval perplexity: 1.0019562244415283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/49
 24%|â–ˆâ–ˆâ–       | 49/200 [8:53:09<27:09:45, 647.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.565083347260952
INFO:root:current train perplexity1.0012292861938477
INFO:root:current mean train loss 1.5801171949415496
INFO:root:current train perplexity1.001254677772522
INFO:root:current mean train loss 1.5786002936034367
INFO:root:current train perplexity1.0012474060058594
INFO:root:current mean train loss 1.5827564464276096
INFO:root:current train perplexity1.0012518167495728
INFO:root:current mean train loss 1.5819325198729832
INFO:root:current train perplexity1.0012516975402832
INFO:root:current mean train loss 1.5836503940417355
INFO:root:current train perplexity1.0012534856796265
INFO:root:current mean train loss 1.5839279326079767
INFO:root:current train perplexity1.0012538433074951
INFO:root:current mean train loss 1.584980759138618
INFO:root:current train perplexity1.0012532472610474
INFO:root:current mean train loss 1.5864341909495683
INFO:root:current train perplexity1.0012544393539429
INFO:root:current mean train loss 1.586619018229292
INFO:root:current train perplexity1.0012544393539429
INFO:root:current mean train loss 1.5869679605776026
INFO:root:current train perplexity1.0012550354003906
INFO:root:current mean train loss 1.5877677921482194
INFO:root:current train perplexity1.001255750656128
INFO:root:current mean train loss 1.588507382900684
INFO:root:current train perplexity1.001255750656128
INFO:root:current mean train loss 1.5887710764422431
INFO:root:current train perplexity1.0012558698654175
INFO:root:current mean train loss 1.5900379448463131
INFO:root:current train perplexity1.001257300376892
INFO:root:current mean train loss 1.5904765530603673
INFO:root:current train perplexity1.0012571811676025
INFO:root:current mean train loss 1.5904704541260122
INFO:root:current train perplexity1.0012565851211548
INFO:root:current mean train loss 1.5913693869086392
INFO:root:current train perplexity1.0012574195861816
INFO:root:current mean train loss 1.5918879311157625
INFO:root:current train perplexity1.001257300376892
INFO:root:current mean train loss 1.5919397244418876
INFO:root:current train perplexity1.0012568235397339

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.92s/it]
INFO:root:final mean train loss: 1.5919290233968908
INFO:root:final train perplexity: 1.0012571811676025
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.20s/it]
INFO:root:eval mean loss: 1.8763335621948782
INFO:root:eval perplexity: 1.0015195608139038
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.44s/it]
INFO:root:eval mean loss: 2.379760190104762
INFO:root:eval perplexity: 1.0019583702087402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [9:03:57<26:58:59, 647.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5614588236322209
INFO:root:current train perplexity1.001227855682373
INFO:root:current mean train loss 1.5732852924590144
INFO:root:current train perplexity1.001235008239746
INFO:root:current mean train loss 1.5742063057949265
INFO:root:current train perplexity1.0012412071228027
INFO:root:current mean train loss 1.5764554165154268
INFO:root:current train perplexity1.0012433528900146
INFO:root:current mean train loss 1.5773641460456933
INFO:root:current train perplexity1.001243233680725
INFO:root:current mean train loss 1.5762086744082646
INFO:root:current train perplexity1.0012428760528564
INFO:root:current mean train loss 1.578586476976956
INFO:root:current train perplexity1.0012444257736206
INFO:root:current mean train loss 1.5796920201170428
INFO:root:current train perplexity1.0012449026107788
INFO:root:current mean train loss 1.5802561350789874
INFO:root:current train perplexity1.0012441873550415
INFO:root:current mean train loss 1.5799720223258997
INFO:root:current train perplexity1.0012452602386475
INFO:root:current mean train loss 1.5808955803726603
INFO:root:current train perplexity1.0012472867965698
INFO:root:current mean train loss 1.5812517131692538
INFO:root:current train perplexity1.001247525215149
INFO:root:current mean train loss 1.5817457613131827
INFO:root:current train perplexity1.0012481212615967
INFO:root:current mean train loss 1.581480883774358
INFO:root:current train perplexity1.001247525215149
INFO:root:current mean train loss 1.5820547555379822
INFO:root:current train perplexity1.0012481212615967
INFO:root:current mean train loss 1.5819894815738005
INFO:root:current train perplexity1.001247525215149
INFO:root:current mean train loss 1.5826310791920142
INFO:root:current train perplexity1.0012483596801758
INFO:root:current mean train loss 1.5830468297209175
INFO:root:current train perplexity1.001248836517334
INFO:root:current mean train loss 1.5838230599449286
INFO:root:current train perplexity1.0012502670288086
INFO:root:current mean train loss 1.5844647762040225
INFO:root:current train perplexity1.0012508630752563

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.34s/it]
INFO:root:final mean train loss: 1.5846720265428405
INFO:root:final train perplexity: 1.001251459121704
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.51s/it]
INFO:root:eval mean loss: 1.8738528735248754
INFO:root:eval perplexity: 1.0015175342559814
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.70s/it]
INFO:root:eval mean loss: 2.380454440066155
INFO:root:eval perplexity: 1.001958966255188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [9:14:46<26:49:39, 648.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5630304181214534
INFO:root:current train perplexity1.001226544380188
INFO:root:current mean train loss 1.5682485319045654
INFO:root:current train perplexity1.0012309551239014
INFO:root:current mean train loss 1.5696862277231718
INFO:root:current train perplexity1.0012328624725342
INFO:root:current mean train loss 1.570068330712657
INFO:root:current train perplexity1.001235842704773
INFO:root:current mean train loss 1.571723527151116
INFO:root:current train perplexity1.001236915588379
INFO:root:current mean train loss 1.5715876737668741
INFO:root:current train perplexity1.001235842704773
INFO:root:current mean train loss 1.5724186322710536
INFO:root:current train perplexity1.001239538192749
INFO:root:current mean train loss 1.5727882708955372
INFO:root:current train perplexity1.0012403726577759
INFO:root:current mean train loss 1.5741468081573415
INFO:root:current train perplexity1.0012415647506714
INFO:root:current mean train loss 1.5743048764904093
INFO:root:current train perplexity1.0012413263320923
INFO:root:current mean train loss 1.5745714363863947
INFO:root:current train perplexity1.0012403726577759
INFO:root:current mean train loss 1.5750103376128457
INFO:root:current train perplexity1.0012418031692505
INFO:root:current mean train loss 1.5753051057622707
INFO:root:current train perplexity1.0012422800064087
INFO:root:current mean train loss 1.5758693424185757
INFO:root:current train perplexity1.0012438297271729
INFO:root:current mean train loss 1.5763876176303275
INFO:root:current train perplexity1.0012441873550415
INFO:root:current mean train loss 1.576812519957127
INFO:root:current train perplexity1.001244068145752
INFO:root:current mean train loss 1.5772198462972835
INFO:root:current train perplexity1.0012444257736206
INFO:root:current mean train loss 1.5777600290146176
INFO:root:current train perplexity1.0012450218200684
INFO:root:current mean train loss 1.578216616108231
INFO:root:current train perplexity1.0012457370758057
INFO:root:current mean train loss 1.5779754709251537
INFO:root:current train perplexity1.0012457370758057

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.07s/it]
INFO:root:final mean train loss: 1.5780114850430433
INFO:root:final train perplexity: 1.0012460947036743
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.83s/it]
INFO:root:eval mean loss: 1.8784332068253917
INFO:root:eval perplexity: 1.0015212297439575
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it]
INFO:root:eval mean loss: 2.3856011912332358
INFO:root:eval perplexity: 1.0019631385803223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [9:25:34<26:38:24, 648.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5716269231704345
INFO:root:current train perplexity1.0012308359146118
INFO:root:current mean train loss 1.5660595066560423
INFO:root:current train perplexity1.0012327432632446
INFO:root:current mean train loss 1.5630085725245122
INFO:root:current train perplexity1.0012284517288208
INFO:root:current mean train loss 1.5651996176174352
INFO:root:current train perplexity1.0012327432632446
INFO:root:current mean train loss 1.5660252163869253
INFO:root:current train perplexity1.0012333393096924
INFO:root:current mean train loss 1.5663011722253894
INFO:root:current train perplexity1.0012346506118774
INFO:root:current mean train loss 1.5667303917523185
INFO:root:current train perplexity1.0012341737747192
INFO:root:current mean train loss 1.5687873760859172
INFO:root:current train perplexity1.0012367963790894
INFO:root:current mean train loss 1.5692257180489337
INFO:root:current train perplexity1.0012370347976685
INFO:root:current mean train loss 1.5694909253416323
INFO:root:current train perplexity1.001237392425537
INFO:root:current mean train loss 1.5699832119109558
INFO:root:current train perplexity1.0012381076812744
INFO:root:current mean train loss 1.5697267308440737
INFO:root:current train perplexity1.0012388229370117
INFO:root:current mean train loss 1.5696614124026043
INFO:root:current train perplexity1.0012385845184326
INFO:root:current mean train loss 1.5700449825453742
INFO:root:current train perplexity1.0012391805648804
INFO:root:current mean train loss 1.570476847489139
INFO:root:current train perplexity1.00123929977417
INFO:root:current mean train loss 1.5705195941732266
INFO:root:current train perplexity1.0012396574020386
INFO:root:current mean train loss 1.5709792538476854
INFO:root:current train perplexity1.0012402534484863
INFO:root:current mean train loss 1.5713637652052164
INFO:root:current train perplexity1.0012402534484863
INFO:root:current mean train loss 1.572194249340023
INFO:root:current train perplexity1.0012410879135132
INFO:root:current mean train loss 1.5724104811792956
INFO:root:current train perplexity1.001241683959961

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.25s/it]
INFO:root:final mean train loss: 1.5724104811792956
INFO:root:final train perplexity: 1.001241683959961
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it]
INFO:root:eval mean loss: 1.876741610520275
INFO:root:eval perplexity: 1.001519799232483
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.72s/it]
INFO:root:eval mean loss: 2.382911276310048
INFO:root:eval perplexity: 1.0019609928131104
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [9:36:23<26:28:12, 648.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.543465223312378
INFO:root:current train perplexity1.001218557357788
INFO:root:current mean train loss 1.5535707455873489
INFO:root:current train perplexity1.0012286901474
INFO:root:current mean train loss 1.5547573526700338
INFO:root:current train perplexity1.0012274980545044
INFO:root:current mean train loss 1.5556813594698906
INFO:root:current train perplexity1.001231074333191
INFO:root:current mean train loss 1.5567822396755218
INFO:root:current train perplexity1.001230001449585
INFO:root:current mean train loss 1.5573209913571675
INFO:root:current train perplexity1.001228928565979
INFO:root:current mean train loss 1.5582393419742584
INFO:root:current train perplexity1.0012297630310059
INFO:root:current mean train loss 1.5580199006199837
INFO:root:current train perplexity1.0012295246124268
INFO:root:current mean train loss 1.5597295139895546
INFO:root:current train perplexity1.0012317895889282
INFO:root:current mean train loss 1.5605991353988649
INFO:root:current train perplexity1.001232624053955
INFO:root:current mean train loss 1.5615737860853023
INFO:root:current train perplexity1.0012333393096924
INFO:root:current mean train loss 1.5622376483678817
INFO:root:current train perplexity1.0012332201004028
INFO:root:current mean train loss 1.5616449136917407
INFO:root:current train perplexity1.0012321472167969
INFO:root:current mean train loss 1.5627544764961516
INFO:root:current train perplexity1.0012331008911133
INFO:root:current mean train loss 1.563232653617859
INFO:root:current train perplexity1.001233696937561
INFO:root:current mean train loss 1.5634690476953983
INFO:root:current train perplexity1.0012339353561401
INFO:root:current mean train loss 1.5634722969812505
INFO:root:current train perplexity1.0012335777282715
INFO:root:current mean train loss 1.5641747627655664
INFO:root:current train perplexity1.0012344121932983
INFO:root:current mean train loss 1.5647663287112588
INFO:root:current train perplexity1.0012351274490356

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.68s/it]
INFO:root:final mean train loss: 1.5652354492782885
INFO:root:final train perplexity: 1.001236081123352
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.90s/it]
INFO:root:eval mean loss: 1.8766566041513537
INFO:root:eval perplexity: 1.001519799232483
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.30s/it]
INFO:root:eval mean loss: 2.3879982943230487
INFO:root:eval perplexity: 1.0019651651382446
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [9:47:17<26:21:33, 649.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.550587583990658
INFO:root:current train perplexity1.0012246370315552
INFO:root:current mean train loss 1.544327885676653
INFO:root:current train perplexity1.001222014427185
INFO:root:current mean train loss 1.550735690077329
INFO:root:current train perplexity1.0012272596359253
INFO:root:current mean train loss 1.554533317262066
INFO:root:current train perplexity1.0012316703796387
INFO:root:current mean train loss 1.554713226622529
INFO:root:current train perplexity1.0012317895889282
INFO:root:current mean train loss 1.5538756469232202
INFO:root:current train perplexity1.0012303590774536
INFO:root:current mean train loss 1.552343384952947
INFO:root:current train perplexity1.0012283325195312
INFO:root:current mean train loss 1.553047358906585
INFO:root:current train perplexity1.0012282133102417
INFO:root:current mean train loss 1.5527215297248402
INFO:root:current train perplexity1.0012257099151611
INFO:root:current mean train loss 1.5533330254622348
INFO:root:current train perplexity1.0012263059616089
INFO:root:current mean train loss 1.5538428635020523
INFO:root:current train perplexity1.001227617263794
INFO:root:current mean train loss 1.5546151816898302
INFO:root:current train perplexity1.0012284517288208
INFO:root:current mean train loss 1.5556872260306052
INFO:root:current train perplexity1.0012295246124268
INFO:root:current mean train loss 1.5559079356508538
INFO:root:current train perplexity1.0012292861938477
INFO:root:current mean train loss 1.5555911399818425
INFO:root:current train perplexity1.001228928565979
INFO:root:current mean train loss 1.556000748274746
INFO:root:current train perplexity1.0012288093566895
INFO:root:current mean train loss 1.5565127170447148
INFO:root:current train perplexity1.0012295246124268
INFO:root:current mean train loss 1.557026079177301
INFO:root:current train perplexity1.0012295246124268
INFO:root:current mean train loss 1.5581193710331347
INFO:root:current train perplexity1.0012308359146118
INFO:root:current mean train loss 1.5586096652174717
INFO:root:current train perplexity1.0012308359146118

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.90s/it]
INFO:root:final mean train loss: 1.5587222103031368
INFO:root:final train perplexity: 1.0012309551239014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.68s/it]
INFO:root:eval mean loss: 1.8795141514311446
INFO:root:eval perplexity: 1.0015220642089844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.94s/it]
INFO:root:eval mean loss: 2.3908548236738705
INFO:root:eval perplexity: 1.0019675493240356
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [9:58:03<26:08:18, 648.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5541963612332064
INFO:root:current train perplexity1.0012191534042358
INFO:root:current mean train loss 1.5500512354409517
INFO:root:current train perplexity1.0012234449386597
INFO:root:current mean train loss 1.5473172114445612
INFO:root:current train perplexity1.0012203454971313
INFO:root:current mean train loss 1.548240180144053
INFO:root:current train perplexity1.0012191534042358
INFO:root:current mean train loss 1.5492335717249577
INFO:root:current train perplexity1.0012223720550537
INFO:root:current mean train loss 1.5503108534920083
INFO:root:current train perplexity1.0012233257293701
INFO:root:current mean train loss 1.5513829350095443
INFO:root:current train perplexity1.0012234449386597
INFO:root:current mean train loss 1.5511453273510738
INFO:root:current train perplexity1.0012223720550537
INFO:root:current mean train loss 1.5512911988962754
INFO:root:current train perplexity1.0012238025665283
INFO:root:current mean train loss 1.5509714008143423
INFO:root:current train perplexity1.0012229681015015
INFO:root:current mean train loss 1.5522512939958555
INFO:root:current train perplexity1.0012249946594238
INFO:root:current mean train loss 1.5518183524225966
INFO:root:current train perplexity1.001225233078003
INFO:root:current mean train loss 1.551426543602101
INFO:root:current train perplexity1.0012248754501343
INFO:root:current mean train loss 1.5517693052942427
INFO:root:current train perplexity1.0012255907058716
INFO:root:current mean train loss 1.551936771580365
INFO:root:current train perplexity1.0012259483337402
INFO:root:current mean train loss 1.5522524576920729
INFO:root:current train perplexity1.0012263059616089
INFO:root:current mean train loss 1.5519531770464547
INFO:root:current train perplexity1.0012255907058716
INFO:root:current mean train loss 1.552287364501029
INFO:root:current train perplexity1.0012258291244507
INFO:root:current mean train loss 1.5528062968763687
INFO:root:current train perplexity1.0012264251708984
INFO:root:current mean train loss 1.5526097623367467
INFO:root:current train perplexity1.0012255907058716

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.56s/it]
INFO:root:final mean train loss: 1.5524968097622323
INFO:root:final train perplexity: 1.0012259483337402
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.49s/it]
INFO:root:eval mean loss: 1.8805883137892323
INFO:root:eval perplexity: 1.0015228986740112
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.88s/it]
INFO:root:eval mean loss: 2.3940422467306153
INFO:root:eval perplexity: 1.0019701719284058
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [10:08:50<25:55:24, 648.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.542180570901609
INFO:root:current train perplexity1.0012214183807373
INFO:root:current mean train loss 1.5450991265821141
INFO:root:current train perplexity1.0012203454971313
INFO:root:current mean train loss 1.5424022484585584
INFO:root:current train perplexity1.0012179613113403
INFO:root:current mean train loss 1.5423580017524567
INFO:root:current train perplexity1.0012187957763672
INFO:root:current mean train loss 1.5425994427928376
INFO:root:current train perplexity1.001220464706421
INFO:root:current mean train loss 1.5415387296417016
INFO:root:current train perplexity1.0012181997299194
INFO:root:current mean train loss 1.5410050252615581
INFO:root:current train perplexity1.0012186765670776
INFO:root:current mean train loss 1.5419620710428799
INFO:root:current train perplexity1.0012179613113403
INFO:root:current mean train loss 1.542205378675853
INFO:root:current train perplexity1.0012186765670776
INFO:root:current mean train loss 1.5425786751427235
INFO:root:current train perplexity1.0012191534042358
INFO:root:current mean train loss 1.5429263465411316
INFO:root:current train perplexity1.0012184381484985
INFO:root:current mean train loss 1.543468235991086
INFO:root:current train perplexity1.0012184381484985
INFO:root:current mean train loss 1.5437417539189473
INFO:root:current train perplexity1.0012192726135254
INFO:root:current mean train loss 1.5442992375569375
INFO:root:current train perplexity1.0012199878692627
INFO:root:current mean train loss 1.544693209811131
INFO:root:current train perplexity1.0012197494506836
INFO:root:current mean train loss 1.5451395244155677
INFO:root:current train perplexity1.001219630241394
INFO:root:current mean train loss 1.5449463460461288
INFO:root:current train perplexity1.001219391822815
INFO:root:current mean train loss 1.5453157381355116
INFO:root:current train perplexity1.0012198686599731
INFO:root:current mean train loss 1.544982462944307
INFO:root:current train perplexity1.0012197494506836
INFO:root:current mean train loss 1.545804029241823
INFO:root:current train perplexity1.001220464706421

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.82s/it]
INFO:root:final mean train loss: 1.5459161342783403
INFO:root:final train perplexity: 1.0012208223342896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.23s/it]
INFO:root:eval mean loss: 1.8806340842382283
INFO:root:eval perplexity: 1.0015230178833008
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.26s/it]
INFO:root:eval mean loss: 2.393507354225673
INFO:root:eval perplexity: 1.0019696950912476
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [10:19:36<25:43:24, 647.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5265152471906998
INFO:root:current train perplexity1.0011934041976929
INFO:root:current mean train loss 1.5255834481545858
INFO:root:current train perplexity1.001196265220642
INFO:root:current mean train loss 1.530988034027726
INFO:root:current train perplexity1.0012059211730957
INFO:root:current mean train loss 1.5327357306428577
INFO:root:current train perplexity1.0012080669403076
INFO:root:current mean train loss 1.5329447151758733
INFO:root:current train perplexity1.0012083053588867
INFO:root:current mean train loss 1.5341773530547047
INFO:root:current train perplexity1.0012096166610718
INFO:root:current mean train loss 1.5333644583553612
INFO:root:current train perplexity1.001209020614624
INFO:root:current mean train loss 1.534088590958466
INFO:root:current train perplexity1.0012115240097046
INFO:root:current mean train loss 1.5345646589307742
INFO:root:current train perplexity1.0012109279632568
INFO:root:current mean train loss 1.535006923370125
INFO:root:current train perplexity1.0012108087539673
INFO:root:current mean train loss 1.5360870738600971
INFO:root:current train perplexity1.0012108087539673
INFO:root:current mean train loss 1.5354504401553166
INFO:root:current train perplexity1.0012104511260986
INFO:root:current mean train loss 1.5358724606337983
INFO:root:current train perplexity1.0012110471725464
INFO:root:current mean train loss 1.5368261159512036
INFO:root:current train perplexity1.001212477684021
INFO:root:current mean train loss 1.5371704665938906
INFO:root:current train perplexity1.0012131929397583
INFO:root:current mean train loss 1.5382093862459367
INFO:root:current train perplexity1.001213788986206
INFO:root:current mean train loss 1.5391285970485469
INFO:root:current train perplexity1.001214623451233
INFO:root:current mean train loss 1.5390473727591976
INFO:root:current train perplexity1.0012147426605225
INFO:root:current mean train loss 1.539075517743762
INFO:root:current train perplexity1.0012145042419434
INFO:root:current mean train loss 1.5391615223957271
INFO:root:current train perplexity1.0012149810791016

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.93s/it]
INFO:root:final mean train loss: 1.5394574501150133
INFO:root:final train perplexity: 1.0012156963348389
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.23s/it]
INFO:root:eval mean loss: 1.8834865380686225
INFO:root:eval perplexity: 1.0015252828598022
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it]
INFO:root:eval mean loss: 2.398897681675904
INFO:root:eval perplexity: 1.001974105834961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [10:30:22<25:31:18, 647.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5245323153103099
INFO:root:current train perplexity1.001212239265442
INFO:root:current mean train loss 1.5258197919742482
INFO:root:current train perplexity1.0012054443359375
INFO:root:current mean train loss 1.5259484236700493
INFO:root:current train perplexity1.0012086629867554
INFO:root:current mean train loss 1.5269606745088256
INFO:root:current train perplexity1.001210331916809
INFO:root:current mean train loss 1.5297385896604085
INFO:root:current train perplexity1.0012149810791016
INFO:root:current mean train loss 1.5298391338087554
INFO:root:current train perplexity1.0012127161026
INFO:root:current mean train loss 1.5303274232975759
INFO:root:current train perplexity1.0012108087539673
INFO:root:current mean train loss 1.5300554141876803
INFO:root:current train perplexity1.0012108087539673
INFO:root:current mean train loss 1.530083296123871
INFO:root:current train perplexity1.0012089014053345
INFO:root:current mean train loss 1.5306066338786013
INFO:root:current train perplexity1.001209020614624
INFO:root:current mean train loss 1.5310624916981992
INFO:root:current train perplexity1.0012102127075195
INFO:root:current mean train loss 1.531948547524239
INFO:root:current train perplexity1.0012109279632568
INFO:root:current mean train loss 1.532546111767394
INFO:root:current train perplexity1.001211404800415
INFO:root:current mean train loss 1.5320985992892986
INFO:root:current train perplexity1.00121009349823
INFO:root:current mean train loss 1.5322397162215877
INFO:root:current train perplexity1.00121009349823
INFO:root:current mean train loss 1.5328214689760178
INFO:root:current train perplexity1.0012104511260986
INFO:root:current mean train loss 1.5329794948108117
INFO:root:current train perplexity1.0012109279632568
INFO:root:current mean train loss 1.5337799919085677
INFO:root:current train perplexity1.001211404800415
INFO:root:current mean train loss 1.5339552345579435
INFO:root:current train perplexity1.0012116432189941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:41<00:00, 581.39s/it]
INFO:root:final mean train loss: 1.5339646855208589
INFO:root:final train perplexity: 1.0012112855911255
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.06s/it]
INFO:root:eval mean loss: 1.882923982244857
INFO:root:eval perplexity: 1.001524806022644
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.33s/it]
INFO:root:eval mean loss: 2.3961771544835244
INFO:root:eval perplexity: 1.001971960067749
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [10:41:06<25:18:58, 646.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4316284656524658
INFO:root:current train perplexity1.0011104345321655
INFO:root:current mean train loss 1.5211186035006654
INFO:root:current train perplexity1.001198410987854
INFO:root:current mean train loss 1.518036161909009
INFO:root:current train perplexity1.0011959075927734
INFO:root:current mean train loss 1.521359597610322
INFO:root:current train perplexity1.001197099685669
INFO:root:current mean train loss 1.5217140180554556
INFO:root:current train perplexity1.001198649406433
INFO:root:current mean train loss 1.5225841628602776
INFO:root:current train perplexity1.0011999607086182
INFO:root:current mean train loss 1.5231476841970932
INFO:root:current train perplexity1.0012012720108032
INFO:root:current mean train loss 1.5244005018489653
INFO:root:current train perplexity1.0012034177780151
INFO:root:current mean train loss 1.5243541225233577
INFO:root:current train perplexity1.0012025833129883
INFO:root:current mean train loss 1.524989627549495
INFO:root:current train perplexity1.0012034177780151
INFO:root:current mean train loss 1.5258302624354105
INFO:root:current train perplexity1.0012036561965942
INFO:root:current mean train loss 1.5259505747234323
INFO:root:current train perplexity1.001204490661621
INFO:root:current mean train loss 1.5258046644499614
INFO:root:current train perplexity1.001204013824463
INFO:root:current mean train loss 1.526099875409116
INFO:root:current train perplexity1.001203179359436
INFO:root:current mean train loss 1.5263533394959106
INFO:root:current train perplexity1.0012032985687256
INFO:root:current mean train loss 1.5265557238169898
INFO:root:current train perplexity1.0012036561965942
INFO:root:current mean train loss 1.5270872255240784
INFO:root:current train perplexity1.001204490661621
INFO:root:current mean train loss 1.5272217065831328
INFO:root:current train perplexity1.0012050867080688
INFO:root:current mean train loss 1.5273448636343954
INFO:root:current train perplexity1.001205563545227
INFO:root:current mean train loss 1.5275785748138788
INFO:root:current train perplexity1.0012060403823853

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.82s/it]
INFO:root:final mean train loss: 1.5276284045302622
INFO:root:final train perplexity: 1.0012062788009644
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.70s/it]
INFO:root:eval mean loss: 1.8861196700562821
INFO:root:eval perplexity: 1.0015274286270142
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.24s/it]
INFO:root:eval mean loss: 2.4028060858976756
INFO:root:eval perplexity: 1.0019773244857788
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [10:51:53<25:08:32, 646.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5149427526875545
INFO:root:current train perplexity1.0011903047561646
INFO:root:current mean train loss 1.5060616350975358
INFO:root:current train perplexity1.0011883974075317
INFO:root:current mean train loss 1.5055512892056817
INFO:root:current train perplexity1.0011847019195557
INFO:root:current mean train loss 1.5104489939339856
INFO:root:current train perplexity1.0011883974075317
INFO:root:current mean train loss 1.5109567918185687
INFO:root:current train perplexity1.001190423965454
INFO:root:current mean train loss 1.5123083400818194
INFO:root:current train perplexity1.0011926889419556
INFO:root:current mean train loss 1.5140101420474938
INFO:root:current train perplexity1.001193642616272
INFO:root:current mean train loss 1.5143829114911287
INFO:root:current train perplexity1.001194715499878
INFO:root:current mean train loss 1.5153958750877334
INFO:root:current train perplexity1.0011955499649048
INFO:root:current mean train loss 1.5162048959887715
INFO:root:current train perplexity1.0011959075927734
INFO:root:current mean train loss 1.5163858959321537
INFO:root:current train perplexity1.001197099685669
INFO:root:current mean train loss 1.517217198901138
INFO:root:current train perplexity1.0011980533599854
INFO:root:current mean train loss 1.5171863031348212
INFO:root:current train perplexity1.0011976957321167
INFO:root:current mean train loss 1.5182227259607728
INFO:root:current train perplexity1.0011988878250122
INFO:root:current mean train loss 1.5195354086316077
INFO:root:current train perplexity1.0011996030807495
INFO:root:current mean train loss 1.520958914656322
INFO:root:current train perplexity1.0012013912200928
INFO:root:current mean train loss 1.5210503564343325
INFO:root:current train perplexity1.0012019872665405
INFO:root:current mean train loss 1.5209325558228017
INFO:root:current train perplexity1.0012011528015137
INFO:root:current mean train loss 1.5208631276166853
INFO:root:current train perplexity1.0012006759643555
INFO:root:current mean train loss 1.5208437190570205
INFO:root:current train perplexity1.001200556755066

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.09s/it]
INFO:root:final mean train loss: 1.5212445235649144
INFO:root:final train perplexity: 1.0012012720108032
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.35s/it]
INFO:root:eval mean loss: 1.8896324038505554
INFO:root:eval perplexity: 1.0015302896499634
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.86s/it]
INFO:root:eval mean loss: 2.407503126783574
INFO:root:eval perplexity: 1.001981258392334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [11:02:46<25:01:51, 648.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.5053260491953955
INFO:root:current train perplexity1.0011982917785645
INFO:root:current mean train loss 1.505388625404414
INFO:root:current train perplexity1.0012011528015137
INFO:root:current mean train loss 1.505232334136963
INFO:root:current train perplexity1.0011945962905884
INFO:root:current mean train loss 1.5061933043457212
INFO:root:current train perplexity1.0011948347091675
INFO:root:current mean train loss 1.5051632356753044
INFO:root:current train perplexity1.0011930465698242
INFO:root:current mean train loss 1.5046244239184394
INFO:root:current train perplexity1.0011882781982422
INFO:root:current mean train loss 1.5071293822249527
INFO:root:current train perplexity1.0011916160583496
INFO:root:current mean train loss 1.5067284829590633
INFO:root:current train perplexity1.0011898279190063
INFO:root:current mean train loss 1.5081836900642613
INFO:root:current train perplexity1.001191258430481
INFO:root:current mean train loss 1.5082496011104338
INFO:root:current train perplexity1.0011917352676392
INFO:root:current mean train loss 1.5098961258026624
INFO:root:current train perplexity1.0011930465698242
INFO:root:current mean train loss 1.5103320551800057
INFO:root:current train perplexity1.0011930465698242
INFO:root:current mean train loss 1.5110193795756615
INFO:root:current train perplexity1.0011932849884033
INFO:root:current mean train loss 1.5123966706548622
INFO:root:current train perplexity1.0011948347091675
INFO:root:current mean train loss 1.512599341633592
INFO:root:current train perplexity1.0011951923370361
INFO:root:current mean train loss 1.5131567775582273
INFO:root:current train perplexity1.0011948347091675
INFO:root:current mean train loss 1.5135694175302836
INFO:root:current train perplexity1.0011951923370361
INFO:root:current mean train loss 1.5141882443894987
INFO:root:current train perplexity1.0011959075927734
INFO:root:current mean train loss 1.5143209541934768
INFO:root:current train perplexity1.0011959075927734
INFO:root:current mean train loss 1.5150079733330357
INFO:root:current train perplexity1.0011963844299316

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.06s/it]
INFO:root:final mean train loss: 1.5152759655400834
INFO:root:final train perplexity: 1.0011966228485107
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.19s/it]
INFO:root:eval mean loss: 1.8913950514286122
INFO:root:eval perplexity: 1.001531720161438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.88s/it]
INFO:root:eval mean loss: 2.410294562789565
INFO:root:eval perplexity: 1.0019835233688354
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [11:13:28<24:46:55, 646.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4870298696014117
INFO:root:current train perplexity1.0011696815490723
INFO:root:current mean train loss 1.5004661971447515
INFO:root:current train perplexity1.0011848211288452
INFO:root:current mean train loss 1.5040374848211235
INFO:root:current train perplexity1.0011868476867676
INFO:root:current mean train loss 1.5036508283939307
INFO:root:current train perplexity1.0011850595474243
INFO:root:current mean train loss 1.5014689881806869
INFO:root:current train perplexity1.001183032989502
INFO:root:current mean train loss 1.5028262140522382
INFO:root:current train perplexity1.0011870861053467
INFO:root:current mean train loss 1.5012342719900298
INFO:root:current train perplexity1.0011838674545288
INFO:root:current mean train loss 1.5019644772705645
INFO:root:current train perplexity1.0011839866638184
INFO:root:current mean train loss 1.5023636309231292
INFO:root:current train perplexity1.001184105873108
INFO:root:current mean train loss 1.5036505500767439
INFO:root:current train perplexity1.0011861324310303
INFO:root:current mean train loss 1.5049605506551005
INFO:root:current train perplexity1.0011876821517944
INFO:root:current mean train loss 1.5048657183221186
INFO:root:current train perplexity1.0011876821517944
INFO:root:current mean train loss 1.505622492917328
INFO:root:current train perplexity1.00118887424469
INFO:root:current mean train loss 1.5066449504733879
INFO:root:current train perplexity1.0011895895004272
INFO:root:current mean train loss 1.506902645640101
INFO:root:current train perplexity1.0011895895004272
INFO:root:current mean train loss 1.5076761372382612
INFO:root:current train perplexity1.0011903047561646
INFO:root:current mean train loss 1.5077815071712892
INFO:root:current train perplexity1.0011905431747437
INFO:root:current mean train loss 1.5090262913663117
INFO:root:current train perplexity1.0011913776397705
INFO:root:current mean train loss 1.508837911546134
INFO:root:current train perplexity1.0011910200119019
INFO:root:current mean train loss 1.5089357773951244
INFO:root:current train perplexity1.001191258430481

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:45<00:00, 585.17s/it]
INFO:root:final mean train loss: 1.5089476206539498
INFO:root:final train perplexity: 1.0011916160583496
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.48s/it]
INFO:root:eval mean loss: 1.8926497723193878
INFO:root:eval perplexity: 1.001532793045044
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.75s/it]
INFO:root:eval mean loss: 2.4125707572233592
INFO:root:eval perplexity: 1.0019854307174683
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [11:24:20<24:39:36, 648.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4767609528132848
INFO:root:current train perplexity1.001163125038147
INFO:root:current mean train loss 1.4861750174971189
INFO:root:current train perplexity1.0011749267578125
INFO:root:current mean train loss 1.4938082346209773
INFO:root:current train perplexity1.0011793375015259
INFO:root:current mean train loss 1.4951444451873366
INFO:root:current train perplexity1.0011802911758423
INFO:root:current mean train loss 1.4960072306876486
INFO:root:current train perplexity1.0011812448501587
INFO:root:current mean train loss 1.4952903241441962
INFO:root:current train perplexity1.0011788606643677
INFO:root:current mean train loss 1.4957266030026906
INFO:root:current train perplexity1.0011800527572632
INFO:root:current mean train loss 1.4958794904993726
INFO:root:current train perplexity1.00117826461792
INFO:root:current mean train loss 1.49746348186471
INFO:root:current train perplexity1.001180648803711
INFO:root:current mean train loss 1.4981575538202658
INFO:root:current train perplexity1.00118088722229
INFO:root:current mean train loss 1.4982195001896297
INFO:root:current train perplexity1.0011807680130005
INFO:root:current mean train loss 1.4991546426063929
INFO:root:current train perplexity1.0011823177337646
INFO:root:current mean train loss 1.499756428669757
INFO:root:current train perplexity1.0011826753616333
INFO:root:current mean train loss 1.5002956059727355
INFO:root:current train perplexity1.001183271408081
INFO:root:current mean train loss 1.501035928158533
INFO:root:current train perplexity1.0011838674545288
INFO:root:current mean train loss 1.5023161661093403
INFO:root:current train perplexity1.0011850595474243
INFO:root:current mean train loss 1.5026035340960153
INFO:root:current train perplexity1.0011858940124512
INFO:root:current mean train loss 1.5028567035992941
INFO:root:current train perplexity1.0011863708496094
INFO:root:current mean train loss 1.503388693115928
INFO:root:current train perplexity1.0011874437332153
INFO:root:current mean train loss 1.5032643286104734
INFO:root:current train perplexity1.0011868476867676

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.29s/it]
INFO:root:final mean train loss: 1.5031645156732425
INFO:root:final train perplexity: 1.0011869668960571
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.45s/it]
INFO:root:eval mean loss: 1.8947260231836467
INFO:root:eval perplexity: 1.0015344619750977
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.64s/it]
INFO:root:eval mean loss: 2.4178466035964643
INFO:root:eval perplexity: 1.001989722251892
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [11:35:02<24:25:04, 646.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4834944105696404
INFO:root:current train perplexity1.001172661781311
INFO:root:current mean train loss 1.4888156633326077
INFO:root:current train perplexity1.0011796951293945
INFO:root:current mean train loss 1.4915345624764207
INFO:root:current train perplexity1.0011820793151855
INFO:root:current mean train loss 1.492272260885214
INFO:root:current train perplexity1.0011802911758423
INFO:root:current mean train loss 1.4912417147928194
INFO:root:current train perplexity1.0011776685714722
INFO:root:current mean train loss 1.4898957468988137
INFO:root:current train perplexity1.0011769533157349
INFO:root:current mean train loss 1.4910635830186514
INFO:root:current train perplexity1.0011768341064453
INFO:root:current mean train loss 1.49162253642779
INFO:root:current train perplexity1.0011768341064453
INFO:root:current mean train loss 1.4916052579073384
INFO:root:current train perplexity1.0011756420135498
INFO:root:current mean train loss 1.4933907741712824
INFO:root:current train perplexity1.001178503036499
INFO:root:current mean train loss 1.4940013601457514
INFO:root:current train perplexity1.0011793375015259
INFO:root:current mean train loss 1.4952604007399575
INFO:root:current train perplexity1.0011816024780273
INFO:root:current mean train loss 1.4954447650872373
INFO:root:current train perplexity1.001182198524475
INFO:root:current mean train loss 1.4949814542730506
INFO:root:current train perplexity1.0011814832687378
INFO:root:current mean train loss 1.4953655605758687
INFO:root:current train perplexity1.0011814832687378
INFO:root:current mean train loss 1.4952758191588846
INFO:root:current train perplexity1.00118088722229
INFO:root:current mean train loss 1.495478278727099
INFO:root:current train perplexity1.00118088722229
INFO:root:current mean train loss 1.4962572390364746
INFO:root:current train perplexity1.001181721687317
INFO:root:current mean train loss 1.4969400427110244
INFO:root:current train perplexity1.001182198524475

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:50<00:00, 590.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:50<00:00, 590.96s/it]
INFO:root:final mean train loss: 1.496945868521944
INFO:root:final train perplexity: 1.0011820793151855
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it]
INFO:root:eval mean loss: 1.895944303231882
INFO:root:eval perplexity: 1.001535415649414
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.19s/it]
INFO:root:eval mean loss: 2.4209138894757483
INFO:root:eval perplexity: 1.0019922256469727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [11:46:00<24:22:00, 649.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.455531120300293
INFO:root:current train perplexity1.001198410987854
INFO:root:current mean train loss 1.4759604529692576
INFO:root:current train perplexity1.0011690855026245
INFO:root:current mean train loss 1.4785820970348282
INFO:root:current train perplexity1.0011745691299438
INFO:root:current mean train loss 1.4783961588614865
INFO:root:current train perplexity1.001173734664917
INFO:root:current mean train loss 1.4818254139753853
INFO:root:current train perplexity1.0011746883392334
INFO:root:current mean train loss 1.4827423906988568
INFO:root:current train perplexity1.0011744499206543
INFO:root:current mean train loss 1.4845528424970362
INFO:root:current train perplexity1.0011738538742065
INFO:root:current mean train loss 1.4852128066122532
INFO:root:current train perplexity1.0011738538742065
INFO:root:current mean train loss 1.4861306213027803
INFO:root:current train perplexity1.0011732578277588
INFO:root:current mean train loss 1.4860048089670923
INFO:root:current train perplexity1.0011718273162842
INFO:root:current mean train loss 1.4863527858874712
INFO:root:current train perplexity1.0011723041534424
INFO:root:current mean train loss 1.487448939918608
INFO:root:current train perplexity1.001173496246338
INFO:root:current mean train loss 1.4885230602022026
INFO:root:current train perplexity1.0011751651763916
INFO:root:current mean train loss 1.4887930504017812
INFO:root:current train perplexity1.0011754035949707
INFO:root:current mean train loss 1.4903191682959553
INFO:root:current train perplexity1.001177430152893
INFO:root:current mean train loss 1.4906705265983622
INFO:root:current train perplexity1.0011781454086304
INFO:root:current mean train loss 1.4918358898222297
INFO:root:current train perplexity1.0011792182922363
INFO:root:current mean train loss 1.492035963725596
INFO:root:current train perplexity1.0011787414550781
INFO:root:current mean train loss 1.4920595907451308
INFO:root:current train perplexity1.0011780261993408
INFO:root:current mean train loss 1.4919782026725656
INFO:root:current train perplexity1.0011775493621826

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.30s/it]
INFO:root:final mean train loss: 1.4917586346918204
INFO:root:final train perplexity: 1.0011780261993408
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.28s/it]
INFO:root:eval mean loss: 1.898438387306024
INFO:root:eval perplexity: 1.0015374422073364
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.51s/it]
INFO:root:eval mean loss: 2.4237624969042786
INFO:root:eval perplexity: 1.0019946098327637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [11:56:48<24:10:06, 649.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.473359329359872
INFO:root:current train perplexity1.0011701583862305
INFO:root:current mean train loss 1.47872504813612
INFO:root:current train perplexity1.0011664628982544
INFO:root:current mean train loss 1.4775230404478392
INFO:root:current train perplexity1.0011651515960693
INFO:root:current mean train loss 1.4748896308405748
INFO:root:current train perplexity1.0011643171310425
INFO:root:current mean train loss 1.4774301009619888
INFO:root:current train perplexity1.0011669397354126
INFO:root:current mean train loss 1.4794761638037304
INFO:root:current train perplexity1.0011698007583618
INFO:root:current mean train loss 1.4801415604285764
INFO:root:current train perplexity1.0011699199676514
INFO:root:current mean train loss 1.4806521347260178
INFO:root:current train perplexity1.0011712312698364
INFO:root:current mean train loss 1.4807331138057918
INFO:root:current train perplexity1.0011712312698364
INFO:root:current mean train loss 1.4803091384688885
INFO:root:current train perplexity1.001168966293335
INFO:root:current mean train loss 1.4803018413491393
INFO:root:current train perplexity1.0011682510375977
INFO:root:current mean train loss 1.481991481398175
INFO:root:current train perplexity1.0011694431304932
INFO:root:current mean train loss 1.4825047920416068
INFO:root:current train perplexity1.0011701583862305
INFO:root:current mean train loss 1.4833826660478233
INFO:root:current train perplexity1.0011709928512573
INFO:root:current mean train loss 1.4836178424910371
INFO:root:current train perplexity1.00117027759552
INFO:root:current mean train loss 1.4847190288396983
INFO:root:current train perplexity1.0011712312698364
INFO:root:current mean train loss 1.4848736879047533
INFO:root:current train perplexity1.001171588897705
INFO:root:current mean train loss 1.4854999905336326
INFO:root:current train perplexity1.0011725425720215
INFO:root:current mean train loss 1.485716750633579
INFO:root:current train perplexity1.0011727809906006
INFO:root:current mean train loss 1.4864861357032104
INFO:root:current train perplexity1.0011733770370483

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.80s/it]
INFO:root:final mean train loss: 1.4868214569245692
INFO:root:final train perplexity: 1.0011740922927856
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.53s/it]
INFO:root:eval mean loss: 1.9021700654469482
INFO:root:eval perplexity: 1.0015404224395752
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.49s/it]
INFO:root:eval mean loss: 2.429045780330685
INFO:root:eval perplexity: 1.001999020576477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [12:07:31<23:55:00, 647.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4560362822131108
INFO:root:current train perplexity1.001146912574768
INFO:root:current mean train loss 1.4657848796982695
INFO:root:current train perplexity1.001158595085144
INFO:root:current mean train loss 1.468761500190286
INFO:root:current train perplexity1.0011601448059082
INFO:root:current mean train loss 1.4704199197024284
INFO:root:current train perplexity1.0011636018753052
INFO:root:current mean train loss 1.4717962779955232
INFO:root:current train perplexity1.0011643171310425
INFO:root:current mean train loss 1.473373810154798
INFO:root:current train perplexity1.0011661052703857
INFO:root:current mean train loss 1.4740786531875874
INFO:root:current train perplexity1.0011664628982544
INFO:root:current mean train loss 1.474827428335743
INFO:root:current train perplexity1.0011667013168335
INFO:root:current mean train loss 1.4753189384226013
INFO:root:current train perplexity1.0011659860610962
INFO:root:current mean train loss 1.4760341723082162
INFO:root:current train perplexity1.0011658668518066
INFO:root:current mean train loss 1.4763805332211402
INFO:root:current train perplexity1.0011649131774902
INFO:root:current mean train loss 1.4770776634685603
INFO:root:current train perplexity1.001165747642517
INFO:root:current mean train loss 1.4782991480557715
INFO:root:current train perplexity1.0011667013168335
INFO:root:current mean train loss 1.4779973437255096
INFO:root:current train perplexity1.0011659860610962
INFO:root:current mean train loss 1.4786653385208777
INFO:root:current train perplexity1.0011670589447021
INFO:root:current mean train loss 1.4790500727989584
INFO:root:current train perplexity1.0011677742004395
INFO:root:current mean train loss 1.4795456923844614
INFO:root:current train perplexity1.0011682510375977
INFO:root:current mean train loss 1.4800346241995983
INFO:root:current train perplexity1.0011684894561768
INFO:root:current mean train loss 1.4803825575327327
INFO:root:current train perplexity1.0011687278747559
INFO:root:current mean train loss 1.4807455242602818
INFO:root:current train perplexity1.0011687278747559

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.85s/it]
INFO:root:final mean train loss: 1.4808158014257087
INFO:root:final train perplexity: 1.0011693239212036
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.72s/it]
INFO:root:eval mean loss: 1.9013903834295611
INFO:root:eval perplexity: 1.0015398263931274
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.02s/it]
INFO:root:eval mean loss: 2.428337519895946
INFO:root:eval perplexity: 1.0019984245300293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [12:18:17<23:43:03, 646.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4644467613913796
INFO:root:current train perplexity1.0011558532714844
INFO:root:current mean train loss 1.465954142232095
INFO:root:current train perplexity1.0011588335037231
INFO:root:current mean train loss 1.4622711349936093
INFO:root:current train perplexity1.0011528730392456
INFO:root:current mean train loss 1.4634660573072837
INFO:root:current train perplexity1.0011541843414307
INFO:root:current mean train loss 1.4651704683408633
INFO:root:current train perplexity1.0011557340621948
INFO:root:current mean train loss 1.464338762266142
INFO:root:current train perplexity1.0011537075042725
INFO:root:current mean train loss 1.4677218471774618
INFO:root:current train perplexity1.0011582374572754
INFO:root:current mean train loss 1.4683304987206365
INFO:root:current train perplexity1.0011577606201172
INFO:root:current mean train loss 1.4682659441964667
INFO:root:current train perplexity1.0011581182479858
INFO:root:current mean train loss 1.4698826389162953
INFO:root:current train perplexity1.0011603832244873
INFO:root:current mean train loss 1.4703201848748735
INFO:root:current train perplexity1.0011602640151978
INFO:root:current mean train loss 1.4705911751949425
INFO:root:current train perplexity1.001160740852356
INFO:root:current mean train loss 1.4719396079203997
INFO:root:current train perplexity1.0011622905731201
INFO:root:current mean train loss 1.4728638249569714
INFO:root:current train perplexity1.0011630058288574
INFO:root:current mean train loss 1.473122339969648
INFO:root:current train perplexity1.0011627674102783
INFO:root:current mean train loss 1.474090026735876
INFO:root:current train perplexity1.0011636018753052
INFO:root:current mean train loss 1.4744490173287983
INFO:root:current train perplexity1.0011639595031738
INFO:root:current mean train loss 1.4747989115891633
INFO:root:current train perplexity1.0011645555496216
INFO:root:current mean train loss 1.4748662376018227
INFO:root:current train perplexity1.001164197921753
INFO:root:current mean train loss 1.4755000859575198
INFO:root:current train perplexity1.0011647939682007

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.87s/it]
INFO:root:final mean train loss: 1.4755622519427216
INFO:root:final train perplexity: 1.0011651515960693
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:33<00:00, 33.56s/it]
INFO:root:eval mean loss: 1.9051298245470574
INFO:root:eval perplexity: 1.0015428066253662
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:34<00:00, 34.11s/it]
INFO:root:eval mean loss: 2.4340130741714585
INFO:root:eval perplexity: 1.0020030736923218
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [12:29:03<23:32:05, 646.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4642685701449711
INFO:root:current train perplexity1.0011588335037231
INFO:root:current mean train loss 1.464306304621142
INFO:root:current train perplexity1.0011521577835083
INFO:root:current mean train loss 1.4668259778443504
INFO:root:current train perplexity1.0011576414108276
INFO:root:current mean train loss 1.4637133036890337
INFO:root:current train perplexity1.0011532306671143
INFO:root:current mean train loss 1.4631445566981525
INFO:root:current train perplexity1.001152753829956
INFO:root:current mean train loss 1.4645940409256861
INFO:root:current train perplexity1.0011540651321411
INFO:root:current mean train loss 1.464860509903658
INFO:root:current train perplexity1.001152753829956
INFO:root:current mean train loss 1.465213352071189
INFO:root:current train perplexity1.001153588294983
INFO:root:current mean train loss 1.4652655344764027
INFO:root:current train perplexity1.0011550188064575
INFO:root:current mean train loss 1.4651839354155975
INFO:root:current train perplexity1.0011552572250366
INFO:root:current mean train loss 1.4654390466079783
INFO:root:current train perplexity1.0011560916900635
INFO:root:current mean train loss 1.4654818479315006
INFO:root:current train perplexity1.0011563301086426
INFO:root:current mean train loss 1.465560293328837
INFO:root:current train perplexity1.0011565685272217
INFO:root:current mean train loss 1.466545869327495
INFO:root:current train perplexity1.0011578798294067
INFO:root:current mean train loss 1.4671460859801457
INFO:root:current train perplexity1.0011581182479858
INFO:root:current mean train loss 1.4669632490051308
INFO:root:current train perplexity1.0011574029922485
INFO:root:current mean train loss 1.4675321177575007
INFO:root:current train perplexity1.001157283782959
INFO:root:current mean train loss 1.4684763935564873
INFO:root:current train perplexity1.0011588335037231
INFO:root:current mean train loss 1.4687652319797084
INFO:root:current train perplexity1.0011589527130127
INFO:root:current mean train loss 1.469098117603008
INFO:root:current train perplexity1.0011595487594604

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:49<00:00, 589.36s/it]
INFO:root:final mean train loss: 1.4692154534224964
INFO:root:final train perplexity: 1.0011601448059082
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.81s/it]
INFO:root:eval mean loss: 1.906314620312224
INFO:root:eval perplexity: 1.0015437602996826
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it]
INFO:root:eval mean loss: 2.4345029283922615
INFO:root:eval perplexity: 1.0020034313201904
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [12:39:59<23:27:06, 649.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.450040115399307
INFO:root:current train perplexity1.001143455505371
INFO:root:current mean train loss 1.445838172599752
INFO:root:current train perplexity1.0011370182037354
INFO:root:current mean train loss 1.4506716583839336
INFO:root:current train perplexity1.001143217086792
INFO:root:current mean train loss 1.4506440980881836
INFO:root:current train perplexity1.0011426210403442
INFO:root:current mean train loss 1.45203053756238
INFO:root:current train perplexity1.0011447668075562
INFO:root:current mean train loss 1.4545948135023004
INFO:root:current train perplexity1.0011478662490845
INFO:root:current mean train loss 1.4567636540389373
INFO:root:current train perplexity1.0011494159698486
INFO:root:current mean train loss 1.4571720271056143
INFO:root:current train perplexity1.0011494159698486
INFO:root:current mean train loss 1.4582348009330484
INFO:root:current train perplexity1.0011504888534546
INFO:root:current mean train loss 1.4593898867934008
INFO:root:current train perplexity1.0011515617370605
INFO:root:current mean train loss 1.4607697457102486
INFO:root:current train perplexity1.0011529922485352
INFO:root:current mean train loss 1.4615641234801335
INFO:root:current train perplexity1.0011532306671143
INFO:root:current mean train loss 1.461901531559816
INFO:root:current train perplexity1.0011539459228516
INFO:root:current mean train loss 1.4625511066445007
INFO:root:current train perplexity1.0011540651321411
INFO:root:current mean train loss 1.4627364384719714
INFO:root:current train perplexity1.0011546611785889
INFO:root:current mean train loss 1.4631993695577785
INFO:root:current train perplexity1.0011552572250366
INFO:root:current mean train loss 1.4638698693914058
INFO:root:current train perplexity1.0011560916900635
INFO:root:current mean train loss 1.4641261619852535
INFO:root:current train perplexity1.0011564493179321
INFO:root:current mean train loss 1.4646417627642558
INFO:root:current train perplexity1.0011568069458008

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:40<00:00, 580.86s/it]
INFO:root:final mean train loss: 1.4648165638496102
INFO:root:final train perplexity: 1.0011566877365112
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.50s/it]
INFO:root:eval mean loss: 1.9091138522675697
INFO:root:eval perplexity: 1.001546025276184
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.20s/it]
INFO:root:eval mean loss: 2.4399218795992805
INFO:root:eval perplexity: 1.0020079612731934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [12:50:44<23:13:15, 648.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4661307136217754
INFO:root:current train perplexity1.0011992454528809
INFO:root:current mean train loss 1.448085756796711
INFO:root:current train perplexity1.0011485815048218
INFO:root:current mean train loss 1.450741460022417
INFO:root:current train perplexity1.0011476278305054
INFO:root:current mean train loss 1.4485134692753063
INFO:root:current train perplexity1.0011407136917114
INFO:root:current mean train loss 1.4506672550304769
INFO:root:current train perplexity1.0011438131332397
INFO:root:current mean train loss 1.4534179483477778
INFO:root:current train perplexity1.0011467933654785
INFO:root:current mean train loss 1.4549354478077527
INFO:root:current train perplexity1.0011487007141113
INFO:root:current mean train loss 1.456220158774184
INFO:root:current train perplexity1.00114905834198
INFO:root:current mean train loss 1.4575926282861393
INFO:root:current train perplexity1.0011515617370605
INFO:root:current mean train loss 1.4576290239298317
INFO:root:current train perplexity1.0011518001556396
INFO:root:current mean train loss 1.4580096520676054
INFO:root:current train perplexity1.0011523962020874
INFO:root:current mean train loss 1.4580117331491025
INFO:root:current train perplexity1.0011529922485352
INFO:root:current mean train loss 1.4584592587714567
INFO:root:current train perplexity1.0011528730392456
INFO:root:current mean train loss 1.4583670137304625
INFO:root:current train perplexity1.001152753829956
INFO:root:current mean train loss 1.458447776173121
INFO:root:current train perplexity1.001152753829956
INFO:root:current mean train loss 1.4585231371609813
INFO:root:current train perplexity1.0011526346206665
INFO:root:current mean train loss 1.45883984509322
INFO:root:current train perplexity1.0011526346206665
INFO:root:current mean train loss 1.4590743048529273
INFO:root:current train perplexity1.0011526346206665
INFO:root:current mean train loss 1.4590935685309856
INFO:root:current train perplexity1.0011523962020874
INFO:root:current mean train loss 1.459160703051628
INFO:root:current train perplexity1.0011521577835083

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.29s/it]
INFO:root:final mean train loss: 1.4595085365629172
INFO:root:final train perplexity: 1.001152515411377
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.81s/it]
INFO:root:eval mean loss: 1.908499852139899
INFO:root:eval perplexity: 1.0015455484390259
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2.4388918576511087
INFO:root:eval perplexity: 1.0020071268081665
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [13:01:33<23:03:38, 648.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.440451590911202
INFO:root:current train perplexity1.001150131225586
INFO:root:current mean train loss 1.4403516655045796
INFO:root:current train perplexity1.0011415481567383
INFO:root:current mean train loss 1.437369901503148
INFO:root:current train perplexity1.0011367797851562
INFO:root:current mean train loss 1.4424868209443225
INFO:root:current train perplexity1.0011379718780518
INFO:root:current mean train loss 1.4446525694912489
INFO:root:current train perplexity1.0011422634124756
INFO:root:current mean train loss 1.4467089625207235
INFO:root:current train perplexity1.0011439323425293
INFO:root:current mean train loss 1.4469061640254184
INFO:root:current train perplexity1.001142978668213
INFO:root:current mean train loss 1.4480139778857408
INFO:root:current train perplexity1.001142978668213
INFO:root:current mean train loss 1.4490371635443964
INFO:root:current train perplexity1.0011433362960815
INFO:root:current mean train loss 1.4494773619611498
INFO:root:current train perplexity1.0011441707611084
INFO:root:current mean train loss 1.449397374224919
INFO:root:current train perplexity1.0011438131332397
INFO:root:current mean train loss 1.449711844417949
INFO:root:current train perplexity1.0011438131332397
INFO:root:current mean train loss 1.4505540397648722
INFO:root:current train perplexity1.0011444091796875
INFO:root:current mean train loss 1.4502907090356472
INFO:root:current train perplexity1.001144289970398
INFO:root:current mean train loss 1.4505777841542526
INFO:root:current train perplexity1.0011447668075562
INFO:root:current mean train loss 1.45134170119573
INFO:root:current train perplexity1.0011457204818726
INFO:root:current mean train loss 1.451601643218571
INFO:root:current train perplexity1.0011452436447144
INFO:root:current mean train loss 1.4523847746558474
INFO:root:current train perplexity1.0011460781097412
INFO:root:current mean train loss 1.4528921618935047
INFO:root:current train perplexity1.0011464357376099
INFO:root:current mean train loss 1.4531597489189867
INFO:root:current train perplexity1.001146912574768

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:13<00:00, 673.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:13<00:00, 673.59s/it]
INFO:root:final mean train loss: 1.4533121983811401
INFO:root:final train perplexity: 1.0011476278305054
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.85s/it]
INFO:root:eval mean loss: 1.9140528081156682
INFO:root:eval perplexity: 1.0015500783920288
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.96s/it]
INFO:root:eval mean loss: 2.4457722386569842
INFO:root:eval perplexity: 1.0020127296447754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [13:14:04<23:57:31, 679.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4221409946680068
INFO:root:current train perplexity1.0011276006698608
INFO:root:current mean train loss 1.4324290548052108
INFO:root:current train perplexity1.0011285543441772
INFO:root:current mean train loss 1.431599223613739
INFO:root:current train perplexity1.0011322498321533
INFO:root:current mean train loss 1.4338046280776753
INFO:root:current train perplexity1.0011330842971802
INFO:root:current mean train loss 1.4370904889973728
INFO:root:current train perplexity1.0011332035064697
INFO:root:current mean train loss 1.4394335744557558
INFO:root:current train perplexity1.0011351108551025
INFO:root:current mean train loss 1.4392163172364234
INFO:root:current train perplexity1.0011345148086548
INFO:root:current mean train loss 1.4406074989486386
INFO:root:current train perplexity1.0011361837387085
INFO:root:current mean train loss 1.441066756560689
INFO:root:current train perplexity1.0011377334594727
INFO:root:current mean train loss 1.4418775666267314
INFO:root:current train perplexity1.0011385679244995
INFO:root:current mean train loss 1.444389298099738
INFO:root:current train perplexity1.00114107131958
INFO:root:current mean train loss 1.4441148369981531
INFO:root:current train perplexity1.001140832901001
INFO:root:current mean train loss 1.4442114129181831
INFO:root:current train perplexity1.0011402368545532
INFO:root:current mean train loss 1.4451509735477504
INFO:root:current train perplexity1.0011414289474487
INFO:root:current mean train loss 1.445550119380156
INFO:root:current train perplexity1.0011414289474487
INFO:root:current mean train loss 1.445780759198325
INFO:root:current train perplexity1.0011411905288696
INFO:root:current mean train loss 1.4458416437957344
INFO:root:current train perplexity1.0011414289474487
INFO:root:current mean train loss 1.446453021381093
INFO:root:current train perplexity1.0011423826217651
INFO:root:current mean train loss 1.4474847896591476
INFO:root:current train perplexity1.0011435747146606
INFO:root:current mean train loss 1.4476545873990994
INFO:root:current train perplexity1.0011433362960815

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:37<00:00, 637.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:37<00:00, 637.31s/it]
INFO:root:final mean train loss: 1.4479765536144293
INFO:root:final train perplexity: 1.0011433362960815
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.06s/it]
INFO:root:eval mean loss: 1.912467840292775
INFO:root:eval perplexity: 1.0015487670898438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it]
INFO:root:eval mean loss: 2.445590211567304
INFO:root:eval perplexity: 1.0020126104354858
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [13:25:47<24:01:08, 686.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4317125625777662
INFO:root:current train perplexity1.001140832901001
INFO:root:current mean train loss 1.4335486046068229
INFO:root:current train perplexity1.001129150390625
INFO:root:current mean train loss 1.4358636122269388
INFO:root:current train perplexity1.001132607460022
INFO:root:current mean train loss 1.4369933167759443
INFO:root:current train perplexity1.0011334419250488
INFO:root:current mean train loss 1.4374411246969612
INFO:root:current train perplexity1.0011342763900757
INFO:root:current mean train loss 1.4391821449492093
INFO:root:current train perplexity1.0011368989944458
INFO:root:current mean train loss 1.4397325597397268
INFO:root:current train perplexity1.001137614250183
INFO:root:current mean train loss 1.4397828434071107
INFO:root:current train perplexity1.0011383295059204
INFO:root:current mean train loss 1.439901672416696
INFO:root:current train perplexity1.0011389255523682
INFO:root:current mean train loss 1.439547651489202
INFO:root:current train perplexity1.001137614250183
INFO:root:current mean train loss 1.4408140550160475
INFO:root:current train perplexity1.0011389255523682
INFO:root:current mean train loss 1.4405378368051276
INFO:root:current train perplexity1.0011388063430786
INFO:root:current mean train loss 1.4411104811309536
INFO:root:current train perplexity1.0011388063430786
INFO:root:current mean train loss 1.4415012248403989
INFO:root:current train perplexity1.0011388063430786
INFO:root:current mean train loss 1.4421704466734018
INFO:root:current train perplexity1.0011398792266846
INFO:root:current mean train loss 1.44224206361544
INFO:root:current train perplexity1.001139521598816
INFO:root:current mean train loss 1.4424324732770673
INFO:root:current train perplexity1.0011394023895264
INFO:root:current mean train loss 1.4424121816116229
INFO:root:current train perplexity1.0011388063430786
INFO:root:current mean train loss 1.4429559534958247
INFO:root:current train perplexity1.0011392831802368
INFO:root:current mean train loss 1.4436352087882447
INFO:root:current train perplexity1.001139521598816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:51<00:00, 651.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:51<00:00, 651.53s/it]
INFO:root:final mean train loss: 1.4438246693565457
INFO:root:final train perplexity: 1.0011401176452637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it]
INFO:root:eval mean loss: 1.9173893446617938
INFO:root:eval perplexity: 1.0015528202056885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it]
INFO:root:eval mean loss: 2.452709177284376
INFO:root:eval perplexity: 1.0020184516906738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [13:37:53<24:14:33, 698.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4249791470733848
INFO:root:current train perplexity1.0011147260665894
INFO:root:current mean train loss 1.4265851940231762
INFO:root:current train perplexity1.0011191368103027
INFO:root:current mean train loss 1.4313243266439786
INFO:root:current train perplexity1.0011320114135742
INFO:root:current mean train loss 1.432298445446606
INFO:root:current train perplexity1.0011342763900757
INFO:root:current mean train loss 1.4345311960087548
INFO:root:current train perplexity1.0011366605758667
INFO:root:current mean train loss 1.434419382739981
INFO:root:current train perplexity1.001136064529419
INFO:root:current mean train loss 1.4334498516529888
INFO:root:current train perplexity1.0011335611343384
INFO:root:current mean train loss 1.4337838905110223
INFO:root:current train perplexity1.0011330842971802
INFO:root:current mean train loss 1.4344978936749684
INFO:root:current train perplexity1.0011332035064697
INFO:root:current mean train loss 1.4349018086887728
INFO:root:current train perplexity1.0011337995529175
INFO:root:current mean train loss 1.4352630324647857
INFO:root:current train perplexity1.001133918762207
INFO:root:current mean train loss 1.4346752208379911
INFO:root:current train perplexity1.0011335611343384
INFO:root:current mean train loss 1.4344151254912936
INFO:root:current train perplexity1.0011327266693115
INFO:root:current mean train loss 1.4351307416586896
INFO:root:current train perplexity1.0011337995529175
INFO:root:current mean train loss 1.435023962561764
INFO:root:current train perplexity1.0011332035064697
INFO:root:current mean train loss 1.4356314573990798
INFO:root:current train perplexity1.0011335611343384
INFO:root:current mean train loss 1.4361036029625396
INFO:root:current train perplexity1.0011337995529175
INFO:root:current mean train loss 1.436873327583713
INFO:root:current train perplexity1.0011342763900757
INFO:root:current mean train loss 1.437504154132041
INFO:root:current train perplexity1.0011347532272339
INFO:root:current mean train loss 1.4377700283655401
INFO:root:current train perplexity1.001134991645813

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:04<00:00, 664.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:04<00:00, 664.20s/it]
INFO:root:final mean train loss: 1.4377936450628337
INFO:root:final train perplexity: 1.0011353492736816
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.54s/it]
INFO:root:eval mean loss: 1.9189153398182375
INFO:root:eval perplexity: 1.001554012298584
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.51s/it]
INFO:root:eval mean loss: 2.4541661747803927
INFO:root:eval perplexity: 1.0020196437835693
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [13:50:03<24:22:53, 707.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4280263578498757
INFO:root:current train perplexity1.001128911972046
INFO:root:current mean train loss 1.427662122936149
INFO:root:current train perplexity1.0011297464370728
INFO:root:current mean train loss 1.4229429292514972
INFO:root:current train perplexity1.001124382019043
INFO:root:current mean train loss 1.4255349160460256
INFO:root:current train perplexity1.0011262893676758
INFO:root:current mean train loss 1.4261838690569348
INFO:root:current train perplexity1.0011253356933594
INFO:root:current mean train loss 1.4284714173947897
INFO:root:current train perplexity1.00112783908844
INFO:root:current mean train loss 1.428926661459651
INFO:root:current train perplexity1.001129150390625
INFO:root:current mean train loss 1.4295732477672785
INFO:root:current train perplexity1.0011303424835205
INFO:root:current mean train loss 1.4292963497031284
INFO:root:current train perplexity1.001129150390625
INFO:root:current mean train loss 1.4293637657021177
INFO:root:current train perplexity1.0011285543441772
INFO:root:current mean train loss 1.4293964570208932
INFO:root:current train perplexity1.0011281967163086
INFO:root:current mean train loss 1.430997973925721
INFO:root:current train perplexity1.0011308193206787
INFO:root:current mean train loss 1.4313394652144287
INFO:root:current train perplexity1.001131296157837
INFO:root:current mean train loss 1.4321302142098686
INFO:root:current train perplexity1.0011323690414429
INFO:root:current mean train loss 1.4323208426405807
INFO:root:current train perplexity1.0011322498321533
INFO:root:current mean train loss 1.4328050550464413
INFO:root:current train perplexity1.0011322498321533
INFO:root:current mean train loss 1.433242702752017
INFO:root:current train perplexity1.0011327266693115
INFO:root:current mean train loss 1.4333948424114056
INFO:root:current train perplexity1.0011322498321533
INFO:root:current mean train loss 1.4332552020477909
INFO:root:current train perplexity1.001131534576416

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:26<00:00, 626.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:26<00:00, 626.13s/it]
INFO:root:final mean train loss: 1.4331952089982025
INFO:root:final train perplexity: 1.0011317729949951
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.18s/it]
INFO:root:eval mean loss: 1.922808791306002
INFO:root:eval perplexity: 1.0015572309494019
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.25s/it]
INFO:root:eval mean loss: 2.4588506340135075
INFO:root:eval perplexity: 1.0020235776901245
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [14:01:35<24:01:07, 702.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4260336607694626
INFO:root:current train perplexity1.0010968446731567
INFO:root:current mean train loss 1.4228512545426686
INFO:root:current train perplexity1.0011334419250488
INFO:root:current mean train loss 1.4229700977985675
INFO:root:current train perplexity1.0011301040649414
INFO:root:current mean train loss 1.4231187379979469
INFO:root:current train perplexity1.001128911972046
INFO:root:current mean train loss 1.4220892541548784
INFO:root:current train perplexity1.0011264085769653
INFO:root:current mean train loss 1.4234250164407445
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.423488346369643
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.4231220293516493
INFO:root:current train perplexity1.0011264085769653
INFO:root:current mean train loss 1.4234907391047713
INFO:root:current train perplexity1.0011260509490967
INFO:root:current mean train loss 1.423494554563766
INFO:root:current train perplexity1.001125693321228
INFO:root:current mean train loss 1.4246386877364583
INFO:root:current train perplexity1.0011271238327026
INFO:root:current mean train loss 1.4247243109592893
INFO:root:current train perplexity1.001126766204834
INFO:root:current mean train loss 1.4250849706447677
INFO:root:current train perplexity1.0011272430419922
INFO:root:current mean train loss 1.4258421059777613
INFO:root:current train perplexity1.00112783908844
INFO:root:current mean train loss 1.425254411148754
INFO:root:current train perplexity1.0011268854141235
INFO:root:current mean train loss 1.4256468406564677
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.4266177319472109
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.4269770025090256
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.427469004901637
INFO:root:current train perplexity1.0011274814605713
INFO:root:current mean train loss 1.4275838780827992
INFO:root:current train perplexity1.0011272430419922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:15<00:00, 615.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:15<00:00, 615.59s/it]
INFO:root:final mean train loss: 1.427606732028936
INFO:root:final train perplexity: 1.0011272430419922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.55s/it]
INFO:root:eval mean loss: 1.9238708966167262
INFO:root:eval perplexity: 1.0015580654144287
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.77s/it]
INFO:root:eval mean loss: 2.4619135734037303
INFO:root:eval perplexity: 1.002026081085205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [14:13:10<23:44:29, 700.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4108175754547119
INFO:root:current train perplexity1.001103401184082
INFO:root:current mean train loss 1.410152523994446
INFO:root:current train perplexity1.0011078119277954
INFO:root:current mean train loss 1.416309289932251
INFO:root:current train perplexity1.0011169910430908
INFO:root:current mean train loss 1.4170917910795946
INFO:root:current train perplexity1.0011181831359863
INFO:root:current mean train loss 1.4165772788664874
INFO:root:current train perplexity1.0011159181594849
INFO:root:current mean train loss 1.4174109433946156
INFO:root:current train perplexity1.0011175870895386
INFO:root:current mean train loss 1.4187737730026244
INFO:root:current train perplexity1.0011193752288818
INFO:root:current mean train loss 1.4187825151969646
INFO:root:current train perplexity1.0011188983917236
INFO:root:current mean train loss 1.417851278998635
INFO:root:current train perplexity1.0011177062988281
INFO:root:current mean train loss 1.4194803533038578
INFO:root:current train perplexity1.001120686531067
INFO:root:current mean train loss 1.4199008229883705
INFO:root:current train perplexity1.0011202096939087
INFO:root:current mean train loss 1.4204836809370254
INFO:root:current train perplexity1.001120686531067
INFO:root:current mean train loss 1.420858128800684
INFO:root:current train perplexity1.0011212825775146
INFO:root:current mean train loss 1.4210051469982794
INFO:root:current train perplexity1.001120686531067
INFO:root:current mean train loss 1.42131405663072
INFO:root:current train perplexity1.0011210441589355
INFO:root:current mean train loss 1.4218930730663362
INFO:root:current train perplexity1.0011215209960938
INFO:root:current mean train loss 1.4220027101223285
INFO:root:current train perplexity1.0011215209960938
INFO:root:current mean train loss 1.422212890887606
INFO:root:current train perplexity1.0011217594146729
INFO:root:current mean train loss 1.4224565499449429
INFO:root:current train perplexity1.0011225938796997
INFO:root:current mean train loss 1.422566950352161
INFO:root:current train perplexity1.0011227130889893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:28<00:00, 688.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:28<00:00, 688.82s/it]
INFO:root:final mean train loss: 1.4230014561284268
INFO:root:final train perplexity: 1.0011236667633057
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.57s/it]
INFO:root:eval mean loss: 1.9267533387698181
INFO:root:eval perplexity: 1.0015603303909302
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it]
INFO:root:eval mean loss: 2.4675757200159927
INFO:root:eval perplexity: 1.0020307302474976
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [14:25:48<24:07:43, 717.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4198664142971946
INFO:root:current train perplexity1.0011366605758667
INFO:root:current mean train loss 1.4150087640319071
INFO:root:current train perplexity1.001127004623413
INFO:root:current mean train loss 1.4149025496372507
INFO:root:current train perplexity1.0011245012283325
INFO:root:current mean train loss 1.4141071302848949
INFO:root:current train perplexity1.001122236251831
INFO:root:current mean train loss 1.4139344568166259
INFO:root:current train perplexity1.0011215209960938
INFO:root:current mean train loss 1.4134930906700471
INFO:root:current train perplexity1.0011208057403564
INFO:root:current mean train loss 1.4143929873300118
INFO:root:current train perplexity1.0011210441589355
INFO:root:current mean train loss 1.4152575451730076
INFO:root:current train perplexity1.0011208057403564
INFO:root:current mean train loss 1.4146824595197647
INFO:root:current train perplexity1.001118779182434
INFO:root:current mean train loss 1.4145616606795357
INFO:root:current train perplexity1.0011181831359863
INFO:root:current mean train loss 1.4154418081102353
INFO:root:current train perplexity1.0011190176010132
INFO:root:current mean train loss 1.4151944870915387
INFO:root:current train perplexity1.0011183023452759
INFO:root:current mean train loss 1.4151883873002733
INFO:root:current train perplexity1.0011173486709595
INFO:root:current mean train loss 1.4151282050215186
INFO:root:current train perplexity1.0011166334152222
INFO:root:current mean train loss 1.4153705241776042
INFO:root:current train perplexity1.0011168718338013
INFO:root:current mean train loss 1.4159692167926545
INFO:root:current train perplexity1.0011173486709595
INFO:root:current mean train loss 1.4163596230070135
INFO:root:current train perplexity1.0011178255081177
INFO:root:current mean train loss 1.4169799296097696
INFO:root:current train perplexity1.0011186599731445
INFO:root:current mean train loss 1.417435255035127
INFO:root:current train perplexity1.0011188983917236
INFO:root:current mean train loss 1.4181884195114631
INFO:root:current train perplexity1.0011193752288818

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:03<00:00, 663.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:03<00:00, 663.59s/it]
INFO:root:final mean train loss: 1.4183755318924927
INFO:root:final train perplexity: 1.0011199712753296
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.49s/it]
INFO:root:eval mean loss: 1.9280026056242328
INFO:root:eval perplexity: 1.0015614032745361
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it]
INFO:root:eval mean loss: 2.46902596527803
INFO:root:eval perplexity: 1.002031922340393
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [14:38:10<24:10:04, 725.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4105756404036183
INFO:root:current train perplexity1.0011159181594849
INFO:root:current mean train loss 1.4059980815311648
INFO:root:current train perplexity1.001111626625061
INFO:root:current mean train loss 1.406906652174401
INFO:root:current train perplexity1.0011096000671387
INFO:root:current mean train loss 1.4075710165799493
INFO:root:current train perplexity1.001111388206482
INFO:root:current mean train loss 1.4080649128666631
INFO:root:current train perplexity1.001111626625061
INFO:root:current mean train loss 1.4078365329766316
INFO:root:current train perplexity1.0011111497879028
INFO:root:current mean train loss 1.4080199048443158
INFO:root:current train perplexity1.0011115074157715
INFO:root:current mean train loss 1.4093348474213572
INFO:root:current train perplexity1.0011136531829834
INFO:root:current mean train loss 1.4096006257708886
INFO:root:current train perplexity1.0011141300201416
INFO:root:current mean train loss 1.4106557921150056
INFO:root:current train perplexity1.0011157989501953
INFO:root:current mean train loss 1.4107572236524875
INFO:root:current train perplexity1.001115322113037
INFO:root:current mean train loss 1.4108074756405085
INFO:root:current train perplexity1.001115083694458
INFO:root:current mean train loss 1.4101075117695985
INFO:root:current train perplexity1.0011143684387207
INFO:root:current mean train loss 1.4106944453812769
INFO:root:current train perplexity1.0011138916015625
INFO:root:current mean train loss 1.41117717714813
INFO:root:current train perplexity1.0011149644851685
INFO:root:current mean train loss 1.4122093326392429
INFO:root:current train perplexity1.0011156797409058
INFO:root:current mean train loss 1.4121207989720856
INFO:root:current train perplexity1.001114845275879
INFO:root:current mean train loss 1.4126150847972287
INFO:root:current train perplexity1.001115322113037
INFO:root:current mean train loss 1.4130601460347578
INFO:root:current train perplexity1.0011156797409058
INFO:root:current mean train loss 1.4131544143828645
INFO:root:current train perplexity1.0011156797409058

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:37<00:00, 697.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:37<00:00, 697.48s/it]
INFO:root:final mean train loss: 1.4132932406510599
INFO:root:final train perplexity: 1.0011160373687744
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 1.9327441115751334
INFO:root:eval perplexity: 1.0015652179718018
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.01s/it]
INFO:root:eval mean loss: 2.473287140646725
INFO:root:eval perplexity: 1.00203537940979
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [14:51:06<24:28:19, 740.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.4106229371146153
INFO:root:current train perplexity1.0011245012283325
INFO:root:current mean train loss 1.4002097276124088
INFO:root:current train perplexity1.001112699508667
INFO:root:current mean train loss 1.398867684861888
INFO:root:current train perplexity1.0011078119277954
INFO:root:current mean train loss 1.3984427401360044
INFO:root:current train perplexity1.0011050701141357
INFO:root:current mean train loss 1.4006147059071965
INFO:root:current train perplexity1.001105785369873
INFO:root:current mean train loss 1.4008635191453829
INFO:root:current train perplexity1.0011048316955566
INFO:root:current mean train loss 1.4011182261289223
INFO:root:current train perplexity1.0011043548583984
INFO:root:current mean train loss 1.4009784851799305
INFO:root:current train perplexity1.001104712486267
INFO:root:current mean train loss 1.4020118091476563
INFO:root:current train perplexity1.001105785369873
INFO:root:current mean train loss 1.4026793278143055
INFO:root:current train perplexity1.0011063814163208
INFO:root:current mean train loss 1.4034038121815509
INFO:root:current train perplexity1.0011075735092163
INFO:root:current mean train loss 1.4040263808097968
INFO:root:current train perplexity1.001108169555664
INFO:root:current mean train loss 1.4048626669521989
INFO:root:current train perplexity1.0011088848114014
INFO:root:current mean train loss 1.4057532225757143
INFO:root:current train perplexity1.0011099576950073
INFO:root:current mean train loss 1.406609519871916
INFO:root:current train perplexity1.001110553741455
INFO:root:current mean train loss 1.4067538376840842
INFO:root:current train perplexity1.001110315322876
INFO:root:current mean train loss 1.407545843955156
INFO:root:current train perplexity1.0011109113693237
INFO:root:current mean train loss 1.408083585938355
INFO:root:current train perplexity1.0011111497879028
INFO:root:current mean train loss 1.408355593617791
INFO:root:current train perplexity1.0011115074157715
INFO:root:current mean train loss 1.4082497939286445
INFO:root:current train perplexity1.001111626625061

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.18s/it]
INFO:root:final mean train loss: 1.4083120237981437
INFO:root:final train perplexity: 1.0011121034622192
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.95s/it]
INFO:root:eval mean loss: 1.9329000859395833
INFO:root:eval perplexity: 1.0015653371810913
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it]
INFO:root:eval mean loss: 2.4759751321576164
INFO:root:eval perplexity: 1.0020376443862915
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [15:04:01<24:36:53, 750.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3981437580559843
INFO:root:current train perplexity1.0010998249053955
INFO:root:current mean train loss 1.3962040083396003
INFO:root:current train perplexity1.001102328300476
INFO:root:current mean train loss 1.3978156152438792
INFO:root:current train perplexity1.0011029243469238
INFO:root:current mean train loss 1.397212263888682
INFO:root:current train perplexity1.0011024475097656
INFO:root:current mean train loss 1.3968393824405168
INFO:root:current train perplexity1.0011013746261597
INFO:root:current mean train loss 1.3982530064365715
INFO:root:current train perplexity1.001103162765503
INFO:root:current mean train loss 1.3993359485917964
INFO:root:current train perplexity1.0011039972305298
INFO:root:current mean train loss 1.400017416041255
INFO:root:current train perplexity1.0011037588119507
INFO:root:current mean train loss 1.400392541559653
INFO:root:current train perplexity1.0011039972305298
INFO:root:current mean train loss 1.4007304675149292
INFO:root:current train perplexity1.0011051893234253
INFO:root:current mean train loss 1.4009228058070735
INFO:root:current train perplexity1.0011049509048462
INFO:root:current mean train loss 1.4019841910008113
INFO:root:current train perplexity1.0011060237884521
INFO:root:current mean train loss 1.4024141161181811
INFO:root:current train perplexity1.001106858253479
INFO:root:current mean train loss 1.4018697912369542
INFO:root:current train perplexity1.0011060237884521
INFO:root:current mean train loss 1.402320365177571
INFO:root:current train perplexity1.0011066198349
INFO:root:current mean train loss 1.4028296689155038
INFO:root:current train perplexity1.0011075735092163
INFO:root:current mean train loss 1.4033015198209056
INFO:root:current train perplexity1.0011080503463745
INFO:root:current mean train loss 1.4037275296379852
INFO:root:current train perplexity1.0011082887649536
INFO:root:current mean train loss 1.4037462253137298
INFO:root:current train perplexity1.0011080503463745

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.34s/it]
INFO:root:final mean train loss: 1.403857735570368
INFO:root:final train perplexity: 1.0011085271835327
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.75s/it]
INFO:root:eval mean loss: 1.9350965906542243
INFO:root:eval perplexity: 1.0015671253204346
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 2.478561980081788
INFO:root:eval perplexity: 1.0020397901535034
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [15:16:57<24:38:49, 758.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3826758027076722
INFO:root:current train perplexity1.0010831356048584
INFO:root:current mean train loss 1.390781820904125
INFO:root:current train perplexity1.0010908842086792
INFO:root:current mean train loss 1.3918199664070492
INFO:root:current train perplexity1.0010995864868164
INFO:root:current mean train loss 1.3948538549484746
INFO:root:current train perplexity1.0011036396026611
INFO:root:current mean train loss 1.3947494896446786
INFO:root:current train perplexity1.001103162765503
INFO:root:current mean train loss 1.3953575669550429
INFO:root:current train perplexity1.001104712486267
INFO:root:current mean train loss 1.3951752910848523
INFO:root:current train perplexity1.0011045932769775
INFO:root:current mean train loss 1.396388527373193
INFO:root:current train perplexity1.001105546951294
INFO:root:current mean train loss 1.3973140932895518
INFO:root:current train perplexity1.0011063814163208
INFO:root:current mean train loss 1.3967940124836598
INFO:root:current train perplexity1.0011053085327148
INFO:root:current mean train loss 1.3965335546153608
INFO:root:current train perplexity1.0011041164398193
INFO:root:current mean train loss 1.3964540359136222
INFO:root:current train perplexity1.0011030435562134
INFO:root:current mean train loss 1.395829368524315
INFO:root:current train perplexity1.0011016130447388
INFO:root:current mean train loss 1.3956504453229541
INFO:root:current train perplexity1.0011013746261597
INFO:root:current mean train loss 1.3966715499864402
INFO:root:current train perplexity1.0011024475097656
INFO:root:current mean train loss 1.39720892424615
INFO:root:current train perplexity1.0011025667190552
INFO:root:current mean train loss 1.397526987146887
INFO:root:current train perplexity1.0011028051376343
INFO:root:current mean train loss 1.3981278647456252
INFO:root:current train perplexity1.0011029243469238
INFO:root:current mean train loss 1.3984666742672578
INFO:root:current train perplexity1.0011032819747925
INFO:root:current mean train loss 1.3987993323366055
INFO:root:current train perplexity1.0011041164398193

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:39<00:00, 699.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:39<00:00, 699.26s/it]
INFO:root:final mean train loss: 1.3988501556702353
INFO:root:final train perplexity: 1.0011045932769775
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.12s/it]
INFO:root:eval mean loss: 1.9380948467457548
INFO:root:eval perplexity: 1.0015695095062256
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.47s/it]
INFO:root:eval mean loss: 2.482230604540372
INFO:root:eval perplexity: 1.0020427703857422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [15:29:55<24:37:29, 764.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3737132725892243
INFO:root:current train perplexity1.001086950302124
INFO:root:current mean train loss 1.386684404583428
INFO:root:current train perplexity1.0010968446731567
INFO:root:current mean train loss 1.3834094644117985
INFO:root:current train perplexity1.0010913610458374
INFO:root:current mean train loss 1.3861903867225764
INFO:root:current train perplexity1.0010957717895508
INFO:root:current mean train loss 1.3858611765175848
INFO:root:current train perplexity1.001094102859497
INFO:root:current mean train loss 1.3857038310628236
INFO:root:current train perplexity1.0010936260223389
INFO:root:current mean train loss 1.3867636209849916
INFO:root:current train perplexity1.0010939836502075
INFO:root:current mean train loss 1.3880734742097711
INFO:root:current train perplexity1.0010960102081299
INFO:root:current mean train loss 1.3883586950428972
INFO:root:current train perplexity1.001096248626709
INFO:root:current mean train loss 1.3889722719933222
INFO:root:current train perplexity1.0010963678359985
INFO:root:current mean train loss 1.3897471185320192
INFO:root:current train perplexity1.0010972023010254
INFO:root:current mean train loss 1.3908510209187641
INFO:root:current train perplexity1.0010980367660522
INFO:root:current mean train loss 1.3913929525583666
INFO:root:current train perplexity1.001098394393921
INFO:root:current mean train loss 1.3922178081398902
INFO:root:current train perplexity1.001099705696106
INFO:root:current mean train loss 1.3924170736675088
INFO:root:current train perplexity1.001099705696106
INFO:root:current mean train loss 1.392741216501537
INFO:root:current train perplexity1.0011004209518433
INFO:root:current mean train loss 1.3927730642071368
INFO:root:current train perplexity1.0011003017425537
INFO:root:current mean train loss 1.3931090900093128
INFO:root:current train perplexity1.0011004209518433
INFO:root:current mean train loss 1.3935364552645586
INFO:root:current train perplexity1.0011004209518433
INFO:root:current mean train loss 1.3938451499968658
INFO:root:current train perplexity1.0011003017425537

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:38<00:00, 698.32s/it]
INFO:root:final mean train loss: 1.394148508106526
INFO:root:final train perplexity: 1.0011008977890015
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 1.9370855955367392
INFO:root:eval perplexity: 1.0015687942504883
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.50s/it]
INFO:root:eval mean loss: 2.4825634444859013
INFO:root:eval perplexity: 1.0020431280136108
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [15:42:52<24:32:08, 768.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3856842463666743
INFO:root:current train perplexity1.0010862350463867
INFO:root:current mean train loss 1.3764548715617921
INFO:root:current train perplexity1.0010803937911987
INFO:root:current mean train loss 1.3771840118971028
INFO:root:current train perplexity1.0010812282562256
INFO:root:current mean train loss 1.379210436759993
INFO:root:current train perplexity1.0010854005813599
INFO:root:current mean train loss 1.3797907864188288
INFO:root:current train perplexity1.0010874271392822
INFO:root:current mean train loss 1.3817194354008226
INFO:root:current train perplexity1.0010885000228882
INFO:root:current mean train loss 1.382667531137881
INFO:root:current train perplexity1.0010889768600464
INFO:root:current mean train loss 1.3834888321417633
INFO:root:current train perplexity1.0010900497436523
INFO:root:current mean train loss 1.3842934444899808
INFO:root:current train perplexity1.0010919570922852
INFO:root:current mean train loss 1.384451621796115
INFO:root:current train perplexity1.0010921955108643
INFO:root:current mean train loss 1.3845773753763615
INFO:root:current train perplexity1.0010926723480225
INFO:root:current mean train loss 1.3854143765422848
INFO:root:current train perplexity1.001093864440918
INFO:root:current mean train loss 1.3864299955858679
INFO:root:current train perplexity1.0010943412780762
INFO:root:current mean train loss 1.3875246743361156
INFO:root:current train perplexity1.0010960102081299
INFO:root:current mean train loss 1.3883551906515688
INFO:root:current train perplexity1.0010966062545776
INFO:root:current mean train loss 1.3884165090137195
INFO:root:current train perplexity1.001096487045288
INFO:root:current mean train loss 1.3882247115283697
INFO:root:current train perplexity1.001096248626709
INFO:root:current mean train loss 1.3884100503194223
INFO:root:current train perplexity1.0010963678359985
INFO:root:current mean train loss 1.3889294853696597
INFO:root:current train perplexity1.0010968446731567
INFO:root:current mean train loss 1.3892873566091797
INFO:root:current train perplexity1.0010967254638672

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:37<00:00, 697.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:37<00:00, 697.72s/it]
INFO:root:final mean train loss: 1.3894533104687343
INFO:root:final train perplexity: 1.0010972023010254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.39s/it]
INFO:root:eval mean loss: 1.9395571444051485
INFO:root:eval perplexity: 1.001570701599121
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 2.4865225088511798
INFO:root:eval perplexity: 1.0020463466644287
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [15:55:47<24:23:01, 770.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.378312228155918
INFO:root:current train perplexity1.001090407371521
INFO:root:current mean train loss 1.3772777207889912
INFO:root:current train perplexity1.0010899305343628
INFO:root:current mean train loss 1.3753243643662025
INFO:root:current train perplexity1.0010857582092285
INFO:root:current mean train loss 1.3741798532966762
INFO:root:current train perplexity1.001084327697754
INFO:root:current mean train loss 1.3766850804558546
INFO:root:current train perplexity1.0010870695114136
INFO:root:current mean train loss 1.3797651731181697
INFO:root:current train perplexity1.0010918378829956
INFO:root:current mean train loss 1.3812018998992965
INFO:root:current train perplexity1.0010930299758911
INFO:root:current mean train loss 1.381759870537947
INFO:root:current train perplexity1.0010935068130493
INFO:root:current mean train loss 1.3818617634103134
INFO:root:current train perplexity1.0010924339294434
INFO:root:current mean train loss 1.382333359534733
INFO:root:current train perplexity1.0010924339294434
INFO:root:current mean train loss 1.3822420284952563
INFO:root:current train perplexity1.0010915994644165
INFO:root:current mean train loss 1.3825717755785079
INFO:root:current train perplexity1.001092791557312
INFO:root:current mean train loss 1.3825981078121419
INFO:root:current train perplexity1.001092553138733
INFO:root:current mean train loss 1.383072270355533
INFO:root:current train perplexity1.001092791557312
INFO:root:current mean train loss 1.383451440332688
INFO:root:current train perplexity1.0010926723480225
INFO:root:current mean train loss 1.3842554619341305
INFO:root:current train perplexity1.0010936260223389
INFO:root:current mean train loss 1.3848209468662416
INFO:root:current train perplexity1.0010937452316284
INFO:root:current mean train loss 1.3848516329115999
INFO:root:current train perplexity1.0010929107666016
INFO:root:current mean train loss 1.3855560461979024
INFO:root:current train perplexity1.0010937452316284
INFO:root:current mean train loss 1.3855897189402446
INFO:root:current train perplexity1.0010937452316284

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:11<00:00, 671.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:11<00:00, 671.09s/it]
INFO:root:final mean train loss: 1.385507394253937
INFO:root:final train perplexity: 1.0010939836502075
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.42s/it]
INFO:root:eval mean loss: 1.9469985395458573
INFO:root:eval perplexity: 1.0015767812728882
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.20s/it]
INFO:root:eval mean loss: 2.49484546641086
INFO:root:eval perplexity: 1.002053141593933
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [16:08:03<23:51:25, 760.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3569101691246033
INFO:root:current train perplexity1.001067876815796
INFO:root:current mean train loss 1.3613473759608323
INFO:root:current train perplexity1.00106942653656
INFO:root:current mean train loss 1.3689196641496617
INFO:root:current train perplexity1.0010771751403809
INFO:root:current mean train loss 1.369450603212629
INFO:root:current train perplexity1.00107741355896
INFO:root:current mean train loss 1.369811938397555
INFO:root:current train perplexity1.0010772943496704
INFO:root:current mean train loss 1.3708078211566568
INFO:root:current train perplexity1.0010794401168823
INFO:root:current mean train loss 1.3716568380682166
INFO:root:current train perplexity1.0010813474655151
INFO:root:current mean train loss 1.3733310397609033
INFO:root:current train perplexity1.0010826587677002
INFO:root:current mean train loss 1.3748472585493448
INFO:root:current train perplexity1.0010850429534912
INFO:root:current mean train loss 1.3752538466746091
INFO:root:current train perplexity1.0010863542556763
INFO:root:current mean train loss 1.3764537660221885
INFO:root:current train perplexity1.0010868310928345
INFO:root:current mean train loss 1.3764977703070196
INFO:root:current train perplexity1.0010871887207031
INFO:root:current mean train loss 1.376951148252532
INFO:root:current train perplexity1.0010876655578613
INFO:root:current mean train loss 1.377142110912478
INFO:root:current train perplexity1.0010868310928345
INFO:root:current mean train loss 1.377848424188823
INFO:root:current train perplexity1.0010879039764404
INFO:root:current mean train loss 1.3788724787939335
INFO:root:current train perplexity1.0010881423950195
INFO:root:current mean train loss 1.3790622633461163
INFO:root:current train perplexity1.001088261604309
INFO:root:current mean train loss 1.3798035954463603
INFO:root:current train perplexity1.0010889768600464
INFO:root:current mean train loss 1.3806469533136223
INFO:root:current train perplexity1.0010896921157837
INFO:root:current mean train loss 1.3811452256174734
INFO:root:current train perplexity1.001090407371521

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.38s/it]
INFO:root:final mean train loss: 1.3811181936970998
INFO:root:final train perplexity: 1.0010905265808105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.15s/it]
INFO:root:eval mean loss: 1.9466758501445147
INFO:root:eval perplexity: 1.001576542854309
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.61s/it]
INFO:root:eval mean loss: 2.494072342172582
INFO:root:eval perplexity: 1.0020525455474854
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [16:19:02<22:41:42, 729.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.370183391320078
INFO:root:current train perplexity1.0010881423950195
INFO:root:current mean train loss 1.3703379979500403
INFO:root:current train perplexity1.0010836124420166
INFO:root:current mean train loss 1.3704617540715105
INFO:root:current train perplexity1.0010823011398315
INFO:root:current mean train loss 1.3691140666792665
INFO:root:current train perplexity1.0010813474655151
INFO:root:current mean train loss 1.367799664025355
INFO:root:current train perplexity1.0010778903961182
INFO:root:current mean train loss 1.3682944343871428
INFO:root:current train perplexity1.0010788440704346
INFO:root:current mean train loss 1.3693524544187587
INFO:root:current train perplexity1.0010803937911987
INFO:root:current mean train loss 1.3698464303646447
INFO:root:current train perplexity1.00108003616333
INFO:root:current mean train loss 1.3713694099607414
INFO:root:current train perplexity1.0010818243026733
INFO:root:current mean train loss 1.371557965949552
INFO:root:current train perplexity1.001082181930542
INFO:root:current mean train loss 1.3719877992046479
INFO:root:current train perplexity1.0010823011398315
INFO:root:current mean train loss 1.3718159743432718
INFO:root:current train perplexity1.0010825395584106
INFO:root:current mean train loss 1.372748443979094
INFO:root:current train perplexity1.0010837316513062
INFO:root:current mean train loss 1.3728883893686383
INFO:root:current train perplexity1.001083493232727
INFO:root:current mean train loss 1.3735268053003777
INFO:root:current train perplexity1.0010839700698853
INFO:root:current mean train loss 1.3742699848820796
INFO:root:current train perplexity1.001084804534912
INFO:root:current mean train loss 1.37492111798233
INFO:root:current train perplexity1.0010854005813599
INFO:root:current mean train loss 1.3753616219443532
INFO:root:current train perplexity1.0010852813720703
INFO:root:current mean train loss 1.375415607178117
INFO:root:current train perplexity1.001085638999939

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.68s/it]
INFO:root:final mean train loss: 1.3757442978975813
INFO:root:final train perplexity: 1.0010863542556763
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.31s/it]
INFO:root:eval mean loss: 1.9496521632722084
INFO:root:eval perplexity: 1.0015789270401
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.55s/it]
INFO:root:eval mean loss: 2.4987599752473493
INFO:root:eval perplexity: 1.002056360244751
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [16:30:00<21:50:12, 708.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3644245465596516
INFO:root:current train perplexity1.0010721683502197
INFO:root:current mean train loss 1.3613253980875015
INFO:root:current train perplexity1.0010707378387451
INFO:root:current mean train loss 1.3644958869466242
INFO:root:current train perplexity1.00107741355896
INFO:root:current mean train loss 1.3644119799137115
INFO:root:current train perplexity1.0010751485824585
INFO:root:current mean train loss 1.3651462009230864
INFO:root:current train perplexity1.0010770559310913
INFO:root:current mean train loss 1.366183293517679
INFO:root:current train perplexity1.0010780096054077
INFO:root:current mean train loss 1.3668742777864917
INFO:root:current train perplexity1.0010786056518555
INFO:root:current mean train loss 1.3671127345119969
INFO:root:current train perplexity1.0010790824890137
INFO:root:current mean train loss 1.3664562779987974
INFO:root:current train perplexity1.0010780096054077
INFO:root:current mean train loss 1.3673459342435788
INFO:root:current train perplexity1.0010786056518555
INFO:root:current mean train loss 1.3684305690022796
INFO:root:current train perplexity1.0010789632797241
INFO:root:current mean train loss 1.3692770547789634
INFO:root:current train perplexity1.0010796785354614
INFO:root:current mean train loss 1.369846882304736
INFO:root:current train perplexity1.0010796785354614
INFO:root:current mean train loss 1.370563106111637
INFO:root:current train perplexity1.0010807514190674
INFO:root:current mean train loss 1.3708406518775411
INFO:root:current train perplexity1.0010813474655151
INFO:root:current mean train loss 1.371096895406486
INFO:root:current train perplexity1.0010817050933838
INFO:root:current mean train loss 1.3709722192352523
INFO:root:current train perplexity1.0010817050933838
INFO:root:current mean train loss 1.371105628523314
INFO:root:current train perplexity1.0010820627212524
INFO:root:current mean train loss 1.3715392361018832
INFO:root:current train perplexity1.0010823011398315
INFO:root:current mean train loss 1.371996196431096
INFO:root:current train perplexity1.0010828971862793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:53<00:00, 593.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:53<00:00, 593.70s/it]
INFO:root:final mean train loss: 1.3720317581237353
INFO:root:final train perplexity: 1.0010833740234375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.16s/it]
INFO:root:eval mean loss: 1.950388027421126
INFO:root:eval perplexity: 1.0015795230865479
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.17s/it]
INFO:root:eval mean loss: 2.4992832423101925
INFO:root:eval perplexity: 1.0020568370819092
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [16:40:59<21:11:20, 693.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.365851069318837
INFO:root:current train perplexity1.0010850429534912
INFO:root:current mean train loss 1.3610481125439784
INFO:root:current train perplexity1.001076102256775
INFO:root:current mean train loss 1.3589491771298206
INFO:root:current train perplexity1.001068115234375
INFO:root:current mean train loss 1.3580633108376732
INFO:root:current train perplexity1.0010708570480347
INFO:root:current mean train loss 1.3580392726929316
INFO:root:current train perplexity1.0010699033737183
INFO:root:current mean train loss 1.3585398084499885
INFO:root:current train perplexity1.0010689496994019
INFO:root:current mean train loss 1.3595590424651371
INFO:root:current train perplexity1.0010719299316406
INFO:root:current mean train loss 1.3602802151678683
INFO:root:current train perplexity1.001072883605957
INFO:root:current mean train loss 1.3608738493142858
INFO:root:current train perplexity1.0010731220245361
INFO:root:current mean train loss 1.3617704512613582
INFO:root:current train perplexity1.0010751485824585
INFO:root:current mean train loss 1.3624635900074817
INFO:root:current train perplexity1.001075267791748
INFO:root:current mean train loss 1.3629770805816719
INFO:root:current train perplexity1.0010764598846436
INFO:root:current mean train loss 1.3635676288333145
INFO:root:current train perplexity1.0010771751403809
INFO:root:current mean train loss 1.3643444680557653
INFO:root:current train perplexity1.0010772943496704
INFO:root:current mean train loss 1.3648076622197476
INFO:root:current train perplexity1.0010771751403809
INFO:root:current mean train loss 1.3653941543527957
INFO:root:current train perplexity1.0010772943496704
INFO:root:current mean train loss 1.3655660932528013
INFO:root:current train perplexity1.00107741355896
INFO:root:current mean train loss 1.3662953709783825
INFO:root:current train perplexity1.001078486442566
INFO:root:current mean train loss 1.3668099643103473
INFO:root:current train perplexity1.001078486442566
INFO:root:current mean train loss 1.3669593211836986
INFO:root:current train perplexity1.0010789632797241

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.66s/it]
INFO:root:final mean train loss: 1.367335577294854
INFO:root:final train perplexity: 1.0010796785354614
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.16s/it]
INFO:root:eval mean loss: 1.9546081863396556
INFO:root:eval perplexity: 1.0015829801559448
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.29s/it]
INFO:root:eval mean loss: 2.5060428355602506
INFO:root:eval perplexity: 1.002062439918518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [16:51:57<20:40:33, 682.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.351093686145285
INFO:root:current train perplexity1.0010557174682617
INFO:root:current mean train loss 1.3525314861780977
INFO:root:current train perplexity1.00106680393219
INFO:root:current mean train loss 1.351731092464633
INFO:root:current train perplexity1.001065969467163
INFO:root:current mean train loss 1.354702882684035
INFO:root:current train perplexity1.001070499420166
INFO:root:current mean train loss 1.3556152999133808
INFO:root:current train perplexity1.0010703802108765
INFO:root:current mean train loss 1.3561721166848264
INFO:root:current train perplexity1.0010709762573242
INFO:root:current mean train loss 1.3573275555398072
INFO:root:current train perplexity1.0010713338851929
INFO:root:current mean train loss 1.3582919860014009
INFO:root:current train perplexity1.001072645187378
INFO:root:current mean train loss 1.3589597786853782
INFO:root:current train perplexity1.0010731220245361
INFO:root:current mean train loss 1.35956706090911
INFO:root:current train perplexity1.0010732412338257
INFO:root:current mean train loss 1.3597491133965225
INFO:root:current train perplexity1.0010738372802734
INFO:root:current mean train loss 1.3599989967612487
INFO:root:current train perplexity1.0010735988616943
INFO:root:current mean train loss 1.360647096105793
INFO:root:current train perplexity1.001073956489563
INFO:root:current mean train loss 1.3611565497088043
INFO:root:current train perplexity1.001074194908142
INFO:root:current mean train loss 1.3617358493771956
INFO:root:current train perplexity1.0010749101638794
INFO:root:current mean train loss 1.362295632374703
INFO:root:current train perplexity1.001075267791748
INFO:root:current mean train loss 1.3627312039981345
INFO:root:current train perplexity1.0010758638381958
INFO:root:current mean train loss 1.3628644098667482
INFO:root:current train perplexity1.0010758638381958
INFO:root:current mean train loss 1.3629653628185021
INFO:root:current train perplexity1.0010759830474854
INFO:root:current mean train loss 1.3630859829171467
INFO:root:current train perplexity1.001076102256775

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:48<00:00, 588.23s/it]
INFO:root:final mean train loss: 1.3631305788360457
INFO:root:final train perplexity: 1.001076340675354
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.01s/it]
INFO:root:eval mean loss: 1.9567480235235066
INFO:root:eval perplexity: 1.0015846490859985
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.21s/it]
INFO:root:eval mean loss: 2.5076583552022353
INFO:root:eval perplexity: 1.0020637512207031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [17:02:49<20:12:14, 673.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3565298943292528
INFO:root:current train perplexity1.0010817050933838
INFO:root:current mean train loss 1.356350886309805
INFO:root:current train perplexity1.0010759830474854
INFO:root:current mean train loss 1.3559364148419166
INFO:root:current train perplexity1.0010719299316406
INFO:root:current mean train loss 1.3547635275470324
INFO:root:current train perplexity1.0010721683502197
INFO:root:current mean train loss 1.355192601037077
INFO:root:current train perplexity1.0010712146759033
INFO:root:current mean train loss 1.3566916986214754
INFO:root:current train perplexity1.0010730028152466
INFO:root:current mean train loss 1.3563121672309542
INFO:root:current train perplexity1.0010727643966675
INFO:root:current mean train loss 1.3564631856128397
INFO:root:current train perplexity1.001072883605957
INFO:root:current mean train loss 1.3557372687175127
INFO:root:current train perplexity1.001071810722351
INFO:root:current mean train loss 1.3566948702773574
INFO:root:current train perplexity1.0010722875595093
INFO:root:current mean train loss 1.3573994501962598
INFO:root:current train perplexity1.0010725259780884
INFO:root:current mean train loss 1.3572205345075088
INFO:root:current train perplexity1.0010719299316406
INFO:root:current mean train loss 1.3573278866395626
INFO:root:current train perplexity1.0010716915130615
INFO:root:current mean train loss 1.357151260554397
INFO:root:current train perplexity1.0010712146759033
INFO:root:current mean train loss 1.3574741736553502
INFO:root:current train perplexity1.0010710954666138
INFO:root:current mean train loss 1.3578050428678496
INFO:root:current train perplexity1.001071572303772
INFO:root:current mean train loss 1.3579879356161773
INFO:root:current train perplexity1.0010721683502197
INFO:root:current mean train loss 1.3585956772822112
INFO:root:current train perplexity1.0010724067687988
INFO:root:current mean train loss 1.3591675943926464
INFO:root:current train perplexity1.001072645187378
INFO:root:current mean train loss 1.3597156201098808
INFO:root:current train perplexity1.0010733604431152

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:32<00:00, 572.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:32<00:00, 572.94s/it]
INFO:root:final mean train loss: 1.3597796207957957
INFO:root:final train perplexity: 1.0010737180709839
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.24s/it]
INFO:root:eval mean loss: 1.9570621826969985
INFO:root:eval perplexity: 1.0015848875045776
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.32s/it]
INFO:root:eval mean loss: 2.5097013441383416
INFO:root:eval perplexity: 1.0020654201507568
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [17:13:26<19:41:17, 662.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.351787121593952
INFO:root:current train perplexity1.0010703802108765
INFO:root:current mean train loss 1.345982665485806
INFO:root:current train perplexity1.001063346862793
INFO:root:current mean train loss 1.3450766056776047
INFO:root:current train perplexity1.0010616779327393
INFO:root:current mean train loss 1.3458038822600715
INFO:root:current train perplexity1.001060962677002
INFO:root:current mean train loss 1.3470734087129435
INFO:root:current train perplexity1.001060962677002
INFO:root:current mean train loss 1.347455753951237
INFO:root:current train perplexity1.0010600090026855
INFO:root:current mean train loss 1.3482853901736878
INFO:root:current train perplexity1.0010629892349243
INFO:root:current mean train loss 1.3483829299608867
INFO:root:current train perplexity1.001063346862793
INFO:root:current mean train loss 1.3492996249686589
INFO:root:current train perplexity1.001064658164978
INFO:root:current mean train loss 1.3498286997785374
INFO:root:current train perplexity1.001064419746399
INFO:root:current mean train loss 1.3505390329493416
INFO:root:current train perplexity1.0010648965835571
INFO:root:current mean train loss 1.3506360358100826
INFO:root:current train perplexity1.001064419746399
INFO:root:current mean train loss 1.351228677853942
INFO:root:current train perplexity1.0010650157928467
INFO:root:current mean train loss 1.3515091718970866
INFO:root:current train perplexity1.0010658502578735
INFO:root:current mean train loss 1.352113945903005
INFO:root:current train perplexity1.0010665655136108
INFO:root:current mean train loss 1.35222268119643
INFO:root:current train perplexity1.0010666847229004
INFO:root:current mean train loss 1.353261569780963
INFO:root:current train perplexity1.0010679960250854
INFO:root:current mean train loss 1.3538942245954877
INFO:root:current train perplexity1.0010689496994019
INFO:root:current mean train loss 1.3542176518034428
INFO:root:current train perplexity1.0010689496994019
INFO:root:current mean train loss 1.3550329682200846
INFO:root:current train perplexity1.0010696649551392

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.79s/it]
INFO:root:final mean train loss: 1.3550706493451747
INFO:root:final train perplexity: 1.0010700225830078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.08s/it]
INFO:root:eval mean loss: 1.9612056291695181
INFO:root:eval perplexity: 1.0015883445739746
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.20s/it]
INFO:root:eval mean loss: 2.5136627330847667
INFO:root:eval perplexity: 1.0020686388015747
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [17:24:03<19:16:53, 654.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3367917574558061
INFO:root:current train perplexity1.0010578632354736
INFO:root:current mean train loss 1.3379527651113907
INFO:root:current train perplexity1.0010589361190796
INFO:root:current mean train loss 1.3390457072241941
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.3412073478290356
INFO:root:current train perplexity1.0010608434677124
INFO:root:current mean train loss 1.345007689186265
INFO:root:current train perplexity1.0010645389556885
INFO:root:current mean train loss 1.3464161045786922
INFO:root:current train perplexity1.0010647773742676
INFO:root:current mean train loss 1.3470996062416942
INFO:root:current train perplexity1.001065731048584
INFO:root:current mean train loss 1.347540503314029
INFO:root:current train perplexity1.0010673999786377
INFO:root:current mean train loss 1.347299181498014
INFO:root:current train perplexity1.0010669231414795
INFO:root:current mean train loss 1.3473241913880603
INFO:root:current train perplexity1.0010660886764526
INFO:root:current mean train loss 1.3479046006580864
INFO:root:current train perplexity1.0010662078857422
INFO:root:current mean train loss 1.3482321994744846
INFO:root:current train perplexity1.0010654926300049
INFO:root:current mean train loss 1.3485492350233457
INFO:root:current train perplexity1.0010656118392944
INFO:root:current mean train loss 1.348894578777387
INFO:root:current train perplexity1.0010654926300049
INFO:root:current mean train loss 1.3493079983877514
INFO:root:current train perplexity1.0010660886764526
INFO:root:current mean train loss 1.3497521091268894
INFO:root:current train perplexity1.0010662078857422
INFO:root:current mean train loss 1.3499670315997068
INFO:root:current train perplexity1.001065969467163
INFO:root:current mean train loss 1.350357355312566
INFO:root:current train perplexity1.001065969467163
INFO:root:current mean train loss 1.3506673225680337
INFO:root:current train perplexity1.0010660886764526

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:34<00:00, 574.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:34<00:00, 574.13s/it]
INFO:root:final mean train loss: 1.3508413895416067
INFO:root:final train perplexity: 1.0010666847229004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.16s/it]
INFO:root:eval mean loss: 1.9643566266019294
INFO:root:eval perplexity: 1.0015908479690552
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.36s/it]
INFO:root:eval mean loss: 2.5205007001017847
INFO:root:eval perplexity: 1.0020743608474731
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [17:34:40<18:56:59, 649.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3619010278156825
INFO:root:current train perplexity1.0010788440704346
INFO:root:current mean train loss 1.3409375558819687
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.336888147291736
INFO:root:current train perplexity1.0010533332824707
INFO:root:current mean train loss 1.3385423604090503
INFO:root:current train perplexity1.0010545253753662
INFO:root:current mean train loss 1.3389472728190215
INFO:root:current train perplexity1.0010559558868408
INFO:root:current mean train loss 1.339523939770947
INFO:root:current train perplexity1.0010573863983154
INFO:root:current mean train loss 1.340224396910652
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.3402702801701736
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.3410439007991068
INFO:root:current train perplexity1.001057744026184
INFO:root:current mean train loss 1.3419216871261597
INFO:root:current train perplexity1.00105881690979
INFO:root:current mean train loss 1.343034212405865
INFO:root:current train perplexity1.001059889793396
INFO:root:current mean train loss 1.3437745969214174
INFO:root:current train perplexity1.0010606050491333
INFO:root:current mean train loss 1.3442598998644797
INFO:root:current train perplexity1.0010615587234497
INFO:root:current mean train loss 1.3452816700826498
INFO:root:current train perplexity1.0010629892349243
INFO:root:current mean train loss 1.3459390658131751
INFO:root:current train perplexity1.0010638236999512
INFO:root:current mean train loss 1.3455633376989529
INFO:root:current train perplexity1.001063346862793
INFO:root:current mean train loss 1.3459181987958622
INFO:root:current train perplexity1.0010632276535034
INFO:root:current mean train loss 1.3461302600914011
INFO:root:current train perplexity1.0010631084442139
INFO:root:current mean train loss 1.346376430107714
INFO:root:current train perplexity1.0010631084442139
INFO:root:current mean train loss 1.3467481389190212
INFO:root:current train perplexity1.0010632276535034

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:34<00:00, 574.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:34<00:00, 574.34s/it]
INFO:root:final mean train loss: 1.3470610653097679
INFO:root:final train perplexity: 1.0010637044906616
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.31s/it]
INFO:root:eval mean loss: 1.967806637709868
INFO:root:eval perplexity: 1.0015935897827148
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.18s/it]
INFO:root:eval mean loss: 2.5247808394702616
INFO:root:eval perplexity: 1.0020778179168701
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [17:45:18<18:40:01, 646.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3286068362574424
INFO:root:current train perplexity1.0010393857955933
INFO:root:current mean train loss 1.3294407465985714
INFO:root:current train perplexity1.0010515451431274
INFO:root:current mean train loss 1.3286245854902061
INFO:root:current train perplexity1.0010477304458618
INFO:root:current mean train loss 1.3307778482350698
INFO:root:current train perplexity1.0010509490966797
INFO:root:current mean train loss 1.33330454488918
INFO:root:current train perplexity1.001054048538208
INFO:root:current mean train loss 1.3336593718627527
INFO:root:current train perplexity1.0010552406311035
INFO:root:current mean train loss 1.3349135339543483
INFO:root:current train perplexity1.0010559558868408
INFO:root:current mean train loss 1.3351830545756787
INFO:root:current train perplexity1.0010555982589722
INFO:root:current mean train loss 1.3366619101787158
INFO:root:current train perplexity1.001056432723999
INFO:root:current mean train loss 1.338878062306618
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.3393328233787323
INFO:root:current train perplexity1.0010586977005005
INFO:root:current mean train loss 1.3392803754351184
INFO:root:current train perplexity1.0010583400726318
INFO:root:current mean train loss 1.3394508329859407
INFO:root:current train perplexity1.0010584592819214
INFO:root:current mean train loss 1.340271363573626
INFO:root:current train perplexity1.001059651374817
INFO:root:current mean train loss 1.340842694023287
INFO:root:current train perplexity1.001059889793396
INFO:root:current mean train loss 1.3414190476583858
INFO:root:current train perplexity1.001060128211975
INFO:root:current mean train loss 1.3412917506687485
INFO:root:current train perplexity1.0010602474212646
INFO:root:current mean train loss 1.3418281968792762
INFO:root:current train perplexity1.0010603666305542
INFO:root:current mean train loss 1.3420410170573345
INFO:root:current train perplexity1.0010600090026855
INFO:root:current mean train loss 1.3423226744569317
INFO:root:current train perplexity1.0010597705841064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.77s/it]
INFO:root:final mean train loss: 1.3424578843186494
INFO:root:final train perplexity: 1.0010600090026855
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.17s/it]
INFO:root:eval mean loss: 1.9688109696334135
INFO:root:eval perplexity: 1.0015944242477417
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.25s/it]
INFO:root:eval mean loss: 2.5259446975187205
INFO:root:eval perplexity: 1.0020787715911865
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [17:55:56<18:24:40, 643.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3267456019918125
INFO:root:current train perplexity1.0010590553283691
INFO:root:current mean train loss 1.328081176893131
INFO:root:current train perplexity1.0010515451431274
INFO:root:current mean train loss 1.3301220930391742
INFO:root:current train perplexity1.0010530948638916
INFO:root:current mean train loss 1.3320091808664387
INFO:root:current train perplexity1.0010545253753662
INFO:root:current mean train loss 1.3327816450702292
INFO:root:current train perplexity1.001054286956787
INFO:root:current mean train loss 1.3326343791763278
INFO:root:current train perplexity1.0010534524917603
INFO:root:current mean train loss 1.3332926675125405
INFO:root:current train perplexity1.001054286956787
INFO:root:current mean train loss 1.333370653065768
INFO:root:current train perplexity1.001053810119629
INFO:root:current mean train loss 1.3347818386161103
INFO:root:current train perplexity1.0010548830032349
INFO:root:current mean train loss 1.3352281483919812
INFO:root:current train perplexity1.0010552406311035
INFO:root:current mean train loss 1.3351318132103855
INFO:root:current train perplexity1.0010548830032349
INFO:root:current mean train loss 1.335389149002082
INFO:root:current train perplexity1.0010546445846558
INFO:root:current mean train loss 1.3358719011720939
INFO:root:current train perplexity1.0010552406311035
INFO:root:current mean train loss 1.3361975986632113
INFO:root:current train perplexity1.0010555982589722
INFO:root:current mean train loss 1.33694265025426
INFO:root:current train perplexity1.0010563135147095
INFO:root:current mean train loss 1.3373705995791334
INFO:root:current train perplexity1.0010560750961304
INFO:root:current mean train loss 1.3376225246098434
INFO:root:current train perplexity1.0010558366775513
INFO:root:current mean train loss 1.337979120848108
INFO:root:current train perplexity1.001056432723999
INFO:root:current mean train loss 1.3380872327915
INFO:root:current train perplexity1.001056432723999
INFO:root:current mean train loss 1.3383507328968518
INFO:root:current train perplexity1.0010566711425781

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.49s/it]
INFO:root:final mean train loss: 1.3383107084970671
INFO:root:final train perplexity: 1.0010567903518677
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.03s/it]
INFO:root:eval mean loss: 1.971664200015102
INFO:root:eval perplexity: 1.0015968084335327
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.23s/it]
INFO:root:eval mean loss: 2.5265840004521904
INFO:root:eval perplexity: 1.0020793676376343
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [18:06:38<18:13:36, 643.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3300244936576255
INFO:root:current train perplexity1.001052975654602
INFO:root:current mean train loss 1.3269426223003502
INFO:root:current train perplexity1.0010477304458618
INFO:root:current mean train loss 1.3250410925667242
INFO:root:current train perplexity1.0010452270507812
INFO:root:current mean train loss 1.3279411227735756
INFO:root:current train perplexity1.0010470151901245
INFO:root:current mean train loss 1.3282972151233303
INFO:root:current train perplexity1.0010484457015991
INFO:root:current mean train loss 1.3285256238110297
INFO:root:current train perplexity1.0010498762130737
INFO:root:current mean train loss 1.3294320215856221
INFO:root:current train perplexity1.0010510683059692
INFO:root:current mean train loss 1.329439521459193
INFO:root:current train perplexity1.001051425933838
INFO:root:current mean train loss 1.3305585653106602
INFO:root:current train perplexity1.001052975654602
INFO:root:current mean train loss 1.3311115266127909
INFO:root:current train perplexity1.001052737236023
INFO:root:current mean train loss 1.330715831344676
INFO:root:current train perplexity1.0010517835617065
INFO:root:current mean train loss 1.3317985620621449
INFO:root:current train perplexity1.0010521411895752
INFO:root:current mean train loss 1.332521998929412
INFO:root:current train perplexity1.001052737236023
INFO:root:current mean train loss 1.332364728511908
INFO:root:current train perplexity1.001052737236023
INFO:root:current mean train loss 1.3330061803498772
INFO:root:current train perplexity1.001052737236023
INFO:root:current mean train loss 1.3332367535216358
INFO:root:current train perplexity1.001052975654602
INFO:root:current mean train loss 1.3338437646000951
INFO:root:current train perplexity1.0010534524917603
INFO:root:current mean train loss 1.333850384771655
INFO:root:current train perplexity1.0010533332824707
INFO:root:current mean train loss 1.3341023538451413
INFO:root:current train perplexity1.0010534524917603
INFO:root:current mean train loss 1.3345358685379416
INFO:root:current train perplexity1.0010533332824707

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.22s/it]
INFO:root:final mean train loss: 1.334509674191054
INFO:root:final train perplexity: 1.0010536909103394
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.97s/it]
INFO:root:eval mean loss: 1.9731364791274917
INFO:root:eval perplexity: 1.0015980005264282
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it]
INFO:root:eval mean loss: 2.5304295395282987
INFO:root:eval perplexity: 1.0020824670791626
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [18:17:36<18:10:10, 647.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3104144102189599
INFO:root:current train perplexity1.0010287761688232
INFO:root:current mean train loss 1.3150725633233458
INFO:root:current train perplexity1.001031756401062
INFO:root:current mean train loss 1.3216226523649608
INFO:root:current train perplexity1.0010403394699097
INFO:root:current mean train loss 1.3243525902014128
INFO:root:current train perplexity1.0010452270507812
INFO:root:current mean train loss 1.3241144591841956
INFO:root:current train perplexity1.001044750213623
INFO:root:current mean train loss 1.325298883660962
INFO:root:current train perplexity1.00104558467865
INFO:root:current mean train loss 1.3258936496074598
INFO:root:current train perplexity1.0010465383529663
INFO:root:current mean train loss 1.326369533758334
INFO:root:current train perplexity1.0010472536087036
INFO:root:current mean train loss 1.3283679035245155
INFO:root:current train perplexity1.001049518585205
INFO:root:current mean train loss 1.3279220472770894
INFO:root:current train perplexity1.0010486841201782
INFO:root:current mean train loss 1.326937953777983
INFO:root:current train perplexity1.001047134399414
INFO:root:current mean train loss 1.327087818063455
INFO:root:current train perplexity1.0010472536087036
INFO:root:current mean train loss 1.3276974810824937
INFO:root:current train perplexity1.0010473728179932
INFO:root:current mean train loss 1.3281222091914948
INFO:root:current train perplexity1.001047968864441
INFO:root:current mean train loss 1.3288179354468177
INFO:root:current train perplexity1.0010489225387573
INFO:root:current mean train loss 1.3289884588058318
INFO:root:current train perplexity1.0010486841201782
INFO:root:current mean train loss 1.3295973437294524
INFO:root:current train perplexity1.001049280166626
INFO:root:current mean train loss 1.330271137616985
INFO:root:current train perplexity1.0010498762130737
INFO:root:current mean train loss 1.3303980133738704
INFO:root:current train perplexity1.0010498762130737
INFO:root:current mean train loss 1.3310545653435586
INFO:root:current train perplexity1.0010507106781006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.22s/it]
INFO:root:final mean train loss: 1.3310024202080368
INFO:root:final train perplexity: 1.0010509490966797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.26s/it]
INFO:root:eval mean loss: 1.9754293856891334
INFO:root:eval perplexity: 1.0015997886657715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.25s/it]
INFO:root:eval mean loss: 2.535165110801129
INFO:root:eval perplexity: 1.0020864009857178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [18:28:34<18:04:28, 650.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3161802881895894
INFO:root:current train perplexity1.0010353326797485
INFO:root:current mean train loss 1.3156305803126427
INFO:root:current train perplexity1.0010344982147217
INFO:root:current mean train loss 1.3194015899230804
INFO:root:current train perplexity1.0010411739349365
INFO:root:current mean train loss 1.3201949205016135
INFO:root:current train perplexity1.0010405778884888
INFO:root:current mean train loss 1.3216489414891643
INFO:root:current train perplexity1.0010416507720947
INFO:root:current mean train loss 1.3228163337070675
INFO:root:current train perplexity1.0010440349578857
INFO:root:current mean train loss 1.3228540331850065
INFO:root:current train perplexity1.0010432004928589
INFO:root:current mean train loss 1.3236498107599823
INFO:root:current train perplexity1.0010440349578857
INFO:root:current mean train loss 1.324655226791263
INFO:root:current train perplexity1.001046061515808
INFO:root:current mean train loss 1.3248485586903356
INFO:root:current train perplexity1.001046061515808
INFO:root:current mean train loss 1.3250794623308122
INFO:root:current train perplexity1.0010462999343872
INFO:root:current mean train loss 1.325093391639576
INFO:root:current train perplexity1.0010465383529663
INFO:root:current mean train loss 1.3255249648207972
INFO:root:current train perplexity1.0010466575622559
INFO:root:current mean train loss 1.3258608346159242
INFO:root:current train perplexity1.0010472536087036
INFO:root:current mean train loss 1.3254612802743435
INFO:root:current train perplexity1.0010466575622559
INFO:root:current mean train loss 1.3257993980822822
INFO:root:current train perplexity1.0010464191436768
INFO:root:current mean train loss 1.3260598200219318
INFO:root:current train perplexity1.0010465383529663
INFO:root:current mean train loss 1.32622330463085
INFO:root:current train perplexity1.0010467767715454
INFO:root:current mean train loss 1.3265048672237667
INFO:root:current train perplexity1.0010470151901245

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.57s/it]
INFO:root:final mean train loss: 1.3273052500764708
INFO:root:final train perplexity: 1.0010480880737305
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.29s/it]
INFO:root:eval mean loss: 1.9776603697885013
INFO:root:eval perplexity: 1.0016015768051147
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.19s/it]
INFO:root:eval mean loss: 2.5377183856693564
INFO:root:eval perplexity: 1.0020885467529297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [18:39:31<17:56:48, 652.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3235421031713486
INFO:root:current train perplexity1.0010533332824707
INFO:root:current mean train loss 1.3097595366938362
INFO:root:current train perplexity1.0010381937026978
INFO:root:current mean train loss 1.3126335585558857
INFO:root:current train perplexity1.0010366439819336
INFO:root:current mean train loss 1.3140003273758707
INFO:root:current train perplexity1.0010372400283813
INFO:root:current mean train loss 1.3149237718719702
INFO:root:current train perplexity1.0010370016098022
INFO:root:current mean train loss 1.3158942929996076
INFO:root:current train perplexity1.00103759765625
INFO:root:current mean train loss 1.3159812354035192
INFO:root:current train perplexity1.0010381937026978
INFO:root:current mean train loss 1.3165762322908008
INFO:root:current train perplexity1.0010381937026978
INFO:root:current mean train loss 1.318489536789118
INFO:root:current train perplexity1.0010402202606201
INFO:root:current mean train loss 1.3197857789597658
INFO:root:current train perplexity1.0010411739349365
INFO:root:current mean train loss 1.3202931093653356
INFO:root:current train perplexity1.0010417699813843
INFO:root:current mean train loss 1.32067977378018
INFO:root:current train perplexity1.0010424852371216
INFO:root:current mean train loss 1.3208153441940482
INFO:root:current train perplexity1.0010426044464111
INFO:root:current mean train loss 1.3209729097715628
INFO:root:current train perplexity1.001042127609253
INFO:root:current mean train loss 1.3212617407579206
INFO:root:current train perplexity1.0010422468185425
INFO:root:current mean train loss 1.3223072743352926
INFO:root:current train perplexity1.0010430812835693
INFO:root:current mean train loss 1.3225739264104626
INFO:root:current train perplexity1.0010437965393066
INFO:root:current mean train loss 1.3231019738254013
INFO:root:current train perplexity1.0010443925857544
INFO:root:current mean train loss 1.3227524183001287
INFO:root:current train perplexity1.0010435581207275
INFO:root:current mean train loss 1.3229536619465136
INFO:root:current train perplexity1.0010441541671753

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.74s/it]
INFO:root:final mean train loss: 1.3231297475306962
INFO:root:final train perplexity: 1.001044750213623
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.11s/it]
INFO:root:eval mean loss: 1.9787439839214298
INFO:root:eval perplexity: 1.0016025304794312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.33s/it]
INFO:root:eval mean loss: 2.5395513595418726
INFO:root:eval perplexity: 1.0020899772644043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [18:50:29<17:48:40, 654.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2992082147887258
INFO:root:current train perplexity1.0010175704956055
INFO:root:current mean train loss 1.302374125423288
INFO:root:current train perplexity1.0010249614715576
INFO:root:current mean train loss 1.3034438595751325
INFO:root:current train perplexity1.0010225772857666
INFO:root:current mean train loss 1.3068668881694119
INFO:root:current train perplexity1.0010260343551636
INFO:root:current mean train loss 1.3080547233101403
INFO:root:current train perplexity1.0010297298431396
INFO:root:current mean train loss 1.3088447689786413
INFO:root:current train perplexity1.001029372215271
INFO:root:current mean train loss 1.3106811658654356
INFO:root:current train perplexity1.0010312795639038
INFO:root:current mean train loss 1.311449675898272
INFO:root:current train perplexity1.0010313987731934
INFO:root:current mean train loss 1.3121336014473997
INFO:root:current train perplexity1.0010322332382202
INFO:root:current mean train loss 1.3129782951402
INFO:root:current train perplexity1.0010335445404053
INFO:root:current mean train loss 1.3140558454443385
INFO:root:current train perplexity1.0010353326797485
INFO:root:current mean train loss 1.3149110432336106
INFO:root:current train perplexity1.0010361671447754
INFO:root:current mean train loss 1.3158847633069448
INFO:root:current train perplexity1.0010374784469604
INFO:root:current mean train loss 1.315941323039233
INFO:root:current train perplexity1.00103759765625
INFO:root:current mean train loss 1.3165073387288415
INFO:root:current train perplexity1.0010383129119873
INFO:root:current mean train loss 1.3168314661142955
INFO:root:current train perplexity1.0010385513305664
INFO:root:current mean train loss 1.3180420170221603
INFO:root:current train perplexity1.0010401010513306
INFO:root:current mean train loss 1.3187749176718773
INFO:root:current train perplexity1.0010405778884888
INFO:root:current mean train loss 1.3191095449465322
INFO:root:current train perplexity1.001041054725647
INFO:root:current mean train loss 1.3191909878228034
INFO:root:current train perplexity1.001041293144226

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:52<00:00, 592.40s/it]
INFO:root:final mean train loss: 1.3191919581672487
INFO:root:final train perplexity: 1.0010416507720947
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 32.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 32.00s/it]
INFO:root:eval mean loss: 1.9844034360655656
INFO:root:eval perplexity: 1.001607060432434
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.28s/it]
INFO:root:eval mean loss: 2.5461404209441327
INFO:root:eval perplexity: 1.0020954608917236
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [19:01:27<17:39:26, 655.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3031145763397216
INFO:root:current train perplexity1.0010277032852173
INFO:root:current mean train loss 1.3069220328330993
INFO:root:current train perplexity1.001034140586853
INFO:root:current mean train loss 1.3048572468757629
INFO:root:current train perplexity1.0010288953781128
INFO:root:current mean train loss 1.3081744050979613
INFO:root:current train perplexity1.001033067703247
INFO:root:current mean train loss 1.3097031897968716
INFO:root:current train perplexity1.0010334253311157
INFO:root:current mean train loss 1.3099788011204112
INFO:root:current train perplexity1.0010340213775635
INFO:root:current mean train loss 1.3110107407203087
INFO:root:current train perplexity1.001036286354065
INFO:root:current mean train loss 1.3108423908551534
INFO:root:current train perplexity1.0010361671447754
INFO:root:current mean train loss 1.3106402042332816
INFO:root:current train perplexity1.001035213470459
INFO:root:current mean train loss 1.3109526691938702
INFO:root:current train perplexity1.0010350942611694
INFO:root:current mean train loss 1.3118271622203646
INFO:root:current train perplexity1.0010350942611694
INFO:root:current mean train loss 1.3117930632052215
INFO:root:current train perplexity1.0010353326797485
INFO:root:current mean train loss 1.3122583864212036
INFO:root:current train perplexity1.0010358095169067
INFO:root:current mean train loss 1.3126244283605506
INFO:root:current train perplexity1.0010359287261963
INFO:root:current mean train loss 1.3128818698587088
INFO:root:current train perplexity1.001036286354065
INFO:root:current mean train loss 1.3133449443694083
INFO:root:current train perplexity1.001036524772644
INFO:root:current mean train loss 1.313759354389075
INFO:root:current train perplexity1.0010368824005127
INFO:root:current mean train loss 1.3142985526493618
INFO:root:current train perplexity1.0010371208190918
INFO:root:current mean train loss 1.314981506837381
INFO:root:current train perplexity1.0010380744934082
INFO:root:current mean train loss 1.3157094551966741
INFO:root:current train perplexity1.001038670539856

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:51<00:00, 591.74s/it]
INFO:root:final mean train loss: 1.3159215629431436
INFO:root:final train perplexity: 1.0010390281677246
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.50s/it]
INFO:root:eval mean loss: 1.9842559860107747
INFO:root:eval perplexity: 1.0016069412231445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.56s/it]
INFO:root:eval mean loss: 2.546829087937132
INFO:root:eval perplexity: 1.0020960569381714
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [19:12:23<17:28:48, 655.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3030373107141524
INFO:root:current train perplexity1.0010333061218262
INFO:root:current mean train loss 1.3031079733443118
INFO:root:current train perplexity1.0010308027267456
INFO:root:current mean train loss 1.302945239713576
INFO:root:current train perplexity1.0010294914245605
INFO:root:current mean train loss 1.303363950116108
INFO:root:current train perplexity1.0010284185409546
INFO:root:current mean train loss 1.3044723366772082
INFO:root:current train perplexity1.0010302066802979
INFO:root:current mean train loss 1.304903660922244
INFO:root:current train perplexity1.0010302066802979
INFO:root:current mean train loss 1.307034502858701
INFO:root:current train perplexity1.0010329484939575
INFO:root:current mean train loss 1.3073357907858478
INFO:root:current train perplexity1.0010327100753784
INFO:root:current mean train loss 1.3072468981473366
INFO:root:current train perplexity1.0010318756103516
INFO:root:current mean train loss 1.3079201200015789
INFO:root:current train perplexity1.0010323524475098
INFO:root:current mean train loss 1.3076567511303803
INFO:root:current train perplexity1.0010318756103516
INFO:root:current mean train loss 1.3085104206635998
INFO:root:current train perplexity1.0010324716567993
INFO:root:current mean train loss 1.309288523003353
INFO:root:current train perplexity1.001033902168274
INFO:root:current mean train loss 1.3103522986675058
INFO:root:current train perplexity1.0010350942611694
INFO:root:current mean train loss 1.3105720978004232
INFO:root:current train perplexity1.0010348558425903
INFO:root:current mean train loss 1.3108548006588436
INFO:root:current train perplexity1.0010350942611694
INFO:root:current mean train loss 1.3116072693769276
INFO:root:current train perplexity1.0010359287261963
INFO:root:current mean train loss 1.312182519215949
INFO:root:current train perplexity1.0010364055633545
INFO:root:current mean train loss 1.3121145910672727
INFO:root:current train perplexity1.0010360479354858
INFO:root:current mean train loss 1.3123837080350913
INFO:root:current train perplexity1.0010361671447754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.20s/it]
INFO:root:final mean train loss: 1.3124942423055344
INFO:root:final train perplexity: 1.0010364055633545
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.24s/it]
INFO:root:eval mean loss: 1.9862395639115191
INFO:root:eval perplexity: 1.0016086101531982
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.42s/it]
INFO:root:eval mean loss: 2.551190363599899
INFO:root:eval perplexity: 1.002099633216858
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [19:23:06<17:11:57, 651.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3033101956049602
INFO:root:current train perplexity1.0010251998901367
INFO:root:current mean train loss 1.3036525800176288
INFO:root:current train perplexity1.0010273456573486
INFO:root:current mean train loss 1.3022351991122878
INFO:root:current train perplexity1.0010284185409546
INFO:root:current mean train loss 1.3026921569059293
INFO:root:current train perplexity1.0010294914245605
INFO:root:current mean train loss 1.3014849303675091
INFO:root:current train perplexity1.0010281801223755
INFO:root:current mean train loss 1.3027474637309167
INFO:root:current train perplexity1.0010285377502441
INFO:root:current mean train loss 1.3035962011730462
INFO:root:current train perplexity1.0010287761688232
INFO:root:current mean train loss 1.3030277257975267
INFO:root:current train perplexity1.0010286569595337
INFO:root:current mean train loss 1.3028794355791617
INFO:root:current train perplexity1.0010286569595337
INFO:root:current mean train loss 1.30321208635966
INFO:root:current train perplexity1.0010286569595337
INFO:root:current mean train loss 1.3036210067377758
INFO:root:current train perplexity1.0010285377502441
INFO:root:current mean train loss 1.3043966361799755
INFO:root:current train perplexity1.0010290145874023
INFO:root:current mean train loss 1.3051857720840014
INFO:root:current train perplexity1.0010297298431396
INFO:root:current mean train loss 1.3060302860130464
INFO:root:current train perplexity1.0010303258895874
INFO:root:current mean train loss 1.30648765634655
INFO:root:current train perplexity1.0010303258895874
INFO:root:current mean train loss 1.307280681651048
INFO:root:current train perplexity1.001031517982483
INFO:root:current mean train loss 1.3075048098796338
INFO:root:current train perplexity1.0010318756103516
INFO:root:current mean train loss 1.3072488375442446
INFO:root:current train perplexity1.0010310411453247
INFO:root:current mean train loss 1.307997872874995
INFO:root:current train perplexity1.0010318756103516

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:47<00:00, 587.62s/it]
INFO:root:final mean train loss: 1.3086387943391902
INFO:root:final train perplexity: 1.0010333061218262
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.35s/it]
INFO:root:eval mean loss: 1.9890639579042475
INFO:root:eval perplexity: 1.0016108751296997
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it]
INFO:root:eval mean loss: 2.555448924818783
INFO:root:eval perplexity: 1.0021030902862549
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [19:34:03<17:03:44, 653.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.3402975797653198
INFO:root:current train perplexity1.000990390777588
INFO:root:current mean train loss 1.2852254303375092
INFO:root:current train perplexity1.0010128021240234
INFO:root:current mean train loss 1.2903971440756499
INFO:root:current train perplexity1.00101900100708
INFO:root:current mean train loss 1.2902448014959544
INFO:root:current train perplexity1.0010164976119995
INFO:root:current mean train loss 1.2939209777518104
INFO:root:current train perplexity1.001019835472107
INFO:root:current mean train loss 1.2961305420317812
INFO:root:current train perplexity1.001022458076477
INFO:root:current mean train loss 1.2970955238961142
INFO:root:current train perplexity1.0010230541229248
INFO:root:current mean train loss 1.2978957568357743
INFO:root:current train perplexity1.0010240077972412
INFO:root:current mean train loss 1.299043934145819
INFO:root:current train perplexity1.0010257959365845
INFO:root:current mean train loss 1.2994279091419045
INFO:root:current train perplexity1.001025676727295
INFO:root:current mean train loss 1.2998227534832416
INFO:root:current train perplexity1.0010257959365845
INFO:root:current mean train loss 1.3004343571390053
INFO:root:current train perplexity1.0010260343551636
INFO:root:current mean train loss 1.3013112767352153
INFO:root:current train perplexity1.0010273456573486
INFO:root:current mean train loss 1.3019068281436132
INFO:root:current train perplexity1.0010275840759277
INFO:root:current mean train loss 1.3018421027423823
INFO:root:current train perplexity1.0010279417037964
INFO:root:current mean train loss 1.30278999117674
INFO:root:current train perplexity1.0010288953781128
INFO:root:current mean train loss 1.3028858394491754
INFO:root:current train perplexity1.0010285377502441
INFO:root:current mean train loss 1.3033509829827858
INFO:root:current train perplexity1.001029133796692
INFO:root:current mean train loss 1.3038409888115543
INFO:root:current train perplexity1.0010297298431396
INFO:root:current mean train loss 1.3038155073118736
INFO:root:current train perplexity1.0010292530059814

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:31<00:00, 691.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [11:31<00:00, 691.78s/it]
INFO:root:final mean train loss: 1.3041241918617705
INFO:root:final train perplexity: 1.0010297298431396
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it]
INFO:root:eval mean loss: 1.9924657272954358
INFO:root:eval perplexity: 1.0016136169433594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.22s/it]
INFO:root:eval mean loss: 2.55798451021208
INFO:root:eval perplexity: 1.0021052360534668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [19:46:53<17:46:55, 688.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2917072110705905
INFO:root:current train perplexity1.0010145902633667
INFO:root:current mean train loss 1.2950560693013466
INFO:root:current train perplexity1.0010251998901367
INFO:root:current mean train loss 1.29271146244959
INFO:root:current train perplexity1.001021385192871
INFO:root:current mean train loss 1.2930547456321477
INFO:root:current train perplexity1.001020073890686
INFO:root:current mean train loss 1.292648737225236
INFO:root:current train perplexity1.0010188817977905
INFO:root:current mean train loss 1.293356844126948
INFO:root:current train perplexity1.0010194778442383
INFO:root:current mean train loss 1.2933319045116214
INFO:root:current train perplexity1.00101900100708
INFO:root:current mean train loss 1.2931927706537805
INFO:root:current train perplexity1.0010193586349487
INFO:root:current mean train loss 1.294765973295151
INFO:root:current train perplexity1.001021146774292
INFO:root:current mean train loss 1.2954600327414885
INFO:root:current train perplexity1.0010226964950562
INFO:root:current mean train loss 1.296701328108025
INFO:root:current train perplexity1.0010236501693726
INFO:root:current mean train loss 1.297210650601839
INFO:root:current train perplexity1.001023530960083
INFO:root:current mean train loss 1.2976405160571947
INFO:root:current train perplexity1.0010231733322144
INFO:root:current mean train loss 1.2980176249295698
INFO:root:current train perplexity1.0010236501693726
INFO:root:current mean train loss 1.2989076739137702
INFO:root:current train perplexity1.001024842262268
INFO:root:current mean train loss 1.2992332843923757
INFO:root:current train perplexity1.0010249614715576
INFO:root:current mean train loss 1.2996512513667602
INFO:root:current train perplexity1.0010255575180054
INFO:root:current mean train loss 1.2999622347885016
INFO:root:current train perplexity1.001025915145874
INFO:root:current mean train loss 1.3007372076099593
INFO:root:current train perplexity1.0010265111923218
INFO:root:current mean train loss 1.3012768689609047
INFO:root:current train perplexity1.0010268688201904

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:36<00:00, 636.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:36<00:00, 636.71s/it]
INFO:root:final mean train loss: 1.301396001545516
INFO:root:final train perplexity: 1.0010275840759277
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.05s/it]
INFO:root:eval mean loss: 1.9974849460818243
INFO:root:eval perplexity: 1.001617670059204
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.88s/it]
INFO:root:eval mean loss: 2.5635749448275735
INFO:root:eval perplexity: 1.0021097660064697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [19:58:33<17:40:40, 691.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2754276548113141
INFO:root:current train perplexity1.000998854637146
INFO:root:current mean train loss 1.2799106068081325
INFO:root:current train perplexity1.0010110139846802
INFO:root:current mean train loss 1.2857982041987968
INFO:root:current train perplexity1.0010147094726562
INFO:root:current mean train loss 1.2884857152824971
INFO:root:current train perplexity1.0010136365890503
INFO:root:current mean train loss 1.2909382017179467
INFO:root:current train perplexity1.0010162591934204
INFO:root:current mean train loss 1.2927797689616123
INFO:root:current train perplexity1.0010186433792114
INFO:root:current mean train loss 1.2929435690556925
INFO:root:current train perplexity1.001017689704895
INFO:root:current mean train loss 1.293347616422744
INFO:root:current train perplexity1.001018762588501
INFO:root:current mean train loss 1.2939723029108106
INFO:root:current train perplexity1.0010193586349487
INFO:root:current mean train loss 1.2947975074543672
INFO:root:current train perplexity1.0010205507278442
INFO:root:current mean train loss 1.2946816831395247
INFO:root:current train perplexity1.0010212659835815
INFO:root:current mean train loss 1.294742942696626
INFO:root:current train perplexity1.001021146774292
INFO:root:current mean train loss 1.2958994808467295
INFO:root:current train perplexity1.0010221004486084
INFO:root:current mean train loss 1.296440619833014
INFO:root:current train perplexity1.0010228157043457
INFO:root:current mean train loss 1.2966713568890136
INFO:root:current train perplexity1.0010228157043457
INFO:root:current mean train loss 1.2970085643401752
INFO:root:current train perplexity1.0010234117507935
INFO:root:current mean train loss 1.297237885034777
INFO:root:current train perplexity1.001023769378662
INFO:root:current mean train loss 1.2974987739788353
INFO:root:current train perplexity1.0010244846343994
INFO:root:current mean train loss 1.2977732771423922
INFO:root:current train perplexity1.001024603843689
INFO:root:current mean train loss 1.2983534830793237
INFO:root:current train perplexity1.0010253190994263

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.88s/it]
INFO:root:final mean train loss: 1.2979288575508592
INFO:root:final train perplexity: 1.001024842262268
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 1.9988895236177648
INFO:root:eval perplexity: 1.0016188621520996
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 2.5644072070189403
INFO:root:eval perplexity: 1.002110481262207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [20:08:49<16:54:48, 669.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2969874418698824
INFO:root:current train perplexity1.0010273456573486
INFO:root:current mean train loss 1.2916333706755387
INFO:root:current train perplexity1.0010181665420532
INFO:root:current mean train loss 1.2911926834356218
INFO:root:current train perplexity1.001021146774292
INFO:root:current mean train loss 1.2922765734520825
INFO:root:current train perplexity1.0010230541229248
INFO:root:current mean train loss 1.2920398179408723
INFO:root:current train perplexity1.001023530960083
INFO:root:current mean train loss 1.2917526595402455
INFO:root:current train perplexity1.0010226964950562
INFO:root:current mean train loss 1.2908949605160696
INFO:root:current train perplexity1.0010207891464233
INFO:root:current mean train loss 1.2902966636292479
INFO:root:current train perplexity1.001020073890686
INFO:root:current mean train loss 1.2907379055526895
INFO:root:current train perplexity1.0010193586349487
INFO:root:current mean train loss 1.2918082897653098
INFO:root:current train perplexity1.0010204315185547
INFO:root:current mean train loss 1.2918960574461933
INFO:root:current train perplexity1.0010199546813965
INFO:root:current mean train loss 1.2924597281962633
INFO:root:current train perplexity1.0010210275650024
INFO:root:current mean train loss 1.2927779319187322
INFO:root:current train perplexity1.0010216236114502
INFO:root:current mean train loss 1.292668104965306
INFO:root:current train perplexity1.001021146774292
INFO:root:current mean train loss 1.2929367006810244
INFO:root:current train perplexity1.001021385192871
INFO:root:current mean train loss 1.2933576395063056
INFO:root:current train perplexity1.0010212659835815
INFO:root:current mean train loss 1.293998767330918
INFO:root:current train perplexity1.0010219812393188
INFO:root:current mean train loss 1.2942016543182608
INFO:root:current train perplexity1.0010217428207397
INFO:root:current mean train loss 1.2941932688521516
INFO:root:current train perplexity1.0010218620300293
INFO:root:current mean train loss 1.2941051156061594
INFO:root:current train perplexity1.0010215044021606

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.52s/it]
INFO:root:final mean train loss: 1.2941527781436013
INFO:root:final train perplexity: 1.0010218620300293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.0010812861699585
INFO:root:eval perplexity: 1.0016206502914429
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.5684509556344213
INFO:root:eval perplexity: 1.0021138191223145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [20:18:59<16:16:57, 651.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2814019797504812
INFO:root:current train perplexity1.0010089874267578
INFO:root:current mean train loss 1.2823972137722037
INFO:root:current train perplexity1.0010089874267578
INFO:root:current mean train loss 1.2819975268885107
INFO:root:current train perplexity1.001010537147522
INFO:root:current mean train loss 1.2843011246141056
INFO:root:current train perplexity1.0010126829147339
INFO:root:current mean train loss 1.2836545001723365
INFO:root:current train perplexity1.0010125637054443
INFO:root:current mean train loss 1.2838147849315917
INFO:root:current train perplexity1.001011848449707
INFO:root:current mean train loss 1.284184920591802
INFO:root:current train perplexity1.0010125637054443
INFO:root:current mean train loss 1.283885558187884
INFO:root:current train perplexity1.0010117292404175
INFO:root:current mean train loss 1.2848535452393859
INFO:root:current train perplexity1.0010125637054443
INFO:root:current mean train loss 1.2864685578981052
INFO:root:current train perplexity1.0010147094726562
INFO:root:current mean train loss 1.2875601190401083
INFO:root:current train perplexity1.0010157823562622
INFO:root:current mean train loss 1.288138607304967
INFO:root:current train perplexity1.0010157823562622
INFO:root:current mean train loss 1.2890894362267027
INFO:root:current train perplexity1.0010168552398682
INFO:root:current mean train loss 1.2898976473672448
INFO:root:current train perplexity1.0010179281234741
INFO:root:current mean train loss 1.290093331453831
INFO:root:current train perplexity1.001018762588501
INFO:root:current mean train loss 1.2900786049583295
INFO:root:current train perplexity1.0010186433792114
INFO:root:current mean train loss 1.290099077413295
INFO:root:current train perplexity1.001018762588501
INFO:root:current mean train loss 1.2905793169786055
INFO:root:current train perplexity1.0010188817977905
INFO:root:current mean train loss 1.2908566686289396
INFO:root:current train perplexity1.0010185241699219
INFO:root:current mean train loss 1.290995068148104
INFO:root:current train perplexity1.0010191202163696

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.63s/it]
INFO:root:final mean train loss: 1.290941467864671
INFO:root:final train perplexity: 1.0010193586349487
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2.004187730609948
INFO:root:eval perplexity: 1.0016231536865234
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.5724058895246356
INFO:root:eval perplexity: 1.0021170377731323
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [20:29:07<15:46:47, 638.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2704353609750436
INFO:root:current train perplexity1.0009970664978027
INFO:root:current mean train loss 1.2770415659873717
INFO:root:current train perplexity1.0010089874267578
INFO:root:current mean train loss 1.2785524033999944
INFO:root:current train perplexity1.0010089874267578
INFO:root:current mean train loss 1.280200134904891
INFO:root:current train perplexity1.0010093450546265
INFO:root:current mean train loss 1.2817446258332994
INFO:root:current train perplexity1.0010126829147339
INFO:root:current mean train loss 1.2825504474265583
INFO:root:current train perplexity1.001012921333313
INFO:root:current mean train loss 1.2822567342669207
INFO:root:current train perplexity1.0010123252868652
INFO:root:current mean train loss 1.2831172835432543
INFO:root:current train perplexity1.0010132789611816
INFO:root:current mean train loss 1.282531408773857
INFO:root:current train perplexity1.0010120868682861
INFO:root:current mean train loss 1.283285496563747
INFO:root:current train perplexity1.0010132789611816
INFO:root:current mean train loss 1.2842455228608836
INFO:root:current train perplexity1.0010136365890503
INFO:root:current mean train loss 1.284866900307332
INFO:root:current train perplexity1.0010144710540771
INFO:root:current mean train loss 1.2852948112398803
INFO:root:current train perplexity1.0010148286819458
INFO:root:current mean train loss 1.2853973218028851
INFO:root:current train perplexity1.0010147094726562
INFO:root:current mean train loss 1.2855709143795204
INFO:root:current train perplexity1.0010147094726562
INFO:root:current mean train loss 1.2858720818654286
INFO:root:current train perplexity1.0010149478912354
INFO:root:current mean train loss 1.285841689839482
INFO:root:current train perplexity1.0010144710540771
INFO:root:current mean train loss 1.2860996316688726
INFO:root:current train perplexity1.001015067100525
INFO:root:current mean train loss 1.286493021951374
INFO:root:current train perplexity1.0010157823562622

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.34s/it]
INFO:root:final mean train loss: 1.2872404859213893
INFO:root:final train perplexity: 1.00101637840271
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.006611798671966
INFO:root:eval perplexity: 1.0016250610351562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2.574446937716599
INFO:root:eval perplexity: 1.0021188259124756
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [20:39:16<15:23:07, 629.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2973584334055583
INFO:root:current train perplexity1.0010255575180054
INFO:root:current mean train loss 1.272378124079658
INFO:root:current train perplexity1.0010006427764893
INFO:root:current mean train loss 1.2762331199176207
INFO:root:current train perplexity1.001009225845337
INFO:root:current mean train loss 1.2766157412292933
INFO:root:current train perplexity1.0010110139846802
INFO:root:current mean train loss 1.2753982694805703
INFO:root:current train perplexity1.0010077953338623
INFO:root:current mean train loss 1.2757795200670217
INFO:root:current train perplexity1.001007318496704
INFO:root:current mean train loss 1.2767780732554979
INFO:root:current train perplexity1.0010082721710205
INFO:root:current mean train loss 1.2776832963799685
INFO:root:current train perplexity1.0010079145431519
INFO:root:current mean train loss 1.2789196902758455
INFO:root:current train perplexity1.0010080337524414
INFO:root:current mean train loss 1.2801214372068808
INFO:root:current train perplexity1.0010091066360474
INFO:root:current mean train loss 1.2807396649125804
INFO:root:current train perplexity1.001010537147522
INFO:root:current mean train loss 1.2806990932359117
INFO:root:current train perplexity1.0010112524032593
INFO:root:current mean train loss 1.2811577436038086
INFO:root:current train perplexity1.001011610031128
INFO:root:current mean train loss 1.2808124008127477
INFO:root:current train perplexity1.0010110139846802
INFO:root:current mean train loss 1.2814907997060654
INFO:root:current train perplexity1.001011848449707
INFO:root:current mean train loss 1.281344584838121
INFO:root:current train perplexity1.0010122060775757
INFO:root:current mean train loss 1.281553329636139
INFO:root:current train perplexity1.0010119676589966
INFO:root:current mean train loss 1.282096811903272
INFO:root:current train perplexity1.0010123252868652
INFO:root:current mean train loss 1.2828280615661123
INFO:root:current train perplexity1.0010126829147339
INFO:root:current mean train loss 1.2831445805223378
INFO:root:current train perplexity1.001013159751892

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.46s/it]
INFO:root:final mean train loss: 1.283307621530734
INFO:root:final train perplexity: 1.0010132789611816
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.0052991061345904
INFO:root:eval perplexity: 1.0016239881515503
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.5750705071374878
INFO:root:eval perplexity: 1.0021193027496338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [20:49:23<15:03:11, 622.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2775268316268922
INFO:root:current train perplexity1.0009931325912476
INFO:root:current mean train loss 1.2718637237946193
INFO:root:current train perplexity1.0009980201721191
INFO:root:current mean train loss 1.2713594420389696
INFO:root:current train perplexity1.000999093055725
INFO:root:current mean train loss 1.2730213031172752
INFO:root:current train perplexity1.0010044574737549
INFO:root:current mean train loss 1.276506854522796
INFO:root:current train perplexity1.0010069608688354
INFO:root:current mean train loss 1.2756604185471168
INFO:root:current train perplexity1.0010054111480713
INFO:root:current mean train loss 1.2762709044641065
INFO:root:current train perplexity1.0010062456130981
INFO:root:current mean train loss 1.275889167851872
INFO:root:current train perplexity1.0010064840316772
INFO:root:current mean train loss 1.2761451959609986
INFO:root:current train perplexity1.0010069608688354
INFO:root:current mean train loss 1.2763803300650223
INFO:root:current train perplexity1.001006841659546
INFO:root:current mean train loss 1.277361329279694
INFO:root:current train perplexity1.0010080337524414
INFO:root:current mean train loss 1.2771452059703214
INFO:root:current train perplexity1.0010086297988892
INFO:root:current mean train loss 1.2772978645856263
INFO:root:current train perplexity1.0010087490081787
INFO:root:current mean train loss 1.2780891991022862
INFO:root:current train perplexity1.0010099411010742
INFO:root:current mean train loss 1.2789511445542456
INFO:root:current train perplexity1.0010100603103638
INFO:root:current mean train loss 1.2791892497947341
INFO:root:current train perplexity1.0010104179382324
INFO:root:current mean train loss 1.2792739968976856
INFO:root:current train perplexity1.0010101795196533
INFO:root:current mean train loss 1.2799566085255423
INFO:root:current train perplexity1.0010106563568115
INFO:root:current mean train loss 1.2801001279563695
INFO:root:current train perplexity1.0010104179382324
INFO:root:current mean train loss 1.2803251397485533
INFO:root:current train perplexity1.0010102987289429

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.57s/it]
INFO:root:final mean train loss: 1.280669511897481
INFO:root:final train perplexity: 1.0010112524032593
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2.009856621847085
INFO:root:eval perplexity: 1.0016276836395264
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2.581539564944328
INFO:root:eval perplexity: 1.0021246671676636
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [20:59:31<14:46:20, 618.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2654367878630355
INFO:root:current train perplexity1.0009944438934326
INFO:root:current mean train loss 1.2697253488276126
INFO:root:current train perplexity1.0010050535202026
INFO:root:current mean train loss 1.2712115280738863
INFO:root:current train perplexity1.0010074377059937
INFO:root:current mean train loss 1.2716616448733502
INFO:root:current train perplexity1.0010061264038086
INFO:root:current mean train loss 1.2716686261873222
INFO:root:current train perplexity1.0010052919387817
INFO:root:current mean train loss 1.2708977740570153
INFO:root:current train perplexity1.0010042190551758
INFO:root:current mean train loss 1.2713564106583408
INFO:root:current train perplexity1.0010043382644653
INFO:root:current mean train loss 1.2725569336514622
INFO:root:current train perplexity1.00100576877594
INFO:root:current mean train loss 1.2729022136868258
INFO:root:current train perplexity1.00100576877594
INFO:root:current mean train loss 1.2727239958886276
INFO:root:current train perplexity1.001006007194519
INFO:root:current mean train loss 1.2737989879735065
INFO:root:current train perplexity1.0010074377059937
INFO:root:current mean train loss 1.274081136955959
INFO:root:current train perplexity1.0010074377059937
INFO:root:current mean train loss 1.2748595514698384
INFO:root:current train perplexity1.0010077953338623
INFO:root:current mean train loss 1.275084199349026
INFO:root:current train perplexity1.0010076761245728
INFO:root:current mean train loss 1.2750709985973276
INFO:root:current train perplexity1.001007318496704
INFO:root:current mean train loss 1.2753819770136667
INFO:root:current train perplexity1.0010075569152832
INFO:root:current mean train loss 1.2759113470519106
INFO:root:current train perplexity1.0010079145431519
INFO:root:current mean train loss 1.2760874993786018
INFO:root:current train perplexity1.0010077953338623
INFO:root:current mean train loss 1.2763175796281403
INFO:root:current train perplexity1.0010076761245728
INFO:root:current mean train loss 1.2767679013723434
INFO:root:current train perplexity1.0010080337524414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.32s/it]
INFO:root:final mean train loss: 1.2768939660164182
INFO:root:final train perplexity: 1.0010082721710205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.010928637169777
INFO:root:eval perplexity: 1.0016286373138428
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.5811498211630695
INFO:root:eval perplexity: 1.002124309539795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [21:09:40<14:31:53, 615.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.261767934869837
INFO:root:current train perplexity1.0009989738464355
INFO:root:current mean train loss 1.2640886624138077
INFO:root:current train perplexity1.0009945631027222
INFO:root:current mean train loss 1.2648196253250903
INFO:root:current train perplexity1.0009958744049072
INFO:root:current mean train loss 1.2662808123954945
INFO:root:current train perplexity1.0010007619857788
INFO:root:current mean train loss 1.2670465634257784
INFO:root:current train perplexity1.0010008811950684
INFO:root:current mean train loss 1.2675149742016294
INFO:root:current train perplexity1.0010004043579102
INFO:root:current mean train loss 1.2688275081666602
INFO:root:current train perplexity1.0010019540786743
INFO:root:current mean train loss 1.269929995587397
INFO:root:current train perplexity1.0010035037994385
INFO:root:current mean train loss 1.2694865250196614
INFO:root:current train perplexity1.001002550125122
INFO:root:current mean train loss 1.2704596986810617
INFO:root:current train perplexity1.0010044574737549
INFO:root:current mean train loss 1.2707131032247019
INFO:root:current train perplexity1.0010043382644653
INFO:root:current mean train loss 1.27088860250635
INFO:root:current train perplexity1.0010042190551758
INFO:root:current mean train loss 1.2715044570121279
INFO:root:current train perplexity1.0010043382644653
INFO:root:current mean train loss 1.271642908211825
INFO:root:current train perplexity1.0010040998458862
INFO:root:current mean train loss 1.2718391957932358
INFO:root:current train perplexity1.0010042190551758
INFO:root:current mean train loss 1.272294205127996
INFO:root:current train perplexity1.0010045766830444
INFO:root:current mean train loss 1.2727857640749598
INFO:root:current train perplexity1.001004934310913
INFO:root:current mean train loss 1.2735458001443434
INFO:root:current train perplexity1.0010055303573608
INFO:root:current mean train loss 1.2732457155291446
INFO:root:current train perplexity1.0010048151016235
INFO:root:current mean train loss 1.2737299295994495
INFO:root:current train perplexity1.0010055303573608

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.67s/it]
INFO:root:final mean train loss: 1.2738161456146087
INFO:root:final train perplexity: 1.00100576877594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2.015012354292768
INFO:root:eval perplexity: 1.0016318559646606
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2.5868541526456252
INFO:root:eval perplexity: 1.0021289587020874
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [21:19:49<14:18:53, 613.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2587500793833128
INFO:root:current train perplexity1.0009862184524536
INFO:root:current mean train loss 1.263747801557619
INFO:root:current train perplexity1.0009987354278564
INFO:root:current mean train loss 1.2592693466102065
INFO:root:current train perplexity1.000994086265564
INFO:root:current mean train loss 1.2595551987542617
INFO:root:current train perplexity1.0009937286376953
INFO:root:current mean train loss 1.2627361521852498
INFO:root:current train perplexity1.0009969472885132
INFO:root:current mean train loss 1.263387257706263
INFO:root:current train perplexity1.0009976625442505
INFO:root:current mean train loss 1.2648613182869468
INFO:root:current train perplexity1.0009983777999878
INFO:root:current mean train loss 1.2655258869846506
INFO:root:current train perplexity1.0009984970092773
INFO:root:current mean train loss 1.2669402066656696
INFO:root:current train perplexity1.0010000467300415
INFO:root:current mean train loss 1.2675790063630172
INFO:root:current train perplexity1.0010002851486206
INFO:root:current mean train loss 1.2679530856322174
INFO:root:current train perplexity1.001001000404358
INFO:root:current mean train loss 1.2684682961919183
INFO:root:current train perplexity1.0010018348693848
INFO:root:current mean train loss 1.268562863512737
INFO:root:current train perplexity1.0010015964508057
INFO:root:current mean train loss 1.2693693918698379
INFO:root:current train perplexity1.001002550125122
INFO:root:current mean train loss 1.2695803037717341
INFO:root:current train perplexity1.001002550125122
INFO:root:current mean train loss 1.2699876088082298
INFO:root:current train perplexity1.0010027885437012
INFO:root:current mean train loss 1.2705434098491977
INFO:root:current train perplexity1.0010032653808594
INFO:root:current mean train loss 1.2710601909897006
INFO:root:current train perplexity1.001003384590149
INFO:root:current mean train loss 1.271768063928022
INFO:root:current train perplexity1.0010044574737549
INFO:root:current mean train loss 1.2714769249939786
INFO:root:current train perplexity1.001003623008728

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.43s/it]
INFO:root:final mean train loss: 1.271512993345102
INFO:root:final train perplexity: 1.0010039806365967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.0194543638127915
INFO:root:eval perplexity: 1.0016355514526367
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.5934071042013507
INFO:root:eval perplexity: 1.0021344423294067
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [21:29:57<14:06:40, 612.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2764451530846683
INFO:root:current train perplexity1.0010138750076294
INFO:root:current mean train loss 1.2699766140034858
INFO:root:current train perplexity1.001007318496704
INFO:root:current mean train loss 1.2632781631416745
INFO:root:current train perplexity1.000998854637146
INFO:root:current mean train loss 1.2620554351315056
INFO:root:current train perplexity1.0009949207305908
INFO:root:current mean train loss 1.2627078873700783
INFO:root:current train perplexity1.000998616218567
INFO:root:current mean train loss 1.263041473773061
INFO:root:current train perplexity1.00099778175354
INFO:root:current mean train loss 1.2635230671527773
INFO:root:current train perplexity1.0009983777999878
INFO:root:current mean train loss 1.2629790635883507
INFO:root:current train perplexity1.0009979009628296
INFO:root:current mean train loss 1.263458679925214
INFO:root:current train perplexity1.000997543334961
INFO:root:current mean train loss 1.264672690074936
INFO:root:current train perplexity1.0009981393814087
INFO:root:current mean train loss 1.2650708692038761
INFO:root:current train perplexity1.00099778175354
INFO:root:current mean train loss 1.2657123576310347
INFO:root:current train perplexity1.0009984970092773
INFO:root:current mean train loss 1.2660459326290936
INFO:root:current train perplexity1.000999093055725
INFO:root:current mean train loss 1.2663188244320818
INFO:root:current train perplexity1.0009998083114624
INFO:root:current mean train loss 1.2665380792271705
INFO:root:current train perplexity1.0009998083114624
INFO:root:current mean train loss 1.267105022140354
INFO:root:current train perplexity1.0010005235671997
INFO:root:current mean train loss 1.2671323436272652
INFO:root:current train perplexity1.0010005235671997
INFO:root:current mean train loss 1.2671969203607614
INFO:root:current train perplexity1.0010005235671997
INFO:root:current mean train loss 1.267316488391262
INFO:root:current train perplexity1.0010004043579102

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.19s/it]
INFO:root:final mean train loss: 1.267467149267038
INFO:root:final train perplexity: 1.0010007619857788
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2.02015154877453
INFO:root:eval perplexity: 1.001636028289795
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.5937004689629197
INFO:root:eval perplexity: 1.0021346807479858
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [21:40:06<13:55:00, 610.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2590061664581298
INFO:root:current train perplexity1.0010331869125366
INFO:root:current mean train loss 1.2601789667492822
INFO:root:current train perplexity1.0009984970092773
INFO:root:current mean train loss 1.25960835712712
INFO:root:current train perplexity1.0009968280792236
INFO:root:current mean train loss 1.2589834002197766
INFO:root:current train perplexity1.0009950399398804
INFO:root:current mean train loss 1.2604822102888131
INFO:root:current train perplexity1.0009956359863281
INFO:root:current mean train loss 1.2594104554393504
INFO:root:current train perplexity1.0009944438934326
INFO:root:current mean train loss 1.259804156397985
INFO:root:current train perplexity1.00099515914917
INFO:root:current mean train loss 1.260283414691898
INFO:root:current train perplexity1.0009958744049072
INFO:root:current mean train loss 1.2608038278840343
INFO:root:current train perplexity1.0009970664978027
INFO:root:current mean train loss 1.2601337046913021
INFO:root:current train perplexity1.0009958744049072
INFO:root:current mean train loss 1.2603224063987162
INFO:root:current train perplexity1.0009962320327759
INFO:root:current mean train loss 1.2603322812334985
INFO:root:current train perplexity1.0009962320327759
INFO:root:current mean train loss 1.2609337342725255
INFO:root:current train perplexity1.0009968280792236
INFO:root:current mean train loss 1.261433401783764
INFO:root:current train perplexity1.0009969472885132
INFO:root:current mean train loss 1.2618095092501929
INFO:root:current train perplexity1.000996708869934
INFO:root:current mean train loss 1.2625626938683645
INFO:root:current train perplexity1.0009970664978027
INFO:root:current mean train loss 1.2628899244504554
INFO:root:current train perplexity1.0009969472885132
INFO:root:current mean train loss 1.2634604246385637
INFO:root:current train perplexity1.0009974241256714
INFO:root:current mean train loss 1.2639629295988426
INFO:root:current train perplexity1.00099778175354
INFO:root:current mean train loss 1.264351522328034
INFO:root:current train perplexity1.0009981393814087

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.54s/it]
INFO:root:final mean train loss: 1.2645069108247395
INFO:root:final train perplexity: 1.0009984970092773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.01981808826433
INFO:root:eval perplexity: 1.0016357898712158
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.593556582927704
INFO:root:eval perplexity: 1.0021345615386963
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [21:50:15<13:43:58, 610.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.258441150188446
INFO:root:current train perplexity1.0009965896606445
INFO:root:current mean train loss 1.2563131705659334
INFO:root:current train perplexity1.0009914636611938
INFO:root:current mean train loss 1.2581175103917852
INFO:root:current train perplexity1.0009942054748535
INFO:root:current mean train loss 1.2586541994017844
INFO:root:current train perplexity1.0009949207305908
INFO:root:current mean train loss 1.2575150784723002
INFO:root:current train perplexity1.0009928941726685
INFO:root:current mean train loss 1.259330952989644
INFO:root:current train perplexity1.0009956359863281
INFO:root:current mean train loss 1.259156479904506
INFO:root:current train perplexity1.0009948015213013
INFO:root:current mean train loss 1.2593314158949496
INFO:root:current train perplexity1.0009945631027222
INFO:root:current mean train loss 1.2608264142289358
INFO:root:current train perplexity1.0009961128234863
INFO:root:current mean train loss 1.2610617842436354
INFO:root:current train perplexity1.0009955167770386
INFO:root:current mean train loss 1.26133204148007
INFO:root:current train perplexity1.0009958744049072
INFO:root:current mean train loss 1.2611222379772846
INFO:root:current train perplexity1.0009963512420654
INFO:root:current mean train loss 1.260698963304198
INFO:root:current train perplexity1.0009955167770386
INFO:root:current mean train loss 1.260544529570754
INFO:root:current train perplexity1.0009949207305908
INFO:root:current mean train loss 1.260934800203004
INFO:root:current train perplexity1.000995397567749
INFO:root:current mean train loss 1.2612456109144685
INFO:root:current train perplexity1.0009957551956177
INFO:root:current mean train loss 1.2612762296831859
INFO:root:current train perplexity1.0009955167770386
INFO:root:current mean train loss 1.261632443582001
INFO:root:current train perplexity1.0009955167770386
INFO:root:current mean train loss 1.2620386354355335
INFO:root:current train perplexity1.0009963512420654
INFO:root:current mean train loss 1.2618698746927324
INFO:root:current train perplexity1.0009961128234863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.02s/it]
INFO:root:final mean train loss: 1.2623394083868538
INFO:root:final train perplexity: 1.000996708869934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.0246545909144356
INFO:root:eval perplexity: 1.001639723777771
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.600469506378715
INFO:root:eval perplexity: 1.0021401643753052
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [22:00:24<13:33:25, 610.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2639322433716211
INFO:root:current train perplexity1.0009928941726685
INFO:root:current mean train loss 1.2514684397539646
INFO:root:current train perplexity1.0009849071502686
INFO:root:current mean train loss 1.2477633399444643
INFO:root:current train perplexity1.0009801387786865
INFO:root:current mean train loss 1.2498418029430693
INFO:root:current train perplexity1.0009862184524536
INFO:root:current mean train loss 1.2524382465249586
INFO:root:current train perplexity1.000990390777588
INFO:root:current mean train loss 1.2519468453908011
INFO:root:current train perplexity1.000990390777588
INFO:root:current mean train loss 1.2535926461779456
INFO:root:current train perplexity1.0009909868240356
INFO:root:current mean train loss 1.2550431810954588
INFO:root:current train perplexity1.0009922981262207
INFO:root:current mean train loss 1.255088449376985
INFO:root:current train perplexity1.0009924173355103
INFO:root:current mean train loss 1.2557936982986646
INFO:root:current train perplexity1.0009926557540894
INFO:root:current mean train loss 1.255768661090568
INFO:root:current train perplexity1.000991940498352
INFO:root:current mean train loss 1.25667531140757
INFO:root:current train perplexity1.0009925365447998
INFO:root:current mean train loss 1.2570638134173562
INFO:root:current train perplexity1.0009925365447998
INFO:root:current mean train loss 1.2573006665021114
INFO:root:current train perplexity1.0009924173355103
INFO:root:current mean train loss 1.2573784625389413
INFO:root:current train perplexity1.0009925365447998
INFO:root:current mean train loss 1.2574284395821456
INFO:root:current train perplexity1.0009925365447998
INFO:root:current mean train loss 1.2575907128987478
INFO:root:current train perplexity1.0009925365447998
INFO:root:current mean train loss 1.2577389824177905
INFO:root:current train perplexity1.0009926557540894
INFO:root:current mean train loss 1.2582474567761819
INFO:root:current train perplexity1.0009931325912476
INFO:root:current mean train loss 1.2584023666849082
INFO:root:current train perplexity1.0009931325912476

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.58s/it]
INFO:root:final mean train loss: 1.2586193426173562
INFO:root:final train perplexity: 1.0009938478469849
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.0259529982052795
INFO:root:eval perplexity: 1.001640796661377
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 2.602043107891759
INFO:root:eval perplexity: 1.0021414756774902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [22:10:33<13:22:48, 609.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2529112015451704
INFO:root:current train perplexity1.0009816884994507
INFO:root:current mean train loss 1.2511999003398113
INFO:root:current train perplexity1.0009841918945312
INFO:root:current mean train loss 1.2476364658214152
INFO:root:current train perplexity1.0009799003601074
INFO:root:current mean train loss 1.2476862709173995
INFO:root:current train perplexity1.0009825229644775
INFO:root:current mean train loss 1.2497310329947555
INFO:root:current train perplexity1.0009870529174805
INFO:root:current mean train loss 1.2510496412249779
INFO:root:current train perplexity1.000988245010376
INFO:root:current mean train loss 1.251522640084348
INFO:root:current train perplexity1.0009887218475342
INFO:root:current mean train loss 1.2515682414095237
INFO:root:current train perplexity1.0009886026382446
INFO:root:current mean train loss 1.2523122368571915
INFO:root:current train perplexity1.000989556312561
INFO:root:current mean train loss 1.2526307216997425
INFO:root:current train perplexity1.0009891986846924
INFO:root:current mean train loss 1.2534350568823742
INFO:root:current train perplexity1.0009897947311401
INFO:root:current mean train loss 1.2530683538905476
INFO:root:current train perplexity1.000989317893982
INFO:root:current mean train loss 1.2537178072580106
INFO:root:current train perplexity1.000989556312561
INFO:root:current mean train loss 1.2539567973761432
INFO:root:current train perplexity1.0009896755218506
INFO:root:current mean train loss 1.253918668644114
INFO:root:current train perplexity1.0009896755218506
INFO:root:current mean train loss 1.254131898705328
INFO:root:current train perplexity1.0009894371032715
INFO:root:current mean train loss 1.2546887919666687
INFO:root:current train perplexity1.0009900331497192
INFO:root:current mean train loss 1.2554396602042988
INFO:root:current train perplexity1.000990867614746
INFO:root:current mean train loss 1.2553041313999687
INFO:root:current train perplexity1.0009911060333252
INFO:root:current mean train loss 1.2556578572420498
INFO:root:current train perplexity1.0009913444519043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.31s/it]
INFO:root:final mean train loss: 1.2555739459399917
INFO:root:final train perplexity: 1.0009913444519043
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.0278758279820708
INFO:root:eval perplexity: 1.0016423463821411
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 2.6055169680439834
INFO:root:eval perplexity: 1.0021443367004395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [22:20:42<13:12:14, 609.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2459840921506489
INFO:root:current train perplexity1.0009747743606567
INFO:root:current mean train loss 1.2474799721227217
INFO:root:current train perplexity1.000984787940979
INFO:root:current mean train loss 1.243693239522941
INFO:root:current train perplexity1.0009821653366089
INFO:root:current mean train loss 1.244934339945182
INFO:root:current train perplexity1.0009821653366089
INFO:root:current mean train loss 1.2442847931359837
INFO:root:current train perplexity1.000981330871582
INFO:root:current mean train loss 1.2445289570623668
INFO:root:current train perplexity1.0009812116622925
INFO:root:current mean train loss 1.246443449830938
INFO:root:current train perplexity1.0009833574295044
INFO:root:current mean train loss 1.246484467468163
INFO:root:current train perplexity1.0009829998016357
INFO:root:current mean train loss 1.2470644894061356
INFO:root:current train perplexity1.000983476638794
INFO:root:current mean train loss 1.2475458640846417
INFO:root:current train perplexity1.0009833574295044
INFO:root:current mean train loss 1.248513832452246
INFO:root:current train perplexity1.0009845495224
INFO:root:current mean train loss 1.2486748076460856
INFO:root:current train perplexity1.0009845495224
INFO:root:current mean train loss 1.2495539134071272
INFO:root:current train perplexity1.000985860824585
INFO:root:current mean train loss 1.250267896961315
INFO:root:current train perplexity1.000986933708191
INFO:root:current mean train loss 1.2501824355335844
INFO:root:current train perplexity1.0009865760803223
INFO:root:current mean train loss 1.2510801738640798
INFO:root:current train perplexity1.0009876489639282
INFO:root:current mean train loss 1.25161421926023
INFO:root:current train perplexity1.0009880065917969
INFO:root:current mean train loss 1.252613533639128
INFO:root:current train perplexity1.0009888410568237
INFO:root:current mean train loss 1.252608019848654
INFO:root:current train perplexity1.0009888410568237
INFO:root:current mean train loss 1.2527616122502454
INFO:root:current train perplexity1.0009888410568237

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.78s/it]
INFO:root:final mean train loss: 1.2528534595616465
INFO:root:final train perplexity: 1.0009891986846924
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.030483439036295
INFO:root:eval perplexity: 1.001644492149353
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 2.6093268563561405
INFO:root:eval perplexity: 1.0021475553512573
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [22:30:51<13:01:57, 609.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2465969218148125
INFO:root:current train perplexity1.0009886026382446
INFO:root:current mean train loss 1.2404570780302349
INFO:root:current train perplexity1.000981092453003
INFO:root:current mean train loss 1.2391301607263498
INFO:root:current train perplexity1.000977635383606
INFO:root:current mean train loss 1.2426822763222916
INFO:root:current train perplexity1.0009793043136597
INFO:root:current mean train loss 1.2416839918311762
INFO:root:current train perplexity1.0009793043136597
INFO:root:current mean train loss 1.2423372078750092
INFO:root:current train perplexity1.000980019569397
INFO:root:current mean train loss 1.2442375673763995
INFO:root:current train perplexity1.0009820461273193
INFO:root:current mean train loss 1.2440332295019416
INFO:root:current train perplexity1.0009803771972656
INFO:root:current mean train loss 1.245258903905247
INFO:root:current train perplexity1.0009821653366089
INFO:root:current mean train loss 1.2458797946120754
INFO:root:current train perplexity1.0009829998016357
INFO:root:current mean train loss 1.2457847446476649
INFO:root:current train perplexity1.0009825229644775
INFO:root:current mean train loss 1.2459873454911368
INFO:root:current train perplexity1.0009825229644775
INFO:root:current mean train loss 1.2462719352670417
INFO:root:current train perplexity1.0009827613830566
INFO:root:current mean train loss 1.2467709548181767
INFO:root:current train perplexity1.0009840726852417
INFO:root:current mean train loss 1.2476714277427468
INFO:root:current train perplexity1.0009855031967163
INFO:root:current mean train loss 1.2480506042264543
INFO:root:current train perplexity1.0009852647781372
INFO:root:current mean train loss 1.2488572453606057
INFO:root:current train perplexity1.0009859800338745
INFO:root:current mean train loss 1.2493047941996398
INFO:root:current train perplexity1.0009864568710327
INFO:root:current mean train loss 1.249304378284979
INFO:root:current train perplexity1.0009859800338745

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.19s/it]
INFO:root:final mean train loss: 1.249702716138227
INFO:root:final train perplexity: 1.0009866952896118
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.0327770511309304
INFO:root:eval perplexity: 1.0016462802886963
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2.611407554741447
INFO:root:eval perplexity: 1.002149224281311
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [22:41:00<12:51:54, 609.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2375916923795427
INFO:root:current train perplexity1.0009719133377075
INFO:root:current mean train loss 1.2428648538678606
INFO:root:current train perplexity1.0009838342666626
INFO:root:current mean train loss 1.2417966196502463
INFO:root:current train perplexity1.0009815692901611
INFO:root:current mean train loss 1.2423903072307474
INFO:root:current train perplexity1.0009816884994507
INFO:root:current mean train loss 1.2424269044721448
INFO:root:current train perplexity1.0009806156158447
INFO:root:current mean train loss 1.242643103561928
INFO:root:current train perplexity1.0009807348251343
INFO:root:current mean train loss 1.2423897469063372
INFO:root:current train perplexity1.0009807348251343
INFO:root:current mean train loss 1.2431070312246406
INFO:root:current train perplexity1.000981330871582
INFO:root:current mean train loss 1.243203355920359
INFO:root:current train perplexity1.000981330871582
INFO:root:current mean train loss 1.2434819279589753
INFO:root:current train perplexity1.0009816884994507
INFO:root:current mean train loss 1.243606115879108
INFO:root:current train perplexity1.0009821653366089
INFO:root:current mean train loss 1.2444231918295332
INFO:root:current train perplexity1.0009831190109253
INFO:root:current mean train loss 1.2443173271422556
INFO:root:current train perplexity1.0009822845458984
INFO:root:current mean train loss 1.2449524261245495
INFO:root:current train perplexity1.0009833574295044
INFO:root:current mean train loss 1.2451435597805411
INFO:root:current train perplexity1.0009832382202148
INFO:root:current mean train loss 1.2457251637363245
INFO:root:current train perplexity1.000983476638794
INFO:root:current mean train loss 1.2460822363261785
INFO:root:current train perplexity1.0009835958480835
INFO:root:current mean train loss 1.2462976212800423
INFO:root:current train perplexity1.000983715057373
INFO:root:current mean train loss 1.2464588234684517
INFO:root:current train perplexity1.000983715057373
INFO:root:current mean train loss 1.2464617808951592
INFO:root:current train perplexity1.0009835958480835

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.45s/it]
INFO:root:final mean train loss: 1.2463311250920375
INFO:root:final train perplexity: 1.0009840726852417
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.035304216628379
INFO:root:eval perplexity: 1.0016483068466187
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2.615958110660526
INFO:root:eval perplexity: 1.002152919769287
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [22:51:09<12:41:30, 609.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2406259775161743
INFO:root:current train perplexity1.0009938478469849
INFO:root:current mean train loss 1.2373947049340894
INFO:root:current train perplexity1.0009806156158447
INFO:root:current mean train loss 1.2362121897084373
INFO:root:current train perplexity1.000975251197815
INFO:root:current mean train loss 1.2374928387594812
INFO:root:current train perplexity1.0009771585464478
INFO:root:current mean train loss 1.2383350243545928
INFO:root:current train perplexity1.0009777545928955
INFO:root:current mean train loss 1.2389204390631376
INFO:root:current train perplexity1.000977635383606
INFO:root:current mean train loss 1.2399128805368373
INFO:root:current train perplexity1.0009783506393433
INFO:root:current mean train loss 1.2404468187969693
INFO:root:current train perplexity1.0009783506393433
INFO:root:current mean train loss 1.2403766744345137
INFO:root:current train perplexity1.0009779930114746
INFO:root:current mean train loss 1.2406616527022738
INFO:root:current train perplexity1.000977635383606
INFO:root:current mean train loss 1.2417770199244842
INFO:root:current train perplexity1.0009793043136597
INFO:root:current mean train loss 1.2425615769476228
INFO:root:current train perplexity1.000980019569397
INFO:root:current mean train loss 1.2426165712425132
INFO:root:current train perplexity1.0009799003601074
INFO:root:current mean train loss 1.2429986887828102
INFO:root:current train perplexity1.000980257987976
INFO:root:current mean train loss 1.2431349898322244
INFO:root:current train perplexity1.0009809732437134
INFO:root:current mean train loss 1.243355704730577
INFO:root:current train perplexity1.0009809732437134
INFO:root:current mean train loss 1.2437758305536701
INFO:root:current train perplexity1.0009815692901611
INFO:root:current mean train loss 1.244070526687839
INFO:root:current train perplexity1.0009816884994507
INFO:root:current mean train loss 1.2442363197343391
INFO:root:current train perplexity1.0009819269180298
INFO:root:current mean train loss 1.24427691027677
INFO:root:current train perplexity1.0009820461273193

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.98s/it]
INFO:root:final mean train loss: 1.2445218516189644
INFO:root:final train perplexity: 1.000982642173767
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.03652360084209
INFO:root:eval perplexity: 1.0016493797302246
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2.6171364953331913
INFO:root:eval perplexity: 1.002153992652893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [23:01:18<12:31:23, 609.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2341270592154525
INFO:root:current train perplexity1.000970482826233
INFO:root:current mean train loss 1.2338278530337285
INFO:root:current train perplexity1.0009751319885254
INFO:root:current mean train loss 1.2353257697647537
INFO:root:current train perplexity1.0009758472442627
INFO:root:current mean train loss 1.2348359253399532
INFO:root:current train perplexity1.0009738206863403
INFO:root:current mean train loss 1.2369398303042733
INFO:root:current train perplexity1.0009758472442627
INFO:root:current mean train loss 1.2362945573387216
INFO:root:current train perplexity1.0009748935699463
INFO:root:current mean train loss 1.2374333293873137
INFO:root:current train perplexity1.0009777545928955
INFO:root:current mean train loss 1.2379257218879567
INFO:root:current train perplexity1.000977873802185
INFO:root:current mean train loss 1.238077169366172
INFO:root:current train perplexity1.000977635383606
INFO:root:current mean train loss 1.2390012364838505
INFO:root:current train perplexity1.000978708267212
INFO:root:current mean train loss 1.2390959924740017
INFO:root:current train perplexity1.0009783506393433
INFO:root:current mean train loss 1.238879218945683
INFO:root:current train perplexity1.0009781122207642
INFO:root:current mean train loss 1.238860413203981
INFO:root:current train perplexity1.0009781122207642
INFO:root:current mean train loss 1.2394561591564408
INFO:root:current train perplexity1.000978946685791
INFO:root:current mean train loss 1.2393649169589644
INFO:root:current train perplexity1.0009785890579224
INFO:root:current mean train loss 1.2398564866577781
INFO:root:current train perplexity1.0009791851043701
INFO:root:current mean train loss 1.2396384521928376
INFO:root:current train perplexity1.0009784698486328
INFO:root:current mean train loss 1.2405408594130374
INFO:root:current train perplexity1.0009795427322388
INFO:root:current mean train loss 1.2406822569923255
INFO:root:current train perplexity1.0009794235229492
INFO:root:current mean train loss 1.2409479577537665
INFO:root:current train perplexity1.0009795427322388

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.77s/it]
INFO:root:final mean train loss: 1.2410370585777757
INFO:root:final train perplexity: 1.0009799003601074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.0374245423797173
INFO:root:eval perplexity: 1.001650094985962
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.617679759965721
INFO:root:eval perplexity: 1.0021543502807617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [23:11:26<12:20:48, 608.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.22611429773528
INFO:root:current train perplexity1.0009678602218628
INFO:root:current mean train loss 1.22694399009777
INFO:root:current train perplexity1.0009653568267822
INFO:root:current mean train loss 1.2306947624960611
INFO:root:current train perplexity1.000970482826233
INFO:root:current mean train loss 1.2317322142963303
INFO:root:current train perplexity1.0009715557098389
INFO:root:current mean train loss 1.2311878776966745
INFO:root:current train perplexity1.000969648361206
INFO:root:current mean train loss 1.2316109696169482
INFO:root:current train perplexity1.0009701251983643
INFO:root:current mean train loss 1.2331655404125665
INFO:root:current train perplexity1.000971794128418
INFO:root:current mean train loss 1.2328926148703985
INFO:root:current train perplexity1.000973105430603
INFO:root:current mean train loss 1.2330786503834046
INFO:root:current train perplexity1.000972867012024
INFO:root:current mean train loss 1.2338993327851584
INFO:root:current train perplexity1.000973105430603
INFO:root:current mean train loss 1.2345948044654327
INFO:root:current train perplexity1.0009739398956299
INFO:root:current mean train loss 1.2350606494205192
INFO:root:current train perplexity1.000974416732788
INFO:root:current mean train loss 1.235084059412794
INFO:root:current train perplexity1.000974416732788
INFO:root:current mean train loss 1.2355922541316373
INFO:root:current train perplexity1.0009748935699463
INFO:root:current mean train loss 1.2362512588828025
INFO:root:current train perplexity1.0009758472442627
INFO:root:current mean train loss 1.2369122278705642
INFO:root:current train perplexity1.0009760856628418
INFO:root:current mean train loss 1.2371488938285589
INFO:root:current train perplexity1.0009760856628418
INFO:root:current mean train loss 1.2374252139911717
INFO:root:current train perplexity1.0009759664535522
INFO:root:current mean train loss 1.238124281520864
INFO:root:current train perplexity1.0009770393371582
INFO:root:current mean train loss 1.2384675464177155
INFO:root:current train perplexity1.0009775161743164

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.26s/it]
INFO:root:final mean train loss: 1.2385230062588621
INFO:root:final train perplexity: 1.000977873802185
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.0414858236380504
INFO:root:eval perplexity: 1.0016533136367798
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2.621988611018404
INFO:root:eval perplexity: 1.0021579265594482
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [23:21:36<12:10:56, 609.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2308706823984783
INFO:root:current train perplexity1.0009582042694092
INFO:root:current mean train loss 1.229715302331107
INFO:root:current train perplexity1.0009655952453613
INFO:root:current mean train loss 1.2310246060111305
INFO:root:current train perplexity1.000970482826233
INFO:root:current mean train loss 1.2299651575088502
INFO:root:current train perplexity1.000969648361206
INFO:root:current mean train loss 1.2302157261497095
INFO:root:current train perplexity1.0009706020355225
INFO:root:current mean train loss 1.2303794730227926
INFO:root:current train perplexity1.0009708404541016
INFO:root:current mean train loss 1.2306626839107937
INFO:root:current train perplexity1.000971794128418
INFO:root:current mean train loss 1.2311171331713278
INFO:root:current train perplexity1.0009729862213135
INFO:root:current mean train loss 1.231388457434518
INFO:root:current train perplexity1.0009729862213135
INFO:root:current mean train loss 1.23214155123784
INFO:root:current train perplexity1.0009738206863403
INFO:root:current mean train loss 1.2334124519658642
INFO:root:current train perplexity1.0009751319885254
INFO:root:current mean train loss 1.234073192413817
INFO:root:current train perplexity1.0009753704071045
INFO:root:current mean train loss 1.234409708696253
INFO:root:current train perplexity1.000975251197815
INFO:root:current mean train loss 1.2343911346955734
INFO:root:current train perplexity1.0009750127792358
INFO:root:current mean train loss 1.2349901988950827
INFO:root:current train perplexity1.0009756088256836
INFO:root:current mean train loss 1.2350248677389963
INFO:root:current train perplexity1.0009750127792358
INFO:root:current mean train loss 1.2351023560139671
INFO:root:current train perplexity1.0009746551513672
INFO:root:current mean train loss 1.2356438230460798
INFO:root:current train perplexity1.0009748935699463
INFO:root:current mean train loss 1.2362341019948324
INFO:root:current train perplexity1.0009759664535522
INFO:root:current mean train loss 1.2362904453277588
INFO:root:current train perplexity1.0009759664535522

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.27s/it]
INFO:root:final mean train loss: 1.2363167125891388
INFO:root:final train perplexity: 1.0009762048721313
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.23s/it]
INFO:root:eval mean loss: 2.044893340016088
INFO:root:eval perplexity: 1.001656174659729
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it]
INFO:root:eval mean loss: 2.627361902953885
INFO:root:eval perplexity: 1.0021623373031616
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [23:31:54<12:03:53, 611.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2262343142343604
INFO:root:current train perplexity1.000969648361206
INFO:root:current mean train loss 1.2254793296257656
INFO:root:current train perplexity1.000967025756836
INFO:root:current mean train loss 1.2227618388117176
INFO:root:current train perplexity1.0009617805480957
INFO:root:current mean train loss 1.2260700132773847
INFO:root:current train perplexity1.0009658336639404
INFO:root:current mean train loss 1.2274457899535574
INFO:root:current train perplexity1.0009689331054688
INFO:root:current mean train loss 1.2291359595350317
INFO:root:current train perplexity1.0009709596633911
INFO:root:current mean train loss 1.2287537162703586
INFO:root:current train perplexity1.0009706020355225
INFO:root:current mean train loss 1.2288212732534216
INFO:root:current train perplexity1.0009711980819702
INFO:root:current mean train loss 1.228946170197474
INFO:root:current train perplexity1.000970482826233
INFO:root:current mean train loss 1.2290300789379305
INFO:root:current train perplexity1.0009700059890747
INFO:root:current mean train loss 1.228695659519552
INFO:root:current train perplexity1.000969648361206
INFO:root:current mean train loss 1.22927867526176
INFO:root:current train perplexity1.0009703636169434
INFO:root:current mean train loss 1.2294726643030858
INFO:root:current train perplexity1.000969648361206
INFO:root:current mean train loss 1.2304014488198292
INFO:root:current train perplexity1.0009700059890747
INFO:root:current mean train loss 1.2305247239548782
INFO:root:current train perplexity1.0009701251983643
INFO:root:current mean train loss 1.2313071503561346
INFO:root:current train perplexity1.0009706020355225
INFO:root:current mean train loss 1.2318006112784077
INFO:root:current train perplexity1.0009709596633911
INFO:root:current mean train loss 1.2323541743015605
INFO:root:current train perplexity1.0009721517562866
INFO:root:current mean train loss 1.2326304746228596
INFO:root:current train perplexity1.0009727478027344

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.94s/it]
INFO:root:final mean train loss: 1.2332389407607562
INFO:root:final train perplexity: 1.0009737014770508
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2.044205592456439
INFO:root:eval perplexity: 1.0016555786132812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.6266453147779965
INFO:root:eval perplexity: 1.0021617412567139
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [23:42:04<11:53:09, 611.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2082044283548992
INFO:root:current train perplexity1.0009669065475464
INFO:root:current mean train loss 1.2155527488900981
INFO:root:current train perplexity1.00095534324646
INFO:root:current mean train loss 1.220913803748537
INFO:root:current train perplexity1.0009604692459106
INFO:root:current mean train loss 1.2222602494712014
INFO:root:current train perplexity1.000962257385254
INFO:root:current mean train loss 1.2220944921078485
INFO:root:current train perplexity1.0009626150131226
INFO:root:current mean train loss 1.2230476742877474
INFO:root:current train perplexity1.0009630918502808
INFO:root:current mean train loss 1.2214851960760031
INFO:root:current train perplexity1.000961184501648
INFO:root:current mean train loss 1.2221307811683257
INFO:root:current train perplexity1.0009617805480957
INFO:root:current mean train loss 1.2238510368191562
INFO:root:current train perplexity1.0009641647338867
INFO:root:current mean train loss 1.224342303963253
INFO:root:current train perplexity1.0009647607803345
INFO:root:current mean train loss 1.2253409845032706
INFO:root:current train perplexity1.0009660720825195
INFO:root:current mean train loss 1.2261279677355794
INFO:root:current train perplexity1.000967264175415
INFO:root:current mean train loss 1.2270588063326031
INFO:root:current train perplexity1.0009685754776
INFO:root:current mean train loss 1.2273941114715623
INFO:root:current train perplexity1.0009688138961792
INFO:root:current mean train loss 1.2280882191708784
INFO:root:current train perplexity1.0009697675704956
INFO:root:current mean train loss 1.2284246292076337
INFO:root:current train perplexity1.0009698867797852
INFO:root:current mean train loss 1.228617798178147
INFO:root:current train perplexity1.0009702444076538
INFO:root:current mean train loss 1.2294022711903956
INFO:root:current train perplexity1.0009713172912598
INFO:root:current mean train loss 1.229832657204602
INFO:root:current train perplexity1.0009709596633911
INFO:root:current mean train loss 1.2302871125727315
INFO:root:current train perplexity1.0009714365005493

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.94s/it]
INFO:root:final mean train loss: 1.2304884725547593
INFO:root:final train perplexity: 1.0009715557098389
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.0477208714958626
INFO:root:eval perplexity: 1.0016584396362305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.6320167230376117
INFO:root:eval perplexity: 1.0021661520004272
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [23:52:12<11:41:54, 610.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2189399600028992
INFO:root:current train perplexity1.0009597539901733
INFO:root:current mean train loss 1.216580491217356
INFO:root:current train perplexity1.0009520053863525
INFO:root:current mean train loss 1.2218777368553972
INFO:root:current train perplexity1.0009602308273315
INFO:root:current mean train loss 1.223359378934638
INFO:root:current train perplexity1.0009651184082031
INFO:root:current mean train loss 1.2221830593588208
INFO:root:current train perplexity1.0009632110595703
INFO:root:current mean train loss 1.2230694074141208
INFO:root:current train perplexity1.0009645223617554
INFO:root:current mean train loss 1.2241424375448744
INFO:root:current train perplexity1.0009665489196777
INFO:root:current mean train loss 1.2248334364129163
INFO:root:current train perplexity1.0009675025939941
INFO:root:current mean train loss 1.22517637204893
INFO:root:current train perplexity1.0009678602218628
INFO:root:current mean train loss 1.2255097868375593
INFO:root:current train perplexity1.000967264175415
INFO:root:current mean train loss 1.2257622691855328
INFO:root:current train perplexity1.0009675025939941
INFO:root:current mean train loss 1.225655355419613
INFO:root:current train perplexity1.000967264175415
INFO:root:current mean train loss 1.225706484053302
INFO:root:current train perplexity1.0009675025939941
INFO:root:current mean train loss 1.2260590502219681
INFO:root:current train perplexity1.000968098640442
INFO:root:current mean train loss 1.2265901655000524
INFO:root:current train perplexity1.0009690523147583
INFO:root:current mean train loss 1.226432557421618
INFO:root:current train perplexity1.0009686946868896
INFO:root:current mean train loss 1.2267416486880875
INFO:root:current train perplexity1.0009686946868896
INFO:root:current mean train loss 1.2269434559386685
INFO:root:current train perplexity1.0009686946868896
INFO:root:current mean train loss 1.2272201423315987
INFO:root:current train perplexity1.0009684562683105
INFO:root:current mean train loss 1.2276168721239638
INFO:root:current train perplexity1.0009688138961792

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.55s/it]
INFO:root:final mean train loss: 1.2279434036378962
INFO:root:final train perplexity: 1.0009695291519165
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2.047752208320807
INFO:root:eval perplexity: 1.0016584396362305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2.63168825325391
INFO:root:eval perplexity: 1.0021659135818481
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [24:02:21<11:31:12, 609.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2188474522080532
INFO:root:current train perplexity1.0009589195251465
INFO:root:current mean train loss 1.2150272614472395
INFO:root:current train perplexity1.0009562969207764
INFO:root:current mean train loss 1.2171337776223328
INFO:root:current train perplexity1.000958800315857
INFO:root:current mean train loss 1.2187398029138325
INFO:root:current train perplexity1.0009604692459106
INFO:root:current mean train loss 1.2217762612865957
INFO:root:current train perplexity1.000964641571045
INFO:root:current mean train loss 1.2228033077211888
INFO:root:current train perplexity1.0009669065475464
INFO:root:current mean train loss 1.222704618195909
INFO:root:current train perplexity1.0009649991989136
INFO:root:current mean train loss 1.222940698606971
INFO:root:current train perplexity1.000964641571045
INFO:root:current mean train loss 1.2234349622703815
INFO:root:current train perplexity1.0009644031524658
INFO:root:current mean train loss 1.224432650325407
INFO:root:current train perplexity1.00096595287323
INFO:root:current mean train loss 1.2245653016226632
INFO:root:current train perplexity1.0009657144546509
INFO:root:current mean train loss 1.22462605616552
INFO:root:current train perplexity1.00096595287323
INFO:root:current mean train loss 1.2243786861315333
INFO:root:current train perplexity1.0009658336639404
INFO:root:current mean train loss 1.2250283788764822
INFO:root:current train perplexity1.0009665489196777
INFO:root:current mean train loss 1.2255612548159298
INFO:root:current train perplexity1.0009669065475464
INFO:root:current mean train loss 1.2253456461839065
INFO:root:current train perplexity1.0009665489196777
INFO:root:current mean train loss 1.2254789213690174
INFO:root:current train perplexity1.000967025756836
INFO:root:current mean train loss 1.2259473040811097
INFO:root:current train perplexity1.0009673833847046
INFO:root:current mean train loss 1.225923036920979
INFO:root:current train perplexity1.0009676218032837
INFO:root:current mean train loss 1.2264789133052267
INFO:root:current train perplexity1.0009682178497314

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.19s/it]
INFO:root:final mean train loss: 1.2265916729117665
INFO:root:final train perplexity: 1.0009684562683105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2.0506007654447083
INFO:root:eval perplexity: 1.001660704612732
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.63562895473859
INFO:root:eval perplexity: 1.002169132232666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [24:12:29<11:20:14, 609.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2209835310777029
INFO:root:current train perplexity1.0009602308273315
INFO:root:current mean train loss 1.2136311426758766
INFO:root:current train perplexity1.000957727432251
INFO:root:current mean train loss 1.214897389136828
INFO:root:current train perplexity1.0009599924087524
INFO:root:current mean train loss 1.2163429389397302
INFO:root:current train perplexity1.000961184501648
INFO:root:current mean train loss 1.2175862973151
INFO:root:current train perplexity1.0009613037109375
INFO:root:current mean train loss 1.2178630309445517
INFO:root:current train perplexity1.0009623765945435
INFO:root:current mean train loss 1.218452021931157
INFO:root:current train perplexity1.0009633302688599
INFO:root:current mean train loss 1.2180942104050987
INFO:root:current train perplexity1.0009613037109375
INFO:root:current mean train loss 1.2189693114092184
INFO:root:current train perplexity1.000962495803833
INFO:root:current mean train loss 1.2196084454655647
INFO:root:current train perplexity1.0009634494781494
INFO:root:current mean train loss 1.2191865078683168
INFO:root:current train perplexity1.0009618997573853
INFO:root:current mean train loss 1.219341805063445
INFO:root:current train perplexity1.0009621381759644
INFO:root:current mean train loss 1.2197763247149331
INFO:root:current train perplexity1.0009623765945435
INFO:root:current mean train loss 1.2198226211702123
INFO:root:current train perplexity1.000962734222412
INFO:root:current mean train loss 1.220588170992185
INFO:root:current train perplexity1.000963568687439
INFO:root:current mean train loss 1.221041418115298
INFO:root:current train perplexity1.000963568687439
INFO:root:current mean train loss 1.2213558393788626
INFO:root:current train perplexity1.0009636878967285
INFO:root:current mean train loss 1.2221374184570528
INFO:root:current train perplexity1.0009642839431763
INFO:root:current mean train loss 1.2224416274537322
INFO:root:current train perplexity1.0009642839431763
INFO:root:current mean train loss 1.2230850138834544
INFO:root:current train perplexity1.0009652376174927

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.47s/it]
INFO:root:final mean train loss: 1.2232387832361704
INFO:root:final train perplexity: 1.0009658336639404
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2.053789590690153
INFO:root:eval perplexity: 1.001663327217102
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.637203424534899
INFO:root:eval perplexity: 1.002170443534851
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [24:22:37<11:09:55, 609.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2065953743922246
INFO:root:current train perplexity1.0009537935256958
INFO:root:current mean train loss 1.2140506896595498
INFO:root:current train perplexity1.0009608268737793
INFO:root:current mean train loss 1.2145555023682246
INFO:root:current train perplexity1.0009623765945435
INFO:root:current mean train loss 1.2166719863522275
INFO:root:current train perplexity1.0009654760360718
INFO:root:current mean train loss 1.2169266204414129
INFO:root:current train perplexity1.0009657144546509
INFO:root:current mean train loss 1.2178975971773964
INFO:root:current train perplexity1.0009655952453613
INFO:root:current mean train loss 1.2170031731026563
INFO:root:current train perplexity1.000962495803833
INFO:root:current mean train loss 1.217496942982864
INFO:root:current train perplexity1.0009628534317017
INFO:root:current mean train loss 1.2173539932940405
INFO:root:current train perplexity1.0009609460830688
INFO:root:current mean train loss 1.2173380258749549
INFO:root:current train perplexity1.0009605884552002
INFO:root:current mean train loss 1.217662862305743
INFO:root:current train perplexity1.0009602308273315
INFO:root:current mean train loss 1.2184944609615291
INFO:root:current train perplexity1.0009608268737793
INFO:root:current mean train loss 1.21930793261472
INFO:root:current train perplexity1.0009615421295166
INFO:root:current mean train loss 1.2191965440606067
INFO:root:current train perplexity1.0009613037109375
INFO:root:current mean train loss 1.2196589131248747
INFO:root:current train perplexity1.0009616613388062
INFO:root:current mean train loss 1.2199004467484613
INFO:root:current train perplexity1.000962257385254
INFO:root:current mean train loss 1.2202985197856586
INFO:root:current train perplexity1.0009626150131226
INFO:root:current mean train loss 1.220448804559101
INFO:root:current train perplexity1.0009629726409912
INFO:root:current mean train loss 1.220338185734106
INFO:root:current train perplexity1.000962734222412
INFO:root:current mean train loss 1.2211013226781642
INFO:root:current train perplexity1.000963807106018

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.44s/it]
INFO:root:final mean train loss: 1.2211792211128616
INFO:root:final train perplexity: 1.0009641647338867
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.055285553137461
INFO:root:eval perplexity: 1.0016645193099976
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.6402534956627703
INFO:root:eval perplexity: 1.0021729469299316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [24:32:45<10:59:19, 608.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.224596696965238
INFO:root:current train perplexity1.000976324081421
INFO:root:current mean train loss 1.216185578365916
INFO:root:current train perplexity1.0009645223617554
INFO:root:current mean train loss 1.2153568701679205
INFO:root:current train perplexity1.000960111618042
INFO:root:current mean train loss 1.2160078865017383
INFO:root:current train perplexity1.0009621381759644
INFO:root:current mean train loss 1.21383209604966
INFO:root:current train perplexity1.0009593963623047
INFO:root:current mean train loss 1.2151795376832237
INFO:root:current train perplexity1.0009605884552002
INFO:root:current mean train loss 1.2156032863199195
INFO:root:current train perplexity1.0009599924087524
INFO:root:current mean train loss 1.2161533897109236
INFO:root:current train perplexity1.000960350036621
INFO:root:current mean train loss 1.2162574737664038
INFO:root:current train perplexity1.0009592771530151
INFO:root:current mean train loss 1.2167436114739125
INFO:root:current train perplexity1.0009599924087524
INFO:root:current mean train loss 1.2162705599934789
INFO:root:current train perplexity1.000958800315857
INFO:root:current mean train loss 1.2163716131318953
INFO:root:current train perplexity1.0009595155715942
INFO:root:current mean train loss 1.2167274348518395
INFO:root:current train perplexity1.0009597539901733
INFO:root:current mean train loss 1.2167015497277422
INFO:root:current train perplexity1.0009592771530151
INFO:root:current mean train loss 1.216997785182044
INFO:root:current train perplexity1.0009597539901733
INFO:root:current mean train loss 1.2172739092947698
INFO:root:current train perplexity1.0009597539901733
INFO:root:current mean train loss 1.2174998224837157
INFO:root:current train perplexity1.0009602308273315
INFO:root:current mean train loss 1.2177441015158477
INFO:root:current train perplexity1.0009604692459106
INFO:root:current mean train loss 1.2181662505506337
INFO:root:current train perplexity1.0009613037109375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.51s/it]
INFO:root:final mean train loss: 1.2184360537093755
INFO:root:final train perplexity: 1.0009620189666748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.0568410230021104
INFO:root:eval perplexity: 1.0016658306121826
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.6423973893442896
INFO:root:eval perplexity: 1.002174735069275
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [24:42:53<10:48:54, 608.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1855064955624668
INFO:root:current train perplexity1.0009148120880127
INFO:root:current mean train loss 1.2136787616454803
INFO:root:current train perplexity1.0009629726409912
INFO:root:current mean train loss 1.2131150559791457
INFO:root:current train perplexity1.000959873199463
INFO:root:current mean train loss 1.2115932503697189
INFO:root:current train perplexity1.0009576082229614
INFO:root:current mean train loss 1.2123004032747589
INFO:root:current train perplexity1.0009576082229614
INFO:root:current mean train loss 1.212842327974547
INFO:root:current train perplexity1.0009580850601196
INFO:root:current mean train loss 1.2126174308664475
INFO:root:current train perplexity1.00095796585083
INFO:root:current mean train loss 1.212874618093005
INFO:root:current train perplexity1.0009585618972778
INFO:root:current mean train loss 1.212160386521778
INFO:root:current train perplexity1.0009573698043823
INFO:root:current mean train loss 1.2122389449769657
INFO:root:current train perplexity1.0009573698043823
INFO:root:current mean train loss 1.2132020277274234
INFO:root:current train perplexity1.00095796585083
INFO:root:current mean train loss 1.2132634496388406
INFO:root:current train perplexity1.0009582042694092
INFO:root:current mean train loss 1.2135870036362224
INFO:root:current train perplexity1.000957727432251
INFO:root:current mean train loss 1.2136525661321562
INFO:root:current train perplexity1.0009572505950928
INFO:root:current mean train loss 1.2145258510982966
INFO:root:current train perplexity1.00095796585083
INFO:root:current mean train loss 1.2152286756914392
INFO:root:current train perplexity1.0009585618972778
INFO:root:current mean train loss 1.215166471005966
INFO:root:current train perplexity1.0009585618972778
INFO:root:current mean train loss 1.215472900679347
INFO:root:current train perplexity1.0009585618972778
INFO:root:current mean train loss 1.2159596750711223
INFO:root:current train perplexity1.0009596347808838
INFO:root:current mean train loss 1.2158670794708075
INFO:root:current train perplexity1.0009595155715942

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.30s/it]
INFO:root:final mean train loss: 1.2163836947006825
INFO:root:final train perplexity: 1.0009604692459106
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2.05900695307035
INFO:root:eval perplexity: 1.0016676187515259
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.646194935690427
INFO:root:eval perplexity: 1.0021778345108032
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [24:53:00<10:38:28, 608.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2064512542315893
INFO:root:current train perplexity1.0009492635726929
INFO:root:current mean train loss 1.206825790926814
INFO:root:current train perplexity1.0009584426879883
INFO:root:current mean train loss 1.2079316476980846
INFO:root:current train perplexity1.000954031944275
INFO:root:current mean train loss 1.2078329070312221
INFO:root:current train perplexity1.0009552240371704
INFO:root:current mean train loss 1.2101304918249076
INFO:root:current train perplexity1.0009574890136719
INFO:root:current mean train loss 1.2096510798190578
INFO:root:current train perplexity1.0009560585021973
INFO:root:current mean train loss 1.209750336256756
INFO:root:current train perplexity1.0009548664093018
INFO:root:current mean train loss 1.2109694618445177
INFO:root:current train perplexity1.0009558200836182
INFO:root:current mean train loss 1.2112640486535242
INFO:root:current train perplexity1.0009562969207764
INFO:root:current mean train loss 1.2121165038953567
INFO:root:current train perplexity1.0009567737579346
INFO:root:current mean train loss 1.211470146703349
INFO:root:current train perplexity1.0009560585021973
INFO:root:current mean train loss 1.2116929975384516
INFO:root:current train perplexity1.000956416130066
INFO:root:current mean train loss 1.2120236118569825
INFO:root:current train perplexity1.0009570121765137
INFO:root:current mean train loss 1.2122915351247212
INFO:root:current train perplexity1.0009570121765137
INFO:root:current mean train loss 1.2120642541837292
INFO:root:current train perplexity1.0009568929672241
INFO:root:current mean train loss 1.211658126162609
INFO:root:current train perplexity1.000956416130066
INFO:root:current mean train loss 1.2118630308191078
INFO:root:current train perplexity1.0009561777114868
INFO:root:current mean train loss 1.2121075374522694
INFO:root:current train perplexity1.0009561777114868
INFO:root:current mean train loss 1.2126783921629907
INFO:root:current train perplexity1.0009568929672241
INFO:root:current mean train loss 1.2131958762763446
INFO:root:current train perplexity1.0009574890136719

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.29s/it]
INFO:root:final mean train loss: 1.2132777012903404
INFO:root:final train perplexity: 1.00095796585083
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.063031773195199
INFO:root:eval perplexity: 1.0016708374023438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2.6518526542271283
INFO:root:eval perplexity: 1.0021824836730957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [25:03:09<10:28:28, 608.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2009034236272176
INFO:root:current train perplexity1.0009478330612183
INFO:root:current mean train loss 1.2020798099452052
INFO:root:current train perplexity1.0009433031082153
INFO:root:current mean train loss 1.2037739855902536
INFO:root:current train perplexity1.0009472370147705
INFO:root:current mean train loss 1.2048258733058321
INFO:root:current train perplexity1.0009498596191406
INFO:root:current mean train loss 1.2038564550742674
INFO:root:current train perplexity1.0009485483169556
INFO:root:current mean train loss 1.2051518928020373
INFO:root:current train perplexity1.000949501991272
INFO:root:current mean train loss 1.2057605691658435
INFO:root:current train perplexity1.000949501991272
INFO:root:current mean train loss 1.2067454091654528
INFO:root:current train perplexity1.000950574874878
INFO:root:current mean train loss 1.2072476724195762
INFO:root:current train perplexity1.0009516477584839
INFO:root:current mean train loss 1.208962089548666
INFO:root:current train perplexity1.0009535551071167
INFO:root:current mean train loss 1.2094947439631778
INFO:root:current train perplexity1.000954508781433
INFO:root:current mean train loss 1.209749889894344
INFO:root:current train perplexity1.0009548664093018
INFO:root:current mean train loss 1.2094426935456364
INFO:root:current train perplexity1.000954508781433
INFO:root:current mean train loss 1.209987618222999
INFO:root:current train perplexity1.0009548664093018
INFO:root:current mean train loss 1.2098483520395615
INFO:root:current train perplexity1.0009547472000122
INFO:root:current mean train loss 1.2101251145396803
INFO:root:current train perplexity1.0009552240371704
INFO:root:current mean train loss 1.2106468306486367
INFO:root:current train perplexity1.0009558200836182
INFO:root:current mean train loss 1.2108240898154186
INFO:root:current train perplexity1.0009557008743286
INFO:root:current mean train loss 1.2107030094477542
INFO:root:current train perplexity1.0009554624557495
INFO:root:current mean train loss 1.2112538702996654
INFO:root:current train perplexity1.0009560585021973

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.40s/it]
INFO:root:final mean train loss: 1.2112938235999957
INFO:root:final train perplexity: 1.000956416130066
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2.0622150183569454
INFO:root:eval perplexity: 1.0016701221466064
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.649815370850529
INFO:root:eval perplexity: 1.002180814743042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [25:13:16<10:18:09, 608.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.209940941103043
INFO:root:current train perplexity1.000953197479248
INFO:root:current mean train loss 1.2072678208351135
INFO:root:current train perplexity1.00095534324646
INFO:root:current mean train loss 1.207117717229683
INFO:root:current train perplexity1.000952959060669
INFO:root:current mean train loss 1.2048323302637807
INFO:root:current train perplexity1.0009512901306152
INFO:root:current mean train loss 1.205359560844702
INFO:root:current train perplexity1.0009526014328003
INFO:root:current mean train loss 1.2057895509797907
INFO:root:current train perplexity1.0009516477584839
INFO:root:current mean train loss 1.2065771837969201
INFO:root:current train perplexity1.0009530782699585
INFO:root:current mean train loss 1.207142507153859
INFO:root:current train perplexity1.000954031944275
INFO:root:current mean train loss 1.2068554333080548
INFO:root:current train perplexity1.0009539127349854
INFO:root:current mean train loss 1.2073315540619056
INFO:root:current train perplexity1.000954031944275
INFO:root:current mean train loss 1.2073319738418593
INFO:root:current train perplexity1.0009536743164062
INFO:root:current mean train loss 1.2080658669110624
INFO:root:current train perplexity1.0009543895721436
INFO:root:current mean train loss 1.2076562455074533
INFO:root:current train perplexity1.0009530782699585
INFO:root:current mean train loss 1.208239082285311
INFO:root:current train perplexity1.0009536743164062
INFO:root:current mean train loss 1.2085636899017929
INFO:root:current train perplexity1.0009543895721436
INFO:root:current mean train loss 1.2087467618849457
INFO:root:current train perplexity1.000954270362854
INFO:root:current mean train loss 1.208353176300563
INFO:root:current train perplexity1.0009535551071167
INFO:root:current mean train loss 1.2087204389350232
INFO:root:current train perplexity1.0009539127349854
INFO:root:current mean train loss 1.208831266032894
INFO:root:current train perplexity1.000954031944275
INFO:root:current mean train loss 1.209393178286538
INFO:root:current train perplexity1.0009546279907227

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.64s/it]
INFO:root:final mean train loss: 1.209372398953094
INFO:root:final train perplexity: 1.0009548664093018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it]
INFO:root:eval mean loss: 2.063531664246363
INFO:root:eval perplexity: 1.0016711950302124
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.651361308199294
INFO:root:eval perplexity: 1.002182126045227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [25:23:24<10:07:57, 607.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1976960097687155
INFO:root:current train perplexity1.000944972038269
INFO:root:current mean train loss 1.1965098407681427
INFO:root:current train perplexity1.000938892364502
INFO:root:current mean train loss 1.2012288288403583
INFO:root:current train perplexity1.000946044921875
INFO:root:current mean train loss 1.2027979340591028
INFO:root:current train perplexity1.0009493827819824
INFO:root:current mean train loss 1.201317727690201
INFO:root:current train perplexity1.0009479522705078
INFO:root:current mean train loss 1.201705592489819
INFO:root:current train perplexity1.000948190689087
INFO:root:current mean train loss 1.203097880325542
INFO:root:current train perplexity1.0009498596191406
INFO:root:current mean train loss 1.203350689199984
INFO:root:current train perplexity1.0009499788284302
INFO:root:current mean train loss 1.203477116439394
INFO:root:current train perplexity1.0009504556655884
INFO:root:current mean train loss 1.2037545727514514
INFO:root:current train perplexity1.0009509325027466
INFO:root:current mean train loss 1.2040072261900456
INFO:root:current train perplexity1.0009511709213257
INFO:root:current mean train loss 1.204309806973374
INFO:root:current train perplexity1.0009514093399048
INFO:root:current mean train loss 1.2050396836857202
INFO:root:current train perplexity1.000952124595642
INFO:root:current mean train loss 1.2056451495615272
INFO:root:current train perplexity1.0009522438049316
INFO:root:current mean train loss 1.2054081572480424
INFO:root:current train perplexity1.0009511709213257
INFO:root:current mean train loss 1.2056040564543693
INFO:root:current train perplexity1.0009516477584839
INFO:root:current mean train loss 1.2062102807569248
INFO:root:current train perplexity1.000952124595642
INFO:root:current mean train loss 1.2062165855088216
INFO:root:current train perplexity1.000951886177063
INFO:root:current mean train loss 1.2061486846275187
INFO:root:current train perplexity1.0009516477584839
INFO:root:current mean train loss 1.2068699354954353
INFO:root:current train perplexity1.0009527206420898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.28s/it]
INFO:root:final mean train loss: 1.206825952306277
INFO:root:final train perplexity: 1.0009528398513794
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2.0676370022144726
INFO:root:eval perplexity: 1.0016745328903198
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.656160144095725
INFO:root:eval perplexity: 1.0021860599517822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [25:33:32<9:57:40, 607.80s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1994472692410152
INFO:root:current train perplexity1.0009543895721436
INFO:root:current mean train loss 1.2013696724054765
INFO:root:current train perplexity1.000949501991272
INFO:root:current mean train loss 1.2004547441327893
INFO:root:current train perplexity1.0009469985961914
INFO:root:current mean train loss 1.200744548831323
INFO:root:current train perplexity1.0009475946426392
INFO:root:current mean train loss 1.2011838800484134
INFO:root:current train perplexity1.0009478330612183
INFO:root:current mean train loss 1.2025580118166521
INFO:root:current train perplexity1.0009502172470093
INFO:root:current mean train loss 1.202437229026323
INFO:root:current train perplexity1.0009499788284302
INFO:root:current mean train loss 1.2015293167164578
INFO:root:current train perplexity1.0009492635726929
INFO:root:current mean train loss 1.201857636150505
INFO:root:current train perplexity1.0009503364562988
INFO:root:current mean train loss 1.2024589013383091
INFO:root:current train perplexity1.0009511709213257
INFO:root:current mean train loss 1.2028689550874878
INFO:root:current train perplexity1.000950813293457
INFO:root:current mean train loss 1.2033685389968465
INFO:root:current train perplexity1.0009511709213257
INFO:root:current mean train loss 1.2033780030446288
INFO:root:current train perplexity1.0009503364562988
INFO:root:current mean train loss 1.2036837608390687
INFO:root:current train perplexity1.0009509325027466
INFO:root:current mean train loss 1.2040955315936694
INFO:root:current train perplexity1.0009510517120361
INFO:root:current mean train loss 1.204339390335824
INFO:root:current train perplexity1.000950813293457
INFO:root:current mean train loss 1.2044352326753005
INFO:root:current train perplexity1.000950813293457
INFO:root:current mean train loss 1.204446270962335
INFO:root:current train perplexity1.000950813293457
INFO:root:current mean train loss 1.204503009905292
INFO:root:current train perplexity1.000950813293457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.27s/it]
INFO:root:final mean train loss: 1.2049923387369723
INFO:root:final train perplexity: 1.0009514093399048
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it]
INFO:root:eval mean loss: 2.0682911327544677
INFO:root:eval perplexity: 1.0016751289367676
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2.6588591404840454
INFO:root:eval perplexity: 1.0021883249282837
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [25:43:39<9:47:25, 607.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1894391041535597
INFO:root:current train perplexity1.0009485483169556
INFO:root:current mean train loss 1.1937808167617934
INFO:root:current train perplexity1.0009371042251587
INFO:root:current mean train loss 1.1970976786994039
INFO:root:current train perplexity1.000941276550293
INFO:root:current mean train loss 1.1977967259982905
INFO:root:current train perplexity1.0009446144104004
INFO:root:current mean train loss 1.1979035733687098
INFO:root:current train perplexity1.0009444952011108
INFO:root:current mean train loss 1.1973323940533644
INFO:root:current train perplexity1.0009434223175049
INFO:root:current mean train loss 1.1988300754039944
INFO:root:current train perplexity1.0009453296661377
INFO:root:current mean train loss 1.2000301877378916
INFO:root:current train perplexity1.0009467601776123
INFO:root:current mean train loss 1.1999663594024208
INFO:root:current train perplexity1.000946283340454
INFO:root:current mean train loss 1.2002149175318022
INFO:root:current train perplexity1.0009464025497437
INFO:root:current mean train loss 1.200108615220005
INFO:root:current train perplexity1.0009456872940063
INFO:root:current mean train loss 1.200778557605178
INFO:root:current train perplexity1.0009464025497437
INFO:root:current mean train loss 1.2009472236688337
INFO:root:current train perplexity1.000947117805481
INFO:root:current mean train loss 1.2007942724482121
INFO:root:current train perplexity1.0009464025497437
INFO:root:current mean train loss 1.201426877168924
INFO:root:current train perplexity1.000947117805481
INFO:root:current mean train loss 1.2017879919429055
INFO:root:current train perplexity1.00094735622406
INFO:root:current mean train loss 1.2018505606825722
INFO:root:current train perplexity1.0009478330612183
INFO:root:current mean train loss 1.2020702070474485
INFO:root:current train perplexity1.000948429107666
INFO:root:current mean train loss 1.202340122552715
INFO:root:current train perplexity1.0009489059448242
INFO:root:current mean train loss 1.202865207625258
INFO:root:current train perplexity1.000949740409851

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.84s/it]
INFO:root:final mean train loss: 1.2025689642295896
INFO:root:final train perplexity: 1.000949501991272
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2.0694884767769075
INFO:root:eval perplexity: 1.001676082611084
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.659537360177818
INFO:root:eval perplexity: 1.0021889209747314
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [25:53:46<9:37:07, 607.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.2076291918754578
INFO:root:current train perplexity1.0009735822677612
INFO:root:current mean train loss 1.1989015973531283
INFO:root:current train perplexity1.0009464025497437
INFO:root:current mean train loss 1.1942213405733524
INFO:root:current train perplexity1.0009400844573975
INFO:root:current mean train loss 1.1925854975526984
INFO:root:current train perplexity1.0009382963180542
INFO:root:current mean train loss 1.1922405412030774
INFO:root:current train perplexity1.0009379386901855
INFO:root:current mean train loss 1.192090099937511
INFO:root:current train perplexity1.0009382963180542
INFO:root:current mean train loss 1.1931147609438215
INFO:root:current train perplexity1.0009396076202393
INFO:root:current mean train loss 1.193498862442905
INFO:root:current train perplexity1.0009406805038452
INFO:root:current mean train loss 1.1956563237201736
INFO:root:current train perplexity1.0009433031082153
INFO:root:current mean train loss 1.1966532095786064
INFO:root:current train perplexity1.0009440183639526
INFO:root:current mean train loss 1.1968537304007891
INFO:root:current train perplexity1.0009441375732422
INFO:root:current mean train loss 1.197459659428723
INFO:root:current train perplexity1.0009446144104004
INFO:root:current mean train loss 1.1976687190978508
INFO:root:current train perplexity1.00094473361969
INFO:root:current mean train loss 1.1986277873354747
INFO:root:current train perplexity1.0009459257125854
INFO:root:current mean train loss 1.199022833450691
INFO:root:current train perplexity1.000946283340454
INFO:root:current mean train loss 1.1997420381097232
INFO:root:current train perplexity1.00094735622406
INFO:root:current mean train loss 1.1994711211122617
INFO:root:current train perplexity1.0009469985961914
INFO:root:current mean train loss 1.1996479453379019
INFO:root:current train perplexity1.0009472370147705
INFO:root:current mean train loss 1.1995064964059923
INFO:root:current train perplexity1.0009466409683228
INFO:root:current mean train loss 1.1996296276700311
INFO:root:current train perplexity1.0009467601776123

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.66s/it]
INFO:root:final mean train loss: 1.1998153382699055
INFO:root:final train perplexity: 1.00094735622406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2.0709745207576886
INFO:root:eval perplexity: 1.0016772747039795
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.661926950123293
INFO:root:eval perplexity: 1.0021908283233643
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [26:03:53<9:26:48, 607.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1879095904370571
INFO:root:current train perplexity1.0009379386901855
INFO:root:current mean train loss 1.1866583678187157
INFO:root:current train perplexity1.0009334087371826
INFO:root:current mean train loss 1.1925291904071083
INFO:root:current train perplexity1.0009417533874512
INFO:root:current mean train loss 1.1926700759689814
INFO:root:current train perplexity1.0009434223175049
INFO:root:current mean train loss 1.193068967302907
INFO:root:current train perplexity1.0009429454803467
INFO:root:current mean train loss 1.1933048346796682
INFO:root:current train perplexity1.000942587852478
INFO:root:current mean train loss 1.1930407569434212
INFO:root:current train perplexity1.000942587852478
INFO:root:current mean train loss 1.19362008699292
INFO:root:current train perplexity1.0009419918060303
INFO:root:current mean train loss 1.1942576001076095
INFO:root:current train perplexity1.0009417533874512
INFO:root:current mean train loss 1.1954983470306477
INFO:root:current train perplexity1.0009430646896362
INFO:root:current mean train loss 1.1959307196033262
INFO:root:current train perplexity1.0009434223175049
INFO:root:current mean train loss 1.1955916815045198
INFO:root:current train perplexity1.0009429454803467
INFO:root:current mean train loss 1.1961124416533335
INFO:root:current train perplexity1.0009437799453735
INFO:root:current mean train loss 1.1959390393344225
INFO:root:current train perplexity1.000943660736084
INFO:root:current mean train loss 1.1962841602548204
INFO:root:current train perplexity1.0009440183639526
INFO:root:current mean train loss 1.1964570141331796
INFO:root:current train perplexity1.000943899154663
INFO:root:current mean train loss 1.1971012512408536
INFO:root:current train perplexity1.0009450912475586
INFO:root:current mean train loss 1.197287562921106
INFO:root:current train perplexity1.0009452104568481
INFO:root:current mean train loss 1.1979154773705962
INFO:root:current train perplexity1.0009456872940063
INFO:root:current mean train loss 1.1984478762410637
INFO:root:current train perplexity1.000946044921875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.81s/it]
INFO:root:final mean train loss: 1.1985246442630324
INFO:root:final train perplexity: 1.000946283340454
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2.073133999151541
INFO:root:eval perplexity: 1.0016790628433228
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.66392597492705
INFO:root:eval perplexity: 1.002192497253418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [26:14:00<9:16:36, 607.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1905522122979164
INFO:root:current train perplexity1.0009416341781616
INFO:root:current mean train loss 1.192300813954051
INFO:root:current train perplexity1.000942349433899
INFO:root:current mean train loss 1.1887939513632746
INFO:root:current train perplexity1.0009392499923706
INFO:root:current mean train loss 1.190286683512258
INFO:root:current train perplexity1.0009393692016602
INFO:root:current mean train loss 1.1922535279701496
INFO:root:current train perplexity1.000942587852478
INFO:root:current mean train loss 1.1923941929712363
INFO:root:current train perplexity1.0009434223175049
INFO:root:current mean train loss 1.1923289631145546
INFO:root:current train perplexity1.0009422302246094
INFO:root:current mean train loss 1.192040052713524
INFO:root:current train perplexity1.000941276550293
INFO:root:current mean train loss 1.1919741453947845
INFO:root:current train perplexity1.0009411573410034
INFO:root:current mean train loss 1.1919399399727706
INFO:root:current train perplexity1.000941276550293
INFO:root:current mean train loss 1.192309159421383
INFO:root:current train perplexity1.0009413957595825
INFO:root:current mean train loss 1.192807915182048
INFO:root:current train perplexity1.0009419918060303
INFO:root:current mean train loss 1.1940431923051424
INFO:root:current train perplexity1.0009430646896362
INFO:root:current mean train loss 1.1949492856379478
INFO:root:current train perplexity1.0009440183639526
INFO:root:current mean train loss 1.1951036435333107
INFO:root:current train perplexity1.000943899154663
INFO:root:current mean train loss 1.1954022419574621
INFO:root:current train perplexity1.0009441375732422
INFO:root:current mean train loss 1.1956901130481408
INFO:root:current train perplexity1.0009441375732422
INFO:root:current mean train loss 1.1960799649459164
INFO:root:current train perplexity1.0009444952011108
INFO:root:current mean train loss 1.196134232016592
INFO:root:current train perplexity1.0009441375732422
INFO:root:current mean train loss 1.1964038567363366
INFO:root:current train perplexity1.0009443759918213

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.09s/it]
INFO:root:final mean train loss: 1.196402550104346
INFO:root:final train perplexity: 1.0009446144104004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2.0748442339558975
INFO:root:eval perplexity: 1.0016803741455078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2.6658745437649123
INFO:root:eval perplexity: 1.0021940469741821
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [26:24:07<9:06:31, 607.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1792916692333457
INFO:root:current train perplexity1.0009260177612305
INFO:root:current mean train loss 1.1855416884079824
INFO:root:current train perplexity1.0009373426437378
INFO:root:current mean train loss 1.1890452255982098
INFO:root:current train perplexity1.0009397268295288
INFO:root:current mean train loss 1.1907313886902777
INFO:root:current train perplexity1.0009407997131348
INFO:root:current mean train loss 1.1904119295290752
INFO:root:current train perplexity1.0009407997131348
INFO:root:current mean train loss 1.190645766545491
INFO:root:current train perplexity1.0009404420852661
INFO:root:current mean train loss 1.1904204201593274
INFO:root:current train perplexity1.000939965248108
INFO:root:current mean train loss 1.1913752029250435
INFO:root:current train perplexity1.000941276550293
INFO:root:current mean train loss 1.1912545147061213
INFO:root:current train perplexity1.0009413957595825
INFO:root:current mean train loss 1.191496107313368
INFO:root:current train perplexity1.000941276550293
INFO:root:current mean train loss 1.1918753956777977
INFO:root:current train perplexity1.0009416341781616
INFO:root:current mean train loss 1.1920953818441549
INFO:root:current train perplexity1.000941514968872
INFO:root:current mean train loss 1.1923998372802318
INFO:root:current train perplexity1.0009416341781616
INFO:root:current mean train loss 1.1931350641195364
INFO:root:current train perplexity1.0009424686431885
INFO:root:current mean train loss 1.193510002416988
INFO:root:current train perplexity1.0009429454803467
INFO:root:current mean train loss 1.193592983284455
INFO:root:current train perplexity1.000942587852478
INFO:root:current mean train loss 1.1937797353921513
INFO:root:current train perplexity1.0009419918060303
INFO:root:current mean train loss 1.1942061240722597
INFO:root:current train perplexity1.0009427070617676
INFO:root:current mean train loss 1.1945340659376282
INFO:root:current train perplexity1.0009430646896362
INFO:root:current mean train loss 1.1944522769808588
INFO:root:current train perplexity1.0009428262710571

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.95s/it]
INFO:root:final mean train loss: 1.194492510515695
INFO:root:final train perplexity: 1.0009431838989258
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2.0772438049316406
INFO:root:eval perplexity: 1.0016824007034302
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.6701889672177903
INFO:root:eval perplexity: 1.0021976232528687
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [26:34:14<8:56:22, 607.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1909624909868046
INFO:root:current train perplexity1.0009419918060303
INFO:root:current mean train loss 1.189841258405435
INFO:root:current train perplexity1.000938057899475
INFO:root:current mean train loss 1.190754842838185
INFO:root:current train perplexity1.0009405612945557
INFO:root:current mean train loss 1.191086640310048
INFO:root:current train perplexity1.000942587852478
INFO:root:current mean train loss 1.1902159931190521
INFO:root:current train perplexity1.0009410381317139
INFO:root:current mean train loss 1.1888582845994062
INFO:root:current train perplexity1.0009381771087646
INFO:root:current mean train loss 1.1886757518295572
INFO:root:current train perplexity1.0009381771087646
INFO:root:current mean train loss 1.1892550072275607
INFO:root:current train perplexity1.000938892364502
INFO:root:current mean train loss 1.1892549919922792
INFO:root:current train perplexity1.0009382963180542
INFO:root:current mean train loss 1.1890212932903925
INFO:root:current train perplexity1.000938057899475
INFO:root:current mean train loss 1.1893577944819826
INFO:root:current train perplexity1.0009387731552124
INFO:root:current mean train loss 1.1900016178869843
INFO:root:current train perplexity1.0009396076202393
INFO:root:current mean train loss 1.1902618683725366
INFO:root:current train perplexity1.0009394884109497
INFO:root:current mean train loss 1.1912785275640747
INFO:root:current train perplexity1.0009407997131348
INFO:root:current mean train loss 1.1912347378336061
INFO:root:current train perplexity1.0009405612945557
INFO:root:current mean train loss 1.1913143843971892
INFO:root:current train perplexity1.000940203666687
INFO:root:current mean train loss 1.1917769392191029
INFO:root:current train perplexity1.0009407997131348
INFO:root:current mean train loss 1.1918658164637506
INFO:root:current train perplexity1.0009404420852661
INFO:root:current mean train loss 1.1918894784844964
INFO:root:current train perplexity1.0009405612945557

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it]
INFO:root:final mean train loss: 1.1922299636474356
INFO:root:final train perplexity: 1.0009413957595825
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2.0799868381615227
INFO:root:eval perplexity: 1.001684546470642
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2.672855941961843
INFO:root:eval perplexity: 1.0021998882293701
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [26:44:22<8:46:20, 607.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.184949517250061
INFO:root:current train perplexity1.0009310245513916
INFO:root:current mean train loss 1.186951406105705
INFO:root:current train perplexity1.0009392499923706
INFO:root:current mean train loss 1.1855118862418241
INFO:root:current train perplexity1.0009346008300781
INFO:root:current mean train loss 1.1877297371152848
INFO:root:current train perplexity1.0009382963180542
INFO:root:current mean train loss 1.1872786059437028
INFO:root:current train perplexity1.0009373426437378
INFO:root:current mean train loss 1.1881802547325209
INFO:root:current train perplexity1.0009387731552124
INFO:root:current mean train loss 1.1884466601581107
INFO:root:current train perplexity1.0009397268295288
INFO:root:current mean train loss 1.1885527460725158
INFO:root:current train perplexity1.0009396076202393
INFO:root:current mean train loss 1.1886777048462007
INFO:root:current train perplexity1.0009393692016602
INFO:root:current mean train loss 1.1886678945822793
INFO:root:current train perplexity1.0009384155273438
INFO:root:current mean train loss 1.1889129298073904
INFO:root:current train perplexity1.000938892364502
INFO:root:current mean train loss 1.188782373779023
INFO:root:current train perplexity1.0009385347366333
INFO:root:current mean train loss 1.188468122972873
INFO:root:current train perplexity1.0009381771087646
INFO:root:current mean train loss 1.1886756204380282
INFO:root:current train perplexity1.0009381771087646
INFO:root:current mean train loss 1.1886892456047948
INFO:root:current train perplexity1.0009379386901855
INFO:root:current mean train loss 1.1886216163635255
INFO:root:current train perplexity1.000937581062317
INFO:root:current mean train loss 1.1893495880782419
INFO:root:current train perplexity1.0009385347366333
INFO:root:current mean train loss 1.189655847035066
INFO:root:current train perplexity1.0009385347366333
INFO:root:current mean train loss 1.1898629895254929
INFO:root:current train perplexity1.0009390115737915
INFO:root:current mean train loss 1.1902468396851662
INFO:root:current train perplexity1.0009392499923706

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.11s/it]
INFO:root:final mean train loss: 1.1905721151281714
INFO:root:final train perplexity: 1.0009400844573975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:32<00:00, 32.47s/it]
INFO:root:eval mean loss: 2.079515797026614
INFO:root:eval perplexity: 1.0016841888427734
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.71s/it]
INFO:root:eval mean loss: 2.6721646701190487
INFO:root:eval perplexity: 1.0021992921829224
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [26:54:37<8:38:16, 609.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1770203970372677
INFO:root:current train perplexity1.0009315013885498
INFO:root:current mean train loss 1.1849772162509686
INFO:root:current train perplexity1.0009366273880005
INFO:root:current mean train loss 1.1841604005673836
INFO:root:current train perplexity1.0009348392486572
INFO:root:current mean train loss 1.1836340071925198
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.1851177817141567
INFO:root:current train perplexity1.0009346008300781
INFO:root:current mean train loss 1.1838285037897582
INFO:root:current train perplexity1.000933289527893
INFO:root:current mean train loss 1.1837230890612058
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.184423920072493
INFO:root:current train perplexity1.0009347200393677
INFO:root:current mean train loss 1.183699050774941
INFO:root:current train perplexity1.0009331703186035
INFO:root:current mean train loss 1.184253004359585
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.1840884783702303
INFO:root:current train perplexity1.0009334087371826
INFO:root:current mean train loss 1.1847170004785692
INFO:root:current train perplexity1.0009338855743408
INFO:root:current mean train loss 1.1857109854554202
INFO:root:current train perplexity1.0009350776672363
INFO:root:current mean train loss 1.1862608072636005
INFO:root:current train perplexity1.000935673713684
INFO:root:current mean train loss 1.1868315936799823
INFO:root:current train perplexity1.000936508178711
INFO:root:current mean train loss 1.1870083910055633
INFO:root:current train perplexity1.0009363889694214
INFO:root:current mean train loss 1.1870715421642744
INFO:root:current train perplexity1.000936508178711
INFO:root:current mean train loss 1.1876141528747373
INFO:root:current train perplexity1.0009371042251587
INFO:root:current mean train loss 1.1881339761497673
INFO:root:current train perplexity1.0009373426437378
INFO:root:current mean train loss 1.1881188072156215
INFO:root:current train perplexity1.0009377002716064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.03s/it]
INFO:root:final mean train loss: 1.1881783547812619
INFO:root:final train perplexity: 1.0009381771087646
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it]
INFO:root:eval mean loss: 2.083601558462102
INFO:root:eval perplexity: 1.0016875267028809
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.91s/it]
INFO:root:eval mean loss: 2.678740941886361
INFO:root:eval perplexity: 1.0022046566009521
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [27:05:17<8:35:37, 618.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.176535881295496
INFO:root:current train perplexity1.0009369850158691
INFO:root:current mean train loss 1.176509770771001
INFO:root:current train perplexity1.0009291172027588
INFO:root:current mean train loss 1.1789188724923805
INFO:root:current train perplexity1.0009289979934692
INFO:root:current mean train loss 1.1818234514028774
INFO:root:current train perplexity1.000931978225708
INFO:root:current mean train loss 1.1803706897659132
INFO:root:current train perplexity1.0009301900863647
INFO:root:current mean train loss 1.1821811246958804
INFO:root:current train perplexity1.000931978225708
INFO:root:current mean train loss 1.1817784678587009
INFO:root:current train perplexity1.000931739807129
INFO:root:current mean train loss 1.1812390103359247
INFO:root:current train perplexity1.000931978225708
INFO:root:current mean train loss 1.1829891187984616
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.183151001171518
INFO:root:current train perplexity1.0009338855743408
INFO:root:current mean train loss 1.1841028857162956
INFO:root:current train perplexity1.0009349584579468
INFO:root:current mean train loss 1.1841758345395612
INFO:root:current train perplexity1.0009351968765259
INFO:root:current mean train loss 1.184882836880161
INFO:root:current train perplexity1.0009360313415527
INFO:root:current mean train loss 1.1853845046130351
INFO:root:current train perplexity1.000936508178711
INFO:root:current mean train loss 1.185251539785176
INFO:root:current train perplexity1.0009362697601318
INFO:root:current mean train loss 1.18588112561144
INFO:root:current train perplexity1.0009371042251587
INFO:root:current mean train loss 1.1862082351981835
INFO:root:current train perplexity1.0009372234344482
INFO:root:current mean train loss 1.1863725902422146
INFO:root:current train perplexity1.0009371042251587
INFO:root:current mean train loss 1.1865834574108836
INFO:root:current train perplexity1.0009373426437378
INFO:root:current mean train loss 1.1868034083149628
INFO:root:current train perplexity1.0009371042251587

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.49s/it]
INFO:root:final mean train loss: 1.1868728087276625
INFO:root:final train perplexity: 1.0009371042251587
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.0830027342688107
INFO:root:eval perplexity: 1.0016870498657227
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.6770263208565135
INFO:root:eval perplexity: 1.0022032260894775
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [27:15:27<8:23:07, 616.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1807799050302217
INFO:root:current train perplexity1.0009340047836304
INFO:root:current mean train loss 1.1765954472932472
INFO:root:current train perplexity1.0009280443191528
INFO:root:current mean train loss 1.1783859312982488
INFO:root:current train perplexity1.0009303092956543
INFO:root:current mean train loss 1.1807722908551577
INFO:root:current train perplexity1.000935435295105
INFO:root:current mean train loss 1.1805596609995601
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.1810035292756853
INFO:root:current train perplexity1.0009340047836304
INFO:root:current mean train loss 1.1825222301411558
INFO:root:current train perplexity1.0009351968765259
INFO:root:current mean train loss 1.1830101181259354
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1820300337073435
INFO:root:current train perplexity1.0009325742721558
INFO:root:current mean train loss 1.182234495569707
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.1822710141232045
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.1826992317088472
INFO:root:current train perplexity1.00093412399292
INFO:root:current mean train loss 1.182944876887787
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1833340214077237
INFO:root:current train perplexity1.000934362411499
INFO:root:current mean train loss 1.1838229835277365
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1837233362831223
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1834143960461612
INFO:root:current train perplexity1.0009337663650513
INFO:root:current mean train loss 1.1837273319067259
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1837581433156168
INFO:root:current train perplexity1.0009342432022095
INFO:root:current mean train loss 1.1841459800001322
INFO:root:current train perplexity1.0009347200393677

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.82s/it]
INFO:root:final mean train loss: 1.1841625823435973
INFO:root:final train perplexity: 1.0009349584579468
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.084090415044879
INFO:root:eval perplexity: 1.0016878843307495
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.678349514379569
INFO:root:eval perplexity: 1.002204418182373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [27:25:36<8:11:11, 614.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1772362194865582
INFO:root:current train perplexity1.0009304285049438
INFO:root:current mean train loss 1.1782830660460426
INFO:root:current train perplexity1.0009288787841797
INFO:root:current mean train loss 1.1761234615379845
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1779277514539872
INFO:root:current train perplexity1.000927209854126
INFO:root:current mean train loss 1.178209812991614
INFO:root:current train perplexity1.000929355621338
INFO:root:current mean train loss 1.1796294129733156
INFO:root:current train perplexity1.0009310245513916
INFO:root:current mean train loss 1.1797064020902486
INFO:root:current train perplexity1.0009311437606812
INFO:root:current mean train loss 1.1806343358443006
INFO:root:current train perplexity1.0009315013885498
INFO:root:current mean train loss 1.180526768229691
INFO:root:current train perplexity1.0009311437606812
INFO:root:current mean train loss 1.1813520633313324
INFO:root:current train perplexity1.000931978225708
INFO:root:current mean train loss 1.1812886478498041
INFO:root:current train perplexity1.0009318590164185
INFO:root:current mean train loss 1.1818545899548374
INFO:root:current train perplexity1.0009328126907349
INFO:root:current mean train loss 1.182389082707935
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.1825527849183597
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.1828664523560053
INFO:root:current train perplexity1.00093412399292
INFO:root:current mean train loss 1.1830874287622757
INFO:root:current train perplexity1.000934362411499
INFO:root:current mean train loss 1.1829593727012087
INFO:root:current train perplexity1.0009338855743408
INFO:root:current mean train loss 1.1828334806878715
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.1834617172601813
INFO:root:current train perplexity1.0009342432022095
INFO:root:current mean train loss 1.1833800433924027
INFO:root:current train perplexity1.000934362411499

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.48s/it]
INFO:root:final mean train loss: 1.1833800433924027
INFO:root:final train perplexity: 1.000934362411499
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.086138234070852
INFO:root:eval perplexity: 1.0016895532608032
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.6822329810325134
INFO:root:eval perplexity: 1.0022075176239014
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [27:35:44<7:59:30, 612.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1734664833545685
INFO:root:current train perplexity1.0009289979934692
INFO:root:current mean train loss 1.1764026379585266
INFO:root:current train perplexity1.0009329319000244
INFO:root:current mean train loss 1.1769118420283
INFO:root:current train perplexity1.0009329319000244
INFO:root:current mean train loss 1.1786020955443381
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1778488795757294
INFO:root:current train perplexity1.0009326934814453
INFO:root:current mean train loss 1.1793275755643844
INFO:root:current train perplexity1.0009350776672363
INFO:root:current mean train loss 1.180061595099313
INFO:root:current train perplexity1.0009353160858154
INFO:root:current mean train loss 1.1797003270685673
INFO:root:current train perplexity1.0009338855743408
INFO:root:current mean train loss 1.1802723097801209
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1805788724422455
INFO:root:current train perplexity1.0009346008300781
INFO:root:current mean train loss 1.180713542916558
INFO:root:current train perplexity1.0009344816207886
INFO:root:current mean train loss 1.1813347771763802
INFO:root:current train perplexity1.0009350776672363
INFO:root:current mean train loss 1.1812455901732812
INFO:root:current train perplexity1.000934362411499
INFO:root:current mean train loss 1.1812736139127187
INFO:root:current train perplexity1.0009335279464722
INFO:root:current mean train loss 1.1815287888844808
INFO:root:current train perplexity1.000933289527893
INFO:root:current mean train loss 1.1818175636976957
INFO:root:current train perplexity1.0009336471557617
INFO:root:current mean train loss 1.1817931074955885
INFO:root:current train perplexity1.0009337663650513
INFO:root:current mean train loss 1.1814399709304173
INFO:root:current train perplexity1.0009329319000244
INFO:root:current mean train loss 1.181427274754173
INFO:root:current train perplexity1.0009328126907349

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.11s/it]
INFO:root:final mean train loss: 1.181192264734827
INFO:root:final train perplexity: 1.0009325742721558
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.089042473346629
INFO:root:eval perplexity: 1.0016919374465942
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2.684541704806876
INFO:root:eval perplexity: 1.0022094249725342
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [27:45:53<7:48:41, 611.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1643129587173462
INFO:root:current train perplexity1.0009088516235352
INFO:root:current mean train loss 1.1713143709378364
INFO:root:current train perplexity1.0009191036224365
INFO:root:current mean train loss 1.1742700371324741
INFO:root:current train perplexity1.0009270906448364
INFO:root:current mean train loss 1.175242295024523
INFO:root:current train perplexity1.0009280443191528
INFO:root:current mean train loss 1.1742037711955375
INFO:root:current train perplexity1.0009264945983887
INFO:root:current mean train loss 1.1744563118401532
INFO:root:current train perplexity1.0009264945983887
INFO:root:current mean train loss 1.1760244404284261
INFO:root:current train perplexity1.0009279251098633
INFO:root:current mean train loss 1.1758850036472124
INFO:root:current train perplexity1.0009275674819946
INFO:root:current mean train loss 1.1760449634187808
INFO:root:current train perplexity1.000927209854126
INFO:root:current mean train loss 1.1768855600086656
INFO:root:current train perplexity1.000928282737732
INFO:root:current mean train loss 1.177455891784548
INFO:root:current train perplexity1.0009292364120483
INFO:root:current mean train loss 1.1774069463555688
INFO:root:current train perplexity1.0009291172027588
INFO:root:current mean train loss 1.1775685842642554
INFO:root:current train perplexity1.0009289979934692
INFO:root:current mean train loss 1.1779217732703948
INFO:root:current train perplexity1.0009297132492065
INFO:root:current mean train loss 1.1784711631150337
INFO:root:current train perplexity1.0009305477142334
INFO:root:current mean train loss 1.1783442362877752
INFO:root:current train perplexity1.0009301900863647
INFO:root:current mean train loss 1.179072121521095
INFO:root:current train perplexity1.0009307861328125
INFO:root:current mean train loss 1.179324188271302
INFO:root:current train perplexity1.0009305477142334
INFO:root:current mean train loss 1.1798844508699617
INFO:root:current train perplexity1.0009313821792603
INFO:root:current mean train loss 1.1801846921599402
INFO:root:current train perplexity1.0009316205978394

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.68s/it]
INFO:root:final mean train loss: 1.1802994024855766
INFO:root:final train perplexity: 1.000931978225708
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.09076449127062
INFO:root:eval perplexity: 1.0016933679580688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2.68537818878255
INFO:root:eval perplexity: 1.0022101402282715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [27:56:02<7:37:59, 610.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1605761086239534
INFO:root:current train perplexity1.0009175539016724
INFO:root:current mean train loss 1.1674547168745923
INFO:root:current train perplexity1.0009227991104126
INFO:root:current mean train loss 1.1679854311494746
INFO:root:current train perplexity1.0009211301803589
INFO:root:current mean train loss 1.1707679158199333
INFO:root:current train perplexity1.0009243488311768
INFO:root:current mean train loss 1.1725234735396601
INFO:root:current train perplexity1.000925898551941
INFO:root:current mean train loss 1.171751508552037
INFO:root:current train perplexity1.0009242296218872
INFO:root:current mean train loss 1.1724105671001157
INFO:root:current train perplexity1.0009249448776245
INFO:root:current mean train loss 1.173509692463628
INFO:root:current train perplexity1.0009260177612305
INFO:root:current mean train loss 1.1742477672849057
INFO:root:current train perplexity1.0009263753890991
INFO:root:current mean train loss 1.1739106526721979
INFO:root:current train perplexity1.0009251832962036
INFO:root:current mean train loss 1.1740312885269666
INFO:root:current train perplexity1.0009251832962036
INFO:root:current mean train loss 1.1741392834266446
INFO:root:current train perplexity1.0009251832962036
INFO:root:current mean train loss 1.1743217534725245
INFO:root:current train perplexity1.0009254217147827
INFO:root:current mean train loss 1.1752667406509663
INFO:root:current train perplexity1.0009266138076782
INFO:root:current mean train loss 1.1754986067696096
INFO:root:current train perplexity1.0009269714355469
INFO:root:current mean train loss 1.1758910348393616
INFO:root:current train perplexity1.0009276866912842
INFO:root:current mean train loss 1.1762791405838882
INFO:root:current train perplexity1.0009281635284424
INFO:root:current mean train loss 1.1768637004608102
INFO:root:current train perplexity1.0009287595748901
INFO:root:current mean train loss 1.1771350686916737
INFO:root:current train perplexity1.0009289979934692
INFO:root:current mean train loss 1.1777658088377367
INFO:root:current train perplexity1.000929594039917

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.38s/it]
INFO:root:final mean train loss: 1.1779060055376362
INFO:root:final train perplexity: 1.0009300708770752
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.0920676838421652
INFO:root:eval perplexity: 1.0016943216323853
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.687608625871915
INFO:root:eval perplexity: 1.0022120475769043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [28:06:12<7:27:36, 610.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1660023693944894
INFO:root:current train perplexity1.0009208917617798
INFO:root:current mean train loss 1.1674908344319324
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.1693859399552364
INFO:root:current train perplexity1.0009210109710693
INFO:root:current mean train loss 1.1703428553040551
INFO:root:current train perplexity1.0009207725524902
INFO:root:current mean train loss 1.1702534340966302
INFO:root:current train perplexity1.000921368598938
INFO:root:current mean train loss 1.1713745710854089
INFO:root:current train perplexity1.0009231567382812
INFO:root:current mean train loss 1.1718451356008854
INFO:root:current train perplexity1.0009231567382812
INFO:root:current mean train loss 1.1720149386897385
INFO:root:current train perplexity1.0009227991104126
INFO:root:current mean train loss 1.1721099898061518
INFO:root:current train perplexity1.0009227991104126
INFO:root:current mean train loss 1.1729093386422447
INFO:root:current train perplexity1.0009236335754395
INFO:root:current mean train loss 1.173729316378411
INFO:root:current train perplexity1.0009253025054932
INFO:root:current mean train loss 1.1739101621195298
INFO:root:current train perplexity1.0009262561798096
INFO:root:current mean train loss 1.1735799159172722
INFO:root:current train perplexity1.0009257793426514
INFO:root:current mean train loss 1.1742873800674427
INFO:root:current train perplexity1.0009263753890991
INFO:root:current mean train loss 1.1747828275067982
INFO:root:current train perplexity1.0009268522262573
INFO:root:current mean train loss 1.175451245560022
INFO:root:current train perplexity1.0009278059005737
INFO:root:current mean train loss 1.175826694299639
INFO:root:current train perplexity1.0009281635284424
INFO:root:current mean train loss 1.1759818131279631
INFO:root:current train perplexity1.0009281635284424
INFO:root:current mean train loss 1.17640808979799
INFO:root:current train perplexity1.0009287595748901
INFO:root:current mean train loss 1.1765604029552317
INFO:root:current train perplexity1.0009287595748901

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.44s/it]
INFO:root:final mean train loss: 1.1763915519069916
INFO:root:final train perplexity: 1.0009288787841797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.0931129945930858
INFO:root:eval perplexity: 1.001695156097412
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2.690162961364638
INFO:root:eval perplexity: 1.0022140741348267
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [28:16:23<7:17:32, 610.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1681919799131506
INFO:root:current train perplexity1.000925064086914
INFO:root:current mean train loss 1.1712891871020907
INFO:root:current train perplexity1.0009231567382812
INFO:root:current mean train loss 1.171533312815339
INFO:root:current train perplexity1.0009253025054932
INFO:root:current mean train loss 1.170978435355684
INFO:root:current train perplexity1.0009241104125977
INFO:root:current mean train loss 1.1711463329629002
INFO:root:current train perplexity1.0009219646453857
INFO:root:current mean train loss 1.170617746215471
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1720923467310602
INFO:root:current train perplexity1.0009236335754395
INFO:root:current mean train loss 1.172231610243519
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1722961870206665
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1728636961091647
INFO:root:current train perplexity1.0009254217147827
INFO:root:current mean train loss 1.1733659443337372
INFO:root:current train perplexity1.00092613697052
INFO:root:current mean train loss 1.1735570545065892
INFO:root:current train perplexity1.0009262561798096
INFO:root:current mean train loss 1.1741648309802408
INFO:root:current train perplexity1.000927209854126
INFO:root:current mean train loss 1.1740128411012782
INFO:root:current train perplexity1.0009273290634155
INFO:root:current mean train loss 1.173765175592672
INFO:root:current train perplexity1.0009273290634155
INFO:root:current mean train loss 1.1738368899998617
INFO:root:current train perplexity1.0009270906448364
INFO:root:current mean train loss 1.1738834847077477
INFO:root:current train perplexity1.0009268522262573
INFO:root:current mean train loss 1.1742105117885235
INFO:root:current train perplexity1.0009270906448364
INFO:root:current mean train loss 1.1744225850324814
INFO:root:current train perplexity1.0009269714355469
INFO:root:current mean train loss 1.1747058284113077
INFO:root:current train perplexity1.000927209854126

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.99s/it]
INFO:root:final mean train loss: 1.174720356936895
INFO:root:final train perplexity: 1.0009275674819946
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.093726763065825
INFO:root:eval perplexity: 1.0016957521438599
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2.690536077140916
INFO:root:eval perplexity: 1.0022144317626953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [28:26:32<7:07:06, 610.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1637140175875496
INFO:root:current train perplexity1.000916838645935
INFO:root:current mean train loss 1.1665489390089705
INFO:root:current train perplexity1.0009177923202515
INFO:root:current mean train loss 1.1656663162666454
INFO:root:current train perplexity1.000916838645935
INFO:root:current mean train loss 1.167691975754577
INFO:root:current train perplexity1.0009204149246216
INFO:root:current mean train loss 1.1681774660484079
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.168837121816782
INFO:root:current train perplexity1.0009230375289917
INFO:root:current mean train loss 1.1695223693429988
INFO:root:current train perplexity1.00092351436615
INFO:root:current mean train loss 1.1709914222644393
INFO:root:current train perplexity1.0009247064590454
INFO:root:current mean train loss 1.1703898641349233
INFO:root:current train perplexity1.00092351436615
INFO:root:current mean train loss 1.1706775005698808
INFO:root:current train perplexity1.0009231567382812
INFO:root:current mean train loss 1.1709948894614997
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.171383560257119
INFO:root:current train perplexity1.0009241104125977
INFO:root:current mean train loss 1.1714095774327735
INFO:root:current train perplexity1.0009242296218872
INFO:root:current mean train loss 1.1717466335434346
INFO:root:current train perplexity1.0009247064590454
INFO:root:current mean train loss 1.1717105020176282
INFO:root:current train perplexity1.0009245872497559
INFO:root:current mean train loss 1.1719911985216833
INFO:root:current train perplexity1.0009247064590454
INFO:root:current mean train loss 1.1725270972407535
INFO:root:current train perplexity1.0009253025054932
INFO:root:current mean train loss 1.1726980523902828
INFO:root:current train perplexity1.0009260177612305
INFO:root:current mean train loss 1.172508822575172
INFO:root:current train perplexity1.0009254217147827

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.53s/it]
INFO:root:final mean train loss: 1.17285520280183
INFO:root:final train perplexity: 1.0009260177612305
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.096832664723092
INFO:root:eval perplexity: 1.0016982555389404
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.6940985765862973
INFO:root:eval perplexity: 1.0022172927856445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [28:36:40<6:56:28, 609.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1951477527618408
INFO:root:current train perplexity1.0009781122207642
INFO:root:current mean train loss 1.1632911516170876
INFO:root:current train perplexity1.0009217262268066
INFO:root:current mean train loss 1.166644010213342
INFO:root:current train perplexity1.0009219646453857
INFO:root:current mean train loss 1.169466289858155
INFO:root:current train perplexity1.000925898551941
INFO:root:current mean train loss 1.1688392257215965
INFO:root:current train perplexity1.0009249448776245
INFO:root:current mean train loss 1.1689733193215146
INFO:root:current train perplexity1.0009243488311768
INFO:root:current mean train loss 1.1690875228457276
INFO:root:current train perplexity1.0009249448776245
INFO:root:current mean train loss 1.169287521445174
INFO:root:current train perplexity1.000925898551941
INFO:root:current mean train loss 1.1703315788968245
INFO:root:current train perplexity1.0009267330169678
INFO:root:current mean train loss 1.1707986052707664
INFO:root:current train perplexity1.0009270906448364
INFO:root:current mean train loss 1.1712360334491538
INFO:root:current train perplexity1.0009269714355469
INFO:root:current mean train loss 1.1716306566543024
INFO:root:current train perplexity1.000927209854126
INFO:root:current mean train loss 1.1715132789484872
INFO:root:current train perplexity1.0009262561798096
INFO:root:current mean train loss 1.171550918284649
INFO:root:current train perplexity1.0009257793426514
INFO:root:current mean train loss 1.1709632667767338
INFO:root:current train perplexity1.000925064086914
INFO:root:current mean train loss 1.171085903314395
INFO:root:current train perplexity1.000924825668335
INFO:root:current mean train loss 1.1709033582689758
INFO:root:current train perplexity1.0009247064590454
INFO:root:current mean train loss 1.170926095946275
INFO:root:current train perplexity1.0009245872497559
INFO:root:current mean train loss 1.1712878034196867
INFO:root:current train perplexity1.0009245872497559
INFO:root:current mean train loss 1.171423037345477
INFO:root:current train perplexity1.0009245872497559

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.06s/it]
INFO:root:final mean train loss: 1.1716321663488598
INFO:root:final train perplexity: 1.000925064086914
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2.0974568881041615
INFO:root:eval perplexity: 1.0016987323760986
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.6945837127401475
INFO:root:eval perplexity: 1.0022177696228027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [28:46:49<6:46:17, 609.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1662794853511609
INFO:root:current train perplexity1.000905156135559
INFO:root:current mean train loss 1.1684325422559465
INFO:root:current train perplexity1.0009217262268066
INFO:root:current mean train loss 1.1644938944681595
INFO:root:current train perplexity1.0009181499481201
INFO:root:current mean train loss 1.1652309322805614
INFO:root:current train perplexity1.0009201765060425
INFO:root:current mean train loss 1.1664215019039437
INFO:root:current train perplexity1.0009230375289917
INFO:root:current mean train loss 1.1664004863342108
INFO:root:current train perplexity1.000922441482544
INFO:root:current mean train loss 1.1665141326352737
INFO:root:current train perplexity1.0009218454360962
INFO:root:current mean train loss 1.1674872547263728
INFO:root:current train perplexity1.000922679901123
INFO:root:current mean train loss 1.1678264611224407
INFO:root:current train perplexity1.0009225606918335
INFO:root:current mean train loss 1.1679829517308464
INFO:root:current train perplexity1.0009218454360962
INFO:root:current mean train loss 1.1682697407045823
INFO:root:current train perplexity1.0009222030639648
INFO:root:current mean train loss 1.1684568833836069
INFO:root:current train perplexity1.000922441482544
INFO:root:current mean train loss 1.168732567435899
INFO:root:current train perplexity1.000922441482544
INFO:root:current mean train loss 1.169017455991784
INFO:root:current train perplexity1.0009231567382812
INFO:root:current mean train loss 1.169450299348354
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.16957140039503
INFO:root:current train perplexity1.0009232759475708
INFO:root:current mean train loss 1.1696457522676784
INFO:root:current train perplexity1.00092351436615
INFO:root:current mean train loss 1.1701187091625451
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.170075078990972
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1704370423242907
INFO:root:current train perplexity1.0009242296218872

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.64s/it]
INFO:root:final mean train loss: 1.1704291462357215
INFO:root:final train perplexity: 1.0009241104125977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.099476120150681
INFO:root:eval perplexity: 1.0017004013061523
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.6960823282282402
INFO:root:eval perplexity: 1.0022189617156982
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [28:56:58<6:36:02, 609.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.159868233733707
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.1635741661576664
INFO:root:current train perplexity1.0009167194366455
INFO:root:current mean train loss 1.165553905196109
INFO:root:current train perplexity1.0009218454360962
INFO:root:current mean train loss 1.165563820728234
INFO:root:current train perplexity1.0009211301803589
INFO:root:current mean train loss 1.1684620839740159
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1681937012654633
INFO:root:current train perplexity1.000923752784729
INFO:root:current mean train loss 1.1677820735007711
INFO:root:current train perplexity1.000922441482544
INFO:root:current mean train loss 1.1675333605836267
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.1675439578209197
INFO:root:current train perplexity1.0009212493896484
INFO:root:current mean train loss 1.167234214452597
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.1672781234082108
INFO:root:current train perplexity1.0009198188781738
INFO:root:current mean train loss 1.1670743010833229
INFO:root:current train perplexity1.0009204149246216
INFO:root:current mean train loss 1.1671826436295865
INFO:root:current train perplexity1.0009205341339111
INFO:root:current mean train loss 1.1673398735280522
INFO:root:current train perplexity1.0009211301803589
INFO:root:current mean train loss 1.167948890197244
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.1680226577445865
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1685158622731207
INFO:root:current train perplexity1.0009217262268066
INFO:root:current mean train loss 1.169145597084876
INFO:root:current train perplexity1.0009230375289917
INFO:root:current mean train loss 1.1689958363316937
INFO:root:current train perplexity1.0009227991104126
INFO:root:current mean train loss 1.1686579660812686
INFO:root:current train perplexity1.0009220838546753

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.00s/it]
INFO:root:final mean train loss: 1.1690246063353615
INFO:root:final train perplexity: 1.0009230375289917
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2.1004622523666274
INFO:root:eval perplexity: 1.0017011165618896
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2.6976995315957577
INFO:root:eval perplexity: 1.0022202730178833
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [29:07:07<6:25:42, 609.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1694002151489258
INFO:root:current train perplexity1.0009244680404663
INFO:root:current mean train loss 1.1683088416367575
INFO:root:current train perplexity1.0009268522262573
INFO:root:current mean train loss 1.166945572898322
INFO:root:current train perplexity1.00092351436615
INFO:root:current mean train loss 1.165312602904971
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.165040918796531
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1648663829937982
INFO:root:current train perplexity1.000921368598938
INFO:root:current mean train loss 1.1653546284753002
INFO:root:current train perplexity1.0009210109710693
INFO:root:current mean train loss 1.1650790956074182
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.1645327656937092
INFO:root:current train perplexity1.0009201765060425
INFO:root:current mean train loss 1.1649640709755928
INFO:root:current train perplexity1.0009207725524902
INFO:root:current mean train loss 1.1652099348088163
INFO:root:current train perplexity1.000920295715332
INFO:root:current mean train loss 1.1657393206127395
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.1657000481368824
INFO:root:current train perplexity1.0009204149246216
INFO:root:current mean train loss 1.166185041488935
INFO:root:current train perplexity1.0009208917617798
INFO:root:current mean train loss 1.16599884454907
INFO:root:current train perplexity1.0009204149246216
INFO:root:current mean train loss 1.1666086191372647
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1667001197645321
INFO:root:current train perplexity1.0009212493896484
INFO:root:current mean train loss 1.1667144490321977
INFO:root:current train perplexity1.0009212493896484
INFO:root:current mean train loss 1.1671315225728833
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1668014313340858
INFO:root:current train perplexity1.0009208917617798

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.50s/it]
INFO:root:final mean train loss: 1.166988557057133
INFO:root:final train perplexity: 1.000921368598938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2.101501814862515
INFO:root:eval perplexity: 1.001702070236206
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2.6983743610111532
INFO:root:eval perplexity: 1.002220869064331
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [29:17:16<6:15:31, 608.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1686542170388359
INFO:root:current train perplexity1.0009232759475708
INFO:root:current mean train loss 1.1649477832457598
INFO:root:current train perplexity1.000921368598938
INFO:root:current mean train loss 1.1647554847929213
INFO:root:current train perplexity1.0009187459945679
INFO:root:current mean train loss 1.1638286883766586
INFO:root:current train perplexity1.000917673110962
INFO:root:current mean train loss 1.1642503327511726
INFO:root:current train perplexity1.0009191036224365
INFO:root:current mean train loss 1.1646142771369532
INFO:root:current train perplexity1.0009199380874634
INFO:root:current mean train loss 1.164295370009408
INFO:root:current train perplexity1.0009196996688843
INFO:root:current mean train loss 1.164355136666979
INFO:root:current train perplexity1.0009195804595947
INFO:root:current mean train loss 1.1654491143665096
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.1659790557684357
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.1661706337304873
INFO:root:current train perplexity1.0009223222732544
INFO:root:current mean train loss 1.1658948677217857
INFO:root:current train perplexity1.0009214878082275
INFO:root:current mean train loss 1.1654884223862896
INFO:root:current train perplexity1.0009207725524902
INFO:root:current mean train loss 1.1653732909773389
INFO:root:current train perplexity1.0009206533432007
INFO:root:current mean train loss 1.16574879836063
INFO:root:current train perplexity1.0009212493896484
INFO:root:current mean train loss 1.1653341866602565
INFO:root:current train perplexity1.0009204149246216
INFO:root:current mean train loss 1.1654103571783283
INFO:root:current train perplexity1.0009205341339111
INFO:root:current mean train loss 1.1653524414967682
INFO:root:current train perplexity1.000920295715332
INFO:root:current mean train loss 1.1655585192104074
INFO:root:current train perplexity1.0009201765060425
INFO:root:current mean train loss 1.165588519657929
INFO:root:current train perplexity1.0009199380874634

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.67s/it]
INFO:root:final mean train loss: 1.1655804449659493
INFO:root:final train perplexity: 1.000920295715332
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2.1034379994615597
INFO:root:eval perplexity: 1.0017036199569702
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.7009975935550448
INFO:root:eval perplexity: 1.002223014831543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [29:27:24<6:05:11, 608.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1634566564669555
INFO:root:current train perplexity1.000921607017517
INFO:root:current mean train loss 1.1596443997347419
INFO:root:current train perplexity1.0009158849716187
INFO:root:current mean train loss 1.1576215336131717
INFO:root:current train perplexity1.0009124279022217
INFO:root:current mean train loss 1.1603342648932484
INFO:root:current train perplexity1.0009169578552246
INFO:root:current mean train loss 1.158724941512153
INFO:root:current train perplexity1.0009143352508545
INFO:root:current mean train loss 1.1595372847032304
INFO:root:current train perplexity1.0009154081344604
INFO:root:current mean train loss 1.1599761854960235
INFO:root:current train perplexity1.00091552734375
INFO:root:current mean train loss 1.1591316469136763
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.1599076720208703
INFO:root:current train perplexity1.0009143352508545
INFO:root:current mean train loss 1.1603206937194717
INFO:root:current train perplexity1.0009137392044067
INFO:root:current mean train loss 1.1607149252676592
INFO:root:current train perplexity1.0009149312973022
INFO:root:current mean train loss 1.1618480232450876
INFO:root:current train perplexity1.0009161233901978
INFO:root:current mean train loss 1.1626557385912215
INFO:root:current train perplexity1.0009174346923828
INFO:root:current mean train loss 1.1628040426827715
INFO:root:current train perplexity1.0009174346923828
INFO:root:current mean train loss 1.1630789945810212
INFO:root:current train perplexity1.0009177923202515
INFO:root:current mean train loss 1.1636821509308986
INFO:root:current train perplexity1.0009186267852783
INFO:root:current mean train loss 1.1639051168150074
INFO:root:current train perplexity1.0009188652038574
INFO:root:current mean train loss 1.1642112661888964
INFO:root:current train perplexity1.0009187459945679
INFO:root:current mean train loss 1.1644125289871508
INFO:root:current train perplexity1.0009191036224365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it]
INFO:root:final mean train loss: 1.1644139795428385
INFO:root:final train perplexity: 1.0009193420410156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2.1043653200704155
INFO:root:eval perplexity: 1.0017043352127075
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2.7031822077771452
INFO:root:eval perplexity: 1.0022248029708862
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [29:37:32<5:55:02, 608.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.181224912405014
INFO:root:current train perplexity1.000940203666687
INFO:root:current mean train loss 1.1612593141885905
INFO:root:current train perplexity1.0009191036224365
INFO:root:current mean train loss 1.1584384231006397
INFO:root:current train perplexity1.000913381576538
INFO:root:current mean train loss 1.16096871934439
INFO:root:current train perplexity1.0009181499481201
INFO:root:current mean train loss 1.1611206732173958
INFO:root:current train perplexity1.0009193420410156
INFO:root:current mean train loss 1.1610414314837683
INFO:root:current train perplexity1.0009201765060425
INFO:root:current mean train loss 1.1612705544920157
INFO:root:current train perplexity1.000919222831726
INFO:root:current mean train loss 1.1612423556772145
INFO:root:current train perplexity1.0009186267852783
INFO:root:current mean train loss 1.159886141766363
INFO:root:current train perplexity1.0009162425994873
INFO:root:current mean train loss 1.1603355861343114
INFO:root:current train perplexity1.000915765762329
INFO:root:current mean train loss 1.1607109095708308
INFO:root:current train perplexity1.000915765762329
INFO:root:current mean train loss 1.1609610629038534
INFO:root:current train perplexity1.0009156465530396
INFO:root:current mean train loss 1.1607894428148617
INFO:root:current train perplexity1.000915288925171
INFO:root:current mean train loss 1.1614902706051164
INFO:root:current train perplexity1.0009163618087769
INFO:root:current mean train loss 1.1618026197805704
INFO:root:current train perplexity1.0009161233901978
INFO:root:current mean train loss 1.1623029170676749
INFO:root:current train perplexity1.0009173154830933
INFO:root:current mean train loss 1.1622010574971053
INFO:root:current train perplexity1.0009174346923828
INFO:root:current mean train loss 1.1626682533344752
INFO:root:current train perplexity1.000917911529541
INFO:root:current mean train loss 1.1630413450317214
INFO:root:current train perplexity1.0009183883666992
INFO:root:current mean train loss 1.1629250768979056
INFO:root:current train perplexity1.000917911529541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.01s/it]
INFO:root:final mean train loss: 1.1629892345395765
INFO:root:final train perplexity: 1.0009182691574097
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2.1050282953478767
INFO:root:eval perplexity: 1.0017049312591553
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.704229321885616
INFO:root:eval perplexity: 1.0022257566452026
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [29:47:42<5:45:04, 608.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.154294036683582
INFO:root:current train perplexity1.000923991203308
INFO:root:current mean train loss 1.16012878752937
INFO:root:current train perplexity1.0009188652038574
INFO:root:current mean train loss 1.1569301294525285
INFO:root:current train perplexity1.0009146928787231
INFO:root:current mean train loss 1.1581883170523004
INFO:root:current train perplexity1.000916838645935
INFO:root:current mean train loss 1.1590366086031083
INFO:root:current train perplexity1.0009183883666992
INFO:root:current mean train loss 1.1584301946716895
INFO:root:current train perplexity1.0009161233901978
INFO:root:current mean train loss 1.15917942719759
INFO:root:current train perplexity1.0009161233901978
INFO:root:current mean train loss 1.1590754848578104
INFO:root:current train perplexity1.0009160041809082
INFO:root:current mean train loss 1.1586860232463563
INFO:root:current train perplexity1.0009146928787231
INFO:root:current mean train loss 1.1584494808986054
INFO:root:current train perplexity1.000914454460144
INFO:root:current mean train loss 1.1591530335404847
INFO:root:current train perplexity1.0009156465530396
INFO:root:current mean train loss 1.1594533671448
INFO:root:current train perplexity1.0009158849716187
INFO:root:current mean train loss 1.1594007222209184
INFO:root:current train perplexity1.00091552734375
INFO:root:current mean train loss 1.1601080073270718
INFO:root:current train perplexity1.000916600227356
INFO:root:current mean train loss 1.160425804770387
INFO:root:current train perplexity1.000916600227356
INFO:root:current mean train loss 1.1605443670748097
INFO:root:current train perplexity1.0009170770645142
INFO:root:current mean train loss 1.1603266990603964
INFO:root:current train perplexity1.0009170770645142
INFO:root:current mean train loss 1.160455754231326
INFO:root:current train perplexity1.0009167194366455
INFO:root:current mean train loss 1.1610739193795343
INFO:root:current train perplexity1.0009170770645142
INFO:root:current mean train loss 1.160983587167711
INFO:root:current train perplexity1.0009167194366455

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.41s/it]
INFO:root:final mean train loss: 1.1610072393701105
INFO:root:final train perplexity: 1.0009167194366455
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2.1069641062553894
INFO:root:eval perplexity: 1.0017064809799194
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.70546171428464
INFO:root:eval perplexity: 1.002226710319519
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [29:57:54<5:35:26, 609.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1635443003554093
INFO:root:current train perplexity1.0009251832962036
INFO:root:current mean train loss 1.1548630690229111
INFO:root:current train perplexity1.0009150505065918
INFO:root:current mean train loss 1.15858814646216
INFO:root:current train perplexity1.0009182691574097
INFO:root:current mean train loss 1.1581825321242654
INFO:root:current train perplexity1.0009143352508545
INFO:root:current mean train loss 1.1568432817720387
INFO:root:current train perplexity1.000913143157959
INFO:root:current mean train loss 1.156929158145167
INFO:root:current train perplexity1.0009124279022217
INFO:root:current mean train loss 1.156738054976568
INFO:root:current train perplexity1.0009112358093262
INFO:root:current mean train loss 1.157269111655269
INFO:root:current train perplexity1.0009121894836426
INFO:root:current mean train loss 1.157588301524342
INFO:root:current train perplexity1.000912070274353
INFO:root:current mean train loss 1.1584968565623643
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.1586086105749098
INFO:root:current train perplexity1.0009146928787231
INFO:root:current mean train loss 1.1583694059316638
INFO:root:current train perplexity1.000914454460144
INFO:root:current mean train loss 1.1586948404019406
INFO:root:current train perplexity1.0009150505065918
INFO:root:current mean train loss 1.1589164760615258
INFO:root:current train perplexity1.00091552734375
INFO:root:current mean train loss 1.1597039600904198
INFO:root:current train perplexity1.000916600227356
INFO:root:current mean train loss 1.1593641093865483
INFO:root:current train perplexity1.0009160041809082
INFO:root:current mean train loss 1.1594094685728005
INFO:root:current train perplexity1.0009161233901978
INFO:root:current mean train loss 1.1600628609761545
INFO:root:current train perplexity1.0009163618087769
INFO:root:current mean train loss 1.1598304808464093
INFO:root:current train perplexity1.0009156465530396
INFO:root:current mean train loss 1.1597562178246623
INFO:root:current train perplexity1.000915288925171

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.68s/it]
INFO:root:final mean train loss: 1.1599000309431007
INFO:root:final train perplexity: 1.000915765762329
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2.106308288185309
INFO:root:eval perplexity: 1.0017058849334717
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.7054124402661697
INFO:root:eval perplexity: 1.002226710319519
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [30:08:06<5:25:31, 610.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.159251076524908
INFO:root:current train perplexity1.0009194612503052
INFO:root:current mean train loss 1.1563972696181266
INFO:root:current train perplexity1.000910758972168
INFO:root:current mean train loss 1.1535109851874557
INFO:root:current train perplexity1.0009082555770874
INFO:root:current mean train loss 1.1544737013292985
INFO:root:current train perplexity1.0009098052978516
INFO:root:current mean train loss 1.1549700372821683
INFO:root:current train perplexity1.0009125471115112
INFO:root:current mean train loss 1.1549197179777129
INFO:root:current train perplexity1.0009126663208008
INFO:root:current mean train loss 1.1556025257547393
INFO:root:current train perplexity1.0009135007858276
INFO:root:current mean train loss 1.1561705941396043
INFO:root:current train perplexity1.000914454460144
INFO:root:current mean train loss 1.156278115545797
INFO:root:current train perplexity1.0009143352508545
INFO:root:current mean train loss 1.1564251700621
INFO:root:current train perplexity1.0009138584136963
INFO:root:current mean train loss 1.1567861971131999
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1570869598037752
INFO:root:current train perplexity1.0009139776229858
INFO:root:current mean train loss 1.1573668485143744
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1574222429652055
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1574919628933122
INFO:root:current train perplexity1.0009146928787231
INFO:root:current mean train loss 1.158078722823471
INFO:root:current train perplexity1.0009148120880127
INFO:root:current mean train loss 1.1583337853322215
INFO:root:current train perplexity1.0009150505065918
INFO:root:current mean train loss 1.1582990057108409
INFO:root:current train perplexity1.0009145736694336
INFO:root:current mean train loss 1.1587302237186792
INFO:root:current train perplexity1.0009149312973022
INFO:root:current mean train loss 1.158841623156272
INFO:root:current train perplexity1.0009145736694336

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.72s/it]
INFO:root:final mean train loss: 1.158927214398146
INFO:root:final train perplexity: 1.0009150505065918
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.108028044091894
INFO:root:eval perplexity: 1.0017073154449463
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.706258847358379
INFO:root:eval perplexity: 1.0022274255752563
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [30:18:18<5:15:41, 611.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1491298211945429
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.1540338750495467
INFO:root:current train perplexity1.0009177923202515
INFO:root:current mean train loss 1.1533473822123863
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1530592499240753
INFO:root:current train perplexity1.0009126663208008
INFO:root:current mean train loss 1.1547685475167582
INFO:root:current train perplexity1.0009145736694336
INFO:root:current mean train loss 1.15551176708895
INFO:root:current train perplexity1.000914454460144
INFO:root:current mean train loss 1.1554576605558395
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.155773837177247
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1567717972425147
INFO:root:current train perplexity1.0009151697158813
INFO:root:current mean train loss 1.15640284176226
INFO:root:current train perplexity1.0009146928787231
INFO:root:current mean train loss 1.1561689178890258
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1564180945577067
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.1562566147098001
INFO:root:current train perplexity1.0009135007858276
INFO:root:current mean train loss 1.1567666569708388
INFO:root:current train perplexity1.0009138584136963
INFO:root:current mean train loss 1.1563715360572804
INFO:root:current train perplexity1.000913143157959
INFO:root:current mean train loss 1.1567503106199755
INFO:root:current train perplexity1.0009135007858276
INFO:root:current mean train loss 1.1570744212164263
INFO:root:current train perplexity1.0009135007858276
INFO:root:current mean train loss 1.157537781559856
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.157739789567442
INFO:root:current train perplexity1.000914216041565
INFO:root:current mean train loss 1.1578290259378676
INFO:root:current train perplexity1.0009139776229858

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.08s/it]
INFO:root:final mean train loss: 1.157946698123376
INFO:root:final train perplexity: 1.000914216041565
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.95s/it]
INFO:root:eval mean loss: 2.110805199501362
INFO:root:eval perplexity: 1.0017095804214478
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:31<00:00, 31.09s/it]
INFO:root:eval mean loss: 2.7107873977498804
INFO:root:eval perplexity: 1.0022311210632324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [30:28:40<5:07:11, 614.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1623043183530315
INFO:root:current train perplexity1.0009225606918335
INFO:root:current mean train loss 1.1599490724543415
INFO:root:current train perplexity1.0009191036224365
INFO:root:current mean train loss 1.1544825152129863
INFO:root:current train perplexity1.0009115934371948
INFO:root:current mean train loss 1.1556288172469347
INFO:root:current train perplexity1.0009123086929321
INFO:root:current mean train loss 1.1571961619371285
INFO:root:current train perplexity1.000913381576538
INFO:root:current mean train loss 1.1586619611506956
INFO:root:current train perplexity1.0009167194366455
INFO:root:current mean train loss 1.1583909401872854
INFO:root:current train perplexity1.0009158849716187
INFO:root:current mean train loss 1.1582456595997392
INFO:root:current train perplexity1.0009158849716187
INFO:root:current mean train loss 1.157934262326115
INFO:root:current train perplexity1.000915765762329
INFO:root:current mean train loss 1.1575011350026145
INFO:root:current train perplexity1.0009154081344604
INFO:root:current mean train loss 1.1569125655597234
INFO:root:current train perplexity1.0009140968322754
INFO:root:current mean train loss 1.1563849910354294
INFO:root:current train perplexity1.000913381576538
INFO:root:current mean train loss 1.15625711112544
INFO:root:current train perplexity1.0009127855300903
INFO:root:current mean train loss 1.1560477986449043
INFO:root:current train perplexity1.0009124279022217
INFO:root:current mean train loss 1.1561101283540336
INFO:root:current train perplexity1.0009124279022217
INFO:root:current mean train loss 1.1559469366763806
INFO:root:current train perplexity1.0009123086929321
INFO:root:current mean train loss 1.1560927702168025
INFO:root:current train perplexity1.0009123086929321
INFO:root:current mean train loss 1.1562727601785376
INFO:root:current train perplexity1.0009125471115112
INFO:root:current mean train loss 1.1566666400009769
INFO:root:current train perplexity1.0009127855300903

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.93s/it]
INFO:root:final mean train loss: 1.1569550880806287
INFO:root:final train perplexity: 1.0009135007858276
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2.11113658948993
INFO:root:eval perplexity: 1.0017098188400269
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it]
INFO:root:eval mean loss: 2.7107116905510003
INFO:root:eval perplexity: 1.0022310018539429
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [30:39:02<4:57:59, 616.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1725950439771016
INFO:root:current train perplexity1.0009509325027466
INFO:root:current mean train loss 1.1528047368211567
INFO:root:current train perplexity1.0009088516235352
INFO:root:current mean train loss 1.1536650675014384
INFO:root:current train perplexity1.0009119510650635
INFO:root:current mean train loss 1.150590358216778
INFO:root:current train perplexity1.0009064674377441
INFO:root:current mean train loss 1.1503852867140558
INFO:root:current train perplexity1.0009044408798218
INFO:root:current mean train loss 1.1499298438724321
INFO:root:current train perplexity1.0009043216705322
INFO:root:current mean train loss 1.1493492860211791
INFO:root:current train perplexity1.0009042024612427
INFO:root:current mean train loss 1.1509135831516795
INFO:root:current train perplexity1.0009063482284546
INFO:root:current mean train loss 1.1513613036489367
INFO:root:current train perplexity1.000907063484192
INFO:root:current mean train loss 1.1520498216020614
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.1524354701724726
INFO:root:current train perplexity1.0009089708328247
INFO:root:current mean train loss 1.153138367974521
INFO:root:current train perplexity1.0009101629257202
INFO:root:current mean train loss 1.1531474383711617
INFO:root:current train perplexity1.0009099245071411
INFO:root:current mean train loss 1.1532855443713494
INFO:root:current train perplexity1.0009098052978516
INFO:root:current mean train loss 1.1533564146009312
INFO:root:current train perplexity1.000909686088562
INFO:root:current mean train loss 1.1534805296268438
INFO:root:current train perplexity1.0009095668792725
INFO:root:current mean train loss 1.1537569255045015
INFO:root:current train perplexity1.0009099245071411
INFO:root:current mean train loss 1.154335478126374
INFO:root:current train perplexity1.000910997390747
INFO:root:current mean train loss 1.1545299598544936
INFO:root:current train perplexity1.0009112358093262
INFO:root:current mean train loss 1.15471473620796
INFO:root:current train perplexity1.0009111166000366

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.30s/it]
INFO:root:final mean train loss: 1.1552843727731248
INFO:root:final train perplexity: 1.0009121894836426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2.111603154358289
INFO:root:eval perplexity: 1.0017101764678955
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 2.711204432426615
INFO:root:eval perplexity: 1.002231478691101
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [30:49:14<4:47:04, 615.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1567425986994868
INFO:root:current train perplexity1.0009201765060425
INFO:root:current mean train loss 1.1497094069069964
INFO:root:current train perplexity1.000905990600586
INFO:root:current mean train loss 1.1500109386016435
INFO:root:current train perplexity1.000903844833374
INFO:root:current mean train loss 1.149392433579861
INFO:root:current train perplexity1.0009046792984009
INFO:root:current mean train loss 1.150245102302966
INFO:root:current train perplexity1.0009065866470337
INFO:root:current mean train loss 1.1507518232210645
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.151099039310437
INFO:root:current train perplexity1.0009076595306396
INFO:root:current mean train loss 1.1520457350201956
INFO:root:current train perplexity1.0009084939956665
INFO:root:current mean train loss 1.153025730267475
INFO:root:current train perplexity1.000909686088562
INFO:root:current mean train loss 1.1528162845165215
INFO:root:current train perplexity1.0009089708328247
INFO:root:current mean train loss 1.1531719478577347
INFO:root:current train perplexity1.0009093284606934
INFO:root:current mean train loss 1.1534070784984056
INFO:root:current train perplexity1.000909447669983
INFO:root:current mean train loss 1.1531922992681345
INFO:root:current train perplexity1.0009093284606934
INFO:root:current mean train loss 1.153231545911084
INFO:root:current train perplexity1.0009090900421143
INFO:root:current mean train loss 1.1537337074460736
INFO:root:current train perplexity1.0009098052978516
INFO:root:current mean train loss 1.15366987554321
INFO:root:current train perplexity1.000909686088562
INFO:root:current mean train loss 1.153818116978371
INFO:root:current train perplexity1.0009098052978516
INFO:root:current mean train loss 1.153710954699901
INFO:root:current train perplexity1.0009099245071411
INFO:root:current mean train loss 1.1539483659553842
INFO:root:current train perplexity1.0009106397628784
INFO:root:current mean train loss 1.1539284098724865
INFO:root:current train perplexity1.000910758972168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.56s/it]
INFO:root:final mean train loss: 1.1543074022082445
INFO:root:final train perplexity: 1.0009113550186157
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.113396219750668
INFO:root:eval perplexity: 1.0017116069793701
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.713051343640537
INFO:root:eval perplexity: 1.0022329092025757
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [30:59:27<4:36:33, 614.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1459541350603104
INFO:root:current train perplexity1.0008965730667114
INFO:root:current mean train loss 1.1562007359095983
INFO:root:current train perplexity1.000915765762329
INFO:root:current mean train loss 1.1534091020623842
INFO:root:current train perplexity1.0009125471115112
INFO:root:current mean train loss 1.1521781560252695
INFO:root:current train perplexity1.0009119510650635
INFO:root:current mean train loss 1.150951859625903
INFO:root:current train perplexity1.000909447669983
INFO:root:current mean train loss 1.152761110773793
INFO:root:current train perplexity1.000910758972168
INFO:root:current mean train loss 1.1525860164314508
INFO:root:current train perplexity1.0009112358093262
INFO:root:current mean train loss 1.1524438170162408
INFO:root:current train perplexity1.0009108781814575
INFO:root:current mean train loss 1.1532518335751125
INFO:root:current train perplexity1.0009117126464844
INFO:root:current mean train loss 1.1535217573034002
INFO:root:current train perplexity1.0009115934371948
INFO:root:current mean train loss 1.1534796020159355
INFO:root:current train perplexity1.0009117126464844
INFO:root:current mean train loss 1.1539351207122468
INFO:root:current train perplexity1.0009113550186157
INFO:root:current mean train loss 1.1538969481183636
INFO:root:current train perplexity1.0009111166000366
INFO:root:current mean train loss 1.1537167015360363
INFO:root:current train perplexity1.0009111166000366
INFO:root:current mean train loss 1.1534580977426634
INFO:root:current train perplexity1.0009108781814575
INFO:root:current mean train loss 1.1535469356295351
INFO:root:current train perplexity1.0009108781814575
INFO:root:current mean train loss 1.153188313725518
INFO:root:current train perplexity1.0009104013442993
INFO:root:current mean train loss 1.1533975289470848
INFO:root:current train perplexity1.0009105205535889
INFO:root:current mean train loss 1.1532192658471025
INFO:root:current train perplexity1.0009101629257202
INFO:root:current mean train loss 1.1533731325385497
INFO:root:current train perplexity1.0009102821350098

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.60s/it]
INFO:root:final mean train loss: 1.1533026224546101
INFO:root:final train perplexity: 1.0009106397628784
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.1133186550850565
INFO:root:eval perplexity: 1.0017116069793701
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.713744112785826
INFO:root:eval perplexity: 1.0022335052490234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [31:09:38<4:25:52, 613.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1527121443497508
INFO:root:current train perplexity1.0009064674377441
INFO:root:current mean train loss 1.1471726598253675
INFO:root:current train perplexity1.0009071826934814
INFO:root:current mean train loss 1.1470689708620658
INFO:root:current train perplexity1.000907301902771
INFO:root:current mean train loss 1.1478412288243698
INFO:root:current train perplexity1.0009065866470337
INFO:root:current mean train loss 1.1484758958253087
INFO:root:current train perplexity1.0009074211120605
INFO:root:current mean train loss 1.1481056613596807
INFO:root:current train perplexity1.0009074211120605
INFO:root:current mean train loss 1.1478251327662707
INFO:root:current train perplexity1.0009068250656128
INFO:root:current mean train loss 1.1485005322661708
INFO:root:current train perplexity1.000907063484192
INFO:root:current mean train loss 1.1485842516430618
INFO:root:current train perplexity1.0009064674377441
INFO:root:current mean train loss 1.1496863702746145
INFO:root:current train perplexity1.000907301902771
INFO:root:current mean train loss 1.1497686036713528
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.149618505607315
INFO:root:current train perplexity1.000907301902771
INFO:root:current mean train loss 1.1500607526956708
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.1501756544576995
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.1509770435589834
INFO:root:current train perplexity1.0009080171585083
INFO:root:current mean train loss 1.1508980361635899
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.1517134534355813
INFO:root:current train perplexity1.0009088516235352
INFO:root:current mean train loss 1.1517211077358482
INFO:root:current train perplexity1.0009087324142456
INFO:root:current mean train loss 1.15211220762071
INFO:root:current train perplexity1.000909447669983
INFO:root:current mean train loss 1.1525476131746217
INFO:root:current train perplexity1.0009098052978516

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.40s/it]
INFO:root:final mean train loss: 1.1525745460978676
INFO:root:final train perplexity: 1.0009100437164307
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2.1150469927923052
INFO:root:eval perplexity: 1.0017130374908447
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.83s/it]
INFO:root:eval mean loss: 2.7165143980202098
INFO:root:eval perplexity: 1.002235770225525
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [31:19:51<4:15:35, 613.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1458564352344822
INFO:root:current train perplexity1.0009058713912964
INFO:root:current mean train loss 1.145414772389949
INFO:root:current train perplexity1.0009040832519531
INFO:root:current mean train loss 1.1464323040342679
INFO:root:current train perplexity1.0009018182754517
INFO:root:current mean train loss 1.14847105996494
INFO:root:current train perplexity1.0009067058563232
INFO:root:current mean train loss 1.1486829290410134
INFO:root:current train perplexity1.0009078979492188
INFO:root:current mean train loss 1.148350911273358
INFO:root:current train perplexity1.0009078979492188
INFO:root:current mean train loss 1.1489806376861889
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.148586801010201
INFO:root:current train perplexity1.000907063484192
INFO:root:current mean train loss 1.149608142043142
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.1498695980352054
INFO:root:current train perplexity1.0009076595306396
INFO:root:current mean train loss 1.1503983351993383
INFO:root:current train perplexity1.0009082555770874
INFO:root:current mean train loss 1.1503808707557301
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.150728551053926
INFO:root:current train perplexity1.0009089708328247
INFO:root:current mean train loss 1.1506958547464452
INFO:root:current train perplexity1.0009087324142456
INFO:root:current mean train loss 1.1505133231427
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.1507225869722864
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.1507197209014117
INFO:root:current train perplexity1.0009078979492188
INFO:root:current mean train loss 1.1507118890626857
INFO:root:current train perplexity1.0009080171585083
INFO:root:current mean train loss 1.1507412815424778
INFO:root:current train perplexity1.0009081363677979
INFO:root:current mean train loss 1.1509738903881448
INFO:root:current train perplexity1.000908374786377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.82s/it]
INFO:root:final mean train loss: 1.150923460346247
INFO:root:final train perplexity: 1.0009087324142456
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.1171169534642647
INFO:root:eval perplexity: 1.0017147064208984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 2.718196562841429
INFO:root:eval perplexity: 1.0022372007369995
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [31:30:04<4:05:15, 613.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1486684267337506
INFO:root:current train perplexity1.0009112358093262
INFO:root:current mean train loss 1.1444270273777828
INFO:root:current train perplexity1.0009031295776367
INFO:root:current mean train loss 1.145372538632134
INFO:root:current train perplexity1.0009032487869263
INFO:root:current mean train loss 1.1454300124322057
INFO:root:current train perplexity1.0009043216705322
INFO:root:current mean train loss 1.1454029928156897
INFO:root:current train perplexity1.0009043216705322
INFO:root:current mean train loss 1.146867443260606
INFO:root:current train perplexity1.0009071826934814
INFO:root:current mean train loss 1.1463838443052234
INFO:root:current train perplexity1.000905156135559
INFO:root:current mean train loss 1.1469016867853448
INFO:root:current train perplexity1.0009061098098755
INFO:root:current mean train loss 1.147227671545095
INFO:root:current train perplexity1.0009064674377441
INFO:root:current mean train loss 1.1477644365322217
INFO:root:current train perplexity1.000907301902771
INFO:root:current mean train loss 1.1482539986602545
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.149224566272284
INFO:root:current train perplexity1.0009088516235352
INFO:root:current mean train loss 1.1492793228532066
INFO:root:current train perplexity1.0009089708328247
INFO:root:current mean train loss 1.149012602635376
INFO:root:current train perplexity1.0009080171585083
INFO:root:current mean train loss 1.1491514669817458
INFO:root:current train perplexity1.0009082555770874
INFO:root:current mean train loss 1.1492544008005647
INFO:root:current train perplexity1.0009081363677979
INFO:root:current mean train loss 1.1494641373102514
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.14991191523758
INFO:root:current train perplexity1.000908613204956
INFO:root:current mean train loss 1.1500579516513205
INFO:root:current train perplexity1.000908613204956

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.35s/it]
INFO:root:final mean train loss: 1.1496405774033316
INFO:root:final train perplexity: 1.0009076595306396
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2.116509569874892
INFO:root:eval perplexity: 1.0017142295837402
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it]
INFO:root:eval mean loss: 2.7180351678361285
INFO:root:eval perplexity: 1.00223708152771
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [31:40:16<3:54:55, 612.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1413021385669708
INFO:root:current train perplexity1.0009182691574097
INFO:root:current mean train loss 1.1442002411241885
INFO:root:current train perplexity1.000903606414795
INFO:root:current mean train loss 1.1467674947701967
INFO:root:current train perplexity1.0009074211120605
INFO:root:current mean train loss 1.1459618922951933
INFO:root:current train perplexity1.0009052753448486
INFO:root:current mean train loss 1.1467940439196194
INFO:root:current train perplexity1.0009056329727173
INFO:root:current mean train loss 1.147793689112025
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.1480254842654656
INFO:root:current train perplexity1.000908374786377
INFO:root:current mean train loss 1.1489742135597487
INFO:root:current train perplexity1.0009087324142456
INFO:root:current mean train loss 1.1487151472875388
INFO:root:current train perplexity1.0009088516235352
INFO:root:current mean train loss 1.1499002327740455
INFO:root:current train perplexity1.0009099245071411
INFO:root:current mean train loss 1.150129687928018
INFO:root:current train perplexity1.000909686088562
INFO:root:current mean train loss 1.1490398771280848
INFO:root:current train perplexity1.0009081363677979
INFO:root:current mean train loss 1.1487590213682477
INFO:root:current train perplexity1.0009071826934814
INFO:root:current mean train loss 1.1489669669658766
INFO:root:current train perplexity1.0009071826934814
INFO:root:current mean train loss 1.1489949385550888
INFO:root:current train perplexity1.000907063484192
INFO:root:current mean train loss 1.149080577121173
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.149160176886255
INFO:root:current train perplexity1.0009077787399292
INFO:root:current mean train loss 1.1493903318947876
INFO:root:current train perplexity1.00090754032135
INFO:root:current mean train loss 1.1497844807078352
INFO:root:current train perplexity1.0009080171585083
INFO:root:current mean train loss 1.1496663754591152
INFO:root:current train perplexity1.00090754032135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.37s/it]
INFO:root:final mean train loss: 1.1496815213156781
INFO:root:final train perplexity: 1.0009077787399292
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2.11693811205262
INFO:root:eval perplexity: 1.0017144680023193
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it]
INFO:root:eval mean loss: 2.718626763803739
INFO:root:eval perplexity: 1.0022375583648682
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [31:50:29<3:44:43, 612.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1.1513773727416992
INFO:root:current train perplexity1.0009227991104126
INFO:root:current mean train loss 1.1410120944976807
INFO:root:current train perplexity1.0009021759033203
INFO:root:current mean train loss 1.1430979140599569
INFO:root:current train perplexity1.0009028911590576
INFO:root:current mean train loss 1.1435664325494033
INFO:root:current train perplexity1.0009021759033203
INFO:root:current mean train loss 1.145990188963273
INFO:root:current train perplexity1.0009044408798218
INFO:root:current mean train loss 1.1472992489451455
INFO:root:current train perplexity1.0009061098098755
INFO:root:current mean train loss 1.1476625054359435
INFO:root:current train perplexity1.0009068250656128
INFO:root:current mean train loss 1.1477207842366448
INFO:root:current train perplexity1.0009068250656128
INFO:root:current mean train loss 1.147390285766486
INFO:root:current train perplexity1.0009063482284546
INFO:root:current mean train loss 1.1473117602193679
INFO:root:current train perplexity1.0009061098098755
INFO:root:current mean train loss 1.147146456590513
INFO:root:current train perplexity1.0009055137634277
INFO:root:current mean train loss 1.1471248760753208
INFO:root:current train perplexity1.0009050369262695
INFO:root:current mean train loss 1.147226357411365
INFO:root:current train perplexity1.0009053945541382
INFO:root:current mean train loss 1.147693726386664
INFO:root:current train perplexity1.0009063482284546
INFO:root:current mean train loss 1.1481154945858738
INFO:root:current train perplexity1.0009065866470337
INFO:root:current mean train loss 1.1481260633859478
INFO:root:current train perplexity1.0009063482284546
INFO:root:current mean train loss 1.148246059014247
INFO:root:current train perplexity1.0009064674377441
INFO:root:current mean train loss 1.1486192995223445
INFO:root:current train perplexity1.000907301902771
INFO:root:current mean train loss 1.1485628772435124
INFO:root:current train perplexity1.0009069442749023
INFO:root:current mean train loss 1.1482551673480443
INFO:root:current train perplexity1.0009063482284546

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.91s/it]
INFO:root:final mean train loss: 1.1482796238277875
INFO:root:final train perplexity: 1.0009065866470337
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 30150396 ON gr010 CANCELLED AT 2023-02-13T23:46:42 ***
