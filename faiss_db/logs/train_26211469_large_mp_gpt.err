INFO:root:Output: large_mp_gpt2
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Using pad_token, but it is not set yet.
Some weights of RetrievalGenerationModelGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn_v.weight', 'h.3.crossattention.q_attn.weight', 'h.6.crossattention.masked_bias', 'h.4.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.5.ln_cross_attn.weight', 'h.3.crossattention.bias', 'h.10.crossattention.c_attn_v.bias', 'h.4.crossattention.c_attn_v.bias', 'h.7.crossattention.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.11.crossattention.c_attn_v.bias', 'h.1.crossattention.masked_bias', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.weight', 'h.7.crossattention.masked_bias', 'h.5.crossattention.c_attn_v.bias', 'h.1.crossattention.bias', 'h.4.crossattention.c_attn_v.weight', 'h.2.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.9.ln_cross_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.2.crossattention.bias', 'h.9.crossattention.masked_bias', 'h.11.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_attn_v.weight', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.8.ln_cross_attn.weight', 'h.1.crossattention.c_attn_v.weight', 'h.1.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.7.crossattention.c_attn_v.bias', 'h.2.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.bias', 'h.1.crossattention.c_proj.weight', 'h.8.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.6.ln_cross_attn.weight', 'h.4.crossattention.bias', 'h.10.crossattention.masked_bias', 'h.5.crossattention.c_attn_v.weight', 'h.1.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.q_attn.weight', 'h.7.crossattention.c_attn_v.weight', 'h.0.crossattention.c_attn_v.bias', 'h.9.crossattention.c_attn_v.bias', 'h.2.crossattention.c_attn_v.bias', 'h.5.crossattention.masked_bias', 'h.8.crossattention.c_attn_v.bias', 'h.7.ln_cross_attn.weight', 'h.5.crossattention.bias', 'h.7.crossattention.bias', 'h.1.crossattention.c_attn.weight', 'h.9.crossattention.c_attn_v.weight', 'h.1.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.6.crossattention.bias', 'h.0.crossattention.bias', 'h.7.crossattention.c_proj.weight', 'h.2.crossattention.c_attn_v.weight', 'h.9.crossattention.bias', 'h.10.crossattention.bias', 'h.3.crossattention.c_attn_v.bias', 'h.6.crossattention.c_proj.weight', 'h.3.crossattention.masked_bias', 'h.9.crossattention.c_proj.weight', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_attn_v.weight', 'h.3.crossattention.c_proj.weight', 'h.10.crossattention.c_attn_v.weight', 'h.1.crossattention.c_attn_v.bias', 'h.6.crossattention.c_attn_v.bias', 'h.5.crossattention.c_proj.weight', 'h.8.crossattention.bias', 'h.0.crossattention.c_proj.weight', 'h.3.crossattention.c_attn_v.weight', 'h.8.crossattention.c_attn_v.weight', 'h.0.crossattention.masked_bias', 'h.0.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.10.crossattention.c_attn.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A  0%|          | 0/1 [02:14<?, ?it/s]
  0%|          | 0/200 [02:14<?, ?it/s]
Traceback (most recent call last):
  File "train_script.py", line 621, in <module>
    handler.train()
  File "train_script.py", line 99, in train
    outputs = self.args.model(**batch.to(self.args.device))
  File "/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zw2374/public/faiss_db/wrappers_gpt2.py", line 86, in forward
    transformer_outputs = self.transformer(
  File "/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zw2374/public/faiss_db/models_gpt2.py", line 359, in forward
    outputs = block(
  File "/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zw2374/public/faiss_db/models_gpt2.py", line 164, in forward
    cross_attn_outputs = self.crossattention(
  File "/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/zw2374/public/faiss_db/models_gpt2.py", line 106, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 205, in _attn
    attn_weights = attn_weights + attention_mask
RuntimeError: The size of tensor a (336) must match the size of tensor b (380) at non-singleton dimension 3
