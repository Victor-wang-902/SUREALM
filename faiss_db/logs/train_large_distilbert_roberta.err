INFO:root:Output: large_distilbert_roberta_64_low
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11362.85198666351
INFO:root:current train perplexity7831.8486328125
INFO:root:current mean train loss 9725.110994425251
INFO:root:current train perplexity2168.767578125
INFO:root:current mean train loss 8963.448821919419
INFO:root:current train perplexity1193.251708984375
INFO:root:current mean train loss 8532.011558437109
INFO:root:current train perplexity841.3985595703125
INFO:root:current mean train loss 8180.300380056989
INFO:root:current train perplexity644.8637084960938
INFO:root:current mean train loss 7921.37414245096
INFO:root:current train perplexity521.2393798828125
INFO:root:current mean train loss 7697.753758158977
INFO:root:current train perplexity436.286865234375
INFO:root:current mean train loss 7492.058364581703
INFO:root:current train perplexity371.4097595214844
INFO:root:current mean train loss 7304.964524384733
INFO:root:current train perplexity321.2594909667969
INFO:root:current mean train loss 7141.271868548236
INFO:root:current train perplexity280.42071533203125
INFO:root:current mean train loss 6971.4011828046805
INFO:root:current train perplexity245.76820373535156
INFO:root:current mean train loss 6810.661320957569
INFO:root:current train perplexity216.10231018066406
INFO:root:current mean train loss 6659.306991518416
INFO:root:current train perplexity191.5246124267578
INFO:root:current mean train loss 6511.660202320854
INFO:root:current train perplexity170.49227905273438
INFO:root:current mean train loss 6377.270030443473
INFO:root:current train perplexity153.08071899414062
INFO:root:current mean train loss 6250.6249981678
INFO:root:current train perplexity138.4635772705078
INFO:root:current mean train loss 6131.828777239139
INFO:root:current train perplexity125.9384765625
INFO:root:current mean train loss 6018.75647820838
INFO:root:current train perplexity115.22187042236328
INFO:root:current mean train loss 5913.925145764424
INFO:root:current train perplexity106.13116455078125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.06s/it]
INFO:root:eval mean loss: 4044.22509618994
INFO:root:eval perplexity: 28.105804443359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/1

  0%|          | 1/200 [08:42<28:51:49, 522.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3975.5587310791016
INFO:root:current train perplexity22.184589385986328
INFO:root:current mean train loss 3861.9292265793374
INFO:root:current train perplexity20.733943939208984
INFO:root:current mean train loss 3824.437025282118
INFO:root:current train perplexity20.141130447387695
INFO:root:current mean train loss 3780.448530366149
INFO:root:current train perplexity19.473020553588867
INFO:root:current mean train loss 3747.932421170748
INFO:root:current train perplexity18.973852157592773
INFO:root:current mean train loss 3711.8365663040518
INFO:root:current train perplexity18.502586364746094
INFO:root:current mean train loss 3683.315083689504
INFO:root:current train perplexity18.096349716186523
INFO:root:current mean train loss 3649.0670022804643
INFO:root:current train perplexity17.693981170654297
INFO:root:current mean train loss 3616.2174218869677
INFO:root:current train perplexity17.304555892944336
INFO:root:current mean train loss 3590.080446201641
INFO:root:current train perplexity16.92908477783203
INFO:root:current mean train loss 3563.5153671625094
INFO:root:current train perplexity16.58366584777832
INFO:root:current mean train loss 3538.5367565086667
INFO:root:current train perplexity16.275527954101562
INFO:root:current mean train loss 3514.201522024054
INFO:root:current train perplexity15.973357200622559
INFO:root:current mean train loss 3492.928471434805
INFO:root:current train perplexity15.698004722595215
INFO:root:current mean train loss 3471.2836050259866
INFO:root:current train perplexity15.446698188781738
INFO:root:current mean train loss 3450.1921550982233
INFO:root:current train perplexity15.201278686523438
INFO:root:current mean train loss 3428.884018850799
INFO:root:current train perplexity14.97872257232666
INFO:root:current mean train loss 3411.917281713241
INFO:root:current train perplexity14.7778902053833
INFO:root:current mean train loss 3394.880167688042
INFO:root:current train perplexity14.571792602539062
INFO:root:current mean train loss 3378.0749086129144
INFO:root:current train perplexity14.378116607666016


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:13<00:00, 493.18s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.91s/it]
INFO:root:eval mean loss: 3411.930866413288
INFO:root:eval perplexity: 16.68338966369629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/2

  1%|          | 2/200 [17:48<29:30:24, 536.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3055.2551047585225
INFO:root:current train perplexity10.83337116241455
INFO:root:current mean train loss 3017.35009765625
INFO:root:current train perplexity10.709844589233398
INFO:root:current mean train loss 2994.1194738298013
INFO:root:current train perplexity10.574560165405273
INFO:root:current mean train loss 2993.336741037913
INFO:root:current train perplexity10.57469654083252
INFO:root:current mean train loss 2990.599139136475
INFO:root:current train perplexity10.553627014160156
INFO:root:current mean train loss 2978.922977526237
INFO:root:current train perplexity10.458105087280273
INFO:root:current mean train loss 2966.334224273055
INFO:root:current train perplexity10.3617582321167
INFO:root:current mean train loss 2955.977047450546
INFO:root:current train perplexity10.287901878356934
INFO:root:current mean train loss 2947.468396831389
INFO:root:current train perplexity10.221880912780762
INFO:root:current mean train loss 2937.7340575910202
INFO:root:current train perplexity10.143656730651855
INFO:root:current mean train loss 2928.40874694647
INFO:root:current train perplexity10.069217681884766
INFO:root:current mean train loss 2919.409542773955
INFO:root:current train perplexity9.999566078186035
INFO:root:current mean train loss 2912.420659049004
INFO:root:current train perplexity9.941134452819824
INFO:root:current mean train loss 2905.2125627842506
INFO:root:current train perplexity9.886553764343262
INFO:root:current mean train loss 2897.680842951304
INFO:root:current train perplexity9.837308883666992
INFO:root:current mean train loss 2891.502350948406
INFO:root:current train perplexity9.78288459777832
INFO:root:current mean train loss 2884.2327273124138
INFO:root:current train perplexity9.726523399353027
INFO:root:current mean train loss 2875.7798434795154
INFO:root:current train perplexity9.665550231933594
INFO:root:current mean train loss 2868.873708838397
INFO:root:current train perplexity9.615998268127441
INFO:root:current mean train loss 2862.049108008924
INFO:root:current train perplexity9.56743049621582


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:16<00:00, 496.37s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.26s/it]
INFO:root:eval mean loss: 3203.7956894883164
INFO:root:eval perplexity: 14.051485061645508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/3

  2%|â–         | 3/200 [26:53<29:33:15, 540.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2742.303515625
INFO:root:current train perplexity8.51408576965332
INFO:root:current mean train loss 2715.351529947917
INFO:root:current train perplexity8.454312324523926
INFO:root:current mean train loss 2717.8326484375
INFO:root:current train perplexity8.49027156829834
INFO:root:current mean train loss 2707.8054185267856
INFO:root:current train perplexity8.429658889770508
INFO:root:current mean train loss 2707.2625103081596
INFO:root:current train perplexity8.417466163635254
INFO:root:current mean train loss 2698.5143417080967
INFO:root:current train perplexity8.380825996398926
INFO:root:current mean train loss 2692.4371559495194
INFO:root:current train perplexity8.349478721618652
INFO:root:current mean train loss 2684.663923502604
INFO:root:current train perplexity8.315437316894531
INFO:root:current mean train loss 2681.3031907743566
INFO:root:current train perplexity8.29230785369873
INFO:root:current mean train loss 2679.090333830181
INFO:root:current train perplexity8.266465187072754
INFO:root:current mean train loss 2675.250154157366
INFO:root:current train perplexity8.242423057556152
INFO:root:current mean train loss 2669.7419257387905
INFO:root:current train perplexity8.208889961242676
INFO:root:current mean train loss 2665.224279296875
INFO:root:current train perplexity8.171320915222168
INFO:root:current mean train loss 2662.7822433810766
INFO:root:current train perplexity8.148770332336426
INFO:root:current mean train loss 2657.5150582570045
INFO:root:current train perplexity8.126555442810059
INFO:root:current mean train loss 2655.2504988344253
INFO:root:current train perplexity8.110106468200684
INFO:root:current mean train loss 2651.454315222538
INFO:root:current train perplexity8.09063720703125
INFO:root:current mean train loss 2646.824576171875
INFO:root:current train perplexity8.063589096069336
INFO:root:current mean train loss 2642.7221584010767
INFO:root:current train perplexity8.042171478271484
INFO:root:current mean train loss 2639.1291800005006
INFO:root:current train perplexity8.019798278808594


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.35s/it]
INFO:root:eval mean loss: 3090.1720077010605
INFO:root:eval perplexity: 12.794337272644043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/4

  2%|â–         | 4/200 [35:52<29:23:38, 539.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2563.0175052472014
INFO:root:current train perplexity7.538499355316162
INFO:root:current mean train loss 2554.569467510292
INFO:root:current train perplexity7.491649150848389
INFO:root:current mean train loss 2545.0663852191597
INFO:root:current train perplexity7.422409534454346
INFO:root:current mean train loss 2539.4357451145265
INFO:root:current train perplexity7.418842792510986
INFO:root:current mean train loss 2546.348065590705
INFO:root:current train perplexity7.431987762451172
INFO:root:current mean train loss 2543.9252494798557
INFO:root:current train perplexity7.404327869415283
INFO:root:current mean train loss 2535.6594157755108
INFO:root:current train perplexity7.385778427124023
INFO:root:current mean train loss 2532.065781733825
INFO:root:current train perplexity7.3654704093933105
INFO:root:current mean train loss 2527.418152629848
INFO:root:current train perplexity7.344343662261963
INFO:root:current mean train loss 2525.7290834349956
INFO:root:current train perplexity7.334962368011475
INFO:root:current mean train loss 2523.7396414130226
INFO:root:current train perplexity7.327191352844238
INFO:root:current mean train loss 2518.319377882826
INFO:root:current train perplexity7.314128875732422
INFO:root:current mean train loss 2514.7838163669408
INFO:root:current train perplexity7.299829959869385
INFO:root:current mean train loss 2512.251483953514
INFO:root:current train perplexity7.282351493835449
INFO:root:current mean train loss 2510.75539173004
INFO:root:current train perplexity7.274859428405762
INFO:root:current mean train loss 2509.5741508206365
INFO:root:current train perplexity7.259377479553223
INFO:root:current mean train loss 2507.5019107262533
INFO:root:current train perplexity7.2453460693359375
INFO:root:current mean train loss 2508.4597431176385
INFO:root:current train perplexity7.240335464477539
INFO:root:current mean train loss 2506.750856061383
INFO:root:current train perplexity7.227262496948242
INFO:root:current mean train loss 2504.1841506065907
INFO:root:current train perplexity7.213912010192871


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.12s/it]
INFO:root:eval mean loss: 3017.4224945160004
INFO:root:eval perplexity: 12.049141883850098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/5

  2%|â–Ž         | 5/200 [44:49<29:11:25, 538.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2399.753718784877
INFO:root:current train perplexity6.773796558380127
INFO:root:current mean train loss 2423.2901385763416
INFO:root:current train perplexity6.798364162445068
INFO:root:current mean train loss 2425.6866863411915
INFO:root:current train perplexity6.8010334968566895
INFO:root:current mean train loss 2430.357636451721
INFO:root:current train perplexity6.817309856414795
INFO:root:current mean train loss 2431.9296004870707
INFO:root:current train perplexity6.816047668457031
INFO:root:current mean train loss 2427.4410578061456
INFO:root:current train perplexity6.79443359375
INFO:root:current mean train loss 2426.7775513052243
INFO:root:current train perplexity6.7783098220825195
INFO:root:current mean train loss 2424.955698908592
INFO:root:current train perplexity6.775784015655518
INFO:root:current mean train loss 2422.3629864308627
INFO:root:current train perplexity6.7696003913879395
INFO:root:current mean train loss 2419.946258420867
INFO:root:current train perplexity6.755954265594482
INFO:root:current mean train loss 2418.9330669557917
INFO:root:current train perplexity6.747750759124756
INFO:root:current mean train loss 2417.952052142169
INFO:root:current train perplexity6.737766265869141
INFO:root:current mean train loss 2416.274937900056
INFO:root:current train perplexity6.735854625701904
INFO:root:current mean train loss 2417.0785735907584
INFO:root:current train perplexity6.730994701385498
INFO:root:current mean train loss 2414.7295935288917
INFO:root:current train perplexity6.719609260559082
INFO:root:current mean train loss 2412.9524041378136
INFO:root:current train perplexity6.7110114097595215
INFO:root:current mean train loss 2412.1376260136767
INFO:root:current train perplexity6.708005905151367
INFO:root:current mean train loss 2410.7847663639905
INFO:root:current train perplexity6.702362060546875
INFO:root:current mean train loss 2409.6343127856085
INFO:root:current train perplexity6.695347309112549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.45s/it]
INFO:root:eval mean loss: 2963.8053532047675
INFO:root:eval perplexity: 11.527852058410645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/6

  3%|â–Ž         | 6/200 [53:48<29:02:28, 538.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2366.91748046875
INFO:root:current train perplexity6.7658514976501465
INFO:root:current mean train loss 2349.058651763614
INFO:root:current train perplexity6.380522727966309
INFO:root:current mean train loss 2355.2199445885803
INFO:root:current train perplexity6.395683765411377
INFO:root:current mean train loss 2349.9682974070806
INFO:root:current train perplexity6.4032745361328125
INFO:root:current mean train loss 2352.3917464639185
INFO:root:current train perplexity6.403598785400391
INFO:root:current mean train loss 2347.880596960376
INFO:root:current train perplexity6.388813495635986
INFO:root:current mean train loss 2346.0706632744254
INFO:root:current train perplexity6.38572359085083
INFO:root:current mean train loss 2343.009826921362
INFO:root:current train perplexity6.3738694190979
INFO:root:current mean train loss 2338.893019467853
INFO:root:current train perplexity6.36553430557251
INFO:root:current mean train loss 2340.7317779236178
INFO:root:current train perplexity6.369263172149658
INFO:root:current mean train loss 2340.6492058966423
INFO:root:current train perplexity6.364802837371826
INFO:root:current mean train loss 2341.3243167610412
INFO:root:current train perplexity6.359011650085449
INFO:root:current mean train loss 2340.357301735858
INFO:root:current train perplexity6.350662708282471
INFO:root:current mean train loss 2341.2316732208697
INFO:root:current train perplexity6.353668689727783
INFO:root:current mean train loss 2342.031548423141
INFO:root:current train perplexity6.351937770843506
INFO:root:current mean train loss 2343.155153481663
INFO:root:current train perplexity6.353897571563721
INFO:root:current mean train loss 2341.341453004226
INFO:root:current train perplexity6.344890117645264
INFO:root:current mean train loss 2341.499726220904
INFO:root:current train perplexity6.34263801574707
INFO:root:current mean train loss 2341.0637024705243
INFO:root:current train perplexity6.337369441986084
INFO:root:current mean train loss 2340.131174536017
INFO:root:current train perplexity6.3333024978637695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.92s/it]
INFO:root:eval mean loss: 2918.725371123076
INFO:root:eval perplexity: 11.107059478759766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/7

  4%|â–Ž         | 7/200 [1:02:39<28:44:54, 536.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2293.138909233941
INFO:root:current train perplexity6.117098331451416
INFO:root:current mean train loss 2287.5128287622483
INFO:root:current train perplexity6.10642147064209
INFO:root:current mean train loss 2289.8249808495198
INFO:root:current train perplexity6.080528736114502
INFO:root:current mean train loss 2279.7695381596404
INFO:root:current train perplexity6.058807849884033
INFO:root:current mean train loss 2283.098826139167
INFO:root:current train perplexity6.064521789550781
INFO:root:current mean train loss 2286.1330778497527
INFO:root:current train perplexity6.06678581237793
INFO:root:current mean train loss 2286.9602864583335
INFO:root:current train perplexity6.076143264770508
INFO:root:current mean train loss 2286.417303483798
INFO:root:current train perplexity6.068999767303467
INFO:root:current mean train loss 2285.9999877631226
INFO:root:current train perplexity6.073340892791748
INFO:root:current mean train loss 2286.5801555159824
INFO:root:current train perplexity6.068552017211914
INFO:root:current mean train loss 2287.988583308072
INFO:root:current train perplexity6.072066783905029
INFO:root:current mean train loss 2287.1065371382
INFO:root:current train perplexity6.072894096374512
INFO:root:current mean train loss 2284.7013275196596
INFO:root:current train perplexity6.068609237670898
INFO:root:current mean train loss 2284.6156279082
INFO:root:current train perplexity6.062797546386719
INFO:root:current mean train loss 2283.8742003447583
INFO:root:current train perplexity6.062552452087402
INFO:root:current mean train loss 2282.6103505975175
INFO:root:current train perplexity6.0549492835998535
INFO:root:current mean train loss 2282.440864251777
INFO:root:current train perplexity6.0511274337768555
INFO:root:current mean train loss 2280.661834183894
INFO:root:current train perplexity6.044933795928955
INFO:root:current mean train loss 2279.234110715759
INFO:root:current train perplexity6.044257164001465
INFO:root:current mean train loss 2280.128406640014
INFO:root:current train perplexity6.044523239135742


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.65s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.36s/it]
INFO:root:eval mean loss: 2895.6857250316725
INFO:root:eval perplexity: 10.897961616516113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/8

  4%|â–         | 8/200 [1:11:31<28:31:50, 534.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2229.2542864118304
INFO:root:current train perplexity5.753492832183838
INFO:root:current mean train loss 2236.9451416015627
INFO:root:current train perplexity5.852609157562256
INFO:root:current mean train loss 2243.861873545545
INFO:root:current train perplexity5.870116233825684
INFO:root:current mean train loss 2243.447065939832
INFO:root:current train perplexity5.866407871246338
INFO:root:current mean train loss 2246.6332988169
INFO:root:current train perplexity5.868364334106445
INFO:root:current mean train loss 2245.53626309689
INFO:root:current train perplexity5.851912021636963
INFO:root:current mean train loss 2242.711291984498
INFO:root:current train perplexity5.844420433044434
INFO:root:current mean train loss 2244.476176027051
INFO:root:current train perplexity5.844536304473877
INFO:root:current mean train loss 2243.022978164764
INFO:root:current train perplexity5.843436241149902
INFO:root:current mean train loss 2243.502068928601
INFO:root:current train perplexity5.845245361328125
INFO:root:current mean train loss 2241.860460895041
INFO:root:current train perplexity5.840905666351318
INFO:root:current mean train loss 2237.6699400511084
INFO:root:current train perplexity5.828976154327393
INFO:root:current mean train loss 2234.6841081256325
INFO:root:current train perplexity5.828389644622803
INFO:root:current mean train loss 2234.526567711991
INFO:root:current train perplexity5.824876308441162
INFO:root:current mean train loss 2233.5986756859756
INFO:root:current train perplexity5.821464538574219
INFO:root:current mean train loss 2232.37391926023
INFO:root:current train perplexity5.81729793548584
INFO:root:current mean train loss 2232.9339689202266
INFO:root:current train perplexity5.817200183868408
INFO:root:current mean train loss 2232.9204803730636
INFO:root:current train perplexity5.817829132080078
INFO:root:current mean train loss 2232.0432017147054
INFO:root:current train perplexity5.813961982727051
INFO:root:current mean train loss 2231.806270692022
INFO:root:current train perplexity5.817098140716553


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.39s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.89s/it]
INFO:root:eval mean loss: 2875.3941756698105
INFO:root:eval perplexity: 10.717073440551758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/9

  4%|â–         | 9/200 [1:20:28<28:24:26, 535.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2202.8824580265928
INFO:root:current train perplexity5.644903182983398
INFO:root:current mean train loss 2206.219105770713
INFO:root:current train perplexity5.625606060028076
INFO:root:current mean train loss 2212.4669417123946
INFO:root:current train perplexity5.643725872039795
INFO:root:current mean train loss 2207.604326074774
INFO:root:current train perplexity5.645749092102051
INFO:root:current mean train loss 2205.366664920233
INFO:root:current train perplexity5.652753829956055
INFO:root:current mean train loss 2205.2196624313574
INFO:root:current train perplexity5.655852794647217
INFO:root:current mean train loss 2200.074372086788
INFO:root:current train perplexity5.65133810043335
INFO:root:current mean train loss 2195.650181222469
INFO:root:current train perplexity5.649531364440918
INFO:root:current mean train loss 2198.909980308282
INFO:root:current train perplexity5.657009601593018
INFO:root:current mean train loss 2199.295337452608
INFO:root:current train perplexity5.655945777893066
INFO:root:current mean train loss 2198.2928091999242
INFO:root:current train perplexity5.655223369598389
INFO:root:current mean train loss 2197.720469792684
INFO:root:current train perplexity5.653944492340088
INFO:root:current mean train loss 2195.347788655339
INFO:root:current train perplexity5.6479973793029785
INFO:root:current mean train loss 2195.1350562643015
INFO:root:current train perplexity5.646789073944092
INFO:root:current mean train loss 2195.645988107056
INFO:root:current train perplexity5.645082950592041
INFO:root:current mean train loss 2195.283148146167
INFO:root:current train perplexity5.645177364349365
INFO:root:current mean train loss 2195.6197108529673
INFO:root:current train perplexity5.644894599914551
INFO:root:current mean train loss 2194.584175353725
INFO:root:current train perplexity5.640180587768555
INFO:root:current mean train loss 2192.8940668291457
INFO:root:current train perplexity5.6377763748168945
INFO:root:current mean train loss 2191.4914596432545
INFO:root:current train perplexity5.634810924530029


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.23s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.47s/it]
INFO:root:eval mean loss: 2855.5018732111016
INFO:root:eval perplexity: 10.542654991149902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/10

  5%|â–Œ         | 10/200 [1:29:28<28:19:54, 536.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2160.61474963202
INFO:root:current train perplexity5.511098384857178
INFO:root:current mean train loss 2141.4317742522653
INFO:root:current train perplexity5.451730728149414
INFO:root:current mean train loss 2150.546302313255
INFO:root:current train perplexity5.474010944366455
INFO:root:current mean train loss 2151.5950064310214
INFO:root:current train perplexity5.485404014587402
INFO:root:current mean train loss 2152.5697497792844
INFO:root:current train perplexity5.47896671295166
INFO:root:current mean train loss 2156.5974826913307
INFO:root:current train perplexity5.48781681060791
INFO:root:current mean train loss 2161.2082302395715
INFO:root:current train perplexity5.495429515838623
INFO:root:current mean train loss 2163.1366174745003
INFO:root:current train perplexity5.493934154510498
INFO:root:current mean train loss 2160.938043065395
INFO:root:current train perplexity5.487832546234131
INFO:root:current mean train loss 2160.2035631429794
INFO:root:current train perplexity5.48527193069458
INFO:root:current mean train loss 2159.3730434492663
INFO:root:current train perplexity5.492558002471924
INFO:root:current mean train loss 2158.410116987008
INFO:root:current train perplexity5.494428634643555
INFO:root:current mean train loss 2158.244582733082
INFO:root:current train perplexity5.492232322692871
INFO:root:current mean train loss 2158.545232891253
INFO:root:current train perplexity5.491313934326172
INFO:root:current mean train loss 2160.8027731815596
INFO:root:current train perplexity5.493849754333496
INFO:root:current mean train loss 2160.9740837491036
INFO:root:current train perplexity5.492995262145996
INFO:root:current mean train loss 2160.0695870264053
INFO:root:current train perplexity5.495360374450684
INFO:root:current mean train loss 2157.834848113893
INFO:root:current train perplexity5.490756988525391
INFO:root:current mean train loss 2157.741249602896
INFO:root:current train perplexity5.487589359283447
INFO:root:current mean train loss 2156.6996146570673
INFO:root:current train perplexity5.484338760375977


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.11s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.52s/it]
INFO:root:eval mean loss: 2849.64026355457
INFO:root:eval perplexity: 10.491803169250488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/11

  6%|â–Œ         | 11/200 [1:38:21<28:08:01, 535.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2130.2441732717116
INFO:root:current train perplexity5.311617851257324
INFO:root:current mean train loss 2119.574694561702
INFO:root:current train perplexity5.336581230163574
INFO:root:current mean train loss 2106.8979534869427
INFO:root:current train perplexity5.302034854888916
INFO:root:current mean train loss 2109.4975525851078
INFO:root:current train perplexity5.317732334136963
INFO:root:current mean train loss 2113.817001782327
INFO:root:current train perplexity5.32175874710083
INFO:root:current mean train loss 2115.057018709671
INFO:root:current train perplexity5.3310041427612305
INFO:root:current mean train loss 2118.9513368300723
INFO:root:current train perplexity5.339138507843018
INFO:root:current mean train loss 2119.754875668436
INFO:root:current train perplexity5.34196138381958
INFO:root:current mean train loss 2122.590774570577
INFO:root:current train perplexity5.349287033081055
INFO:root:current mean train loss 2123.363994606126
INFO:root:current train perplexity5.347672462463379
INFO:root:current mean train loss 2124.588936203312
INFO:root:current train perplexity5.35040283203125
INFO:root:current mean train loss 2125.020843145618
INFO:root:current train perplexity5.353301048278809
INFO:root:current mean train loss 2126.7231968335377
INFO:root:current train perplexity5.357126235961914
INFO:root:current mean train loss 2126.932440159125
INFO:root:current train perplexity5.353498935699463
INFO:root:current mean train loss 2125.78991345987
INFO:root:current train perplexity5.350410461425781
INFO:root:current mean train loss 2127.079542046988
INFO:root:current train perplexity5.352138042449951
INFO:root:current mean train loss 2126.9261271158853
INFO:root:current train perplexity5.350048542022705
INFO:root:current mean train loss 2127.3458857520295
INFO:root:current train perplexity5.351500034332275
INFO:root:current mean train loss 2126.1819505903904
INFO:root:current train perplexity5.350669860839844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.55s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.57s/it]
INFO:root:eval mean loss: 2834.0315322646866
INFO:root:eval perplexity: 10.35758113861084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/12

  6%|â–Œ         | 12/200 [1:47:05<27:47:06, 532.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2166.66357421875
INFO:root:current train perplexity5.382018089294434
INFO:root:current mean train loss 2095.554994453504
INFO:root:current train perplexity5.195737361907959
INFO:root:current mean train loss 2095.0513416910408
INFO:root:current train perplexity5.205440044403076
INFO:root:current mean train loss 2101.030612655992
INFO:root:current train perplexity5.232989311218262
INFO:root:current mean train loss 2101.7510365374924
INFO:root:current train perplexity5.234856605529785
INFO:root:current mean train loss 2099.882691157741
INFO:root:current train perplexity5.233936309814453
INFO:root:current mean train loss 2100.8925082837764
INFO:root:current train perplexity5.237849235534668
INFO:root:current mean train loss 2099.786267732319
INFO:root:current train perplexity5.235804080963135
INFO:root:current mean train loss 2097.73965457902
INFO:root:current train perplexity5.230934143066406
INFO:root:current mean train loss 2099.256087969572
INFO:root:current train perplexity5.234870910644531
INFO:root:current mean train loss 2098.0176527302856
INFO:root:current train perplexity5.230795860290527
INFO:root:current mean train loss 2099.209976763479
INFO:root:current train perplexity5.2377238273620605
INFO:root:current mean train loss 2097.8064446833764
INFO:root:current train perplexity5.235862731933594
INFO:root:current mean train loss 2096.9232717354485
INFO:root:current train perplexity5.2392473220825195
INFO:root:current mean train loss 2096.1541360867336
INFO:root:current train perplexity5.236588954925537
INFO:root:current mean train loss 2096.4241482042426
INFO:root:current train perplexity5.235557556152344
INFO:root:current mean train loss 2097.487342839232
INFO:root:current train perplexity5.237578868865967
INFO:root:current mean train loss 2096.7737087355035
INFO:root:current train perplexity5.233684539794922
INFO:root:current mean train loss 2096.7646907525
INFO:root:current train perplexity5.233519077301025
INFO:root:current mean train loss 2097.724504175151
INFO:root:current train perplexity5.237300395965576


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.26s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.85s/it]
INFO:root:eval mean loss: 2822.722217823292
INFO:root:eval perplexity: 10.261408805847168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/13

  6%|â–‹         | 13/200 [1:55:57<27:38:28, 532.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2100.84365234375
INFO:root:current train perplexity5.206228733062744
INFO:root:current mean train loss 2068.700003051758
INFO:root:current train perplexity5.118337631225586
INFO:root:current mean train loss 2071.8105130282315
INFO:root:current train perplexity5.125001430511475
INFO:root:current mean train loss 2064.4307182312014
INFO:root:current train perplexity5.109711647033691
INFO:root:current mean train loss 2068.7585277739026
INFO:root:current train perplexity5.119304180145264
INFO:root:current mean train loss 2068.6802201491137
INFO:root:current train perplexity5.120525360107422
INFO:root:current mean train loss 2070.648648957283
INFO:root:current train perplexity5.124859809875488
INFO:root:current mean train loss 2070.669006178114
INFO:root:current train perplexity5.127915859222412
INFO:root:current mean train loss 2070.773800287014
INFO:root:current train perplexity5.128942489624023
INFO:root:current mean train loss 2072.3344972029977
INFO:root:current train perplexity5.135098457336426
INFO:root:current mean train loss 2072.0727858599494
INFO:root:current train perplexity5.13808536529541
INFO:root:current mean train loss 2071.3531587873185
INFO:root:current train perplexity5.141526222229004
INFO:root:current mean train loss 2072.5046287661694
INFO:root:current train perplexity5.144257068634033
INFO:root:current mean train loss 2073.717465209961
INFO:root:current train perplexity5.1425089836120605
INFO:root:current mean train loss 2073.573113755777
INFO:root:current train perplexity5.139039993286133
INFO:root:current mean train loss 2072.5946236058285
INFO:root:current train perplexity5.136326313018799
INFO:root:current mean train loss 2072.7632514859424
INFO:root:current train perplexity5.136927127838135
INFO:root:current mean train loss 2072.51221277991
INFO:root:current train perplexity5.136331081390381
INFO:root:current mean train loss 2073.5218116844094
INFO:root:current train perplexity5.135113716125488
INFO:root:current mean train loss 2073.0995719273883
INFO:root:current train perplexity5.133721828460693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.28s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.17s/it]
INFO:root:eval mean loss: 2823.9478866073105
INFO:root:eval perplexity: 10.2717924118042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/14

  7%|â–‹         | 14/200 [2:04:56<27:35:52, 534.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2054.2940310916383
INFO:root:current train perplexity5.000677108764648
INFO:root:current mean train loss 2052.9495038777372
INFO:root:current train perplexity5.055951118469238
INFO:root:current mean train loss 2053.3624138811974
INFO:root:current train perplexity5.056671142578125
INFO:root:current mean train loss 2054.614826508021
INFO:root:current train perplexity5.043126583099365
INFO:root:current mean train loss 2054.0735050437106
INFO:root:current train perplexity5.048879146575928
INFO:root:current mean train loss 2052.0465938063753
INFO:root:current train perplexity5.049536228179932
INFO:root:current mean train loss 2049.0599307744433
INFO:root:current train perplexity5.048184394836426
INFO:root:current mean train loss 2047.2080306696278
INFO:root:current train perplexity5.0499138832092285
INFO:root:current mean train loss 2043.6570194659125
INFO:root:current train perplexity5.041818618774414
INFO:root:current mean train loss 2044.807994993163
INFO:root:current train perplexity5.040822982788086
INFO:root:current mean train loss 2044.098488023219
INFO:root:current train perplexity5.037065029144287
INFO:root:current mean train loss 2045.4371239117813
INFO:root:current train perplexity5.03690242767334
INFO:root:current mean train loss 2047.4392117474863
INFO:root:current train perplexity5.038956165313721
INFO:root:current mean train loss 2046.55548365043
INFO:root:current train perplexity5.035881996154785
INFO:root:current mean train loss 2047.8507616100164
INFO:root:current train perplexity5.037308216094971
INFO:root:current mean train loss 2048.52724113787
INFO:root:current train perplexity5.036961555480957
INFO:root:current mean train loss 2048.5291070955586
INFO:root:current train perplexity5.036848545074463
INFO:root:current mean train loss 2049.334532321014
INFO:root:current train perplexity5.035821914672852
INFO:root:current mean train loss 2049.4355255442597
INFO:root:current train perplexity5.035384654998779
INFO:root:current mean train loss 2050.1995710334522
INFO:root:current train perplexity5.039986610412598


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.75s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.14s/it]
INFO:root:eval mean loss: 2825.282502228791
INFO:root:eval perplexity: 10.28310489654541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/15

  8%|â–Š         | 15/200 [2:13:47<27:24:03, 533.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2018.6136271158855
INFO:root:current train perplexity4.848692893981934
INFO:root:current mean train loss 2010.7929219828025
INFO:root:current train perplexity4.911571025848389
INFO:root:current mean train loss 2022.2190691129429
INFO:root:current train perplexity4.94146728515625
INFO:root:current mean train loss 2020.3208345747264
INFO:root:current train perplexity4.9238152503967285
INFO:root:current mean train loss 2025.5893716013904
INFO:root:current train perplexity4.930398941040039
INFO:root:current mean train loss 2028.454602403331
INFO:root:current train perplexity4.942636013031006
INFO:root:current mean train loss 2026.9962438180905
INFO:root:current train perplexity4.942828178405762
INFO:root:current mean train loss 2028.110280327835
INFO:root:current train perplexity4.9454665184021
INFO:root:current mean train loss 2026.913838227962
INFO:root:current train perplexity4.940652370452881
INFO:root:current mean train loss 2028.748033695501
INFO:root:current train perplexity4.9473137855529785
INFO:root:current mean train loss 2029.7461366020073
INFO:root:current train perplexity4.944982528686523
INFO:root:current mean train loss 2030.3336623801722
INFO:root:current train perplexity4.9481353759765625
INFO:root:current mean train loss 2031.5535378585403
INFO:root:current train perplexity4.95175838470459
INFO:root:current mean train loss 2030.9904616565789
INFO:root:current train perplexity4.952898025512695
INFO:root:current mean train loss 2032.3238834344372
INFO:root:current train perplexity4.95623254776001
INFO:root:current mean train loss 2032.2297042787766
INFO:root:current train perplexity4.957376956939697
INFO:root:current mean train loss 2031.6579410502238
INFO:root:current train perplexity4.957717418670654
INFO:root:current mean train loss 2032.2974318327083
INFO:root:current train perplexity4.9605584144592285
INFO:root:current mean train loss 2030.6595714449754
INFO:root:current train perplexity4.955337047576904
INFO:root:current mean train loss 2029.2588177996258
INFO:root:current train perplexity4.957242012023926


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.98s/it]
INFO:root:eval mean loss: 2814.9332125093842
INFO:root:eval perplexity: 10.195691108703613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/16

  8%|â–Š         | 16/200 [2:22:37<27:12:38, 532.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2002.723545128191
INFO:root:current train perplexity4.840583324432373
INFO:root:current mean train loss 1992.9156301398027
INFO:root:current train perplexity4.831117153167725
INFO:root:current mean train loss 1993.7203981744408
INFO:root:current train perplexity4.828448295593262
INFO:root:current mean train loss 1996.7492346750757
INFO:root:current train perplexity4.834958076477051
INFO:root:current mean train loss 1997.1428131945827
INFO:root:current train perplexity4.840034008026123
INFO:root:current mean train loss 2002.4002268669276
INFO:root:current train perplexity4.850711345672607
INFO:root:current mean train loss 2005.3663491989569
INFO:root:current train perplexity4.8596272468566895
INFO:root:current mean train loss 2006.3862698922362
INFO:root:current train perplexity4.855266571044922
INFO:root:current mean train loss 2007.075838739371
INFO:root:current train perplexity4.864532470703125
INFO:root:current mean train loss 2006.5207713134012
INFO:root:current train perplexity4.8650031089782715
INFO:root:current mean train loss 2007.665769660729
INFO:root:current train perplexity4.869743347167969
INFO:root:current mean train loss 2006.1854969418832
INFO:root:current train perplexity4.870274543762207
INFO:root:current mean train loss 2005.870400801688
INFO:root:current train perplexity4.869719982147217
INFO:root:current mean train loss 2005.6500606522952
INFO:root:current train perplexity4.87086820602417
INFO:root:current mean train loss 2006.8234840875414
INFO:root:current train perplexity4.875176429748535
INFO:root:current mean train loss 2007.837348340652
INFO:root:current train perplexity4.875430107116699
INFO:root:current mean train loss 2009.094607852591
INFO:root:current train perplexity4.877931118011475
INFO:root:current mean train loss 2009.3504704842119
INFO:root:current train perplexity4.878017425537109
INFO:root:current mean train loss 2008.883667840351
INFO:root:current train perplexity4.877371311187744
INFO:root:current mean train loss 2008.4602966153761
INFO:root:current train perplexity4.877709865570068


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.04s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.32s/it]
INFO:root:eval mean loss: 2807.692893821556
INFO:root:eval perplexity: 10.13498306274414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/17
#############best##########
  8%|â–Š         | 17/200 [2:31:34<27:07:36, 533.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1982.250991821289
INFO:root:current train perplexity4.773631572723389
INFO:root:current mean train loss 1989.5168846617353
INFO:root:current train perplexity4.790432929992676
INFO:root:current mean train loss 1989.9066747029622
INFO:root:current train perplexity4.783588886260986
INFO:root:current mean train loss 1989.2915942005275
INFO:root:current train perplexity4.773781776428223
INFO:root:current mean train loss 1990.2517039814934
INFO:root:current train perplexity4.784803867340088
INFO:root:current mean train loss 1983.2301714631164
INFO:root:current train perplexity4.776766300201416
INFO:root:current mean train loss 1983.9564540774322
INFO:root:current train perplexity4.782719612121582
INFO:root:current mean train loss 1986.266881952431
INFO:root:current train perplexity4.782314777374268
INFO:root:current mean train loss 1986.0911536689277
INFO:root:current train perplexity4.782922744750977
INFO:root:current mean train loss 1983.5355879439999
INFO:root:current train perplexity4.781615257263184
INFO:root:current mean train loss 1985.9843516630285
INFO:root:current train perplexity4.7892584800720215
INFO:root:current mean train loss 1984.285418886127
INFO:root:current train perplexity4.788661003112793
INFO:root:current mean train loss 1984.34990782175
INFO:root:current train perplexity4.789272785186768
INFO:root:current mean train loss 1985.1270128409526
INFO:root:current train perplexity4.790682792663574
INFO:root:current mean train loss 1984.6057121522965
INFO:root:current train perplexity4.790290355682373
INFO:root:current mean train loss 1986.1769875014759
INFO:root:current train perplexity4.793177127838135
INFO:root:current mean train loss 1987.2487476222323
INFO:root:current train perplexity4.798490524291992
INFO:root:current mean train loss 1987.9187258180623
INFO:root:current train perplexity4.799694061279297
INFO:root:current mean train loss 1988.2529640844314
INFO:root:current train perplexity4.801204681396484


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.01s/it]
INFO:root:eval mean loss: 2812.5886494404563
INFO:root:eval perplexity: 10.175993919372559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/18

  9%|â–‰         | 18/200 [2:40:24<26:55:47, 532.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1919.442626953125
INFO:root:current train perplexity4.734897136688232
INFO:root:current mean train loss 1970.5009137834822
INFO:root:current train perplexity4.730075359344482
INFO:root:current mean train loss 1962.2699849942835
INFO:root:current train perplexity4.714427471160889
INFO:root:current mean train loss 1967.8234407018442
INFO:root:current train perplexity4.724313259124756
INFO:root:current mean train loss 1966.3933262201003
INFO:root:current train perplexity4.719324588775635
INFO:root:current mean train loss 1964.5082178701268
INFO:root:current train perplexity4.7072577476501465
INFO:root:current mean train loss 1963.0883887929365
INFO:root:current train perplexity4.71022891998291
INFO:root:current mean train loss 1968.0366387549866
INFO:root:current train perplexity4.718598365783691
INFO:root:current mean train loss 1967.711042738257
INFO:root:current train perplexity4.716960430145264
INFO:root:current mean train loss 1967.631560908365
INFO:root:current train perplexity4.719462871551514
INFO:root:current mean train loss 1969.9512045485462
INFO:root:current train perplexity4.725767135620117
INFO:root:current mean train loss 1970.212291872879
INFO:root:current train perplexity4.728430271148682
INFO:root:current mean train loss 1970.839446236385
INFO:root:current train perplexity4.730233192443848
INFO:root:current mean train loss 1970.5818684895833
INFO:root:current train perplexity4.732755184173584
INFO:root:current mean train loss 1971.4589775981428
INFO:root:current train perplexity4.73283576965332
INFO:root:current mean train loss 1969.6939498546512
INFO:root:current train perplexity4.728672504425049
INFO:root:current mean train loss 1969.6313327492212
INFO:root:current train perplexity4.729785442352295
INFO:root:current mean train loss 1970.3836084270756
INFO:root:current train perplexity4.731973171234131
INFO:root:current mean train loss 1970.3132859840287
INFO:root:current train perplexity4.73402738571167
INFO:root:current mean train loss 1970.8653416430857
INFO:root:current train perplexity4.735286235809326


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.31s/it]
INFO:root:eval mean loss: 2815.2522016645553
INFO:root:eval perplexity: 10.198375701904297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/19

 10%|â–‰         | 19/200 [2:49:19<26:48:30, 533.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1980.1163607510653
INFO:root:current train perplexity4.554020881652832
INFO:root:current mean train loss 1951.288222015881
INFO:root:current train perplexity4.64312219619751
INFO:root:current mean train loss 1961.2108451224663
INFO:root:current train perplexity4.661518096923828
INFO:root:current mean train loss 1956.4551558405717
INFO:root:current train perplexity4.655496597290039
INFO:root:current mean train loss 1953.2552429488485
INFO:root:current train perplexity4.65620231628418
INFO:root:current mean train loss 1949.3285894284304
INFO:root:current train perplexity4.651374340057373
INFO:root:current mean train loss 1950.8927153069108
INFO:root:current train perplexity4.659418106079102
INFO:root:current mean train loss 1951.9975733030535
INFO:root:current train perplexity4.660898685455322
INFO:root:current mean train loss 1949.5511877055296
INFO:root:current train perplexity4.657613754272461
INFO:root:current mean train loss 1951.4523315429688
INFO:root:current train perplexity4.662550449371338
INFO:root:current mean train loss 1950.8692683091135
INFO:root:current train perplexity4.661655902862549
INFO:root:current mean train loss 1951.4832558045414
INFO:root:current train perplexity4.662539005279541
INFO:root:current mean train loss 1951.5345474967394
INFO:root:current train perplexity4.663543701171875
INFO:root:current mean train loss 1952.768062897421
INFO:root:current train perplexity4.666670322418213
INFO:root:current mean train loss 1952.4668666662546
INFO:root:current train perplexity4.66776180267334
INFO:root:current mean train loss 1952.9865466003819
INFO:root:current train perplexity4.6686530113220215
INFO:root:current mean train loss 1953.5826039719964
INFO:root:current train perplexity4.670990943908691
INFO:root:current mean train loss 1952.474070053898
INFO:root:current train perplexity4.670772552490234
INFO:root:current mean train loss 1952.9426537523154
INFO:root:current train perplexity4.67138671875
INFO:root:current mean train loss 1955.1129944927363
INFO:root:current train perplexity4.674410820007324


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.31s/it]
INFO:root:eval mean loss: 2812.8472698772994
INFO:root:eval perplexity: 10.178165435791016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/20

 10%|â–ˆ         | 20/200 [2:58:01<26:30:14, 530.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1931.0282702323718
INFO:root:current train perplexity4.5452165603637695
INFO:root:current mean train loss 1935.7004174980327
INFO:root:current train perplexity4.572960376739502
INFO:root:current mean train loss 1931.130629024745
INFO:root:current train perplexity4.572749137878418
INFO:root:current mean train loss 1937.502343101839
INFO:root:current train perplexity4.5756707191467285
INFO:root:current mean train loss 1933.5260685462342
INFO:root:current train perplexity4.5653533935546875
INFO:root:current mean train loss 1935.072303446414
INFO:root:current train perplexity4.575751304626465
INFO:root:current mean train loss 1933.4605093942562
INFO:root:current train perplexity4.584650039672852
INFO:root:current mean train loss 1934.392846382358
INFO:root:current train perplexity4.586965560913086
INFO:root:current mean train loss 1933.8960403242327
INFO:root:current train perplexity4.590714454650879
INFO:root:current mean train loss 1935.4591122953275
INFO:root:current train perplexity4.5969743728637695
INFO:root:current mean train loss 1937.157500310169
INFO:root:current train perplexity4.601598262786865
INFO:root:current mean train loss 1936.2355154303734
INFO:root:current train perplexity4.599630355834961
INFO:root:current mean train loss 1937.4016710332173
INFO:root:current train perplexity4.602585792541504
INFO:root:current mean train loss 1936.7564442002836
INFO:root:current train perplexity4.604559421539307
INFO:root:current mean train loss 1937.5813152003072
INFO:root:current train perplexity4.610215187072754
INFO:root:current mean train loss 1937.20166554987
INFO:root:current train perplexity4.610977649688721
INFO:root:current mean train loss 1936.7480520884972
INFO:root:current train perplexity4.613497734069824
INFO:root:current mean train loss 1936.8394194872233
INFO:root:current train perplexity4.613925933837891
INFO:root:current mean train loss 1937.827725666123
INFO:root:current train perplexity4.614748477935791
INFO:root:current mean train loss 1938.4149953085716
INFO:root:current train perplexity4.61571741104126


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:09<00:00, 489.79s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.39s/it]
INFO:root:eval mean loss: 2821.699461424315
INFO:root:eval perplexity: 10.252756118774414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/21

 10%|â–ˆ         | 21/200 [3:07:01<26:29:41, 532.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1925.188960484096
INFO:root:current train perplexity4.556319713592529
INFO:root:current mean train loss 1913.6213206755808
INFO:root:current train perplexity4.554294586181641
INFO:root:current mean train loss 1907.219183921814
INFO:root:current train perplexity4.54689884185791
INFO:root:current mean train loss 1916.063688128182
INFO:root:current train perplexity4.559355735778809
INFO:root:current mean train loss 1916.2421888384902
INFO:root:current train perplexity4.551213264465332
INFO:root:current mean train loss 1916.0860117082116
INFO:root:current train perplexity4.5469160079956055
INFO:root:current mean train loss 1915.073442970834
INFO:root:current train perplexity4.543546676635742
INFO:root:current mean train loss 1914.4149126325335
INFO:root:current train perplexity4.541881084442139
INFO:root:current mean train loss 1915.9695246331046
INFO:root:current train perplexity4.545505523681641
INFO:root:current mean train loss 1918.2490208837278
INFO:root:current train perplexity4.551816463470459
INFO:root:current mean train loss 1921.45980684685
INFO:root:current train perplexity4.55832576751709
INFO:root:current mean train loss 1920.937968745776
INFO:root:current train perplexity4.557339668273926
INFO:root:current mean train loss 1922.3263231022343
INFO:root:current train perplexity4.559388637542725
INFO:root:current mean train loss 1922.3989291120772
INFO:root:current train perplexity4.561573028564453
INFO:root:current mean train loss 1923.1354398622618
INFO:root:current train perplexity4.559898376464844
INFO:root:current mean train loss 1923.8921323643858
INFO:root:current train perplexity4.561758041381836
INFO:root:current mean train loss 1924.3058891112103
INFO:root:current train perplexity4.5621256828308105
INFO:root:current mean train loss 1924.20098466808
INFO:root:current train perplexity4.562416076660156
INFO:root:current mean train loss 1923.9117478995488
INFO:root:current train perplexity4.560911178588867
INFO:root:current mean train loss 1924.0747394834802
INFO:root:current train perplexity4.562627792358398


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.00s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.50s/it]
INFO:root:eval mean loss: 2831.890032610736
INFO:root:eval perplexity: 10.339305877685547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/22

 11%|â–ˆ         | 22/200 [3:15:43<26:11:44, 529.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1878.584476000642
INFO:root:current train perplexity4.458333969116211
INFO:root:current mean train loss 1899.512714363936
INFO:root:current train perplexity4.495428085327148
INFO:root:current mean train loss 1908.5382737379807
INFO:root:current train perplexity4.498717308044434
INFO:root:current mean train loss 1904.3094754052865
INFO:root:current train perplexity4.492825984954834
INFO:root:current mean train loss 1903.6837811137354
INFO:root:current train perplexity4.4936299324035645
INFO:root:current mean train loss 1901.881331252386
INFO:root:current train perplexity4.494041919708252
INFO:root:current mean train loss 1902.9921662782667
INFO:root:current train perplexity4.4945597648620605
INFO:root:current mean train loss 1904.508472437692
INFO:root:current train perplexity4.497140407562256
INFO:root:current mean train loss 1903.4622549644723
INFO:root:current train perplexity4.493058204650879
INFO:root:current mean train loss 1903.7163250287047
INFO:root:current train perplexity4.494674205780029
INFO:root:current mean train loss 1903.695092932709
INFO:root:current train perplexity4.491754531860352
INFO:root:current mean train loss 1904.644095938732
INFO:root:current train perplexity4.493790149688721
INFO:root:current mean train loss 1903.8887624927888
INFO:root:current train perplexity4.4959588050842285
INFO:root:current mean train loss 1906.0418073483306
INFO:root:current train perplexity4.4981536865234375
INFO:root:current mean train loss 1906.1945066536193
INFO:root:current train perplexity4.501412868499756
INFO:root:current mean train loss 1906.330141216649
INFO:root:current train perplexity4.500868320465088
INFO:root:current mean train loss 1905.7963345488317
INFO:root:current train perplexity4.50353479385376
INFO:root:current mean train loss 1907.2163134820705
INFO:root:current train perplexity4.505319118499756
INFO:root:current mean train loss 1906.9121620353376
INFO:root:current train perplexity4.5034027099609375
INFO:root:current mean train loss 1907.3920237661555
INFO:root:current train perplexity4.504088878631592


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.28s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.66s/it]
INFO:root:eval mean loss: 2826.5748140718842
INFO:root:eval perplexity: 10.29407024383545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/23

 12%|â–ˆâ–        | 23/200 [3:24:26<25:56:05, 527.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1889.7396362304687
INFO:root:current train perplexity4.490804195404053
INFO:root:current mean train loss 1882.6363062808389
INFO:root:current train perplexity4.450963973999023
INFO:root:current mean train loss 1880.4159832132273
INFO:root:current train perplexity4.442776203155518
INFO:root:current mean train loss 1881.798779296875
INFO:root:current train perplexity4.4447126388549805
INFO:root:current mean train loss 1885.3471186423787
INFO:root:current train perplexity4.450981140136719
INFO:root:current mean train loss 1885.2633673199152
INFO:root:current train perplexity4.443577766418457
INFO:root:current mean train loss 1887.529401961617
INFO:root:current train perplexity4.441554546356201
INFO:root:current mean train loss 1886.098293024377
INFO:root:current train perplexity4.441615581512451
INFO:root:current mean train loss 1885.8783515844452
INFO:root:current train perplexity4.44085693359375
INFO:root:current mean train loss 1886.8712124911222
INFO:root:current train perplexity4.441530227661133
INFO:root:current mean train loss 1888.5439270579486
INFO:root:current train perplexity4.443011283874512
INFO:root:current mean train loss 1889.6977189264378
INFO:root:current train perplexity4.443482875823975
INFO:root:current mean train loss 1889.6384608542273
INFO:root:current train perplexity4.4430437088012695
INFO:root:current mean train loss 1890.691302621965
INFO:root:current train perplexity4.444348335266113
INFO:root:current mean train loss 1890.9157555906565
INFO:root:current train perplexity4.443517684936523
INFO:root:current mean train loss 1890.685723224376
INFO:root:current train perplexity4.445557117462158
INFO:root:current mean train loss 1890.8866174099714
INFO:root:current train perplexity4.446461200714111
INFO:root:current mean train loss 1892.2456947369283
INFO:root:current train perplexity4.448436737060547
INFO:root:current mean train loss 1893.5010335286458
INFO:root:current train perplexity4.452310562133789


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.64s/it]
INFO:root:eval mean loss: 2831.5848707594314
INFO:root:eval perplexity: 10.336700439453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/24

 12%|â–ˆâ–        | 24/200 [3:33:01<25:36:57, 523.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1896.3228236607142
INFO:root:current train perplexity4.546974182128906
INFO:root:current mean train loss 1880.3143276321555
INFO:root:current train perplexity4.37451171875
INFO:root:current mean train loss 1872.1862910911082
INFO:root:current train perplexity4.372281074523926
INFO:root:current mean train loss 1874.6529541015625
INFO:root:current train perplexity4.367387294769287
INFO:root:current mean train loss 1875.4445683809697
INFO:root:current train perplexity4.371179580688477
INFO:root:current mean train loss 1874.6212589181152
INFO:root:current train perplexity4.385936737060547
INFO:root:current mean train loss 1875.2089463662865
INFO:root:current train perplexity4.389297008514404
INFO:root:current mean train loss 1876.3455819179853
INFO:root:current train perplexity4.392317295074463
INFO:root:current mean train loss 1874.5363722639308
INFO:root:current train perplexity4.385467529296875
INFO:root:current mean train loss 1874.4705016484202
INFO:root:current train perplexity4.390239715576172
INFO:root:current mean train loss 1874.2941522380447
INFO:root:current train perplexity4.392725944519043
INFO:root:current mean train loss 1873.776812904034
INFO:root:current train perplexity4.394957065582275
INFO:root:current mean train loss 1876.215445302791
INFO:root:current train perplexity4.399994373321533
INFO:root:current mean train loss 1877.9370036865794
INFO:root:current train perplexity4.403265953063965
INFO:root:current mean train loss 1877.9541848514125
INFO:root:current train perplexity4.404431343078613
INFO:root:current mean train loss 1879.0597783261446
INFO:root:current train perplexity4.405030727386475
INFO:root:current mean train loss 1879.4793398844654
INFO:root:current train perplexity4.407090187072754
INFO:root:current mean train loss 1880.4467085495844
INFO:root:current train perplexity4.407125949859619
INFO:root:current mean train loss 1880.4181246649316
INFO:root:current train perplexity4.407500743865967
INFO:root:current mean train loss 1880.1426726702814
INFO:root:current train perplexity4.407088756561279


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.06s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.95s/it]
INFO:root:eval mean loss: 2828.825395463823
INFO:root:eval perplexity: 10.313199043273926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/25

 12%|â–ˆâ–Ž        | 25/200 [3:41:43<25:25:51, 523.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1869.2740681966145
INFO:root:current train perplexity4.2732744216918945
INFO:root:current mean train loss 1873.32321757655
INFO:root:current train perplexity4.335925102233887
INFO:root:current mean train loss 1866.9997307913643
INFO:root:current train perplexity4.3259711265563965
INFO:root:current mean train loss 1869.3625424232011
INFO:root:current train perplexity4.347401142120361
INFO:root:current mean train loss 1864.1488494873047
INFO:root:current train perplexity4.337743282318115
INFO:root:current mean train loss 1861.7086642898676
INFO:root:current train perplexity4.334003925323486
INFO:root:current mean train loss 1862.209963676257
INFO:root:current train perplexity4.337304592132568
INFO:root:current mean train loss 1863.7654262142287
INFO:root:current train perplexity4.346264839172363
INFO:root:current mean train loss 1862.6762210882982
INFO:root:current train perplexity4.343411445617676
INFO:root:current mean train loss 1864.5353195388595
INFO:root:current train perplexity4.347280502319336
INFO:root:current mean train loss 1865.0357642173767
INFO:root:current train perplexity4.34561824798584
INFO:root:current mean train loss 1864.7393299252112
INFO:root:current train perplexity4.345386981964111
INFO:root:current mean train loss 1863.9656463822507
INFO:root:current train perplexity4.345218181610107
INFO:root:current mean train loss 1863.2442542130852
INFO:root:current train perplexity4.344233512878418
INFO:root:current mean train loss 1863.8196646015297
INFO:root:current train perplexity4.3476667404174805
INFO:root:current mean train loss 1862.8966757306277
INFO:root:current train perplexity4.347116470336914
INFO:root:current mean train loss 1863.4536357560182
INFO:root:current train perplexity4.349920272827148
INFO:root:current mean train loss 1864.2217295352377
INFO:root:current train perplexity4.3521904945373535
INFO:root:current mean train loss 1864.176772535893
INFO:root:current train perplexity4.353574275970459
INFO:root:current mean train loss 1864.5523325073495
INFO:root:current train perplexity4.3555450439453125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.22s/it]
INFO:root:eval mean loss: 2826.3584343620964
INFO:root:eval perplexity: 10.292234420776367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/26

 13%|â–ˆâ–Ž        | 26/200 [3:50:19<25:11:08, 521.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1824.0744747999238
INFO:root:current train perplexity4.285943984985352
INFO:root:current mean train loss 1829.9931839746787
INFO:root:current train perplexity4.265584945678711
INFO:root:current mean train loss 1836.5244429339018
INFO:root:current train perplexity4.281005382537842
INFO:root:current mean train loss 1845.7564310649973
INFO:root:current train perplexity4.287387371063232
INFO:root:current mean train loss 1843.576547497254
INFO:root:current train perplexity4.292553424835205
INFO:root:current mean train loss 1841.7496062611917
INFO:root:current train perplexity4.288107395172119
INFO:root:current mean train loss 1843.4358527173117
INFO:root:current train perplexity4.293913841247559
INFO:root:current mean train loss 1846.529034613276
INFO:root:current train perplexity4.297144889831543
INFO:root:current mean train loss 1847.2044014403425
INFO:root:current train perplexity4.299803733825684
INFO:root:current mean train loss 1848.6023103590346
INFO:root:current train perplexity4.301922798156738
INFO:root:current mean train loss 1850.428044417177
INFO:root:current train perplexity4.302544116973877
INFO:root:current mean train loss 1849.2636893136162
INFO:root:current train perplexity4.297147274017334
INFO:root:current mean train loss 1850.8969718693343
INFO:root:current train perplexity4.300172328948975
INFO:root:current mean train loss 1850.6750856039687
INFO:root:current train perplexity4.302982807159424
INFO:root:current mean train loss 1850.4459567364515
INFO:root:current train perplexity4.304221153259277
INFO:root:current mean train loss 1850.902638192149
INFO:root:current train perplexity4.309165954589844
INFO:root:current mean train loss 1851.0290532550894
INFO:root:current train perplexity4.309237957000732
INFO:root:current mean train loss 1851.8150796731404
INFO:root:current train perplexity4.310659885406494
INFO:root:current mean train loss 1851.516500909195
INFO:root:current train perplexity4.310810565948486
INFO:root:current mean train loss 1852.1009267407062
INFO:root:current train perplexity4.312751293182373


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.72s/it]
INFO:root:eval mean loss: 2843.3814568963494
INFO:root:eval perplexity: 10.437776565551758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/27

 14%|â–ˆâ–Ž        | 27/200 [3:58:56<24:59:21, 520.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1821.880265860722
INFO:root:current train perplexity4.203575134277344
INFO:root:current mean train loss 1830.5098289779469
INFO:root:current train perplexity4.248498916625977
INFO:root:current mean train loss 1827.9392662344053
INFO:root:current train perplexity4.2417497634887695
INFO:root:current mean train loss 1827.234729958646
INFO:root:current train perplexity4.235674858093262
INFO:root:current mean train loss 1833.0437075685727
INFO:root:current train perplexity4.246567726135254
INFO:root:current mean train loss 1833.7577757476479
INFO:root:current train perplexity4.243682861328125
INFO:root:current mean train loss 1837.090932364884
INFO:root:current train perplexity4.253067493438721
INFO:root:current mean train loss 1835.6754319485385
INFO:root:current train perplexity4.253754615783691
INFO:root:current mean train loss 1835.044848888904
INFO:root:current train perplexity4.25310754776001
INFO:root:current mean train loss 1835.5277797882143
INFO:root:current train perplexity4.254105567932129
INFO:root:current mean train loss 1834.9587903085865
INFO:root:current train perplexity4.254971027374268
INFO:root:current mean train loss 1835.5834369560598
INFO:root:current train perplexity4.257564544677734
INFO:root:current mean train loss 1836.0414583967297
INFO:root:current train perplexity4.2591657638549805
INFO:root:current mean train loss 1834.865934616336
INFO:root:current train perplexity4.258755683898926
INFO:root:current mean train loss 1835.720099303948
INFO:root:current train perplexity4.26161527633667
INFO:root:current mean train loss 1836.3688632636995
INFO:root:current train perplexity4.26293420791626
INFO:root:current mean train loss 1837.906453278729
INFO:root:current train perplexity4.265416622161865
INFO:root:current mean train loss 1839.0129772963105
INFO:root:current train perplexity4.267319202423096
INFO:root:current mean train loss 1839.7468349099545
INFO:root:current train perplexity4.268678188323975
INFO:root:current mean train loss 1839.3382653147744
INFO:root:current train perplexity4.268867492675781


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.29s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.65s/it]
INFO:root:eval mean loss: 2848.2744250598253
INFO:root:eval perplexity: 10.479988098144531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/28

 14%|â–ˆâ–        | 28/200 [4:07:30<24:45:32, 518.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1832.8366178385418
INFO:root:current train perplexity4.217085838317871
INFO:root:current mean train loss 1817.3834416852678
INFO:root:current train perplexity4.198452472686768
INFO:root:current mean train loss 1813.7834330610794
INFO:root:current train perplexity4.186244487762451
INFO:root:current mean train loss 1813.0685
INFO:root:current train perplexity4.1952290534973145
INFO:root:current mean train loss 1815.1094292249177
INFO:root:current train perplexity4.200921535491943
INFO:root:current mean train loss 1818.4072730553669
INFO:root:current train perplexity4.209340572357178
INFO:root:current mean train loss 1818.721925998264
INFO:root:current train perplexity4.210707187652588
INFO:root:current mean train loss 1820.3177787928428
INFO:root:current train perplexity4.211821556091309
INFO:root:current mean train loss 1821.1529522879464
INFO:root:current train perplexity4.209081649780273
INFO:root:current mean train loss 1821.611308969351
INFO:root:current train perplexity4.2116007804870605
INFO:root:current mean train loss 1821.8782721656976
INFO:root:current train perplexity4.211024761199951
INFO:root:current mean train loss 1823.1771445935838
INFO:root:current train perplexity4.212738037109375
INFO:root:current mean train loss 1822.965423560049
INFO:root:current train perplexity4.215746879577637
INFO:root:current mean train loss 1823.3585531782671
INFO:root:current train perplexity4.2178955078125
INFO:root:current mean train loss 1824.3381739108845
INFO:root:current train perplexity4.220968246459961
INFO:root:current mean train loss 1824.002521623884
INFO:root:current train perplexity4.219552993774414
INFO:root:current mean train loss 1824.2013754226912
INFO:root:current train perplexity4.222540855407715
INFO:root:current mean train loss 1824.832021484375
INFO:root:current train perplexity4.224067687988281
INFO:root:current mean train loss 1825.9993251953124
INFO:root:current train perplexity4.225704193115234
INFO:root:current mean train loss 1826.9268206091772
INFO:root:current train perplexity4.227001190185547


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.41s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.83s/it]
INFO:root:eval mean loss: 2855.8011060376784
INFO:root:eval perplexity: 10.545259475708008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/29

 14%|â–ˆâ–        | 29/200 [4:16:11<24:38:48, 518.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1793.7430380116339
INFO:root:current train perplexity4.113790988922119
INFO:root:current mean train loss 1797.2074813842773
INFO:root:current train perplexity4.134455680847168
INFO:root:current mean train loss 1806.085503147073
INFO:root:current train perplexity4.150478363037109
INFO:root:current mean train loss 1809.4978656379544
INFO:root:current train perplexity4.16080904006958
INFO:root:current mean train loss 1811.5452272988916
INFO:root:current train perplexity4.16365385055542
INFO:root:current mean train loss 1814.394727964659
INFO:root:current train perplexity4.167938232421875
INFO:root:current mean train loss 1813.8771150622065
INFO:root:current train perplexity4.167308330535889
INFO:root:current mean train loss 1810.1320238209735
INFO:root:current train perplexity4.163025856018066
INFO:root:current mean train loss 1811.210719224049
INFO:root:current train perplexity4.167466163635254
INFO:root:current mean train loss 1811.6708968377882
INFO:root:current train perplexity4.171536922454834
INFO:root:current mean train loss 1812.1793073158124
INFO:root:current train perplexity4.173271179199219
INFO:root:current mean train loss 1812.5055917829475
INFO:root:current train perplexity4.174203872680664
INFO:root:current mean train loss 1814.0163894511597
INFO:root:current train perplexity4.176962852478027
INFO:root:current mean train loss 1814.5023557290262
INFO:root:current train perplexity4.179636478424072
INFO:root:current mean train loss 1814.117435976903
INFO:root:current train perplexity4.180750370025635
INFO:root:current mean train loss 1813.9384743388573
INFO:root:current train perplexity4.182291507720947
INFO:root:current mean train loss 1813.1825943173528
INFO:root:current train perplexity4.183033466339111
INFO:root:current mean train loss 1813.6562756129674
INFO:root:current train perplexity4.183408737182617
INFO:root:current mean train loss 1813.6695708260727
INFO:root:current train perplexity4.18386697769165


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.34s/it]
INFO:root:eval mean loss: 2855.9709355351447
INFO:root:eval perplexity: 10.546735763549805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/30

 15%|â–ˆâ–Œ        | 30/200 [4:24:48<24:28:46, 518.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1806.5128716362847
INFO:root:current train perplexity4.134418487548828
INFO:root:current mean train loss 1796.7426231454272
INFO:root:current train perplexity4.121304512023926
INFO:root:current mean train loss 1796.5918173173968
INFO:root:current train perplexity4.1133575439453125
INFO:root:current mean train loss 1793.9294757534385
INFO:root:current train perplexity4.11994743347168
INFO:root:current mean train loss 1794.8426158503974
INFO:root:current train perplexity4.128304481506348
INFO:root:current mean train loss 1791.8007539100872
INFO:root:current train perplexity4.120782375335693
INFO:root:current mean train loss 1793.0203767222138
INFO:root:current train perplexity4.122017860412598
INFO:root:current mean train loss 1795.2697530081982
INFO:root:current train perplexity4.124879837036133
INFO:root:current mean train loss 1795.4298777727615
INFO:root:current train perplexity4.128819942474365
INFO:root:current mean train loss 1794.7955253777332
INFO:root:current train perplexity4.128311634063721
INFO:root:current mean train loss 1795.398801049345
INFO:root:current train perplexity4.129712104797363
INFO:root:current mean train loss 1796.211979225372
INFO:root:current train perplexity4.132437229156494
INFO:root:current mean train loss 1798.4225699627468
INFO:root:current train perplexity4.133866310119629
INFO:root:current mean train loss 1798.8975667815066
INFO:root:current train perplexity4.133647441864014
INFO:root:current mean train loss 1798.58190554097
INFO:root:current train perplexity4.133484363555908
INFO:root:current mean train loss 1798.5931646449428
INFO:root:current train perplexity4.134288311004639
INFO:root:current mean train loss 1799.8745471487289
INFO:root:current train perplexity4.1390180587768555
INFO:root:current mean train loss 1800.7000866706362
INFO:root:current train perplexity4.14285945892334
INFO:root:current mean train loss 1801.993418864877
INFO:root:current train perplexity4.145625114440918
INFO:root:current mean train loss 1802.1281653874328
INFO:root:current train perplexity4.146425724029541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.23s/it]
INFO:root:eval mean loss: 2862.7905104811844
INFO:root:eval perplexity: 10.606230735778809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/31

 16%|â–ˆâ–Œ        | 31/200 [4:33:23<24:17:18, 517.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1790.936283991887
INFO:root:current train perplexity4.1019744873046875
INFO:root:current mean train loss 1781.886454264323
INFO:root:current train perplexity4.073237419128418
INFO:root:current mean train loss 1774.7996550703472
INFO:root:current train perplexity4.070215702056885
INFO:root:current mean train loss 1773.8906257488977
INFO:root:current train perplexity4.0705413818359375
INFO:root:current mean train loss 1780.8708229602223
INFO:root:current train perplexity4.07809591293335
INFO:root:current mean train loss 1784.6099468274715
INFO:root:current train perplexity4.089367866516113
INFO:root:current mean train loss 1784.8261125948482
INFO:root:current train perplexity4.084465503692627
INFO:root:current mean train loss 1783.1212380149148
INFO:root:current train perplexity4.080636978149414
INFO:root:current mean train loss 1784.8692799861437
INFO:root:current train perplexity4.085423469543457
INFO:root:current mean train loss 1784.8606156087592
INFO:root:current train perplexity4.087560653686523
INFO:root:current mean train loss 1784.9894126014635
INFO:root:current train perplexity4.087526321411133
INFO:root:current mean train loss 1786.8112816819078
INFO:root:current train perplexity4.090458393096924
INFO:root:current mean train loss 1787.271141064692
INFO:root:current train perplexity4.0911335945129395
INFO:root:current mean train loss 1786.987906569629
INFO:root:current train perplexity4.091805934906006
INFO:root:current mean train loss 1787.8028950524028
INFO:root:current train perplexity4.094301700592041
INFO:root:current mean train loss 1788.5423445595356
INFO:root:current train perplexity4.096897602081299
INFO:root:current mean train loss 1789.1009950907614
INFO:root:current train perplexity4.100268840789795
INFO:root:current mean train loss 1789.3550413624575
INFO:root:current train perplexity4.102834701538086
INFO:root:current mean train loss 1789.8834693131546
INFO:root:current train perplexity4.1031928062438965
INFO:root:current mean train loss 1789.6235627266476
INFO:root:current train perplexity4.104426383972168


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.98s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.41s/it]
INFO:root:eval mean loss: 2859.6149073878564
INFO:root:eval perplexity: 10.578483581542969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/32

 16%|â–ˆâ–Œ        | 32/200 [4:42:11<24:17:13, 520.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1731.2899624136992
INFO:root:current train perplexity4.000935077667236
INFO:root:current mean train loss 1759.2283722137238
INFO:root:current train perplexity4.021411418914795
INFO:root:current mean train loss 1760.5698985661008
INFO:root:current train perplexity4.027958393096924
INFO:root:current mean train loss 1766.7394055069014
INFO:root:current train perplexity4.033117294311523
INFO:root:current mean train loss 1767.134814673568
INFO:root:current train perplexity4.032931327819824
INFO:root:current mean train loss 1769.4479004805478
INFO:root:current train perplexity4.038040637969971
INFO:root:current mean train loss 1773.137108501713
INFO:root:current train perplexity4.046089172363281
INFO:root:current mean train loss 1773.4996438109858
INFO:root:current train perplexity4.050854206085205
INFO:root:current mean train loss 1774.5199417711576
INFO:root:current train perplexity4.053051948547363
INFO:root:current mean train loss 1774.674143333071
INFO:root:current train perplexity4.053518772125244
INFO:root:current mean train loss 1775.1806859485484
INFO:root:current train perplexity4.058218955993652
INFO:root:current mean train loss 1776.1757051029022
INFO:root:current train perplexity4.057913780212402
INFO:root:current mean train loss 1777.5047575013828
INFO:root:current train perplexity4.061432361602783
INFO:root:current mean train loss 1777.3383587278365
INFO:root:current train perplexity4.0599260330200195
INFO:root:current mean train loss 1777.6171721037445
INFO:root:current train perplexity4.060268878936768
INFO:root:current mean train loss 1777.6795498445601
INFO:root:current train perplexity4.062235355377197
INFO:root:current mean train loss 1777.4117997042376
INFO:root:current train perplexity4.064295291900635
INFO:root:current mean train loss 1778.4318021948284
INFO:root:current train perplexity4.068234443664551
INFO:root:current mean train loss 1778.231636465469
INFO:root:current train perplexity4.067582607269287
INFO:root:current mean train loss 1778.6763000017088
INFO:root:current train perplexity4.0691070556640625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.61s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.30s/it]
INFO:root:eval mean loss: 2868.86926892713
INFO:root:eval perplexity: 10.6595458984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/33

 16%|â–ˆâ–‹        | 33/200 [4:50:42<24:00:49, 517.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1750.681376139323
INFO:root:current train perplexity4.027525424957275
INFO:root:current mean train loss 1750.0360626220704
INFO:root:current train perplexity4.005016803741455
INFO:root:current mean train loss 1757.6498014009917
INFO:root:current train perplexity4.001663684844971
INFO:root:current mean train loss 1763.3569529215495
INFO:root:current train perplexity4.013210773468018
INFO:root:current mean train loss 1761.5276157545006
INFO:root:current train perplexity4.008111000061035
INFO:root:current mean train loss 1763.379046848842
INFO:root:current train perplexity4.0108962059021
INFO:root:current mean train loss 1765.0866837935014
INFO:root:current train perplexity4.015334606170654
INFO:root:current mean train loss 1764.5071713096218
INFO:root:current train perplexity4.015876770019531
INFO:root:current mean train loss 1763.5660890091297
INFO:root:current train perplexity4.016268253326416
INFO:root:current mean train loss 1763.3020851135254
INFO:root:current train perplexity4.016895771026611
INFO:root:current mean train loss 1761.6822826457474
INFO:root:current train perplexity4.017698764801025
INFO:root:current mean train loss 1764.1370361328125
INFO:root:current train perplexity4.019528865814209
INFO:root:current mean train loss 1763.8079750666543
INFO:root:current train perplexity4.018856525421143
INFO:root:current mean train loss 1763.8412551879883
INFO:root:current train perplexity4.021936416625977
INFO:root:current mean train loss 1764.3360946864298
INFO:root:current train perplexity4.023746967315674
INFO:root:current mean train loss 1765.9328431740785
INFO:root:current train perplexity4.025406837463379
INFO:root:current mean train loss 1765.9938386848173
INFO:root:current train perplexity4.027196884155273
INFO:root:current mean train loss 1766.3008418690074
INFO:root:current train perplexity4.029469966888428
INFO:root:current mean train loss 1767.0841976042716
INFO:root:current train perplexity4.031037330627441
INFO:root:current mean train loss 1767.2549367476483
INFO:root:current train perplexity4.0322265625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.19s/it]
INFO:root:eval mean loss: 2873.7210359773835
INFO:root:eval perplexity: 10.702291488647461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/34

 17%|â–ˆâ–‹        | 34/200 [4:59:25<23:56:39, 519.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1744.343786462561
INFO:root:current train perplexity3.949643850326538
INFO:root:current mean train loss 1744.9921785343838
INFO:root:current train perplexity3.968376398086548
INFO:root:current mean train loss 1745.1990173560187
INFO:root:current train perplexity3.9670042991638184
INFO:root:current mean train loss 1744.8894836263885
INFO:root:current train perplexity3.9626762866973877
INFO:root:current mean train loss 1743.6475569047268
INFO:root:current train perplexity3.9661474227905273
INFO:root:current mean train loss 1745.2386918886075
INFO:root:current train perplexity3.963949203491211
INFO:root:current mean train loss 1744.4623668225513
INFO:root:current train perplexity3.9627552032470703
INFO:root:current mean train loss 1747.33428680145
INFO:root:current train perplexity3.966688394546509
INFO:root:current mean train loss 1746.512179332285
INFO:root:current train perplexity3.966571569442749
INFO:root:current mean train loss 1747.2567588470365
INFO:root:current train perplexity3.970301389694214
INFO:root:current mean train loss 1749.2235148225323
INFO:root:current train perplexity3.977100372314453
INFO:root:current mean train loss 1751.6307677963373
INFO:root:current train perplexity3.9824624061584473
INFO:root:current mean train loss 1752.9683677296948
INFO:root:current train perplexity3.983036756515503
INFO:root:current mean train loss 1753.2179324746392
INFO:root:current train perplexity3.985830307006836
INFO:root:current mean train loss 1753.6812921832684
INFO:root:current train perplexity3.988083600997925
INFO:root:current mean train loss 1754.416044110653
INFO:root:current train perplexity3.9912586212158203
INFO:root:current mean train loss 1755.1665674526917
INFO:root:current train perplexity3.9921512603759766
INFO:root:current mean train loss 1755.3636627111398
INFO:root:current train perplexity3.994306802749634
INFO:root:current mean train loss 1756.2131741767116
INFO:root:current train perplexity3.9964492321014404
INFO:root:current mean train loss 1756.1823773072956
INFO:root:current train perplexity3.996729850769043


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.97s/it]
INFO:root:eval mean loss: 2881.5938372454484
INFO:root:eval perplexity: 10.772021293640137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/35

 18%|â–ˆâ–Š        | 35/200 [5:07:51<23:37:26, 515.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1750.0937331179355
INFO:root:current train perplexity3.9525229930877686
INFO:root:current mean train loss 1734.246829318017
INFO:root:current train perplexity3.9483444690704346
INFO:root:current mean train loss 1740.9292133357249
INFO:root:current train perplexity3.952789306640625
INFO:root:current mean train loss 1738.1490961839704
INFO:root:current train perplexity3.9535787105560303
INFO:root:current mean train loss 1738.5199345268218
INFO:root:current train perplexity3.953613042831421
INFO:root:current mean train loss 1739.3464092421611
INFO:root:current train perplexity3.9505715370178223
INFO:root:current mean train loss 1739.596168715947
INFO:root:current train perplexity3.9487946033477783
INFO:root:current mean train loss 1738.2994354017435
INFO:root:current train perplexity3.9426205158233643
INFO:root:current mean train loss 1740.2595128821047
INFO:root:current train perplexity3.9459519386291504
INFO:root:current mean train loss 1740.0891791176748
INFO:root:current train perplexity3.945594310760498
INFO:root:current mean train loss 1740.4981498648524
INFO:root:current train perplexity3.9462010860443115
INFO:root:current mean train loss 1740.23892472617
INFO:root:current train perplexity3.9468109607696533
INFO:root:current mean train loss 1741.2142166066944
INFO:root:current train perplexity3.948723316192627
INFO:root:current mean train loss 1741.4526914489834
INFO:root:current train perplexity3.9482150077819824
INFO:root:current mean train loss 1742.5222551991822
INFO:root:current train perplexity3.951385736465454
INFO:root:current mean train loss 1743.2057320512224
INFO:root:current train perplexity3.954374313354492
INFO:root:current mean train loss 1743.9007244808192
INFO:root:current train perplexity3.9571533203125
INFO:root:current mean train loss 1744.385234309678
INFO:root:current train perplexity3.9585838317871094
INFO:root:current mean train loss 1744.4689438043451
INFO:root:current train perplexity3.959951639175415


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.81s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.79s/it]
INFO:root:eval mean loss: 2890.3213777742585
INFO:root:eval perplexity: 10.849846839904785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/36

 18%|â–ˆâ–Š        | 36/200 [5:16:40<23:39:45, 519.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.520663174716
INFO:root:current train perplexity3.916398048400879
INFO:root:current mean train loss 1708.767079946157
INFO:root:current train perplexity3.8944296836853027
INFO:root:current mean train loss 1720.2951712224155
INFO:root:current train perplexity3.8926784992218018
INFO:root:current mean train loss 1717.515002088148
INFO:root:current train perplexity3.8985865116119385
INFO:root:current mean train loss 1717.4184433688793
INFO:root:current train perplexity3.890974521636963
INFO:root:current mean train loss 1718.8583413439487
INFO:root:current train perplexity3.8933629989624023
INFO:root:current mean train loss 1719.8614310156888
INFO:root:current train perplexity3.8939316272735596
INFO:root:current mean train loss 1721.6347760979804
INFO:root:current train perplexity3.896555185317993
INFO:root:current mean train loss 1722.4673134536838
INFO:root:current train perplexity3.8984429836273193
INFO:root:current mean train loss 1723.1305191592771
INFO:root:current train perplexity3.902942657470703
INFO:root:current mean train loss 1726.0188841928243
INFO:root:current train perplexity3.9101102352142334
INFO:root:current mean train loss 1727.0319596779013
INFO:root:current train perplexity3.910950183868408
INFO:root:current mean train loss 1727.2778621708235
INFO:root:current train perplexity3.9118785858154297
INFO:root:current mean train loss 1728.4947131729416
INFO:root:current train perplexity3.91312575340271
INFO:root:current mean train loss 1729.1244014142508
INFO:root:current train perplexity3.916339874267578
INFO:root:current mean train loss 1729.176497837473
INFO:root:current train perplexity3.9164793491363525
INFO:root:current mean train loss 1729.7569305779843
INFO:root:current train perplexity3.9190685749053955
INFO:root:current mean train loss 1730.1071506948376
INFO:root:current train perplexity3.920985460281372
INFO:root:current mean train loss 1731.2355163675327
INFO:root:current train perplexity3.9221010208129883
INFO:root:current mean train loss 1732.532419664831
INFO:root:current train perplexity3.9250237941741943


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.47s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.85s/it]
INFO:root:eval mean loss: 2897.1791713588586
INFO:root:eval perplexity: 10.911395072937012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/37

 18%|â–ˆâ–Š        | 37/200 [5:25:04<23:18:11, 514.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1703.2956760951452
INFO:root:current train perplexity3.9231441020965576
INFO:root:current mean train loss 1708.9331140518188
INFO:root:current train perplexity3.8670709133148193
INFO:root:current mean train loss 1716.6377493875068
INFO:root:current train perplexity3.87500262260437
INFO:root:current mean train loss 1712.2250001488662
INFO:root:current train perplexity3.864920139312744
INFO:root:current mean train loss 1714.1575009354922
INFO:root:current train perplexity3.8686232566833496
INFO:root:current mean train loss 1712.8692238547585
INFO:root:current train perplexity3.8744423389434814
INFO:root:current mean train loss 1713.2969745222929
INFO:root:current train perplexity3.872621536254883
INFO:root:current mean train loss 1715.0001975258629
INFO:root:current train perplexity3.876915454864502
INFO:root:current mean train loss 1716.8030889391323
INFO:root:current train perplexity3.880992889404297
INFO:root:current mean train loss 1717.2648087863265
INFO:root:current train perplexity3.880250930786133
INFO:root:current mean train loss 1717.9490834989435
INFO:root:current train perplexity3.882301092147827
INFO:root:current mean train loss 1719.170558415406
INFO:root:current train perplexity3.8826041221618652
INFO:root:current mean train loss 1720.1225195272739
INFO:root:current train perplexity3.885061264038086
INFO:root:current mean train loss 1720.6044517425169
INFO:root:current train perplexity3.88627028465271
INFO:root:current mean train loss 1721.1182469814098
INFO:root:current train perplexity3.8871917724609375
INFO:root:current mean train loss 1722.4172323336777
INFO:root:current train perplexity3.889925479888916
INFO:root:current mean train loss 1722.7907901548344
INFO:root:current train perplexity3.8915445804595947
INFO:root:current mean train loss 1724.3612855981899
INFO:root:current train perplexity3.8934919834136963
INFO:root:current mean train loss 1724.1562554758018
INFO:root:current train perplexity3.895056962966919
INFO:root:current mean train loss 1723.7020184528778
INFO:root:current train perplexity3.895315647125244


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.42s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.17s/it]
INFO:root:eval mean loss: 2906.1915147569443
INFO:root:eval perplexity: 10.992814064025879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/38

 19%|â–ˆâ–‰        | 38/200 [5:33:28<23:01:28, 511.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1751.4660319010416
INFO:root:current train perplexity3.8227791786193848
INFO:root:current mean train loss 1716.4678382610452
INFO:root:current train perplexity3.8058533668518066
INFO:root:current mean train loss 1709.4038972815688
INFO:root:current train perplexity3.812333583831787
INFO:root:current mean train loss 1708.6458287335824
INFO:root:current train perplexity3.824842691421509
INFO:root:current mean train loss 1709.3560011960149
INFO:root:current train perplexity3.832155466079712
INFO:root:current mean train loss 1709.9045605020785
INFO:root:current train perplexity3.838320255279541
INFO:root:current mean train loss 1706.4412162366764
INFO:root:current train perplexity3.834397315979004
INFO:root:current mean train loss 1707.120717216338
INFO:root:current train perplexity3.839906930923462
INFO:root:current mean train loss 1705.479120631472
INFO:root:current train perplexity3.844724178314209
INFO:root:current mean train loss 1705.7706752232143
INFO:root:current train perplexity3.8462815284729004
INFO:root:current mean train loss 1706.859802888569
INFO:root:current train perplexity3.8459315299987793
INFO:root:current mean train loss 1707.0618728038005
INFO:root:current train perplexity3.8468379974365234
INFO:root:current mean train loss 1708.2453977040977
INFO:root:current train perplexity3.8472609519958496
INFO:root:current mean train loss 1708.1560366265392
INFO:root:current train perplexity3.8488457202911377
INFO:root:current mean train loss 1709.0094596466804
INFO:root:current train perplexity3.8504443168640137
INFO:root:current mean train loss 1710.7862947038077
INFO:root:current train perplexity3.8532280921936035
INFO:root:current mean train loss 1710.9323820259071
INFO:root:current train perplexity3.8556647300720215
INFO:root:current mean train loss 1711.9750294507746
INFO:root:current train perplexity3.856626510620117
INFO:root:current mean train loss 1712.459144422743
INFO:root:current train perplexity3.8586959838867188
INFO:root:current mean train loss 1712.3272500476985
INFO:root:current train perplexity3.8612139225006104


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.56s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.81s/it]
INFO:root:eval mean loss: 2914.5107084623687
INFO:root:eval perplexity: 11.06850814819336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/39

 20%|â–ˆâ–‰        | 39/200 [5:41:46<22:41:27, 507.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.5127740675402
INFO:root:current train perplexity3.803109884262085
INFO:root:current mean train loss 1702.1847715024594
INFO:root:current train perplexity3.799142837524414
INFO:root:current mean train loss 1701.247168621034
INFO:root:current train perplexity3.801344871520996
INFO:root:current mean train loss 1701.2528942803651
INFO:root:current train perplexity3.8092453479766846
INFO:root:current mean train loss 1698.7003649426745
INFO:root:current train perplexity3.8061776161193848
INFO:root:current mean train loss 1697.0916281051907
INFO:root:current train perplexity3.7989249229431152
INFO:root:current mean train loss 1696.5086022691064
INFO:root:current train perplexity3.8007917404174805
INFO:root:current mean train loss 1696.1772262292898
INFO:root:current train perplexity3.8012750148773193
INFO:root:current mean train loss 1697.7988864695112
INFO:root:current train perplexity3.806426763534546
INFO:root:current mean train loss 1698.7579947172233
INFO:root:current train perplexity3.811133861541748
INFO:root:current mean train loss 1699.2093021945989
INFO:root:current train perplexity3.8092851638793945
INFO:root:current mean train loss 1699.0874370108784
INFO:root:current train perplexity3.809537649154663
INFO:root:current mean train loss 1699.2082868718119
INFO:root:current train perplexity3.8121843338012695
INFO:root:current mean train loss 1699.601307693907
INFO:root:current train perplexity3.817575693130493
INFO:root:current mean train loss 1700.195555471689
INFO:root:current train perplexity3.8199446201324463
INFO:root:current mean train loss 1700.1888907575424
INFO:root:current train perplexity3.8223187923431396
INFO:root:current mean train loss 1700.2841168161476
INFO:root:current train perplexity3.825265884399414
INFO:root:current mean train loss 1701.0150962595771
INFO:root:current train perplexity3.827228546142578
INFO:root:current mean train loss 1702.1545422612405
INFO:root:current train perplexity3.829167127609253
INFO:root:current mean train loss 1701.93877557877
INFO:root:current train perplexity3.8295884132385254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.78s/it]
INFO:root:eval mean loss: 2919.2703993055557
INFO:root:eval perplexity: 11.112054824829102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/40

 20%|â–ˆâ–ˆ        | 40/200 [5:50:05<22:26:21, 504.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1691.1534300212618
INFO:root:current train perplexity3.7664942741394043
INFO:root:current mean train loss 1682.3321758248953
INFO:root:current train perplexity3.776477575302124
INFO:root:current mean train loss 1682.172617047491
INFO:root:current train perplexity3.769951581954956
INFO:root:current mean train loss 1681.6071303878423
INFO:root:current train perplexity3.774789333343506
INFO:root:current mean train loss 1684.019364072286
INFO:root:current train perplexity3.771399736404419
INFO:root:current mean train loss 1680.899372318248
INFO:root:current train perplexity3.773776054382324
INFO:root:current mean train loss 1680.8931395765258
INFO:root:current train perplexity3.773463010787964
INFO:root:current mean train loss 1680.7546908534077
INFO:root:current train perplexity3.773303747177124
INFO:root:current mean train loss 1683.7641679331982
INFO:root:current train perplexity3.7767951488494873
INFO:root:current mean train loss 1683.1746240134623
INFO:root:current train perplexity3.7799575328826904
INFO:root:current mean train loss 1685.477938308221
INFO:root:current train perplexity3.7842626571655273
INFO:root:current mean train loss 1686.1156512156092
INFO:root:current train perplexity3.7886171340942383
INFO:root:current mean train loss 1688.0870641927593
INFO:root:current train perplexity3.7915942668914795
INFO:root:current mean train loss 1688.2795288882626
INFO:root:current train perplexity3.790473461151123
INFO:root:current mean train loss 1689.3324232946143
INFO:root:current train perplexity3.7929906845092773
INFO:root:current mean train loss 1689.7986410381372
INFO:root:current train perplexity3.794390916824341
INFO:root:current mean train loss 1689.6262742861031
INFO:root:current train perplexity3.79484486579895
INFO:root:current mean train loss 1690.789265058495
INFO:root:current train perplexity3.7967865467071533
INFO:root:current mean train loss 1692.199578009621
INFO:root:current train perplexity3.7999789714813232
INFO:root:current mean train loss 1692.8020185458051
INFO:root:current train perplexity3.8021175861358643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.61s/it]
INFO:root:eval mean loss: 2925.9178895692567
INFO:root:eval perplexity: 11.1731538772583
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/41

 20%|â–ˆâ–ˆ        | 41/200 [5:58:23<22:12:47, 502.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1661.8328895568848
INFO:root:current train perplexity3.7058217525482178
INFO:root:current mean train loss 1666.4605121223294
INFO:root:current train perplexity3.71429705619812
INFO:root:current mean train loss 1665.7734189420132
INFO:root:current train perplexity3.7212331295013428
INFO:root:current mean train loss 1671.042879355074
INFO:root:current train perplexity3.733253240585327
INFO:root:current mean train loss 1675.9002328688098
INFO:root:current train perplexity3.736783742904663
INFO:root:current mean train loss 1676.6800029165793
INFO:root:current train perplexity3.7404119968414307
INFO:root:current mean train loss 1678.595569303666
INFO:root:current train perplexity3.74542498588562
INFO:root:current mean train loss 1678.9009850276775
INFO:root:current train perplexity3.7464516162872314
INFO:root:current mean train loss 1681.2795857020787
INFO:root:current train perplexity3.7494003772735596
INFO:root:current mean train loss 1680.9044740975621
INFO:root:current train perplexity3.7511277198791504
INFO:root:current mean train loss 1681.0482272405693
INFO:root:current train perplexity3.757033586502075
INFO:root:current mean train loss 1680.9729479531381
INFO:root:current train perplexity3.7593231201171875
INFO:root:current mean train loss 1681.5215643423576
INFO:root:current train perplexity3.761298894882202
INFO:root:current mean train loss 1681.7837248968874
INFO:root:current train perplexity3.762956142425537
INFO:root:current mean train loss 1680.8932335185496
INFO:root:current train perplexity3.7629354000091553
INFO:root:current mean train loss 1681.154701557972
INFO:root:current train perplexity3.7654316425323486
INFO:root:current mean train loss 1680.5867748620376
INFO:root:current train perplexity3.7662556171417236
INFO:root:current mean train loss 1681.663179937078
INFO:root:current train perplexity3.7678048610687256
INFO:root:current mean train loss 1681.5199763945889
INFO:root:current train perplexity3.7688937187194824


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.46s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.55s/it]
INFO:root:eval mean loss: 2933.292797191723
INFO:root:eval perplexity: 11.24133014678955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/42

 21%|â–ˆâ–ˆ        | 42/200 [6:06:44<22:02:58, 502.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1664.5641714242788
INFO:root:current train perplexity3.708401679992676
INFO:root:current mean train loss 1649.6363406561118
INFO:root:current train perplexity3.7027275562286377
INFO:root:current mean train loss 1656.5232890670848
INFO:root:current train perplexity3.6999337673187256
INFO:root:current mean train loss 1664.4859478740266
INFO:root:current train perplexity3.7127292156219482
INFO:root:current mean train loss 1662.4386651360094
INFO:root:current train perplexity3.710211753845215
INFO:root:current mean train loss 1662.9422547933418
INFO:root:current train perplexity3.711860418319702
INFO:root:current mean train loss 1662.1077598086383
INFO:root:current train perplexity3.7133638858795166
INFO:root:current mean train loss 1664.3198704445344
INFO:root:current train perplexity3.7185444831848145
INFO:root:current mean train loss 1666.6600660110605
INFO:root:current train perplexity3.7206315994262695
INFO:root:current mean train loss 1666.8786728055688
INFO:root:current train perplexity3.7211124897003174
INFO:root:current mean train loss 1668.3455559899046
INFO:root:current train perplexity3.7279903888702393
INFO:root:current mean train loss 1670.0525243438694
INFO:root:current train perplexity3.7294580936431885
INFO:root:current mean train loss 1669.9150499310852
INFO:root:current train perplexity3.7284486293792725
INFO:root:current mean train loss 1669.8024349169066
INFO:root:current train perplexity3.728503704071045
INFO:root:current mean train loss 1669.9012051182103
INFO:root:current train perplexity3.7314820289611816
INFO:root:current mean train loss 1669.8731501466457
INFO:root:current train perplexity3.7323687076568604
INFO:root:current mean train loss 1671.0286663171303
INFO:root:current train perplexity3.7335164546966553
INFO:root:current mean train loss 1671.91154149507
INFO:root:current train perplexity3.7355852127075195
INFO:root:current mean train loss 1672.1080978065318
INFO:root:current train perplexity3.7364540100097656
INFO:root:current mean train loss 1671.8486464680393
INFO:root:current train perplexity3.738717555999756


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.10s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.64s/it]
INFO:root:eval mean loss: 2938.727562523461
INFO:root:eval perplexity: 11.291834831237793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/43

 22%|â–ˆâ–ˆâ–       | 43/200 [6:15:05<21:53:20, 501.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1645.8057413736979
INFO:root:current train perplexity3.7164127826690674
INFO:root:current mean train loss 1654.4047203650841
INFO:root:current train perplexity3.6893136501312256
INFO:root:current mean train loss 1656.7902741805367
INFO:root:current train perplexity3.698613405227661
INFO:root:current mean train loss 1655.1128454959753
INFO:root:current train perplexity3.70055890083313
INFO:root:current mean train loss 1656.3722730059956
INFO:root:current train perplexity3.700502872467041
INFO:root:current mean train loss 1655.1578309257075
INFO:root:current train perplexity3.698580503463745
INFO:root:current mean train loss 1655.936937313988
INFO:root:current train perplexity3.7021772861480713
INFO:root:current mean train loss 1655.029291356753
INFO:root:current train perplexity3.6987497806549072
INFO:root:current mean train loss 1657.2156851527202
INFO:root:current train perplexity3.698359966278076
INFO:root:current mean train loss 1656.8081453713037
INFO:root:current train perplexity3.69632625579834
INFO:root:current mean train loss 1658.492886974742
INFO:root:current train perplexity3.7008697986602783
INFO:root:current mean train loss 1658.0620941432176
INFO:root:current train perplexity3.698591947555542
INFO:root:current mean train loss 1660.048198420827
INFO:root:current train perplexity3.7041103839874268
INFO:root:current mean train loss 1661.2515857208941
INFO:root:current train perplexity3.705770969390869
INFO:root:current mean train loss 1661.3977823324137
INFO:root:current train perplexity3.7061946392059326
INFO:root:current mean train loss 1661.5876740100337
INFO:root:current train perplexity3.7082862854003906
INFO:root:current mean train loss 1661.2352622939034
INFO:root:current train perplexity3.7104270458221436
INFO:root:current mean train loss 1662.281255150944
INFO:root:current train perplexity3.7108993530273438
INFO:root:current mean train loss 1662.8251911100795
INFO:root:current train perplexity3.7126548290252686
INFO:root:current mean train loss 1662.577581249494
INFO:root:current train perplexity3.7146048545837402


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.95s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.65s/it]
INFO:root:eval mean loss: 2939.179092911271
INFO:root:eval perplexity: 11.296043395996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:23:25<21:43:12, 501.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1617.415358523105
INFO:root:current train perplexity3.6421282291412354
INFO:root:current mean train loss 1631.8136467966092
INFO:root:current train perplexity3.6396799087524414
INFO:root:current mean train loss 1637.0971506713372
INFO:root:current train perplexity3.657254695892334
INFO:root:current mean train loss 1640.927176791584
INFO:root:current train perplexity3.6585047245025635
INFO:root:current mean train loss 1642.7941250043693
INFO:root:current train perplexity3.6636366844177246
INFO:root:current mean train loss 1642.6037195962351
INFO:root:current train perplexity3.6667799949645996
INFO:root:current mean train loss 1641.6810578194431
INFO:root:current train perplexity3.668328285217285
INFO:root:current mean train loss 1643.260267633032
INFO:root:current train perplexity3.6698005199432373
INFO:root:current mean train loss 1642.9759057415417
INFO:root:current train perplexity3.665109872817993
INFO:root:current mean train loss 1643.9886785263498
INFO:root:current train perplexity3.665158748626709
INFO:root:current mean train loss 1646.372166499709
INFO:root:current train perplexity3.667160749435425
INFO:root:current mean train loss 1646.9181220030514
INFO:root:current train perplexity3.6686224937438965
INFO:root:current mean train loss 1647.6690439868185
INFO:root:current train perplexity3.6707730293273926
INFO:root:current mean train loss 1649.1860545497518
INFO:root:current train perplexity3.6714186668395996
INFO:root:current mean train loss 1650.6626230488996
INFO:root:current train perplexity3.6747472286224365
INFO:root:current mean train loss 1650.6245447021959
INFO:root:current train perplexity3.6749229431152344
INFO:root:current mean train loss 1651.7329726366831
INFO:root:current train perplexity3.6779675483703613
INFO:root:current mean train loss 1652.0006216013167
INFO:root:current train perplexity3.6798670291900635
INFO:root:current mean train loss 1651.6263081416866
INFO:root:current train perplexity3.6799156665802
INFO:root:current mean train loss 1652.799618290459
INFO:root:current train perplexity3.6838865280151367


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.01s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.17s/it]
INFO:root:eval mean loss: 2952.7002129082207
INFO:root:eval perplexity: 11.422736167907715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:31:50<21:37:59, 502.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1627.4688129425049
INFO:root:current train perplexity3.6199967861175537
INFO:root:current mean train loss 1631.6160873785252
INFO:root:current train perplexity3.6234018802642822
INFO:root:current mean train loss 1627.6145579020183
INFO:root:current train perplexity3.62196946144104
INFO:root:current mean train loss 1636.81249664642
INFO:root:current train perplexity3.6295359134674072
INFO:root:current mean train loss 1635.4467954964473
INFO:root:current train perplexity3.6325368881225586
INFO:root:current mean train loss 1634.4392849536653
INFO:root:current train perplexity3.633436441421509
INFO:root:current mean train loss 1636.7266413677169
INFO:root:current train perplexity3.635833501815796
INFO:root:current mean train loss 1635.6295191580089
INFO:root:current train perplexity3.6350347995758057
INFO:root:current mean train loss 1636.0911650481048
INFO:root:current train perplexity3.6399919986724854
INFO:root:current mean train loss 1636.0312375903625
INFO:root:current train perplexity3.6404333114624023
INFO:root:current mean train loss 1636.7343429909613
INFO:root:current train perplexity3.6397242546081543
INFO:root:current mean train loss 1637.9399398331789
INFO:root:current train perplexity3.643359422683716
INFO:root:current mean train loss 1638.3251538819904
INFO:root:current train perplexity3.643789768218994
INFO:root:current mean train loss 1639.2543677724352
INFO:root:current train perplexity3.645916700363159
INFO:root:current mean train loss 1640.3176494660925
INFO:root:current train perplexity3.6494014263153076
INFO:root:current mean train loss 1640.3851803050322
INFO:root:current train perplexity3.649467945098877
INFO:root:current mean train loss 1641.5599146622878
INFO:root:current train perplexity3.651853322982788
INFO:root:current mean train loss 1641.9553422646727
INFO:root:current train perplexity3.65236496925354
INFO:root:current mean train loss 1641.8615667646022
INFO:root:current train perplexity3.653902530670166
INFO:root:current mean train loss 1642.7777762170235
INFO:root:current train perplexity3.6554431915283203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 2957.0929801872185
INFO:root:eval perplexity: 11.464200973510742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:40:14<21:30:37, 502.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.0166377314815
INFO:root:current train perplexity3.621973752975464
INFO:root:current mean train loss 1616.9320601152451
INFO:root:current train perplexity3.6131112575531006
INFO:root:current mean train loss 1621.7844564091693
INFO:root:current train perplexity3.608826160430908
INFO:root:current mean train loss 1621.0183505961902
INFO:root:current train perplexity3.5984082221984863
INFO:root:current mean train loss 1620.5878535724728
INFO:root:current train perplexity3.6007239818573
INFO:root:current mean train loss 1619.162611943309
INFO:root:current train perplexity3.5974106788635254
INFO:root:current mean train loss 1620.8975123826979
INFO:root:current train perplexity3.602766513824463
INFO:root:current mean train loss 1623.2973056065441
INFO:root:current train perplexity3.607333183288574
INFO:root:current mean train loss 1626.9764330860705
INFO:root:current train perplexity3.6117234230041504
INFO:root:current mean train loss 1628.2904129883807
INFO:root:current train perplexity3.6127490997314453
INFO:root:current mean train loss 1627.512920820746
INFO:root:current train perplexity3.6150286197662354
INFO:root:current mean train loss 1628.0842672763085
INFO:root:current train perplexity3.6161065101623535
INFO:root:current mean train loss 1628.1865090482595
INFO:root:current train perplexity3.6172921657562256
INFO:root:current mean train loss 1630.002563918526
INFO:root:current train perplexity3.620598554611206
INFO:root:current mean train loss 1630.163088245379
INFO:root:current train perplexity3.6214916706085205
INFO:root:current mean train loss 1630.926440476014
INFO:root:current train perplexity3.6228599548339844
INFO:root:current mean train loss 1631.7173709177248
INFO:root:current train perplexity3.625201463699341
INFO:root:current mean train loss 1631.8794170810158
INFO:root:current train perplexity3.6249120235443115
INFO:root:current mean train loss 1632.5931672294491
INFO:root:current train perplexity3.6272335052490234
INFO:root:current mean train loss 1633.4869962766397
INFO:root:current train perplexity3.628326416015625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.16s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.70s/it]
INFO:root:eval mean loss: 2968.598115938204
INFO:root:eval perplexity: 11.573517799377441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [6:48:41<21:25:24, 504.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1612.5004147899394
INFO:root:current train perplexity3.564352035522461
INFO:root:current mean train loss 1605.0876902570628
INFO:root:current train perplexity3.5650336742401123
INFO:root:current mean train loss 1612.0980949657876
INFO:root:current train perplexity3.5614840984344482
INFO:root:current mean train loss 1614.5633281151854
INFO:root:current train perplexity3.563521385192871
INFO:root:current mean train loss 1616.7663748254738
INFO:root:current train perplexity3.5676252841949463
INFO:root:current mean train loss 1615.380764249974
INFO:root:current train perplexity3.5707759857177734
INFO:root:current mean train loss 1615.5788199963065
INFO:root:current train perplexity3.5739951133728027
INFO:root:current mean train loss 1616.844027947065
INFO:root:current train perplexity3.578183889389038
INFO:root:current mean train loss 1618.9233810322853
INFO:root:current train perplexity3.58309006690979
INFO:root:current mean train loss 1619.5417886554358
INFO:root:current train perplexity3.584990978240967
INFO:root:current mean train loss 1620.7406974194914
INFO:root:current train perplexity3.587946891784668
INFO:root:current mean train loss 1620.7020617247822
INFO:root:current train perplexity3.5888001918792725
INFO:root:current mean train loss 1620.4195003656466
INFO:root:current train perplexity3.5906755924224854
INFO:root:current mean train loss 1621.4054079942607
INFO:root:current train perplexity3.5910918712615967
INFO:root:current mean train loss 1621.7805161928143
INFO:root:current train perplexity3.5935745239257812
INFO:root:current mean train loss 1622.0433364887263
INFO:root:current train perplexity3.595245599746704
INFO:root:current mean train loss 1622.779392849009
INFO:root:current train perplexity3.5988171100616455
INFO:root:current mean train loss 1624.1868279343585
INFO:root:current train perplexity3.6007559299468994
INFO:root:current mean train loss 1624.2377936762175
INFO:root:current train perplexity3.6013598442077637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.37s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.39s/it]
INFO:root:eval mean loss: 2979.7753774282096
INFO:root:eval perplexity: 11.68071460723877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/48

 24%|â–ˆâ–ˆâ–       | 48/200 [6:57:09<21:20:34, 505.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1588.6762939453124
INFO:root:current train perplexity3.4764809608459473
INFO:root:current mean train loss 1614.74609375
INFO:root:current train perplexity3.5234522819519043
INFO:root:current mean train loss 1607.936567723474
INFO:root:current train perplexity3.526249408721924
INFO:root:current mean train loss 1605.5948459201388
INFO:root:current train perplexity3.5319743156433105
INFO:root:current mean train loss 1606.9694927169617
INFO:root:current train perplexity3.538743734359741
INFO:root:current mean train loss 1607.745200621966
INFO:root:current train perplexity3.545515775680542
INFO:root:current mean train loss 1605.4678607723577
INFO:root:current train perplexity3.5456786155700684
INFO:root:current mean train loss 1605.7815499685862
INFO:root:current train perplexity3.5456414222717285
INFO:root:current mean train loss 1606.3904743217984
INFO:root:current train perplexity3.5480265617370605
INFO:root:current mean train loss 1608.2355269968834
INFO:root:current train perplexity3.5508172512054443
INFO:root:current mean train loss 1609.816780999846
INFO:root:current train perplexity3.5511391162872314
INFO:root:current mean train loss 1610.3938134984585
INFO:root:current train perplexity3.555877208709717
INFO:root:current mean train loss 1611.5293345510224
INFO:root:current train perplexity3.558152914047241
INFO:root:current mean train loss 1613.0549228218572
INFO:root:current train perplexity3.5591626167297363
INFO:root:current mean train loss 1613.0076488481393
INFO:root:current train perplexity3.562756061553955
INFO:root:current mean train loss 1612.7962084880362
INFO:root:current train perplexity3.5656116008758545
INFO:root:current mean train loss 1612.5905527404218
INFO:root:current train perplexity3.566477060317993
INFO:root:current mean train loss 1614.1541747335095
INFO:root:current train perplexity3.570000410079956
INFO:root:current mean train loss 1614.5462684820507
INFO:root:current train perplexity3.57330584526062
INFO:root:current mean train loss 1614.7191069042715
INFO:root:current train perplexity3.5751442909240723


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.71s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.71s/it]
INFO:root:eval mean loss: 2982.833323802318
INFO:root:eval perplexity: 11.71021842956543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/49

 24%|â–ˆâ–ˆâ–       | 49/200 [7:05:35<21:12:08, 505.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1609.590389251709
INFO:root:current train perplexity3.5234110355377197
INFO:root:current mean train loss 1598.281631007339
INFO:root:current train perplexity3.5225601196289062
INFO:root:current mean train loss 1597.4405628072805
INFO:root:current train perplexity3.516179084777832
INFO:root:current mean train loss 1597.6980031944183
INFO:root:current train perplexity3.5187246799468994
INFO:root:current mean train loss 1594.6187204431606
INFO:root:current train perplexity3.5136942863464355
INFO:root:current mean train loss 1597.9908965834998
INFO:root:current train perplexity3.517604351043701
INFO:root:current mean train loss 1600.5322466500197
INFO:root:current train perplexity3.526931047439575
INFO:root:current mean train loss 1600.5748988083803
INFO:root:current train perplexity3.5277774333953857
INFO:root:current mean train loss 1601.5007115877593
INFO:root:current train perplexity3.5304501056671143
INFO:root:current mean train loss 1601.3735669835955
INFO:root:current train perplexity3.530761241912842
INFO:root:current mean train loss 1601.7169084179309
INFO:root:current train perplexity3.5358269214630127
INFO:root:current mean train loss 1602.4176182831134
INFO:root:current train perplexity3.537229061126709
INFO:root:current mean train loss 1602.7494021329012
INFO:root:current train perplexity3.538430690765381
INFO:root:current mean train loss 1604.5170817790447
INFO:root:current train perplexity3.5412542819976807
INFO:root:current mean train loss 1605.0807806260093
INFO:root:current train perplexity3.5456578731536865
INFO:root:current mean train loss 1605.858477082016
INFO:root:current train perplexity3.547595500946045
INFO:root:current mean train loss 1605.9820867052265
INFO:root:current train perplexity3.547565460205078
INFO:root:current mean train loss 1605.727999151992
INFO:root:current train perplexity3.5481791496276855
INFO:root:current mean train loss 1605.6646668546584
INFO:root:current train perplexity3.5501110553741455
INFO:root:current mean train loss 1606.8436855529405
INFO:root:current train perplexity3.5515682697296143


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.69s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.21s/it]
INFO:root:eval mean loss: 2995.5751175980668
INFO:root:eval perplexity: 11.833944320678711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [7:14:02<21:04:51, 505.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.2553461814414
INFO:root:current train perplexity3.4710450172424316
INFO:root:current mean train loss 1586.8973478790897
INFO:root:current train perplexity3.473842144012451
INFO:root:current mean train loss 1582.165506753577
INFO:root:current train perplexity3.4804587364196777
INFO:root:current mean train loss 1582.9829423352435
INFO:root:current train perplexity3.4872939586639404
INFO:root:current mean train loss 1585.5021442506786
INFO:root:current train perplexity3.493198871612549
INFO:root:current mean train loss 1588.5772698407616
INFO:root:current train perplexity3.5010735988616943
INFO:root:current mean train loss 1586.885292652759
INFO:root:current train perplexity3.502790927886963
INFO:root:current mean train loss 1588.0064961289531
INFO:root:current train perplexity3.503305435180664
INFO:root:current mean train loss 1589.3022540017207
INFO:root:current train perplexity3.508286237716675
INFO:root:current mean train loss 1590.658260494188
INFO:root:current train perplexity3.510279655456543
INFO:root:current mean train loss 1591.1009853133937
INFO:root:current train perplexity3.51371431350708
INFO:root:current mean train loss 1591.8362601523404
INFO:root:current train perplexity3.517157554626465
INFO:root:current mean train loss 1592.6561431762598
INFO:root:current train perplexity3.5192372798919678
INFO:root:current mean train loss 1593.5411791394956
INFO:root:current train perplexity3.5213186740875244
INFO:root:current mean train loss 1594.4212900060386
INFO:root:current train perplexity3.5229973793029785
INFO:root:current mean train loss 1594.573984617722
INFO:root:current train perplexity3.5241339206695557
INFO:root:current mean train loss 1594.6413476503278
INFO:root:current train perplexity3.5241944789886475
INFO:root:current mean train loss 1594.6183971058238
INFO:root:current train perplexity3.5244767665863037
INFO:root:current mean train loss 1595.9259746875423
INFO:root:current train perplexity3.525334119796753
INFO:root:current mean train loss 1596.7659627758826
INFO:root:current train perplexity3.5266432762145996


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.54s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.71s/it]
INFO:root:eval mean loss: 2997.9047440995682
INFO:root:eval perplexity: 11.856710433959961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:22:30<20:58:16, 506.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.4833614464962
INFO:root:current train perplexity3.45348858833313
INFO:root:current mean train loss 1579.0287674134036
INFO:root:current train perplexity3.475137233734131
INFO:root:current mean train loss 1576.6848291382753
INFO:root:current train perplexity3.478001356124878
INFO:root:current mean train loss 1575.912553964417
INFO:root:current train perplexity3.4798521995544434
INFO:root:current mean train loss 1575.648632131421
INFO:root:current train perplexity3.478752851486206
INFO:root:current mean train loss 1578.4035564732635
INFO:root:current train perplexity3.479588031768799
INFO:root:current mean train loss 1578.7384522584107
INFO:root:current train perplexity3.483497142791748
INFO:root:current mean train loss 1581.2170749594586
INFO:root:current train perplexity3.4888079166412354
INFO:root:current mean train loss 1582.6537923834892
INFO:root:current train perplexity3.4917309284210205
INFO:root:current mean train loss 1583.001704308804
INFO:root:current train perplexity3.493945598602295
INFO:root:current mean train loss 1583.952952658705
INFO:root:current train perplexity3.4918525218963623
INFO:root:current mean train loss 1584.7348704002734
INFO:root:current train perplexity3.4943413734436035
INFO:root:current mean train loss 1584.7332620003024
INFO:root:current train perplexity3.494009017944336
INFO:root:current mean train loss 1583.8887185226597
INFO:root:current train perplexity3.4940576553344727
INFO:root:current mean train loss 1584.9017835255372
INFO:root:current train perplexity3.495227098464966
INFO:root:current mean train loss 1586.2981363932292
INFO:root:current train perplexity3.4964168071746826
INFO:root:current mean train loss 1586.07799325785
INFO:root:current train perplexity3.4975523948669434
INFO:root:current mean train loss 1587.3371552308581
INFO:root:current train perplexity3.499119520187378
INFO:root:current mean train loss 1587.895487140357
INFO:root:current train perplexity3.5009024143218994
INFO:root:current mean train loss 1588.7170718126113
INFO:root:current train perplexity3.5025391578674316


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.52s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.94s/it]
INFO:root:eval mean loss: 3007.58728723841
INFO:root:eval perplexity: 11.951786041259766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:30:59<20:51:14, 507.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.4033909073796
INFO:root:current train perplexity3.4700875282287598
INFO:root:current mean train loss 1563.091958301315
INFO:root:current train perplexity3.46189022064209
INFO:root:current mean train loss 1560.4931135952684
INFO:root:current train perplexity3.4615559577941895
INFO:root:current mean train loss 1561.8994347793937
INFO:root:current train perplexity3.462099075317383
INFO:root:current mean train loss 1564.046218903662
INFO:root:current train perplexity3.4582624435424805
INFO:root:current mean train loss 1566.1514320962424
INFO:root:current train perplexity3.458554267883301
INFO:root:current mean train loss 1567.3461754995767
INFO:root:current train perplexity3.4584081172943115
INFO:root:current mean train loss 1567.7699849212763
INFO:root:current train perplexity3.461925745010376
INFO:root:current mean train loss 1569.1867541683625
INFO:root:current train perplexity3.461885690689087
INFO:root:current mean train loss 1571.6903543093924
INFO:root:current train perplexity3.4642317295074463
INFO:root:current mean train loss 1573.4129077351324
INFO:root:current train perplexity3.465125560760498
INFO:root:current mean train loss 1573.5504247386477
INFO:root:current train perplexity3.4686930179595947
INFO:root:current mean train loss 1574.7068160332838
INFO:root:current train perplexity3.46824312210083
INFO:root:current mean train loss 1574.1668063899529
INFO:root:current train perplexity3.4683353900909424
INFO:root:current mean train loss 1575.4927433602968
INFO:root:current train perplexity3.4694840908050537
INFO:root:current mean train loss 1576.8572644096948
INFO:root:current train perplexity3.47184157371521
INFO:root:current mean train loss 1577.9815274905536
INFO:root:current train perplexity3.4729104042053223
INFO:root:current mean train loss 1577.9432550367183
INFO:root:current train perplexity3.4733989238739014
INFO:root:current mean train loss 1578.3081581735637
INFO:root:current train perplexity3.475461483001709
INFO:root:current mean train loss 1579.455106380307
INFO:root:current train perplexity3.4781808853149414


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.56s/it]
INFO:root:eval mean loss: 3015.9021619275527
INFO:root:eval perplexity: 12.034040451049805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [7:39:17<20:35:42, 504.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1554.8281848144532
INFO:root:current train perplexity3.4068872928619385
INFO:root:current mean train loss 1564.2413336181642
INFO:root:current train perplexity3.423882007598877
INFO:root:current mean train loss 1569.379599609375
INFO:root:current train perplexity3.432053804397583
INFO:root:current mean train loss 1567.1061273193359
INFO:root:current train perplexity3.4311182498931885
INFO:root:current mean train loss 1568.6634868164062
INFO:root:current train perplexity3.438000202178955
INFO:root:current mean train loss 1569.3335951741537
INFO:root:current train perplexity3.4350478649139404
INFO:root:current mean train loss 1569.1170929827008
INFO:root:current train perplexity3.437329053878784
INFO:root:current mean train loss 1569.8936560058594
INFO:root:current train perplexity3.4372520446777344
INFO:root:current mean train loss 1569.1699522569445
INFO:root:current train perplexity3.4389495849609375
INFO:root:current mean train loss 1567.6210021972656
INFO:root:current train perplexity3.4390835762023926
INFO:root:current mean train loss 1566.3190101207385
INFO:root:current train perplexity3.439769983291626
INFO:root:current mean train loss 1566.361925354004
INFO:root:current train perplexity3.4422459602355957
INFO:root:current mean train loss 1567.981685603215
INFO:root:current train perplexity3.4449427127838135
INFO:root:current mean train loss 1568.518077305385
INFO:root:current train perplexity3.446211338043213
INFO:root:current mean train loss 1569.023848876953
INFO:root:current train perplexity3.4468259811401367
INFO:root:current mean train loss 1569.9654300689697
INFO:root:current train perplexity3.4486451148986816
INFO:root:current mean train loss 1569.917977438534
INFO:root:current train perplexity3.4512779712677
INFO:root:current mean train loss 1570.3042398410373
INFO:root:current train perplexity3.4524986743927
INFO:root:current mean train loss 1570.9366769891037
INFO:root:current train perplexity3.4539167881011963


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.02s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.62s/it]
INFO:root:eval mean loss: 3013.832858248874
INFO:root:eval perplexity: 12.013517379760742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [7:47:42<20:28:18, 504.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.2743997012867
INFO:root:current train perplexity3.444899797439575
INFO:root:current mean train loss 1552.9544980301816
INFO:root:current train perplexity3.400937080383301
INFO:root:current mean train loss 1550.9321587206582
INFO:root:current train perplexity3.4106571674346924
INFO:root:current mean train loss 1551.9082347015478
INFO:root:current train perplexity3.410848379135132
INFO:root:current mean train loss 1555.2699838176334
INFO:root:current train perplexity3.410146951675415
INFO:root:current mean train loss 1555.6867397168157
INFO:root:current train perplexity3.40948224067688
INFO:root:current mean train loss 1555.0699466847523
INFO:root:current train perplexity3.4122488498687744
INFO:root:current mean train loss 1555.8799097122342
INFO:root:current train perplexity3.411891460418701
INFO:root:current mean train loss 1557.620935073535
INFO:root:current train perplexity3.412994146347046
INFO:root:current mean train loss 1558.2560859438897
INFO:root:current train perplexity3.415292501449585
INFO:root:current mean train loss 1557.33372475053
INFO:root:current train perplexity3.4149603843688965
INFO:root:current mean train loss 1558.4783182579595
INFO:root:current train perplexity3.417310953140259
INFO:root:current mean train loss 1559.9302350610812
INFO:root:current train perplexity3.4183974266052246
INFO:root:current mean train loss 1560.0702802445187
INFO:root:current train perplexity3.4199724197387695
INFO:root:current mean train loss 1560.3387248726403
INFO:root:current train perplexity3.4217960834503174
INFO:root:current mean train loss 1560.5563806321327
INFO:root:current train perplexity3.425204277038574
INFO:root:current mean train loss 1561.5791086587333
INFO:root:current train perplexity3.426607608795166
INFO:root:current mean train loss 1561.6846269753066
INFO:root:current train perplexity3.426330089569092
INFO:root:current mean train loss 1561.769617310578
INFO:root:current train perplexity3.4279937744140625
INFO:root:current mean train loss 1562.6450089607379
INFO:root:current train perplexity3.4302773475646973


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.83s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.68s/it]
INFO:root:eval mean loss: 3026.761091169294
INFO:root:eval perplexity: 12.142317771911621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [7:56:06<20:19:03, 504.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1530.9982766544117
INFO:root:current train perplexity3.3497912883758545
INFO:root:current mean train loss 1541.8081829014109
INFO:root:current train perplexity3.3882899284362793
INFO:root:current mean train loss 1541.1095899272168
INFO:root:current train perplexity3.393141269683838
INFO:root:current mean train loss 1543.1080311301225
INFO:root:current train perplexity3.3886303901672363
INFO:root:current mean train loss 1546.056419829619
INFO:root:current train perplexity3.3903589248657227
INFO:root:current mean train loss 1546.0900945199116
INFO:root:current train perplexity3.3892242908477783
INFO:root:current mean train loss 1548.169880863996
INFO:root:current train perplexity3.3897573947906494
INFO:root:current mean train loss 1548.0018491822952
INFO:root:current train perplexity3.3888514041900635
INFO:root:current mean train loss 1545.9774275306318
INFO:root:current train perplexity3.386993646621704
INFO:root:current mean train loss 1545.3718542715721
INFO:root:current train perplexity3.3905320167541504
INFO:root:current mean train loss 1546.0986757850278
INFO:root:current train perplexity3.3943727016448975
INFO:root:current mean train loss 1547.2368348136781
INFO:root:current train perplexity3.3964972496032715
INFO:root:current mean train loss 1549.3816844872176
INFO:root:current train perplexity3.399040699005127
INFO:root:current mean train loss 1550.5947190589275
INFO:root:current train perplexity3.401045799255371
INFO:root:current mean train loss 1550.602115221436
INFO:root:current train perplexity3.4029879570007324
INFO:root:current mean train loss 1551.4781911121354
INFO:root:current train perplexity3.4039292335510254
INFO:root:current mean train loss 1552.131754955726
INFO:root:current train perplexity3.4046430587768555
INFO:root:current mean train loss 1552.7538646447206
INFO:root:current train perplexity3.4054203033447266
INFO:root:current mean train loss 1553.3164780678119
INFO:root:current train perplexity3.407027006149292
INFO:root:current mean train loss 1554.110216616105
INFO:root:current train perplexity3.408381462097168


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.83s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 3032.5703902144332
INFO:root:eval perplexity: 12.200640678405762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [8:04:31<20:10:54, 504.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1525.184761795343
INFO:root:current train perplexity3.328773260116577
INFO:root:current mean train loss 1529.5319250245757
INFO:root:current train perplexity3.335658073425293
INFO:root:current mean train loss 1532.9052637107818
INFO:root:current train perplexity3.3466358184814453
INFO:root:current mean train loss 1534.2158836082176
INFO:root:current train perplexity3.3464787006378174
INFO:root:current mean train loss 1537.0573716935455
INFO:root:current train perplexity3.3526294231414795
INFO:root:current mean train loss 1536.2616130738857
INFO:root:current train perplexity3.361330270767212
INFO:root:current mean train loss 1535.759793001752
INFO:root:current train perplexity3.363726854324341
INFO:root:current mean train loss 1538.2564032461926
INFO:root:current train perplexity3.3675880432128906
INFO:root:current mean train loss 1538.5016498857044
INFO:root:current train perplexity3.3702917098999023
INFO:root:current mean train loss 1539.0782565689487
INFO:root:current train perplexity3.370342493057251
INFO:root:current mean train loss 1539.3829113409477
INFO:root:current train perplexity3.371884822845459
INFO:root:current mean train loss 1539.9153900013914
INFO:root:current train perplexity3.373262882232666
INFO:root:current mean train loss 1541.6373727190123
INFO:root:current train perplexity3.374755620956421
INFO:root:current mean train loss 1542.8521053559864
INFO:root:current train perplexity3.3774123191833496
INFO:root:current mean train loss 1543.6758122092522
INFO:root:current train perplexity3.379307508468628
INFO:root:current mean train loss 1543.8577856461054
INFO:root:current train perplexity3.3803300857543945
INFO:root:current mean train loss 1544.1499007910688
INFO:root:current train perplexity3.3829922676086426
INFO:root:current mean train loss 1545.229725313214
INFO:root:current train perplexity3.3844714164733887
INFO:root:current mean train loss 1546.5299262196743
INFO:root:current train perplexity3.3863472938537598
INFO:root:current mean train loss 1546.053862289671
INFO:root:current train perplexity3.3864240646362305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.67s/it]
INFO:root:eval mean loss: 3042.806342964058
INFO:root:eval perplexity: 12.30409049987793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [8:12:48<19:57:03, 502.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1521.0303596047795
INFO:root:current train perplexity3.3091514110565186
INFO:root:current mean train loss 1523.2822694324311
INFO:root:current train perplexity3.323700189590454
INFO:root:current mean train loss 1527.3114806217934
INFO:root:current train perplexity3.3384199142456055
INFO:root:current mean train loss 1529.8678270422895
INFO:root:current train perplexity3.3365421295166016
INFO:root:current mean train loss 1529.1944517477964
INFO:root:current train perplexity3.338264226913452
INFO:root:current mean train loss 1529.0515098034496
INFO:root:current train perplexity3.341806411743164
INFO:root:current mean train loss 1527.640804085189
INFO:root:current train perplexity3.3434672355651855
INFO:root:current mean train loss 1528.5812695821126
INFO:root:current train perplexity3.34452486038208
INFO:root:current mean train loss 1528.7738056798134
INFO:root:current train perplexity3.3447389602661133
INFO:root:current mean train loss 1528.0319155858567
INFO:root:current train perplexity3.3449413776397705
INFO:root:current mean train loss 1530.2561150597276
INFO:root:current train perplexity3.347841262817383
INFO:root:current mean train loss 1530.839047993699
INFO:root:current train perplexity3.3491437435150146
INFO:root:current mean train loss 1532.5774472594637
INFO:root:current train perplexity3.3512651920318604
INFO:root:current mean train loss 1533.9393997638547
INFO:root:current train perplexity3.3521909713745117
INFO:root:current mean train loss 1534.1087948333989
INFO:root:current train perplexity3.3545191287994385
INFO:root:current mean train loss 1534.3992044877032
INFO:root:current train perplexity3.3559818267822266
INFO:root:current mean train loss 1535.340675921177
INFO:root:current train perplexity3.357973575592041
INFO:root:current mean train loss 1535.8770087747014
INFO:root:current train perplexity3.3599154949188232
INFO:root:current mean train loss 1537.4739962134677
INFO:root:current train perplexity3.3632445335388184
INFO:root:current mean train loss 1538.2863552434658
INFO:root:current train perplexity3.365575075149536


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.65s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 3049.013501049878
INFO:root:eval perplexity: 12.367253303527832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:21:14<19:51:53, 503.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1505.9699649586396
INFO:root:current train perplexity3.303159713745117
INFO:root:current mean train loss 1512.128833667652
INFO:root:current train perplexity3.3029046058654785
INFO:root:current mean train loss 1517.720951548794
INFO:root:current train perplexity3.3141045570373535
INFO:root:current mean train loss 1520.9562382685674
INFO:root:current train perplexity3.324258804321289
INFO:root:current mean train loss 1523.4782188808795
INFO:root:current train perplexity3.3231561183929443
INFO:root:current mean train loss 1527.036381126469
INFO:root:current train perplexity3.325068235397339
INFO:root:current mean train loss 1526.6580243855497
INFO:root:current train perplexity3.3256030082702637
INFO:root:current mean train loss 1528.3561783128482
INFO:root:current train perplexity3.327357530593872
INFO:root:current mean train loss 1527.3626270358845
INFO:root:current train perplexity3.328591823577881
INFO:root:current mean train loss 1526.9824878053616
INFO:root:current train perplexity3.3301384449005127
INFO:root:current mean train loss 1526.6733938472062
INFO:root:current train perplexity3.328923225402832
INFO:root:current mean train loss 1526.9869017009494
INFO:root:current train perplexity3.330348491668701
INFO:root:current mean train loss 1527.6466960268726
INFO:root:current train perplexity3.333449363708496
INFO:root:current mean train loss 1527.285540881656
INFO:root:current train perplexity3.3358747959136963
INFO:root:current mean train loss 1528.1877909958964
INFO:root:current train perplexity3.3377509117126465
INFO:root:current mean train loss 1528.5975696070336
INFO:root:current train perplexity3.3418383598327637
INFO:root:current mean train loss 1528.2954854269055
INFO:root:current train perplexity3.3425071239471436
INFO:root:current mean train loss 1528.7829847661721
INFO:root:current train perplexity3.3442881107330322
INFO:root:current mean train loss 1529.657183497907
INFO:root:current train perplexity3.3444387912750244


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.47s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it]
INFO:root:eval mean loss: 3058.295189476586
INFO:root:eval perplexity: 12.46230411529541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:29:46<19:48:56, 505.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1522.3939208984375
INFO:root:current train perplexity3.541132926940918
INFO:root:current mean train loss 1509.6720090379902
INFO:root:current train perplexity3.2869136333465576
INFO:root:current mean train loss 1504.9166283937964
INFO:root:current train perplexity3.2869348526000977
INFO:root:current mean train loss 1506.9024294417425
INFO:root:current train perplexity3.292914628982544
INFO:root:current mean train loss 1510.3731719818875
INFO:root:current train perplexity3.3068416118621826
INFO:root:current mean train loss 1512.1490162397286
INFO:root:current train perplexity3.302365779876709
INFO:root:current mean train loss 1514.8115051877855
INFO:root:current train perplexity3.304572105407715
INFO:root:current mean train loss 1514.413960426961
INFO:root:current train perplexity3.307170867919922
INFO:root:current mean train loss 1514.750479301015
INFO:root:current train perplexity3.3084335327148438
INFO:root:current mean train loss 1515.9308632725888
INFO:root:current train perplexity3.307823657989502
INFO:root:current mean train loss 1517.163856612946
INFO:root:current train perplexity3.309743881225586
INFO:root:current mean train loss 1517.8083100639108
INFO:root:current train perplexity3.310368061065674
INFO:root:current mean train loss 1517.0643704584156
INFO:root:current train perplexity3.3117024898529053
INFO:root:current mean train loss 1516.9773664577033
INFO:root:current train perplexity3.3131730556488037
INFO:root:current mean train loss 1517.9653573682406
INFO:root:current train perplexity3.3162806034088135
INFO:root:current mean train loss 1518.0921477255586
INFO:root:current train perplexity3.3158137798309326
INFO:root:current mean train loss 1519.3602160812168
INFO:root:current train perplexity3.317293167114258
INFO:root:current mean train loss 1520.0797634102344
INFO:root:current train perplexity3.318009376525879
INFO:root:current mean train loss 1521.0160268430043
INFO:root:current train perplexity3.3209071159362793
INFO:root:current mean train loss 1520.903156204304
INFO:root:current train perplexity3.3210952281951904


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.49s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.20s/it]
INFO:root:eval mean loss: 3069.5724709084084
INFO:root:eval perplexity: 12.578770637512207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [8:38:15<19:42:32, 506.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1497.7764635587994
INFO:root:current train perplexity3.259918212890625
INFO:root:current mean train loss 1506.7297424829308
INFO:root:current train perplexity3.2778987884521484
INFO:root:current mean train loss 1507.8932798899473
INFO:root:current train perplexity3.2801380157470703
INFO:root:current mean train loss 1509.1982349168545
INFO:root:current train perplexity3.2802796363830566
INFO:root:current mean train loss 1508.67298878235
INFO:root:current train perplexity3.2817909717559814
INFO:root:current mean train loss 1506.9144729290854
INFO:root:current train perplexity3.281216621398926
INFO:root:current mean train loss 1505.3672719040287
INFO:root:current train perplexity3.2829320430755615
INFO:root:current mean train loss 1506.2832216307916
INFO:root:current train perplexity3.284590244293213
INFO:root:current mean train loss 1506.6895501552483
INFO:root:current train perplexity3.288012981414795
INFO:root:current mean train loss 1506.9752955722083
INFO:root:current train perplexity3.2852344512939453
INFO:root:current mean train loss 1506.7051882158903
INFO:root:current train perplexity3.2868738174438477
INFO:root:current mean train loss 1508.2128368442457
INFO:root:current train perplexity3.2881438732147217
INFO:root:current mean train loss 1508.9449339718776
INFO:root:current train perplexity3.291186571121216
INFO:root:current mean train loss 1510.135539323114
INFO:root:current train perplexity3.295163869857788
INFO:root:current mean train loss 1511.615165554528
INFO:root:current train perplexity3.2968788146972656
INFO:root:current mean train loss 1511.0360692459317
INFO:root:current train perplexity3.2981984615325928
INFO:root:current mean train loss 1511.7237105484433
INFO:root:current train perplexity3.2984509468078613
INFO:root:current mean train loss 1511.9317630503745
INFO:root:current train perplexity3.2980852127075195
INFO:root:current mean train loss 1512.742455732567
INFO:root:current train perplexity3.299893856048584
INFO:root:current mean train loss 1513.569731982193
INFO:root:current train perplexity3.3015236854553223


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.04s/it]
INFO:root:eval mean loss: 3076.46350574207
INFO:root:eval perplexity: 12.650474548339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [8:46:40<19:33:05, 506.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1482.8326280381943
INFO:root:current train perplexity3.2587931156158447
INFO:root:current mean train loss 1490.079386991613
INFO:root:current train perplexity3.2446413040161133
INFO:root:current mean train loss 1494.5167872542042
INFO:root:current train perplexity3.2473716735839844
INFO:root:current mean train loss 1500.0883291335333
INFO:root:current train perplexity3.2622287273406982
INFO:root:current mean train loss 1497.7167870757776
INFO:root:current train perplexity3.260814666748047
INFO:root:current mean train loss 1497.463007684964
INFO:root:current train perplexity3.26363205909729
INFO:root:current mean train loss 1496.0407536344708
INFO:root:current train perplexity3.2613697052001953
INFO:root:current mean train loss 1496.0905498007069
INFO:root:current train perplexity3.26254940032959
INFO:root:current mean train loss 1497.335666638242
INFO:root:current train perplexity3.264324188232422
INFO:root:current mean train loss 1498.1717919243706
INFO:root:current train perplexity3.263105630874634
INFO:root:current mean train loss 1498.4161930747014
INFO:root:current train perplexity3.2652926445007324
INFO:root:current mean train loss 1499.1502492125605
INFO:root:current train perplexity3.267062187194824
INFO:root:current mean train loss 1499.8970385307632
INFO:root:current train perplexity3.2692742347717285
INFO:root:current mean train loss 1500.020264768315
INFO:root:current train perplexity3.271608591079712
INFO:root:current mean train loss 1501.202701324208
INFO:root:current train perplexity3.2748892307281494
INFO:root:current mean train loss 1502.2144485314686
INFO:root:current train perplexity3.2764434814453125
INFO:root:current mean train loss 1503.253997578889
INFO:root:current train perplexity3.277672052383423
INFO:root:current mean train loss 1504.2803225187663
INFO:root:current train perplexity3.2789011001586914
INFO:root:current mean train loss 1505.0876206873809
INFO:root:current train perplexity3.280590057373047
INFO:root:current mean train loss 1505.6965164310675
INFO:root:current train perplexity3.2809691429138184


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.84s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.46s/it]
INFO:root:eval mean loss: 3077.848582957958
INFO:root:eval perplexity: 12.664941787719727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [8:55:05<19:23:55, 506.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1486.5654158682194
INFO:root:current train perplexity3.2243542671203613
INFO:root:current mean train loss 1482.6749459060968
INFO:root:current train perplexity3.224839925765991
INFO:root:current mean train loss 1484.6138332201087
INFO:root:current train perplexity3.222649097442627
INFO:root:current mean train loss 1484.4927055207595
INFO:root:current train perplexity3.233574390411377
INFO:root:current mean train loss 1486.6586970651388
INFO:root:current train perplexity3.235790491104126
INFO:root:current mean train loss 1490.4785549170717
INFO:root:current train perplexity3.2383291721343994
INFO:root:current mean train loss 1490.2926141291994
INFO:root:current train perplexity3.2401437759399414
INFO:root:current mean train loss 1491.7435646411748
INFO:root:current train perplexity3.242990255355835
INFO:root:current mean train loss 1492.1901283040556
INFO:root:current train perplexity3.2439157962799072
INFO:root:current mean train loss 1491.6265943433154
INFO:root:current train perplexity3.245131015777588
INFO:root:current mean train loss 1492.8666754538744
INFO:root:current train perplexity3.2475292682647705
INFO:root:current mean train loss 1494.3908643763891
INFO:root:current train perplexity3.2482831478118896
INFO:root:current mean train loss 1494.9014447357592
INFO:root:current train perplexity3.2505812644958496
INFO:root:current mean train loss 1495.225109475327
INFO:root:current train perplexity3.2517101764678955
INFO:root:current mean train loss 1497.1302429745301
INFO:root:current train perplexity3.253856658935547
INFO:root:current mean train loss 1497.1004352557297
INFO:root:current train perplexity3.255207061767578
INFO:root:current mean train loss 1497.7301295673349
INFO:root:current train perplexity3.2575385570526123
INFO:root:current mean train loss 1498.0110722578258
INFO:root:current train perplexity3.2591567039489746
INFO:root:current mean train loss 1497.9942395724418
INFO:root:current train perplexity3.2604520320892334
INFO:root:current mean train loss 1498.4822469888072
INFO:root:current train perplexity3.2621214389801025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.10s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.43s/it]
INFO:root:eval mean loss: 3078.3747419294295
INFO:root:eval perplexity: 12.670434951782227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [9:03:32<19:15:53, 506.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1469.8238734654017
INFO:root:current train perplexity3.2007999420166016
INFO:root:current mean train loss 1478.533819221048
INFO:root:current train perplexity3.222391128540039
INFO:root:current mean train loss 1481.5423299153647
INFO:root:current train perplexity3.228476047515869
INFO:root:current mean train loss 1484.3475256017737
INFO:root:current train perplexity3.226595163345337
INFO:root:current mean train loss 1485.5916062375331
INFO:root:current train perplexity3.2289180755615234
INFO:root:current mean train loss 1483.6838389614172
INFO:root:current train perplexity3.2241709232330322
INFO:root:current mean train loss 1484.315447360366
INFO:root:current train perplexity3.2253661155700684
INFO:root:current mean train loss 1485.5511022790686
INFO:root:current train perplexity3.2256433963775635
INFO:root:current mean train loss 1484.6464292328933
INFO:root:current train perplexity3.2263503074645996
INFO:root:current mean train loss 1485.5558065198131
INFO:root:current train perplexity3.2276923656463623
INFO:root:current mean train loss 1486.0486314434872
INFO:root:current train perplexity3.2290194034576416
INFO:root:current mean train loss 1487.4719657702324
INFO:root:current train perplexity3.232802391052246
INFO:root:current mean train loss 1487.3247502845104
INFO:root:current train perplexity3.232969284057617
INFO:root:current mean train loss 1488.0461293018648
INFO:root:current train perplexity3.2332048416137695
INFO:root:current mean train loss 1489.0060774407418
INFO:root:current train perplexity3.234841823577881
INFO:root:current mean train loss 1489.1818710813097
INFO:root:current train perplexity3.236598014831543
INFO:root:current mean train loss 1489.791408662168
INFO:root:current train perplexity3.236811637878418
INFO:root:current mean train loss 1490.038910084525
INFO:root:current train perplexity3.2385878562927246
INFO:root:current mean train loss 1490.469680998287
INFO:root:current train perplexity3.2396481037139893
INFO:root:current mean train loss 1491.126146531468
INFO:root:current train perplexity3.2424895763397217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.13s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it]
INFO:root:eval mean loss: 3094.2639988621436
INFO:root:eval perplexity: 12.837597846984863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [9:11:58<19:07:25, 506.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1468.7225369859016
INFO:root:current train perplexity3.196643590927124
INFO:root:current mean train loss 1470.311055392505
INFO:root:current train perplexity3.205385208129883
INFO:root:current mean train loss 1474.4874833269816
INFO:root:current train perplexity3.2054178714752197
INFO:root:current mean train loss 1474.5727722010256
INFO:root:current train perplexity3.2059431076049805
INFO:root:current mean train loss 1474.925530842932
INFO:root:current train perplexity3.206533908843994
INFO:root:current mean train loss 1476.6597586376704
INFO:root:current train perplexity3.2085142135620117
INFO:root:current mean train loss 1476.263787726301
INFO:root:current train perplexity3.2102692127227783
INFO:root:current mean train loss 1478.137544547133
INFO:root:current train perplexity3.2122604846954346
INFO:root:current mean train loss 1478.8029765889232
INFO:root:current train perplexity3.2127702236175537
INFO:root:current mean train loss 1480.323331854143
INFO:root:current train perplexity3.2154104709625244
INFO:root:current mean train loss 1479.4194493157775
INFO:root:current train perplexity3.216798782348633
INFO:root:current mean train loss 1480.3840074932866
INFO:root:current train perplexity3.21777606010437
INFO:root:current mean train loss 1480.0484165763708
INFO:root:current train perplexity3.2182178497314453
INFO:root:current mean train loss 1481.4068500543551
INFO:root:current train perplexity3.2191596031188965
INFO:root:current mean train loss 1482.0310628309935
INFO:root:current train perplexity3.221156120300293
INFO:root:current mean train loss 1482.809582388769
INFO:root:current train perplexity3.221985101699829
INFO:root:current mean train loss 1483.0547293960942
INFO:root:current train perplexity3.2220640182495117
INFO:root:current mean train loss 1482.8729045165605
INFO:root:current train perplexity3.22145414352417
INFO:root:current mean train loss 1483.0680627059735
INFO:root:current train perplexity3.221897602081299


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.57s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.32s/it]
INFO:root:eval mean loss: 3098.6293666713586
INFO:root:eval perplexity: 12.883903503417969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [9:20:24<18:58:49, 506.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1506.7032165527344
INFO:root:current train perplexity3.2472293376922607
INFO:root:current mean train loss 1464.9169311523438
INFO:root:current train perplexity3.15913724899292
INFO:root:current mean train loss 1466.3734376196767
INFO:root:current train perplexity3.162332057952881
INFO:root:current mean train loss 1468.0932038959704
INFO:root:current train perplexity3.1707754135131836
INFO:root:current mean train loss 1466.8183210014117
INFO:root:current train perplexity3.1731371879577637
INFO:root:current mean train loss 1467.1047910660031
INFO:root:current train perplexity3.1726856231689453
INFO:root:current mean train loss 1466.7217275859505
INFO:root:current train perplexity3.17276668548584
INFO:root:current mean train loss 1469.7461197592995
INFO:root:current train perplexity3.1800310611724854
INFO:root:current mean train loss 1471.093055990798
INFO:root:current train perplexity3.181380271911621
INFO:root:current mean train loss 1470.855539372537
INFO:root:current train perplexity3.18371319770813
INFO:root:current mean train loss 1472.0525199008653
INFO:root:current train perplexity3.1874639987945557
INFO:root:current mean train loss 1472.8376214953437
INFO:root:current train perplexity3.192716360092163
INFO:root:current mean train loss 1473.89208984375
INFO:root:current train perplexity3.197906732559204
INFO:root:current mean train loss 1474.679908986472
INFO:root:current train perplexity3.198784828186035
INFO:root:current mean train loss 1474.5238044934395
INFO:root:current train perplexity3.2008888721466064
INFO:root:current mean train loss 1474.9656243831553
INFO:root:current train perplexity3.2013163566589355
INFO:root:current mean train loss 1475.36350918054
INFO:root:current train perplexity3.20369815826416
INFO:root:current mean train loss 1475.7315096429816
INFO:root:current train perplexity3.2036285400390625
INFO:root:current mean train loss 1476.5250305717113
INFO:root:current train perplexity3.2052674293518066
INFO:root:current mean train loss 1476.4036173459863
INFO:root:current train perplexity3.20577073097229


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.50s/it]
INFO:root:eval mean loss: 3106.830012874202
INFO:root:eval perplexity: 12.971354484558105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:28:52<18:51:38, 506.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1468.1916097005208
INFO:root:current train perplexity3.182624101638794
INFO:root:current mean train loss 1447.5622477886106
INFO:root:current train perplexity3.1736457347869873
INFO:root:current mean train loss 1453.9174163956448
INFO:root:current train perplexity3.1670398712158203
INFO:root:current mean train loss 1456.3559368763385
INFO:root:current train perplexity3.1700944900512695
INFO:root:current mean train loss 1457.6510514284257
INFO:root:current train perplexity3.168564796447754
INFO:root:current mean train loss 1459.8742504742233
INFO:root:current train perplexity3.1742537021636963
INFO:root:current mean train loss 1460.4429058867377
INFO:root:current train perplexity3.1736316680908203
INFO:root:current mean train loss 1461.3486031837833
INFO:root:current train perplexity3.1741750240325928
INFO:root:current mean train loss 1460.1423550976324
INFO:root:current train perplexity3.1726927757263184
INFO:root:current mean train loss 1461.9022125343547
INFO:root:current train perplexity3.1732864379882812
INFO:root:current mean train loss 1462.08295054347
INFO:root:current train perplexity3.1740407943725586
INFO:root:current mean train loss 1463.3036670531683
INFO:root:current train perplexity3.1747026443481445
INFO:root:current mean train loss 1464.2849369033452
INFO:root:current train perplexity3.175701856613159
INFO:root:current mean train loss 1465.2347256864768
INFO:root:current train perplexity3.1782851219177246
INFO:root:current mean train loss 1465.6681531869888
INFO:root:current train perplexity3.1795356273651123
INFO:root:current mean train loss 1466.0444326306706
INFO:root:current train perplexity3.1805646419525146
INFO:root:current mean train loss 1466.7832579474475
INFO:root:current train perplexity3.180453062057495
INFO:root:current mean train loss 1467.8159213733838
INFO:root:current train perplexity3.183237314224243
INFO:root:current mean train loss 1468.456575882821
INFO:root:current train perplexity3.1854233741760254
INFO:root:current mean train loss 1468.5276193454947
INFO:root:current train perplexity3.1860101222991943


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.40s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.83s/it]
INFO:root:eval mean loss: 3121.406664232592
INFO:root:eval perplexity: 13.128262519836426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [9:37:22<18:45:20, 507.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1442.4067832545231
INFO:root:current train perplexity3.1411843299865723
INFO:root:current mean train loss 1444.247532941293
INFO:root:current train perplexity3.151437997817993
INFO:root:current mean train loss 1444.5043252896862
INFO:root:current train perplexity3.1509459018707275
INFO:root:current mean train loss 1452.1717706262712
INFO:root:current train perplexity3.1556620597839355
INFO:root:current mean train loss 1452.3812392422053
INFO:root:current train perplexity3.157771587371826
INFO:root:current mean train loss 1451.624800557984
INFO:root:current train perplexity3.153874158859253
INFO:root:current mean train loss 1454.4216105781004
INFO:root:current train perplexity3.1565322875976562
INFO:root:current mean train loss 1454.9931700171494
INFO:root:current train perplexity3.1548800468444824
INFO:root:current mean train loss 1456.7807059276645
INFO:root:current train perplexity3.1567180156707764
INFO:root:current mean train loss 1457.041586414329
INFO:root:current train perplexity3.154480218887329
INFO:root:current mean train loss 1457.6882318338678
INFO:root:current train perplexity3.156888246536255
INFO:root:current mean train loss 1458.0213560831778
INFO:root:current train perplexity3.156299591064453
INFO:root:current mean train loss 1458.6889428553172
INFO:root:current train perplexity3.1577470302581787
INFO:root:current mean train loss 1459.74896897115
INFO:root:current train perplexity3.1620874404907227
INFO:root:current mean train loss 1460.003711005411
INFO:root:current train perplexity3.1635913848876953
INFO:root:current mean train loss 1460.5634530691239
INFO:root:current train perplexity3.1639561653137207
INFO:root:current mean train loss 1461.5906562851753
INFO:root:current train perplexity3.1659324169158936
INFO:root:current mean train loss 1462.2742748545832
INFO:root:current train perplexity3.168092966079712
INFO:root:current mean train loss 1461.70763024464
INFO:root:current train perplexity3.1678755283355713
INFO:root:current mean train loss 1462.2848151081978
INFO:root:current train perplexity3.1697709560394287


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.11s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.08s/it]
INFO:root:eval mean loss: 3117.43909021326
INFO:root:eval perplexity: 13.085367202758789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [9:45:50<18:37:14, 507.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1459.403788618608
INFO:root:current train perplexity3.123777389526367
INFO:root:current mean train loss 1457.3746747416835
INFO:root:current train perplexity3.132662773132324
INFO:root:current mean train loss 1454.3482986749386
INFO:root:current train perplexity3.138517379760742
INFO:root:current mean train loss 1451.7883737483494
INFO:root:current train perplexity3.135601758956909
INFO:root:current mean train loss 1450.4999584156078
INFO:root:current train perplexity3.1376545429229736
INFO:root:current mean train loss 1450.452896035684
INFO:root:current train perplexity3.137502908706665
INFO:root:current mean train loss 1450.3362381097925
INFO:root:current train perplexity3.1388142108917236
INFO:root:current mean train loss 1450.9847945661734
INFO:root:current train perplexity3.140096664428711
INFO:root:current mean train loss 1450.915343167489
INFO:root:current train perplexity3.1398227214813232
INFO:root:current mean train loss 1452.088133103935
INFO:root:current train perplexity3.141812324523926
INFO:root:current mean train loss 1453.120016754295
INFO:root:current train perplexity3.145080804824829
INFO:root:current mean train loss 1453.139259820583
INFO:root:current train perplexity3.146482467651367
INFO:root:current mean train loss 1454.7043980328685
INFO:root:current train perplexity3.149553060531616
INFO:root:current mean train loss 1455.091874621627
INFO:root:current train perplexity3.14941143989563
INFO:root:current mean train loss 1454.4235831454039
INFO:root:current train perplexity3.149116039276123
INFO:root:current mean train loss 1453.9607370848823
INFO:root:current train perplexity3.1492912769317627
INFO:root:current mean train loss 1453.7002033521762
INFO:root:current train perplexity3.149376630783081
INFO:root:current mean train loss 1454.3253803307514
INFO:root:current train perplexity3.1505894660949707
INFO:root:current mean train loss 1454.8449389845855
INFO:root:current train perplexity3.1505191326141357
INFO:root:current mean train loss 1454.8710784521859
INFO:root:current train perplexity3.1516427993774414


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.75s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.64s/it]
INFO:root:eval mean loss: 3136.3423262129318
INFO:root:eval perplexity: 13.291007041931152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [9:54:16<18:27:13, 507.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1441.9265001085068
INFO:root:current train perplexity3.0916266441345215
INFO:root:current mean train loss 1440.0559422692588
INFO:root:current train perplexity3.098346471786499
INFO:root:current mean train loss 1440.6289286893957
INFO:root:current train perplexity3.1022210121154785
INFO:root:current mean train loss 1438.2307893486434
INFO:root:current train perplexity3.100867509841919
INFO:root:current mean train loss 1440.6234523967162
INFO:root:current train perplexity3.1089847087860107
INFO:root:current mean train loss 1442.2883106578481
INFO:root:current train perplexity3.1131646633148193
INFO:root:current mean train loss 1443.0284093221028
INFO:root:current train perplexity3.1169559955596924
INFO:root:current mean train loss 1443.549661270695
INFO:root:current train perplexity3.1163828372955322
INFO:root:current mean train loss 1443.5529055814131
INFO:root:current train perplexity3.119375228881836
INFO:root:current mean train loss 1441.1844291530028
INFO:root:current train perplexity3.1173171997070312
INFO:root:current mean train loss 1442.3908476189001
INFO:root:current train perplexity3.1210367679595947
INFO:root:current mean train loss 1442.9116567149504
INFO:root:current train perplexity3.1224334239959717
INFO:root:current mean train loss 1444.0361066134471
INFO:root:current train perplexity3.125917911529541
INFO:root:current mean train loss 1444.7278828523597
INFO:root:current train perplexity3.1268906593322754
INFO:root:current mean train loss 1445.4915730849557
INFO:root:current train perplexity3.12919282913208
INFO:root:current mean train loss 1446.6604619693514
INFO:root:current train perplexity3.1299707889556885
INFO:root:current mean train loss 1446.8575537284596
INFO:root:current train perplexity3.131638288497925
INFO:root:current mean train loss 1447.8867259143976
INFO:root:current train perplexity3.1333136558532715
INFO:root:current mean train loss 1448.6982013669788
INFO:root:current train perplexity3.133796453475952
INFO:root:current mean train loss 1448.182043172283
INFO:root:current train perplexity3.1344075202941895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.30s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 3145.4478440843186
INFO:root:eval perplexity: 13.391206741333008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [10:02:46<18:20:48, 508.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.6658249758602
INFO:root:current train perplexity3.084866523742676
INFO:root:current mean train loss 1430.5924595424108
INFO:root:current train perplexity3.0851664543151855
INFO:root:current mean train loss 1435.9134377872242
INFO:root:current train perplexity3.095609188079834
INFO:root:current mean train loss 1436.2488398613232
INFO:root:current train perplexity3.089627265930176
INFO:root:current mean train loss 1436.1762438190983
INFO:root:current train perplexity3.0934560298919678
INFO:root:current mean train loss 1437.8141213175802
INFO:root:current train perplexity3.0952444076538086
INFO:root:current mean train loss 1436.1899970377133
INFO:root:current train perplexity3.095391273498535
INFO:root:current mean train loss 1435.1066414914053
INFO:root:current train perplexity3.0970141887664795
INFO:root:current mean train loss 1436.3285358373173
INFO:root:current train perplexity3.0991804599761963
INFO:root:current mean train loss 1436.9772841836373
INFO:root:current train perplexity3.1024386882781982
INFO:root:current mean train loss 1437.3178010350307
INFO:root:current train perplexity3.1038365364074707
INFO:root:current mean train loss 1438.0441552652242
INFO:root:current train perplexity3.106675386428833
INFO:root:current mean train loss 1438.8946908221308
INFO:root:current train perplexity3.108130931854248
INFO:root:current mean train loss 1439.0196765215815
INFO:root:current train perplexity3.110558032989502
INFO:root:current mean train loss 1439.1174405765983
INFO:root:current train perplexity3.1123621463775635
INFO:root:current mean train loss 1439.25257584492
INFO:root:current train perplexity3.112884521484375
INFO:root:current mean train loss 1439.1318054379904
INFO:root:current train perplexity3.1132681369781494
INFO:root:current mean train loss 1439.6954802187413
INFO:root:current train perplexity3.114445209503174
INFO:root:current mean train loss 1440.8036881260546
INFO:root:current train perplexity3.117175817489624


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.21s/it]
INFO:root:eval mean loss: 3150.346628366648
INFO:root:eval perplexity: 13.445429801940918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [10:11:15<18:12:59, 508.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1483.374287923177
INFO:root:current train perplexity3.1096556186676025
INFO:root:current mean train loss 1413.9385122623084
INFO:root:current train perplexity3.075270652770996
INFO:root:current mean train loss 1423.2819338307797
INFO:root:current train perplexity3.085223913192749
INFO:root:current mean train loss 1426.311300838695
INFO:root:current train perplexity3.093331813812256
INFO:root:current mean train loss 1426.4667833450392
INFO:root:current train perplexity3.0904147624969482
INFO:root:current mean train loss 1425.4676788691947
INFO:root:current train perplexity3.087073564529419
INFO:root:current mean train loss 1425.0608413746647
INFO:root:current train perplexity3.0836522579193115
INFO:root:current mean train loss 1424.980750065012
INFO:root:current train perplexity3.0830087661743164
INFO:root:current mean train loss 1426.5286388160573
INFO:root:current train perplexity3.083012342453003
INFO:root:current mean train loss 1426.6496402833109
INFO:root:current train perplexity3.083338499069214
INFO:root:current mean train loss 1426.9198713966202
INFO:root:current train perplexity3.084563732147217
INFO:root:current mean train loss 1427.850061454566
INFO:root:current train perplexity3.0861754417419434
INFO:root:current mean train loss 1428.875319144026
INFO:root:current train perplexity3.08667254447937
INFO:root:current mean train loss 1429.6952487542471
INFO:root:current train perplexity3.086437702178955
INFO:root:current mean train loss 1431.3810772956858
INFO:root:current train perplexity3.0887537002563477
INFO:root:current mean train loss 1431.957436448866
INFO:root:current train perplexity3.0907559394836426
INFO:root:current mean train loss 1432.213339229598
INFO:root:current train perplexity3.0921225547790527
INFO:root:current mean train loss 1432.737861874794
INFO:root:current train perplexity3.0946621894836426
INFO:root:current mean train loss 1433.0125038256808
INFO:root:current train perplexity3.0963175296783447
INFO:root:current mean train loss 1433.8521359614786
INFO:root:current train perplexity3.0992376804351807


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.60s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.58s/it]
INFO:root:eval mean loss: 3156.9331897815787
INFO:root:eval perplexity: 13.518678665161133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [10:19:44<18:05:10, 508.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1390.3519180961277
INFO:root:current train perplexity3.0674867630004883
INFO:root:current mean train loss 1414.7653074186992
INFO:root:current train perplexity3.0565245151519775
INFO:root:current mean train loss 1419.0060307113579
INFO:root:current train perplexity3.0582427978515625
INFO:root:current mean train loss 1419.2459244388544
INFO:root:current train perplexity3.0615415573120117
INFO:root:current mean train loss 1419.6185262332854
INFO:root:current train perplexity3.067504644393921
INFO:root:current mean train loss 1419.01333904084
INFO:root:current train perplexity3.067107915878296
INFO:root:current mean train loss 1417.9582290282028
INFO:root:current train perplexity3.0668587684631348
INFO:root:current mean train loss 1419.0930692427387
INFO:root:current train perplexity3.0690674781799316
INFO:root:current mean train loss 1420.1477922923943
INFO:root:current train perplexity3.0715718269348145
INFO:root:current mean train loss 1420.5193721486069
INFO:root:current train perplexity3.071161985397339
INFO:root:current mean train loss 1420.8862154336969
INFO:root:current train perplexity3.0701792240142822
INFO:root:current mean train loss 1422.3139196244713
INFO:root:current train perplexity3.072100877761841
INFO:root:current mean train loss 1421.7440918168375
INFO:root:current train perplexity3.0724661350250244
INFO:root:current mean train loss 1422.5759347467285
INFO:root:current train perplexity3.075162172317505
INFO:root:current mean train loss 1422.6298259378568
INFO:root:current train perplexity3.075472116470337
INFO:root:current mean train loss 1423.981568985837
INFO:root:current train perplexity3.0774624347686768
INFO:root:current mean train loss 1424.337297121096
INFO:root:current train perplexity3.0782573223114014
INFO:root:current mean train loss 1424.8064463610435
INFO:root:current train perplexity3.078256368637085
INFO:root:current mean train loss 1425.0858467675407
INFO:root:current train perplexity3.0794014930725098
INFO:root:current mean train loss 1426.1526788561778
INFO:root:current train perplexity3.0808939933776855


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.34s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.40s/it]
INFO:root:eval mean loss: 3158.038963524071
INFO:root:eval perplexity: 13.531015396118164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [10:28:08<17:53:35, 507.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1399.2701232910156
INFO:root:current train perplexity3.0344350337982178
INFO:root:current mean train loss 1410.0365155901227
INFO:root:current train perplexity3.060743808746338
INFO:root:current mean train loss 1416.2226328531901
INFO:root:current train perplexity3.0551390647888184
INFO:root:current mean train loss 1414.781337962431
INFO:root:current train perplexity3.0499160289764404
INFO:root:current mean train loss 1415.0541140469638
INFO:root:current train perplexity3.047858715057373
INFO:root:current mean train loss 1413.9243482801648
INFO:root:current train perplexity3.0484442710876465
INFO:root:current mean train loss 1415.7456594467162
INFO:root:current train perplexity3.051215171813965
INFO:root:current mean train loss 1416.6928329880172
INFO:root:current train perplexity3.0534613132476807
INFO:root:current mean train loss 1416.344588070824
INFO:root:current train perplexity3.053715467453003
INFO:root:current mean train loss 1417.357782891456
INFO:root:current train perplexity3.054795265197754
INFO:root:current mean train loss 1417.0916444044847
INFO:root:current train perplexity3.0568196773529053
INFO:root:current mean train loss 1417.4847951788652
INFO:root:current train perplexity3.05918550491333
INFO:root:current mean train loss 1418.0225564279865
INFO:root:current train perplexity3.0606937408447266
INFO:root:current mean train loss 1418.3825659908466
INFO:root:current train perplexity3.0618271827697754
INFO:root:current mean train loss 1419.1616973876953
INFO:root:current train perplexity3.0637459754943848
INFO:root:current mean train loss 1418.9939155083198
INFO:root:current train perplexity3.0641183853149414
INFO:root:current mean train loss 1419.5604768334365
INFO:root:current train perplexity3.0651121139526367
INFO:root:current mean train loss 1420.0283454982714
INFO:root:current train perplexity3.066192150115967
INFO:root:current mean train loss 1420.7694349869437
INFO:root:current train perplexity3.0677926540374756
INFO:root:current mean train loss 1420.713035473381
INFO:root:current train perplexity3.0687122344970703


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.95s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.21s/it]
INFO:root:eval mean loss: 3164.1337186796172
INFO:root:eval perplexity: 13.599214553833008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [10:36:37<17:45:48, 507.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1412.5252792626097
INFO:root:current train perplexity3.013803005218506
INFO:root:current mean train loss 1399.1684912420383
INFO:root:current train perplexity3.014366626739502
INFO:root:current mean train loss 1399.2952876109557
INFO:root:current train perplexity3.0224475860595703
INFO:root:current mean train loss 1403.4261570350798
INFO:root:current train perplexity3.0232834815979004
INFO:root:current mean train loss 1404.7944223750342
INFO:root:current train perplexity3.0224528312683105
INFO:root:current mean train loss 1406.188389776425
INFO:root:current train perplexity3.0289158821105957
INFO:root:current mean train loss 1407.5202906128116
INFO:root:current train perplexity3.029613733291626
INFO:root:current mean train loss 1408.7723977253963
INFO:root:current train perplexity3.0310099124908447
INFO:root:current mean train loss 1409.0142299514112
INFO:root:current train perplexity3.032383680343628
INFO:root:current mean train loss 1409.5168647088476
INFO:root:current train perplexity3.0330984592437744
INFO:root:current mean train loss 1409.26920199507
INFO:root:current train perplexity3.036267042160034
INFO:root:current mean train loss 1410.572572858146
INFO:root:current train perplexity3.0400025844573975
INFO:root:current mean train loss 1411.6832817277932
INFO:root:current train perplexity3.042375087738037
INFO:root:current mean train loss 1411.3916836023857
INFO:root:current train perplexity3.0438456535339355
INFO:root:current mean train loss 1411.1905914704605
INFO:root:current train perplexity3.04571795463562
INFO:root:current mean train loss 1411.3563148532835
INFO:root:current train perplexity3.0470638275146484
INFO:root:current mean train loss 1412.6810311574711
INFO:root:current train perplexity3.0483274459838867
INFO:root:current mean train loss 1413.4546508441679
INFO:root:current train perplexity3.0507984161376953
INFO:root:current mean train loss 1414.3897611076754
INFO:root:current train perplexity3.0523977279663086
INFO:root:current mean train loss 1414.6140597055442
INFO:root:current train perplexity3.0532360076904297


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.23s/it]
INFO:root:eval mean loss: 3177.588178021772
INFO:root:eval perplexity: 13.750978469848633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [10:45:02<17:36:08, 506.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1400.4501656197212
INFO:root:current train perplexity3.0362844467163086
INFO:root:current mean train loss 1396.8394831514906
INFO:root:current train perplexity3.0127041339874268
INFO:root:current mean train loss 1397.643141697793
INFO:root:current train perplexity3.010308027267456
INFO:root:current mean train loss 1397.968215371198
INFO:root:current train perplexity3.011345386505127
INFO:root:current mean train loss 1399.2902288638086
INFO:root:current train perplexity3.0136501789093018
INFO:root:current mean train loss 1400.5864863910742
INFO:root:current train perplexity3.014970302581787
INFO:root:current mean train loss 1401.7117084989916
INFO:root:current train perplexity3.016619920730591
INFO:root:current mean train loss 1401.7407739131643
INFO:root:current train perplexity3.0172476768493652
INFO:root:current mean train loss 1400.8027339559944
INFO:root:current train perplexity3.01849365234375
INFO:root:current mean train loss 1401.0202305850553
INFO:root:current train perplexity3.0186667442321777
INFO:root:current mean train loss 1401.7418890301299
INFO:root:current train perplexity3.0220861434936523
INFO:root:current mean train loss 1404.0057573724646
INFO:root:current train perplexity3.02451491355896
INFO:root:current mean train loss 1404.2534074289272
INFO:root:current train perplexity3.0265722274780273
INFO:root:current mean train loss 1404.7379307642775
INFO:root:current train perplexity3.0264816284179688
INFO:root:current mean train loss 1404.854404899773
INFO:root:current train perplexity3.029402256011963
INFO:root:current mean train loss 1404.8764847751797
INFO:root:current train perplexity3.0295324325561523
INFO:root:current mean train loss 1405.3481569278767
INFO:root:current train perplexity3.030665397644043
INFO:root:current mean train loss 1406.1975989443877
INFO:root:current train perplexity3.031601905822754
INFO:root:current mean train loss 1406.0511409470469
INFO:root:current train perplexity3.0330705642700195
INFO:root:current mean train loss 1406.8394580597574
INFO:root:current train perplexity3.034525156021118


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.48s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.60s/it]
INFO:root:eval mean loss: 3181.855052317943
INFO:root:eval perplexity: 13.799464225769043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [10:53:25<17:25:22, 505.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1382.7461983816963
INFO:root:current train perplexity3.010070562362671
INFO:root:current mean train loss 1385.5055896699116
INFO:root:current train perplexity3.0071117877960205
INFO:root:current mean train loss 1385.5568243596972
INFO:root:current train perplexity2.9969756603240967
INFO:root:current mean train loss 1387.097998109315
INFO:root:current train perplexity2.9998538494110107
INFO:root:current mean train loss 1390.7831414683044
INFO:root:current train perplexity3.003729820251465
INFO:root:current mean train loss 1391.424993886157
INFO:root:current train perplexity3.0030510425567627
INFO:root:current mean train loss 1392.4748881404894
INFO:root:current train perplexity3.0046398639678955
INFO:root:current mean train loss 1393.1152231093454
INFO:root:current train perplexity3.0070583820343018
INFO:root:current mean train loss 1394.6675067734639
INFO:root:current train perplexity3.007106065750122
INFO:root:current mean train loss 1394.6526686959983
INFO:root:current train perplexity3.008232593536377
INFO:root:current mean train loss 1396.918223967583
INFO:root:current train perplexity3.0112805366516113
INFO:root:current mean train loss 1397.6449250318142
INFO:root:current train perplexity3.0133447647094727
INFO:root:current mean train loss 1398.8683759599207
INFO:root:current train perplexity3.0145905017852783
INFO:root:current mean train loss 1398.7080349294854
INFO:root:current train perplexity3.01617169380188
INFO:root:current mean train loss 1399.3424908172997
INFO:root:current train perplexity3.017186164855957
INFO:root:current mean train loss 1399.9703200344468
INFO:root:current train perplexity3.018605947494507
INFO:root:current mean train loss 1400.1257910387253
INFO:root:current train perplexity3.018594264984131
INFO:root:current mean train loss 1400.9495037170445
INFO:root:current train perplexity3.0196495056152344
INFO:root:current mean train loss 1401.3949794436435
INFO:root:current train perplexity3.0207159519195557


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.79s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.02s/it]
INFO:root:eval mean loss: 3194.7633148285004
INFO:root:eval perplexity: 13.947182655334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [11:01:45<17:13:19, 504.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1380.9704284667969
INFO:root:current train perplexity2.9974164962768555
INFO:root:current mean train loss 1392.0056446216724
INFO:root:current train perplexity2.9923272132873535
INFO:root:current mean train loss 1387.201562147874
INFO:root:current train perplexity2.9907732009887695
INFO:root:current mean train loss 1385.9570233233562
INFO:root:current train perplexity2.989912986755371
INFO:root:current mean train loss 1387.857863482307
INFO:root:current train perplexity2.989534854888916
INFO:root:current mean train loss 1388.9610792745755
INFO:root:current train perplexity2.9900944232940674
INFO:root:current mean train loss 1387.9430813036467
INFO:root:current train perplexity2.991431713104248
INFO:root:current mean train loss 1388.6540429066804
INFO:root:current train perplexity2.994032859802246
INFO:root:current mean train loss 1388.4348356039218
INFO:root:current train perplexity2.9940993785858154
INFO:root:current mean train loss 1389.1246952275349
INFO:root:current train perplexity2.998828649520874
INFO:root:current mean train loss 1391.5352327861483
INFO:root:current train perplexity3.00026535987854
INFO:root:current mean train loss 1391.8033142089844
INFO:root:current train perplexity3.0004870891571045
INFO:root:current mean train loss 1390.7136251689583
INFO:root:current train perplexity2.999602794647217
INFO:root:current mean train loss 1391.3688714730265
INFO:root:current train perplexity3.000425338745117
INFO:root:current mean train loss 1392.04120462591
INFO:root:current train perplexity3.0015110969543457
INFO:root:current mean train loss 1392.7351107407628
INFO:root:current train perplexity3.0032079219818115
INFO:root:current mean train loss 1393.6829956206516
INFO:root:current train perplexity3.005152940750122
INFO:root:current mean train loss 1394.0693513034937
INFO:root:current train perplexity3.005689859390259
INFO:root:current mean train loss 1394.3464553292874
INFO:root:current train perplexity3.0060300827026367
INFO:root:current mean train loss 1394.7015797996921
INFO:root:current train perplexity3.0063517093658447


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.04s/it]
INFO:root:eval mean loss: 3199.723983260604
INFO:root:eval perplexity: 14.004364013671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [11:10:12<17:06:16, 504.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.1590869140625
INFO:root:current train perplexity2.983288288116455
INFO:root:current mean train loss 1376.77883984375
INFO:root:current train perplexity2.969019889831543
INFO:root:current mean train loss 1378.5195833333332
INFO:root:current train perplexity2.9660444259643555
INFO:root:current mean train loss 1378.8153203876202
INFO:root:current train perplexity2.967848777770996
INFO:root:current mean train loss 1378.0695866842832
INFO:root:current train perplexity2.9674408435821533
INFO:root:current mean train loss 1378.8150069754465
INFO:root:current train perplexity2.9680399894714355
INFO:root:current mean train loss 1382.2468130859374
INFO:root:current train perplexity2.9722909927368164
INFO:root:current mean train loss 1383.2677293238146
INFO:root:current train perplexity2.9738247394561768
INFO:root:current mean train loss 1381.9709823330966
INFO:root:current train perplexity2.973621129989624
INFO:root:current mean train loss 1382.1657689769847
INFO:root:current train perplexity2.9772937297821045
INFO:root:current mean train loss 1382.912968273628
INFO:root:current train perplexity2.9769790172576904
INFO:root:current mean train loss 1384.2972934027778
INFO:root:current train perplexity2.978788375854492
INFO:root:current mean train loss 1384.8994682716836
INFO:root:current train perplexity2.9821934700012207
INFO:root:current mean train loss 1385.282843823703
INFO:root:current train perplexity2.9847466945648193
INFO:root:current mean train loss 1386.12258472108
INFO:root:current train perplexity2.9859418869018555
INFO:root:current mean train loss 1386.4327888063524
INFO:root:current train perplexity2.986224889755249
INFO:root:current mean train loss 1385.8268635817308
INFO:root:current train perplexity2.9857876300811768
INFO:root:current mean train loss 1386.708527867414
INFO:root:current train perplexity2.9871723651885986
INFO:root:current mean train loss 1387.0793719231592
INFO:root:current train perplexity2.988208770751953
INFO:root:current mean train loss 1387.6383431412337
INFO:root:current train perplexity2.988908052444458


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.83s/it]
INFO:root:eval mean loss: 3205.8607804581925
INFO:root:eval perplexity: 14.075438499450684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [11:18:38<16:59:08, 505.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1380.3799670991443
INFO:root:current train perplexity2.962693691253662
INFO:root:current mean train loss 1370.0093770631602
INFO:root:current train perplexity2.95105242729187
INFO:root:current mean train loss 1372.8617271234182
INFO:root:current train perplexity2.9548892974853516
INFO:root:current mean train loss 1375.4730460183662
INFO:root:current train perplexity2.951223850250244
INFO:root:current mean train loss 1375.1165139038637
INFO:root:current train perplexity2.9515068531036377
INFO:root:current mean train loss 1374.2382015214196
INFO:root:current train perplexity2.9519217014312744
INFO:root:current mean train loss 1374.3430911625658
INFO:root:current train perplexity2.9515440464019775
INFO:root:current mean train loss 1375.9996239181478
INFO:root:current train perplexity2.9562718868255615
INFO:root:current mean train loss 1375.2260155032197
INFO:root:current train perplexity2.9570717811584473
INFO:root:current mean train loss 1376.1403655681893
INFO:root:current train perplexity2.960219383239746
INFO:root:current mean train loss 1376.4575813864562
INFO:root:current train perplexity2.9607954025268555
INFO:root:current mean train loss 1377.3922410954945
INFO:root:current train perplexity2.963667392730713
INFO:root:current mean train loss 1378.3300461822853
INFO:root:current train perplexity2.965121030807495
INFO:root:current mean train loss 1378.7200856784416
INFO:root:current train perplexity2.9663453102111816
INFO:root:current mean train loss 1379.005201448184
INFO:root:current train perplexity2.9666450023651123
INFO:root:current mean train loss 1378.7400238789164
INFO:root:current train perplexity2.9691922664642334
INFO:root:current mean train loss 1379.1323528405956
INFO:root:current train perplexity2.9700448513031006
INFO:root:current mean train loss 1379.8863591260942
INFO:root:current train perplexity2.971386671066284
INFO:root:current mean train loss 1380.2004916080305
INFO:root:current train perplexity2.9724619388580322
INFO:root:current mean train loss 1380.8030797799265
INFO:root:current train perplexity2.973067045211792


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.89s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 3212.943953230574
INFO:root:eval perplexity: 14.157917976379395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_64_low/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [11:26:59<16:48:02, 504.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1361.6306752350372
INFO:root:current train perplexity2.9284918308258057
INFO:root:current mean train loss 1361.7097713062599
INFO:root:current train perplexity2.9192183017730713
INFO:root:current mean train loss 1361.3832951254826
INFO:root:current train perplexity2.927684783935547
INFO:root:current mean train loss 1366.1042919105805
INFO:root:current train perplexity2.930567979812622
slurmstepd: error: *** JOB 25954148 ON ga003 CANCELLED AT 2022-10-17T04:18:06 ***
