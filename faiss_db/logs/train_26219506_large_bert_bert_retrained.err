INFO:root:Output: large_distilbert_bert_retrained
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of the model checkpoint at bert-base-uncased were not used when initializing RetrievalGenerationModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing RetrievalGenerationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RetrievalGenerationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9850.624585700758
INFO:root:current train perplexity2284.59912109375
INFO:root:current mean train loss 8544.14590040044
INFO:root:current train perplexity827.9732055664062
INFO:root:current mean train loss 7580.416188728052
INFO:root:current train perplexity392.5211486816406
INFO:root:current mean train loss 6885.201924488957
INFO:root:current train perplexity226.34503173828125
INFO:root:current mean train loss 6353.838850552668
INFO:root:current train perplexity148.99652099609375
INFO:root:current mean train loss 5931.793772090855
INFO:root:current train perplexity107.88636016845703
INFO:root:current mean train loss 5595.501556352826
INFO:root:current train perplexity83.36772155761719
INFO:root:current mean train loss 5331.826595378012
INFO:root:current train perplexity67.55516052246094
INFO:root:current mean train loss 5111.8562419072405
INFO:root:current train perplexity56.618751525878906
INFO:root:current mean train loss 4922.840307348364
INFO:root:current train perplexity48.831111907958984
INFO:root:current mean train loss 4763.864568819666
INFO:root:current train perplexity42.99099349975586
INFO:root:current mean train loss 4625.842935519183
INFO:root:current train perplexity38.4364013671875
INFO:root:current mean train loss 4502.631395394294
INFO:root:current train perplexity34.897605895996094
INFO:root:current mean train loss 4395.562952332023
INFO:root:current train perplexity32.0208740234375
INFO:root:current mean train loss 4298.987481400361
INFO:root:current train perplexity29.663084030151367
INFO:root:current mean train loss 4210.802300601695
INFO:root:current train perplexity27.64834976196289
INFO:root:current mean train loss 4131.649955942604
INFO:root:current train perplexity25.944076538085938
INFO:root:current mean train loss 4058.343728286548
INFO:root:current train perplexity24.51660919189453
INFO:root:current mean train loss 3991.5801697902266
INFO:root:current train perplexity23.265148162841797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:36<00:00, 576.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:36<00:00, 576.33s/it]
INFO:root:final mean train loss: 3939.9686887493895
INFO:root:final train perplexity: 22.360504150390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.81s/it]
INFO:root:eval mean loss: 2546.561804372368
INFO:root:eval perplexity: 7.8421244621276855
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.84s/it]
INFO:root:eval mean loss: 2862.9697105461823
INFO:root:eval perplexity: 10.395942687988281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/1
  0%|          | 1/200 [11:25<37:53:19, 685.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2691.8497772216797
INFO:root:current train perplexity8.598118782043457
INFO:root:current mean train loss 2704.4766845703125
INFO:root:current train perplexity8.560883522033691
INFO:root:current mean train loss 2691.579593234592
INFO:root:current train perplexity8.51046371459961
INFO:root:current mean train loss 2684.0610312932654
INFO:root:current train perplexity8.4541015625
INFO:root:current mean train loss 2686.2576845609224
INFO:root:current train perplexity8.418935775756836
INFO:root:current mean train loss 2675.556437174479
INFO:root:current train perplexity8.326079368591309
INFO:root:current mean train loss 2671.965584098519
INFO:root:current train perplexity8.286408424377441
INFO:root:current mean train loss 2663.8237021675322
INFO:root:current train perplexity8.23598861694336
INFO:root:current mean train loss 2654.5791159237133
INFO:root:current train perplexity8.173900604248047
INFO:root:current mean train loss 2647.8294040729907
INFO:root:current train perplexity8.118806838989258
INFO:root:current mean train loss 2640.29692353977
INFO:root:current train perplexity8.06700611114502
INFO:root:current mean train loss 2632.5157354758203
INFO:root:current train perplexity8.014241218566895
INFO:root:current mean train loss 2626.1515183699757
INFO:root:current train perplexity7.964869499206543
INFO:root:current mean train loss 2619.0962682103677
INFO:root:current train perplexity7.914984703063965
INFO:root:current mean train loss 2613.5224192129017
INFO:root:current train perplexity7.88051700592041
INFO:root:current mean train loss 2607.2536450388566
INFO:root:current train perplexity7.837464332580566
INFO:root:current mean train loss 2601.9550874917813
INFO:root:current train perplexity7.795498371124268
INFO:root:current mean train loss 2596.4260760398456
INFO:root:current train perplexity7.759005069732666
INFO:root:current mean train loss 2592.7834560041388
INFO:root:current train perplexity7.728236198425293
INFO:root:current mean train loss 2587.4510498046875
INFO:root:current train perplexity7.691844940185547

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:23<00:00, 623.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:23<00:00, 623.61s/it]
INFO:root:final mean train loss: 2582.966335310097
INFO:root:final train perplexity: 7.668203830718994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.35s/it]
INFO:root:eval mean loss: 2323.578684705369
INFO:root:eval perplexity: 6.548093318939209
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.10s/it]
INFO:root:eval mean loss: 2675.103157638658
INFO:root:eval perplexity: 8.915334701538086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/2
  1%|          | 2/200 [24:16<40:27:54, 735.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2461.3650124289775
INFO:root:current train perplexity7.013543605804443
INFO:root:current mean train loss 2456.4616368802867
INFO:root:current train perplexity6.9492974281311035
INFO:root:current mean train loss 2443.9107147347772
INFO:root:current train perplexity6.905233383178711
INFO:root:current mean train loss 2440.869609477642
INFO:root:current train perplexity6.871526718139648
INFO:root:current mean train loss 2438.9158363254187
INFO:root:current train perplexity6.848221302032471
INFO:root:current mean train loss 2436.777493074285
INFO:root:current train perplexity6.832624912261963
INFO:root:current mean train loss 2431.733884983153
INFO:root:current train perplexity6.80484676361084
INFO:root:current mean train loss 2426.8661383281465
INFO:root:current train perplexity6.7837748527526855
INFO:root:current mean train loss 2426.790958180147
INFO:root:current train perplexity6.773281574249268
INFO:root:current mean train loss 2427.994654811847
INFO:root:current train perplexity6.769954681396484
INFO:root:current mean train loss 2425.133550594068
INFO:root:current train perplexity6.748712062835693
INFO:root:current mean train loss 2420.1389392876354
INFO:root:current train perplexity6.732794284820557
INFO:root:current mean train loss 2414.3508424534607
INFO:root:current train perplexity6.719138145446777
INFO:root:current mean train loss 2412.706910003868
INFO:root:current train perplexity6.705224990844727
INFO:root:current mean train loss 2409.431303377012
INFO:root:current train perplexity6.688946723937988
INFO:root:current mean train loss 2406.9403185262763
INFO:root:current train perplexity6.67471981048584
INFO:root:current mean train loss 2403.682009003751
INFO:root:current train perplexity6.658044815063477
INFO:root:current mean train loss 2400.9932189342676
INFO:root:current train perplexity6.643962860107422
INFO:root:current mean train loss 2399.551596916769
INFO:root:current train perplexity6.635310649871826
INFO:root:current mean train loss 2396.9398286190913
INFO:root:current train perplexity6.622561454772949

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:07<00:00, 607.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:07<00:00, 607.04s/it]
INFO:root:final mean train loss: 2395.8590681313626
INFO:root:final train perplexity: 6.616183757781982
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.08s/it]
INFO:root:eval mean loss: 2221.781345232159
INFO:root:eval perplexity: 6.030599117279053
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.90s/it]
INFO:root:eval mean loss: 2582.7006268873283
INFO:root:eval perplexity: 8.266437530517578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/3
  2%|â–         | 3/200 [37:08<41:09:56, 752.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2309.466589355469
INFO:root:current train perplexity6.236632823944092
INFO:root:current mean train loss 2300.5407104492188
INFO:root:current train perplexity6.153255462646484
INFO:root:current mean train loss 2308.3122231445313
INFO:root:current train perplexity6.177067279815674
INFO:root:current mean train loss 2299.572967354911
INFO:root:current train perplexity6.1739301681518555
INFO:root:current mean train loss 2301.6732739257814
INFO:root:current train perplexity6.165588855743408
INFO:root:current mean train loss 2301.751649280895
INFO:root:current train perplexity6.156529426574707
INFO:root:current mean train loss 2305.9643513371393
INFO:root:current train perplexity6.166431427001953
INFO:root:current mean train loss 2308.4520970052085
INFO:root:current train perplexity6.167494297027588
INFO:root:current mean train loss 2304.353753303079
INFO:root:current train perplexity6.152127742767334
INFO:root:current mean train loss 2303.3793535413242
INFO:root:current train perplexity6.151890754699707
INFO:root:current mean train loss 2299.759895484561
INFO:root:current train perplexity6.140585899353027
INFO:root:current mean train loss 2298.919596637228
INFO:root:current train perplexity6.134310245513916
INFO:root:current mean train loss 2297.5287512695313
INFO:root:current train perplexity6.128690719604492
INFO:root:current mean train loss 2297.1871725802953
INFO:root:current train perplexity6.121633529663086
INFO:root:current mean train loss 2295.540268217942
INFO:root:current train perplexity6.109591484069824
INFO:root:current mean train loss 2293.5385991053427
INFO:root:current train perplexity6.104001045227051
INFO:root:current mean train loss 2292.778517178622
INFO:root:current train perplexity6.099894046783447
INFO:root:current mean train loss 2291.1781452287946
INFO:root:current train perplexity6.092284202575684
INFO:root:current mean train loss 2289.9624774994722
INFO:root:current train perplexity6.0836405754089355
INFO:root:current mean train loss 2289.499642553085
INFO:root:current train perplexity6.078005790710449

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:06<00:00, 606.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:06<00:00, 606.50s/it]
INFO:root:final mean train loss: 2287.7848154011244
INFO:root:final train perplexity: 6.075623035430908
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.95s/it]
INFO:root:eval mean loss: 2157.0560852483654
INFO:root:eval perplexity: 5.723042011260986
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it]
INFO:root:eval mean loss: 2528.6819756690493
INFO:root:eval perplexity: 7.909191608428955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/4
  2%|â–         | 4/200 [49:57<41:19:23, 759.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2206.466159194263
INFO:root:current train perplexity5.747951030731201
INFO:root:current mean train loss 2213.1768740351326
INFO:root:current train perplexity5.732299327850342
INFO:root:current mean train loss 2226.7283405203943
INFO:root:current train perplexity5.758966445922852
INFO:root:current mean train loss 2226.4001268599923
INFO:root:current train perplexity5.770065784454346
INFO:root:current mean train loss 2221.65978977767
INFO:root:current train perplexity5.765628337860107
INFO:root:current mean train loss 2224.71634734623
INFO:root:current train perplexity5.771984100341797
INFO:root:current mean train loss 2220.9978755739317
INFO:root:current train perplexity5.762853622436523
INFO:root:current mean train loss 2221.2970839678233
INFO:root:current train perplexity5.758731842041016
INFO:root:current mean train loss 2218.7134460660413
INFO:root:current train perplexity5.755171298980713
INFO:root:current mean train loss 2216.2652659845203
INFO:root:current train perplexity5.743244647979736
INFO:root:current mean train loss 2215.740110474205
INFO:root:current train perplexity5.744606018066406
INFO:root:current mean train loss 2214.3036231765814
INFO:root:current train perplexity5.745704650878906
INFO:root:current mean train loss 2215.6645870073253
INFO:root:current train perplexity5.743532180786133
INFO:root:current mean train loss 2215.9861404206863
INFO:root:current train perplexity5.744010925292969
INFO:root:current mean train loss 2216.142182291001
INFO:root:current train perplexity5.7392802238464355
INFO:root:current mean train loss 2215.918381779226
INFO:root:current train perplexity5.736078262329102
INFO:root:current mean train loss 2215.286292228859
INFO:root:current train perplexity5.735374450683594
INFO:root:current mean train loss 2215.4219978302294
INFO:root:current train perplexity5.734783172607422
INFO:root:current mean train loss 2213.9650169499237
INFO:root:current train perplexity5.730935573577881
INFO:root:current mean train loss 2213.531457463678
INFO:root:current train perplexity5.725617408752441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:08<00:00, 608.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:08<00:00, 608.62s/it]
INFO:root:final mean train loss: 2212.0368353477706
INFO:root:final train perplexity: 5.7232985496521
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.36s/it]
INFO:root:eval mean loss: 2114.832316080729
INFO:root:eval perplexity: 5.530909061431885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.97s/it]
INFO:root:eval mean loss: 2492.379643433483
INFO:root:eval perplexity: 7.677826404571533
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/5
  2%|â–Ž         | 5/200 [1:02:43<41:15:21, 761.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2167.833289736793
INFO:root:current train perplexity5.513123512268066
INFO:root:current mean train loss 2150.509437892748
INFO:root:current train perplexity5.460352420806885
INFO:root:current mean train loss 2156.5802676778444
INFO:root:current train perplexity5.4577460289001465
INFO:root:current mean train loss 2160.1456457773843
INFO:root:current train perplexity5.466097831726074
INFO:root:current mean train loss 2163.067890514027
INFO:root:current train perplexity5.482896327972412
INFO:root:current mean train loss 2160.8998872939856
INFO:root:current train perplexity5.475297451019287
INFO:root:current mean train loss 2161.9573767589545
INFO:root:current train perplexity5.479130744934082
INFO:root:current mean train loss 2162.12614767892
INFO:root:current train perplexity5.485966205596924
INFO:root:current mean train loss 2165.9450058052444
INFO:root:current train perplexity5.492142677307129
INFO:root:current mean train loss 2165.220262480945
INFO:root:current train perplexity5.491004467010498
INFO:root:current mean train loss 2163.9082183274836
INFO:root:current train perplexity5.484793186187744
INFO:root:current mean train loss 2162.334387495711
INFO:root:current train perplexity5.486430644989014
INFO:root:current mean train loss 2161.5750037457715
INFO:root:current train perplexity5.486220836639404
INFO:root:current mean train loss 2158.7326472287923
INFO:root:current train perplexity5.480762481689453
INFO:root:current mean train loss 2159.1555464505504
INFO:root:current train perplexity5.482274532318115
INFO:root:current mean train loss 2156.9398155597723
INFO:root:current train perplexity5.479925155639648
INFO:root:current mean train loss 2156.7634536851897
INFO:root:current train perplexity5.478693008422852
INFO:root:current mean train loss 2155.2343409927435
INFO:root:current train perplexity5.473212242126465
INFO:root:current mean train loss 2154.945812444019
INFO:root:current train perplexity5.4684553146362305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:58<00:00, 598.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:58<00:00, 598.88s/it]
INFO:root:final mean train loss: 2153.4077425450314
INFO:root:final train perplexity: 5.464687347412109
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.87s/it]
INFO:root:eval mean loss: 2079.489192448609
INFO:root:eval perplexity: 5.375054836273193
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.14s/it]
INFO:root:eval mean loss: 2462.5102660267066
INFO:root:eval perplexity: 7.4925456047058105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/6
  3%|â–Ž         | 6/200 [1:15:21<40:58:15, 760.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2096.678466796875
INFO:root:current train perplexity4.919568061828613
INFO:root:current mean train loss 2113.38295149095
INFO:root:current train perplexity5.275344371795654
INFO:root:current mean train loss 2110.83642699588
INFO:root:current train perplexity5.256314754486084
INFO:root:current mean train loss 2113.424712627829
INFO:root:current train perplexity5.283567905426025
INFO:root:current mean train loss 2113.150138874006
INFO:root:current train perplexity5.279366970062256
INFO:root:current mean train loss 2109.386764556824
INFO:root:current train perplexity5.272693634033203
INFO:root:current mean train loss 2109.813927471142
INFO:root:current train perplexity5.283876419067383
INFO:root:current mean train loss 2110.971615082527
INFO:root:current train perplexity5.283054351806641
INFO:root:current mean train loss 2108.5023769421523
INFO:root:current train perplexity5.274503231048584
INFO:root:current mean train loss 2109.527809270082
INFO:root:current train perplexity5.2721076011657715
INFO:root:current mean train loss 2107.731443605223
INFO:root:current train perplexity5.270232200622559
INFO:root:current mean train loss 2108.7347701485864
INFO:root:current train perplexity5.274420261383057
INFO:root:current mean train loss 2107.254667537794
INFO:root:current train perplexity5.268606185913086
INFO:root:current mean train loss 2105.9609184529027
INFO:root:current train perplexity5.267640590667725
INFO:root:current mean train loss 2107.068678970936
INFO:root:current train perplexity5.270970821380615
INFO:root:current mean train loss 2105.8322249685107
INFO:root:current train perplexity5.271243095397949
INFO:root:current mean train loss 2106.421073498985
INFO:root:current train perplexity5.268612861633301
INFO:root:current mean train loss 2105.475199130337
INFO:root:current train perplexity5.265627384185791
INFO:root:current mean train loss 2105.129247043743
INFO:root:current train perplexity5.263999938964844
INFO:root:current mean train loss 2105.6601584974806
INFO:root:current train perplexity5.264793872833252

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:43<00:00, 583.39s/it]
INFO:root:final mean train loss: 2105.4105877128445
INFO:root:final train perplexity: 5.261695384979248
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.83s/it]
INFO:root:eval mean loss: 2058.3011539540394
INFO:root:eval perplexity: 5.2837347984313965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.61s/it]
INFO:root:eval mean loss: 2444.843808870789
INFO:root:eval perplexity: 7.385070323944092
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/7
  4%|â–Ž         | 7/200 [1:27:15<39:56:22, 744.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2097.0965983072915
INFO:root:current train perplexity5.191788196563721
INFO:root:current mean train loss 2066.4245181326137
INFO:root:current train perplexity5.1284708976745605
INFO:root:current mean train loss 2071.6816938207785
INFO:root:current train perplexity5.135545253753662
INFO:root:current mean train loss 2075.2032171285377
INFO:root:current train perplexity5.123077392578125
INFO:root:current mean train loss 2072.8160934813286
INFO:root:current train perplexity5.1104817390441895
INFO:root:current mean train loss 2071.5344933469323
INFO:root:current train perplexity5.112520694732666
INFO:root:current mean train loss 2071.696809145239
INFO:root:current train perplexity5.109957218170166
INFO:root:current mean train loss 2072.2617639738205
INFO:root:current train perplexity5.112542629241943
INFO:root:current mean train loss 2072.1750177882413
INFO:root:current train perplexity5.113280773162842
INFO:root:current mean train loss 2069.5098080437688
INFO:root:current train perplexity5.111392974853516
INFO:root:current mean train loss 2067.0066172181973
INFO:root:current train perplexity5.10469388961792
INFO:root:current mean train loss 2065.136616879158
INFO:root:current train perplexity5.098968029022217
INFO:root:current mean train loss 2065.798722591306
INFO:root:current train perplexity5.102963447570801
INFO:root:current mean train loss 2065.728438844811
INFO:root:current train perplexity5.105863571166992
INFO:root:current mean train loss 2065.5417341869875
INFO:root:current train perplexity5.102377414703369
INFO:root:current mean train loss 2065.5364911427455
INFO:root:current train perplexity5.100568771362305
INFO:root:current mean train loss 2064.778686749773
INFO:root:current train perplexity5.095571994781494
INFO:root:current mean train loss 2065.715048811072
INFO:root:current train perplexity5.0970540046691895
INFO:root:current mean train loss 2066.3094928938694
INFO:root:current train perplexity5.098805904388428
INFO:root:current mean train loss 2066.6721184405346
INFO:root:current train perplexity5.098170280456543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.78s/it]
INFO:root:final mean train loss: 2065.0815691310713
INFO:root:final train perplexity: 5.0969767570495605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.16s/it]
INFO:root:eval mean loss: 2031.8734360282303
INFO:root:eval perplexity: 5.172002792358398
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.84s/it]
INFO:root:eval mean loss: 2419.643640829316
INFO:root:eval perplexity: 7.234424591064453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/8
  4%|â–         | 8/200 [1:38:37<38:40:35, 725.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2014.3405308314732
INFO:root:current train perplexity4.908824443817139
INFO:root:current mean train loss 2009.8780327690972
INFO:root:current train perplexity4.89567756652832
INFO:root:current mean train loss 2025.6119395154587
INFO:root:current train perplexity4.940303325653076
INFO:root:current mean train loss 2030.2701058914413
INFO:root:current train perplexity4.941370964050293
INFO:root:current mean train loss 2027.8030001234733
INFO:root:current train perplexity4.9495849609375
INFO:root:current mean train loss 2028.761818687938
INFO:root:current train perplexity4.952300071716309
INFO:root:current mean train loss 2028.4418970303273
INFO:root:current train perplexity4.955522537231445
INFO:root:current mean train loss 2030.296946747449
INFO:root:current train perplexity4.962951183319092
INFO:root:current mean train loss 2029.806626151993
INFO:root:current train perplexity4.963916778564453
INFO:root:current mean train loss 2030.2793729894302
INFO:root:current train perplexity4.963030815124512
INFO:root:current mean train loss 2029.6473214117225
INFO:root:current train perplexity4.96339750289917
INFO:root:current mean train loss 2032.2766808060298
INFO:root:current train perplexity4.970849990844727
INFO:root:current mean train loss 2033.3205373663652
INFO:root:current train perplexity4.971291542053223
INFO:root:current mean train loss 2032.4952355088367
INFO:root:current train perplexity4.970565319061279
INFO:root:current mean train loss 2032.9236372359537
INFO:root:current train perplexity4.968435287475586
INFO:root:current mean train loss 2032.9428475544585
INFO:root:current train perplexity4.9681572914123535
INFO:root:current mean train loss 2033.2123132734853
INFO:root:current train perplexity4.965401649475098
INFO:root:current mean train loss 2032.1530279769677
INFO:root:current train perplexity4.9648637771606445
INFO:root:current mean train loss 2032.0045635670342
INFO:root:current train perplexity4.963306903839111
INFO:root:current mean train loss 2031.1450576348514
INFO:root:current train perplexity4.958559989929199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:33<00:00, 573.90s/it]
INFO:root:final mean train loss: 2030.084193796686
INFO:root:final train perplexity: 4.958218574523926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.31s/it]
INFO:root:eval mean loss: 2016.660741062029
INFO:root:eval perplexity: 5.108760833740234
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.40s/it]
INFO:root:eval mean loss: 2410.892982861674
INFO:root:eval perplexity: 7.1828389167785645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/9
  4%|â–         | 9/200 [1:49:51<37:36:51, 708.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1995.4056326059194
INFO:root:current train perplexity4.815788269042969
INFO:root:current mean train loss 1995.9835719058387
INFO:root:current train perplexity4.786581516265869
INFO:root:current mean train loss 1992.6605292426216
INFO:root:current train perplexity4.81314754486084
INFO:root:current mean train loss 1992.9432047063653
INFO:root:current train perplexity4.808210849761963
INFO:root:current mean train loss 1993.6429651311014
INFO:root:current train perplexity4.8219685554504395
INFO:root:current mean train loss 1994.4128285283628
INFO:root:current train perplexity4.82834005355835
INFO:root:current mean train loss 1999.5555592168328
INFO:root:current train perplexity4.832861423492432
INFO:root:current mean train loss 1999.9257622576774
INFO:root:current train perplexity4.837565898895264
INFO:root:current mean train loss 2000.2448405234468
INFO:root:current train perplexity4.837129592895508
INFO:root:current mean train loss 1999.335539104558
INFO:root:current train perplexity4.843644142150879
INFO:root:current mean train loss 2000.7423305729044
INFO:root:current train perplexity4.846856594085693
INFO:root:current mean train loss 2001.9048738479614
INFO:root:current train perplexity4.845954418182373
INFO:root:current mean train loss 2002.2788981964793
INFO:root:current train perplexity4.847306251525879
INFO:root:current mean train loss 2001.6697522225465
INFO:root:current train perplexity4.846273422241211
INFO:root:current mean train loss 2000.89979523798
INFO:root:current train perplexity4.844071865081787
INFO:root:current mean train loss 2000.2649468490758
INFO:root:current train perplexity4.842978000640869
INFO:root:current mean train loss 2000.2124613838103
INFO:root:current train perplexity4.844123363494873
INFO:root:current mean train loss 2000.3730134310788
INFO:root:current train perplexity4.842184543609619
INFO:root:current mean train loss 1999.4697240578175
INFO:root:current train perplexity4.840233325958252
INFO:root:current mean train loss 2000.4090608690606
INFO:root:current train perplexity4.840489387512207

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:35<00:00, 575.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:35<00:00, 575.38s/it]
INFO:root:final mean train loss: 1999.5506450828134
INFO:root:final train perplexity: 4.840247631072998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.46s/it]
INFO:root:eval mean loss: 2001.893852937306
INFO:root:eval perplexity: 5.0481109619140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.39s/it]
INFO:root:eval mean loss: 2401.69364810159
INFO:root:eval perplexity: 7.129000663757324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/10
  5%|â–Œ         | 10/200 [2:01:05<36:51:35, 698.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1974.7379044242527
INFO:root:current train perplexity4.703981876373291
INFO:root:current mean train loss 1965.5831486628606
INFO:root:current train perplexity4.715323448181152
INFO:root:current mean train loss 1965.1143803366056
INFO:root:current train perplexity4.7385077476501465
INFO:root:current mean train loss 1966.7532472688008
INFO:root:current train perplexity4.72737979888916
INFO:root:current mean train loss 1969.2682288196295
INFO:root:current train perplexity4.734795093536377
INFO:root:current mean train loss 1970.7830673244591
INFO:root:current train perplexity4.73062801361084
INFO:root:current mean train loss 1971.1681862504672
INFO:root:current train perplexity4.727970123291016
INFO:root:current mean train loss 1970.6085811461212
INFO:root:current train perplexity4.7296342849731445
INFO:root:current mean train loss 1972.7261754991819
INFO:root:current train perplexity4.734572410583496
INFO:root:current mean train loss 1972.4422903464429
INFO:root:current train perplexity4.734443187713623
INFO:root:current mean train loss 1971.9787465194545
INFO:root:current train perplexity4.7321343421936035
INFO:root:current mean train loss 1971.4655962210622
INFO:root:current train perplexity4.732608318328857
INFO:root:current mean train loss 1971.0463718086646
INFO:root:current train perplexity4.72734260559082
INFO:root:current mean train loss 1972.2846141115779
INFO:root:current train perplexity4.728014945983887
INFO:root:current mean train loss 1972.2302581807883
INFO:root:current train perplexity4.726977825164795
INFO:root:current mean train loss 1971.4621158791927
INFO:root:current train perplexity4.728572368621826
INFO:root:current mean train loss 1972.1762369109028
INFO:root:current train perplexity4.730515003204346
INFO:root:current mean train loss 1971.3144358046786
INFO:root:current train perplexity4.729835510253906
INFO:root:current mean train loss 1971.8529862617668
INFO:root:current train perplexity4.733147621154785
INFO:root:current mean train loss 1972.4205712841028
INFO:root:current train perplexity4.735083103179932

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.33s/it]
INFO:root:final mean train loss: 1971.4764043872428
INFO:root:final train perplexity: 4.734257698059082
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it]
INFO:root:eval mean loss: 1996.1762227809174
INFO:root:eval perplexity: 5.02482271194458
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.34s/it]
INFO:root:eval mean loss: 2399.849132781333
INFO:root:eval perplexity: 7.118253707885742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/11
  6%|â–Œ         | 11/200 [2:12:27<36:23:16, 693.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1929.9940057798874
INFO:root:current train perplexity4.61422872543335
INFO:root:current mean train loss 1936.760233561198
INFO:root:current train perplexity4.623414516448975
INFO:root:current mean train loss 1940.6742079941544
INFO:root:current train perplexity4.619727611541748
INFO:root:current mean train loss 1940.1550656649733
INFO:root:current train perplexity4.61625862121582
INFO:root:current mean train loss 1939.0758315349312
INFO:root:current train perplexity4.618529796600342
INFO:root:current mean train loss 1937.7313365740988
INFO:root:current train perplexity4.619421482086182
INFO:root:current mean train loss 1939.4782387424837
INFO:root:current train perplexity4.627747535705566
INFO:root:current mean train loss 1940.712369418933
INFO:root:current train perplexity4.627920150756836
INFO:root:current mean train loss 1941.1291574172458
INFO:root:current train perplexity4.6259026527404785
INFO:root:current mean train loss 1942.3880023453346
INFO:root:current train perplexity4.627762317657471
INFO:root:current mean train loss 1943.2333287472663
INFO:root:current train perplexity4.630187034606934
INFO:root:current mean train loss 1941.7055510702664
INFO:root:current train perplexity4.629587650299072
INFO:root:current mean train loss 1943.247919014416
INFO:root:current train perplexity4.633625507354736
INFO:root:current mean train loss 1942.7727787078372
INFO:root:current train perplexity4.631476879119873
INFO:root:current mean train loss 1943.3491535417807
INFO:root:current train perplexity4.633810520172119
INFO:root:current mean train loss 1944.5672099436968
INFO:root:current train perplexity4.637241840362549
INFO:root:current mean train loss 1944.6892740740611
INFO:root:current train perplexity4.637642860412598
INFO:root:current mean train loss 1944.6573018141316
INFO:root:current train perplexity4.637269973754883
INFO:root:current mean train loss 1945.9038555189761
INFO:root:current train perplexity4.637479305267334

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:38<00:00, 578.37s/it]
INFO:root:final mean train loss: 1945.2645173796607
INFO:root:final train perplexity: 4.637394428253174
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.66s/it]
INFO:root:eval mean loss: 1982.650200593556
INFO:root:eval perplexity: 4.970155715942383
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.91s/it]
INFO:root:eval mean loss: 2389.1056921126997
INFO:root:eval perplexity: 7.055985450744629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/12
  6%|â–Œ         | 12/200 [2:23:46<35:58:44, 688.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2028.6767578125
INFO:root:current train perplexity4.699362754821777
INFO:root:current mean train loss 1923.8021228382888
INFO:root:current train perplexity4.55756139755249
INFO:root:current mean train loss 1922.6260415063116
INFO:root:current train perplexity4.539605617523193
INFO:root:current mean train loss 1924.8224134791408
INFO:root:current train perplexity4.5297627449035645
INFO:root:current mean train loss 1923.1094713234725
INFO:root:current train perplexity4.535068511962891
INFO:root:current mean train loss 1919.3060683749068
INFO:root:current train perplexity4.531269550323486
INFO:root:current mean train loss 1918.1635993211028
INFO:root:current train perplexity4.527945518493652
INFO:root:current mean train loss 1916.8626455467083
INFO:root:current train perplexity4.527904510498047
INFO:root:current mean train loss 1916.959162235854
INFO:root:current train perplexity4.534383296966553
INFO:root:current mean train loss 1917.984130994558
INFO:root:current train perplexity4.536798000335693
INFO:root:current mean train loss 1919.8457397582642
INFO:root:current train perplexity4.538284778594971
INFO:root:current mean train loss 1920.826564757692
INFO:root:current train perplexity4.543855667114258
INFO:root:current mean train loss 1921.022936128916
INFO:root:current train perplexity4.543985843658447
INFO:root:current mean train loss 1922.0265817239663
INFO:root:current train perplexity4.546998500823975
INFO:root:current mean train loss 1922.8426277883887
INFO:root:current train perplexity4.548213481903076
INFO:root:current mean train loss 1923.125274191201
INFO:root:current train perplexity4.549560546875
INFO:root:current mean train loss 1921.8761752408814
INFO:root:current train perplexity4.549814224243164
INFO:root:current mean train loss 1922.5061666653287
INFO:root:current train perplexity4.55104398727417
INFO:root:current mean train loss 1921.832025562864
INFO:root:current train perplexity4.5514116287231445
INFO:root:current mean train loss 1921.8318348983307
INFO:root:current train perplexity4.5524725914001465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:35<00:00, 575.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:35<00:00, 575.25s/it]
INFO:root:final mean train loss: 1921.5731256266645
INFO:root:final train perplexity: 4.551551342010498
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.44s/it]
INFO:root:eval mean loss: 1976.0518664637357
INFO:root:eval perplexity: 4.943703651428223
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.40s/it]
INFO:root:eval mean loss: 2388.4999095294493
INFO:root:eval perplexity: 7.052489757537842
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/13
  6%|â–‹         | 13/200 [2:35:00<35:32:49, 684.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1865.4655700683593
INFO:root:current train perplexity4.411606311798096
INFO:root:current mean train loss 1876.1846048990885
INFO:root:current train perplexity4.413663387298584
INFO:root:current mean train loss 1884.0788130326705
INFO:root:current train perplexity4.431356430053711
INFO:root:current mean train loss 1887.8541255950927
INFO:root:current train perplexity4.432684898376465
INFO:root:current mean train loss 1888.359654017857
INFO:root:current train perplexity4.425756454467773
INFO:root:current mean train loss 1891.8047971285307
INFO:root:current train perplexity4.4348554611206055
INFO:root:current mean train loss 1894.6699770035282
INFO:root:current train perplexity4.439572811126709
INFO:root:current mean train loss 1897.2610985649956
INFO:root:current train perplexity4.446263790130615
INFO:root:current mean train loss 1896.3686228682354
INFO:root:current train perplexity4.449056148529053
INFO:root:current mean train loss 1896.0434793223505
INFO:root:current train perplexity4.453038215637207
INFO:root:current mean train loss 1897.7906498927696
INFO:root:current train perplexity4.4567365646362305
INFO:root:current mean train loss 1898.3064068385534
INFO:root:current train perplexity4.460909843444824
INFO:root:current mean train loss 1899.0986873439101
INFO:root:current train perplexity4.462148666381836
INFO:root:current mean train loss 1899.151528005889
INFO:root:current train perplexity4.465237140655518
INFO:root:current mean train loss 1900.1763013383033
INFO:root:current train perplexity4.466545104980469
INFO:root:current mean train loss 1901.7412849827817
INFO:root:current train perplexity4.470339298248291
INFO:root:current mean train loss 1900.5233470775463
INFO:root:current train perplexity4.4698309898376465
INFO:root:current mean train loss 1899.715918678461
INFO:root:current train perplexity4.471961498260498
INFO:root:current mean train loss 1900.4253682230856
INFO:root:current train perplexity4.471847057342529
INFO:root:current mean train loss 1900.3267429987588
INFO:root:current train perplexity4.472935199737549

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:39<00:00, 579.92s/it]
INFO:root:final mean train loss: 1899.6753781963585
INFO:root:final train perplexity: 4.4736223220825195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.25s/it]
INFO:root:eval mean loss: 1970.7188690401983
INFO:root:eval perplexity: 4.922427654266357
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.23s/it]
INFO:root:eval mean loss: 2383.823636535212
INFO:root:eval perplexity: 7.025570869445801
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/14
  7%|â–‹         | 14/200 [2:46:21<35:18:15, 683.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1873.6206351615288
INFO:root:current train perplexity4.438111782073975
INFO:root:current mean train loss 1869.8545661425069
INFO:root:current train perplexity4.401790142059326
INFO:root:current mean train loss 1863.9970378634296
INFO:root:current train perplexity4.3861494064331055
INFO:root:current mean train loss 1864.4857999988408
INFO:root:current train perplexity4.392324924468994
INFO:root:current mean train loss 1870.098773039724
INFO:root:current train perplexity4.400171756744385
INFO:root:current mean train loss 1867.1299739674262
INFO:root:current train perplexity4.393218517303467
INFO:root:current mean train loss 1869.9129477700035
INFO:root:current train perplexity4.391230583190918
INFO:root:current mean train loss 1870.0203125331263
INFO:root:current train perplexity4.388215065002441
INFO:root:current mean train loss 1873.5560013965894
INFO:root:current train perplexity4.395333290100098
INFO:root:current mean train loss 1873.8894590135571
INFO:root:current train perplexity4.3925604820251465
INFO:root:current mean train loss 1874.0454987955416
INFO:root:current train perplexity4.395333766937256
INFO:root:current mean train loss 1872.627505179131
INFO:root:current train perplexity4.39219331741333
INFO:root:current mean train loss 1873.1815652315329
INFO:root:current train perplexity4.394533634185791
INFO:root:current mean train loss 1874.636183083191
INFO:root:current train perplexity4.3952531814575195
INFO:root:current mean train loss 1875.7459808540743
INFO:root:current train perplexity4.399593830108643
INFO:root:current mean train loss 1878.7979722826528
INFO:root:current train perplexity4.4029459953308105
INFO:root:current mean train loss 1878.7623278338806
INFO:root:current train perplexity4.40178108215332
INFO:root:current mean train loss 1878.7299205931563
INFO:root:current train perplexity4.401139736175537
INFO:root:current mean train loss 1880.2409109781147
INFO:root:current train perplexity4.401491165161133
INFO:root:current mean train loss 1879.8753866925335
INFO:root:current train perplexity4.402171611785889

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:42<00:00, 582.39s/it]
INFO:root:final mean train loss: 1879.4412453671146
INFO:root:final train perplexity: 4.402799129486084
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.75s/it]
INFO:root:eval mean loss: 1966.172441631344
INFO:root:eval perplexity: 4.904361248016357
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.59s/it]
INFO:root:eval mean loss: 2385.040193165448
INFO:root:eval perplexity: 7.032565116882324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/15
  8%|â–Š         | 15/200 [2:57:46<35:08:38, 683.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1847.1824973777489
INFO:root:current train perplexity4.298196315765381
INFO:root:current mean train loss 1838.7555977957588
INFO:root:current train perplexity4.316033840179443
INFO:root:current mean train loss 1852.1798672413263
INFO:root:current train perplexity4.324304103851318
INFO:root:current mean train loss 1853.4773583708509
INFO:root:current train perplexity4.328930377960205
INFO:root:current mean train loss 1853.8808921780355
INFO:root:current train perplexity4.325406551361084
INFO:root:current mean train loss 1855.6610618618852
INFO:root:current train perplexity4.3348774909973145
INFO:root:current mean train loss 1858.223003982404
INFO:root:current train perplexity4.331140518188477
INFO:root:current mean train loss 1857.7748090910975
INFO:root:current train perplexity4.333152770996094
INFO:root:current mean train loss 1859.1133235600848
INFO:root:current train perplexity4.3341803550720215
INFO:root:current mean train loss 1857.123138619669
INFO:root:current train perplexity4.332001686096191
INFO:root:current mean train loss 1857.666194908533
INFO:root:current train perplexity4.330183506011963
INFO:root:current mean train loss 1859.3186411733643
INFO:root:current train perplexity4.330810070037842
INFO:root:current mean train loss 1859.6933253043387
INFO:root:current train perplexity4.3321404457092285
INFO:root:current mean train loss 1859.6348827367694
INFO:root:current train perplexity4.33272647857666
INFO:root:current mean train loss 1859.3943072585325
INFO:root:current train perplexity4.333682537078857
INFO:root:current mean train loss 1858.3526068531562
INFO:root:current train perplexity4.334771156311035
INFO:root:current mean train loss 1857.1043910034623
INFO:root:current train perplexity4.333464622497559
INFO:root:current mean train loss 1858.3990378298265
INFO:root:current train perplexity4.335294723510742
INFO:root:current mean train loss 1858.9781390769324
INFO:root:current train perplexity4.333957195281982
INFO:root:current mean train loss 1859.5501275428608
INFO:root:current train perplexity4.333024501800537

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:44<00:00, 584.61s/it]
INFO:root:final mean train loss: 1859.2703564403878
INFO:root:final train perplexity: 4.333313941955566
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.78s/it]
INFO:root:eval mean loss: 1963.5185737339318
INFO:root:eval perplexity: 4.89384651184082
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.24s/it]
INFO:root:eval mean loss: 2381.927577674812
INFO:root:eval perplexity: 7.014684677124023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/16
  8%|â–Š         | 16/200 [3:09:13<35:00:12, 684.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1828.9792532047754
INFO:root:current train perplexity4.2404069900512695
INFO:root:current mean train loss 1829.9032146610014
INFO:root:current train perplexity4.256777763366699
INFO:root:current mean train loss 1825.1600909356262
INFO:root:current train perplexity4.2401957511901855
INFO:root:current mean train loss 1831.5140640793463
INFO:root:current train perplexity4.2469892501831055
INFO:root:current mean train loss 1834.6595484901638
INFO:root:current train perplexity4.250247001647949
INFO:root:current mean train loss 1836.562300326319
INFO:root:current train perplexity4.256626129150391
INFO:root:current mean train loss 1836.4695189883919
INFO:root:current train perplexity4.25633430480957
INFO:root:current mean train loss 1835.3171875949963
INFO:root:current train perplexity4.255796909332275
INFO:root:current mean train loss 1837.4434874717458
INFO:root:current train perplexity4.256944179534912
INFO:root:current mean train loss 1839.5416434510973
INFO:root:current train perplexity4.261312007904053
INFO:root:current mean train loss 1838.4384342767055
INFO:root:current train perplexity4.259456634521484
INFO:root:current mean train loss 1839.589369958736
INFO:root:current train perplexity4.263169288635254
INFO:root:current mean train loss 1841.3772271349146
INFO:root:current train perplexity4.266170978546143
INFO:root:current mean train loss 1840.5230033535056
INFO:root:current train perplexity4.265048980712891
INFO:root:current mean train loss 1840.5596347915161
INFO:root:current train perplexity4.265824794769287
INFO:root:current mean train loss 1841.8538447719407
INFO:root:current train perplexity4.26646089553833
INFO:root:current mean train loss 1842.7624372919472
INFO:root:current train perplexity4.268139362335205
INFO:root:current mean train loss 1842.4030750690376
INFO:root:current train perplexity4.268481254577637
INFO:root:current mean train loss 1841.5195644588664
INFO:root:current train perplexity4.268210411071777
INFO:root:current mean train loss 1840.9706362371576
INFO:root:current train perplexity4.268513202667236

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:37<00:00, 577.04s/it]
INFO:root:final mean train loss: 1840.3944259851314
INFO:root:final train perplexity: 4.269282341003418
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.83s/it]
INFO:root:eval mean loss: 1960.3314105094748
INFO:root:eval perplexity: 4.881247043609619
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.34s/it]
INFO:root:eval mean loss: 2384.618146747562
INFO:root:eval perplexity: 7.0301384925842285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/17
  8%|â–Š         | 17/200 [3:20:31<34:42:17, 682.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1818.5251908735795
INFO:root:current train perplexity4.1596479415893555
INFO:root:current mean train loss 1813.9859249033827
INFO:root:current train perplexity4.152613162994385
INFO:root:current mean train loss 1811.209774017334
INFO:root:current train perplexity4.17131233215332
INFO:root:current mean train loss 1812.948161960877
INFO:root:current train perplexity4.185598373413086
INFO:root:current mean train loss 1817.5696956446914
INFO:root:current train perplexity4.188132286071777
INFO:root:current mean train loss 1817.6001985511002
INFO:root:current train perplexity4.191344261169434
INFO:root:current mean train loss 1820.303149378577
INFO:root:current train perplexity4.194478511810303
INFO:root:current mean train loss 1818.9692110168148
INFO:root:current train perplexity4.190598011016846
INFO:root:current mean train loss 1822.4488125362911
INFO:root:current train perplexity4.194699287414551
INFO:root:current mean train loss 1822.2489601783907
INFO:root:current train perplexity4.196763038635254
INFO:root:current mean train loss 1824.1532127155976
INFO:root:current train perplexity4.199188709259033
INFO:root:current mean train loss 1823.381154481008
INFO:root:current train perplexity4.200007915496826
INFO:root:current mean train loss 1822.3717789738814
INFO:root:current train perplexity4.2002081871032715
INFO:root:current mean train loss 1823.3316841235421
INFO:root:current train perplexity4.201980113983154
INFO:root:current mean train loss 1822.8159114878665
INFO:root:current train perplexity4.202103137969971
INFO:root:current mean train loss 1823.0437132405393
INFO:root:current train perplexity4.2060675621032715
INFO:root:current mean train loss 1823.2286367551976
INFO:root:current train perplexity4.207606792449951
INFO:root:current mean train loss 1823.7805132769897
INFO:root:current train perplexity4.209823131561279
INFO:root:current mean train loss 1822.8157511565644
INFO:root:current train perplexity4.20964241027832

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:25<00:00, 565.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:25<00:00, 565.09s/it]
INFO:root:final mean train loss: 1822.984638716197
INFO:root:final train perplexity: 4.211063861846924
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.69s/it]
INFO:root:eval mean loss: 1959.5123360275377
INFO:root:eval perplexity: 4.878015995025635
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.25s/it]
INFO:root:eval mean loss: 2386.7194841533687
INFO:root:eval perplexity: 7.042228698730469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/18
  9%|â–‰         | 18/200 [3:31:36<34:15:14, 677.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1837.80068359375
INFO:root:current train perplexity4.1795148849487305
INFO:root:current mean train loss 1797.189197358631
INFO:root:current train perplexity4.111528396606445
INFO:root:current mean train loss 1795.165361804497
INFO:root:current train perplexity4.132484436035156
INFO:root:current mean train loss 1795.7216588755123
INFO:root:current train perplexity4.1333136558532715
INFO:root:current mean train loss 1789.8841281467014
INFO:root:current train perplexity4.126739025115967
INFO:root:current mean train loss 1794.2873871151764
INFO:root:current train perplexity4.133361339569092
INFO:root:current mean train loss 1795.8883179719783
INFO:root:current train perplexity4.137808322906494
INFO:root:current mean train loss 1797.6630196212877
INFO:root:current train perplexity4.1378302574157715
INFO:root:current mean train loss 1801.056525681774
INFO:root:current train perplexity4.140042304992676
INFO:root:current mean train loss 1802.5132026124395
INFO:root:current train perplexity4.142558574676514
INFO:root:current mean train loss 1803.2056947926383
INFO:root:current train perplexity4.1447577476501465
INFO:root:current mean train loss 1804.3924962660847
INFO:root:current train perplexity4.147802352905273
INFO:root:current mean train loss 1804.9273783956821
INFO:root:current train perplexity4.149542808532715
INFO:root:current mean train loss 1806.0411980289152
INFO:root:current train perplexity4.152251243591309
INFO:root:current mean train loss 1806.1281846015902
INFO:root:current train perplexity4.153512001037598
INFO:root:current mean train loss 1805.232770322882
INFO:root:current train perplexity4.150622844696045
INFO:root:current mean train loss 1804.5926552460573
INFO:root:current train perplexity4.150000095367432
INFO:root:current mean train loss 1804.8311131094208
INFO:root:current train perplexity4.150934219360352
INFO:root:current mean train loss 1804.8720237837604
INFO:root:current train perplexity4.151698589324951
INFO:root:current mean train loss 1805.6086931363805
INFO:root:current train perplexity4.152803421020508

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.89s/it]
INFO:root:final mean train loss: 1805.6028028710346
INFO:root:final train perplexity: 4.153731346130371
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it]
INFO:root:eval mean loss: 1959.4984710909796
INFO:root:eval perplexity: 4.877962112426758
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.47s/it]
INFO:root:eval mean loss: 2388.581130873227
INFO:root:eval perplexity: 7.052960395812988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/19
 10%|â–‰         | 19/200 [3:42:36<33:48:17, 672.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1787.257695978338
INFO:root:current train perplexity3.9949533939361572
INFO:root:current mean train loss 1775.1411593077612
INFO:root:current train perplexity4.031291961669922
INFO:root:current mean train loss 1776.2776236319328
INFO:root:current train perplexity4.054975986480713
INFO:root:current mean train loss 1774.9885496530474
INFO:root:current train perplexity4.044338226318359
INFO:root:current mean train loss 1776.5336801248704
INFO:root:current train perplexity4.054362773895264
INFO:root:current mean train loss 1777.9885939090188
INFO:root:current train perplexity4.058823585510254
INFO:root:current mean train loss 1781.3247401982642
INFO:root:current train perplexity4.071868896484375
INFO:root:current mean train loss 1783.0504990680727
INFO:root:current train perplexity4.076478481292725
INFO:root:current mean train loss 1783.590237434183
INFO:root:current train perplexity4.072229385375977
INFO:root:current mean train loss 1781.7728621013252
INFO:root:current train perplexity4.071036338806152
INFO:root:current mean train loss 1782.6959246432011
INFO:root:current train perplexity4.075808525085449
INFO:root:current mean train loss 1783.8860946900068
INFO:root:current train perplexity4.077266216278076
INFO:root:current mean train loss 1784.2713039666658
INFO:root:current train perplexity4.0829691886901855
INFO:root:current mean train loss 1785.3619729184888
INFO:root:current train perplexity4.08514404296875
INFO:root:current mean train loss 1786.2934681909833
INFO:root:current train perplexity4.087831974029541
INFO:root:current mean train loss 1787.1178344245338
INFO:root:current train perplexity4.090973377227783
INFO:root:current mean train loss 1787.3383149359877
INFO:root:current train perplexity4.092388153076172
INFO:root:current mean train loss 1786.9557953766969
INFO:root:current train perplexity4.093227386474609
INFO:root:current mean train loss 1786.4264128399209
INFO:root:current train perplexity4.092203140258789
INFO:root:current mean train loss 1787.9538926075945
INFO:root:current train perplexity4.094729900360107

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.51s/it]
INFO:root:final mean train loss: 1788.3394583328409
INFO:root:final train perplexity: 4.097561836242676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.50s/it]
INFO:root:eval mean loss: 1960.6220079787233
INFO:root:eval perplexity: 4.882395267486572
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.07s/it]
INFO:root:eval mean loss: 2393.4406898444427
INFO:root:eval perplexity: 7.081046104431152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/20
 10%|â–ˆ         | 20/200 [3:53:42<33:31:10, 670.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1778.6578776041667
INFO:root:current train perplexity4.04986047744751
INFO:root:current mean train loss 1774.7770451607464
INFO:root:current train perplexity4.034038543701172
INFO:root:current mean train loss 1770.1242747286872
INFO:root:current train perplexity4.039781093597412
INFO:root:current mean train loss 1764.9013052521202
INFO:root:current train perplexity4.033505439758301
INFO:root:current mean train loss 1761.9026557383613
INFO:root:current train perplexity4.032423973083496
INFO:root:current mean train loss 1765.646244084459
INFO:root:current train perplexity4.03357458114624
INFO:root:current mean train loss 1769.4387256699922
INFO:root:current train perplexity4.033482074737549
INFO:root:current mean train loss 1770.223983826592
INFO:root:current train perplexity4.037951469421387
INFO:root:current mean train loss 1769.9385378158988
INFO:root:current train perplexity4.036723613739014
INFO:root:current mean train loss 1771.0413118957586
INFO:root:current train perplexity4.041722774505615
INFO:root:current mean train loss 1770.6541869059793
INFO:root:current train perplexity4.039741516113281
INFO:root:current mean train loss 1770.5918757544996
INFO:root:current train perplexity4.040340900421143
INFO:root:current mean train loss 1770.1666985882011
INFO:root:current train perplexity4.041041851043701
INFO:root:current mean train loss 1768.692830069373
INFO:root:current train perplexity4.040602207183838
INFO:root:current mean train loss 1771.2033072147542
INFO:root:current train perplexity4.041943073272705
INFO:root:current mean train loss 1772.105283304652
INFO:root:current train perplexity4.044147491455078
INFO:root:current mean train loss 1772.4847453817351
INFO:root:current train perplexity4.0451459884643555
INFO:root:current mean train loss 1772.5567172085025
INFO:root:current train perplexity4.046003818511963
INFO:root:current mean train loss 1772.3223637326248
INFO:root:current train perplexity4.04665994644165
INFO:root:current mean train loss 1772.694484763812
INFO:root:current train perplexity4.0468316078186035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:23<00:00, 563.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:23<00:00, 563.15s/it]
INFO:root:final mean train loss: 1772.6538403271065
INFO:root:final train perplexity: 4.047184944152832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:48<00:00, 48.23s/it]
INFO:root:eval mean loss: 1961.1246138768838
INFO:root:eval perplexity: 4.88438081741333
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.73s/it]
INFO:root:eval mean loss: 2398.2378159110426
INFO:root:eval perplexity: 7.108880996704102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/21
 10%|â–ˆ         | 21/200 [4:04:42<33:10:18, 667.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1746.4930201939173
INFO:root:current train perplexity3.9562923908233643
INFO:root:current mean train loss 1736.9402184119592
INFO:root:current train perplexity3.9481585025787354
INFO:root:current mean train loss 1744.7372646331787
INFO:root:current train perplexity3.9612717628479004
INFO:root:current mean train loss 1745.8852803090986
INFO:root:current train perplexity3.967777967453003
INFO:root:current mean train loss 1749.4866437410053
INFO:root:current train perplexity3.9713587760925293
INFO:root:current mean train loss 1752.206784474764
INFO:root:current train perplexity3.980884075164795
INFO:root:current mean train loss 1752.262162929628
INFO:root:current train perplexity3.9819064140319824
INFO:root:current mean train loss 1751.1851346454923
INFO:root:current train perplexity3.983828544616699
INFO:root:current mean train loss 1754.9999261303483
INFO:root:current train perplexity3.989609479904175
INFO:root:current mean train loss 1755.5136605107136
INFO:root:current train perplexity3.989283561706543
INFO:root:current mean train loss 1754.705478437019
INFO:root:current train perplexity3.988246202468872
INFO:root:current mean train loss 1754.0381138151492
INFO:root:current train perplexity3.9857518672943115
INFO:root:current mean train loss 1755.12685253362
INFO:root:current train perplexity3.988992691040039
INFO:root:current mean train loss 1756.3049222783002
INFO:root:current train perplexity3.9913618564605713
INFO:root:current mean train loss 1756.8249531840231
INFO:root:current train perplexity3.9908676147460938
INFO:root:current mean train loss 1758.0664414746595
INFO:root:current train perplexity3.9923858642578125
INFO:root:current mean train loss 1757.945237385478
INFO:root:current train perplexity3.9937098026275635
INFO:root:current mean train loss 1757.3144422109685
INFO:root:current train perplexity3.993783950805664
INFO:root:current mean train loss 1757.490941409407
INFO:root:current train perplexity3.9946558475494385
INFO:root:current mean train loss 1757.2399069819226
INFO:root:current train perplexity3.995835542678833

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.85s/it]
INFO:root:final mean train loss: 1756.542800264652
INFO:root:final train perplexity: 3.9960861206054688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.62s/it]
INFO:root:eval mean loss: 1962.2694481382978
INFO:root:eval perplexity: 4.888904094696045
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.71s/it]
INFO:root:eval mean loss: 2400.7473399926585
INFO:root:eval perplexity: 7.123485565185547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/22
 11%|â–ˆ         | 22/200 [4:15:20<32:33:39, 658.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1717.907201479559
INFO:root:current train perplexity3.8934483528137207
INFO:root:current mean train loss 1719.1584818404533
INFO:root:current train perplexity3.865316390991211
INFO:root:current mean train loss 1727.0925954741872
INFO:root:current train perplexity3.88590145111084
INFO:root:current mean train loss 1725.338050658198
INFO:root:current train perplexity3.8929638862609863
INFO:root:current mean train loss 1727.5964092230445
INFO:root:current train perplexity3.9025542736053467
INFO:root:current mean train loss 1730.5579403649242
INFO:root:current train perplexity3.9102659225463867
INFO:root:current mean train loss 1732.6521710740155
INFO:root:current train perplexity3.9170143604278564
INFO:root:current mean train loss 1733.8152837716284
INFO:root:current train perplexity3.9230306148529053
INFO:root:current mean train loss 1733.0029077344198
INFO:root:current train perplexity3.9235548973083496
INFO:root:current mean train loss 1736.1917788592787
INFO:root:current train perplexity3.930685520172119
INFO:root:current mean train loss 1737.6383485536317
INFO:root:current train perplexity3.9355711936950684
INFO:root:current mean train loss 1736.9781201713022
INFO:root:current train perplexity3.935978889465332
INFO:root:current mean train loss 1737.5135102972495
INFO:root:current train perplexity3.9373867511749268
INFO:root:current mean train loss 1739.2944921839437
INFO:root:current train perplexity3.939987897872925
INFO:root:current mean train loss 1739.581833766256
INFO:root:current train perplexity3.9423155784606934
INFO:root:current mean train loss 1740.121117419069
INFO:root:current train perplexity3.9433090686798096
INFO:root:current mean train loss 1740.6719369472178
INFO:root:current train perplexity3.9451982975006104
INFO:root:current mean train loss 1740.7513442881284
INFO:root:current train perplexity3.945409059524536
INFO:root:current mean train loss 1741.301920811887
INFO:root:current train perplexity3.948187828063965
INFO:root:current mean train loss 1741.7449684510423
INFO:root:current train perplexity3.948803424835205

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.34s/it]
INFO:root:final mean train loss: 1741.4267272795323
INFO:root:final train perplexity: 3.9487295150756836
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.15s/it]
INFO:root:eval mean loss: 1961.6120774289395
INFO:root:eval perplexity: 4.8863067626953125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.99s/it]
INFO:root:eval mean loss: 2402.4446627569537
INFO:root:eval perplexity: 7.13338041305542
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/23
 12%|â–ˆâ–        | 23/200 [4:25:28<31:37:40, 643.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1709.930753580729
INFO:root:current train perplexity3.8762407302856445
INFO:root:current mean train loss 1711.4450073242188
INFO:root:current train perplexity3.875540256500244
INFO:root:current mean train loss 1712.5031241581357
INFO:root:current train perplexity3.8759093284606934
INFO:root:current mean train loss 1716.754267139924
INFO:root:current train perplexity3.8788259029388428
INFO:root:current mean train loss 1717.0847100705516
INFO:root:current train perplexity3.8756308555603027
INFO:root:current mean train loss 1719.959291205972
INFO:root:current train perplexity3.8811025619506836
INFO:root:current mean train loss 1717.341599439538
INFO:root:current train perplexity3.8819196224212646
INFO:root:current mean train loss 1717.712366031695
INFO:root:current train perplexity3.882197380065918
INFO:root:current mean train loss 1718.3790838691625
INFO:root:current train perplexity3.878838539123535
INFO:root:current mean train loss 1719.2815470377604
INFO:root:current train perplexity3.8816452026367188
INFO:root:current mean train loss 1721.4851665531824
INFO:root:current train perplexity3.889056921005249
INFO:root:current mean train loss 1722.6562030183168
INFO:root:current train perplexity3.8889896869659424
INFO:root:current mean train loss 1721.8612129625424
INFO:root:current train perplexity3.8875222206115723
INFO:root:current mean train loss 1723.3285873742411
INFO:root:current train perplexity3.8925397396087646
INFO:root:current mean train loss 1723.664174247588
INFO:root:current train perplexity3.8940374851226807
INFO:root:current mean train loss 1724.438833407036
INFO:root:current train perplexity3.8963983058929443
INFO:root:current mean train loss 1724.9507248376249
INFO:root:current train perplexity3.8968381881713867
INFO:root:current mean train loss 1725.9190679965739
INFO:root:current train perplexity3.8981213569641113
INFO:root:current mean train loss 1725.7601695550181
INFO:root:current train perplexity3.8984925746917725

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:19<00:00, 499.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:19<00:00, 499.26s/it]
INFO:root:final mean train loss: 1725.9205202842325
INFO:root:final train perplexity: 3.900733709335327
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.29s/it]
INFO:root:eval mean loss: 1965.4774304112643
INFO:root:eval perplexity: 4.90160608291626
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.56s/it]
INFO:root:eval mean loss: 2408.9383099062225
INFO:root:eval perplexity: 7.171364784240723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/24
 12%|â–ˆâ–        | 24/200 [4:35:08<30:31:29, 624.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1690.6402239118304
INFO:root:current train perplexity3.7343642711639404
INFO:root:current mean train loss 1693.5944630275262
INFO:root:current train perplexity3.827763319015503
INFO:root:current mean train loss 1699.502561117716
INFO:root:current train perplexity3.8305888175964355
INFO:root:current mean train loss 1703.4812266197578
INFO:root:current train perplexity3.8370630741119385
INFO:root:current mean train loss 1704.0832243598356
INFO:root:current train perplexity3.841768980026245
INFO:root:current mean train loss 1706.0031740688949
INFO:root:current train perplexity3.840787887573242
INFO:root:current mean train loss 1710.0524180379298
INFO:root:current train perplexity3.8424575328826904
INFO:root:current mean train loss 1709.2903980217357
INFO:root:current train perplexity3.8422741889953613
INFO:root:current mean train loss 1710.165500569964
INFO:root:current train perplexity3.846724271774292
INFO:root:current mean train loss 1710.3304528149117
INFO:root:current train perplexity3.8497707843780518
INFO:root:current mean train loss 1709.8975977968673
INFO:root:current train perplexity3.8481526374816895
INFO:root:current mean train loss 1711.2306068096489
INFO:root:current train perplexity3.8503353595733643
INFO:root:current mean train loss 1711.2184361771501
INFO:root:current train perplexity3.852145195007324
INFO:root:current mean train loss 1710.804076494656
INFO:root:current train perplexity3.8507237434387207
INFO:root:current mean train loss 1710.2485745449649
INFO:root:current train perplexity3.8506479263305664
INFO:root:current mean train loss 1711.6723523459532
INFO:root:current train perplexity3.8537192344665527
INFO:root:current mean train loss 1711.3585719338246
INFO:root:current train perplexity3.852348566055298
INFO:root:current mean train loss 1711.4297952679913
INFO:root:current train perplexity3.8537285327911377
INFO:root:current mean train loss 1711.6332666664146
INFO:root:current train perplexity3.8549106121063232
INFO:root:current mean train loss 1711.7282009434812
INFO:root:current train perplexity3.8560597896575928

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.98s/it]
INFO:root:final mean train loss: 1711.1964609643394
INFO:root:final train perplexity: 3.855699300765991
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.17s/it]
INFO:root:eval mean loss: 1969.7571696829289
INFO:root:eval perplexity: 4.918600559234619
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.63s/it]
INFO:root:eval mean loss: 2420.479325098349
INFO:root:eval perplexity: 7.239371299743652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/25
 12%|â–ˆâ–Ž        | 25/200 [4:44:21<29:18:30, 602.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1696.5866241455078
INFO:root:current train perplexity3.763747453689575
INFO:root:current mean train loss 1687.1252077164188
INFO:root:current train perplexity3.7393276691436768
INFO:root:current mean train loss 1695.3652294703893
INFO:root:current train perplexity3.7642860412597656
INFO:root:current mean train loss 1695.3081062222705
INFO:root:current train perplexity3.781949758529663
INFO:root:current mean train loss 1700.4541156696823
INFO:root:current train perplexity3.7895331382751465
INFO:root:current mean train loss 1699.5302175274333
INFO:root:current train perplexity3.7886433601379395
INFO:root:current mean train loss 1696.6169353387295
INFO:root:current train perplexity3.7872836589813232
INFO:root:current mean train loss 1695.1191353982324
INFO:root:current train perplexity3.7908222675323486
INFO:root:current mean train loss 1693.8921695449976
INFO:root:current train perplexity3.7924716472625732
INFO:root:current mean train loss 1693.0995212571445
INFO:root:current train perplexity3.792564630508423
INFO:root:current mean train loss 1694.2445553541183
INFO:root:current train perplexity3.795405864715576
INFO:root:current mean train loss 1694.2318575713134
INFO:root:current train perplexity3.7953951358795166
INFO:root:current mean train loss 1695.1355304593353
INFO:root:current train perplexity3.7982170581817627
INFO:root:current mean train loss 1695.706228757553
INFO:root:current train perplexity3.800225257873535
INFO:root:current mean train loss 1696.5800992987129
INFO:root:current train perplexity3.8044803142547607
INFO:root:current mean train loss 1696.8019196918317
INFO:root:current train perplexity3.805532455444336
INFO:root:current mean train loss 1696.331380759554
INFO:root:current train perplexity3.806682825088501
INFO:root:current mean train loss 1696.3665023060244
INFO:root:current train perplexity3.8082268238067627
INFO:root:current mean train loss 1696.342972738701
INFO:root:current train perplexity3.809727430343628
INFO:root:current mean train loss 1696.9440029723232
INFO:root:current train perplexity3.810094118118286

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.46s/it]
INFO:root:final mean train loss: 1696.702091280523
INFO:root:final train perplexity: 3.811875343322754
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.96s/it]
INFO:root:eval mean loss: 1969.7204641788564
INFO:root:eval perplexity: 4.918453216552734
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.74s/it]
INFO:root:eval mean loss: 2422.725186395307
INFO:root:eval perplexity: 7.252682685852051
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/26
 13%|â–ˆâ–Ž        | 26/200 [4:54:35<29:18:01, 606.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1702.4461223323171
INFO:root:current train perplexity3.754019021987915
INFO:root:current mean train loss 1682.8855283480164
INFO:root:current train perplexity3.736804962158203
INFO:root:current mean train loss 1681.5582042393348
INFO:root:current train perplexity3.7360167503356934
INFO:root:current mean train loss 1684.808884069717
INFO:root:current train perplexity3.7457311153411865
INFO:root:current mean train loss 1686.2944839719742
INFO:root:current train perplexity3.7484617233276367
INFO:root:current mean train loss 1682.657525081952
INFO:root:current train perplexity3.7499608993530273
INFO:root:current mean train loss 1682.6912009585851
INFO:root:current train perplexity3.7521305084228516
INFO:root:current mean train loss 1680.9614890403593
INFO:root:current train perplexity3.755037546157837
INFO:root:current mean train loss 1680.8307585835314
INFO:root:current train perplexity3.7554430961608887
INFO:root:current mean train loss 1678.5626615064177
INFO:root:current train perplexity3.756031036376953
INFO:root:current mean train loss 1680.2754815034748
INFO:root:current train perplexity3.7579164505004883
INFO:root:current mean train loss 1681.2140708876534
INFO:root:current train perplexity3.7584657669067383
INFO:root:current mean train loss 1680.4147162302957
INFO:root:current train perplexity3.759734869003296
INFO:root:current mean train loss 1680.8342208691624
INFO:root:current train perplexity3.7610814571380615
INFO:root:current mean train loss 1681.330777339684
INFO:root:current train perplexity3.7617835998535156
INFO:root:current mean train loss 1682.2784594140371
INFO:root:current train perplexity3.7656612396240234
INFO:root:current mean train loss 1681.8635033718483
INFO:root:current train perplexity3.7652368545532227
INFO:root:current mean train loss 1682.7621371966543
INFO:root:current train perplexity3.767791748046875
INFO:root:current mean train loss 1682.909682489879
INFO:root:current train perplexity3.769606828689575
INFO:root:current mean train loss 1682.8159190378872
INFO:root:current train perplexity3.7685635089874268

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.74s/it]
INFO:root:final mean train loss: 1682.5108700443025
INFO:root:final train perplexity: 3.7694504261016846
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.79s/it]
INFO:root:eval mean loss: 1973.6644191877217
INFO:root:eval perplexity: 4.934167385101318
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.10s/it]
INFO:root:eval mean loss: 2427.0780146172706
INFO:root:eval perplexity: 7.27854585647583
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/27
 14%|â–ˆâ–Ž        | 27/200 [5:04:33<29:00:39, 603.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1651.6068915005387
INFO:root:current train perplexity3.701108455657959
INFO:root:current mean train loss 1655.5503818173952
INFO:root:current train perplexity3.68937349319458
INFO:root:current mean train loss 1656.7672956599745
INFO:root:current train perplexity3.696380138397217
INFO:root:current mean train loss 1657.0751898568435
INFO:root:current train perplexity3.69921612739563
INFO:root:current mean train loss 1659.781014121776
INFO:root:current train perplexity3.705502986907959
INFO:root:current mean train loss 1659.1659744973679
INFO:root:current train perplexity3.7086193561553955
INFO:root:current mean train loss 1661.1535501683013
INFO:root:current train perplexity3.7088465690612793
INFO:root:current mean train loss 1661.9134998170555
INFO:root:current train perplexity3.7109482288360596
INFO:root:current mean train loss 1661.3024895230096
INFO:root:current train perplexity3.7113759517669678
INFO:root:current mean train loss 1661.7449001877692
INFO:root:current train perplexity3.7084579467773438
INFO:root:current mean train loss 1661.8952070210967
INFO:root:current train perplexity3.7101054191589355
INFO:root:current mean train loss 1663.9325077669607
INFO:root:current train perplexity3.7160348892211914
INFO:root:current mean train loss 1664.719401203392
INFO:root:current train perplexity3.717040777206421
INFO:root:current mean train loss 1665.128849439663
INFO:root:current train perplexity3.7163774967193604
INFO:root:current mean train loss 1665.55896615655
INFO:root:current train perplexity3.7175745964050293
INFO:root:current mean train loss 1666.1958924515104
INFO:root:current train perplexity3.7187230587005615
INFO:root:current mean train loss 1666.5172978344815
INFO:root:current train perplexity3.7196266651153564
INFO:root:current mean train loss 1666.6505111676977
INFO:root:current train perplexity3.7203025817871094
INFO:root:current mean train loss 1667.975931584386
INFO:root:current train perplexity3.7239937782287598
INFO:root:current mean train loss 1668.3886489322651
INFO:root:current train perplexity3.726142168045044

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.62s/it]
INFO:root:final mean train loss: 1667.9603903996963
INFO:root:final train perplexity: 3.726442337036133
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 1976.6145755416112
INFO:root:eval perplexity: 4.945952892303467
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.88s/it]
INFO:root:eval mean loss: 2435.099428866772
INFO:root:eval perplexity: 7.326452732086182
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/28
 14%|â–ˆâ–        | 28/200 [5:13:57<28:16:55, 591.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1630.0951041666667
INFO:root:current train perplexity3.6368460655212402
INFO:root:current mean train loss 1626.1581410435267
INFO:root:current train perplexity3.6228764057159424
INFO:root:current mean train loss 1633.2789195667613
INFO:root:current train perplexity3.650209426879883
INFO:root:current mean train loss 1639.0360634765625
INFO:root:current train perplexity3.656998634338379
INFO:root:current mean train loss 1643.4880576685855
INFO:root:current train perplexity3.660334825515747
INFO:root:current mean train loss 1642.8338264266304
INFO:root:current train perplexity3.658108711242676
INFO:root:current mean train loss 1641.3610848885996
INFO:root:current train perplexity3.659569025039673
INFO:root:current mean train loss 1639.906160061744
INFO:root:current train perplexity3.6593549251556396
INFO:root:current mean train loss 1641.8544497767857
INFO:root:current train perplexity3.6641788482666016
INFO:root:current mean train loss 1645.196076973157
INFO:root:current train perplexity3.6674811840057373
INFO:root:current mean train loss 1646.4798664607558
INFO:root:current train perplexity3.6713764667510986
INFO:root:current mean train loss 1647.5070346783577
INFO:root:current train perplexity3.6715691089630127
INFO:root:current mean train loss 1648.4093030981924
INFO:root:current train perplexity3.6724393367767334
INFO:root:current mean train loss 1649.6286702769887
INFO:root:current train perplexity3.6727116107940674
INFO:root:current mean train loss 1651.6463392147775
INFO:root:current train perplexity3.6788742542266846
INFO:root:current mean train loss 1652.9437452721975
INFO:root:current train perplexity3.680960178375244
INFO:root:current mean train loss 1653.3317691814366
INFO:root:current train perplexity3.6805057525634766
INFO:root:current mean train loss 1654.460322403169
INFO:root:current train perplexity3.682833194732666
INFO:root:current mean train loss 1654.8541768229168
INFO:root:current train perplexity3.6856892108917236
INFO:root:current mean train loss 1654.6975430181963
INFO:root:current train perplexity3.6864266395568848

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.78s/it]
INFO:root:final mean train loss: 1654.219695660182
INFO:root:final train perplexity: 3.6862776279449463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 1982.4912884218472
INFO:root:eval perplexity: 4.969516754150391
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.77s/it]
INFO:root:eval mean loss: 2441.752664336076
INFO:root:eval perplexity: 7.366424083709717
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/29
 14%|â–ˆâ–        | 29/200 [5:23:17<27:39:29, 582.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1612.8396088973336
INFO:root:current train perplexity3.608558416366577
INFO:root:current mean train loss 1624.4547659556072
INFO:root:current train perplexity3.6093785762786865
INFO:root:current mean train loss 1632.740261130137
INFO:root:current train perplexity3.621325731277466
INFO:root:current mean train loss 1633.2299829599808
INFO:root:current train perplexity3.618368625640869
INFO:root:current mean train loss 1632.5004848077044
INFO:root:current train perplexity3.615468978881836
INFO:root:current mean train loss 1631.07506953059
INFO:root:current train perplexity3.615267276763916
INFO:root:current mean train loss 1632.7066853253139
INFO:root:current train perplexity3.617017984390259
INFO:root:current mean train loss 1634.7731614546342
INFO:root:current train perplexity3.6204051971435547
INFO:root:current mean train loss 1634.3893702211935
INFO:root:current train perplexity3.625048875808716
INFO:root:current mean train loss 1633.7208078445926
INFO:root:current train perplexity3.6265547275543213
INFO:root:current mean train loss 1633.6746916718535
INFO:root:current train perplexity3.625457286834717
INFO:root:current mean train loss 1633.4997434680095
INFO:root:current train perplexity3.6265547275543213
INFO:root:current mean train loss 1634.8481509560033
INFO:root:current train perplexity3.629058837890625
INFO:root:current mean train loss 1635.572991557505
INFO:root:current train perplexity3.6329357624053955
INFO:root:current mean train loss 1636.0235670156198
INFO:root:current train perplexity3.6341521739959717
INFO:root:current mean train loss 1636.3929671857825
INFO:root:current train perplexity3.6351940631866455
INFO:root:current mean train loss 1637.5909797542201
INFO:root:current train perplexity3.637831211090088
INFO:root:current mean train loss 1639.422378880637
INFO:root:current train perplexity3.642470359802246
INFO:root:current mean train loss 1640.358658256289
INFO:root:current train perplexity3.64582896232605

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.17s/it]
INFO:root:final mean train loss: 1640.681598580611
INFO:root:final train perplexity: 3.6471290588378906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.65s/it]
INFO:root:eval mean loss: 1985.1787758685173
INFO:root:eval perplexity: 4.980329513549805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.08s/it]
INFO:root:eval mean loss: 2442.416247212295
INFO:root:eval perplexity: 7.370423316955566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/30
 15%|â–ˆâ–Œ        | 30/200 [5:32:36<27:10:03, 575.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1559.1744384765625
INFO:root:current train perplexity3.5166592597961426
INFO:root:current mean train loss 1603.5462624086153
INFO:root:current train perplexity3.5620102882385254
INFO:root:current mean train loss 1610.5238165604442
INFO:root:current train perplexity3.5691349506378174
INFO:root:current mean train loss 1613.2527191260872
INFO:root:current train perplexity3.57049298286438
INFO:root:current mean train loss 1614.1083062132298
INFO:root:current train perplexity3.576716423034668
INFO:root:current mean train loss 1615.9199719981734
INFO:root:current train perplexity3.5792858600616455
INFO:root:current mean train loss 1616.570259783264
INFO:root:current train perplexity3.575897455215454
INFO:root:current mean train loss 1619.7046914255334
INFO:root:current train perplexity3.5852413177490234
INFO:root:current mean train loss 1620.1118497530226
INFO:root:current train perplexity3.5881197452545166
INFO:root:current mean train loss 1620.3862076393186
INFO:root:current train perplexity3.5913102626800537
INFO:root:current mean train loss 1621.282682057769
INFO:root:current train perplexity3.5926570892333984
INFO:root:current mean train loss 1621.7203929409236
INFO:root:current train perplexity3.5950114727020264
INFO:root:current mean train loss 1622.9913607740127
INFO:root:current train perplexity3.5977065563201904
INFO:root:current mean train loss 1623.334929603944
INFO:root:current train perplexity3.5983898639678955
INFO:root:current mean train loss 1625.457527848319
INFO:root:current train perplexity3.6015119552612305
INFO:root:current mean train loss 1625.348852846463
INFO:root:current train perplexity3.6020944118499756
INFO:root:current mean train loss 1626.6741581472868
INFO:root:current train perplexity3.604069471359253
INFO:root:current mean train loss 1626.6866697933046
INFO:root:current train perplexity3.6049132347106934
INFO:root:current mean train loss 1627.6192572969571
INFO:root:current train perplexity3.6061654090881348
INFO:root:current mean train loss 1627.7373869203036
INFO:root:current train perplexity3.608335256576538

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.61s/it]
INFO:root:final mean train loss: 1627.4995644127428
INFO:root:final train perplexity: 3.6094090938568115
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.36s/it]
INFO:root:eval mean loss: 1986.1262497056462
INFO:root:eval perplexity: 4.984147071838379
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.36s/it]
INFO:root:eval mean loss: 2447.4078581248614
INFO:root:eval perplexity: 7.400572299957275
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/31
 16%|â–ˆâ–Œ        | 31/200 [5:41:55<26:46:09, 570.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1594.6057269756611
INFO:root:current train perplexity3.4833626747131348
INFO:root:current mean train loss 1584.9991658528645
INFO:root:current train perplexity3.5167582035064697
INFO:root:current mean train loss 1596.4984255090224
INFO:root:current train perplexity3.525608777999878
INFO:root:current mean train loss 1603.6718004846866
INFO:root:current train perplexity3.539553642272949
INFO:root:current mean train loss 1601.503232570881
INFO:root:current train perplexity3.539262533187866
INFO:root:current mean train loss 1602.7709740468304
INFO:root:current train perplexity3.545305013656616
INFO:root:current mean train loss 1603.7475160836411
INFO:root:current train perplexity3.545462131500244
INFO:root:current mean train loss 1606.6574231192428
INFO:root:current train perplexity3.5533030033111572
INFO:root:current mean train loss 1606.7397137288608
INFO:root:current train perplexity3.5533623695373535
INFO:root:current mean train loss 1608.2494616251013
INFO:root:current train perplexity3.553192377090454
INFO:root:current mean train loss 1608.973978083501
INFO:root:current train perplexity3.5572779178619385
INFO:root:current mean train loss 1609.1920026165978
INFO:root:current train perplexity3.5591816902160645
INFO:root:current mean train loss 1610.65070267097
INFO:root:current train perplexity3.562070608139038
INFO:root:current mean train loss 1611.437571529889
INFO:root:current train perplexity3.563871383666992
INFO:root:current mean train loss 1611.130314338759
INFO:root:current train perplexity3.5638327598571777
INFO:root:current mean train loss 1612.239733454753
INFO:root:current train perplexity3.5650625228881836
INFO:root:current mean train loss 1613.112569473473
INFO:root:current train perplexity3.5658981800079346
INFO:root:current mean train loss 1614.0062333656215
INFO:root:current train perplexity3.567563533782959
INFO:root:current mean train loss 1614.2036393532226
INFO:root:current train perplexity3.56929874420166
INFO:root:current mean train loss 1613.8665144020151
INFO:root:current train perplexity3.5704379081726074

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.45s/it]
INFO:root:final mean train loss: 1614.0714519768126
INFO:root:final train perplexity: 3.5713863372802734
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 1991.8166789602726
INFO:root:eval perplexity: 5.007136821746826
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.21s/it]
INFO:root:eval mean loss: 2453.1750791292666
INFO:root:eval perplexity: 7.435560703277588
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/32
 16%|â–ˆâ–Œ        | 32/200 [5:51:12<26:26:16, 566.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.8271739871002
INFO:root:current train perplexity3.495048761367798
INFO:root:current mean train loss 1588.935913939576
INFO:root:current train perplexity3.5103228092193604
INFO:root:current mean train loss 1592.1878340607318
INFO:root:current train perplexity3.5153872966766357
INFO:root:current mean train loss 1589.0764491134066
INFO:root:current train perplexity3.5065653324127197
INFO:root:current mean train loss 1589.3878044317862
INFO:root:current train perplexity3.50882625579834
INFO:root:current mean train loss 1589.0049493554327
INFO:root:current train perplexity3.5114076137542725
INFO:root:current mean train loss 1589.5721175459273
INFO:root:current train perplexity3.5128324031829834
INFO:root:current mean train loss 1592.547668703472
INFO:root:current train perplexity3.5180413722991943
INFO:root:current mean train loss 1593.3428174581109
INFO:root:current train perplexity3.5186097621917725
INFO:root:current mean train loss 1593.1325371621901
INFO:root:current train perplexity3.517385482788086
INFO:root:current mean train loss 1594.6022618002082
INFO:root:current train perplexity3.519669771194458
INFO:root:current mean train loss 1595.0668781911295
INFO:root:current train perplexity3.5197315216064453
INFO:root:current mean train loss 1596.5757556771043
INFO:root:current train perplexity3.5233519077301025
INFO:root:current mean train loss 1596.9494253515043
INFO:root:current train perplexity3.525012969970703
INFO:root:current mean train loss 1598.4216410107524
INFO:root:current train perplexity3.5274548530578613
INFO:root:current mean train loss 1599.7256619644413
INFO:root:current train perplexity3.5295979976654053
INFO:root:current mean train loss 1600.1828811654796
INFO:root:current train perplexity3.5323293209075928
INFO:root:current mean train loss 1600.6934862076655
INFO:root:current train perplexity3.5332672595977783
INFO:root:current mean train loss 1601.3901104236256
INFO:root:current train perplexity3.534346580505371
INFO:root:current mean train loss 1602.0290556871823
INFO:root:current train perplexity3.5357699394226074

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.06s/it]
INFO:root:final mean train loss: 1601.8069213313163
INFO:root:final train perplexity: 3.53700852394104
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it]
INFO:root:eval mean loss: 1997.7371899760362
INFO:root:eval perplexity: 5.031169891357422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.28s/it]
INFO:root:eval mean loss: 2464.4619560512247
INFO:root:eval perplexity: 7.504515171051025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/33
 16%|â–ˆâ–‹        | 33/200 [6:00:30<26:09:24, 563.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1592.4920145670574
INFO:root:current train perplexity3.5004618167877197
INFO:root:current mean train loss 1579.3348556518554
INFO:root:current train perplexity3.492370367050171
INFO:root:current mean train loss 1575.3732661320612
INFO:root:current train perplexity3.475876808166504
INFO:root:current mean train loss 1572.1760898166233
INFO:root:current train perplexity3.470133066177368
INFO:root:current mean train loss 1575.6908158012059
INFO:root:current train perplexity3.470322608947754
INFO:root:current mean train loss 1576.6667033604213
INFO:root:current train perplexity3.4724557399749756
INFO:root:current mean train loss 1578.7064149798769
INFO:root:current train perplexity3.4746029376983643
INFO:root:current mean train loss 1580.9194103040193
INFO:root:current train perplexity3.477104663848877
INFO:root:current mean train loss 1581.6551032487737
INFO:root:current train perplexity3.4794013500213623
INFO:root:current mean train loss 1581.6769120534261
INFO:root:current train perplexity3.482099771499634
INFO:root:current mean train loss 1581.6963524008697
INFO:root:current train perplexity3.485257625579834
INFO:root:current mean train loss 1581.7871592554552
INFO:root:current train perplexity3.4873194694519043
INFO:root:current mean train loss 1582.3991899762834
INFO:root:current train perplexity3.4879746437072754
INFO:root:current mean train loss 1583.3275945326861
INFO:root:current train perplexity3.4896905422210693
INFO:root:current mean train loss 1584.0377937212381
INFO:root:current train perplexity3.4909608364105225
INFO:root:current mean train loss 1585.5081767546824
INFO:root:current train perplexity3.494758367538452
INFO:root:current mean train loss 1585.6552824089326
INFO:root:current train perplexity3.495229959487915
INFO:root:current mean train loss 1585.7516322742808
INFO:root:current train perplexity3.4960808753967285
INFO:root:current mean train loss 1586.8625391150033
INFO:root:current train perplexity3.4970734119415283
INFO:root:current mean train loss 1587.95759862783
INFO:root:current train perplexity3.497817039489746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.85s/it]
INFO:root:final mean train loss: 1587.9337708843036
INFO:root:final train perplexity: 3.4985201358795166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.75s/it]
INFO:root:eval mean loss: 2000.299127240553
INFO:root:eval perplexity: 5.041604518890381
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.11s/it]
INFO:root:eval mean loss: 2467.8852937306074
INFO:root:eval perplexity: 7.525553226470947
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/34
 17%|â–ˆâ–‹        | 34/200 [6:09:49<25:55:47, 562.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1554.8017800071023
INFO:root:current train perplexity3.425044536590576
INFO:root:current mean train loss 1562.7991977842514
INFO:root:current train perplexity3.4216341972351074
INFO:root:current mean train loss 1562.4351502566562
INFO:root:current train perplexity3.41310453414917
INFO:root:current mean train loss 1564.1434384454783
INFO:root:current train perplexity3.4146835803985596
INFO:root:current mean train loss 1565.9288770247806
INFO:root:current train perplexity3.4266793727874756
INFO:root:current mean train loss 1567.6617609351306
INFO:root:current train perplexity3.4316163063049316
INFO:root:current mean train loss 1568.9625098088995
INFO:root:current train perplexity3.4342081546783447
INFO:root:current mean train loss 1568.3561678656893
INFO:root:current train perplexity3.4351894855499268
INFO:root:current mean train loss 1569.0794613706616
INFO:root:current train perplexity3.4396822452545166
INFO:root:current mean train loss 1568.4803035739988
INFO:root:current train perplexity3.43981671333313
INFO:root:current mean train loss 1569.7811984289767
INFO:root:current train perplexity3.444154739379883
INFO:root:current mean train loss 1571.0913343145976
INFO:root:current train perplexity3.4494237899780273
INFO:root:current mean train loss 1571.9287579685054
INFO:root:current train perplexity3.4516677856445312
INFO:root:current mean train loss 1572.9662211853781
INFO:root:current train perplexity3.453850030899048
INFO:root:current mean train loss 1573.2566736178699
INFO:root:current train perplexity3.454954147338867
INFO:root:current mean train loss 1573.5028121067742
INFO:root:current train perplexity3.457782745361328
INFO:root:current mean train loss 1574.2702015812508
INFO:root:current train perplexity3.458407163619995
INFO:root:current mean train loss 1574.3855541016724
INFO:root:current train perplexity3.4602670669555664
INFO:root:current mean train loss 1574.8943730203407
INFO:root:current train perplexity3.461968183517456
INFO:root:current mean train loss 1575.7391062897145
INFO:root:current train perplexity3.4638328552246094

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.55s/it]
INFO:root:final mean train loss: 1575.236831087929
INFO:root:final train perplexity: 3.4636623859405518
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.81s/it]
INFO:root:eval mean loss: 2006.0958377486425
INFO:root:eval perplexity: 5.065295219421387
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 2476.413527901291
INFO:root:eval perplexity: 7.578228950500488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/35
 18%|â–ˆâ–Š        | 35/200 [6:21:41<27:49:38, 607.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1542.7743608190658
INFO:root:current train perplexity3.39365291595459
INFO:root:current mean train loss 1545.1603443892961
INFO:root:current train perplexity3.3948938846588135
INFO:root:current mean train loss 1545.7651379643655
INFO:root:current train perplexity3.3941850662231445
INFO:root:current mean train loss 1553.5047622913032
INFO:root:current train perplexity3.400347948074341
INFO:root:current mean train loss 1555.9609543032009
INFO:root:current train perplexity3.404266357421875
INFO:root:current mean train loss 1555.7128908305056
INFO:root:current train perplexity3.4029343128204346
INFO:root:current mean train loss 1556.390013065393
INFO:root:current train perplexity3.4048309326171875
INFO:root:current mean train loss 1557.8919523993427
INFO:root:current train perplexity3.407335042953491
INFO:root:current mean train loss 1558.8635193826901
INFO:root:current train perplexity3.4119174480438232
INFO:root:current mean train loss 1559.2496996136979
INFO:root:current train perplexity3.414433240890503
INFO:root:current mean train loss 1559.658642645074
INFO:root:current train perplexity3.415480136871338
INFO:root:current mean train loss 1560.662840876747
INFO:root:current train perplexity3.417639970779419
INFO:root:current mean train loss 1560.6202804824852
INFO:root:current train perplexity3.4200544357299805
INFO:root:current mean train loss 1560.2669677734375
INFO:root:current train perplexity3.4201180934906006
INFO:root:current mean train loss 1560.3908129261838
INFO:root:current train perplexity3.421074151992798
INFO:root:current mean train loss 1561.1324755430521
INFO:root:current train perplexity3.423063278198242
INFO:root:current mean train loss 1561.557206227056
INFO:root:current train perplexity3.4240200519561768
INFO:root:current mean train loss 1562.5076357906346
INFO:root:current train perplexity3.4268040657043457
INFO:root:current mean train loss 1562.7686143047329
INFO:root:current train perplexity3.4285190105438232

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.13s/it]
INFO:root:final mean train loss: 1562.8686055285848
INFO:root:final train perplexity: 3.4300408363342285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.63s/it]
INFO:root:eval mean loss: 2014.2710034525987
INFO:root:eval perplexity: 5.098896026611328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.76s/it]
INFO:root:eval mean loss: 2487.005390573055
INFO:root:eval perplexity: 7.644153118133545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/36
 18%|â–ˆâ–Š        | 36/200 [6:32:47<28:28:02, 624.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1489.1890314275568
INFO:root:current train perplexity3.31368350982666
INFO:root:current mean train loss 1537.6620752832912
INFO:root:current train perplexity3.384739398956299
INFO:root:current mean train loss 1541.5667487411138
INFO:root:current train perplexity3.3828277587890625
INFO:root:current mean train loss 1539.7016040274568
INFO:root:current train perplexity3.3819940090179443
INFO:root:current mean train loss 1540.5542630754828
INFO:root:current train perplexity3.379368782043457
INFO:root:current mean train loss 1541.6256968279874
INFO:root:current train perplexity3.3749125003814697
INFO:root:current mean train loss 1542.3573851140548
INFO:root:current train perplexity3.3760387897491455
INFO:root:current mean train loss 1542.1534375755427
INFO:root:current train perplexity3.3774170875549316
INFO:root:current mean train loss 1543.4976431850146
INFO:root:current train perplexity3.378704071044922
INFO:root:current mean train loss 1543.7959135790427
INFO:root:current train perplexity3.3815765380859375
INFO:root:current mean train loss 1545.8685324467963
INFO:root:current train perplexity3.3847856521606445
INFO:root:current mean train loss 1548.021846740338
INFO:root:current train perplexity3.38789701461792
INFO:root:current mean train loss 1548.680306117265
INFO:root:current train perplexity3.3880767822265625
INFO:root:current mean train loss 1548.7430453442328
INFO:root:current train perplexity3.3886563777923584
INFO:root:current mean train loss 1548.9438390914302
INFO:root:current train perplexity3.3895645141601562
INFO:root:current mean train loss 1549.2115276546212
INFO:root:current train perplexity3.391403913497925
INFO:root:current mean train loss 1549.3070476775877
INFO:root:current train perplexity3.393167018890381
INFO:root:current mean train loss 1549.3282530632443
INFO:root:current train perplexity3.39347243309021
INFO:root:current mean train loss 1550.3812991921116
INFO:root:current train perplexity3.395641565322876
INFO:root:current mean train loss 1550.287526751987
INFO:root:current train perplexity3.3960371017456055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.52s/it]
INFO:root:final mean train loss: 1550.4863897449611
INFO:root:final train perplexity: 3.3967087268829346
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.53s/it]
INFO:root:eval mean loss: 2016.2016848300366
INFO:root:eval perplexity: 5.106864929199219
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.57s/it]
INFO:root:eval mean loss: 2491.3583196545324
INFO:root:eval perplexity: 7.671416759490967
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/37
 18%|â–ˆâ–Š        | 37/200 [6:42:09<27:26:11, 605.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1498.7490931919642
INFO:root:current train perplexity3.348090648651123
INFO:root:current mean train loss 1526.6604557037354
INFO:root:current train perplexity3.325137138366699
INFO:root:current mean train loss 1527.3478388200726
INFO:root:current train perplexity3.329267740249634
INFO:root:current mean train loss 1525.8761425483517
INFO:root:current train perplexity3.3352811336517334
INFO:root:current mean train loss 1523.393496219243
INFO:root:current train perplexity3.3313324451446533
INFO:root:current mean train loss 1527.1991701993074
INFO:root:current train perplexity3.341879367828369
INFO:root:current mean train loss 1530.8766588344695
INFO:root:current train perplexity3.346937894821167
INFO:root:current mean train loss 1531.0516169621394
INFO:root:current train perplexity3.347243547439575
INFO:root:current mean train loss 1530.119827196794
INFO:root:current train perplexity3.348917007446289
INFO:root:current mean train loss 1530.9425033043171
INFO:root:current train perplexity3.3499486446380615
INFO:root:current mean train loss 1532.0246251918927
INFO:root:current train perplexity3.3509180545806885
INFO:root:current mean train loss 1532.652502398119
INFO:root:current train perplexity3.3526270389556885
INFO:root:current mean train loss 1533.457463565789
INFO:root:current train perplexity3.353738307952881
INFO:root:current mean train loss 1533.9816345766367
INFO:root:current train perplexity3.3538401126861572
INFO:root:current mean train loss 1535.7198729101015
INFO:root:current train perplexity3.3556299209594727
INFO:root:current mean train loss 1535.9843400885297
INFO:root:current train perplexity3.356513500213623
INFO:root:current mean train loss 1536.2861303381019
INFO:root:current train perplexity3.359342575073242
INFO:root:current mean train loss 1537.1344425766556
INFO:root:current train perplexity3.3610944747924805
INFO:root:current mean train loss 1537.5595327164465
INFO:root:current train perplexity3.3621397018432617
INFO:root:current mean train loss 1538.9592209178877
INFO:root:current train perplexity3.3631250858306885

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.57s/it]
INFO:root:final mean train loss: 1538.254232170963
INFO:root:final train perplexity: 3.364098072052002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.08s/it]
INFO:root:eval mean loss: 2020.8416085750498
INFO:root:eval perplexity: 5.126063346862793
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it]
INFO:root:eval mean loss: 2495.2683490726117
INFO:root:eval perplexity: 7.695986747741699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/38
 19%|â–ˆâ–‰        | 38/200 [6:51:30<26:40:11, 592.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.004638671875
INFO:root:current train perplexity3.2982771396636963
INFO:root:current mean train loss 1514.1999377020475
INFO:root:current train perplexity3.3035011291503906
INFO:root:current mean train loss 1512.2565320073343
INFO:root:current train perplexity3.306147813796997
INFO:root:current mean train loss 1512.8996889860734
INFO:root:current train perplexity3.3058247566223145
INFO:root:current mean train loss 1513.740680137377
INFO:root:current train perplexity3.3069815635681152
INFO:root:current mean train loss 1517.4081769190798
INFO:root:current train perplexity3.3120622634887695
INFO:root:current mean train loss 1518.0849993565287
INFO:root:current train perplexity3.3158018589019775
INFO:root:current mean train loss 1519.5292347748007
INFO:root:current train perplexity3.3164920806884766
INFO:root:current mean train loss 1519.5781135875093
INFO:root:current train perplexity3.315932273864746
INFO:root:current mean train loss 1519.1839544064153
INFO:root:current train perplexity3.3154714107513428
INFO:root:current mean train loss 1521.1712785492673
INFO:root:current train perplexity3.3196561336517334
INFO:root:current mean train loss 1522.0704512017262
INFO:root:current train perplexity3.320256471633911
INFO:root:current mean train loss 1521.8746863430283
INFO:root:current train perplexity3.320218086242676
INFO:root:current mean train loss 1522.1966955702544
INFO:root:current train perplexity3.3216769695281982
INFO:root:current mean train loss 1522.4731442778168
INFO:root:current train perplexity3.3217854499816895
INFO:root:current mean train loss 1522.9096559592435
INFO:root:current train perplexity3.3221042156219482
INFO:root:current mean train loss 1523.2402882491926
INFO:root:current train perplexity3.3238613605499268
INFO:root:current mean train loss 1524.6433381088825
INFO:root:current train perplexity3.3265299797058105
INFO:root:current mean train loss 1525.4181183440253
INFO:root:current train perplexity3.3296844959259033
INFO:root:current mean train loss 1526.0982627731362
INFO:root:current train perplexity3.3305089473724365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.32s/it]
INFO:root:final mean train loss: 1525.3757310983694
INFO:root:final train perplexity: 3.3301026821136475
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 2029.7786873891844
INFO:root:eval perplexity: 5.163248538970947
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.48s/it]
INFO:root:eval mean loss: 2508.0799833257147
INFO:root:eval perplexity: 7.777045726776123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/39
 20%|â–ˆâ–‰        | 39/200 [7:00:46<26:00:37, 581.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1495.2165153257308
INFO:root:current train perplexity3.2465405464172363
INFO:root:current mean train loss 1502.594652717496
INFO:root:current train perplexity3.2561521530151367
INFO:root:current mean train loss 1504.0045892846492
INFO:root:current train perplexity3.2715492248535156
INFO:root:current mean train loss 1505.2152008562457
INFO:root:current train perplexity3.268144369125366
INFO:root:current mean train loss 1505.5065931179822
INFO:root:current train perplexity3.2715718746185303
INFO:root:current mean train loss 1504.540528429785
INFO:root:current train perplexity3.273829221725464
INFO:root:current mean train loss 1507.178493349934
INFO:root:current train perplexity3.278120279312134
INFO:root:current mean train loss 1506.5185063079273
INFO:root:current train perplexity3.277883529663086
INFO:root:current mean train loss 1507.3642178776645
INFO:root:current train perplexity3.28206205368042
INFO:root:current mean train loss 1508.2449734186184
INFO:root:current train perplexity3.282081365585327
INFO:root:current mean train loss 1509.5089236616864
INFO:root:current train perplexity3.284820079803467
INFO:root:current mean train loss 1510.4888409665448
INFO:root:current train perplexity3.288076162338257
INFO:root:current mean train loss 1512.2066889307955
INFO:root:current train perplexity3.2921104431152344
INFO:root:current mean train loss 1514.209655223964
INFO:root:current train perplexity3.2938737869262695
INFO:root:current mean train loss 1514.5657729371953
INFO:root:current train perplexity3.2952797412872314
INFO:root:current mean train loss 1514.7978533599503
INFO:root:current train perplexity3.2953720092773438
INFO:root:current mean train loss 1514.9304470241284
INFO:root:current train perplexity3.2963101863861084
INFO:root:current mean train loss 1514.4973006665234
INFO:root:current train perplexity3.297311305999756
INFO:root:current mean train loss 1514.3816255596091
INFO:root:current train perplexity3.2984344959259033
INFO:root:current mean train loss 1514.0434672348847
INFO:root:current train perplexity3.299652099609375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.37s/it]
INFO:root:final mean train loss: 1513.8156797685108
INFO:root:final train perplexity: 3.299880266189575
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it]
INFO:root:eval mean loss: 2031.5480277419936
INFO:root:eval perplexity: 5.1706414222717285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.05s/it]
INFO:root:eval mean loss: 2511.394897893811
INFO:root:eval perplexity: 7.798160076141357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/40
 20%|â–ˆâ–ˆ        | 40/200 [7:09:57<25:26:03, 572.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1487.4696709355221
INFO:root:current train perplexity3.2208290100097656
INFO:root:current mean train loss 1493.4046767250786
INFO:root:current train perplexity3.228912115097046
INFO:root:current mean train loss 1494.5216410100247
INFO:root:current train perplexity3.2385010719299316
INFO:root:current mean train loss 1494.8693303332166
INFO:root:current train perplexity3.2469730377197266
INFO:root:current mean train loss 1493.473973793874
INFO:root:current train perplexity3.251591205596924
INFO:root:current mean train loss 1496.889976909947
INFO:root:current train perplexity3.25209641456604
INFO:root:current mean train loss 1496.1796763536681
INFO:root:current train perplexity3.2528510093688965
INFO:root:current mean train loss 1497.4095788057105
INFO:root:current train perplexity3.2538774013519287
INFO:root:current mean train loss 1497.984506930372
INFO:root:current train perplexity3.2551193237304688
INFO:root:current mean train loss 1497.5172934605225
INFO:root:current train perplexity3.2540152072906494
INFO:root:current mean train loss 1497.3229469108405
INFO:root:current train perplexity3.2566065788269043
INFO:root:current mean train loss 1497.944529137842
INFO:root:current train perplexity3.2590479850769043
INFO:root:current mean train loss 1498.709181462721
INFO:root:current train perplexity3.261401891708374
INFO:root:current mean train loss 1499.6995789415166
INFO:root:current train perplexity3.2617876529693604
INFO:root:current mean train loss 1500.035766766634
INFO:root:current train perplexity3.2632687091827393
INFO:root:current mean train loss 1500.3962458005958
INFO:root:current train perplexity3.2638821601867676
INFO:root:current mean train loss 1501.1115650679114
INFO:root:current train perplexity3.2659804821014404
INFO:root:current mean train loss 1502.2328559485227
INFO:root:current train perplexity3.2676639556884766
INFO:root:current mean train loss 1502.5800396653806
INFO:root:current train perplexity3.267939329147339
INFO:root:current mean train loss 1502.3431854170944
INFO:root:current train perplexity3.2693397998809814

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.63s/it]
INFO:root:final mean train loss: 1501.9261188670594
INFO:root:final train perplexity: 3.2690818309783936
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 2041.63492881829
INFO:root:eval perplexity: 5.212993621826172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it]
INFO:root:eval mean loss: 2524.8467147502492
INFO:root:eval perplexity: 7.884422302246094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/41
 20%|â–ˆâ–ˆ        | 41/200 [7:19:08<25:00:12, 566.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1478.9999961853027
INFO:root:current train perplexity3.229654312133789
INFO:root:current mean train loss 1477.1310729980469
INFO:root:current train perplexity3.2187371253967285
INFO:root:current mean train loss 1477.9601885821369
INFO:root:current train perplexity3.2111363410949707
INFO:root:current mean train loss 1479.031571821733
INFO:root:current train perplexity3.2111778259277344
INFO:root:current mean train loss 1480.667837081417
INFO:root:current train perplexity3.213690996170044
INFO:root:current mean train loss 1482.3914598298552
INFO:root:current train perplexity3.2141692638397217
INFO:root:current mean train loss 1484.541430944684
INFO:root:current train perplexity3.216510772705078
INFO:root:current mean train loss 1485.050428534273
INFO:root:current train perplexity3.2197113037109375
INFO:root:current mean train loss 1485.4295748301915
INFO:root:current train perplexity3.2237794399261475
INFO:root:current mean train loss 1485.4654880508363
INFO:root:current train perplexity3.2265145778656006
INFO:root:current mean train loss 1485.1317129761633
INFO:root:current train perplexity3.2277791500091553
INFO:root:current mean train loss 1486.454341212244
INFO:root:current train perplexity3.230412483215332
INFO:root:current mean train loss 1486.7910564092942
INFO:root:current train perplexity3.2310292720794678
INFO:root:current mean train loss 1486.9972831485607
INFO:root:current train perplexity3.2329604625701904
INFO:root:current mean train loss 1488.1858114150757
INFO:root:current train perplexity3.2330522537231445
INFO:root:current mean train loss 1488.840330042636
INFO:root:current train perplexity3.2344939708709717
INFO:root:current mean train loss 1488.9028624048774
INFO:root:current train perplexity3.2365481853485107
INFO:root:current mean train loss 1489.7068208214434
INFO:root:current train perplexity3.238663911819458
INFO:root:current mean train loss 1490.4359286022589
INFO:root:current train perplexity3.2386362552642822

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.80s/it]
INFO:root:final mean train loss: 1490.5534546083113
INFO:root:final train perplexity: 3.2398922443389893
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.88s/it]
INFO:root:eval mean loss: 2049.5536918910684
INFO:root:eval perplexity: 5.246486663818359
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.43s/it]
INFO:root:eval mean loss: 2535.6881679237313
INFO:root:eval perplexity: 7.954638957977295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/42
 21%|â–ˆâ–ˆ        | 42/200 [7:28:25<24:42:56, 563.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1495.6339674729568
INFO:root:current train perplexity3.1629645824432373
INFO:root:current mean train loss 1464.697059293764
INFO:root:current train perplexity3.1530165672302246
INFO:root:current mean train loss 1469.3719665813894
INFO:root:current train perplexity3.1728615760803223
INFO:root:current mean train loss 1468.4935965736072
INFO:root:current train perplexity3.1683664321899414
INFO:root:current mean train loss 1470.8432735415406
INFO:root:current train perplexity3.1720030307769775
INFO:root:current mean train loss 1471.910974573206
INFO:root:current train perplexity3.1790578365325928
INFO:root:current mean train loss 1470.3675220483278
INFO:root:current train perplexity3.1805520057678223
INFO:root:current mean train loss 1471.0541559034777
INFO:root:current train perplexity3.1836068630218506
INFO:root:current mean train loss 1473.6570309196745
INFO:root:current train perplexity3.19084095954895
INFO:root:current mean train loss 1473.2872989650364
INFO:root:current train perplexity3.194629430770874
INFO:root:current mean train loss 1473.2349507669824
INFO:root:current train perplexity3.1941211223602295
INFO:root:current mean train loss 1474.3292079490257
INFO:root:current train perplexity3.196721315383911
INFO:root:current mean train loss 1473.7988957517518
INFO:root:current train perplexity3.197903633117676
INFO:root:current mean train loss 1474.7605815344155
INFO:root:current train perplexity3.1984641551971436
INFO:root:current mean train loss 1475.7574686643002
INFO:root:current train perplexity3.1994285583496094
INFO:root:current mean train loss 1476.457582865814
INFO:root:current train perplexity3.20125150680542
INFO:root:current mean train loss 1476.1037533329054
INFO:root:current train perplexity3.2026641368865967
INFO:root:current mean train loss 1476.9151199438804
INFO:root:current train perplexity3.204430341720581
INFO:root:current mean train loss 1477.9221479581063
INFO:root:current train perplexity3.2056870460510254
INFO:root:current mean train loss 1478.393371294882
INFO:root:current train perplexity3.2078192234039307

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.58s/it]
INFO:root:final mean train loss: 1478.4186758713715
INFO:root:final train perplexity: 3.209033727645874
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it]
INFO:root:eval mean loss: 2052.0588162469526
INFO:root:eval perplexity: 5.257126808166504
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.75s/it]
INFO:root:eval mean loss: 2538.5849717593364
INFO:root:eval perplexity: 7.973506927490234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/43
 22%|â–ˆâ–ˆâ–       | 43/200 [7:37:42<24:29:01, 561.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.555489095052
INFO:root:current train perplexity3.0742502212524414
INFO:root:current mean train loss 1463.6021719125602
INFO:root:current train perplexity3.142423391342163
INFO:root:current mean train loss 1459.4887631623642
INFO:root:current train perplexity3.1325480937957764
INFO:root:current mean train loss 1461.5662053888495
INFO:root:current train perplexity3.145112991333008
INFO:root:current mean train loss 1459.8221989121548
INFO:root:current train perplexity3.1480813026428223
INFO:root:current mean train loss 1462.1287180774616
INFO:root:current train perplexity3.1511142253875732
INFO:root:current mean train loss 1463.863497295077
INFO:root:current train perplexity3.1556622982025146
INFO:root:current mean train loss 1465.3164177881529
INFO:root:current train perplexity3.159234046936035
INFO:root:current mean train loss 1462.5643979727504
INFO:root:current train perplexity3.157745599746704
INFO:root:current mean train loss 1463.4380678238408
INFO:root:current train perplexity3.1629257202148438
INFO:root:current mean train loss 1463.2774447246663
INFO:root:current train perplexity3.1665689945220947
INFO:root:current mean train loss 1463.6115703211422
INFO:root:current train perplexity3.168006181716919
INFO:root:current mean train loss 1463.2793220830158
INFO:root:current train perplexity3.1690194606781006
INFO:root:current mean train loss 1463.2841263620478
INFO:root:current train perplexity3.170076608657837
INFO:root:current mean train loss 1464.5296554031906
INFO:root:current train perplexity3.171386480331421
INFO:root:current mean train loss 1465.088149844899
INFO:root:current train perplexity3.173649549484253
INFO:root:current mean train loss 1465.3765424295436
INFO:root:current train perplexity3.1748008728027344
INFO:root:current mean train loss 1466.3032051571531
INFO:root:current train perplexity3.177589178085327
INFO:root:current mean train loss 1467.3519187051743
INFO:root:current train perplexity3.179605007171631
INFO:root:current mean train loss 1467.6178678048088
INFO:root:current train perplexity3.180975914001465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.47s/it]
INFO:root:final mean train loss: 1467.8195661967052
INFO:root:final train perplexity: 3.1823208332061768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.42s/it]
INFO:root:eval mean loss: 2059.6464956297095
INFO:root:eval perplexity: 5.289487361907959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.08s/it]
INFO:root:eval mean loss: 2547.9439956989695
INFO:root:eval perplexity: 8.034771919250488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/44
 22%|â–ˆâ–ˆâ–       | 44/200 [7:46:54<24:12:23, 558.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1456.514960106383
INFO:root:current train perplexity3.1032297611236572
INFO:root:current mean train loss 1451.3290874455251
INFO:root:current train perplexity3.122378349304199
INFO:root:current mean train loss 1444.838436234818
INFO:root:current train perplexity3.1293368339538574
INFO:root:current mean train loss 1444.300223666584
INFO:root:current train perplexity3.126296043395996
INFO:root:current mean train loss 1446.042501496522
INFO:root:current train perplexity3.1288816928863525
INFO:root:current mean train loss 1446.5236390164391
INFO:root:current train perplexity3.126668691635132
INFO:root:current mean train loss 1447.1801946483772
INFO:root:current train perplexity3.130512237548828
INFO:root:current mean train loss 1447.7617408109
INFO:root:current train perplexity3.1323843002319336
INFO:root:current mean train loss 1448.2664074317904
INFO:root:current train perplexity3.1355960369110107
INFO:root:current mean train loss 1448.4755827149468
INFO:root:current train perplexity3.1352548599243164
INFO:root:current mean train loss 1449.99707660839
INFO:root:current train perplexity3.1379852294921875
INFO:root:current mean train loss 1450.335213805035
INFO:root:current train perplexity3.139240026473999
INFO:root:current mean train loss 1450.8363428674131
INFO:root:current train perplexity3.141291856765747
INFO:root:current mean train loss 1451.795021651848
INFO:root:current train perplexity3.1446950435638428
INFO:root:current mean train loss 1453.147832716261
INFO:root:current train perplexity3.147266149520874
INFO:root:current mean train loss 1454.1679768774998
INFO:root:current train perplexity3.1482748985290527
INFO:root:current mean train loss 1454.864451776075
INFO:root:current train perplexity3.147418975830078
INFO:root:current mean train loss 1455.5922688056758
INFO:root:current train perplexity3.1481587886810303
INFO:root:current mean train loss 1456.0925735779304
INFO:root:current train perplexity3.1490087509155273
INFO:root:current mean train loss 1456.3606049822242
INFO:root:current train perplexity3.1525444984436035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.56s/it]
INFO:root:final mean train loss: 1456.054570231243
INFO:root:final train perplexity: 3.15293025970459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it]
INFO:root:eval mean loss: 2065.2822650882367
INFO:root:eval perplexity: 5.313651084899902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.72s/it]
INFO:root:eval mean loss: 2557.65360774047
INFO:root:eval perplexity: 8.09882640838623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [7:56:03<23:55:53, 555.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1418.0269622802734
INFO:root:current train perplexity3.0787994861602783
INFO:root:current mean train loss 1426.4127703410823
INFO:root:current train perplexity3.0826573371887207
INFO:root:current mean train loss 1430.9102556633227
INFO:root:current train perplexity3.0896785259246826
INFO:root:current mean train loss 1434.8249837016012
INFO:root:current train perplexity3.10050630569458
INFO:root:current mean train loss 1435.1847449993265
INFO:root:current train perplexity3.0981991291046143
INFO:root:current mean train loss 1437.1892425320673
INFO:root:current train perplexity3.101900100708008
INFO:root:current mean train loss 1438.8800640795605
INFO:root:current train perplexity3.1045002937316895
INFO:root:current mean train loss 1439.6480033834566
INFO:root:current train perplexity3.1069071292877197
INFO:root:current mean train loss 1439.9373776471173
INFO:root:current train perplexity3.110605478286743
INFO:root:current mean train loss 1440.6936472026143
INFO:root:current train perplexity3.1134026050567627
INFO:root:current mean train loss 1441.6876988231688
INFO:root:current train perplexity3.1161301136016846
INFO:root:current mean train loss 1442.0703586434172
INFO:root:current train perplexity3.1167078018188477
INFO:root:current mean train loss 1443.0751660503918
INFO:root:current train perplexity3.117366313934326
INFO:root:current mean train loss 1444.3173378863294
INFO:root:current train perplexity3.1192150115966797
INFO:root:current mean train loss 1443.9458635674148
INFO:root:current train perplexity3.118830919265747
INFO:root:current mean train loss 1443.9636939943905
INFO:root:current train perplexity3.120537757873535
INFO:root:current mean train loss 1444.319202569815
INFO:root:current train perplexity3.1238291263580322
INFO:root:current mean train loss 1444.4049732441804
INFO:root:current train perplexity3.124873161315918
INFO:root:current mean train loss 1445.0377647825576
INFO:root:current train perplexity3.124940872192383
INFO:root:current mean train loss 1445.4670304494575
INFO:root:current train perplexity3.125600814819336

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.29s/it]
INFO:root:final mean train loss: 1444.989065965738
INFO:root:final train perplexity: 3.125534772872925
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.24s/it]
INFO:root:eval mean loss: 2073.0633402073636
INFO:root:eval perplexity: 5.347192764282227
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it]
INFO:root:eval mean loss: 2564.7526080625275
INFO:root:eval perplexity: 8.145986557006836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [8:05:11<23:40:18, 553.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1425.6517107928241
INFO:root:current train perplexity3.0792016983032227
INFO:root:current mean train loss 1423.5527262819405
INFO:root:current train perplexity3.0725996494293213
INFO:root:current mean train loss 1421.493759643989
INFO:root:current train perplexity3.0755863189697266
INFO:root:current mean train loss 1422.9556189509515
INFO:root:current train perplexity3.076796531677246
INFO:root:current mean train loss 1424.6917569800871
INFO:root:current train perplexity3.0736196041107178
INFO:root:current mean train loss 1423.6899023269416
INFO:root:current train perplexity3.072817325592041
INFO:root:current mean train loss 1424.767341692192
INFO:root:current train perplexity3.074230909347534
INFO:root:current mean train loss 1425.762391934169
INFO:root:current train perplexity3.0764386653900146
INFO:root:current mean train loss 1426.5306828787866
INFO:root:current train perplexity3.079847812652588
INFO:root:current mean train loss 1427.1196654900134
INFO:root:current train perplexity3.0836281776428223
INFO:root:current mean train loss 1427.092266975565
INFO:root:current train perplexity3.084282875061035
INFO:root:current mean train loss 1428.8409727711883
INFO:root:current train perplexity3.084660530090332
INFO:root:current mean train loss 1430.2418085198026
INFO:root:current train perplexity3.086660861968994
INFO:root:current mean train loss 1430.5022109664928
INFO:root:current train perplexity3.087250232696533
INFO:root:current mean train loss 1431.9478327038319
INFO:root:current train perplexity3.0906550884246826
INFO:root:current mean train loss 1432.7283767999388
INFO:root:current train perplexity3.0923993587493896
INFO:root:current mean train loss 1433.1894836970414
INFO:root:current train perplexity3.0944576263427734
INFO:root:current mean train loss 1433.7613104553052
INFO:root:current train perplexity3.0959599018096924
INFO:root:current mean train loss 1434.6409049634917
INFO:root:current train perplexity3.0970983505249023
INFO:root:current mean train loss 1434.3640434346014
INFO:root:current train perplexity3.0982885360717773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.53s/it]
INFO:root:final mean train loss: 1433.899146837474
INFO:root:final train perplexity: 3.0983171463012695
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 2080.6925377811945
INFO:root:eval perplexity: 5.380288600921631
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.59s/it]
INFO:root:eval mean loss: 2575.989794575576
INFO:root:eval perplexity: 8.221192359924316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [8:14:13<23:22:36, 550.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.7215426698024
INFO:root:current train perplexity3.0304925441741943
INFO:root:current mean train loss 1408.873291015625
INFO:root:current train perplexity3.0322365760803223
INFO:root:current mean train loss 1410.0736624698511
INFO:root:current train perplexity3.0476815700531006
INFO:root:current mean train loss 1409.70639697511
INFO:root:current train perplexity3.0471699237823486
INFO:root:current mean train loss 1412.0677583380398
INFO:root:current train perplexity3.054774522781372
INFO:root:current mean train loss 1412.3634078111936
INFO:root:current train perplexity3.0532140731811523
INFO:root:current mean train loss 1415.0942861999686
INFO:root:current train perplexity3.0544912815093994
INFO:root:current mean train loss 1415.6185314972001
INFO:root:current train perplexity3.056859016418457
INFO:root:current mean train loss 1415.0725693054878
INFO:root:current train perplexity3.057704210281372
INFO:root:current mean train loss 1416.9485931090696
INFO:root:current train perplexity3.0601918697357178
INFO:root:current mean train loss 1417.4242406959743
INFO:root:current train perplexity3.061248540878296
INFO:root:current mean train loss 1418.74867821695
INFO:root:current train perplexity3.0623042583465576
INFO:root:current mean train loss 1419.6203176912798
INFO:root:current train perplexity3.0630271434783936
INFO:root:current mean train loss 1420.0805735663112
INFO:root:current train perplexity3.064610242843628
INFO:root:current mean train loss 1420.9228058472495
INFO:root:current train perplexity3.06542706489563
INFO:root:current mean train loss 1421.4927273441167
INFO:root:current train perplexity3.0671613216400146
INFO:root:current mean train loss 1422.5508936869662
INFO:root:current train perplexity3.069028615951538
INFO:root:current mean train loss 1422.8236873571545
INFO:root:current train perplexity3.070138931274414
INFO:root:current mean train loss 1423.4256047690003
INFO:root:current train perplexity3.070650339126587

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.59s/it]
INFO:root:final mean train loss: 1423.3557105514058
INFO:root:final train perplexity: 3.0726609230041504
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.85s/it]
INFO:root:eval mean loss: 2081.2086952155364
INFO:root:eval perplexity: 5.382534027099609
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.34s/it]
INFO:root:eval mean loss: 2576.1990243898217
INFO:root:eval perplexity: 8.222599029541016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/48
 24%|â–ˆâ–ˆâ–       | 48/200 [8:23:27<23:16:03, 551.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1378.6914143880208
INFO:root:current train perplexity3.0178470611572266
INFO:root:current mean train loss 1395.9102730129075
INFO:root:current train perplexity3.0217933654785156
INFO:root:current mean train loss 1398.540132176599
INFO:root:current train perplexity3.0325746536254883
INFO:root:current mean train loss 1398.911214580233
INFO:root:current train perplexity3.0237066745758057
INFO:root:current mean train loss 1399.6004929875753
INFO:root:current train perplexity3.024376392364502
INFO:root:current mean train loss 1402.3042783866808
INFO:root:current train perplexity3.0234375
INFO:root:current mean train loss 1404.1451513274899
INFO:root:current train perplexity3.025876998901367
INFO:root:current mean train loss 1403.9935089324738
INFO:root:current train perplexity3.0265960693359375
INFO:root:current mean train loss 1406.1804092875288
INFO:root:current train perplexity3.0285792350769043
INFO:root:current mean train loss 1406.518762006916
INFO:root:current train perplexity3.027653455734253
INFO:root:current mean train loss 1406.326119679418
INFO:root:current train perplexity3.0283849239349365
INFO:root:current mean train loss 1406.8398320356291
INFO:root:current train perplexity3.030259370803833
INFO:root:current mean train loss 1407.5579697346
INFO:root:current train perplexity3.0331552028656006
INFO:root:current mean train loss 1408.4040904230037
INFO:root:current train perplexity3.0343377590179443
INFO:root:current mean train loss 1409.2153141736142
INFO:root:current train perplexity3.036750078201294
INFO:root:current mean train loss 1409.6877386615615
INFO:root:current train perplexity3.037705421447754
INFO:root:current mean train loss 1410.8663700446257
INFO:root:current train perplexity3.0412487983703613
INFO:root:current mean train loss 1411.7093363503325
INFO:root:current train perplexity3.0419728755950928
INFO:root:current mean train loss 1412.4336390135375
INFO:root:current train perplexity3.044250011444092
INFO:root:current mean train loss 1412.7162541561277
INFO:root:current train perplexity3.0452303886413574

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.38s/it]
INFO:root:final mean train loss: 1412.3209745298416
INFO:root:final train perplexity: 3.046036958694458
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.31s/it]
INFO:root:eval mean loss: 2093.041349803302
INFO:root:eval perplexity: 5.434289932250977
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.86s/it]
INFO:root:eval mean loss: 2592.7848623289283
INFO:root:eval perplexity: 8.334893226623535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/49
 24%|â–ˆâ–ˆâ–       | 49/200 [8:32:38<23:07:20, 551.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1389.2829551696777
INFO:root:current train perplexity2.971359968185425
INFO:root:current mean train loss 1389.1324842048414
INFO:root:current train perplexity2.9738736152648926
INFO:root:current mean train loss 1387.0633613323344
INFO:root:current train perplexity2.991056203842163
INFO:root:current mean train loss 1389.4098396760871
INFO:root:current train perplexity2.9928641319274902
INFO:root:current mean train loss 1388.8780865139431
INFO:root:current train perplexity2.9909579753875732
INFO:root:current mean train loss 1391.3273320018798
INFO:root:current train perplexity2.995769739151001
INFO:root:current mean train loss 1393.3668365478516
INFO:root:current train perplexity2.999382495880127
INFO:root:current mean train loss 1395.1091592090377
INFO:root:current train perplexity3.003563404083252
INFO:root:current mean train loss 1394.447813620934
INFO:root:current train perplexity3.00347900390625
INFO:root:current mean train loss 1395.055989277721
INFO:root:current train perplexity3.0076441764831543
INFO:root:current mean train loss 1396.6210833409036
INFO:root:current train perplexity3.0105066299438477
INFO:root:current mean train loss 1397.319740430205
INFO:root:current train perplexity3.010359048843384
INFO:root:current mean train loss 1398.3654669229086
INFO:root:current train perplexity3.0127270221710205
INFO:root:current mean train loss 1399.989218130484
INFO:root:current train perplexity3.014592409133911
INFO:root:current mean train loss 1400.0173366269585
INFO:root:current train perplexity3.0153608322143555
INFO:root:current mean train loss 1400.520403112506
INFO:root:current train perplexity3.0172007083892822
INFO:root:current mean train loss 1400.6111954333735
INFO:root:current train perplexity3.0181071758270264
INFO:root:current mean train loss 1401.1708512163052
INFO:root:current train perplexity3.019245147705078
INFO:root:current mean train loss 1402.1638984513595
INFO:root:current train perplexity3.0214216709136963
INFO:root:current mean train loss 1402.5537449301646
INFO:root:current train perplexity3.0223076343536377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.07s/it]
INFO:root:final mean train loss: 1402.4633745356034
INFO:root:final train perplexity: 3.0224478244781494
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.14s/it]
INFO:root:eval mean loss: 2097.960353553718
INFO:root:eval perplexity: 5.455952167510986
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.56s/it]
INFO:root:eval mean loss: 2597.702365307098
INFO:root:eval perplexity: 8.368480682373047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [8:41:56<23:03:03, 553.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.4717893016582
INFO:root:current train perplexity2.969621419906616
INFO:root:current mean train loss 1382.0577605586725
INFO:root:current train perplexity2.9761202335357666
INFO:root:current mean train loss 1383.3955852707707
INFO:root:current train perplexity2.979626178741455
INFO:root:current mean train loss 1386.091672006514
INFO:root:current train perplexity2.985065221786499
INFO:root:current mean train loss 1386.7600769178905
INFO:root:current train perplexity2.980640411376953
INFO:root:current mean train loss 1385.2102144168375
INFO:root:current train perplexity2.9795589447021484
INFO:root:current mean train loss 1385.7561227007898
INFO:root:current train perplexity2.982985734939575
INFO:root:current mean train loss 1385.3662047443465
INFO:root:current train perplexity2.9834089279174805
INFO:root:current mean train loss 1386.7095484289882
INFO:root:current train perplexity2.9863059520721436
INFO:root:current mean train loss 1387.9332873522294
INFO:root:current train perplexity2.986499309539795
INFO:root:current mean train loss 1388.6153295642428
INFO:root:current train perplexity2.9880733489990234
INFO:root:current mean train loss 1389.020846507153
INFO:root:current train perplexity2.9878485202789307
INFO:root:current mean train loss 1389.2678074099904
INFO:root:current train perplexity2.9883103370666504
INFO:root:current mean train loss 1389.875751153198
INFO:root:current train perplexity2.9897875785827637
INFO:root:current mean train loss 1390.2886566098925
INFO:root:current train perplexity2.9914517402648926
INFO:root:current mean train loss 1390.3814242555682
INFO:root:current train perplexity2.993468761444092
INFO:root:current mean train loss 1391.031433882751
INFO:root:current train perplexity2.9951183795928955
INFO:root:current mean train loss 1391.307536086469
INFO:root:current train perplexity2.995635986328125
INFO:root:current mean train loss 1392.1514402712403
INFO:root:current train perplexity2.997182846069336
INFO:root:current mean train loss 1392.4947747141964
INFO:root:current train perplexity2.997209072113037

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.15s/it]
INFO:root:final mean train loss: 1392.1703254211088
INFO:root:final train perplexity: 2.998011589050293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.04s/it]
INFO:root:eval mean loss: 2107.589879245623
INFO:root:eval perplexity: 5.498607158660889
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.49s/it]
INFO:root:eval mean loss: 2609.0750355821974
INFO:root:eval perplexity: 8.44667911529541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [8:51:11<22:54:55, 553.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.975079160748
INFO:root:current train perplexity2.9404938220977783
INFO:root:current mean train loss 1363.9771846173758
INFO:root:current train perplexity2.9438204765319824
INFO:root:current mean train loss 1365.0063444438733
INFO:root:current train perplexity2.9501330852508545
INFO:root:current mean train loss 1367.4150940941984
INFO:root:current train perplexity2.9567513465881348
INFO:root:current mean train loss 1369.6262751894446
INFO:root:current train perplexity2.955587863922119
INFO:root:current mean train loss 1372.2354330864896
INFO:root:current train perplexity2.9573962688446045
INFO:root:current mean train loss 1373.6971466705963
INFO:root:current train perplexity2.9577219486236572
INFO:root:current mean train loss 1373.83730513371
INFO:root:current train perplexity2.9567627906799316
INFO:root:current mean train loss 1374.8513685407045
INFO:root:current train perplexity2.9580442905426025
INFO:root:current mean train loss 1375.3997121617415
INFO:root:current train perplexity2.958466053009033
INFO:root:current mean train loss 1376.4614913969058
INFO:root:current train perplexity2.9608640670776367
INFO:root:current mean train loss 1377.0551060566975
INFO:root:current train perplexity2.9614667892456055
INFO:root:current mean train loss 1377.4060913857313
INFO:root:current train perplexity2.963881015777588
INFO:root:current mean train loss 1378.1443633362978
INFO:root:current train perplexity2.9645743370056152
INFO:root:current mean train loss 1379.1096415396114
INFO:root:current train perplexity2.965677261352539
INFO:root:current mean train loss 1380.3145363760177
INFO:root:current train perplexity2.967857599258423
INFO:root:current mean train loss 1380.650710381809
INFO:root:current train perplexity2.968160629272461
INFO:root:current mean train loss 1381.0665264540053
INFO:root:current train perplexity2.969634771347046
INFO:root:current mean train loss 1381.4262940630651
INFO:root:current train perplexity2.971496343612671
INFO:root:current mean train loss 1381.8125476235655
INFO:root:current train perplexity2.9727747440338135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.79s/it]
INFO:root:final mean train loss: 1381.6177515596437
INFO:root:final train perplexity: 2.9731645584106445
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.54s/it]
INFO:root:eval mean loss: 2113.3130670642176
INFO:root:eval perplexity: 5.524117946624756
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.46s/it]
INFO:root:eval mean loss: 2618.586484652039
INFO:root:eval perplexity: 8.512639045715332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [9:00:27<22:47:21, 554.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.5617911097515
INFO:root:current train perplexity2.9202091693878174
INFO:root:current mean train loss 1360.2763338349555
INFO:root:current train perplexity2.913709878921509
INFO:root:current mean train loss 1357.55415004555
INFO:root:current train perplexity2.9194886684417725
INFO:root:current mean train loss 1356.736387088467
INFO:root:current train perplexity2.9215104579925537
INFO:root:current mean train loss 1358.6137753441221
INFO:root:current train perplexity2.927762746810913
INFO:root:current mean train loss 1359.0233403462694
INFO:root:current train perplexity2.9298481941223145
INFO:root:current mean train loss 1360.8883701843888
INFO:root:current train perplexity2.9322457313537598
INFO:root:current mean train loss 1362.9384413289233
INFO:root:current train perplexity2.9309237003326416
INFO:root:current mean train loss 1364.118228761148
INFO:root:current train perplexity2.9336211681365967
INFO:root:current mean train loss 1365.4307973091381
INFO:root:current train perplexity2.9352517127990723
INFO:root:current mean train loss 1366.5625234447139
INFO:root:current train perplexity2.935347557067871
INFO:root:current mean train loss 1367.1129347477943
INFO:root:current train perplexity2.9363648891448975
INFO:root:current mean train loss 1367.9484017447205
INFO:root:current train perplexity2.938979148864746
INFO:root:current mean train loss 1368.530551648364
INFO:root:current train perplexity2.9397671222686768
INFO:root:current mean train loss 1369.2634392582077
INFO:root:current train perplexity2.9416401386260986
INFO:root:current mean train loss 1370.0208454144129
INFO:root:current train perplexity2.9433109760284424
INFO:root:current mean train loss 1370.6402625606943
INFO:root:current train perplexity2.9457077980041504
INFO:root:current mean train loss 1371.1214958357798
INFO:root:current train perplexity2.9468915462493896
INFO:root:current mean train loss 1371.406414402715
INFO:root:current train perplexity2.947323799133301
INFO:root:current mean train loss 1371.453589304252
INFO:root:current train perplexity2.9494264125823975

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.91s/it]
INFO:root:final mean train loss: 1371.453589304252
INFO:root:final train perplexity: 2.9494264125823975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.43s/it]
INFO:root:eval mean loss: 2117.9618404740136
INFO:root:eval perplexity: 5.544925689697266
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:46<00:00, 46.54s/it]
INFO:root:eval mean loss: 2622.1246809722684
INFO:root:eval perplexity: 8.537307739257812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [9:09:50<22:44:24, 556.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1351.4948693847657
INFO:root:current train perplexity2.89078426361084
INFO:root:current mean train loss 1347.831739501953
INFO:root:current train perplexity2.88899564743042
INFO:root:current mean train loss 1346.4448645019531
INFO:root:current train perplexity2.892421245574951
INFO:root:current mean train loss 1350.2063357543946
INFO:root:current train perplexity2.8970248699188232
INFO:root:current mean train loss 1349.9606166992187
INFO:root:current train perplexity2.901240348815918
INFO:root:current mean train loss 1350.4629189046225
INFO:root:current train perplexity2.9040114879608154
INFO:root:current mean train loss 1350.2039212472098
INFO:root:current train perplexity2.906116008758545
INFO:root:current mean train loss 1353.21804977417
INFO:root:current train perplexity2.9099578857421875
INFO:root:current mean train loss 1353.1430695258246
INFO:root:current train perplexity2.9123873710632324
INFO:root:current mean train loss 1353.8549943847656
INFO:root:current train perplexity2.91384220123291
INFO:root:current mean train loss 1354.9388323419744
INFO:root:current train perplexity2.9148147106170654
INFO:root:current mean train loss 1355.3104166666667
INFO:root:current train perplexity2.91587495803833
INFO:root:current mean train loss 1356.817239051232
INFO:root:current train perplexity2.9182777404785156
INFO:root:current mean train loss 1357.1438255092075
INFO:root:current train perplexity2.920335292816162
INFO:root:current mean train loss 1357.7489348144532
INFO:root:current train perplexity2.9216833114624023
INFO:root:current mean train loss 1358.807900390625
INFO:root:current train perplexity2.924055576324463
INFO:root:current mean train loss 1360.1765563964843
INFO:root:current train perplexity2.925057888031006
INFO:root:current mean train loss 1361.0327627224392
INFO:root:current train perplexity2.9262661933898926
INFO:root:current mean train loss 1361.3799671695108
INFO:root:current train perplexity2.9262115955352783

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.86s/it]
INFO:root:final mean train loss: 1361.8760995561886
INFO:root:final train perplexity: 2.927232503890991
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.98s/it]
INFO:root:eval mean loss: 2126.621576836769
INFO:root:eval perplexity: 5.583896160125732
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.03s/it]
INFO:root:eval mean loss: 2632.1169325375386
INFO:root:eval perplexity: 8.607359886169434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [9:18:58<22:28:33, 554.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1339.139590992647
INFO:root:current train perplexity2.8543777465820312
INFO:root:current mean train loss 1338.1462621444311
INFO:root:current train perplexity2.8599019050598145
INFO:root:current mean train loss 1339.4220685123848
INFO:root:current train perplexity2.867136240005493
INFO:root:current mean train loss 1338.0348712909108
INFO:root:current train perplexity2.868180990219116
INFO:root:current mean train loss 1338.7196882142723
INFO:root:current train perplexity2.8773105144500732
INFO:root:current mean train loss 1342.2467477824287
INFO:root:current train perplexity2.8840267658233643
INFO:root:current mean train loss 1342.0740531538063
INFO:root:current train perplexity2.8854522705078125
INFO:root:current mean train loss 1342.0575727518633
INFO:root:current train perplexity2.8843376636505127
INFO:root:current mean train loss 1343.7715976299532
INFO:root:current train perplexity2.889706611633301
INFO:root:current mean train loss 1345.2165886765608
INFO:root:current train perplexity2.891122817993164
INFO:root:current mean train loss 1346.5929394387215
INFO:root:current train perplexity2.8911185264587402
INFO:root:current mean train loss 1347.1265640736906
INFO:root:current train perplexity2.8927974700927734
INFO:root:current mean train loss 1347.0340674470071
INFO:root:current train perplexity2.8939480781555176
INFO:root:current mean train loss 1347.5077765369922
INFO:root:current train perplexity2.8947904109954834
INFO:root:current mean train loss 1348.455431155445
INFO:root:current train perplexity2.896538496017456
INFO:root:current mean train loss 1349.5047243705453
INFO:root:current train perplexity2.8980774879455566
INFO:root:current mean train loss 1349.6877355345548
INFO:root:current train perplexity2.8999273777008057
INFO:root:current mean train loss 1350.4171885522078
INFO:root:current train perplexity2.9008421897888184
INFO:root:current mean train loss 1351.0215470561236
INFO:root:current train perplexity2.902087688446045
INFO:root:current mean train loss 1351.5155147737553
INFO:root:current train perplexity2.903454542160034

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.17s/it]
INFO:root:final mean train loss: 1351.858514905998
INFO:root:final train perplexity: 2.9041972160339355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.02s/it]
INFO:root:eval mean loss: 2135.859089303524
INFO:root:eval perplexity: 5.625767707824707
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.03s/it]
INFO:root:eval mean loss: 2643.6317489299367
INFO:root:eval perplexity: 8.688796997070312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [9:28:10<22:17:58, 553.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1331.9068711224725
INFO:root:current train perplexity2.878917932510376
INFO:root:current mean train loss 1326.7407909791862
INFO:root:current train perplexity2.854243755340576
INFO:root:current mean train loss 1330.49708596254
INFO:root:current train perplexity2.8698062896728516
INFO:root:current mean train loss 1329.384421708341
INFO:root:current train perplexity2.8690834045410156
INFO:root:current mean train loss 1331.9514669251332
INFO:root:current train perplexity2.8679287433624268
INFO:root:current mean train loss 1333.0014060945546
INFO:root:current train perplexity2.8669517040252686
INFO:root:current mean train loss 1334.2544288033566
INFO:root:current train perplexity2.8661251068115234
INFO:root:current mean train loss 1335.2341569697824
INFO:root:current train perplexity2.8668110370635986
INFO:root:current mean train loss 1335.7338630072504
INFO:root:current train perplexity2.86633563041687
INFO:root:current mean train loss 1335.8855933244529
INFO:root:current train perplexity2.8672375679016113
INFO:root:current mean train loss 1336.1951193597376
INFO:root:current train perplexity2.8676555156707764
INFO:root:current mean train loss 1336.717102696656
INFO:root:current train perplexity2.8689987659454346
INFO:root:current mean train loss 1337.6653155507686
INFO:root:current train perplexity2.871211528778076
INFO:root:current mean train loss 1338.5240520608836
INFO:root:current train perplexity2.8722949028015137
INFO:root:current mean train loss 1338.7482241919183
INFO:root:current train perplexity2.874061107635498
INFO:root:current mean train loss 1338.8623971553639
INFO:root:current train perplexity2.8757283687591553
INFO:root:current mean train loss 1339.6868026154423
INFO:root:current train perplexity2.8770570755004883
INFO:root:current mean train loss 1339.9792680399366
INFO:root:current train perplexity2.877937078475952
INFO:root:current mean train loss 1340.8905137123431
INFO:root:current train perplexity2.8796002864837646
INFO:root:current mean train loss 1341.2561759751486
INFO:root:current train perplexity2.8803677558898926

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.32s/it]
INFO:root:final mean train loss: 1341.3520452102146
INFO:root:final train perplexity: 2.880232334136963
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.41s/it]
INFO:root:eval mean loss: 2143.4210646609044
INFO:root:eval perplexity: 5.660278797149658
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.60s/it]
INFO:root:eval mean loss: 2653.3195662261746
INFO:root:eval perplexity: 8.757912635803223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [9:38:03<22:37:00, 565.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1321.3578910079657
INFO:root:current train perplexity2.823573589324951
INFO:root:current mean train loss 1320.2605384675082
INFO:root:current train perplexity2.820000171661377
INFO:root:current mean train loss 1320.4109470321837
INFO:root:current train perplexity2.820861339569092
INFO:root:current mean train loss 1322.89742963742
INFO:root:current train perplexity2.831056594848633
INFO:root:current mean train loss 1325.1636757184556
INFO:root:current train perplexity2.8339767456054688
INFO:root:current mean train loss 1325.8296627314683
INFO:root:current train perplexity2.8375117778778076
INFO:root:current mean train loss 1325.7013385731686
INFO:root:current train perplexity2.8389852046966553
INFO:root:current mean train loss 1327.149154642768
INFO:root:current train perplexity2.840625047683716
INFO:root:current mean train loss 1327.821944311839
INFO:root:current train perplexity2.842778205871582
INFO:root:current mean train loss 1327.664761419928
INFO:root:current train perplexity2.844080924987793
INFO:root:current mean train loss 1327.9518040621656
INFO:root:current train perplexity2.8469409942626953
INFO:root:current mean train loss 1328.3951967506177
INFO:root:current train perplexity2.8487329483032227
INFO:root:current mean train loss 1328.4970799727405
INFO:root:current train perplexity2.8493478298187256
INFO:root:current mean train loss 1329.0805745382472
INFO:root:current train perplexity2.8501152992248535
INFO:root:current mean train loss 1329.5787389690838
INFO:root:current train perplexity2.8506898880004883
INFO:root:current mean train loss 1330.6512613302657
INFO:root:current train perplexity2.853273868560791
INFO:root:current mean train loss 1330.971356729823
INFO:root:current train perplexity2.8557143211364746
INFO:root:current mean train loss 1331.186462437201
INFO:root:current train perplexity2.85693097114563
INFO:root:current mean train loss 1331.3974597504305
INFO:root:current train perplexity2.8584890365600586
INFO:root:current mean train loss 1332.3632629175543
INFO:root:current train perplexity2.859809637069702

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:10<00:00, 610.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:10<00:00, 610.42s/it]
INFO:root:final mean train loss: 1332.3437375036442
INFO:root:final train perplexity: 2.859841823577881
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.67s/it]
INFO:root:eval mean loss: 2146.943047706117
INFO:root:eval perplexity: 5.676425457000732
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.71s/it]
INFO:root:eval mean loss: 2659.162948716617
INFO:root:eval perplexity: 8.7998685836792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [9:50:06<24:20:25, 612.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1323.2079072840074
INFO:root:current train perplexity2.8052501678466797
INFO:root:current mean train loss 1314.54737781343
INFO:root:current train perplexity2.8117778301239014
INFO:root:current mean train loss 1315.298937897184
INFO:root:current train perplexity2.8068082332611084
INFO:root:current mean train loss 1315.6272274514904
INFO:root:current train perplexity2.8076581954956055
INFO:root:current mean train loss 1316.320825299646
INFO:root:current train perplexity2.8094980716705322
INFO:root:current mean train loss 1316.5568757392991
INFO:root:current train perplexity2.8146274089813232
INFO:root:current mean train loss 1315.2540783910695
INFO:root:current train perplexity2.8180313110351562
INFO:root:current mean train loss 1315.5251342455547
INFO:root:current train perplexity2.8198513984680176
INFO:root:current mean train loss 1317.1857682329169
INFO:root:current train perplexity2.821683883666992
INFO:root:current mean train loss 1318.5638596716005
INFO:root:current train perplexity2.823496103286743
INFO:root:current mean train loss 1319.8296188754534
INFO:root:current train perplexity2.8266966342926025
INFO:root:current mean train loss 1320.240673326466
INFO:root:current train perplexity2.827773094177246
INFO:root:current mean train loss 1320.6242902015663
INFO:root:current train perplexity2.829627752304077
INFO:root:current mean train loss 1321.0486619737412
INFO:root:current train perplexity2.8314549922943115
INFO:root:current mean train loss 1321.2861387995997
INFO:root:current train perplexity2.8341996669769287
INFO:root:current mean train loss 1322.1426665636957
INFO:root:current train perplexity2.836148738861084
INFO:root:current mean train loss 1322.3601865333906
INFO:root:current train perplexity2.8364977836608887
INFO:root:current mean train loss 1322.5281357571012
INFO:root:current train perplexity2.8376896381378174
INFO:root:current mean train loss 1323.6759278912104
INFO:root:current train perplexity2.839452028274536
INFO:root:current mean train loss 1324.1052174141737
INFO:root:current train perplexity2.840400218963623

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:46<00:00, 646.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:46<00:00, 646.12s/it]
INFO:root:final mean train loss: 1323.8594331419113
INFO:root:final train perplexity: 2.8407700061798096
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.98s/it]
INFO:root:eval mean loss: 2154.4530864742633
INFO:root:eval perplexity: 5.711005687713623
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.61s/it]
INFO:root:eval mean loss: 2668.0324932125445
INFO:root:eval perplexity: 8.863931655883789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [10:02:45<25:54:10, 656.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1308.7124497357536
INFO:root:current train perplexity2.7858686447143555
INFO:root:current mean train loss 1304.174167282517
INFO:root:current train perplexity2.793482780456543
INFO:root:current mean train loss 1303.8362814384595
INFO:root:current train perplexity2.798121452331543
INFO:root:current mean train loss 1304.438527927151
INFO:root:current train perplexity2.7991232872009277
INFO:root:current mean train loss 1305.4125699701997
INFO:root:current train perplexity2.8022100925445557
INFO:root:current mean train loss 1304.4546178051548
INFO:root:current train perplexity2.80330228805542
INFO:root:current mean train loss 1306.3069674526687
INFO:root:current train perplexity2.8074684143066406
INFO:root:current mean train loss 1307.7370538602208
INFO:root:current train perplexity2.8080103397369385
INFO:root:current mean train loss 1309.168193580067
INFO:root:current train perplexity2.8099498748779297
INFO:root:current mean train loss 1309.5432894789024
INFO:root:current train perplexity2.810298204421997
INFO:root:current mean train loss 1311.2646347116215
INFO:root:current train perplexity2.810065746307373
INFO:root:current mean train loss 1311.121392178435
INFO:root:current train perplexity2.8102989196777344
INFO:root:current mean train loss 1311.7694715922908
INFO:root:current train perplexity2.81095027923584
INFO:root:current mean train loss 1311.531370924526
INFO:root:current train perplexity2.8118538856506348
INFO:root:current mean train loss 1312.280782762521
INFO:root:current train perplexity2.8137989044189453
INFO:root:current mean train loss 1313.0597181061464
INFO:root:current train perplexity2.8154306411743164
INFO:root:current mean train loss 1313.2656486896096
INFO:root:current train perplexity2.8160998821258545
INFO:root:current mean train loss 1313.4699080608807
INFO:root:current train perplexity2.8161356449127197
INFO:root:current mean train loss 1314.435793476459
INFO:root:current train perplexity2.818188190460205

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:57<00:00, 657.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:57<00:00, 657.90s/it]
INFO:root:final mean train loss: 1314.3016263237519
INFO:root:final train perplexity: 2.819437265396118
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.62s/it]
INFO:root:eval mean loss: 2165.1387285814217
INFO:root:eval perplexity: 5.760573863983154
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:58<00:00, 58.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:58<00:00, 58.48s/it]
INFO:root:eval mean loss: 2680.3955246945643
INFO:root:eval perplexity: 8.954008102416992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [10:15:39<27:05:43, 691.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1300.001953125
INFO:root:current train perplexity2.772111654281616
INFO:root:current mean train loss 1291.7109087775734
INFO:root:current train perplexity2.774599313735962
INFO:root:current mean train loss 1291.5077345442064
INFO:root:current train perplexity2.779376983642578
INFO:root:current mean train loss 1291.9455752341164
INFO:root:current train perplexity2.7775397300720215
INFO:root:current mean train loss 1294.525087574821
INFO:root:current train perplexity2.778712034225464
INFO:root:current mean train loss 1295.538719633186
INFO:root:current train perplexity2.7806479930877686
INFO:root:current mean train loss 1295.5814938972956
INFO:root:current train perplexity2.7822351455688477
INFO:root:current mean train loss 1295.9995784922544
INFO:root:current train perplexity2.783531665802002
INFO:root:current mean train loss 1297.7521954391366
INFO:root:current train perplexity2.786159038543701
INFO:root:current mean train loss 1299.1817808299265
INFO:root:current train perplexity2.789635181427002
INFO:root:current mean train loss 1299.4780157702173
INFO:root:current train perplexity2.7910845279693604
INFO:root:current mean train loss 1300.7573108153854
INFO:root:current train perplexity2.792677640914917
INFO:root:current mean train loss 1301.7194136684627
INFO:root:current train perplexity2.791987419128418
INFO:root:current mean train loss 1301.6745822045111
INFO:root:current train perplexity2.791994333267212
INFO:root:current mean train loss 1302.1535172618915
INFO:root:current train perplexity2.7931504249572754
INFO:root:current mean train loss 1302.9988265158174
INFO:root:current train perplexity2.794797658920288
INFO:root:current mean train loss 1304.361162392834
INFO:root:current train perplexity2.7952840328216553
INFO:root:current mean train loss 1304.8866164748733
INFO:root:current train perplexity2.7962491512298584
INFO:root:current mean train loss 1305.3506646532064
INFO:root:current train perplexity2.7978005409240723
INFO:root:current mean train loss 1305.8494362816073
INFO:root:current train perplexity2.799692153930664

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:38<00:00, 638.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:38<00:00, 638.83s/it]
INFO:root:final mean train loss: 1305.4908015741223
INFO:root:final train perplexity: 2.7999138832092285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.06s/it]
INFO:root:eval mean loss: 2170.9775044326243
INFO:root:eval perplexity: 5.787838935852051
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.68s/it]
INFO:root:eval mean loss: 2686.9225390278702
INFO:root:eval perplexity: 9.001932144165039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [10:28:09<27:35:05, 709.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1284.7085217927631
INFO:root:current train perplexity2.762014150619507
INFO:root:current mean train loss 1280.7121705127363
INFO:root:current train perplexity2.752884864807129
INFO:root:current mean train loss 1284.722584902968
INFO:root:current train perplexity2.749967098236084
INFO:root:current mean train loss 1287.3996268245494
INFO:root:current train perplexity2.752743721008301
INFO:root:current mean train loss 1287.6126385600014
INFO:root:current train perplexity2.7568306922912598
INFO:root:current mean train loss 1288.6822846105792
INFO:root:current train perplexity2.7571346759796143
INFO:root:current mean train loss 1289.329678191892
INFO:root:current train perplexity2.759946346282959
INFO:root:current mean train loss 1291.0078398342425
INFO:root:current train perplexity2.7620067596435547
INFO:root:current mean train loss 1290.760516826923
INFO:root:current train perplexity2.76633882522583
INFO:root:current mean train loss 1291.7131291867859
INFO:root:current train perplexity2.7657148838043213
INFO:root:current mean train loss 1293.3817060805632
INFO:root:current train perplexity2.768024444580078
INFO:root:current mean train loss 1293.3758328926149
INFO:root:current train perplexity2.76979660987854
INFO:root:current mean train loss 1294.001042955131
INFO:root:current train perplexity2.7707364559173584
INFO:root:current mean train loss 1294.3292575977896
INFO:root:current train perplexity2.7715373039245605
INFO:root:current mean train loss 1295.0008045113195
INFO:root:current train perplexity2.7747559547424316
INFO:root:current mean train loss 1295.4493435526301
INFO:root:current train perplexity2.7765941619873047
INFO:root:current mean train loss 1295.534226434624
INFO:root:current train perplexity2.777738332748413
INFO:root:current mean train loss 1295.8590057355293
INFO:root:current train perplexity2.778066635131836
INFO:root:current mean train loss 1296.3591180416568
INFO:root:current train perplexity2.7790608406066895
INFO:root:current mean train loss 1297.3335862947417
INFO:root:current train perplexity2.780721664428711

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:35<00:00, 635.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:35<00:00, 635.97s/it]
INFO:root:final mean train loss: 1297.1482205989682
INFO:root:final train perplexity: 2.781552314758301
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.97s/it]
INFO:root:eval mean loss: 2179.4455445201684
INFO:root:eval perplexity: 5.8276143074035645
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:55<00:00, 55.43s/it]
INFO:root:eval mean loss: 2694.6491075880986
INFO:root:eval perplexity: 9.058993339538574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [10:40:38<27:50:53, 721.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1277.6934577094185
INFO:root:current train perplexity2.7099854946136475
INFO:root:current mean train loss 1270.968131570255
INFO:root:current train perplexity2.7237815856933594
INFO:root:current mean train loss 1273.4492953025688
INFO:root:current train perplexity2.73225474357605
INFO:root:current mean train loss 1275.6682724725633
INFO:root:current train perplexity2.737156629562378
INFO:root:current mean train loss 1277.335303350326
INFO:root:current train perplexity2.7403008937835693
INFO:root:current mean train loss 1279.5112869490438
INFO:root:current train perplexity2.743417501449585
INFO:root:current mean train loss 1278.9284128632935
INFO:root:current train perplexity2.745091438293457
INFO:root:current mean train loss 1280.834587097168
INFO:root:current train perplexity2.7463817596435547
INFO:root:current mean train loss 1281.9248354971123
INFO:root:current train perplexity2.7473254203796387
INFO:root:current mean train loss 1282.8245626596304
INFO:root:current train perplexity2.7497189044952393
INFO:root:current mean train loss 1283.4487820776274
INFO:root:current train perplexity2.7515010833740234
INFO:root:current mean train loss 1284.0989516352263
INFO:root:current train perplexity2.7527120113372803
INFO:root:current mean train loss 1284.4984723433708
INFO:root:current train perplexity2.753714084625244
INFO:root:current mean train loss 1284.8857790096076
INFO:root:current train perplexity2.7533795833587646
INFO:root:current mean train loss 1285.6523173127666
INFO:root:current train perplexity2.7544612884521484
INFO:root:current mean train loss 1286.1307010650635
INFO:root:current train perplexity2.7560184001922607
INFO:root:current mean train loss 1286.7369809325576
INFO:root:current train perplexity2.755863666534424
INFO:root:current mean train loss 1287.1736840454664
INFO:root:current train perplexity2.7572243213653564
INFO:root:current mean train loss 1287.6025992998111
INFO:root:current train perplexity2.7591419219970703
INFO:root:current mean train loss 1287.8531896417792
INFO:root:current train perplexity2.760563850402832

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:04<00:00, 604.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:04<00:00, 604.61s/it]
INFO:root:final mean train loss: 1288.0916776979325
INFO:root:final train perplexity: 2.7617554664611816
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.72s/it]
INFO:root:eval mean loss: 2184.042784345911
INFO:root:eval perplexity: 5.8493218421936035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.33s/it]
INFO:root:eval mean loss: 2701.9714892231827
INFO:root:eval perplexity: 9.113404273986816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [10:52:33<27:34:06, 719.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1269.4486590691333
INFO:root:current train perplexity2.7531933784484863
INFO:root:current mean train loss 1262.7096569584864
INFO:root:current train perplexity2.7345879077911377
INFO:root:current mean train loss 1265.4353514660017
INFO:root:current train perplexity2.7311432361602783
INFO:root:current mean train loss 1266.2490628596406
INFO:root:current train perplexity2.72648549079895
INFO:root:current mean train loss 1270.676303754053
INFO:root:current train perplexity2.7263426780700684
INFO:root:current mean train loss 1270.5246434134126
INFO:root:current train perplexity2.723952293395996
INFO:root:current mean train loss 1272.0764700406178
INFO:root:current train perplexity2.7232494354248047
INFO:root:current mean train loss 1272.8153343656622
INFO:root:current train perplexity2.725837469100952
INFO:root:current mean train loss 1274.4260889301545
INFO:root:current train perplexity2.726842164993286
INFO:root:current mean train loss 1274.939306205117
INFO:root:current train perplexity2.729191541671753
INFO:root:current mean train loss 1274.7776937312663
INFO:root:current train perplexity2.7318637371063232
INFO:root:current mean train loss 1275.2138502479943
INFO:root:current train perplexity2.732095718383789
INFO:root:current mean train loss 1276.3279417483975
INFO:root:current train perplexity2.7333290576934814
INFO:root:current mean train loss 1277.2133841391237
INFO:root:current train perplexity2.735743284225464
INFO:root:current mean train loss 1277.8241627135938
INFO:root:current train perplexity2.7372896671295166
INFO:root:current mean train loss 1278.0351766081526
INFO:root:current train perplexity2.7379984855651855
INFO:root:current mean train loss 1278.044651149189
INFO:root:current train perplexity2.7398135662078857
INFO:root:current mean train loss 1278.581326821431
INFO:root:current train perplexity2.7411177158355713
INFO:root:current mean train loss 1279.0615754145515
INFO:root:current train perplexity2.741809129714966
INFO:root:current mean train loss 1280.0644281859038
INFO:root:current train perplexity2.743468761444092

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:24<00:00, 624.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:24<00:00, 624.48s/it]
INFO:root:final mean train loss: 1279.8767263438447
INFO:root:final train perplexity: 2.7439205646514893
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.93s/it]
INFO:root:eval mean loss: 2190.7265252728835
INFO:root:eval perplexity: 5.8810248374938965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.34s/it]
INFO:root:eval mean loss: 2709.2997821780805
INFO:root:eval perplexity: 9.168188095092773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [11:04:49<27:33:38, 724.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1247.9180524553572
INFO:root:current train perplexity2.6896846294403076
INFO:root:current mean train loss 1256.9863123276655
INFO:root:current train perplexity2.6942036151885986
INFO:root:current mean train loss 1258.8791264286747
INFO:root:current train perplexity2.6938040256500244
INFO:root:current mean train loss 1260.0552262589738
INFO:root:current train perplexity2.6995999813079834
INFO:root:current mean train loss 1261.7424098238032
INFO:root:current train perplexity2.7026853561401367
INFO:root:current mean train loss 1262.377960097999
INFO:root:current train perplexity2.7030410766601562
INFO:root:current mean train loss 1263.0354125976562
INFO:root:current train perplexity2.7058868408203125
INFO:root:current mean train loss 1265.2295188210228
INFO:root:current train perplexity2.7083194255828857
INFO:root:current mean train loss 1265.5875742243625
INFO:root:current train perplexity2.7103114128112793
INFO:root:current mean train loss 1266.8006272148841
INFO:root:current train perplexity2.7114250659942627
INFO:root:current mean train loss 1267.9561453846013
INFO:root:current train perplexity2.7140302658081055
INFO:root:current mean train loss 1268.7264699560965
INFO:root:current train perplexity2.7164249420166016
INFO:root:current mean train loss 1268.9780350332185
INFO:root:current train perplexity2.717764377593994
INFO:root:current mean train loss 1268.567655733206
INFO:root:current train perplexity2.7190887928009033
INFO:root:current mean train loss 1269.322457864982
INFO:root:current train perplexity2.7220616340637207
INFO:root:current mean train loss 1270.3079634939788
INFO:root:current train perplexity2.721729278564453
INFO:root:current mean train loss 1270.498749912285
INFO:root:current train perplexity2.7236568927764893
INFO:root:current mean train loss 1271.349025437522
INFO:root:current train perplexity2.7245020866394043
INFO:root:current mean train loss 1271.7456308619862
INFO:root:current train perplexity2.725163221359253
INFO:root:current mean train loss 1272.2272979581417
INFO:root:current train perplexity2.726815938949585

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:26<00:00, 626.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [10:26<00:00, 626.50s/it]
INFO:root:final mean train loss: 1271.9784785668417
INFO:root:final train perplexity: 2.726881504058838
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:56<00:00, 56.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:56<00:00, 56.45s/it]
INFO:root:eval mean loss: 2199.7905671681074
INFO:root:eval perplexity: 5.924293518066406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 2720.5275900549923
INFO:root:eval perplexity: 9.252764701843262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [11:16:57<27:24:36, 725.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1241.5209189228628
INFO:root:current train perplexity2.6834628582000732
INFO:root:current mean train loss 1245.6430376838234
INFO:root:current train perplexity2.6836252212524414
INFO:root:current mean train loss 1247.0440337815767
INFO:root:current train perplexity2.6832213401794434
INFO:root:current mean train loss 1247.0032520540617
INFO:root:current train perplexity2.6812498569488525
INFO:root:current mean train loss 1250.3573699888507
INFO:root:current train perplexity2.6833202838897705
INFO:root:current mean train loss 1252.2470141643155
INFO:root:current train perplexity2.68725323677063
INFO:root:current mean train loss 1253.967321404203
INFO:root:current train perplexity2.6910243034362793
INFO:root:current mean train loss 1254.5163059258855
INFO:root:current train perplexity2.691777467727661
INFO:root:current mean train loss 1254.9528599408998
INFO:root:current train perplexity2.6929211616516113
INFO:root:current mean train loss 1255.8680137935744
INFO:root:current train perplexity2.692979574203491
INFO:root:current mean train loss 1256.4232610090128
INFO:root:current train perplexity2.6958045959472656
INFO:root:current mean train loss 1257.0536977740628
INFO:root:current train perplexity2.696768283843994
INFO:root:current mean train loss 1257.8851696805798
INFO:root:current train perplexity2.698047637939453
INFO:root:current mean train loss 1258.5539574544036
INFO:root:current train perplexity2.697728157043457
INFO:root:current mean train loss 1259.3168318132145
INFO:root:current train perplexity2.6994051933288574
INFO:root:current mean train loss 1259.501524071312
INFO:root:current train perplexity2.701058864593506
INFO:root:current mean train loss 1259.8322775614072
INFO:root:current train perplexity2.7030630111694336
INFO:root:current mean train loss 1260.9022227726505
INFO:root:current train perplexity2.7041826248168945
INFO:root:current mean train loss 1262.0123635814123
INFO:root:current train perplexity2.705732583999634

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.85s/it]
INFO:root:final mean train loss: 1262.365072014713
INFO:root:final train perplexity: 2.7062854766845703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.89s/it]
INFO:root:eval mean loss: 2208.9950358072915
INFO:root:eval perplexity: 5.968559265136719
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.93s/it]
INFO:root:eval mean loss: 2729.639167947972
INFO:root:eval perplexity: 9.321967124938965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [11:26:25<25:25:58, 678.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1240.3359680175781
INFO:root:current train perplexity2.6872339248657227
INFO:root:current mean train loss 1251.6014556884766
INFO:root:current train perplexity2.6823699474334717
INFO:root:current mean train loss 1247.5063614190794
INFO:root:current train perplexity2.675841808319092
INFO:root:current mean train loss 1246.4763376336348
INFO:root:current train perplexity2.6835367679595947
INFO:root:current mean train loss 1249.0721716550318
INFO:root:current train perplexity2.6811327934265137
INFO:root:current mean train loss 1250.291731092665
INFO:root:current train perplexity2.6775858402252197
INFO:root:current mean train loss 1249.8843889046977
INFO:root:current train perplexity2.6756932735443115
INFO:root:current mean train loss 1248.7105931368742
INFO:root:current train perplexity2.676224708557129
INFO:root:current mean train loss 1249.9013252827658
INFO:root:current train perplexity2.678476572036743
INFO:root:current mean train loss 1249.5958669206736
INFO:root:current train perplexity2.6789424419403076
INFO:root:current mean train loss 1250.0822362405845
INFO:root:current train perplexity2.680370807647705
INFO:root:current mean train loss 1250.339415287626
INFO:root:current train perplexity2.6812679767608643
INFO:root:current mean train loss 1251.2437530213417
INFO:root:current train perplexity2.682234525680542
INFO:root:current mean train loss 1252.7922636628882
INFO:root:current train perplexity2.6832947731018066
INFO:root:current mean train loss 1253.408781220091
INFO:root:current train perplexity2.6852211952209473
INFO:root:current mean train loss 1253.5404102244277
INFO:root:current train perplexity2.6872427463531494
INFO:root:current mean train loss 1253.6305038185785
INFO:root:current train perplexity2.6875216960906982
INFO:root:current mean train loss 1254.2024031088386
INFO:root:current train perplexity2.688260316848755
INFO:root:current mean train loss 1255.2512058165016
INFO:root:current train perplexity2.6895108222961426
INFO:root:current mean train loss 1255.476040751994
INFO:root:current train perplexity2.6910383701324463

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.13s/it]
INFO:root:final mean train loss: 1255.390179655736
INFO:root:final train perplexity: 2.691439628601074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.98s/it]
INFO:root:eval mean loss: 2213.424010797595
INFO:root:eval perplexity: 5.989975452423096
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.51s/it]
INFO:root:eval mean loss: 2735.8592200313055
INFO:root:eval perplexity: 9.369510650634766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [11:35:46<23:56:13, 643.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1215.4316871279761
INFO:root:current train perplexity2.6355624198913574
INFO:root:current mean train loss 1228.1875423715135
INFO:root:current train perplexity2.64471697807312
INFO:root:current mean train loss 1230.696848597462
INFO:root:current train perplexity2.6473388671875
INFO:root:current mean train loss 1235.0621531834113
INFO:root:current train perplexity2.65224552154541
INFO:root:current mean train loss 1234.9187542333173
INFO:root:current train perplexity2.6545166969299316
INFO:root:current mean train loss 1237.445916291162
INFO:root:current train perplexity2.6570005416870117
INFO:root:current mean train loss 1238.9036108437751
INFO:root:current train perplexity2.656036376953125
INFO:root:current mean train loss 1239.996313002503
INFO:root:current train perplexity2.6577301025390625
INFO:root:current mean train loss 1241.1425586472765
INFO:root:current train perplexity2.661308765411377
INFO:root:current mean train loss 1242.1468059196015
INFO:root:current train perplexity2.6626853942871094
INFO:root:current mean train loss 1242.1462862648063
INFO:root:current train perplexity2.6638243198394775
INFO:root:current mean train loss 1242.3971328395057
INFO:root:current train perplexity2.665156841278076
INFO:root:current mean train loss 1243.266442701135
INFO:root:current train perplexity2.6667933464050293
INFO:root:current mean train loss 1244.0687556183761
INFO:root:current train perplexity2.6685407161712646
INFO:root:current mean train loss 1244.622227002331
INFO:root:current train perplexity2.669619560241699
INFO:root:current mean train loss 1244.842319185055
INFO:root:current train perplexity2.6703102588653564
INFO:root:current mean train loss 1245.6663268628788
INFO:root:current train perplexity2.671083450317383
INFO:root:current mean train loss 1246.2799208450983
INFO:root:current train perplexity2.6721928119659424
INFO:root:current mean train loss 1247.062851597358
INFO:root:current train perplexity2.673299551010132
INFO:root:current mean train loss 1247.3665026099281
INFO:root:current train perplexity2.6733930110931396

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:07<00:00, 487.19s/it]
INFO:root:final mean train loss: 1247.2371030837312
INFO:root:final train perplexity: 2.6741890907287598
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.54s/it]
INFO:root:eval mean loss: 2220.85371647828
INFO:root:eval perplexity: 6.02607536315918
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.48s/it]
INFO:root:eval mean loss: 2746.202788224457
INFO:root:eval perplexity: 9.449106216430664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [11:45:13<22:55:04, 620.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1216.3960442793996
INFO:root:current train perplexity2.629110097885132
INFO:root:current mean train loss 1227.9560201893682
INFO:root:current train perplexity2.636333465576172
INFO:root:current mean train loss 1230.9239440405067
INFO:root:current train perplexity2.6308443546295166
INFO:root:current mean train loss 1232.04620578021
INFO:root:current train perplexity2.634060859680176
INFO:root:current mean train loss 1229.87628647617
INFO:root:current train perplexity2.6347904205322266
INFO:root:current mean train loss 1231.5375218728218
INFO:root:current train perplexity2.6357667446136475
INFO:root:current mean train loss 1231.2947378128674
INFO:root:current train perplexity2.635300397872925
INFO:root:current mean train loss 1232.9624653637893
INFO:root:current train perplexity2.6366970539093018
INFO:root:current mean train loss 1233.3176744410987
INFO:root:current train perplexity2.6385445594787598
INFO:root:current mean train loss 1233.8090251605395
INFO:root:current train perplexity2.6401562690734863
INFO:root:current mean train loss 1234.2479443265295
INFO:root:current train perplexity2.641610860824585
INFO:root:current mean train loss 1234.4714827445355
INFO:root:current train perplexity2.643728733062744
INFO:root:current mean train loss 1235.1352718519663
INFO:root:current train perplexity2.645448923110962
INFO:root:current mean train loss 1235.8245220098795
INFO:root:current train perplexity2.6462042331695557
INFO:root:current mean train loss 1236.1378336814912
INFO:root:current train perplexity2.6485509872436523
INFO:root:current mean train loss 1237.7570598388988
INFO:root:current train perplexity2.6509175300598145
INFO:root:current mean train loss 1238.1524336259445
INFO:root:current train perplexity2.653337240219116
INFO:root:current mean train loss 1238.6912495532986
INFO:root:current train perplexity2.654529571533203
INFO:root:current mean train loss 1238.5729273815798
INFO:root:current train perplexity2.655482053756714
INFO:root:current mean train loss 1239.4503880550988
INFO:root:current train perplexity2.6568474769592285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.40s/it]
INFO:root:final mean train loss: 1239.2503957281915
INFO:root:final train perplexity: 2.657397747039795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 2229.6804861515125
INFO:root:eval perplexity: 6.069247245788574
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.24s/it]
INFO:root:eval mean loss: 2755.074388436392
INFO:root:eval perplexity: 9.517911911010742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [11:54:30<22:02:38, 601.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1211.209992009943
INFO:root:current train perplexity2.6259148120880127
INFO:root:current mean train loss 1214.4111745526714
INFO:root:current train perplexity2.617648124694824
INFO:root:current mean train loss 1217.3461133769915
INFO:root:current train perplexity2.611266613006592
INFO:root:current mean train loss 1220.652948599802
INFO:root:current train perplexity2.6142122745513916
INFO:root:current mean train loss 1223.9193997896634
INFO:root:current train perplexity2.6178770065307617
INFO:root:current mean train loss 1224.6035576347833
INFO:root:current train perplexity2.620016574859619
INFO:root:current mean train loss 1225.020687842915
INFO:root:current train perplexity2.621964454650879
INFO:root:current mean train loss 1225.8726339378104
INFO:root:current train perplexity2.6241445541381836
INFO:root:current mean train loss 1226.3901194433022
INFO:root:current train perplexity2.62729549407959
INFO:root:current mean train loss 1227.0931879652733
INFO:root:current train perplexity2.629837989807129
INFO:root:current mean train loss 1228.3195385395068
INFO:root:current train perplexity2.631619930267334
INFO:root:current mean train loss 1229.692067120705
INFO:root:current train perplexity2.632208824157715
INFO:root:current mean train loss 1230.107186780223
INFO:root:current train perplexity2.6333272457122803
INFO:root:current mean train loss 1230.6915867879381
INFO:root:current train perplexity2.6350038051605225
INFO:root:current mean train loss 1230.7128501865873
INFO:root:current train perplexity2.637239694595337
INFO:root:current mean train loss 1231.3776673344553
INFO:root:current train perplexity2.6380631923675537
INFO:root:current mean train loss 1231.4645250395345
INFO:root:current train perplexity2.638585090637207
INFO:root:current mean train loss 1231.7106225516382
INFO:root:current train perplexity2.6397321224212646
INFO:root:current mean train loss 1231.5651639624748
INFO:root:current train perplexity2.6406807899475098
INFO:root:current mean train loss 1232.144167848865
INFO:root:current train perplexity2.6421878337860107

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.71s/it]
INFO:root:final mean train loss: 1232.068580431224
INFO:root:final train perplexity: 2.6423888206481934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.64s/it]
INFO:root:eval mean loss: 2236.2105782150375
INFO:root:eval perplexity: 6.101385116577148
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.92s/it]
INFO:root:eval mean loss: 2762.384851766816
INFO:root:eval perplexity: 9.574986457824707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [12:03:42<21:20:36, 586.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1207.7264692518447
INFO:root:current train perplexity2.597628116607666
INFO:root:current mean train loss 1216.890654807867
INFO:root:current train perplexity2.6009466648101807
INFO:root:current mean train loss 1216.1018322215361
INFO:root:current train perplexity2.6049649715423584
INFO:root:current mean train loss 1216.0963348060525
INFO:root:current train perplexity2.609407901763916
INFO:root:current mean train loss 1216.3269591250662
INFO:root:current train perplexity2.611316680908203
INFO:root:current mean train loss 1218.0130621636665
INFO:root:current train perplexity2.611412286758423
INFO:root:current mean train loss 1218.5275108700707
INFO:root:current train perplexity2.609929323196411
INFO:root:current mean train loss 1218.8670062919973
INFO:root:current train perplexity2.612177848815918
INFO:root:current mean train loss 1218.6359924876363
INFO:root:current train perplexity2.6115121841430664
INFO:root:current mean train loss 1219.18374520761
INFO:root:current train perplexity2.613879442214966
INFO:root:current mean train loss 1220.3946281546978
INFO:root:current train perplexity2.6154894828796387
INFO:root:current mean train loss 1220.8443229597176
INFO:root:current train perplexity2.616335868835449
INFO:root:current mean train loss 1221.261494090722
INFO:root:current train perplexity2.6182470321655273
INFO:root:current mean train loss 1221.8487084391513
INFO:root:current train perplexity2.6201484203338623
INFO:root:current mean train loss 1222.1026153564453
INFO:root:current train perplexity2.6206514835357666
INFO:root:current mean train loss 1222.6224431239316
INFO:root:current train perplexity2.6220450401306152
INFO:root:current mean train loss 1223.1785996724543
INFO:root:current train perplexity2.6229326725006104
INFO:root:current mean train loss 1223.8222406873854
INFO:root:current train perplexity2.624497413635254
INFO:root:current mean train loss 1224.6532876430413
INFO:root:current train perplexity2.6254656314849854
INFO:root:current mean train loss 1224.5298451390759
INFO:root:current train perplexity2.6259586811065674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.40s/it]
INFO:root:final mean train loss: 1224.2129305764033
INFO:root:final train perplexity: 2.6260688304901123
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.21s/it]
INFO:root:eval mean loss: 2242.445935837766
INFO:root:eval perplexity: 6.132230758666992
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.43s/it]
INFO:root:eval mean loss: 2770.8922205715317
INFO:root:eval perplexity: 9.641839027404785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_retrained/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [12:12:48<20:44:33, 574.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1207.7275390625
INFO:root:current train perplexity2.5690948963165283
INFO:root:current mean train loss 1205.049935800058
INFO:root:current train perplexity2.5814430713653564
INFO:root:current mean train loss 1203.473392050984
INFO:root:current train perplexity2.586409568786621
INFO:root:current mean train loss 1205.5008491574952
INFO:root:current train perplexity2.5903451442718506
INFO:root:current mean train loss 1205.4819550621485
INFO:root:current train perplexity2.594876766204834
INFO:root:current mean train loss 1207.1019316124389
INFO:root:current train perplexity2.5941686630249023
INFO:root:current mean train loss 1208.426046119546
INFO:root:current train perplexity2.597593069076538
INFO:root:current mean train loss 1209.942601579828
INFO:root:current train perplexity2.5995798110961914
INFO:root:current mean train loss 1211.329435505132
INFO:root:current train perplexity2.601111650466919
INFO:root:current mean train loss 1211.6084562018136
INFO:root:current train perplexity2.602534770965576
INFO:root:current mean train loss 1211.483220768588
INFO:root:current train perplexity2.603926181793213
INFO:root:current mean train loss 1212.0909787267071
INFO:root:current train perplexity2.604841947555542
INFO:root:current mean train loss 1212.621680236769
INFO:root:current train perplexity2.606102466583252
INFO:root:current mean train loss 1213.5165331890637
INFO:root:current train perplexity2.607151746749878
slurmstepd: error: *** JOB 26219506 ON ga017 CANCELLED AT 2022-10-24T11:47:10 ***
