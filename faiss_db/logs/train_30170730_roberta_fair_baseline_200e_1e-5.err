INFO:root:Output: roberta_fair_baseline
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Some weights of RobertaForCausalLMBaseline were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10419.13506155303
INFO:root:current train perplexity3509.865234375
INFO:root:current mean train loss 9168.95513701319
INFO:root:current train perplexity1363.1651611328125
INFO:root:current mean train loss 8421.907510712792
INFO:root:current train perplexity753.929931640625
INFO:root:current mean train loss 7883.885825403352
INFO:root:current train perplexity492.1907653808594
INFO:root:current mean train loss 7453.95942960139
INFO:root:current train perplexity350.9552307128906
INFO:root:current mean train loss 7108.922043738262
INFO:root:current train perplexity268.0523376464844
INFO:root:current mean train loss 6826.441371322872
INFO:root:current train perplexity214.4330596923828
INFO:root:current mean train loss 6591.798462677957
INFO:root:current train perplexity177.8729248046875
INFO:root:current mean train loss 6384.15449403417
INFO:root:current train perplexity151.367919921875
INFO:root:current mean train loss 6208.343228726773
INFO:root:current train perplexity131.9062042236328
INFO:root:current mean train loss 6048.899694635393
INFO:root:current train perplexity117.18366241455078
INFO:root:current mean train loss 5909.019806340896
INFO:root:current train perplexity105.14849090576172
INFO:root:current mean train loss 5784.859849561261
INFO:root:current train perplexity95.58876037597656
INFO:root:current mean train loss 5674.601420099178
INFO:root:current train perplexity87.7795639038086
INFO:root:current mean train loss 5576.8964601075195
INFO:root:current train perplexity81.13058471679688
INFO:root:current mean train loss 5481.763808526569
INFO:root:current train perplexity75.4626235961914
INFO:root:current mean train loss 5392.221380511054
INFO:root:current train perplexity70.54668426513672
INFO:root:current mean train loss 5315.869497404156
INFO:root:current train perplexity66.31613159179688
INFO:root:current mean train loss 5243.656798191482
INFO:root:current train perplexity62.630706787109375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.99s/it]
INFO:root:final mean train loss: 5186.924643712277
INFO:root:final train perplexity: 59.94989776611328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 3531.9119561377993
INFO:root:eval perplexity: 17.42794418334961
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3825.331909612561
INFO:root:eval perplexity: 23.21770477294922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/1
  0%|          | 1/200 [10:08<33:37:34, 608.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3871.6476440429688
INFO:root:current train perplexity21.86362648010254
INFO:root:current mean train loss 3859.543844288793
INFO:root:current train perplexity20.889314651489258
INFO:root:current mean train loss 3792.692953604239
INFO:root:current train perplexity20.183879852294922
INFO:root:current mean train loss 3768.9623196758803
INFO:root:current train perplexity19.866907119750977
INFO:root:current mean train loss 3760.605816767766
INFO:root:current train perplexity19.662416458129883
INFO:root:current mean train loss 3741.6321865347927
INFO:root:current train perplexity19.36741065979004
INFO:root:current mean train loss 3729.682353626598
INFO:root:current train perplexity19.150861740112305
INFO:root:current mean train loss 3721.3418159697976
INFO:root:current train perplexity18.9407901763916
INFO:root:current mean train loss 3704.784771788354
INFO:root:current train perplexity18.702836990356445
INFO:root:current mean train loss 3688.0441193559805
INFO:root:current train perplexity18.459199905395508
INFO:root:current mean train loss 3672.414605809009
INFO:root:current train perplexity18.240816116333008
INFO:root:current mean train loss 3661.3143411178316
INFO:root:current train perplexity18.050884246826172
INFO:root:current mean train loss 3644.2166350515267
INFO:root:current train perplexity17.833011627197266
INFO:root:current mean train loss 3630.234933592266
INFO:root:current train perplexity17.615314483642578
INFO:root:current mean train loss 3616.6471490375066
INFO:root:current train perplexity17.433387756347656
INFO:root:current mean train loss 3604.323642539475
INFO:root:current train perplexity17.2696590423584
INFO:root:current mean train loss 3593.0791325333093
INFO:root:current train perplexity17.09917640686035
INFO:root:current mean train loss 3580.0635613572626
INFO:root:current train perplexity16.92452049255371
INFO:root:current mean train loss 3570.0399855559094
INFO:root:current train perplexity16.769723892211914
INFO:root:current mean train loss 3560.6101692215634
INFO:root:current train perplexity16.612627029418945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.55s/it]
INFO:root:final mean train loss: 3552.6985043031787
INFO:root:final train perplexity: 16.506954193115234
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 3039.0507622035684
INFO:root:eval perplexity: 11.695971488952637
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 3402.368905141844
INFO:root:eval perplexity: 16.398418426513672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/2
  1%|          | 2/200 [20:17<33:28:20, 608.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3274.621330492424
INFO:root:current train perplexity13.462478637695312
INFO:root:current mean train loss 3294.623599403783
INFO:root:current train perplexity13.64505672454834
INFO:root:current mean train loss 3290.2604959512473
INFO:root:current train perplexity13.478707313537598
INFO:root:current mean train loss 3274.209694069069
INFO:root:current train perplexity13.345192909240723
INFO:root:current mean train loss 3259.613973075743
INFO:root:current train perplexity13.249493598937988
INFO:root:current mean train loss 3261.7195909797138
INFO:root:current train perplexity13.231417655944824
INFO:root:current mean train loss 3254.2361071256664
INFO:root:current train perplexity13.134590148925781
INFO:root:current mean train loss 3250.2755228539604
INFO:root:current train perplexity13.05518627166748
INFO:root:current mean train loss 3245.1803888547606
INFO:root:current train perplexity13.004676818847656
INFO:root:current mean train loss 3243.145603323034
INFO:root:current train perplexity12.957015991210938
INFO:root:current mean train loss 3235.5841377605425
INFO:root:current train perplexity12.874992370605469
INFO:root:current mean train loss 3227.7940614570693
INFO:root:current train perplexity12.789936065673828
INFO:root:current mean train loss 3221.0768435092255
INFO:root:current train perplexity12.717157363891602
INFO:root:current mean train loss 3212.5766314015027
INFO:root:current train perplexity12.634300231933594
INFO:root:current mean train loss 3208.764590170861
INFO:root:current train perplexity12.584494590759277
INFO:root:current mean train loss 3203.6003449820105
INFO:root:current train perplexity12.532527923583984
INFO:root:current mean train loss 3195.8411300855405
INFO:root:current train perplexity12.455245971679688
INFO:root:current mean train loss 3190.938326528013
INFO:root:current train perplexity12.397285461425781
INFO:root:current mean train loss 3184.873510249378
INFO:root:current train perplexity12.339794158935547
INFO:root:current mean train loss 3178.9833750717394
INFO:root:current train perplexity12.277562141418457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.75s/it]
INFO:root:final mean train loss: 3175.035143999878
INFO:root:final train perplexity: 12.252508163452148
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2805.576064522385
INFO:root:eval perplexity: 9.682433128356934
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3203.8398848729776
INFO:root:eval perplexity: 13.928938865661621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/3
  2%|â–         | 3/200 [30:26<33:18:44, 608.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3026.819091796875
INFO:root:current train perplexity11.133563995361328
INFO:root:current mean train loss 3031.5073876953124
INFO:root:current train perplexity11.044746398925781
INFO:root:current mean train loss 3041.977119140625
INFO:root:current train perplexity11.020389556884766
INFO:root:current mean train loss 3028.379642159598
INFO:root:current train perplexity10.91141128540039
INFO:root:current mean train loss 3017.7840679253472
INFO:root:current train perplexity10.851073265075684
INFO:root:current mean train loss 3014.635025301847
INFO:root:current train perplexity10.816593170166016
INFO:root:current mean train loss 3009.5419001652645
INFO:root:current train perplexity10.783222198486328
INFO:root:current mean train loss 3008.552404296875
INFO:root:current train perplexity10.75348949432373
INFO:root:current mean train loss 3004.1729130284925
INFO:root:current train perplexity10.720025062561035
INFO:root:current mean train loss 3002.9047430098685
INFO:root:current train perplexity10.69865608215332
INFO:root:current mean train loss 2999.5999172247025
INFO:root:current train perplexity10.67104434967041
INFO:root:current mean train loss 2995.130258152174
INFO:root:current train perplexity10.642638206481934
INFO:root:current mean train loss 2990.9345380859377
INFO:root:current train perplexity10.613341331481934
INFO:root:current mean train loss 2988.8493041087963
INFO:root:current train perplexity10.596304893493652
INFO:root:current mean train loss 2985.784796100485
INFO:root:current train perplexity10.561697006225586
INFO:root:current mean train loss 2981.0481300403226
INFO:root:current train perplexity10.534672737121582
INFO:root:current mean train loss 2977.76757960464
INFO:root:current train perplexity10.509968757629395
INFO:root:current mean train loss 2976.0420835658483
INFO:root:current train perplexity10.480942726135254
INFO:root:current mean train loss 2973.092740181588
INFO:root:current train perplexity10.447837829589844
INFO:root:current mean train loss 2969.8364225260416
INFO:root:current train perplexity10.410877227783203

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.36s/it]
INFO:root:final mean train loss: 2967.6080405985053
INFO:root:final train perplexity: 10.402327537536621
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2685.607880287982
INFO:root:eval perplexity: 8.78664779663086
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3112.3616549444537
INFO:root:eval perplexity: 12.919807434082031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/4
  2%|â–         | 4/200 [40:33<33:07:07, 608.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2923.1044557486007
INFO:root:current train perplexity9.758869171142578
INFO:root:current mean train loss 2889.191213276572
INFO:root:current train perplexity9.708929061889648
INFO:root:current mean train loss 2874.450587583392
INFO:root:current train perplexity9.645536422729492
INFO:root:current mean train loss 2866.0188946217218
INFO:root:current train perplexity9.634209632873535
INFO:root:current mean train loss 2871.5127909821667
INFO:root:current train perplexity9.643204689025879
INFO:root:current mean train loss 2869.7142994929454
INFO:root:current train perplexity9.630358695983887
INFO:root:current mean train loss 2871.7154824687263
INFO:root:current train perplexity9.641045570373535
INFO:root:current mean train loss 2869.7848172542167
INFO:root:current train perplexity9.610451698303223
INFO:root:current mean train loss 2869.895587784746
INFO:root:current train perplexity9.599448204040527
INFO:root:current mean train loss 2864.0375999284997
INFO:root:current train perplexity9.567441940307617
INFO:root:current mean train loss 2861.1760876270355
INFO:root:current train perplexity9.542490005493164
INFO:root:current mean train loss 2857.0213598988457
INFO:root:current train perplexity9.511791229248047
INFO:root:current mean train loss 2852.3629995344563
INFO:root:current train perplexity9.481325149536133
INFO:root:current mean train loss 2846.648220505955
INFO:root:current train perplexity9.456762313842773
INFO:root:current mean train loss 2845.742660470454
INFO:root:current train perplexity9.435990333557129
INFO:root:current mean train loss 2841.610951241674
INFO:root:current train perplexity9.41500186920166
INFO:root:current mean train loss 2838.4664592960316
INFO:root:current train perplexity9.38907527923584
INFO:root:current mean train loss 2836.5918934535493
INFO:root:current train perplexity9.3728666305542
INFO:root:current mean train loss 2834.182146951995
INFO:root:current train perplexity9.354446411132812
INFO:root:current mean train loss 2831.695386350367
INFO:root:current train perplexity9.339112281799316

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.91s/it]
INFO:root:final mean train loss: 2831.18989862316
INFO:root:final train perplexity: 9.340583801269531
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2591.920990206671
INFO:root:eval perplexity: 8.145133972167969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3026.598474813691
INFO:root:eval perplexity: 12.040230751037598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/5
  2%|â–Ž         | 5/200 [50:40<32:55:37, 607.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2782.917186918713
INFO:root:current train perplexity8.976094245910645
INFO:root:current mean train loss 2780.8897877568784
INFO:root:current train perplexity8.888507843017578
INFO:root:current mean train loss 2774.8194021305567
INFO:root:current train perplexity8.874250411987305
INFO:root:current mean train loss 2762.443801879883
INFO:root:current train perplexity8.823990821838379
INFO:root:current mean train loss 2758.647841776698
INFO:root:current train perplexity8.798861503601074
INFO:root:current mean train loss 2748.938213609669
INFO:root:current train perplexity8.773578643798828
INFO:root:current mean train loss 2745.5343395924706
INFO:root:current train perplexity8.75961685180664
INFO:root:current mean train loss 2746.6787124945195
INFO:root:current train perplexity8.77596378326416
INFO:root:current mean train loss 2747.9538842110614
INFO:root:current train perplexity8.76679801940918
INFO:root:current mean train loss 2746.504631228563
INFO:root:current train perplexity8.752362251281738
INFO:root:current mean train loss 2746.4712148293356
INFO:root:current train perplexity8.740378379821777
INFO:root:current mean train loss 2744.9114670624604
INFO:root:current train perplexity8.736932754516602
INFO:root:current mean train loss 2743.4408314547436
INFO:root:current train perplexity8.721863746643066
INFO:root:current mean train loss 2743.308431989196
INFO:root:current train perplexity8.719639778137207
INFO:root:current mean train loss 2743.2409332357647
INFO:root:current train perplexity8.710526466369629
INFO:root:current mean train loss 2740.0287427805893
INFO:root:current train perplexity8.692089080810547
INFO:root:current mean train loss 2737.4105581251856
INFO:root:current train perplexity8.679003715515137
INFO:root:current mean train loss 2735.195886175729
INFO:root:current train perplexity8.668486595153809
INFO:root:current mean train loss 2734.5290720427365
INFO:root:current train perplexity8.656757354736328

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.08s/it]
INFO:root:final mean train loss: 2732.5941307387207
INFO:root:final train perplexity: 8.641337394714355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2534.6871965557125
INFO:root:eval perplexity: 7.776496887207031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2977.1592615005816
INFO:root:eval perplexity: 11.560660362243652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/6
  3%|â–Ž         | 6/200 [1:00:49<32:45:57, 608.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2523.928466796875
INFO:root:current train perplexity7.91526985168457
INFO:root:current mean train loss 2680.569667098546
INFO:root:current train perplexity8.307743072509766
INFO:root:current mean train loss 2673.856054201648
INFO:root:current train perplexity8.279741287231445
INFO:root:current mean train loss 2677.1591699543187
INFO:root:current train perplexity8.274413108825684
INFO:root:current mean train loss 2672.2036723377105
INFO:root:current train perplexity8.26363468170166
INFO:root:current mean train loss 2668.198341110747
INFO:root:current train perplexity8.256437301635742
INFO:root:current mean train loss 2671.024621642965
INFO:root:current train perplexity8.248149871826172
INFO:root:current mean train loss 2673.6521250334345
INFO:root:current train perplexity8.240992546081543
INFO:root:current mean train loss 2670.030477954803
INFO:root:current train perplexity8.221168518066406
INFO:root:current mean train loss 2668.3910555654306
INFO:root:current train perplexity8.215832710266113
INFO:root:current mean train loss 2664.8389654778816
INFO:root:current train perplexity8.202848434448242
INFO:root:current mean train loss 2665.1225375280287
INFO:root:current train perplexity8.20317268371582
INFO:root:current mean train loss 2663.9175603582303
INFO:root:current train perplexity8.19090747833252
INFO:root:current mean train loss 2661.8349067048784
INFO:root:current train perplexity8.175081253051758
INFO:root:current mean train loss 2660.3795719296486
INFO:root:current train perplexity8.166434288024902
INFO:root:current mean train loss 2659.9081048832027
INFO:root:current train perplexity8.1569242477417
INFO:root:current mean train loss 2659.0473393399143
INFO:root:current train perplexity8.157154083251953
INFO:root:current mean train loss 2660.5525175046387
INFO:root:current train perplexity8.154618263244629
INFO:root:current mean train loss 2659.9007981134614
INFO:root:current train perplexity8.152956008911133
INFO:root:current mean train loss 2657.5276288589885
INFO:root:current train perplexity8.142247200012207

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.44s/it]
INFO:root:final mean train loss: 2656.543969751186
INFO:root:final train perplexity: 8.137953758239746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it]
INFO:root:eval mean loss: 2481.541255004017
INFO:root:eval perplexity: 7.4491472244262695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2927.4125539360316
INFO:root:eval perplexity: 11.097387313842773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/7
  4%|â–Ž         | 7/200 [1:10:56<32:35:21, 607.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2627.249321831597
INFO:root:current train perplexity7.914529800415039
INFO:root:current mean train loss 2604.689796577066
INFO:root:current train perplexity7.882279872894287
INFO:root:current mean train loss 2602.030655327193
INFO:root:current train perplexity7.84952449798584
INFO:root:current mean train loss 2616.6646367678854
INFO:root:current train perplexity7.847879409790039
INFO:root:current mean train loss 2620.6003231066834
INFO:root:current train perplexity7.837555885314941
INFO:root:current mean train loss 2609.4404344006393
INFO:root:current train perplexity7.823796272277832
INFO:root:current mean train loss 2614.561539634532
INFO:root:current train perplexity7.840482711791992
INFO:root:current mean train loss 2614.0364075557104
INFO:root:current train perplexity7.825372695922852
INFO:root:current mean train loss 2609.054902689964
INFO:root:current train perplexity7.8104166984558105
INFO:root:current mean train loss 2608.072030260672
INFO:root:current train perplexity7.809932708740234
INFO:root:current mean train loss 2606.44972789592
INFO:root:current train perplexity7.80807638168335
INFO:root:current mean train loss 2608.0642354074657
INFO:root:current train perplexity7.804571628570557
INFO:root:current mean train loss 2605.3500529572684
INFO:root:current train perplexity7.795228958129883
INFO:root:current mean train loss 2604.2672167301903
INFO:root:current train perplexity7.797717571258545
INFO:root:current mean train loss 2604.2456016809547
INFO:root:current train perplexity7.791830539703369
INFO:root:current mean train loss 2602.189123744236
INFO:root:current train perplexity7.77703857421875
INFO:root:current mean train loss 2599.705966341186
INFO:root:current train perplexity7.770142078399658
INFO:root:current mean train loss 2599.188048605869
INFO:root:current train perplexity7.771196365356445
INFO:root:current mean train loss 2598.498161223593
INFO:root:current train perplexity7.764399528503418
INFO:root:current mean train loss 2597.36291153861
INFO:root:current train perplexity7.760072708129883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.57s/it]
INFO:root:final mean train loss: 2595.9201318507116
INFO:root:final train perplexity: 7.757767677307129
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2442.230401221742
INFO:root:eval perplexity: 7.215914249420166
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2897.486367516484
INFO:root:eval perplexity: 10.827690124511719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/8
  4%|â–         | 8/200 [1:21:04<32:25:03, 607.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2553.947412109375
INFO:root:current train perplexity7.522035598754883
INFO:root:current mean train loss 2567.3997061270256
INFO:root:current train perplexity7.551717281341553
INFO:root:current mean train loss 2562.7858424409906
INFO:root:current train perplexity7.511548042297363
INFO:root:current mean train loss 2554.2893427151353
INFO:root:current train perplexity7.469202518463135
INFO:root:current mean train loss 2556.0973787154276
INFO:root:current train perplexity7.484755992889404
INFO:root:current mean train loss 2556.650225202614
INFO:root:current train perplexity7.496496200561523
INFO:root:current mean train loss 2553.391766693836
INFO:root:current train perplexity7.480270862579346
INFO:root:current mean train loss 2553.088852738361
INFO:root:current train perplexity7.4833784103393555
INFO:root:current mean train loss 2551.0392012361995
INFO:root:current train perplexity7.479291915893555
INFO:root:current mean train loss 2550.7325570009607
INFO:root:current train perplexity7.480308532714844
INFO:root:current mean train loss 2551.8624182659646
INFO:root:current train perplexity7.486640453338623
INFO:root:current mean train loss 2551.1268560065046
INFO:root:current train perplexity7.482704162597656
INFO:root:current mean train loss 2550.1193824922507
INFO:root:current train perplexity7.48652458190918
INFO:root:current mean train loss 2547.830625566918
INFO:root:current train perplexity7.474093914031982
INFO:root:current mean train loss 2546.296066443679
INFO:root:current train perplexity7.465289115905762
INFO:root:current mean train loss 2546.808416012444
INFO:root:current train perplexity7.465073585510254
INFO:root:current mean train loss 2545.4741859739343
INFO:root:current train perplexity7.4575934410095215
INFO:root:current mean train loss 2544.5073859223025
INFO:root:current train perplexity7.450262546539307
INFO:root:current mean train loss 2544.664211179645
INFO:root:current train perplexity7.450161457061768
INFO:root:current mean train loss 2545.020914902798
INFO:root:current train perplexity7.4495344161987305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.05s/it]
INFO:root:final mean train loss: 2544.141462132718
INFO:root:final train perplexity: 7.447147846221924
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.50s/it]
INFO:root:eval mean loss: 2411.166046791888
INFO:root:eval perplexity: 7.036782264709473
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2868.895316049562
INFO:root:eval perplexity: 10.576146125793457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/9
  4%|â–         | 9/200 [1:31:11<32:14:13, 607.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2485.0296443058896
INFO:root:current train perplexity7.135040283203125
INFO:root:current mean train loss 2503.3344308953538
INFO:root:current train perplexity7.237983226776123
INFO:root:current mean train loss 2494.4461253332715
INFO:root:current train perplexity7.183870792388916
INFO:root:current mean train loss 2503.6875024275346
INFO:root:current train perplexity7.217507839202881
INFO:root:current mean train loss 2504.256149697093
INFO:root:current train perplexity7.229965686798096
INFO:root:current mean train loss 2504.1477919868803
INFO:root:current train perplexity7.233778953552246
INFO:root:current mean train loss 2512.494083896005
INFO:root:current train perplexity7.250576496124268
INFO:root:current mean train loss 2509.479373850721
INFO:root:current train perplexity7.244719505310059
INFO:root:current mean train loss 2508.8568398918906
INFO:root:current train perplexity7.241265773773193
INFO:root:current mean train loss 2507.2195960293298
INFO:root:current train perplexity7.240416526794434
INFO:root:current mean train loss 2509.6721992057537
INFO:root:current train perplexity7.249469757080078
INFO:root:current mean train loss 2507.4100941552056
INFO:root:current train perplexity7.2318854331970215
INFO:root:current mean train loss 2504.9697989076853
INFO:root:current train perplexity7.22005033493042
INFO:root:current mean train loss 2502.5610439142533
INFO:root:current train perplexity7.2149834632873535
INFO:root:current mean train loss 2503.4979216940806
INFO:root:current train perplexity7.216882228851318
INFO:root:current mean train loss 2503.2011401776185
INFO:root:current train perplexity7.211857795715332
INFO:root:current mean train loss 2502.9805504750398
INFO:root:current train perplexity7.207586288452148
INFO:root:current mean train loss 2502.16492869756
INFO:root:current train perplexity7.200501441955566
INFO:root:current mean train loss 2501.496826831002
INFO:root:current train perplexity7.197023391723633
INFO:root:current mean train loss 2501.143756553775
INFO:root:current train perplexity7.193859100341797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.62s/it]
INFO:root:final mean train loss: 2500.7276467897045
INFO:root:final train perplexity: 7.19631290435791
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2381.1324995324967
INFO:root:eval perplexity: 6.867825031280518
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2840.950918211159
INFO:root:eval perplexity: 10.335939407348633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/10
  5%|â–Œ         | 10/200 [1:41:19<32:04:17, 607.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2424.704451851223
INFO:root:current train perplexity6.892782688140869
INFO:root:current mean train loss 2450.236351238905
INFO:root:current train perplexity6.971449375152588
INFO:root:current mean train loss 2460.260044253892
INFO:root:current train perplexity7.019280433654785
INFO:root:current mean train loss 2465.0047359311484
INFO:root:current train perplexity7.0056915283203125
INFO:root:current mean train loss 2465.3181490704956
INFO:root:current train perplexity6.997865200042725
INFO:root:current mean train loss 2462.1248150709853
INFO:root:current train perplexity6.995660781860352
INFO:root:current mean train loss 2465.0444197262705
INFO:root:current train perplexity7.0013251304626465
INFO:root:current mean train loss 2466.2136843201397
INFO:root:current train perplexity7.007228374481201
INFO:root:current mean train loss 2465.828394425615
INFO:root:current train perplexity7.008098602294922
INFO:root:current mean train loss 2464.9819255313146
INFO:root:current train perplexity7.001274585723877
INFO:root:current mean train loss 2467.058577078096
INFO:root:current train perplexity7.010104656219482
INFO:root:current mean train loss 2466.359288955571
INFO:root:current train perplexity7.007771015167236
INFO:root:current mean train loss 2465.0502491042407
INFO:root:current train perplexity7.0022430419921875
INFO:root:current mean train loss 2466.6572870180676
INFO:root:current train perplexity7.007226467132568
INFO:root:current mean train loss 2465.150766890742
INFO:root:current train perplexity6.999549865722656
INFO:root:current mean train loss 2462.9328054667585
INFO:root:current train perplexity6.9940080642700195
INFO:root:current mean train loss 2462.472567385153
INFO:root:current train perplexity6.985748767852783
INFO:root:current mean train loss 2464.5777920496
INFO:root:current train perplexity6.986297130584717
INFO:root:current mean train loss 2464.3526011100103
INFO:root:current train perplexity6.984614849090576
INFO:root:current mean train loss 2463.8214738728616
INFO:root:current train perplexity6.985913276672363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.22s/it]
INFO:root:final mean train loss: 2463.1557816636723
INFO:root:final train perplexity: 6.986063003540039
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2358.8366491439497
INFO:root:eval perplexity: 6.7450270652771
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2819.378056086547
INFO:root:eval perplexity: 10.154239654541016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/11
  6%|â–Œ         | 11/200 [1:51:27<31:54:55, 607.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2416.4228231740553
INFO:root:current train perplexity6.725497722625732
INFO:root:current mean train loss 2426.395061533938
INFO:root:current train perplexity6.768174171447754
INFO:root:current mean train loss 2424.9704828862546
INFO:root:current train perplexity6.771210670471191
INFO:root:current mean train loss 2433.838305537565
INFO:root:current train perplexity6.783005237579346
INFO:root:current mean train loss 2430.2566847310636
INFO:root:current train perplexity6.775450706481934
INFO:root:current mean train loss 2432.409024912343
INFO:root:current train perplexity6.782084941864014
INFO:root:current mean train loss 2434.9275816981367
INFO:root:current train perplexity6.794602870941162
INFO:root:current mean train loss 2433.59350896549
INFO:root:current train perplexity6.798181056976318
INFO:root:current mean train loss 2434.157738403596
INFO:root:current train perplexity6.79710054397583
INFO:root:current mean train loss 2432.6274776806936
INFO:root:current train perplexity6.797146797180176
INFO:root:current mean train loss 2431.8373737932347
INFO:root:current train perplexity6.791932106018066
INFO:root:current mean train loss 2432.607757619822
INFO:root:current train perplexity6.794361114501953
INFO:root:current mean train loss 2433.7935051569484
INFO:root:current train perplexity6.7979207038879395
INFO:root:current mean train loss 2432.5387026303774
INFO:root:current train perplexity6.79758358001709
INFO:root:current mean train loss 2431.805606888249
INFO:root:current train perplexity6.79976224899292
INFO:root:current mean train loss 2432.1407376802886
INFO:root:current train perplexity6.801380157470703
INFO:root:current mean train loss 2430.3416089185757
INFO:root:current train perplexity6.8019700050354
INFO:root:current mean train loss 2430.2219377712067
INFO:root:current train perplexity6.798940181732178
INFO:root:current mean train loss 2430.396563338829
INFO:root:current train perplexity6.799520015716553

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.26s/it]
INFO:root:final mean train loss: 2428.7998956708193
INFO:root:final train perplexity: 6.799190044403076
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2340.2200057658742
INFO:root:eval perplexity: 6.644173622131348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2803.612464850676
INFO:root:eval perplexity: 10.023478507995605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/12
  6%|â–Œ         | 12/200 [2:01:36<31:45:22, 608.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2342.8333333333335
INFO:root:current train perplexity6.451570510864258
INFO:root:current mean train loss 2406.3135666337985
INFO:root:current train perplexity6.705774307250977
INFO:root:current mean train loss 2398.106044825662
INFO:root:current train perplexity6.634964942932129
INFO:root:current mean train loss 2387.6198057671977
INFO:root:current train perplexity6.627915382385254
INFO:root:current mean train loss 2388.6913078062
INFO:root:current train perplexity6.609649181365967
INFO:root:current mean train loss 2390.0818618562066
INFO:root:current train perplexity6.619261264801025
INFO:root:current mean train loss 2388.59064581066
INFO:root:current train perplexity6.622164249420166
INFO:root:current mean train loss 2391.4100090016004
INFO:root:current train perplexity6.62984037399292
INFO:root:current mean train loss 2394.3963086423955
INFO:root:current train perplexity6.630119323730469
INFO:root:current mean train loss 2397.038968142217
INFO:root:current train perplexity6.641576766967773
INFO:root:current mean train loss 2398.7181228531203
INFO:root:current train perplexity6.636094570159912
INFO:root:current mean train loss 2398.0609768325376
INFO:root:current train perplexity6.6336259841918945
INFO:root:current mean train loss 2399.331184672596
INFO:root:current train perplexity6.6417646408081055
INFO:root:current mean train loss 2400.3145810037236
INFO:root:current train perplexity6.643560409545898
INFO:root:current mean train loss 2400.305812408817
INFO:root:current train perplexity6.637150287628174
INFO:root:current mean train loss 2399.500472200131
INFO:root:current train perplexity6.639494895935059
INFO:root:current mean train loss 2398.8043764225035
INFO:root:current train perplexity6.637526035308838
INFO:root:current mean train loss 2399.008807627509
INFO:root:current train perplexity6.639284133911133
INFO:root:current mean train loss 2397.7528347664916
INFO:root:current train perplexity6.637216567993164
INFO:root:current mean train loss 2399.308400349058
INFO:root:current train perplexity6.641659736633301

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.19s/it]
INFO:root:final mean train loss: 2398.7103227162324
INFO:root:final train perplexity: 6.63963508605957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2323.0286471319537
INFO:root:eval perplexity: 6.552383899688721
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2794.0725989375555
INFO:root:eval perplexity: 9.945172309875488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/13
  6%|â–‹         | 13/200 [2:11:44<31:35:28, 608.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2357.8093139648436
INFO:root:current train perplexity6.296701431274414
INFO:root:current mean train loss 2396.55849609375
INFO:root:current train perplexity6.5619587898254395
INFO:root:current mean train loss 2375.6150068803267
INFO:root:current train perplexity6.487606525421143
INFO:root:current mean train loss 2372.849167251587
INFO:root:current train perplexity6.491406440734863
INFO:root:current mean train loss 2381.535799734933
INFO:root:current train perplexity6.50335693359375
INFO:root:current mean train loss 2378.2646329439604
INFO:root:current train perplexity6.505434989929199
INFO:root:current mean train loss 2371.9036583685106
INFO:root:current train perplexity6.4953789710998535
INFO:root:current mean train loss 2371.332999674479
INFO:root:current train perplexity6.495635986328125
INFO:root:current mean train loss 2371.0151146865473
INFO:root:current train perplexity6.500508785247803
INFO:root:current mean train loss 2371.061152980639
INFO:root:current train perplexity6.506042957305908
INFO:root:current mean train loss 2370.136101576861
INFO:root:current train perplexity6.508533000946045
INFO:root:current mean train loss 2371.1257665361677
INFO:root:current train perplexity6.511092662811279
INFO:root:current mean train loss 2372.451448234183
INFO:root:current train perplexity6.512032508850098
INFO:root:current mean train loss 2373.525407548384
INFO:root:current train perplexity6.510554790496826
INFO:root:current mean train loss 2371.5169311523437
INFO:root:current train perplexity6.505484104156494
INFO:root:current mean train loss 2371.766526553505
INFO:root:current train perplexity6.50783634185791
INFO:root:current mean train loss 2372.4446886604214
INFO:root:current train perplexity6.509246349334717
INFO:root:current mean train loss 2372.386161342887
INFO:root:current train perplexity6.5058088302612305
INFO:root:current mean train loss 2371.5663061791724
INFO:root:current train perplexity6.502662181854248
INFO:root:current mean train loss 2371.033251253764
INFO:root:current train perplexity6.498070240020752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.61s/it]
INFO:root:final mean train loss: 2371.686228480414
INFO:root:final train perplexity: 6.4995293617248535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2305.4978308711493
INFO:root:eval perplexity: 6.460086822509766
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2774.4076131184897
INFO:root:eval perplexity: 9.78568172454834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/14
  7%|â–‹         | 14/200 [2:21:51<31:24:02, 607.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2371.525064004434
INFO:root:current train perplexity6.3283562660217285
INFO:root:current mean train loss 2357.278709690066
INFO:root:current train perplexity6.316923141479492
INFO:root:current mean train loss 2359.2187005537976
INFO:root:current train perplexity6.354763984680176
INFO:root:current mean train loss 2350.834704481176
INFO:root:current train perplexity6.343629360198975
INFO:root:current mean train loss 2348.947664518378
INFO:root:current train perplexity6.365307331085205
INFO:root:current mean train loss 2351.135639893942
INFO:root:current train perplexity6.36502742767334
INFO:root:current mean train loss 2352.918727617249
INFO:root:current train perplexity6.371827125549316
INFO:root:current mean train loss 2354.7986318518383
INFO:root:current train perplexity6.375812530517578
INFO:root:current mean train loss 2350.601469889906
INFO:root:current train perplexity6.374428749084473
INFO:root:current mean train loss 2351.8132884413353
INFO:root:current train perplexity6.38018274307251
INFO:root:current mean train loss 2353.523605479109
INFO:root:current train perplexity6.3855180740356445
INFO:root:current mean train loss 2352.623532901653
INFO:root:current train perplexity6.385286808013916
INFO:root:current mean train loss 2352.487037751206
INFO:root:current train perplexity6.38355016708374
INFO:root:current mean train loss 2351.9118811208627
INFO:root:current train perplexity6.386775970458984
INFO:root:current mean train loss 2349.683778342059
INFO:root:current train perplexity6.380600452423096
INFO:root:current mean train loss 2350.187819828984
INFO:root:current train perplexity6.381882667541504
INFO:root:current mean train loss 2350.2615878804836
INFO:root:current train perplexity6.381153106689453
INFO:root:current mean train loss 2347.96179395993
INFO:root:current train perplexity6.372771263122559
INFO:root:current mean train loss 2348.0819806808613
INFO:root:current train perplexity6.373347282409668
INFO:root:current mean train loss 2347.798300456065
INFO:root:current train perplexity6.376849174499512

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.07s/it]
INFO:root:final mean train loss: 2347.249744563408
INFO:root:final train perplexity: 6.3753838539123535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it]
INFO:root:eval mean loss: 2295.7022774337875
INFO:root:eval perplexity: 6.409082412719727
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2773.8494825430794
INFO:root:eval perplexity: 9.7811918258667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/15
  8%|â–Š         | 15/200 [2:31:58<31:13:27, 607.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2281.0336100260415
INFO:root:current train perplexity6.129024505615234
INFO:root:current mean train loss 2301.4094753513086
INFO:root:current train perplexity6.202103137969971
INFO:root:current mean train loss 2308.504849651667
INFO:root:current train perplexity6.237321853637695
INFO:root:current mean train loss 2319.651401670639
INFO:root:current train perplexity6.256252288818359
INFO:root:current mean train loss 2319.238346049439
INFO:root:current train perplexity6.238607406616211
INFO:root:current mean train loss 2319.1692872856497
INFO:root:current train perplexity6.244570732116699
INFO:root:current mean train loss 2317.603964336057
INFO:root:current train perplexity6.24122953414917
INFO:root:current mean train loss 2318.1065767728364
INFO:root:current train perplexity6.244434356689453
INFO:root:current mean train loss 2319.3720956127872
INFO:root:current train perplexity6.252236843109131
INFO:root:current mean train loss 2324.823314482811
INFO:root:current train perplexity6.256070137023926
INFO:root:current mean train loss 2324.9272502631343
INFO:root:current train perplexity6.26434850692749
INFO:root:current mean train loss 2326.3245584101155
INFO:root:current train perplexity6.271833419799805
INFO:root:current mean train loss 2324.6626378596306
INFO:root:current train perplexity6.267704963684082
INFO:root:current mean train loss 2325.6132203951543
INFO:root:current train perplexity6.264784812927246
INFO:root:current mean train loss 2326.8106045519635
INFO:root:current train perplexity6.264530181884766
INFO:root:current mean train loss 2326.7812359391337
INFO:root:current train perplexity6.264778137207031
INFO:root:current mean train loss 2326.7453332829505
INFO:root:current train perplexity6.265603542327881
INFO:root:current mean train loss 2326.4813743947984
INFO:root:current train perplexity6.2657012939453125
INFO:root:current mean train loss 2326.7032418029853
INFO:root:current train perplexity6.2630934715271
INFO:root:current mean train loss 2324.8029124827076
INFO:root:current train perplexity6.2586236000061035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.78s/it]
INFO:root:final mean train loss: 2323.550429689962
INFO:root:final train perplexity: 6.257250785827637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2282.934480274823
INFO:root:eval perplexity: 6.34320592880249
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2760.4522129356437
INFO:root:eval perplexity: 9.674050331115723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/16
  8%|â–Š         | 16/200 [2:42:06<31:03:37, 607.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2317.0531384105416
INFO:root:current train perplexity6.225472927093506
INFO:root:current mean train loss 2312.738910161961
INFO:root:current train perplexity6.224736213684082
INFO:root:current mean train loss 2305.471804460476
INFO:root:current train perplexity6.180158615112305
INFO:root:current mean train loss 2301.2424990918757
INFO:root:current train perplexity6.177885055541992
INFO:root:current mean train loss 2296.5078645937
INFO:root:current train perplexity6.150110244750977
INFO:root:current mean train loss 2297.6009846435118
INFO:root:current train perplexity6.150165557861328
INFO:root:current mean train loss 2295.780375496286
INFO:root:current train perplexity6.133785247802734
INFO:root:current mean train loss 2297.05208064177
INFO:root:current train perplexity6.143264293670654
INFO:root:current mean train loss 2295.015468452883
INFO:root:current train perplexity6.140605926513672
INFO:root:current mean train loss 2296.3841122785384
INFO:root:current train perplexity6.135292053222656
INFO:root:current mean train loss 2298.3715565002044
INFO:root:current train perplexity6.142360210418701
INFO:root:current mean train loss 2298.01256636205
INFO:root:current train perplexity6.140885353088379
INFO:root:current mean train loss 2298.474464254432
INFO:root:current train perplexity6.14506196975708
INFO:root:current mean train loss 2297.082616403971
INFO:root:current train perplexity6.140477657318115
INFO:root:current mean train loss 2300.374666650955
INFO:root:current train perplexity6.1460347175598145
INFO:root:current mean train loss 2300.0179585554583
INFO:root:current train perplexity6.1504998207092285
INFO:root:current mean train loss 2301.8616914138474
INFO:root:current train perplexity6.1525468826293945
INFO:root:current mean train loss 2301.016671868383
INFO:root:current train perplexity6.150476455688477
INFO:root:current mean train loss 2302.3978360215333
INFO:root:current train perplexity6.155674457550049
INFO:root:current mean train loss 2303.62008318618
INFO:root:current train perplexity6.15610408782959

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.21s/it]
INFO:root:final mean train loss: 2302.8635268372473
INFO:root:final train perplexity: 6.155923366546631
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2273.23716660087
INFO:root:eval perplexity: 6.2936224937438965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2752.906886756843
INFO:root:eval perplexity: 9.614224433898926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/17
  8%|â–Š         | 17/200 [2:52:15<30:54:09, 607.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2261.201041481712
INFO:root:current train perplexity5.949254035949707
INFO:root:current mean train loss 2264.9553735611285
INFO:root:current train perplexity5.98551607131958
INFO:root:current mean train loss 2270.5824008517793
INFO:root:current train perplexity6.003862380981445
INFO:root:current mean train loss 2273.9644533137684
INFO:root:current train perplexity6.022774696350098
INFO:root:current mean train loss 2275.6072490254387
INFO:root:current train perplexity6.032742500305176
INFO:root:current mean train loss 2276.6930631261293
INFO:root:current train perplexity6.050405025482178
INFO:root:current mean train loss 2274.654401734818
INFO:root:current train perplexity6.049227237701416
INFO:root:current mean train loss 2273.598289528474
INFO:root:current train perplexity6.046966552734375
INFO:root:current mean train loss 2274.4331271884676
INFO:root:current train perplexity6.044940948486328
INFO:root:current mean train loss 2278.3128947516684
INFO:root:current train perplexity6.051909923553467
INFO:root:current mean train loss 2276.6116169200222
INFO:root:current train perplexity6.04226016998291
INFO:root:current mean train loss 2276.443875707761
INFO:root:current train perplexity6.041345596313477
INFO:root:current mean train loss 2279.1864632553197
INFO:root:current train perplexity6.045287609100342
INFO:root:current mean train loss 2277.7950350626743
INFO:root:current train perplexity6.037189483642578
INFO:root:current mean train loss 2278.148813637354
INFO:root:current train perplexity6.040714740753174
INFO:root:current mean train loss 2280.399660739851
INFO:root:current train perplexity6.046262741088867
INFO:root:current mean train loss 2280.4431630356053
INFO:root:current train perplexity6.04919958114624
INFO:root:current mean train loss 2282.385425473753
INFO:root:current train perplexity6.054253578186035
INFO:root:current mean train loss 2283.0352587295793
INFO:root:current train perplexity6.055210590362549

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.39s/it]
INFO:root:final mean train loss: 2282.354158602516
INFO:root:final train perplexity: 6.057086944580078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2264.3659460189497
INFO:root:eval perplexity: 6.248605251312256
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2745.8623821718475
INFO:root:eval perplexity: 9.55870532989502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/18
  9%|â–‰         | 18/200 [3:02:23<30:44:39, 608.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2384.150927734375
INFO:root:current train perplexity6.006631851196289
INFO:root:current mean train loss 2264.0309477306546
INFO:root:current train perplexity5.948124408721924
INFO:root:current mean train loss 2269.9871111613948
INFO:root:current train perplexity5.939081192016602
INFO:root:current mean train loss 2263.1427122022283
INFO:root:current train perplexity5.946008205413818
INFO:root:current mean train loss 2259.9014084804207
INFO:root:current train perplexity5.940779209136963
INFO:root:current mean train loss 2268.0095707959467
INFO:root:current train perplexity5.9505391120910645
INFO:root:current mean train loss 2267.516878389721
INFO:root:current train perplexity5.956925392150879
INFO:root:current mean train loss 2267.1423611688274
INFO:root:current train perplexity5.954689025878906
INFO:root:current mean train loss 2266.5223453877134
INFO:root:current train perplexity5.959206581115723
INFO:root:current mean train loss 2265.590717395761
INFO:root:current train perplexity5.961376190185547
INFO:root:current mean train loss 2267.272077721743
INFO:root:current train perplexity5.965447425842285
INFO:root:current mean train loss 2264.666520918764
INFO:root:current train perplexity5.963303565979004
INFO:root:current mean train loss 2266.470399823327
INFO:root:current train perplexity5.971691608428955
INFO:root:current mean train loss 2267.9645823043884
INFO:root:current train perplexity5.976316452026367
INFO:root:current mean train loss 2268.3012869946897
INFO:root:current train perplexity5.974434852600098
INFO:root:current mean train loss 2268.4993177851175
INFO:root:current train perplexity5.973598957061768
INFO:root:current mean train loss 2267.9733352043177
INFO:root:current train perplexity5.974802017211914
INFO:root:current mean train loss 2268.29914443388
INFO:root:current train perplexity5.9759955406188965
INFO:root:current mean train loss 2266.9299755994634
INFO:root:current train perplexity5.976690292358398
INFO:root:current mean train loss 2264.3309974650388
INFO:root:current train perplexity5.971517086029053

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.72s/it]
INFO:root:final mean train loss: 2264.281393985104
INFO:root:final train perplexity: 5.971307754516602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2256.8854699101007
INFO:root:eval perplexity: 6.210894584655762
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2738.384136659879
INFO:root:eval perplexity: 9.500117301940918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/19
 10%|â–‰         | 19/200 [3:12:31<30:34:15, 608.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2222.5348288796163
INFO:root:current train perplexity5.804598331451416
INFO:root:current mean train loss 2240.5546264648438
INFO:root:current train perplexity5.869421005249023
INFO:root:current mean train loss 2244.343901213225
INFO:root:current train perplexity5.894245624542236
INFO:root:current mean train loss 2234.185440347802
INFO:root:current train perplexity5.870649814605713
INFO:root:current mean train loss 2236.625590970731
INFO:root:current train perplexity5.8704047203063965
INFO:root:current mean train loss 2237.8152631854646
INFO:root:current train perplexity5.8728508949279785
INFO:root:current mean train loss 2238.356268879685
INFO:root:current train perplexity5.8694167137146
INFO:root:current mean train loss 2242.508577891004
INFO:root:current train perplexity5.86997652053833
INFO:root:current mean train loss 2242.085912848331
INFO:root:current train perplexity5.867669582366943
INFO:root:current mean train loss 2244.4399346539876
INFO:root:current train perplexity5.872894287109375
INFO:root:current mean train loss 2245.6458451183344
INFO:root:current train perplexity5.872147560119629
INFO:root:current mean train loss 2244.4043598685034
INFO:root:current train perplexity5.874573707580566
INFO:root:current mean train loss 2244.094829652977
INFO:root:current train perplexity5.871054649353027
INFO:root:current mean train loss 2241.9281781495247
INFO:root:current train perplexity5.8703484535217285
INFO:root:current mean train loss 2244.8710952093497
INFO:root:current train perplexity5.876261234283447
INFO:root:current mean train loss 2244.471250596717
INFO:root:current train perplexity5.875857830047607
INFO:root:current mean train loss 2244.661827604809
INFO:root:current train perplexity5.878683090209961
INFO:root:current mean train loss 2246.4070840762465
INFO:root:current train perplexity5.88232946395874
INFO:root:current mean train loss 2246.348291323816
INFO:root:current train perplexity5.883173942565918
INFO:root:current mean train loss 2245.2379819808466
INFO:root:current train perplexity5.884061336517334

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it]
INFO:root:final mean train loss: 2246.53441668735
INFO:root:final train perplexity: 5.888256549835205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2244.4622685858544
INFO:root:eval perplexity: 6.148770332336426
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2732.067287147468
INFO:root:eval perplexity: 9.450906753540039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/20
 10%|â–ˆ         | 20/200 [3:22:39<30:23:38, 607.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2262.302562224559
INFO:root:current train perplexity5.8516058921813965
INFO:root:current mean train loss 2248.7607413092965
INFO:root:current train perplexity5.803581237792969
INFO:root:current mean train loss 2236.7706855550473
INFO:root:current train perplexity5.795525550842285
INFO:root:current mean train loss 2231.734994353798
INFO:root:current train perplexity5.800223350524902
INFO:root:current mean train loss 2230.216794650484
INFO:root:current train perplexity5.803918838500977
INFO:root:current mean train loss 2229.4823527999624
INFO:root:current train perplexity5.801341533660889
INFO:root:current mean train loss 2230.0152994791665
INFO:root:current train perplexity5.793395042419434
INFO:root:current mean train loss 2229.5086197498204
INFO:root:current train perplexity5.804561614990234
INFO:root:current mean train loss 2229.8733004728006
INFO:root:current train perplexity5.808376789093018
INFO:root:current mean train loss 2229.051772632538
INFO:root:current train perplexity5.806642532348633
INFO:root:current mean train loss 2229.882597848931
INFO:root:current train perplexity5.806839466094971
INFO:root:current mean train loss 2228.202805945285
INFO:root:current train perplexity5.799596786499023
INFO:root:current mean train loss 2228.8419809164398
INFO:root:current train perplexity5.801207542419434
INFO:root:current mean train loss 2229.8853769793805
INFO:root:current train perplexity5.805516719818115
INFO:root:current mean train loss 2230.5745965826422
INFO:root:current train perplexity5.808437824249268
INFO:root:current mean train loss 2231.1515136877388
INFO:root:current train perplexity5.807240009307861
INFO:root:current mean train loss 2231.2053288197358
INFO:root:current train perplexity5.8052287101745605
INFO:root:current mean train loss 2230.306460713435
INFO:root:current train perplexity5.806640625
INFO:root:current mean train loss 2230.302419541127
INFO:root:current train perplexity5.808746814727783
INFO:root:current mean train loss 2230.1979377357047
INFO:root:current train perplexity5.810481548309326

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.38s/it]
INFO:root:final mean train loss: 2229.713113404859
INFO:root:final train perplexity: 5.810605049133301
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2238.306489119293
INFO:root:eval perplexity: 6.118216037750244
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2724.6353712149544
INFO:root:eval perplexity: 9.393338203430176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/21
 10%|â–ˆ         | 21/200 [3:32:47<30:14:08, 608.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2213.436837332589
INFO:root:current train perplexity5.761147499084473
INFO:root:current mean train loss 2228.196942451673
INFO:root:current train perplexity5.790475845336914
INFO:root:current mean train loss 2213.106120109558
INFO:root:current train perplexity5.772886276245117
INFO:root:current mean train loss 2212.772050493219
INFO:root:current train perplexity5.7716145515441895
INFO:root:current mean train loss 2208.9478050365783
INFO:root:current train perplexity5.751763820648193
INFO:root:current mean train loss 2209.5630578788923
INFO:root:current train perplexity5.741304874420166
INFO:root:current mean train loss 2210.6285489710367
INFO:root:current train perplexity5.7395501136779785
INFO:root:current mean train loss 2210.7354109829694
INFO:root:current train perplexity5.739542007446289
INFO:root:current mean train loss 2210.919191734813
INFO:root:current train perplexity5.7419610023498535
INFO:root:current mean train loss 2209.664146263729
INFO:root:current train perplexity5.742391586303711
INFO:root:current mean train loss 2213.536392558705
INFO:root:current train perplexity5.7499260902404785
INFO:root:current mean train loss 2212.6354734427377
INFO:root:current train perplexity5.741672515869141
INFO:root:current mean train loss 2212.1345593883734
INFO:root:current train perplexity5.735745429992676
INFO:root:current mean train loss 2212.8829016221307
INFO:root:current train perplexity5.736089706420898
INFO:root:current mean train loss 2214.2786501370942
INFO:root:current train perplexity5.740758895874023
INFO:root:current mean train loss 2215.090126017985
INFO:root:current train perplexity5.7417402267456055
INFO:root:current mean train loss 2214.2729115509183
INFO:root:current train perplexity5.739596843719482
INFO:root:current mean train loss 2214.115820256887
INFO:root:current train perplexity5.740574836730957
INFO:root:current mean train loss 2214.8713392718087
INFO:root:current train perplexity5.74119758605957
INFO:root:current mean train loss 2214.657672780667
INFO:root:current train perplexity5.740921497344971

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.01s/it]
INFO:root:final mean train loss: 2214.178994690957
INFO:root:final train perplexity: 5.739804744720459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2233.880551601978
INFO:root:eval perplexity: 6.096341609954834
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2725.08935676737
INFO:root:eval perplexity: 9.396843910217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/22
 11%|â–ˆ         | 22/200 [3:42:56<30:05:07, 608.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2213.6917222950556
INFO:root:current train perplexity5.723869800567627
INFO:root:current mean train loss 2204.3312508467307
INFO:root:current train perplexity5.696270942687988
INFO:root:current mean train loss 2203.573887863439
INFO:root:current train perplexity5.668175220489502
INFO:root:current mean train loss 2197.331536750691
INFO:root:current train perplexity5.657472133636475
INFO:root:current mean train loss 2196.363826824293
INFO:root:current train perplexity5.65049934387207
INFO:root:current mean train loss 2195.1689229435947
INFO:root:current train perplexity5.651297092437744
INFO:root:current mean train loss 2194.4336895198735
INFO:root:current train perplexity5.649662971496582
INFO:root:current mean train loss 2194.230308779461
INFO:root:current train perplexity5.647388458251953
INFO:root:current mean train loss 2195.5751284744597
INFO:root:current train perplexity5.653203964233398
INFO:root:current mean train loss 2197.3521507710125
INFO:root:current train perplexity5.663349151611328
INFO:root:current mean train loss 2196.3698804416285
INFO:root:current train perplexity5.6625657081604
INFO:root:current mean train loss 2195.4901214625625
INFO:root:current train perplexity5.665943622589111
INFO:root:current mean train loss 2197.3250942425007
INFO:root:current train perplexity5.668408393859863
INFO:root:current mean train loss 2197.524722305598
INFO:root:current train perplexity5.668447494506836
INFO:root:current mean train loss 2198.4680953948414
INFO:root:current train perplexity5.672494411468506
INFO:root:current mean train loss 2198.1831481506774
INFO:root:current train perplexity5.671609401702881
INFO:root:current mean train loss 2199.111370517619
INFO:root:current train perplexity5.674873352050781
INFO:root:current mean train loss 2199.7944465374717
INFO:root:current train perplexity5.675952434539795
INFO:root:current mean train loss 2200.0072149224798
INFO:root:current train perplexity5.675954341888428
INFO:root:current mean train loss 2199.5489592902622
INFO:root:current train perplexity5.670272350311279

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.96s/it]
INFO:root:final mean train loss: 2198.6945441280177
INFO:root:final train perplexity: 5.670089244842529
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2230.070645379682
INFO:root:eval perplexity: 6.077577114105225
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2725.6910621156085
INFO:root:eval perplexity: 9.401495933532715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/23
 12%|â–ˆâ–        | 23/200 [3:53:06<29:55:38, 608.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2162.1721367730033
INFO:root:current train perplexity5.535494804382324
INFO:root:current mean train loss 2167.7812821237662
INFO:root:current train perplexity5.556807994842529
INFO:root:current mean train loss 2168.0191115806842
INFO:root:current train perplexity5.568211555480957
INFO:root:current mean train loss 2177.145177283654
INFO:root:current train perplexity5.587935447692871
INFO:root:current mean train loss 2178.038248863999
INFO:root:current train perplexity5.581026554107666
INFO:root:current mean train loss 2180.4210503012446
INFO:root:current train perplexity5.582810878753662
INFO:root:current mean train loss 2181.6939796337183
INFO:root:current train perplexity5.586147308349609
INFO:root:current mean train loss 2183.0425765798063
INFO:root:current train perplexity5.583719730377197
INFO:root:current mean train loss 2184.3515659289415
INFO:root:current train perplexity5.586296558380127
INFO:root:current mean train loss 2182.98050450797
INFO:root:current train perplexity5.581158638000488
INFO:root:current mean train loss 2182.982018259031
INFO:root:current train perplexity5.585593223571777
INFO:root:current mean train loss 2182.014721987428
INFO:root:current train perplexity5.586277961730957
INFO:root:current mean train loss 2182.116644807564
INFO:root:current train perplexity5.589796543121338
INFO:root:current mean train loss 2182.0165590574416
INFO:root:current train perplexity5.591357231140137
INFO:root:current mean train loss 2182.154428858405
INFO:root:current train perplexity5.597751617431641
INFO:root:current mean train loss 2183.9078961834216
INFO:root:current train perplexity5.59970235824585
INFO:root:current mean train loss 2184.449923074034
INFO:root:current train perplexity5.603384494781494
INFO:root:current mean train loss 2184.256720072997
INFO:root:current train perplexity5.604959011077881
INFO:root:current mean train loss 2183.8062873961435
INFO:root:current train perplexity5.603569984436035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it]
INFO:root:final mean train loss: 2184.735069501899
INFO:root:final train perplexity: 5.60796594619751
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2225.235951092226
INFO:root:eval perplexity: 6.0538458824157715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2721.801648728391
INFO:root:eval perplexity: 9.371482849121094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/24
 12%|â–ˆâ–        | 24/200 [4:03:14<29:44:43, 608.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2197.4211077008927
INFO:root:current train perplexity5.630028247833252
INFO:root:current mean train loss 2174.0075250073014
INFO:root:current train perplexity5.553430080413818
INFO:root:current mean train loss 2169.763281485885
INFO:root:current train perplexity5.553649425506592
INFO:root:current mean train loss 2177.0838579308324
INFO:root:current train perplexity5.549668312072754
INFO:root:current mean train loss 2176.526534846725
INFO:root:current train perplexity5.5527753829956055
INFO:root:current mean train loss 2172.7009084727874
INFO:root:current train perplexity5.548804759979248
INFO:root:current mean train loss 2171.032299362258
INFO:root:current train perplexity5.550719261169434
INFO:root:current mean train loss 2172.983943523747
INFO:root:current train perplexity5.552132606506348
INFO:root:current mean train loss 2173.7314214127364
INFO:root:current train perplexity5.5511369705200195
INFO:root:current mean train loss 2169.063186796918
INFO:root:current train perplexity5.544114589691162
INFO:root:current mean train loss 2169.6634111754825
INFO:root:current train perplexity5.5423054695129395
INFO:root:current mean train loss 2170.707368569861
INFO:root:current train perplexity5.5384745597839355
INFO:root:current mean train loss 2170.2469584568535
INFO:root:current train perplexity5.535489559173584
INFO:root:current mean train loss 2171.570893711595
INFO:root:current train perplexity5.539276599884033
INFO:root:current mean train loss 2172.9732357152575
INFO:root:current train perplexity5.544069290161133
INFO:root:current mean train loss 2173.9396648647457
INFO:root:current train perplexity5.546310901641846
INFO:root:current mean train loss 2173.1386596451803
INFO:root:current train perplexity5.548120498657227
INFO:root:current mean train loss 2172.647234960823
INFO:root:current train perplexity5.549768447875977
INFO:root:current mean train loss 2172.0325386625796
INFO:root:current train perplexity5.550826549530029
INFO:root:current mean train loss 2171.27901227898
INFO:root:current train perplexity5.550095081329346

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.77s/it]
INFO:root:final mean train loss: 2170.818806966146
INFO:root:final train perplexity: 5.546713352203369
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2216.345653777427
INFO:root:eval perplexity: 6.0104498863220215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 2712.5804001828456
INFO:root:eval perplexity: 9.300702095031738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/25
 12%|â–ˆâ–Ž        | 25/200 [4:13:22<29:34:14, 608.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2136.058191935221
INFO:root:current train perplexity5.377139091491699
INFO:root:current mean train loss 2128.2879934003277
INFO:root:current train perplexity5.398770809173584
INFO:root:current mean train loss 2132.5835849217005
INFO:root:current train perplexity5.413751125335693
INFO:root:current mean train loss 2145.14542379497
INFO:root:current train perplexity5.441911220550537
INFO:root:current mean train loss 2148.210192698353
INFO:root:current train perplexity5.466506481170654
INFO:root:current mean train loss 2149.3007267376847
INFO:root:current train perplexity5.470317363739014
INFO:root:current mean train loss 2149.997799604367
INFO:root:current train perplexity5.465209007263184
INFO:root:current mean train loss 2151.6263869480536
INFO:root:current train perplexity5.466906547546387
INFO:root:current mean train loss 2150.9045454599327
INFO:root:current train perplexity5.46659517288208
INFO:root:current mean train loss 2151.512767577068
INFO:root:current train perplexity5.469792366027832
INFO:root:current mean train loss 2154.4212886095047
INFO:root:current train perplexity5.479323387145996
INFO:root:current mean train loss 2155.016293020011
INFO:root:current train perplexity5.483480930328369
INFO:root:current mean train loss 2157.3503501742493
INFO:root:current train perplexity5.490931034088135
INFO:root:current mean train loss 2156.395353565043
INFO:root:current train perplexity5.486029624938965
INFO:root:current mean train loss 2158.6412548108046
INFO:root:current train perplexity5.4883599281311035
INFO:root:current mean train loss 2159.7261280450293
INFO:root:current train perplexity5.492117881774902
INFO:root:current mean train loss 2159.976759436095
INFO:root:current train perplexity5.489950656890869
INFO:root:current mean train loss 2160.262770933784
INFO:root:current train perplexity5.490839958190918
INFO:root:current mean train loss 2159.263426529734
INFO:root:current train perplexity5.490610122680664
INFO:root:current mean train loss 2158.030988284803
INFO:root:current train perplexity5.487964153289795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.20s/it]
INFO:root:final mean train loss: 2157.209114324788
INFO:root:final train perplexity: 5.48745584487915
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2217.3109420884584
INFO:root:eval perplexity: 6.015146255493164
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2715.0598205133533
INFO:root:eval perplexity: 9.319683074951172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/26
 13%|â–ˆâ–Ž        | 26/200 [4:23:31<29:25:06, 608.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2136.03369140625
INFO:root:current train perplexity5.460814476013184
INFO:root:current mean train loss 2136.4656237879544
INFO:root:current train perplexity5.412528038024902
INFO:root:current mean train loss 2139.553398923755
INFO:root:current train perplexity5.415728569030762
INFO:root:current mean train loss 2136.663406327323
INFO:root:current train perplexity5.397497177124023
INFO:root:current mean train loss 2137.876660266971
INFO:root:current train perplexity5.404650688171387
INFO:root:current mean train loss 2138.0310870891576
INFO:root:current train perplexity5.405029296875
INFO:root:current mean train loss 2138.018206948981
INFO:root:current train perplexity5.409334659576416
INFO:root:current mean train loss 2140.268647599317
INFO:root:current train perplexity5.418079853057861
INFO:root:current mean train loss 2142.135400361595
INFO:root:current train perplexity5.4241461753845215
INFO:root:current mean train loss 2141.2152994705184
INFO:root:current train perplexity5.418318748474121
INFO:root:current mean train loss 2142.2175968401025
INFO:root:current train perplexity5.424332618713379
INFO:root:current mean train loss 2142.220416725152
INFO:root:current train perplexity5.426797389984131
INFO:root:current mean train loss 2141.036074875825
INFO:root:current train perplexity5.421780109405518
INFO:root:current mean train loss 2141.706036208549
INFO:root:current train perplexity5.425760269165039
INFO:root:current mean train loss 2142.2280038784643
INFO:root:current train perplexity5.42714262008667
INFO:root:current mean train loss 2143.724705779653
INFO:root:current train perplexity5.430335998535156
INFO:root:current mean train loss 2145.1040330662513
INFO:root:current train perplexity5.434988498687744
INFO:root:current mean train loss 2145.0410128203976
INFO:root:current train perplexity5.433021068572998
INFO:root:current mean train loss 2144.5725704360952
INFO:root:current train perplexity5.430846214294434
INFO:root:current mean train loss 2145.1540249996983
INFO:root:current train perplexity5.433313846588135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.17s/it]
INFO:root:final mean train loss: 2144.6101102535617
INFO:root:final train perplexity: 5.433164119720459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2211.194928541251
INFO:root:eval perplexity: 5.985450267791748
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2712.318952844498
INFO:root:eval perplexity: 9.298704147338867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/27
 14%|â–ˆâ–Ž        | 27/200 [4:33:39<29:14:44, 608.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2140.2047455886313
INFO:root:current train perplexity5.3887248039245605
INFO:root:current mean train loss 2131.309318445906
INFO:root:current train perplexity5.373689651489258
INFO:root:current mean train loss 2138.273914425872
INFO:root:current train perplexity5.358969688415527
INFO:root:current mean train loss 2135.401769542161
INFO:root:current train perplexity5.3542704582214355
INFO:root:current mean train loss 2133.1374562359274
INFO:root:current train perplexity5.35497522354126
INFO:root:current mean train loss 2132.6364789846552
INFO:root:current train perplexity5.354598045349121
INFO:root:current mean train loss 2133.5682645757147
INFO:root:current train perplexity5.357022762298584
INFO:root:current mean train loss 2132.2653544483837
INFO:root:current train perplexity5.352099418640137
INFO:root:current mean train loss 2129.9160009708717
INFO:root:current train perplexity5.354985237121582
INFO:root:current mean train loss 2129.1551790177696
INFO:root:current train perplexity5.35788631439209
INFO:root:current mean train loss 2128.6707716366745
INFO:root:current train perplexity5.361040115356445
INFO:root:current mean train loss 2129.8285087097906
INFO:root:current train perplexity5.367321491241455
INFO:root:current mean train loss 2129.816634962024
INFO:root:current train perplexity5.37190580368042
INFO:root:current mean train loss 2131.108520687592
INFO:root:current train perplexity5.374252796173096
INFO:root:current mean train loss 2132.6871686184522
INFO:root:current train perplexity5.381204605102539
INFO:root:current mean train loss 2132.485796202438
INFO:root:current train perplexity5.377310752868652
INFO:root:current mean train loss 2133.691656059753
INFO:root:current train perplexity5.3785505294799805
INFO:root:current mean train loss 2133.05216874089
INFO:root:current train perplexity5.379491329193115
INFO:root:current mean train loss 2131.8240356445312
INFO:root:current train perplexity5.376706123352051
INFO:root:current mean train loss 2131.8608195818238
INFO:root:current train perplexity5.377718448638916

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.93s/it]
INFO:root:final mean train loss: 2131.5251507011258
INFO:root:final train perplexity: 5.377346515655518
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2208.1126254467254
INFO:root:eval perplexity: 5.970539093017578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2711.717074779754
INFO:root:eval perplexity: 9.294105529785156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/28
 14%|â–ˆâ–        | 28/200 [4:43:48<29:04:14, 608.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2105.105555013021
INFO:root:current train perplexity5.291024208068848
INFO:root:current mean train loss 2112.0410846819195
INFO:root:current train perplexity5.292076110839844
INFO:root:current mean train loss 2114.345204634233
INFO:root:current train perplexity5.294312000274658
INFO:root:current mean train loss 2120.2807685546877
INFO:root:current train perplexity5.313937664031982
INFO:root:current mean train loss 2119.1935243626644
INFO:root:current train perplexity5.31612491607666
INFO:root:current mean train loss 2120.9175602921196
INFO:root:current train perplexity5.327816486358643
INFO:root:current mean train loss 2120.210414496528
INFO:root:current train perplexity5.327457427978516
INFO:root:current mean train loss 2120.95490218624
INFO:root:current train perplexity5.326427459716797
INFO:root:current mean train loss 2124.1906690848214
INFO:root:current train perplexity5.333248138427734
INFO:root:current mean train loss 2124.098887219551
INFO:root:current train perplexity5.336906433105469
INFO:root:current mean train loss 2125.2421002906976
INFO:root:current train perplexity5.34022855758667
INFO:root:current mean train loss 2122.689104886968
INFO:root:current train perplexity5.334161281585693
INFO:root:current mean train loss 2121.7824648628985
INFO:root:current train perplexity5.331387996673584
INFO:root:current mean train loss 2119.9432592329545
INFO:root:current train perplexity5.326534271240234
INFO:root:current mean train loss 2119.057437268273
INFO:root:current train perplexity5.325703144073486
INFO:root:current mean train loss 2119.0445930214532
INFO:root:current train perplexity5.327878952026367
INFO:root:current mean train loss 2118.1747111852847
INFO:root:current train perplexity5.3246917724609375
INFO:root:current mean train loss 2118.8171339266064
INFO:root:current train perplexity5.325901031494141
INFO:root:current mean train loss 2119.1035554036457
INFO:root:current train perplexity5.326033115386963
INFO:root:current mean train loss 2120.004843070115
INFO:root:current train perplexity5.326812267303467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it]
INFO:root:final mean train loss: 2119.5607634682397
INFO:root:final train perplexity: 5.326810836791992
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2208.9366139080507
INFO:root:eval perplexity: 5.974522113800049
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2713.63698107131
INFO:root:eval perplexity: 9.308788299560547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/29
 14%|â–ˆâ–        | 29/200 [4:53:59<28:56:38, 609.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2102.193347433339
INFO:root:current train perplexity5.232122898101807
INFO:root:current mean train loss 2095.9224116007485
INFO:root:current train perplexity5.27354097366333
INFO:root:current mean train loss 2104.302305874759
INFO:root:current train perplexity5.251478672027588
INFO:root:current mean train loss 2103.4355546600964
INFO:root:current train perplexity5.254937171936035
INFO:root:current mean train loss 2099.8317590729007
INFO:root:current train perplexity5.24407958984375
INFO:root:current mean train loss 2098.769319688952
INFO:root:current train perplexity5.254286766052246
INFO:root:current mean train loss 2096.26270501462
INFO:root:current train perplexity5.254118919372559
INFO:root:current mean train loss 2097.585448293975
INFO:root:current train perplexity5.256031036376953
INFO:root:current mean train loss 2100.040611643428
INFO:root:current train perplexity5.261149883270264
INFO:root:current mean train loss 2101.3779472843294
INFO:root:current train perplexity5.259396076202393
INFO:root:current mean train loss 2103.245371836009
INFO:root:current train perplexity5.264817714691162
INFO:root:current mean train loss 2102.0661082427773
INFO:root:current train perplexity5.2599778175354
INFO:root:current mean train loss 2103.205971260189
INFO:root:current train perplexity5.259679794311523
INFO:root:current mean train loss 2104.747203256892
INFO:root:current train perplexity5.2649407386779785
INFO:root:current mean train loss 2106.2778615670295
INFO:root:current train perplexity5.269030570983887
INFO:root:current mean train loss 2105.6858008303234
INFO:root:current train perplexity5.268527507781982
INFO:root:current mean train loss 2105.8066410578735
INFO:root:current train perplexity5.272584915161133
INFO:root:current mean train loss 2106.8398489952087
INFO:root:current train perplexity5.2735819816589355
INFO:root:current mean train loss 2106.8129435694495
INFO:root:current train perplexity5.272735118865967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.73s/it]
INFO:root:final mean train loss: 2107.806876393682
INFO:root:final train perplexity: 5.277626991271973
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 2201.737741023936
INFO:root:eval perplexity: 5.939818859100342
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 2708.2045651699636
INFO:root:eval perplexity: 9.267303466796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/30
 15%|â–ˆâ–Œ        | 30/200 [5:04:10<28:48:10, 609.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2145.244913736979
INFO:root:current train perplexity5.264455318450928
INFO:root:current mean train loss 2084.8048554866687
INFO:root:current train perplexity5.172086238861084
INFO:root:current mean train loss 2091.5359003532444
INFO:root:current train perplexity5.178256988525391
INFO:root:current mean train loss 2093.1547962176373
INFO:root:current train perplexity5.205091953277588
INFO:root:current mean train loss 2092.6597016947776
INFO:root:current train perplexity5.206216335296631
INFO:root:current mean train loss 2089.962096088762
INFO:root:current train perplexity5.202421188354492
INFO:root:current mean train loss 2089.9348777933856
INFO:root:current train perplexity5.207056522369385
INFO:root:current mean train loss 2088.7678694408937
INFO:root:current train perplexity5.2090606689453125
INFO:root:current mean train loss 2090.0294347888016
INFO:root:current train perplexity5.210678577423096
INFO:root:current mean train loss 2088.6848637378384
INFO:root:current train perplexity5.206512928009033
INFO:root:current mean train loss 2091.3801898634943
INFO:root:current train perplexity5.21619176864624
INFO:root:current mean train loss 2092.8664261290787
INFO:root:current train perplexity5.2216410636901855
INFO:root:current mean train loss 2091.15726291098
INFO:root:current train perplexity5.219144344329834
INFO:root:current mean train loss 2093.0748137145483
INFO:root:current train perplexity5.225210666656494
INFO:root:current mean train loss 2093.889291756537
INFO:root:current train perplexity5.224793910980225
INFO:root:current mean train loss 2094.028078679615
INFO:root:current train perplexity5.227146148681641
INFO:root:current mean train loss 2094.9742200245687
INFO:root:current train perplexity5.226363658905029
INFO:root:current mean train loss 2095.0556669910447
INFO:root:current train perplexity5.225700855255127
INFO:root:current mean train loss 2095.2872669394997
INFO:root:current train perplexity5.227462291717529
INFO:root:current mean train loss 2097.1870113350824
INFO:root:current train perplexity5.228517055511475

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.24s/it]
INFO:root:final mean train loss: 2096.29549338321
INFO:root:final train perplexity: 5.229897975921631
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2199.954153940187
INFO:root:eval perplexity: 5.9312520027160645
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2708.2526799195202
INFO:root:eval perplexity: 9.267674446105957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/31
 16%|â–ˆâ–Œ        | 31/200 [5:14:21<28:38:44, 610.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2060.5256864107573
INFO:root:current train perplexity5.067285537719727
INFO:root:current mean train loss 2069.6527496822296
INFO:root:current train perplexity5.164808750152588
INFO:root:current mean train loss 2065.8046550919526
INFO:root:current train perplexity5.145595073699951
INFO:root:current mean train loss 2071.0515709625433
INFO:root:current train perplexity5.1498494148254395
INFO:root:current mean train loss 2070.0596726108606
INFO:root:current train perplexity5.1660542488098145
INFO:root:current mean train loss 2074.742594555757
INFO:root:current train perplexity5.176635265350342
INFO:root:current mean train loss 2078.1060584315096
INFO:root:current train perplexity5.178835391998291
INFO:root:current mean train loss 2081.3149225744664
INFO:root:current train perplexity5.182736873626709
INFO:root:current mean train loss 2082.858020255987
INFO:root:current train perplexity5.176976203918457
INFO:root:current mean train loss 2083.1674584539096
INFO:root:current train perplexity5.178701400756836
INFO:root:current mean train loss 2082.717844704671
INFO:root:current train perplexity5.17926025390625
INFO:root:current mean train loss 2082.5129169037245
INFO:root:current train perplexity5.176857948303223
INFO:root:current mean train loss 2083.206714723472
INFO:root:current train perplexity5.181913375854492
INFO:root:current mean train loss 2082.618345695024
INFO:root:current train perplexity5.178954601287842
INFO:root:current mean train loss 2082.6519682939056
INFO:root:current train perplexity5.175917625427246
INFO:root:current mean train loss 2084.076549205055
INFO:root:current train perplexity5.177887916564941
INFO:root:current mean train loss 2085.6735704710563
INFO:root:current train perplexity5.182960510253906
INFO:root:current mean train loss 2085.8871984028733
INFO:root:current train perplexity5.183704376220703
INFO:root:current mean train loss 2086.151331823209
INFO:root:current train perplexity5.184289932250977
INFO:root:current mean train loss 2085.9618987244858
INFO:root:current train perplexity5.183447360992432

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.03s/it]
INFO:root:final mean train loss: 2084.609579804806
INFO:root:final train perplexity: 5.181886672973633
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 2199.388360206117
INFO:root:eval perplexity: 5.928536415100098
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2711.7108976756426
INFO:root:eval perplexity: 9.294058799743652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/32
 16%|â–ˆâ–Œ        | 32/200 [5:24:31<28:28:36, 610.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2061.613167696221
INFO:root:current train perplexity5.1028218269348145
INFO:root:current mean train loss 2050.187331833206
INFO:root:current train perplexity5.076886177062988
INFO:root:current mean train loss 2054.904527452257
INFO:root:current train perplexity5.066132068634033
INFO:root:current mean train loss 2052.9636525857554
INFO:root:current train perplexity5.063560962677002
INFO:root:current mean train loss 2059.3350744656464
INFO:root:current train perplexity5.072934627532959
INFO:root:current mean train loss 2067.807195449183
INFO:root:current train perplexity5.101656436920166
INFO:root:current mean train loss 2069.996978997072
INFO:root:current train perplexity5.118525505065918
INFO:root:current mean train loss 2069.0900675181906
INFO:root:current train perplexity5.113338470458984
INFO:root:current mean train loss 2070.4102345893107
INFO:root:current train perplexity5.117405891418457
INFO:root:current mean train loss 2071.1118081215204
INFO:root:current train perplexity5.120937824249268
INFO:root:current mean train loss 2072.4842704853413
INFO:root:current train perplexity5.126409530639648
INFO:root:current mean train loss 2073.5785431148497
INFO:root:current train perplexity5.129640579223633
INFO:root:current mean train loss 2074.3593920878798
INFO:root:current train perplexity5.131839752197266
INFO:root:current mean train loss 2073.1775489517404
INFO:root:current train perplexity5.131280899047852
INFO:root:current mean train loss 2072.683088465193
INFO:root:current train perplexity5.133456707000732
INFO:root:current mean train loss 2072.3482154158905
INFO:root:current train perplexity5.134210109710693
INFO:root:current mean train loss 2073.829603068653
INFO:root:current train perplexity5.138509750366211
INFO:root:current mean train loss 2073.389113583239
INFO:root:current train perplexity5.138753414154053
INFO:root:current mean train loss 2074.8793413448902
INFO:root:current train perplexity5.140128135681152
INFO:root:current mean train loss 2075.219594063123
INFO:root:current train perplexity5.1407270431518555

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.81s/it]
INFO:root:final mean train loss: 2074.0856873574307
INFO:root:final train perplexity: 5.139028072357178
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2199.378311914755
INFO:root:eval perplexity: 5.928487777709961
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2716.9315839185783
INFO:root:eval perplexity: 9.33403491973877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/33
 16%|â–ˆâ–‹        | 33/200 [5:34:42<28:18:36, 610.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2046.4931111653646
INFO:root:current train perplexity4.9927825927734375
INFO:root:current mean train loss 2048.7617095947267
INFO:root:current train perplexity5.031303882598877
INFO:root:current mean train loss 2051.902601036659
INFO:root:current train perplexity5.0295186042785645
INFO:root:current mean train loss 2044.548489718967
INFO:root:current train perplexity5.037113666534424
INFO:root:current mean train loss 2046.211216404127
INFO:root:current train perplexity5.042983055114746
INFO:root:current mean train loss 2048.4502123151506
INFO:root:current train perplexity5.055637836456299
INFO:root:current mean train loss 2051.1900795676493
INFO:root:current train perplexity5.063774108886719
INFO:root:current mean train loss 2053.281611874229
INFO:root:current train perplexity5.071426868438721
INFO:root:current mean train loss 2054.079419229197
INFO:root:current train perplexity5.072997570037842
INFO:root:current mean train loss 2053.2864346822103
INFO:root:current train perplexity5.075777530670166
INFO:root:current mean train loss 2053.8141898677036
INFO:root:current train perplexity5.076514720916748
INFO:root:current mean train loss 2055.88225644868
INFO:root:current train perplexity5.0782880783081055
INFO:root:current mean train loss 2057.8530387757314
INFO:root:current train perplexity5.081910133361816
INFO:root:current mean train loss 2059.564618997013
INFO:root:current train perplexity5.085434913635254
INFO:root:current mean train loss 2058.460077238736
INFO:root:current train perplexity5.080460071563721
INFO:root:current mean train loss 2061.0407856476613
INFO:root:current train perplexity5.087052822113037
INFO:root:current mean train loss 2062.06587906803
INFO:root:current train perplexity5.08745002746582
INFO:root:current mean train loss 2062.028651566939
INFO:root:current train perplexity5.088360786437988
INFO:root:current mean train loss 2063.5283092211653
INFO:root:current train perplexity5.092829704284668
INFO:root:current mean train loss 2063.228618014589
INFO:root:current train perplexity5.094189167022705

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.94s/it]
INFO:root:final mean train loss: 2062.9293594860515
INFO:root:final train perplexity: 5.093979835510254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2195.2546092364805
INFO:root:eval perplexity: 5.908738613128662
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2711.479687846299
INFO:root:eval perplexity: 9.292292594909668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/34
 17%|â–ˆâ–‹        | 34/200 [5:44:53<28:09:29, 610.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2052.0498570033483
INFO:root:current train perplexity5.030849933624268
INFO:root:current mean train loss 2040.8184110997088
INFO:root:current train perplexity4.998732566833496
INFO:root:current mean train loss 2045.4664831058212
INFO:root:current train perplexity5.015431880950928
INFO:root:current mean train loss 2042.922177099739
INFO:root:current train perplexity5.0090107917785645
INFO:root:current mean train loss 2045.2036593455189
INFO:root:current train perplexity5.01407527923584
INFO:root:current mean train loss 2047.6366055652215
INFO:root:current train perplexity5.019837856292725
INFO:root:current mean train loss 2047.5877234770242
INFO:root:current train perplexity5.020432472229004
INFO:root:current mean train loss 2047.8361354518581
INFO:root:current train perplexity5.027177333831787
INFO:root:current mean train loss 2048.0753468912576
INFO:root:current train perplexity5.033506393432617
INFO:root:current mean train loss 2048.47929477594
INFO:root:current train perplexity5.035586833953857
INFO:root:current mean train loss 2052.3236641631484
INFO:root:current train perplexity5.041994571685791
INFO:root:current mean train loss 2051.2255721436586
INFO:root:current train perplexity5.039771556854248
INFO:root:current mean train loss 2050.687015351226
INFO:root:current train perplexity5.040608882904053
INFO:root:current mean train loss 2050.3516805810864
INFO:root:current train perplexity5.0445380210876465
INFO:root:current mean train loss 2051.1299520710795
INFO:root:current train perplexity5.047452449798584
INFO:root:current mean train loss 2052.5758615516756
INFO:root:current train perplexity5.047443389892578
INFO:root:current mean train loss 2052.447467110167
INFO:root:current train perplexity5.047892093658447
INFO:root:current mean train loss 2052.0147208416133
INFO:root:current train perplexity5.048449993133545
INFO:root:current mean train loss 2052.8906574523635
INFO:root:current train perplexity5.05068302154541
INFO:root:current mean train loss 2052.751661502296
INFO:root:current train perplexity5.051441669464111

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.23s/it]
INFO:root:final mean train loss: 2052.207771151221
INFO:root:final train perplexity: 5.051058769226074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2195.0692554230386
INFO:root:eval perplexity: 5.9078521728515625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2714.0311500062335
INFO:root:eval perplexity: 9.311802864074707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/35
 18%|â–ˆâ–Š        | 35/200 [5:55:04<27:59:06, 610.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2047.8049965716423
INFO:root:current train perplexity5.024016857147217
INFO:root:current mean train loss 2037.0392927976
INFO:root:current train perplexity4.979923248291016
INFO:root:current mean train loss 2036.8876849323715
INFO:root:current train perplexity4.979726314544678
INFO:root:current mean train loss 2036.6742398799374
INFO:root:current train perplexity4.988104343414307
INFO:root:current mean train loss 2040.0157747461728
INFO:root:current train perplexity4.992819309234619
INFO:root:current mean train loss 2039.1582980685764
INFO:root:current train perplexity4.99261474609375
INFO:root:current mean train loss 2039.103828012428
INFO:root:current train perplexity4.996888160705566
INFO:root:current mean train loss 2037.7364855557305
INFO:root:current train perplexity4.991482257843018
INFO:root:current mean train loss 2036.3126613949769
INFO:root:current train perplexity4.989089488983154
INFO:root:current mean train loss 2038.0003768951601
INFO:root:current train perplexity4.995913028717041
INFO:root:current mean train loss 2037.5730886511653
INFO:root:current train perplexity4.99454402923584
INFO:root:current mean train loss 2037.24428058669
INFO:root:current train perplexity4.998805046081543
INFO:root:current mean train loss 2037.704072035446
INFO:root:current train perplexity5.001110553741455
INFO:root:current mean train loss 2038.4337763300584
INFO:root:current train perplexity5.0026984214782715
INFO:root:current mean train loss 2039.108532763868
INFO:root:current train perplexity5.004026412963867
INFO:root:current mean train loss 2039.0201313396917
INFO:root:current train perplexity5.006327152252197
INFO:root:current mean train loss 2040.9323101381485
INFO:root:current train perplexity5.007800579071045
INFO:root:current mean train loss 2041.8780347468994
INFO:root:current train perplexity5.01098108291626
INFO:root:current mean train loss 2043.0108592950808
INFO:root:current train perplexity5.012183666229248

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.73s/it]
INFO:root:final mean train loss: 2042.2422740818934
INFO:root:final train perplexity: 5.011489391326904
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2194.58984504862
INFO:root:eval perplexity: 5.905561447143555
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 2717.4876929749835
INFO:root:eval perplexity: 9.33830451965332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/36
 18%|â–ˆâ–Š        | 36/200 [6:05:14<27:48:47, 610.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2041.032071200284
INFO:root:current train perplexity4.933966636657715
INFO:root:current mean train loss 2012.5222222955376
INFO:root:current train perplexity4.863439083099365
INFO:root:current mean train loss 2018.789884015847
INFO:root:current train perplexity4.898496150970459
INFO:root:current mean train loss 2022.7188826680567
INFO:root:current train perplexity4.920361042022705
INFO:root:current mean train loss 2023.5929154073524
INFO:root:current train perplexity4.932628631591797
INFO:root:current mean train loss 2018.7532531379954
INFO:root:current train perplexity4.924460411071777
INFO:root:current mean train loss 2025.9392205720642
INFO:root:current train perplexity4.939761638641357
INFO:root:current mean train loss 2028.6405582132888
INFO:root:current train perplexity4.951733112335205
INFO:root:current mean train loss 2026.916559146453
INFO:root:current train perplexity4.94862699508667
INFO:root:current mean train loss 2030.1251491374949
INFO:root:current train perplexity4.952734470367432
INFO:root:current mean train loss 2028.763050656644
INFO:root:current train perplexity4.953073978424072
INFO:root:current mean train loss 2030.1424969279155
INFO:root:current train perplexity4.956352710723877
INFO:root:current mean train loss 2028.806584781108
INFO:root:current train perplexity4.955796241760254
INFO:root:current mean train loss 2031.4547957151924
INFO:root:current train perplexity4.958348751068115
INFO:root:current mean train loss 2031.2004752696448
INFO:root:current train perplexity4.959930896759033
INFO:root:current mean train loss 2031.9339132656096
INFO:root:current train perplexity4.962069988250732
INFO:root:current mean train loss 2032.4187936907151
INFO:root:current train perplexity4.967266082763672
INFO:root:current mean train loss 2032.8784384446003
INFO:root:current train perplexity4.96987771987915
INFO:root:current mean train loss 2033.1992243446084
INFO:root:current train perplexity4.971830368041992
INFO:root:current mean train loss 2032.7914418809892
INFO:root:current train perplexity4.971414566040039

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.73s/it]
INFO:root:final mean train loss: 2032.0258141468105
INFO:root:final train perplexity: 4.971245288848877
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2191.135336152205
INFO:root:eval perplexity: 5.889074325561523
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 2714.071038428773
INFO:root:eval perplexity: 9.312110900878906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/37
 18%|â–ˆâ–Š        | 37/200 [6:15:27<27:40:09, 611.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2046.9283011300224
INFO:root:current train perplexity5.0077385902404785
INFO:root:current mean train loss 2031.0823602676392
INFO:root:current train perplexity4.9321393966674805
INFO:root:current mean train loss 2024.2399024294134
INFO:root:current train perplexity4.923998832702637
INFO:root:current mean train loss 2024.0808027313976
INFO:root:current train perplexity4.915266990661621
INFO:root:current mean train loss 2021.1752635920159
INFO:root:current train perplexity4.917085647583008
INFO:root:current mean train loss 2022.5886757590554
INFO:root:current train perplexity4.919571399688721
INFO:root:current mean train loss 2022.2964242339895
INFO:root:current train perplexity4.922321796417236
INFO:root:current mean train loss 2021.6255107502361
INFO:root:current train perplexity4.925078392028809
INFO:root:current mean train loss 2019.4137613047724
INFO:root:current train perplexity4.916309356689453
INFO:root:current mean train loss 2020.3485628325363
INFO:root:current train perplexity4.91953706741333
INFO:root:current mean train loss 2021.6072650122735
INFO:root:current train perplexity4.92616081237793
INFO:root:current mean train loss 2020.911971071933
INFO:root:current train perplexity4.924280643463135
INFO:root:current mean train loss 2020.6284769163847
INFO:root:current train perplexity4.92807674407959
INFO:root:current mean train loss 2020.8921072167086
INFO:root:current train perplexity4.925436973571777
INFO:root:current mean train loss 2021.0741129215357
INFO:root:current train perplexity4.927765369415283
INFO:root:current mean train loss 2021.8894485553521
INFO:root:current train perplexity4.929421901702881
INFO:root:current mean train loss 2021.3493482135145
INFO:root:current train perplexity4.928501129150391
INFO:root:current mean train loss 2022.1958726953578
INFO:root:current train perplexity4.930166721343994
INFO:root:current mean train loss 2021.7094330568543
INFO:root:current train perplexity4.931396961212158
INFO:root:current mean train loss 2022.9600667359919
INFO:root:current train perplexity4.93428897857666

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.98s/it]
INFO:root:final mean train loss: 2022.0761270604828
INFO:root:final train perplexity: 4.932363033294678
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2190.656812735483
INFO:root:eval perplexity: 5.886795997619629
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it]
INFO:root:eval mean loss: 2714.315599373892
INFO:root:eval perplexity: 9.313981056213379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/38
 19%|â–ˆâ–‰        | 38/200 [6:25:39<27:31:12, 611.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1985.4796603732639
INFO:root:current train perplexity4.912064552307129
INFO:root:current mean train loss 2001.0798878636854
INFO:root:current train perplexity4.865818023681641
INFO:root:current mean train loss 2007.211689851722
INFO:root:current train perplexity4.876626014709473
INFO:root:current mean train loss 2005.3503233978713
INFO:root:current train perplexity4.866199016571045
INFO:root:current mean train loss 2008.4537647033005
INFO:root:current train perplexity4.8747100830078125
INFO:root:current mean train loss 2008.7094630250144
INFO:root:current train perplexity4.8796257972717285
INFO:root:current mean train loss 2009.265787949673
INFO:root:current train perplexity4.877105712890625
INFO:root:current mean train loss 2008.6489529808096
INFO:root:current train perplexity4.877927303314209
INFO:root:current mean train loss 2012.0444052792159
INFO:root:current train perplexity4.883124351501465
INFO:root:current mean train loss 2013.8271528294476
INFO:root:current train perplexity4.886794090270996
INFO:root:current mean train loss 2011.839002574574
INFO:root:current train perplexity4.884279727935791
INFO:root:current mean train loss 2010.2877580001364
INFO:root:current train perplexity4.885951519012451
INFO:root:current mean train loss 2009.097527218248
INFO:root:current train perplexity4.884182929992676
INFO:root:current mean train loss 2009.0075766184073
INFO:root:current train perplexity4.885104179382324
INFO:root:current mean train loss 2011.6387707139381
INFO:root:current train perplexity4.889700412750244
INFO:root:current mean train loss 2011.9748236498785
INFO:root:current train perplexity4.891889572143555
INFO:root:current mean train loss 2011.8934193341565
INFO:root:current train perplexity4.891185283660889
INFO:root:current mean train loss 2013.4466582115194
INFO:root:current train perplexity4.893042087554932
INFO:root:current mean train loss 2012.5381710228235
INFO:root:current train perplexity4.893126487731934
INFO:root:current mean train loss 2012.767959963448
INFO:root:current train perplexity4.892922401428223

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.71s/it]
INFO:root:final mean train loss: 2012.3405011629613
INFO:root:final train perplexity: 4.89461088180542
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2192.431499941129
INFO:root:eval perplexity: 5.895254135131836
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.82s/it]
INFO:root:eval mean loss: 2717.3848712461213
INFO:root:eval perplexity: 9.33751392364502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/39
 20%|â–ˆâ–‰        | 39/200 [6:35:52<27:21:40, 611.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2008.599115187122
INFO:root:current train perplexity4.87445592880249
INFO:root:current mean train loss 1997.7585569782023
INFO:root:current train perplexity4.836479187011719
INFO:root:current mean train loss 2002.1023270701635
INFO:root:current train perplexity4.846704006195068
INFO:root:current mean train loss 2001.5951207619346
INFO:root:current train perplexity4.837768077850342
INFO:root:current mean train loss 1998.5958135695685
INFO:root:current train perplexity4.829415798187256
INFO:root:current mean train loss 2000.5528134383342
INFO:root:current train perplexity4.832789897918701
INFO:root:current mean train loss 1999.2635249111947
INFO:root:current train perplexity4.828428745269775
INFO:root:current mean train loss 1999.4842596579724
INFO:root:current train perplexity4.825098037719727
INFO:root:current mean train loss 1999.9102856841828
INFO:root:current train perplexity4.8332672119140625
INFO:root:current mean train loss 2000.4585486271278
INFO:root:current train perplexity4.834489822387695
INFO:root:current mean train loss 2001.3013398768537
INFO:root:current train perplexity4.835521697998047
INFO:root:current mean train loss 2000.874641878059
INFO:root:current train perplexity4.839908599853516
INFO:root:current mean train loss 2000.3538865755932
INFO:root:current train perplexity4.841845989227295
INFO:root:current mean train loss 1999.9395523407386
INFO:root:current train perplexity4.844240188598633
INFO:root:current mean train loss 2000.0146155403022
INFO:root:current train perplexity4.848923206329346
INFO:root:current mean train loss 2001.8862268738496
INFO:root:current train perplexity4.854371070861816
INFO:root:current mean train loss 2001.2980169670388
INFO:root:current train perplexity4.853396415710449
INFO:root:current mean train loss 2001.6531646416759
INFO:root:current train perplexity4.852068901062012
INFO:root:current mean train loss 2002.1707502092634
INFO:root:current train perplexity4.853246688842773
INFO:root:current mean train loss 2002.9084308402619
INFO:root:current train perplexity4.855140686035156

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.97s/it]
INFO:root:final mean train loss: 2001.9859574202992
INFO:root:final train perplexity: 4.854775428771973
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2190.794770369293
INFO:root:eval perplexity: 5.887452125549316
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 2719.191227473266
INFO:root:eval perplexity: 9.351388931274414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/40
 20%|â–ˆâ–ˆ        | 40/200 [6:46:03<27:11:20, 611.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1996.3552802363529
INFO:root:current train perplexity4.810523986816406
INFO:root:current mean train loss 1994.4425444363217
INFO:root:current train perplexity4.812858581542969
INFO:root:current mean train loss 1991.2896395994344
INFO:root:current train perplexity4.792940139770508
INFO:root:current mean train loss 1992.3390060062459
INFO:root:current train perplexity4.7968831062316895
INFO:root:current mean train loss 1990.2005210032294
INFO:root:current train perplexity4.800013065338135
INFO:root:current mean train loss 1986.8284551590837
INFO:root:current train perplexity4.79753303527832
INFO:root:current mean train loss 1992.659819163349
INFO:root:current train perplexity4.810527324676514
INFO:root:current mean train loss 1992.6810553456455
INFO:root:current train perplexity4.809566974639893
INFO:root:current mean train loss 1993.8152728986686
INFO:root:current train perplexity4.81542444229126
INFO:root:current mean train loss 1991.5010453907048
INFO:root:current train perplexity4.8112359046936035
INFO:root:current mean train loss 1991.9990595268696
INFO:root:current train perplexity4.8154120445251465
INFO:root:current mean train loss 1991.359752600025
INFO:root:current train perplexity4.813762664794922
INFO:root:current mean train loss 1992.6959934786394
INFO:root:current train perplexity4.81560754776001
INFO:root:current mean train loss 1992.622787862865
INFO:root:current train perplexity4.816230297088623
INFO:root:current mean train loss 1992.9310552817572
INFO:root:current train perplexity4.816867351531982
INFO:root:current mean train loss 1993.8634076495953
INFO:root:current train perplexity4.818304538726807
INFO:root:current mean train loss 1993.7841454438328
INFO:root:current train perplexity4.817655563354492
INFO:root:current mean train loss 1994.2342538903308
INFO:root:current train perplexity4.820723056793213
INFO:root:current mean train loss 1994.6026476849388
INFO:root:current train perplexity4.823851108551025
INFO:root:current mean train loss 1994.3668539192774
INFO:root:current train perplexity4.822909832000732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.63s/it]
INFO:root:final mean train loss: 1993.6800682694998
INFO:root:final train perplexity: 4.823056697845459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2192.0998478882702
INFO:root:eval perplexity: 5.893672943115234
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 2719.0927219255595
INFO:root:eval perplexity: 9.350635528564453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/41
 20%|â–ˆâ–ˆ        | 41/200 [6:56:16<27:01:34, 611.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1970.9406623840332
INFO:root:current train perplexity4.745013236999512
INFO:root:current mean train loss 1974.615735735212
INFO:root:current train perplexity4.752658367156982
INFO:root:current mean train loss 1972.568371746991
INFO:root:current train perplexity4.751123905181885
INFO:root:current mean train loss 1970.1928300953875
INFO:root:current train perplexity4.756341934204102
INFO:root:current mean train loss 1972.6312457669167
INFO:root:current train perplexity4.763650417327881
INFO:root:current mean train loss 1976.45011215722
INFO:root:current train perplexity4.768342971801758
INFO:root:current mean train loss 1977.8352671656116
INFO:root:current train perplexity4.764725685119629
INFO:root:current mean train loss 1980.6679661429707
INFO:root:current train perplexity4.770522594451904
INFO:root:current mean train loss 1977.976640020098
INFO:root:current train perplexity4.773165702819824
INFO:root:current mean train loss 1978.9585022217777
INFO:root:current train perplexity4.775674819946289
INFO:root:current mean train loss 1979.1665875511449
INFO:root:current train perplexity4.775655269622803
INFO:root:current mean train loss 1980.3228223921863
INFO:root:current train perplexity4.781981468200684
INFO:root:current mean train loss 1981.662031856584
INFO:root:current train perplexity4.78324556350708
INFO:root:current mean train loss 1981.0045347896894
INFO:root:current train perplexity4.781431674957275
INFO:root:current mean train loss 1981.9842465650588
INFO:root:current train perplexity4.7837724685668945
INFO:root:current mean train loss 1983.584794658766
INFO:root:current train perplexity4.786379337310791
INFO:root:current mean train loss 1983.2625434443635
INFO:root:current train perplexity4.786134719848633
INFO:root:current mean train loss 1983.1112693327839
INFO:root:current train perplexity4.7873311042785645
INFO:root:current mean train loss 1984.068459040002
INFO:root:current train perplexity4.788278579711914

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.94s/it]
INFO:root:final mean train loss: 1984.2513766921174
INFO:root:final train perplexity: 4.7873005867004395
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2189.894210057901
INFO:root:eval perplexity: 5.883162498474121
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 2719.7597470114415
INFO:root:eval perplexity: 9.355762481689453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/42
 21%|â–ˆâ–ˆ        | 42/200 [7:06:28<26:51:57, 612.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1981.2448354867788
INFO:root:current train perplexity4.773881912231445
INFO:root:current mean train loss 1956.9840476787197
INFO:root:current train perplexity4.711953639984131
INFO:root:current mean train loss 1967.6087652215376
INFO:root:current train perplexity4.7294697761535645
INFO:root:current mean train loss 1970.5769733270517
INFO:root:current train perplexity4.728783130645752
INFO:root:current mean train loss 1969.021403093315
INFO:root:current train perplexity4.73625373840332
INFO:root:current mean train loss 1973.5528252733613
INFO:root:current train perplexity4.732317924499512
INFO:root:current mean train loss 1971.7074546129436
INFO:root:current train perplexity4.7260613441467285
INFO:root:current mean train loss 1971.9839861897901
INFO:root:current train perplexity4.725620746612549
INFO:root:current mean train loss 1972.3680227732452
INFO:root:current train perplexity4.730866432189941
INFO:root:current mean train loss 1971.4702754109478
INFO:root:current train perplexity4.732134819030762
INFO:root:current mean train loss 1970.1840164772027
INFO:root:current train perplexity4.733872413635254
INFO:root:current mean train loss 1969.622756121722
INFO:root:current train perplexity4.7343316078186035
INFO:root:current mean train loss 1970.3641703606438
INFO:root:current train perplexity4.73625373840332
INFO:root:current mean train loss 1971.0798181793841
INFO:root:current train perplexity4.738285541534424
INFO:root:current mean train loss 1970.8949592304161
INFO:root:current train perplexity4.739325523376465
INFO:root:current mean train loss 1972.6857194516017
INFO:root:current train perplexity4.742635726928711
INFO:root:current mean train loss 1972.4286374531396
INFO:root:current train perplexity4.744349956512451
INFO:root:current mean train loss 1973.6986818686605
INFO:root:current train perplexity4.748928070068359
INFO:root:current mean train loss 1974.6398115794566
INFO:root:current train perplexity4.750542640686035
INFO:root:current mean train loss 1975.278214641597
INFO:root:current train perplexity4.752411365509033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.47s/it]
INFO:root:final mean train loss: 1975.2865438071753
INFO:root:final train perplexity: 4.753551006317139
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it]
INFO:root:eval mean loss: 2191.6664225260415
INFO:root:eval perplexity: 5.891606330871582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 2724.4974992900875
INFO:root:eval perplexity: 9.392274856567383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/43
 22%|â–ˆâ–ˆâ–       | 43/200 [7:16:39<26:40:54, 611.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1907.9023803710938
INFO:root:current train perplexity4.562829494476318
INFO:root:current mean train loss 1944.034237905649
INFO:root:current train perplexity4.657196998596191
INFO:root:current mean train loss 1955.462583326257
INFO:root:current train perplexity4.687386512756348
INFO:root:current mean train loss 1963.1879723751183
INFO:root:current train perplexity4.695356369018555
INFO:root:current mean train loss 1964.7249792764353
INFO:root:current train perplexity4.70265531539917
INFO:root:current mean train loss 1962.7605646097436
INFO:root:current train perplexity4.697724342346191
INFO:root:current mean train loss 1960.6992776537697
INFO:root:current train perplexity4.697819232940674
INFO:root:current mean train loss 1957.9730089161494
INFO:root:current train perplexity4.693525314331055
INFO:root:current mean train loss 1958.1428134412652
INFO:root:current train perplexity4.693385124206543
INFO:root:current mean train loss 1958.1072143554688
INFO:root:current train perplexity4.696688652038574
INFO:root:current mean train loss 1959.9463180986422
INFO:root:current train perplexity4.704009056091309
INFO:root:current mean train loss 1961.7186575290375
INFO:root:current train perplexity4.703347682952881
INFO:root:current mean train loss 1962.644951648247
INFO:root:current train perplexity4.702303886413574
INFO:root:current mean train loss 1963.9820310664356
INFO:root:current train perplexity4.705184459686279
INFO:root:current mean train loss 1964.310828063538
INFO:root:current train perplexity4.706998348236084
INFO:root:current mean train loss 1965.3588516135621
INFO:root:current train perplexity4.711449146270752
INFO:root:current mean train loss 1965.6301002174798
INFO:root:current train perplexity4.712353706359863
INFO:root:current mean train loss 1965.8054122307396
INFO:root:current train perplexity4.713868618011475
INFO:root:current mean train loss 1965.9020476461108
INFO:root:current train perplexity4.716264247894287
INFO:root:current mean train loss 1966.6313510716889
INFO:root:current train perplexity4.718661785125732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.34s/it]
INFO:root:final mean train loss: 1965.7046153289286
INFO:root:final train perplexity: 4.717740058898926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it]
INFO:root:eval mean loss: 2194.218077314661
INFO:root:eval perplexity: 5.903783321380615
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 2728.562580081588
INFO:root:eval perplexity: 9.423717498779297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/44
 22%|â–ˆâ–ˆâ–       | 44/200 [7:26:50<26:30:02, 611.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1943.800191676363
INFO:root:current train perplexity4.674029350280762
INFO:root:current mean train loss 1938.2545431746917
INFO:root:current train perplexity4.633657932281494
INFO:root:current mean train loss 1948.4146041561235
INFO:root:current train perplexity4.657304286956787
INFO:root:current mean train loss 1949.4309314211096
INFO:root:current train perplexity4.649858474731445
INFO:root:current mean train loss 1947.538827917453
INFO:root:current train perplexity4.651632785797119
INFO:root:current mean train loss 1948.46042266232
INFO:root:current train perplexity4.647258758544922
INFO:root:current mean train loss 1948.7844889197136
INFO:root:current train perplexity4.655177116394043
INFO:root:current mean train loss 1949.4309184982117
INFO:root:current train perplexity4.662612438201904
INFO:root:current mean train loss 1952.5035262034662
INFO:root:current train perplexity4.666477203369141
INFO:root:current mean train loss 1951.6252526481653
INFO:root:current train perplexity4.670373916625977
INFO:root:current mean train loss 1953.310419674703
INFO:root:current train perplexity4.671446800231934
INFO:root:current mean train loss 1953.7203671389698
INFO:root:current train perplexity4.670449256896973
INFO:root:current mean train loss 1953.975624800302
INFO:root:current train perplexity4.676244735717773
INFO:root:current mean train loss 1953.46668123898
INFO:root:current train perplexity4.6767144203186035
INFO:root:current mean train loss 1954.0179802737075
INFO:root:current train perplexity4.679672718048096
INFO:root:current mean train loss 1954.6677681664612
INFO:root:current train perplexity4.680030822753906
INFO:root:current mean train loss 1955.419378302643
INFO:root:current train perplexity4.680334091186523
INFO:root:current mean train loss 1957.0167116144416
INFO:root:current train perplexity4.683255672454834
INFO:root:current mean train loss 1957.612852054218
INFO:root:current train perplexity4.683201789855957
INFO:root:current mean train loss 1957.1745100760986
INFO:root:current train perplexity4.683785915374756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.08s/it]
INFO:root:final mean train loss: 1956.63784908539
INFO:root:final train perplexity: 4.684102535247803
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2192.889688694731
INFO:root:eval perplexity: 5.897441387176514
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 2728.712586747839
INFO:root:eval perplexity: 9.42487907409668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [7:37:01<26:19:12, 611.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1911.5214958190918
INFO:root:current train perplexity4.561557292938232
INFO:root:current mean train loss 1928.714474561738
INFO:root:current train perplexity4.611353874206543
INFO:root:current mean train loss 1941.2132924397786
INFO:root:current train perplexity4.618818759918213
INFO:root:current mean train loss 1942.8464912163033
INFO:root:current train perplexity4.633671283721924
INFO:root:current mean train loss 1944.4114790291621
INFO:root:current train perplexity4.636735916137695
INFO:root:current mean train loss 1946.1223023326684
INFO:root:current train perplexity4.641757488250732
INFO:root:current mean train loss 1945.00831842997
INFO:root:current train perplexity4.637574672698975
INFO:root:current mean train loss 1942.4958318740285
INFO:root:current train perplexity4.641942977905273
INFO:root:current mean train loss 1945.2080489264595
INFO:root:current train perplexity4.644297122955322
INFO:root:current mean train loss 1944.9610063861514
INFO:root:current train perplexity4.64154052734375
INFO:root:current mean train loss 1947.5704882628936
INFO:root:current train perplexity4.648214817047119
INFO:root:current mean train loss 1946.7937296968964
INFO:root:current train perplexity4.647409439086914
INFO:root:current mean train loss 1946.5870929186858
INFO:root:current train perplexity4.6498122215271
INFO:root:current mean train loss 1946.3111650125722
INFO:root:current train perplexity4.645514011383057
INFO:root:current mean train loss 1947.0717115558562
INFO:root:current train perplexity4.645141124725342
INFO:root:current mean train loss 1947.5401576205593
INFO:root:current train perplexity4.647327423095703
INFO:root:current mean train loss 1948.613273693965
INFO:root:current train perplexity4.649189472198486
INFO:root:current mean train loss 1949.41579528947
INFO:root:current train perplexity4.650958061218262
INFO:root:current mean train loss 1950.0679390051846
INFO:root:current train perplexity4.653052806854248
INFO:root:current mean train loss 1949.133480343945
INFO:root:current train perplexity4.654478073120117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.57s/it]
INFO:root:final mean train loss: 1948.431077457953
INFO:root:final train perplexity: 4.653862953186035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2190.963481064384
INFO:root:eval perplexity: 5.888257026672363
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 2727.38020919908
INFO:root:eval perplexity: 9.414559364318848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [7:47:12<26:08:58, 611.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1918.619627399209
INFO:root:current train perplexity4.601203918457031
INFO:root:current mean train loss 1916.668140727512
INFO:root:current train perplexity4.565502166748047
INFO:root:current mean train loss 1921.3571968485876
INFO:root:current train perplexity4.578599452972412
INFO:root:current mean train loss 1923.171239657665
INFO:root:current train perplexity4.570583343505859
INFO:root:current mean train loss 1928.2023278630945
INFO:root:current train perplexity4.578083038330078
INFO:root:current mean train loss 1927.3802946159637
INFO:root:current train perplexity4.580806732177734
INFO:root:current mean train loss 1926.9271639965355
INFO:root:current train perplexity4.576997756958008
INFO:root:current mean train loss 1927.502334028139
INFO:root:current train perplexity4.57924222946167
INFO:root:current mean train loss 1928.9547016607105
INFO:root:current train perplexity4.582942008972168
INFO:root:current mean train loss 1929.7697396779035
INFO:root:current train perplexity4.588390827178955
INFO:root:current mean train loss 1930.8452055840223
INFO:root:current train perplexity4.594613075256348
INFO:root:current mean train loss 1932.1684258159796
INFO:root:current train perplexity4.601231575012207
INFO:root:current mean train loss 1934.02781926199
INFO:root:current train perplexity4.604077339172363
INFO:root:current mean train loss 1933.6824142378712
INFO:root:current train perplexity4.6046271324157715
INFO:root:current mean train loss 1934.3586868399466
INFO:root:current train perplexity4.608204364776611
INFO:root:current mean train loss 1936.6066428949982
INFO:root:current train perplexity4.610790252685547
INFO:root:current mean train loss 1937.0088931962466
INFO:root:current train perplexity4.612628936767578
INFO:root:current mean train loss 1937.4689996238508
INFO:root:current train perplexity4.615835666656494
INFO:root:current mean train loss 1938.8376845137227
INFO:root:current train perplexity4.617946147918701
INFO:root:current mean train loss 1940.0535281709442
INFO:root:current train perplexity4.621336936950684

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.07s/it]
INFO:root:final mean train loss: 1939.5384977016554
INFO:root:final train perplexity: 4.621315956115723
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it]
INFO:root:eval mean loss: 2192.38750831117
INFO:root:eval perplexity: 5.895044803619385
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2732.4608180269283
INFO:root:eval perplexity: 9.453965187072754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [7:57:24<25:59:07, 611.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1904.2483072086256
INFO:root:current train perplexity4.547805309295654
INFO:root:current mean train loss 1913.965172353417
INFO:root:current train perplexity4.593951225280762
INFO:root:current mean train loss 1915.9414406590813
INFO:root:current train perplexity4.587965488433838
INFO:root:current mean train loss 1922.9147298994974
INFO:root:current train perplexity4.586382865905762
INFO:root:current mean train loss 1917.7462471958145
INFO:root:current train perplexity4.575681686401367
INFO:root:current mean train loss 1920.5611149714543
INFO:root:current train perplexity4.573413848876953
INFO:root:current mean train loss 1924.508442438776
INFO:root:current train perplexity4.574188709259033
INFO:root:current mean train loss 1925.2474579392817
INFO:root:current train perplexity4.5712080001831055
INFO:root:current mean train loss 1926.337258523716
INFO:root:current train perplexity4.57625150680542
INFO:root:current mean train loss 1925.3764901629431
INFO:root:current train perplexity4.577507972717285
INFO:root:current mean train loss 1925.271956202328
INFO:root:current train perplexity4.576575756072998
INFO:root:current mean train loss 1928.6267830621023
INFO:root:current train perplexity4.582177639007568
INFO:root:current mean train loss 1928.676612607136
INFO:root:current train perplexity4.582345485687256
INFO:root:current mean train loss 1929.0708027895598
INFO:root:current train perplexity4.583756923675537
INFO:root:current mean train loss 1928.5868773110242
INFO:root:current train perplexity4.582813739776611
INFO:root:current mean train loss 1928.482479854579
INFO:root:current train perplexity4.583528518676758
INFO:root:current mean train loss 1929.7606426908496
INFO:root:current train perplexity4.5843634605407715
INFO:root:current mean train loss 1929.7760330896092
INFO:root:current train perplexity4.5864458084106445
INFO:root:current mean train loss 1931.934346559804
INFO:root:current train perplexity4.590862274169922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.72s/it]
INFO:root:final mean train loss: 1931.1771617279592
INFO:root:final train perplexity: 4.590921401977539
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2193.3651490989305
INFO:root:eval perplexity: 5.8997111320495605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2736.4320799915504
INFO:root:eval perplexity: 9.484881401062012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/48
 24%|â–ˆâ–ˆâ–       | 48/200 [8:07:36<25:49:37, 611.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1890.866064453125
INFO:root:current train perplexity4.545539379119873
INFO:root:current mean train loss 1904.6405220363451
INFO:root:current train perplexity4.530570030212402
INFO:root:current mean train loss 1916.2950013626453
INFO:root:current train perplexity4.545127868652344
INFO:root:current mean train loss 1916.8433295355903
INFO:root:current train perplexity4.549050807952881
INFO:root:current mean train loss 1914.2285038591867
INFO:root:current train perplexity4.538079738616943
INFO:root:current mean train loss 1914.1233635467233
INFO:root:current train perplexity4.538948059082031
INFO:root:current mean train loss 1910.8707072932546
INFO:root:current train perplexity4.52986478805542
INFO:root:current mean train loss 1913.009486143739
INFO:root:current train perplexity4.534215450286865
INFO:root:current mean train loss 1916.7705328256807
INFO:root:current train perplexity4.54311466217041
INFO:root:current mean train loss 1915.623616936689
INFO:root:current train perplexity4.546187877655029
INFO:root:current mean train loss 1915.194031663716
INFO:root:current train perplexity4.548281192779541
INFO:root:current mean train loss 1915.8811040630255
INFO:root:current train perplexity4.550540447235107
INFO:root:current mean train loss 1916.5482361593365
INFO:root:current train perplexity4.54892635345459
INFO:root:current mean train loss 1915.9288940893832
INFO:root:current train perplexity4.550204753875732
INFO:root:current mean train loss 1917.659072541685
INFO:root:current train perplexity4.553110599517822
INFO:root:current mean train loss 1919.8425190639182
INFO:root:current train perplexity4.557531833648682
INFO:root:current mean train loss 1919.58742501935
INFO:root:current train perplexity4.557943820953369
INFO:root:current mean train loss 1920.6793647788356
INFO:root:current train perplexity4.56006383895874
INFO:root:current mean train loss 1921.7241092566287
INFO:root:current train perplexity4.561184406280518
INFO:root:current mean train loss 1923.22431685246
INFO:root:current train perplexity4.561773777008057

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.17s/it]
INFO:root:final mean train loss: 1923.264008661021
INFO:root:final train perplexity: 4.562340259552002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.72s/it]
INFO:root:eval mean loss: 2196.4654610275375
INFO:root:eval perplexity: 5.914529323577881
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2738.9037765611147
INFO:root:eval perplexity: 9.504175186157227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/49
 24%|â–ˆâ–ˆâ–       | 49/200 [8:17:47<25:38:45, 611.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1893.0943984985352
INFO:root:current train perplexity4.419301509857178
INFO:root:current mean train loss 1889.182685620857
INFO:root:current train perplexity4.478137016296387
INFO:root:current mean train loss 1897.1823998812972
INFO:root:current train perplexity4.473410606384277
INFO:root:current mean train loss 1900.240758689053
INFO:root:current train perplexity4.490686893463135
INFO:root:current mean train loss 1898.5376400417751
INFO:root:current train perplexity4.487288951873779
INFO:root:current mean train loss 1899.948961071502
INFO:root:current train perplexity4.494527339935303
INFO:root:current mean train loss 1899.958427525774
INFO:root:current train perplexity4.495441436767578
INFO:root:current mean train loss 1903.593850724684
INFO:root:current train perplexity4.500932216644287
INFO:root:current mean train loss 1905.3033993060772
INFO:root:current train perplexity4.507318496704102
INFO:root:current mean train loss 1905.9103793033714
INFO:root:current train perplexity4.508082866668701
INFO:root:current mean train loss 1905.6934635842492
INFO:root:current train perplexity4.509479522705078
INFO:root:current mean train loss 1906.6203260657644
INFO:root:current train perplexity4.512844085693359
INFO:root:current mean train loss 1908.329985284186
INFO:root:current train perplexity4.5161662101745605
INFO:root:current mean train loss 1908.7353262686515
INFO:root:current train perplexity4.517226696014404
INFO:root:current mean train loss 1909.7036612739776
INFO:root:current train perplexity4.522446632385254
INFO:root:current mean train loss 1910.8335568898651
INFO:root:current train perplexity4.524449348449707
INFO:root:current mean train loss 1911.776398079068
INFO:root:current train perplexity4.524557590484619
INFO:root:current mean train loss 1912.6348515393827
INFO:root:current train perplexity4.52840518951416
INFO:root:current mean train loss 1914.1757414705369
INFO:root:current train perplexity4.530923366546631
INFO:root:current mean train loss 1915.0487886353803
INFO:root:current train perplexity4.5313334465026855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.31s/it]
INFO:root:final mean train loss: 1914.6938243563945
INFO:root:final train perplexity: 4.531586647033691
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2198.4458349782526
INFO:root:eval perplexity: 5.924017429351807
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 2741.9054020009144
INFO:root:eval perplexity: 9.527661323547363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [8:27:59<25:28:59, 611.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1886.3847805723851
INFO:root:current train perplexity4.403451442718506
INFO:root:current mean train loss 1904.0431448917261
INFO:root:current train perplexity4.453601837158203
INFO:root:current mean train loss 1896.3283902210403
INFO:root:current train perplexity4.4558000564575195
INFO:root:current mean train loss 1898.3413516156652
INFO:root:current train perplexity4.465045928955078
INFO:root:current mean train loss 1900.8677258056098
INFO:root:current train perplexity4.46945333480835
INFO:root:current mean train loss 1898.707270943619
INFO:root:current train perplexity4.464583396911621
INFO:root:current mean train loss 1902.0153217991622
INFO:root:current train perplexity4.47466516494751
INFO:root:current mean train loss 1903.9114268243075
INFO:root:current train perplexity4.47962760925293
INFO:root:current mean train loss 1906.5524863522803
INFO:root:current train perplexity4.482478141784668
INFO:root:current mean train loss 1904.285944368867
INFO:root:current train perplexity4.481081485748291
INFO:root:current mean train loss 1903.1879484833025
INFO:root:current train perplexity4.484755516052246
INFO:root:current mean train loss 1903.7587751449971
INFO:root:current train perplexity4.486264705657959
INFO:root:current mean train loss 1904.1226366835656
INFO:root:current train perplexity4.488389492034912
INFO:root:current mean train loss 1904.39192008548
INFO:root:current train perplexity4.487483978271484
INFO:root:current mean train loss 1904.933265196364
INFO:root:current train perplexity4.490005970001221
INFO:root:current mean train loss 1905.5826396314155
INFO:root:current train perplexity4.4897780418396
INFO:root:current mean train loss 1905.8014529698396
INFO:root:current train perplexity4.492445945739746
INFO:root:current mean train loss 1906.1106253657224
INFO:root:current train perplexity4.4941606521606445
INFO:root:current mean train loss 1905.859835685041
INFO:root:current train perplexity4.497281074523926
INFO:root:current mean train loss 1906.3351703798423
INFO:root:current train perplexity4.499887943267822

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it]
INFO:root:final mean train loss: 1906.0879551074268
INFO:root:final train perplexity: 4.500913619995117
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2195.587936076712
INFO:root:eval perplexity: 5.910333156585693
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2742.6411591658357
INFO:root:eval perplexity: 9.533421516418457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [8:38:10<25:18:37, 611.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1892.5908129142992
INFO:root:current train perplexity4.4119391441345215
INFO:root:current mean train loss 1898.596397307982
INFO:root:current train perplexity4.434236526489258
INFO:root:current mean train loss 1899.0019297205415
INFO:root:current train perplexity4.439817428588867
INFO:root:current mean train loss 1895.0817294094732
INFO:root:current train perplexity4.440483093261719
INFO:root:current mean train loss 1897.3100378994266
INFO:root:current train perplexity4.447214603424072
INFO:root:current mean train loss 1898.7697016308248
INFO:root:current train perplexity4.4468889236450195
INFO:root:current mean train loss 1894.7767299159511
INFO:root:current train perplexity4.449320316314697
INFO:root:current mean train loss 1894.4544864186419
INFO:root:current train perplexity4.4508891105651855
INFO:root:current mean train loss 1895.833013873728
INFO:root:current train perplexity4.4566264152526855
INFO:root:current mean train loss 1896.7064307550465
INFO:root:current train perplexity4.457435131072998
INFO:root:current mean train loss 1898.7132152679042
INFO:root:current train perplexity4.458667755126953
INFO:root:current mean train loss 1897.5069210517058
INFO:root:current train perplexity4.460041522979736
INFO:root:current mean train loss 1897.4977317677478
INFO:root:current train perplexity4.461296558380127
INFO:root:current mean train loss 1896.352978462007
INFO:root:current train perplexity4.463146686553955
INFO:root:current mean train loss 1897.0927800989086
INFO:root:current train perplexity4.465251445770264
INFO:root:current mean train loss 1898.2683442993944
INFO:root:current train perplexity4.467088222503662
INFO:root:current mean train loss 1898.6493890036484
INFO:root:current train perplexity4.46882963180542
INFO:root:current mean train loss 1899.026188298503
INFO:root:current train perplexity4.471002101898193
INFO:root:current mean train loss 1898.9425315080093
INFO:root:current train perplexity4.472739219665527
INFO:root:current mean train loss 1898.4464249169475
INFO:root:current train perplexity4.471647262573242

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.76s/it]
INFO:root:final mean train loss: 1897.8788929841642
INFO:root:final train perplexity: 4.471848011016846
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2200.9298433344416
INFO:root:eval perplexity: 5.935935974121094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 2748.6165355925864
INFO:root:eval perplexity: 9.580374717712402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [8:48:22<25:08:18, 611.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1906.7980942323984
INFO:root:current train perplexity4.447806358337402
INFO:root:current mean train loss 1889.7454307014173
INFO:root:current train perplexity4.421980857849121
INFO:root:current mean train loss 1889.1607234671765
INFO:root:current train perplexity4.409780025482178
INFO:root:current mean train loss 1887.3461254309113
INFO:root:current train perplexity4.417257785797119
INFO:root:current mean train loss 1888.489274745649
INFO:root:current train perplexity4.421090602874756
INFO:root:current mean train loss 1887.1965455567245
INFO:root:current train perplexity4.422319412231445
INFO:root:current mean train loss 1888.963059879152
INFO:root:current train perplexity4.4244585037231445
INFO:root:current mean train loss 1889.7695407599476
INFO:root:current train perplexity4.432462692260742
INFO:root:current mean train loss 1890.4030728539956
INFO:root:current train perplexity4.434235095977783
INFO:root:current mean train loss 1890.594912710413
INFO:root:current train perplexity4.435480117797852
INFO:root:current mean train loss 1890.7233049246522
INFO:root:current train perplexity4.4373369216918945
INFO:root:current mean train loss 1888.9237552749235
INFO:root:current train perplexity4.436097621917725
INFO:root:current mean train loss 1889.1119541753947
INFO:root:current train perplexity4.435762882232666
INFO:root:current mean train loss 1889.1302427566318
INFO:root:current train perplexity4.437321662902832
INFO:root:current mean train loss 1889.9058508638266
INFO:root:current train perplexity4.4391326904296875
INFO:root:current mean train loss 1889.5109025059962
INFO:root:current train perplexity4.439233779907227
INFO:root:current mean train loss 1889.6956059619633
INFO:root:current train perplexity4.441080093383789
INFO:root:current mean train loss 1890.511712999071
INFO:root:current train perplexity4.442820072174072
INFO:root:current mean train loss 1891.1963951852306
INFO:root:current train perplexity4.446190357208252
INFO:root:current mean train loss 1890.8526062842757
INFO:root:current train perplexity4.447120189666748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.38s/it]
INFO:root:final mean train loss: 1890.8526062842757
INFO:root:final train perplexity: 4.447120189666748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2198.9574264634584
INFO:root:eval perplexity: 5.926469802856445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 2745.5171162490306
INFO:root:eval perplexity: 9.555991172790527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [8:58:34<24:58:38, 611.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1856.1889331054688
INFO:root:current train perplexity4.325908184051514
INFO:root:current mean train loss 1865.2217517089844
INFO:root:current train perplexity4.367989540100098
INFO:root:current mean train loss 1870.361914876302
INFO:root:current train perplexity4.3745903968811035
INFO:root:current mean train loss 1866.770291748047
INFO:root:current train perplexity4.377220153808594
INFO:root:current mean train loss 1871.1632788085938
INFO:root:current train perplexity4.381939888000488
INFO:root:current mean train loss 1874.2268756103515
INFO:root:current train perplexity4.384704113006592
INFO:root:current mean train loss 1875.031868373326
INFO:root:current train perplexity4.388268947601318
INFO:root:current mean train loss 1874.9616671752929
INFO:root:current train perplexity4.38709020614624
INFO:root:current mean train loss 1875.5131580946181
INFO:root:current train perplexity4.394038677215576
INFO:root:current mean train loss 1876.3968565673829
INFO:root:current train perplexity4.397790908813477
INFO:root:current mean train loss 1877.4934194113991
INFO:root:current train perplexity4.401744842529297
INFO:root:current mean train loss 1879.4247364298503
INFO:root:current train perplexity4.404603481292725
INFO:root:current mean train loss 1879.6152607609677
INFO:root:current train perplexity4.402271270751953
INFO:root:current mean train loss 1880.8444936697824
INFO:root:current train perplexity4.406988143920898
INFO:root:current mean train loss 1881.1115230305988
INFO:root:current train perplexity4.408959865570068
INFO:root:current mean train loss 1881.3912744903564
INFO:root:current train perplexity4.4100728034973145
INFO:root:current mean train loss 1881.8884953038832
INFO:root:current train perplexity4.41020393371582
INFO:root:current mean train loss 1882.3849280463326
INFO:root:current train perplexity4.413129806518555
INFO:root:current mean train loss 1882.613812448602
INFO:root:current train perplexity4.415526866912842

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.39s/it]
INFO:root:final mean train loss: 1882.45140348849
INFO:root:final train perplexity: 4.4177327156066895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2198.831939480829
INFO:root:eval perplexity: 5.925868511199951
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2751.3999928143007
INFO:root:eval perplexity: 9.602320671081543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [9:08:46<24:48:43, 611.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1864.0220731847426
INFO:root:current train perplexity4.3550920486450195
INFO:root:current mean train loss 1852.8291829427083
INFO:root:current train perplexity4.328289031982422
INFO:root:current mean train loss 1860.2768076531897
INFO:root:current train perplexity4.3547444343566895
INFO:root:current mean train loss 1862.729147541034
INFO:root:current train perplexity4.370841026306152
INFO:root:current mean train loss 1862.8057808635904
INFO:root:current train perplexity4.371150016784668
INFO:root:current mean train loss 1863.0226625778228
INFO:root:current train perplexity4.367910385131836
INFO:root:current mean train loss 1862.6149000170938
INFO:root:current train perplexity4.361735820770264
INFO:root:current mean train loss 1864.5844215808054
INFO:root:current train perplexity4.365188121795654
INFO:root:current mean train loss 1867.70087379633
INFO:root:current train perplexity4.364475250244141
INFO:root:current mean train loss 1868.4324551814254
INFO:root:current train perplexity4.367254257202148
INFO:root:current mean train loss 1867.4571018275258
INFO:root:current train perplexity4.369078159332275
INFO:root:current mean train loss 1868.1385694758212
INFO:root:current train perplexity4.3723907470703125
INFO:root:current mean train loss 1869.0475127346317
INFO:root:current train perplexity4.376883506774902
INFO:root:current mean train loss 1869.9771338298453
INFO:root:current train perplexity4.3779706954956055
INFO:root:current mean train loss 1869.955258947573
INFO:root:current train perplexity4.376749515533447
INFO:root:current mean train loss 1870.9623299223385
INFO:root:current train perplexity4.378456115722656
INFO:root:current mean train loss 1871.1721114404568
INFO:root:current train perplexity4.380643367767334
INFO:root:current mean train loss 1872.3888050361504
INFO:root:current train perplexity4.38284158706665
INFO:root:current mean train loss 1872.943553196052
INFO:root:current train perplexity4.38714599609375
INFO:root:current mean train loss 1874.3221536794551
INFO:root:current train perplexity4.389316558837891

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.14s/it]
INFO:root:final mean train loss: 1874.4537810894558
INFO:root:final train perplexity: 4.389937400817871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2202.1893522654864
INFO:root:eval perplexity: 5.941990375518799
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 2754.670166881372
INFO:root:eval perplexity: 9.62817096710205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [9:18:59<24:39:14, 612.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1882.836145737592
INFO:root:current train perplexity4.375641345977783
INFO:root:current mean train loss 1864.8765623178056
INFO:root:current train perplexity4.354000568389893
INFO:root:current mean train loss 1862.8277170556223
INFO:root:current train perplexity4.341935157775879
INFO:root:current mean train loss 1867.0904084165652
INFO:root:current train perplexity4.346573352813721
INFO:root:current mean train loss 1864.585203953053
INFO:root:current train perplexity4.350264549255371
INFO:root:current mean train loss 1865.6055462440747
INFO:root:current train perplexity4.354551792144775
INFO:root:current mean train loss 1867.970155926533
INFO:root:current train perplexity4.358779430389404
INFO:root:current mean train loss 1869.0213032652312
INFO:root:current train perplexity4.357908725738525
INFO:root:current mean train loss 1867.1147146247845
INFO:root:current train perplexity4.3582000732421875
INFO:root:current mean train loss 1867.8530124443757
INFO:root:current train perplexity4.357422828674316
INFO:root:current mean train loss 1867.803791924187
INFO:root:current train perplexity4.36271858215332
INFO:root:current mean train loss 1866.4698712384259
INFO:root:current train perplexity4.360978126525879
INFO:root:current mean train loss 1865.9493695078252
INFO:root:current train perplexity4.359344005584717
INFO:root:current mean train loss 1865.601462574377
INFO:root:current train perplexity4.360574722290039
INFO:root:current mean train loss 1865.4711427994519
INFO:root:current train perplexity4.361181259155273
INFO:root:current mean train loss 1865.7572862607715
INFO:root:current train perplexity4.362485408782959
INFO:root:current mean train loss 1866.0612423171904
INFO:root:current train perplexity4.36140251159668
INFO:root:current mean train loss 1866.63821738484
INFO:root:current train perplexity4.362730979919434
INFO:root:current mean train loss 1866.970611672105
INFO:root:current train perplexity4.364863872528076
INFO:root:current mean train loss 1867.75645457823
INFO:root:current train perplexity4.364192485809326

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.35s/it]
INFO:root:final mean train loss: 1866.8410935702495
INFO:root:final train perplexity: 4.363641262054443
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 2203.4276053787125
INFO:root:eval perplexity: 5.9479451179504395
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2758.348464857602
INFO:root:eval perplexity: 9.657331466674805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [9:29:11<24:28:59, 612.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1847.6854822495404
INFO:root:current train perplexity4.316748142242432
INFO:root:current mean train loss 1857.454558315656
INFO:root:current train perplexity4.33248233795166
INFO:root:current mean train loss 1854.7888305177726
INFO:root:current train perplexity4.322362422943115
INFO:root:current mean train loss 1853.088923875423
INFO:root:current train perplexity4.320992469787598
INFO:root:current mean train loss 1850.9625292860483
INFO:root:current train perplexity4.321073532104492
INFO:root:current mean train loss 1852.0099661216113
INFO:root:current train perplexity4.317314624786377
INFO:root:current mean train loss 1850.0169990879417
INFO:root:current train perplexity4.315371036529541
INFO:root:current mean train loss 1853.570034062656
INFO:root:current train perplexity4.319965362548828
INFO:root:current mean train loss 1853.1660326947617
INFO:root:current train perplexity4.321027755737305
INFO:root:current mean train loss 1853.3497329856318
INFO:root:current train perplexity4.322543621063232
INFO:root:current mean train loss 1855.3946881643599
INFO:root:current train perplexity4.32443904876709
INFO:root:current mean train loss 1856.6017049330405
INFO:root:current train perplexity4.326574325561523
INFO:root:current mean train loss 1856.024997189748
INFO:root:current train perplexity4.327535152435303
INFO:root:current mean train loss 1856.113051114481
INFO:root:current train perplexity4.32968282699585
INFO:root:current mean train loss 1857.508623834317
INFO:root:current train perplexity4.3314924240112305
INFO:root:current mean train loss 1858.9239130469002
INFO:root:current train perplexity4.333581447601318
INFO:root:current mean train loss 1858.7490085021861
INFO:root:current train perplexity4.332653999328613
INFO:root:current mean train loss 1858.9156276630988
INFO:root:current train perplexity4.334265232086182
INFO:root:current mean train loss 1858.398911074778
INFO:root:current train perplexity4.333003520965576
INFO:root:current mean train loss 1859.2886682585654
INFO:root:current train perplexity4.336388111114502

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.85s/it]
INFO:root:final mean train loss: 1859.045527271111
INFO:root:final train perplexity: 4.33687686920166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2203.4998748995736
INFO:root:eval perplexity: 5.948293685913086
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2757.7432138429467
INFO:root:eval perplexity: 9.652527809143066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [9:39:22<24:18:16, 611.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1855.801466997932
INFO:root:current train perplexity4.262737274169922
INFO:root:current mean train loss 1848.4900890531994
INFO:root:current train perplexity4.2574052810668945
INFO:root:current mean train loss 1846.2154349711404
INFO:root:current train perplexity4.277237415313721
INFO:root:current mean train loss 1847.0628234199855
INFO:root:current train perplexity4.284322261810303
INFO:root:current mean train loss 1847.3185419588008
INFO:root:current train perplexity4.285197734832764
INFO:root:current mean train loss 1847.8605378916566
INFO:root:current train perplexity4.289312839508057
INFO:root:current mean train loss 1846.9656474404705
INFO:root:current train perplexity4.286013603210449
INFO:root:current mean train loss 1844.5766933759053
INFO:root:current train perplexity4.288355827331543
INFO:root:current mean train loss 1846.7913310670633
INFO:root:current train perplexity4.290465831756592
INFO:root:current mean train loss 1848.1140503686322
INFO:root:current train perplexity4.292545795440674
INFO:root:current mean train loss 1850.8019400035844
INFO:root:current train perplexity4.297164440155029
INFO:root:current mean train loss 1849.6428255055048
INFO:root:current train perplexity4.294436454772949
INFO:root:current mean train loss 1849.7295393597817
INFO:root:current train perplexity4.29609489440918
INFO:root:current mean train loss 1849.7524572004352
INFO:root:current train perplexity4.299625873565674
INFO:root:current mean train loss 1849.592648041021
INFO:root:current train perplexity4.3009033203125
INFO:root:current mean train loss 1851.2004905233578
INFO:root:current train perplexity4.305272102355957
INFO:root:current mean train loss 1852.1033407892826
INFO:root:current train perplexity4.309131622314453
INFO:root:current mean train loss 1851.6724830731007
INFO:root:current train perplexity4.308811187744141
INFO:root:current mean train loss 1852.122740326908
INFO:root:current train perplexity4.3089518547058105
INFO:root:current mean train loss 1851.5217873798154
INFO:root:current train perplexity4.309087753295898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.15s/it]
INFO:root:final mean train loss: 1851.2352928050047
INFO:root:final train perplexity: 4.310227870941162
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2206.872293242326
INFO:root:eval perplexity: 5.964548587799072
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2763.9943955874614
INFO:root:eval perplexity: 9.702262878417969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [9:49:32<24:06:33, 611.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1819.6532082950368
INFO:root:current train perplexity4.246349334716797
INFO:root:current mean train loss 1833.7941082928633
INFO:root:current train perplexity4.254175662994385
INFO:root:current mean train loss 1829.2189817194353
INFO:root:current train perplexity4.254404067993164
INFO:root:current mean train loss 1828.9579742035307
INFO:root:current train perplexity4.25799036026001
INFO:root:current mean train loss 1828.2682300056379
INFO:root:current train perplexity4.268248558044434
INFO:root:current mean train loss 1832.1156492053954
INFO:root:current train perplexity4.2691121101379395
INFO:root:current mean train loss 1836.3134760278856
INFO:root:current train perplexity4.272006988525391
INFO:root:current mean train loss 1835.8638244240146
INFO:root:current train perplexity4.271197319030762
INFO:root:current mean train loss 1838.9682258562852
INFO:root:current train perplexity4.271935939788818
INFO:root:current mean train loss 1840.173844359732
INFO:root:current train perplexity4.274345874786377
INFO:root:current mean train loss 1839.3082308017713
INFO:root:current train perplexity4.275784015655518
INFO:root:current mean train loss 1840.3221911466574
INFO:root:current train perplexity4.279445648193359
INFO:root:current mean train loss 1840.9752536402602
INFO:root:current train perplexity4.281834125518799
INFO:root:current mean train loss 1841.9805818302966
INFO:root:current train perplexity4.280272006988525
INFO:root:current mean train loss 1842.4906060112846
INFO:root:current train perplexity4.280982971191406
INFO:root:current mean train loss 1843.3261728762077
INFO:root:current train perplexity4.28331995010376
INFO:root:current mean train loss 1842.9677159883856
INFO:root:current train perplexity4.283869743347168
INFO:root:current mean train loss 1844.1512048374036
INFO:root:current train perplexity4.287123203277588
INFO:root:current mean train loss 1844.1437593252654
INFO:root:current train perplexity4.28788948059082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.49s/it]
INFO:root:final mean train loss: 1844.7110581544691
INFO:root:final train perplexity: 4.288092136383057
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2206.174537171709
INFO:root:eval perplexity: 5.961183547973633
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2760.7696901145555
INFO:root:eval perplexity: 9.6765718460083
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [9:59:42<23:55:32, 610.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1757.6282958984375
INFO:root:current train perplexity3.9059560298919678
INFO:root:current mean train loss 1833.112018660003
INFO:root:current train perplexity4.234673023223877
INFO:root:current mean train loss 1829.5773551109994
INFO:root:current train perplexity4.222592353820801
INFO:root:current mean train loss 1836.0539385056654
INFO:root:current train perplexity4.237176418304443
INFO:root:current mean train loss 1834.580979684099
INFO:root:current train perplexity4.238764762878418
INFO:root:current mean train loss 1834.8154695670444
INFO:root:current train perplexity4.242311477661133
INFO:root:current mean train loss 1834.1366289208497
INFO:root:current train perplexity4.244499683380127
INFO:root:current mean train loss 1833.6713999343394
INFO:root:current train perplexity4.248992919921875
INFO:root:current mean train loss 1834.8832965194435
INFO:root:current train perplexity4.24902868270874
INFO:root:current mean train loss 1835.11913602368
INFO:root:current train perplexity4.251697063446045
INFO:root:current mean train loss 1836.762521953164
INFO:root:current train perplexity4.255116939544678
INFO:root:current mean train loss 1835.6985057131565
INFO:root:current train perplexity4.255221843719482
INFO:root:current mean train loss 1836.0091762955296
INFO:root:current train perplexity4.254542350769043
INFO:root:current mean train loss 1838.1824596774193
INFO:root:current train perplexity4.256155014038086
INFO:root:current mean train loss 1838.671653497236
INFO:root:current train perplexity4.257200717926025
INFO:root:current mean train loss 1838.586662607409
INFO:root:current train perplexity4.257925987243652
INFO:root:current mean train loss 1838.6299793562491
INFO:root:current train perplexity4.260095119476318
INFO:root:current mean train loss 1837.936304112579
INFO:root:current train perplexity4.260462760925293
INFO:root:current mean train loss 1837.4332591743766
INFO:root:current train perplexity4.260890960693359
INFO:root:current mean train loss 1837.3109858018493
INFO:root:current train perplexity4.261713027954102

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.81s/it]
INFO:root:final mean train loss: 1836.9608201081262
INFO:root:final train perplexity: 4.261943817138672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2209.9450480143228
INFO:root:eval perplexity: 5.97939920425415
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2768.5161544042276
INFO:root:eval perplexity: 9.738397598266602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [10:09:50<23:43:16, 609.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1831.1993600945723
INFO:root:current train perplexity4.211828231811523
INFO:root:current mean train loss 1812.0471129858192
INFO:root:current train perplexity4.17425537109375
INFO:root:current mean train loss 1816.8643252577413
INFO:root:current train perplexity4.173939228057861
INFO:root:current mean train loss 1823.231925940439
INFO:root:current train perplexity4.194223880767822
INFO:root:current mean train loss 1821.4150754796576
INFO:root:current train perplexity4.196235656738281
INFO:root:current mean train loss 1821.105619515068
INFO:root:current train perplexity4.200997829437256
INFO:root:current mean train loss 1823.8722752091958
INFO:root:current train perplexity4.208103656768799
INFO:root:current mean train loss 1823.0149455488308
INFO:root:current train perplexity4.209589958190918
INFO:root:current mean train loss 1824.1536145332532
INFO:root:current train perplexity4.2137346267700195
INFO:root:current mean train loss 1825.6976156307383
INFO:root:current train perplexity4.216949939727783
INFO:root:current mean train loss 1824.1284661260274
INFO:root:current train perplexity4.2173614501953125
INFO:root:current mean train loss 1824.5930795405357
INFO:root:current train perplexity4.220438480377197
INFO:root:current mean train loss 1825.0555189600532
INFO:root:current train perplexity4.220373630523682
INFO:root:current mean train loss 1825.7515862662292
INFO:root:current train perplexity4.2245259284973145
INFO:root:current mean train loss 1827.9112055041237
INFO:root:current train perplexity4.2299628257751465
INFO:root:current mean train loss 1828.4570242584812
INFO:root:current train perplexity4.235414028167725
INFO:root:current mean train loss 1827.7176668992965
INFO:root:current train perplexity4.23553466796875
INFO:root:current mean train loss 1828.861008711233
INFO:root:current train perplexity4.235264778137207
INFO:root:current mean train loss 1829.342064839133
INFO:root:current train perplexity4.235098361968994
INFO:root:current mean train loss 1829.6087953727526
INFO:root:current train perplexity4.23514986038208

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.89s/it]
INFO:root:final mean train loss: 1829.502721004515
INFO:root:final train perplexity: 4.236932754516602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.50s/it]
INFO:root:eval mean loss: 2214.0483277232934
INFO:root:eval perplexity: 5.99928617477417
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2773.883123303136
INFO:root:eval perplexity: 9.78145980834961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [10:19:57<23:31:05, 609.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1794.3995463053386
INFO:root:current train perplexity4.168371677398682
INFO:root:current mean train loss 1790.4120653937844
INFO:root:current train perplexity4.169073581695557
INFO:root:current mean train loss 1800.475478867353
INFO:root:current train perplexity4.170621395111084
INFO:root:current mean train loss 1802.5777646019346
INFO:root:current train perplexity4.174961090087891
INFO:root:current mean train loss 1802.9664284242403
INFO:root:current train perplexity4.171329975128174
INFO:root:current mean train loss 1809.2583073858004
INFO:root:current train perplexity4.170142650604248
INFO:root:current mean train loss 1809.9775459721404
INFO:root:current train perplexity4.179709434509277
INFO:root:current mean train loss 1811.7800180186396
INFO:root:current train perplexity4.178446292877197
INFO:root:current mean train loss 1813.1453943571976
INFO:root:current train perplexity4.184213638305664
INFO:root:current mean train loss 1812.5255199986645
INFO:root:current train perplexity4.184295654296875
INFO:root:current mean train loss 1814.402986622225
INFO:root:current train perplexity4.190685272216797
INFO:root:current mean train loss 1815.577315531986
INFO:root:current train perplexity4.192603588104248
INFO:root:current mean train loss 1816.9128646109868
INFO:root:current train perplexity4.195405006408691
INFO:root:current mean train loss 1817.9463952344336
INFO:root:current train perplexity4.200927257537842
INFO:root:current mean train loss 1817.6664835045265
INFO:root:current train perplexity4.2015252113342285
INFO:root:current mean train loss 1819.7372127374013
INFO:root:current train perplexity4.20410680770874
INFO:root:current mean train loss 1820.2242459994366
INFO:root:current train perplexity4.205729007720947
INFO:root:current mean train loss 1820.5842704245572
INFO:root:current train perplexity4.208078861236572
INFO:root:current mean train loss 1820.8802121230979
INFO:root:current train perplexity4.208680152893066
INFO:root:current mean train loss 1821.8055093938653
INFO:root:current train perplexity4.211514472961426

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.16s/it]
INFO:root:final mean train loss: 1822.2327297285717
INFO:root:final train perplexity: 4.212693691253662
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2216.1557725405864
INFO:root:eval perplexity: 6.009526252746582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2777.1283482761246
INFO:root:eval perplexity: 9.80759334564209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [10:30:05<23:20:26, 608.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1795.5768766583137
INFO:root:current train perplexity4.1023268699646
INFO:root:current mean train loss 1804.5423895143995
INFO:root:current train perplexity4.1540350914001465
INFO:root:current mean train loss 1809.8103495360363
INFO:root:current train perplexity4.1676154136657715
INFO:root:current mean train loss 1811.9389973497255
INFO:root:current train perplexity4.167019367218018
INFO:root:current mean train loss 1809.8047634907905
INFO:root:current train perplexity4.15823221206665
INFO:root:current mean train loss 1806.5867394555974
INFO:root:current train perplexity4.162498950958252
INFO:root:current mean train loss 1807.7421927342555
INFO:root:current train perplexity4.156677722930908
INFO:root:current mean train loss 1809.370440114542
INFO:root:current train perplexity4.159702301025391
INFO:root:current mean train loss 1810.0975095652752
INFO:root:current train perplexity4.161334037780762
INFO:root:current mean train loss 1810.105240364515
INFO:root:current train perplexity4.166205883026123
INFO:root:current mean train loss 1810.8248758198301
INFO:root:current train perplexity4.171242713928223
INFO:root:current mean train loss 1810.5751266016302
INFO:root:current train perplexity4.17087984085083
INFO:root:current mean train loss 1810.573681562687
INFO:root:current train perplexity4.173556327819824
INFO:root:current mean train loss 1811.8286956538998
INFO:root:current train perplexity4.1776041984558105
INFO:root:current mean train loss 1812.5439768172262
INFO:root:current train perplexity4.178814888000488
INFO:root:current mean train loss 1813.2749326058686
INFO:root:current train perplexity4.181791305541992
INFO:root:current mean train loss 1813.0835545811592
INFO:root:current train perplexity4.1820220947265625
INFO:root:current mean train loss 1814.814152301412
INFO:root:current train perplexity4.187007427215576
INFO:root:current mean train loss 1815.0372582573023
INFO:root:current train perplexity4.18633508682251
INFO:root:current mean train loss 1814.8956450712847
INFO:root:current train perplexity4.186659336090088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.33s/it]
INFO:root:final mean train loss: 1814.397166657556
INFO:root:final train perplexity: 4.186722755432129
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2217.5957585328015
INFO:root:eval perplexity: 6.0165324211120605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2779.7051218452184
INFO:root:eval perplexity: 9.828391075134277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [10:40:13<23:09:22, 608.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1781.5800519670759
INFO:root:current train perplexity4.064787864685059
INFO:root:current mean train loss 1785.149755859375
INFO:root:current train perplexity4.097991466522217
INFO:root:current mean train loss 1796.608396629051
INFO:root:current train perplexity4.127151966094971
INFO:root:current mean train loss 1798.4361714131123
INFO:root:current train perplexity4.132620811462402
INFO:root:current mean train loss 1799.1518936481882
INFO:root:current train perplexity4.136232376098633
INFO:root:current mean train loss 1801.2490701240406
INFO:root:current train perplexity4.133927345275879
INFO:root:current mean train loss 1800.3104271732159
INFO:root:current train perplexity4.135249137878418
INFO:root:current mean train loss 1803.5398768833704
INFO:root:current train perplexity4.135990619659424
INFO:root:current mean train loss 1803.6348468648976
INFO:root:current train perplexity4.1421027183532715
INFO:root:current mean train loss 1804.9711385510632
INFO:root:current train perplexity4.144896030426025
INFO:root:current mean train loss 1805.2740967937718
INFO:root:current train perplexity4.144899845123291
INFO:root:current mean train loss 1805.1429382845886
INFO:root:current train perplexity4.148514270782471
INFO:root:current mean train loss 1806.048681640625
INFO:root:current train perplexity4.150838851928711
INFO:root:current mean train loss 1806.3366255488709
INFO:root:current train perplexity4.152857780456543
INFO:root:current mean train loss 1807.1888685161564
INFO:root:current train perplexity4.1557769775390625
INFO:root:current mean train loss 1808.4781655864351
INFO:root:current train perplexity4.160910606384277
INFO:root:current mean train loss 1807.7808880286302
INFO:root:current train perplexity4.161893367767334
INFO:root:current mean train loss 1807.7109368103372
INFO:root:current train perplexity4.162850379943848
INFO:root:current mean train loss 1807.4096500172334
INFO:root:current train perplexity4.164791107177734
INFO:root:current mean train loss 1808.0229734469185
INFO:root:current train perplexity4.164425373077393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.62s/it]
INFO:root:final mean train loss: 1807.5101018262162
INFO:root:final train perplexity: 4.164029121398926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2220.012409616024
INFO:root:eval perplexity: 6.0283098220825195
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2785.843238343584
INFO:root:eval perplexity: 9.878113746643066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [10:50:21<22:58:45, 608.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1781.7424933773348
INFO:root:current train perplexity4.086228370666504
INFO:root:current mean train loss 1783.8683494527072
INFO:root:current train perplexity4.106582164764404
INFO:root:current mean train loss 1786.8067171847779
INFO:root:current train perplexity4.117650508880615
INFO:root:current mean train loss 1791.2913370452802
INFO:root:current train perplexity4.120581150054932
INFO:root:current mean train loss 1793.1040044075655
INFO:root:current train perplexity4.117589950561523
INFO:root:current mean train loss 1791.0897104500507
INFO:root:current train perplexity4.112512588500977
INFO:root:current mean train loss 1794.1301784820778
INFO:root:current train perplexity4.11714506149292
INFO:root:current mean train loss 1795.4690822421974
INFO:root:current train perplexity4.119473934173584
INFO:root:current mean train loss 1797.4277795148676
INFO:root:current train perplexity4.119787216186523
INFO:root:current mean train loss 1797.2046082717911
INFO:root:current train perplexity4.126253128051758
INFO:root:current mean train loss 1797.2393149732995
INFO:root:current train perplexity4.128512382507324
INFO:root:current mean train loss 1796.8743863575783
INFO:root:current train perplexity4.133159637451172
INFO:root:current mean train loss 1796.2759488097622
INFO:root:current train perplexity4.133650302886963
INFO:root:current mean train loss 1796.283571096144
INFO:root:current train perplexity4.131990909576416
INFO:root:current mean train loss 1797.1633093089326
INFO:root:current train perplexity4.133608818054199
INFO:root:current mean train loss 1798.018351467736
INFO:root:current train perplexity4.133360385894775
INFO:root:current mean train loss 1798.4144153244479
INFO:root:current train perplexity4.134088039398193
INFO:root:current mean train loss 1799.0652291287772
INFO:root:current train perplexity4.137104511260986
INFO:root:current mean train loss 1799.9777722057995
INFO:root:current train perplexity4.139751434326172

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it]
INFO:root:final mean train loss: 1800.1672087191814
INFO:root:final train perplexity: 4.139967918395996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2221.4734276304853
INFO:root:eval perplexity: 6.035442352294922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2789.371198938248
INFO:root:eval perplexity: 9.906805992126465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [11:00:29<22:48:21, 608.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1675.4709167480469
INFO:root:current train perplexity3.969756603240967
INFO:root:current mean train loss 1770.1575211745042
INFO:root:current train perplexity4.0604939460754395
INFO:root:current mean train loss 1767.4406660491345
INFO:root:current train perplexity4.068487644195557
INFO:root:current mean train loss 1767.9845223677785
INFO:root:current train perplexity4.066510200500488
INFO:root:current mean train loss 1774.809341279587
INFO:root:current train perplexity4.079847812652588
INFO:root:current mean train loss 1777.387695796906
INFO:root:current train perplexity4.083515644073486
INFO:root:current mean train loss 1782.75162632418
INFO:root:current train perplexity4.091357231140137
INFO:root:current mean train loss 1784.5147519545121
INFO:root:current train perplexity4.0940680503845215
INFO:root:current mean train loss 1787.7031101207829
INFO:root:current train perplexity4.097781181335449
INFO:root:current mean train loss 1789.6632771449806
INFO:root:current train perplexity4.097644329071045
INFO:root:current mean train loss 1789.723856040681
INFO:root:current train perplexity4.098944187164307
INFO:root:current mean train loss 1790.503264385721
INFO:root:current train perplexity4.102993965148926
INFO:root:current mean train loss 1790.5325670210625
INFO:root:current train perplexity4.107143878936768
INFO:root:current mean train loss 1790.8241353415274
INFO:root:current train perplexity4.108218193054199
INFO:root:current mean train loss 1791.2372786040999
INFO:root:current train perplexity4.113887310028076
INFO:root:current mean train loss 1790.9188623631255
INFO:root:current train perplexity4.115180969238281
INFO:root:current mean train loss 1792.1998445506108
INFO:root:current train perplexity4.119731903076172
INFO:root:current mean train loss 1793.341724592755
INFO:root:current train perplexity4.120629787445068
INFO:root:current mean train loss 1794.6323558189918
INFO:root:current train perplexity4.12092924118042
INFO:root:current mean train loss 1795.2077552090173
INFO:root:current train perplexity4.120689392089844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.67s/it]
INFO:root:final mean train loss: 1793.9992363372357
INFO:root:final train perplexity: 4.1198649406433105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2224.3935823914007
INFO:root:eval perplexity: 6.0497212409973145
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2792.658042528951
INFO:root:eval perplexity: 9.933611869812012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [11:10:36<22:38:01, 608.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1761.654541015625
INFO:root:current train perplexity4.048299789428711
INFO:root:current mean train loss 1779.9262503631844
INFO:root:current train perplexity4.068184852600098
INFO:root:current mean train loss 1779.0201940752263
INFO:root:current train perplexity4.063753604888916
INFO:root:current mean train loss 1774.1439657716364
INFO:root:current train perplexity4.0539140701293945
INFO:root:current mean train loss 1776.1189095322707
INFO:root:current train perplexity4.063215732574463
INFO:root:current mean train loss 1776.4451477870832
INFO:root:current train perplexity4.07059383392334
INFO:root:current mean train loss 1777.9096331757623
INFO:root:current train perplexity4.073461055755615
INFO:root:current mean train loss 1777.1750054855452
INFO:root:current train perplexity4.075169086456299
INFO:root:current mean train loss 1777.2986648689669
INFO:root:current train perplexity4.075397968292236
INFO:root:current mean train loss 1779.8537564520987
INFO:root:current train perplexity4.074436187744141
INFO:root:current mean train loss 1781.0031017337094
INFO:root:current train perplexity4.074442386627197
INFO:root:current mean train loss 1783.2144835282393
INFO:root:current train perplexity4.080944538116455
INFO:root:current mean train loss 1783.3421548879312
INFO:root:current train perplexity4.082769870758057
INFO:root:current mean train loss 1784.1553788744618
INFO:root:current train perplexity4.086114883422852
INFO:root:current mean train loss 1785.8020150106108
INFO:root:current train perplexity4.0873122215271
INFO:root:current mean train loss 1787.067685219428
INFO:root:current train perplexity4.091690540313721
INFO:root:current mean train loss 1786.900569400399
INFO:root:current train perplexity4.092166900634766
INFO:root:current mean train loss 1786.9914108888138
INFO:root:current train perplexity4.094447612762451
INFO:root:current mean train loss 1787.061081678379
INFO:root:current train perplexity4.095098495483398
INFO:root:current mean train loss 1787.9290993892544
INFO:root:current train perplexity4.098154544830322

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.02s/it]
INFO:root:final mean train loss: 1787.6898740613574
INFO:root:final train perplexity: 4.099401473999023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2228.7802807963485
INFO:root:eval perplexity: 6.071235179901123
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2798.762038210605
INFO:root:eval perplexity: 9.983586311340332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [11:20:45<22:27:59, 608.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1755.8283498663652
INFO:root:current train perplexity3.9838783740997314
INFO:root:current mean train loss 1760.9945307192595
INFO:root:current train perplexity4.019534587860107
INFO:root:current mean train loss 1765.8755908613446
INFO:root:current train perplexity4.031183242797852
INFO:root:current mean train loss 1764.2440835625462
INFO:root:current train perplexity4.0360493659973145
INFO:root:current mean train loss 1766.6269199597782
INFO:root:current train perplexity4.041725158691406
INFO:root:current mean train loss 1767.4312587582046
INFO:root:current train perplexity4.047390937805176
INFO:root:current mean train loss 1768.672747094803
INFO:root:current train perplexity4.050265312194824
INFO:root:current mean train loss 1770.1996671681804
INFO:root:current train perplexity4.053316116333008
INFO:root:current mean train loss 1772.528167651775
INFO:root:current train perplexity4.0556535720825195
INFO:root:current mean train loss 1774.63446877811
INFO:root:current train perplexity4.058708667755127
INFO:root:current mean train loss 1776.9465750692436
INFO:root:current train perplexity4.060383319854736
INFO:root:current mean train loss 1777.421177654568
INFO:root:current train perplexity4.0631585121154785
INFO:root:current mean train loss 1778.8118642089055
INFO:root:current train perplexity4.067795753479004
INFO:root:current mean train loss 1779.2563796791794
INFO:root:current train perplexity4.0667405128479
INFO:root:current mean train loss 1779.175491693786
INFO:root:current train perplexity4.069086074829102
INFO:root:current mean train loss 1778.8951525545554
INFO:root:current train perplexity4.070449352264404
INFO:root:current mean train loss 1779.4824481074481
INFO:root:current train perplexity4.072388648986816
INFO:root:current mean train loss 1780.1150251417084
INFO:root:current train perplexity4.074113368988037
INFO:root:current mean train loss 1780.6302226567814
INFO:root:current train perplexity4.075258255004883
INFO:root:current mean train loss 1781.551533701988
INFO:root:current train perplexity4.07678747177124

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.01s/it]
INFO:root:final mean train loss: 1780.7603036762187
INFO:root:final train perplexity: 4.077043533325195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it]
INFO:root:eval mean loss: 2227.8959956608765
INFO:root:eval perplexity: 6.066889762878418
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2797.984046016179
INFO:root:eval perplexity: 9.977205276489258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [11:30:53<22:17:52, 608.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1762.3689608487216
INFO:root:current train perplexity4.015411853790283
INFO:root:current mean train loss 1760.853414030998
INFO:root:current train perplexity4.019465446472168
INFO:root:current mean train loss 1761.31396484375
INFO:root:current train perplexity4.0064215660095215
INFO:root:current mean train loss 1762.2354179274869
INFO:root:current train perplexity4.0108418464660645
INFO:root:current mean train loss 1764.0577035757212
INFO:root:current train perplexity4.0177459716796875
INFO:root:current mean train loss 1765.4496179529137
INFO:root:current train perplexity4.015164375305176
INFO:root:current mean train loss 1766.3373960072759
INFO:root:current train perplexity4.027191638946533
INFO:root:current mean train loss 1768.554994373448
INFO:root:current train perplexity4.029754638671875
INFO:root:current mean train loss 1767.8682618615223
INFO:root:current train perplexity4.02918815612793
INFO:root:current mean train loss 1768.156683062009
INFO:root:current train perplexity4.034935474395752
INFO:root:current mean train loss 1769.2044530787175
INFO:root:current train perplexity4.036495685577393
INFO:root:current mean train loss 1769.2331158262311
INFO:root:current train perplexity4.037527561187744
INFO:root:current mean train loss 1770.0182834093314
INFO:root:current train perplexity4.042574405670166
INFO:root:current mean train loss 1771.2204821371945
INFO:root:current train perplexity4.0462236404418945
INFO:root:current mean train loss 1772.186504225059
INFO:root:current train perplexity4.04737663269043
INFO:root:current mean train loss 1773.2916371238193
INFO:root:current train perplexity4.0510358810424805
INFO:root:current mean train loss 1773.598227067008
INFO:root:current train perplexity4.052306175231934
INFO:root:current mean train loss 1773.5194992543625
INFO:root:current train perplexity4.053478240966797
INFO:root:current mean train loss 1774.2310764035124
INFO:root:current train perplexity4.053872585296631
INFO:root:current mean train loss 1774.835364550032
INFO:root:current train perplexity4.056214332580566

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.47s/it]
INFO:root:final mean train loss: 1774.3321630773194
INFO:root:final train perplexity: 4.056413173675537
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it]
INFO:root:eval mean loss: 2232.2458764475286
INFO:root:eval perplexity: 6.088283061981201
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2804.474103778812
INFO:root:eval perplexity: 10.0305814743042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [11:41:00<22:07:22, 607.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1756.0304514567058
INFO:root:current train perplexity4.010490417480469
INFO:root:current mean train loss 1767.1799004133356
INFO:root:current train perplexity4.013742923736572
INFO:root:current mean train loss 1764.9718910666072
INFO:root:current train perplexity4.023465633392334
INFO:root:current mean train loss 1764.246107860278
INFO:root:current train perplexity4.011885643005371
INFO:root:current mean train loss 1763.7678807145458
INFO:root:current train perplexity4.010000228881836
INFO:root:current mean train loss 1765.0673740627049
INFO:root:current train perplexity4.015213489532471
INFO:root:current mean train loss 1767.9120648701985
INFO:root:current train perplexity4.016580581665039
INFO:root:current mean train loss 1767.4683716136558
INFO:root:current train perplexity4.0176167488098145
INFO:root:current mean train loss 1765.2058637426535
INFO:root:current train perplexity4.017441749572754
INFO:root:current mean train loss 1764.526127442411
INFO:root:current train perplexity4.01671838760376
INFO:root:current mean train loss 1763.9245079382142
INFO:root:current train perplexity4.017638206481934
INFO:root:current mean train loss 1763.6592759272344
INFO:root:current train perplexity4.017849445343018
INFO:root:current mean train loss 1763.4857771771508
INFO:root:current train perplexity4.01810359954834
INFO:root:current mean train loss 1763.8148815277366
INFO:root:current train perplexity4.021823883056641
INFO:root:current mean train loss 1764.8798041965651
INFO:root:current train perplexity4.024129390716553
INFO:root:current mean train loss 1765.5097648484714
INFO:root:current train perplexity4.023507118225098
INFO:root:current mean train loss 1767.1692363976292
INFO:root:current train perplexity4.025978088378906
INFO:root:current mean train loss 1766.9965019828849
INFO:root:current train perplexity4.029221057891846
INFO:root:current mean train loss 1767.594473292685
INFO:root:current train perplexity4.0304646492004395
INFO:root:current mean train loss 1767.3831315417801
INFO:root:current train perplexity4.031522750854492

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.78s/it]
INFO:root:final mean train loss: 1766.6332313125927
INFO:root:final train perplexity: 4.0318403244018555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2233.6153486535904
INFO:root:eval perplexity: 6.095035076141357
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2805.088500543689
INFO:root:eval perplexity: 10.03564739227295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [11:51:09<21:57:53, 608.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1746.427608189958
INFO:root:current train perplexity3.960690975189209
INFO:root:current mean train loss 1745.9787804336145
INFO:root:current train perplexity3.944410562515259
INFO:root:current mean train loss 1747.6645123438852
INFO:root:current train perplexity3.9611480236053467
INFO:root:current mean train loss 1748.7043105569169
INFO:root:current train perplexity3.9613146781921387
INFO:root:current mean train loss 1748.7132952793488
INFO:root:current train perplexity3.9663617610931396
INFO:root:current mean train loss 1750.1233156783876
INFO:root:current train perplexity3.9758195877075195
INFO:root:current mean train loss 1753.0176628123866
INFO:root:current train perplexity3.9844343662261963
INFO:root:current mean train loss 1754.0466684551748
INFO:root:current train perplexity3.985999822616577
INFO:root:current mean train loss 1754.9968692878233
INFO:root:current train perplexity3.9901745319366455
INFO:root:current mean train loss 1756.0788284162902
INFO:root:current train perplexity3.9943792819976807
INFO:root:current mean train loss 1757.1847106541265
INFO:root:current train perplexity3.9994335174560547
INFO:root:current mean train loss 1758.788487363002
INFO:root:current train perplexity4.002724647521973
INFO:root:current mean train loss 1758.5096123031722
INFO:root:current train perplexity4.003799915313721
INFO:root:current mean train loss 1759.9244398827
INFO:root:current train perplexity4.006390571594238
INFO:root:current mean train loss 1759.424649742644
INFO:root:current train perplexity4.00698709487915
INFO:root:current mean train loss 1759.6505843703292
INFO:root:current train perplexity4.008621692657471
INFO:root:current mean train loss 1759.919527477312
INFO:root:current train perplexity4.011167526245117
INFO:root:current mean train loss 1760.0053704114118
INFO:root:current train perplexity4.011959075927734
INFO:root:current mean train loss 1760.609263204531
INFO:root:current train perplexity4.0140509605407715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.27s/it]
INFO:root:final mean train loss: 1761.2841887981435
INFO:root:final train perplexity: 4.014857292175293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2236.912540516955
INFO:root:eval perplexity: 6.111319065093994
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2811.3129103640294
INFO:root:eval perplexity: 10.087138175964355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [12:01:17<21:47:15, 608.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1698.593241373698
INFO:root:current train perplexity4.008814811706543
INFO:root:current mean train loss 1732.6490490031692
INFO:root:current train perplexity3.949406147003174
INFO:root:current mean train loss 1741.0185623909663
INFO:root:current train perplexity3.960909128189087
INFO:root:current mean train loss 1746.6837884242238
INFO:root:current train perplexity3.9537832736968994
INFO:root:current mean train loss 1747.1099167997613
INFO:root:current train perplexity3.961815595626831
INFO:root:current mean train loss 1749.0971715874352
INFO:root:current train perplexity3.972052574157715
INFO:root:current mean train loss 1749.8617007818946
INFO:root:current train perplexity3.9777133464813232
INFO:root:current mean train loss 1752.320858877036
INFO:root:current train perplexity3.982630968093872
INFO:root:current mean train loss 1751.7234522817153
INFO:root:current train perplexity3.987469434738159
INFO:root:current mean train loss 1751.4535570965697
INFO:root:current train perplexity3.9876813888549805
INFO:root:current mean train loss 1751.434100232589
INFO:root:current train perplexity3.9888384342193604
INFO:root:current mean train loss 1750.360685876222
INFO:root:current train perplexity3.988614559173584
INFO:root:current mean train loss 1751.6966145833333
INFO:root:current train perplexity3.9903600215911865
INFO:root:current mean train loss 1751.6149723818255
INFO:root:current train perplexity3.98988676071167
INFO:root:current mean train loss 1751.764023934116
INFO:root:current train perplexity3.990159749984741
INFO:root:current mean train loss 1752.2593299490838
INFO:root:current train perplexity3.9904556274414062
INFO:root:current mean train loss 1752.9985378925708
INFO:root:current train perplexity3.9917335510253906
INFO:root:current mean train loss 1753.5951130946385
INFO:root:current train perplexity3.992727279663086
INFO:root:current mean train loss 1753.9218312682767
INFO:root:current train perplexity3.99286150932312
INFO:root:current mean train loss 1754.6642767955223
INFO:root:current train perplexity3.9933626651763916

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.59s/it]
INFO:root:final mean train loss: 1754.9251984150435
INFO:root:final train perplexity: 3.9947586059570312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2236.1700534685283
INFO:root:eval perplexity: 6.107647895812988
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2810.1657316600176
INFO:root:eval perplexity: 10.07762622833252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [12:11:25<21:37:00, 607.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1711.3565833050272
INFO:root:current train perplexity3.918328285217285
INFO:root:current mean train loss 1724.9741717082698
INFO:root:current train perplexity3.920966386795044
INFO:root:current mean train loss 1725.4826621838215
INFO:root:current train perplexity3.911216974258423
INFO:root:current mean train loss 1736.4330359302437
INFO:root:current train perplexity3.9315571784973145
INFO:root:current mean train loss 1734.7324816115358
INFO:root:current train perplexity3.938913106918335
INFO:root:current mean train loss 1737.0871738411956
INFO:root:current train perplexity3.9461865425109863
INFO:root:current mean train loss 1739.1071826328628
INFO:root:current train perplexity3.9471707344055176
INFO:root:current mean train loss 1741.847248167088
INFO:root:current train perplexity3.9516303539276123
INFO:root:current mean train loss 1743.7145960496089
INFO:root:current train perplexity3.955444097518921
INFO:root:current mean train loss 1743.4180687339178
INFO:root:current train perplexity3.9568896293640137
INFO:root:current mean train loss 1743.8703847159854
INFO:root:current train perplexity3.9567692279815674
INFO:root:current mean train loss 1744.6967601691201
INFO:root:current train perplexity3.957951068878174
INFO:root:current mean train loss 1745.7269701329978
INFO:root:current train perplexity3.9611923694610596
INFO:root:current mean train loss 1745.4336283504288
INFO:root:current train perplexity3.960339069366455
INFO:root:current mean train loss 1745.3861133738965
INFO:root:current train perplexity3.9613234996795654
INFO:root:current mean train loss 1745.6009467783056
INFO:root:current train perplexity3.9640913009643555
INFO:root:current mean train loss 1746.9955067896065
INFO:root:current train perplexity3.965252161026001
INFO:root:current mean train loss 1747.7124775129905
INFO:root:current train perplexity3.968092679977417
INFO:root:current mean train loss 1748.2788397307195
INFO:root:current train perplexity3.9700539112091064
INFO:root:current mean train loss 1748.2653042400993
INFO:root:current train perplexity3.970897674560547

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.47s/it]
INFO:root:final mean train loss: 1747.4890813245595
INFO:root:final train perplexity: 3.9713830947875977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2242.7020475779864
INFO:root:eval perplexity: 6.140016555786133
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2818.112729336353
INFO:root:eval perplexity: 10.143682479858398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [12:21:33<21:27:19, 608.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1703.0140289306642
INFO:root:current train perplexity3.8555357456207275
INFO:root:current mean train loss 1726.5393763950892
INFO:root:current train perplexity3.8945248126983643
INFO:root:current mean train loss 1718.570371500651
INFO:root:current train perplexity3.89021372795105
INFO:root:current mean train loss 1722.7304863424863
INFO:root:current train perplexity3.8988611698150635
INFO:root:current mean train loss 1730.45576060902
INFO:root:current train perplexity3.910944700241089
INFO:root:current mean train loss 1733.2969744646991
INFO:root:current train perplexity3.919734001159668
INFO:root:current mean train loss 1733.659937286377
INFO:root:current train perplexity3.9190640449523926
INFO:root:current mean train loss 1734.3954731709248
INFO:root:current train perplexity3.9241437911987305
INFO:root:current mean train loss 1733.1577798025949
INFO:root:current train perplexity3.9256343841552734
INFO:root:current mean train loss 1733.9371901491854
INFO:root:current train perplexity3.9288511276245117
INFO:root:current mean train loss 1735.9578879723183
INFO:root:current train perplexity3.937899351119995
INFO:root:current mean train loss 1735.6886589184141
INFO:root:current train perplexity3.9367499351501465
INFO:root:current mean train loss 1736.7705076156124
INFO:root:current train perplexity3.937232732772827
INFO:root:current mean train loss 1737.315347153393
INFO:root:current train perplexity3.9406898021698
INFO:root:current mean train loss 1738.315498860677
INFO:root:current train perplexity3.9423370361328125
INFO:root:current mean train loss 1739.2124131239855
INFO:root:current train perplexity3.9432883262634277
INFO:root:current mean train loss 1738.9617496397436
INFO:root:current train perplexity3.9435369968414307
INFO:root:current mean train loss 1738.9258764508127
INFO:root:current train perplexity3.9457461833953857
INFO:root:current mean train loss 1739.675786093007
INFO:root:current train perplexity3.9495208263397217
INFO:root:current mean train loss 1740.483973992731
INFO:root:current train perplexity3.950253486633301

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.54s/it]
INFO:root:final mean train loss: 1741.1461720276648
INFO:root:final train perplexity: 3.9515535831451416
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2240.814032372008
INFO:root:eval perplexity: 6.130642414093018
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2817.8285682624114
INFO:root:eval perplexity: 10.141315460205078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [12:31:41<21:16:54, 608.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1705.1278461657073
INFO:root:current train perplexity3.8883471488952637
INFO:root:current mean train loss 1728.578631164162
INFO:root:current train perplexity3.899121046066284
INFO:root:current mean train loss 1728.720600053958
INFO:root:current train perplexity3.907463550567627
INFO:root:current mean train loss 1730.1020412071077
INFO:root:current train perplexity3.911207675933838
INFO:root:current mean train loss 1729.7010003889154
INFO:root:current train perplexity3.912426233291626
INFO:root:current mean train loss 1729.7641106268234
INFO:root:current train perplexity3.918126344680786
INFO:root:current mean train loss 1729.8896142503804
INFO:root:current train perplexity3.920102119445801
INFO:root:current mean train loss 1728.9694627487204
INFO:root:current train perplexity3.920131206512451
INFO:root:current mean train loss 1728.346387829775
INFO:root:current train perplexity3.9208195209503174
INFO:root:current mean train loss 1729.589513764735
INFO:root:current train perplexity3.9198951721191406
INFO:root:current mean train loss 1730.6565383723466
INFO:root:current train perplexity3.9245517253875732
INFO:root:current mean train loss 1730.1901188671538
INFO:root:current train perplexity3.9234654903411865
INFO:root:current mean train loss 1731.6329707932452
INFO:root:current train perplexity3.9258029460906982
INFO:root:current mean train loss 1732.4350453702157
INFO:root:current train perplexity3.927215099334717
INFO:root:current mean train loss 1732.3970934698332
INFO:root:current train perplexity3.9296329021453857
INFO:root:current mean train loss 1733.3232464995535
INFO:root:current train perplexity3.9300811290740967
INFO:root:current mean train loss 1733.8854874384713
INFO:root:current train perplexity3.9308207035064697
INFO:root:current mean train loss 1734.838454983081
INFO:root:current train perplexity3.930940628051758
INFO:root:current mean train loss 1735.3548381519884
INFO:root:current train perplexity3.932971477508545
INFO:root:current mean train loss 1736.617418666366
INFO:root:current train perplexity3.935600757598877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.69s/it]
INFO:root:final mean train loss: 1736.2102109261732
INFO:root:final train perplexity: 3.936189651489258
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2246.6397501627603
INFO:root:eval perplexity: 6.159612655639648
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2826.071555279671
INFO:root:eval perplexity: 10.210274696350098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [12:41:48<21:06:04, 607.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1730.0546165672508
INFO:root:current train perplexity3.8678176403045654
INFO:root:current mean train loss 1727.0037027994792
INFO:root:current train perplexity3.8733582496643066
INFO:root:current mean train loss 1717.7875067717837
INFO:root:current train perplexity3.8878958225250244
INFO:root:current mean train loss 1716.7396775515958
INFO:root:current train perplexity3.8911213874816895
INFO:root:current mean train loss 1718.4573614064147
INFO:root:current train perplexity3.8993101119995117
INFO:root:current mean train loss 1719.1775359575756
INFO:root:current train perplexity3.8992934226989746
INFO:root:current mean train loss 1720.6977160535864
INFO:root:current train perplexity3.896134376525879
INFO:root:current mean train loss 1722.25731144395
INFO:root:current train perplexity3.897523880004883
INFO:root:current mean train loss 1723.956712386701
INFO:root:current train perplexity3.9004063606262207
INFO:root:current mean train loss 1724.123554331566
INFO:root:current train perplexity3.902008533477783
INFO:root:current mean train loss 1724.7665593948223
INFO:root:current train perplexity3.903442859649658
INFO:root:current mean train loss 1723.9118248908646
INFO:root:current train perplexity3.9012272357940674
INFO:root:current mean train loss 1724.6306549982523
INFO:root:current train perplexity3.9005227088928223
INFO:root:current mean train loss 1724.710268600886
INFO:root:current train perplexity3.9031131267547607
INFO:root:current mean train loss 1725.4016211003752
INFO:root:current train perplexity3.9029314517974854
INFO:root:current mean train loss 1726.3793232589392
INFO:root:current train perplexity3.90521240234375
INFO:root:current mean train loss 1727.05681724047
INFO:root:current train perplexity3.9070425033569336
INFO:root:current mean train loss 1728.3118824233063
INFO:root:current train perplexity3.909959316253662
INFO:root:current mean train loss 1729.110481123787
INFO:root:current train perplexity3.9122931957244873
INFO:root:current mean train loss 1729.337560466237
INFO:root:current train perplexity3.9132843017578125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.28s/it]
INFO:root:final mean train loss: 1728.8021744397693
INFO:root:final train perplexity: 3.9132447242736816
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2248.3765743607323
INFO:root:eval perplexity: 6.168275833129883
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2827.7701022100787
INFO:root:eval perplexity: 10.224542617797852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [12:51:56<20:55:49, 607.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1715.0761852893202
INFO:root:current train perplexity3.877000093460083
INFO:root:current mean train loss 1712.9091368670238
INFO:root:current train perplexity3.875361442565918
INFO:root:current mean train loss 1709.5392579802942
INFO:root:current train perplexity3.857816457748413
INFO:root:current mean train loss 1713.0562453794357
INFO:root:current train perplexity3.867786407470703
INFO:root:current mean train loss 1716.2539413048148
INFO:root:current train perplexity3.87064528465271
INFO:root:current mean train loss 1717.9391368988606
INFO:root:current train perplexity3.8790788650512695
INFO:root:current mean train loss 1716.8399552208643
INFO:root:current train perplexity3.880221366882324
INFO:root:current mean train loss 1716.4813840458578
INFO:root:current train perplexity3.8824472427368164
INFO:root:current mean train loss 1717.842669040667
INFO:root:current train perplexity3.881765604019165
INFO:root:current mean train loss 1718.8587713000993
INFO:root:current train perplexity3.8821849822998047
INFO:root:current mean train loss 1719.4651584922449
INFO:root:current train perplexity3.882222890853882
INFO:root:current mean train loss 1719.2646136920457
INFO:root:current train perplexity3.8876843452453613
INFO:root:current mean train loss 1719.3989480961948
INFO:root:current train perplexity3.8889248371124268
INFO:root:current mean train loss 1719.5409914040033
INFO:root:current train perplexity3.8916516304016113
INFO:root:current mean train loss 1720.2490199170281
INFO:root:current train perplexity3.8924593925476074
INFO:root:current mean train loss 1721.2637966307059
INFO:root:current train perplexity3.894211530685425
INFO:root:current mean train loss 1721.6150711573912
INFO:root:current train perplexity3.895838499069214
INFO:root:current mean train loss 1722.8740332521984
INFO:root:current train perplexity3.8966164588928223
INFO:root:current mean train loss 1723.5668991790885
INFO:root:current train perplexity3.8961589336395264

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it]
INFO:root:final mean train loss: 1723.1652053009664
INFO:root:final train perplexity: 3.8958747386932373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2252.955981531887
INFO:root:eval perplexity: 6.191176414489746
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2833.175890334109
INFO:root:eval perplexity: 10.270085334777832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [13:02:03<20:45:44, 607.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1762.5716094970703
INFO:root:current train perplexity3.8764209747314453
INFO:root:current mean train loss 1694.5759119104457
INFO:root:current train perplexity3.8542816638946533
INFO:root:current mean train loss 1700.2891517052283
INFO:root:current train perplexity3.8559842109680176
INFO:root:current mean train loss 1702.7477103889762
INFO:root:current train perplexity3.857328176498413
INFO:root:current mean train loss 1704.1401860854205
INFO:root:current train perplexity3.8538708686828613
INFO:root:current mean train loss 1706.5452489177073
INFO:root:current train perplexity3.8590617179870605
INFO:root:current mean train loss 1706.886586440237
INFO:root:current train perplexity3.8595616817474365
INFO:root:current mean train loss 1706.7589569953875
INFO:root:current train perplexity3.858131170272827
INFO:root:current mean train loss 1708.352050630173
INFO:root:current train perplexity3.8596081733703613
INFO:root:current mean train loss 1708.9296069712366
INFO:root:current train perplexity3.859825849533081
INFO:root:current mean train loss 1709.4374323042612
INFO:root:current train perplexity3.8639466762542725
INFO:root:current mean train loss 1710.1761633256713
INFO:root:current train perplexity3.8643949031829834
INFO:root:current mean train loss 1710.4093844180074
INFO:root:current train perplexity3.865673303604126
INFO:root:current mean train loss 1711.3580726366888
INFO:root:current train perplexity3.8684306144714355
INFO:root:current mean train loss 1711.3820396770132
INFO:root:current train perplexity3.8664112091064453
INFO:root:current mean train loss 1712.1124956449717
INFO:root:current train perplexity3.8678786754608154
INFO:root:current mean train loss 1714.4759098641314
INFO:root:current train perplexity3.871664047241211
INFO:root:current mean train loss 1715.4341784439266
INFO:root:current train perplexity3.873124837875366
INFO:root:current mean train loss 1715.9621471303753
INFO:root:current train perplexity3.8749701976776123
INFO:root:current mean train loss 1716.5328905917313
INFO:root:current train perplexity3.8754751682281494

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it]
INFO:root:final mean train loss: 1716.5816281040209
INFO:root:final train perplexity: 3.8756847381591797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 2254.2289532600566
INFO:root:eval perplexity: 6.197558879852295
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 2836.7366683635305
INFO:root:eval perplexity: 10.300192832946777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [13:12:12<20:36:06, 607.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1713.69171875
INFO:root:current train perplexity3.81710147857666
INFO:root:current mean train loss 1704.691453125
INFO:root:current train perplexity3.813413381576538
INFO:root:current mean train loss 1705.3447102864584
INFO:root:current train perplexity3.8348193168640137
INFO:root:current mean train loss 1705.2802625450722
INFO:root:current train perplexity3.83730411529541
INFO:root:current mean train loss 1707.5980394071692
INFO:root:current train perplexity3.835850238800049
INFO:root:current mean train loss 1706.9973976934523
INFO:root:current train perplexity3.8386054039001465
INFO:root:current mean train loss 1707.433191015625
INFO:root:current train perplexity3.8432366847991943
INFO:root:current mean train loss 1708.2253673895475
INFO:root:current train perplexity3.8435423374176025
INFO:root:current mean train loss 1707.7670643939393
INFO:root:current train perplexity3.840223789215088
INFO:root:current mean train loss 1706.8781632706925
INFO:root:current train perplexity3.845456600189209
INFO:root:current mean train loss 1708.7450707412347
INFO:root:current train perplexity3.847166061401367
INFO:root:current mean train loss 1709.4499174262153
INFO:root:current train perplexity3.8492863178253174
INFO:root:current mean train loss 1709.3981573860012
INFO:root:current train perplexity3.850536823272705
INFO:root:current mean train loss 1710.7560173754423
INFO:root:current train perplexity3.851375102996826
INFO:root:current mean train loss 1710.8871175986842
INFO:root:current train perplexity3.8523776531219482
INFO:root:current mean train loss 1711.5210754194416
INFO:root:current train perplexity3.8544681072235107
INFO:root:current mean train loss 1711.778397235577
INFO:root:current train perplexity3.854870319366455
INFO:root:current mean train loss 1711.894717150702
INFO:root:current train perplexity3.8556909561157227
INFO:root:current mean train loss 1711.3307823870932
INFO:root:current train perplexity3.856529474258423
INFO:root:current mean train loss 1711.3982698990462
INFO:root:current train perplexity3.856990098953247

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it]
INFO:root:final mean train loss: 1710.9719346931831
INFO:root:final train perplexity: 3.858565330505371
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2257.5747849484706
INFO:root:eval perplexity: 6.214360237121582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2843.257528102144
INFO:root:eval perplexity: 10.355558395385742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [13:22:19<20:25:50, 607.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1682.7640497116815
INFO:root:current train perplexity3.8431341648101807
INFO:root:current mean train loss 1685.6985585387324
INFO:root:current train perplexity3.8263297080993652
INFO:root:current mean train loss 1689.5333231776215
INFO:root:current train perplexity3.826789617538452
INFO:root:current mean train loss 1691.2578581871346
INFO:root:current train perplexity3.8242974281311035
INFO:root:current mean train loss 1691.9466594160951
INFO:root:current train perplexity3.8241219520568848
INFO:root:current mean train loss 1692.1426817271101
INFO:root:current train perplexity3.822650194168091
INFO:root:current mean train loss 1694.0414133992892
INFO:root:current train perplexity3.826292037963867
INFO:root:current mean train loss 1696.5159838077514
INFO:root:current train perplexity3.8297476768493652
INFO:root:current mean train loss 1698.2248294495064
INFO:root:current train perplexity3.8278911113739014
INFO:root:current mean train loss 1698.9638319400212
INFO:root:current train perplexity3.8276984691619873
INFO:root:current mean train loss 1699.8098968095835
INFO:root:current train perplexity3.830817699432373
INFO:root:current mean train loss 1700.350782190647
INFO:root:current train perplexity3.829937696456909
INFO:root:current mean train loss 1701.8303131250943
INFO:root:current train perplexity3.830202579498291
INFO:root:current mean train loss 1702.7860651371554
INFO:root:current train perplexity3.830273389816284
INFO:root:current mean train loss 1703.0308440865822
INFO:root:current train perplexity3.831183910369873
INFO:root:current mean train loss 1703.8619644422322
INFO:root:current train perplexity3.833333730697632
INFO:root:current mean train loss 1704.0759603707131
INFO:root:current train perplexity3.834702730178833
INFO:root:current mean train loss 1704.1706353066024
INFO:root:current train perplexity3.8367562294006348
INFO:root:current mean train loss 1704.9184565673563
INFO:root:current train perplexity3.8384525775909424
INFO:root:current mean train loss 1705.9985726196414
INFO:root:current train perplexity3.8411669731140137

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.40s/it]
INFO:root:final mean train loss: 1705.470691213449
INFO:root:final train perplexity: 3.841848850250244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2259.0712544326243
INFO:root:eval perplexity: 6.221889972686768
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2844.960459607713
INFO:root:eval perplexity: 10.37006664276123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [13:32:28<20:16:10, 608.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1693.0539405952065
INFO:root:current train perplexity3.8139662742614746
INFO:root:current mean train loss 1688.5768758905758
INFO:root:current train perplexity3.797208547592163
INFO:root:current mean train loss 1693.7490474745114
INFO:root:current train perplexity3.8003199100494385
INFO:root:current mean train loss 1692.6753446531163
INFO:root:current train perplexity3.8027608394622803
INFO:root:current mean train loss 1693.4946483204826
INFO:root:current train perplexity3.804574489593506
INFO:root:current mean train loss 1693.8615663695634
INFO:root:current train perplexity3.8041491508483887
INFO:root:current mean train loss 1693.5946674723184
INFO:root:current train perplexity3.804396390914917
INFO:root:current mean train loss 1693.3386375216155
INFO:root:current train perplexity3.8088669776916504
INFO:root:current mean train loss 1693.2525170074214
INFO:root:current train perplexity3.8095526695251465
INFO:root:current mean train loss 1693.1737168742668
INFO:root:current train perplexity3.8132784366607666
INFO:root:current mean train loss 1694.2268105597852
INFO:root:current train perplexity3.8137776851654053
INFO:root:current mean train loss 1694.7263142517459
INFO:root:current train perplexity3.814110279083252
INFO:root:current mean train loss 1694.1515142730154
INFO:root:current train perplexity3.8115603923797607
INFO:root:current mean train loss 1696.3001731440283
INFO:root:current train perplexity3.813920736312866
INFO:root:current mean train loss 1695.854576356612
INFO:root:current train perplexity3.815526008605957
INFO:root:current mean train loss 1697.1522678769193
INFO:root:current train perplexity3.819247245788574
INFO:root:current mean train loss 1698.2813018743643
INFO:root:current train perplexity3.8190596103668213
INFO:root:current mean train loss 1698.7222495802835
INFO:root:current train perplexity3.8209331035614014
INFO:root:current mean train loss 1699.2060063584252
INFO:root:current train perplexity3.822575330734253
INFO:root:current mean train loss 1699.5389485926335
INFO:root:current train perplexity3.8229832649230957

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.15s/it]
INFO:root:final mean train loss: 1699.3957527533842
INFO:root:final train perplexity: 3.8234736919403076
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2264.607111071864
INFO:root:eval perplexity: 6.249824523925781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2849.8599290780144
INFO:root:eval perplexity: 10.411922454833984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [13:42:35<20:05:37, 607.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1678.8664325914885
INFO:root:current train perplexity3.809628486633301
INFO:root:current mean train loss 1672.2332659634676
INFO:root:current train perplexity3.7737207412719727
INFO:root:current mean train loss 1676.8123739491339
INFO:root:current train perplexity3.7701873779296875
INFO:root:current mean train loss 1680.0887308323638
INFO:root:current train perplexity3.769519567489624
INFO:root:current mean train loss 1684.2608393821395
INFO:root:current train perplexity3.777369499206543
INFO:root:current mean train loss 1686.5253893534343
INFO:root:current train perplexity3.77868390083313
INFO:root:current mean train loss 1687.7343081863673
INFO:root:current train perplexity3.7795073986053467
INFO:root:current mean train loss 1686.8836591268323
INFO:root:current train perplexity3.7790005207061768
INFO:root:current mean train loss 1687.8024558150062
INFO:root:current train perplexity3.7826743125915527
INFO:root:current mean train loss 1688.4510166605965
INFO:root:current train perplexity3.7851388454437256
INFO:root:current mean train loss 1688.3383212745412
INFO:root:current train perplexity3.7875659465789795
INFO:root:current mean train loss 1688.8298110442909
INFO:root:current train perplexity3.789679527282715
INFO:root:current mean train loss 1689.8127073090652
INFO:root:current train perplexity3.792619466781616
INFO:root:current mean train loss 1690.3879763581033
INFO:root:current train perplexity3.795868396759033
INFO:root:current mean train loss 1691.543839369363
INFO:root:current train perplexity3.7989389896392822
INFO:root:current mean train loss 1692.1978642807394
INFO:root:current train perplexity3.799454689025879
INFO:root:current mean train loss 1693.2027733559255
INFO:root:current train perplexity3.8023650646209717
INFO:root:current mean train loss 1694.1891935881195
INFO:root:current train perplexity3.8043456077575684
INFO:root:current mean train loss 1694.2909868642973
INFO:root:current train perplexity3.8053669929504395
INFO:root:current mean train loss 1693.7416468323001
INFO:root:current train perplexity3.804874897003174

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.98s/it]
INFO:root:final mean train loss: 1693.3009968275262
INFO:root:final train perplexity: 3.805126905441284
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2264.793398160461
INFO:root:eval perplexity: 6.250767707824707
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 2852.911211162594
INFO:root:eval perplexity: 10.438072204589844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [13:52:43<19:55:07, 607.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1688.5828043619792
INFO:root:current train perplexity3.771653890609741
INFO:root:current mean train loss 1679.2332276655602
INFO:root:current train perplexity3.762159585952759
INFO:root:current mean train loss 1681.8674449725363
INFO:root:current train perplexity3.7673120498657227
INFO:root:current mean train loss 1681.0025740373528
INFO:root:current train perplexity3.7648239135742188
INFO:root:current mean train loss 1681.8588783001078
INFO:root:current train perplexity3.7637031078338623
INFO:root:current mean train loss 1682.5078184697118
INFO:root:current train perplexity3.768697738647461
INFO:root:current mean train loss 1683.916939343209
INFO:root:current train perplexity3.7726552486419678
INFO:root:current mean train loss 1685.9955252687087
INFO:root:current train perplexity3.775221824645996
INFO:root:current mean train loss 1686.5431083175129
INFO:root:current train perplexity3.7765743732452393
INFO:root:current mean train loss 1685.3430874028354
INFO:root:current train perplexity3.77751088142395
INFO:root:current mean train loss 1686.2423950083628
INFO:root:current train perplexity3.778395414352417
INFO:root:current mean train loss 1687.1193303302468
INFO:root:current train perplexity3.7821786403656006
INFO:root:current mean train loss 1686.9154993043974
INFO:root:current train perplexity3.7836592197418213
INFO:root:current mean train loss 1686.9299198980393
INFO:root:current train perplexity3.781829833984375
INFO:root:current mean train loss 1687.0328073980659
INFO:root:current train perplexity3.783306121826172
INFO:root:current mean train loss 1686.7921770171256
INFO:root:current train perplexity3.7850184440612793
INFO:root:current mean train loss 1687.3295778746585
INFO:root:current train perplexity3.7867677211761475
INFO:root:current mean train loss 1687.8515386714394
INFO:root:current train perplexity3.788374185562134
INFO:root:current mean train loss 1688.265270912263
INFO:root:current train perplexity3.7884762287139893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.08s/it]
INFO:root:final mean train loss: 1687.8882811638182
INFO:root:final train perplexity: 3.788907527923584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2267.3585919319316
INFO:root:eval perplexity: 6.263755798339844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2855.8431907275044
INFO:root:eval perplexity: 10.463265419006348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [14:02:50<19:44:46, 607.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1676.9654418945313
INFO:root:current train perplexity3.7173140048980713
INFO:root:current mean train loss 1683.6881269975142
INFO:root:current train perplexity3.7432219982147217
INFO:root:current mean train loss 1672.5292067754835
INFO:root:current train perplexity3.7457480430603027
INFO:root:current mean train loss 1673.7357662077873
INFO:root:current train perplexity3.7566072940826416
INFO:root:current mean train loss 1674.0711125071455
INFO:root:current train perplexity3.755966901779175
INFO:root:current mean train loss 1673.088589776731
INFO:root:current train perplexity3.7579128742218018
INFO:root:current mean train loss 1672.9613797547388
INFO:root:current train perplexity3.757445812225342
INFO:root:current mean train loss 1674.3340005364216
INFO:root:current train perplexity3.761598587036133
INFO:root:current mean train loss 1675.1594343774113
INFO:root:current train perplexity3.764742374420166
INFO:root:current mean train loss 1675.5499291723902
INFO:root:current train perplexity3.7628700733184814
INFO:root:current mean train loss 1676.8988886767095
INFO:root:current train perplexity3.762239456176758
INFO:root:current mean train loss 1678.3612034153293
INFO:root:current train perplexity3.7623887062072754
INFO:root:current mean train loss 1679.1506242736311
INFO:root:current train perplexity3.7603461742401123
INFO:root:current mean train loss 1679.0126157338382
INFO:root:current train perplexity3.759655475616455
INFO:root:current mean train loss 1679.9609353356327
INFO:root:current train perplexity3.763197898864746
INFO:root:current mean train loss 1680.98373417128
INFO:root:current train perplexity3.765035390853882
INFO:root:current mean train loss 1681.3434696932018
INFO:root:current train perplexity3.7661824226379395
INFO:root:current mean train loss 1682.681905039291
INFO:root:current train perplexity3.768493413925171
INFO:root:current mean train loss 1682.9529387247496
INFO:root:current train perplexity3.769711494445801
INFO:root:current mean train loss 1682.4253634627578
INFO:root:current train perplexity3.7708053588867188

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.09s/it]
INFO:root:final mean train loss: 1681.9073429694395
INFO:root:final train perplexity: 3.7710652351379395
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2270.906058237062
INFO:root:eval perplexity: 6.28176212310791
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2860.126130665448
INFO:root:eval perplexity: 10.500173568725586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [14:12:58<19:35:03, 607.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1647.5326831958912
INFO:root:current train perplexity3.6802096366882324
INFO:root:current mean train loss 1663.6354797843874
INFO:root:current train perplexity3.7254981994628906
INFO:root:current mean train loss 1664.1598341349463
INFO:root:current train perplexity3.71425461769104
INFO:root:current mean train loss 1664.2459963177323
INFO:root:current train perplexity3.724215269088745
INFO:root:current mean train loss 1666.275754548906
INFO:root:current train perplexity3.723677635192871
INFO:root:current mean train loss 1666.7426739281902
INFO:root:current train perplexity3.7237141132354736
INFO:root:current mean train loss 1668.7449598783892
INFO:root:current train perplexity3.7275781631469727
INFO:root:current mean train loss 1668.8693097100133
INFO:root:current train perplexity3.7321977615356445
INFO:root:current mean train loss 1669.1880204593976
INFO:root:current train perplexity3.733042001724243
INFO:root:current mean train loss 1670.463020201254
INFO:root:current train perplexity3.735459089279175
INFO:root:current mean train loss 1670.9607500561024
INFO:root:current train perplexity3.7379322052001953
INFO:root:current mean train loss 1672.4871767682107
INFO:root:current train perplexity3.741973876953125
INFO:root:current mean train loss 1673.1645564519981
INFO:root:current train perplexity3.7439723014831543
INFO:root:current mean train loss 1673.1205166987036
INFO:root:current train perplexity3.7467658519744873
INFO:root:current mean train loss 1673.754472375668
INFO:root:current train perplexity3.74761700630188
INFO:root:current mean train loss 1673.425379944847
INFO:root:current train perplexity3.748711585998535
INFO:root:current mean train loss 1673.7277092256645
INFO:root:current train perplexity3.748922109603882
INFO:root:current mean train loss 1674.189258180054
INFO:root:current train perplexity3.749993085861206
INFO:root:current mean train loss 1675.2933497269687
INFO:root:current train perplexity3.7515616416931152
INFO:root:current mean train loss 1676.3205346591
INFO:root:current train perplexity3.752817153930664

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.60s/it]
INFO:root:final mean train loss: 1676.0544895897353
INFO:root:final train perplexity: 3.7536869049072266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2269.678208371426
INFO:root:eval perplexity: 6.275524616241455
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 2860.551708464927
INFO:root:eval perplexity: 10.503847122192383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [14:23:06<19:24:54, 607.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1678.9505920410156
INFO:root:current train perplexity3.7264797687530518
INFO:root:current mean train loss 1665.1470328436958
INFO:root:current train perplexity3.692289113998413
INFO:root:current mean train loss 1666.0126928110592
INFO:root:current train perplexity3.6958487033843994
INFO:root:current mean train loss 1664.2390633516534
INFO:root:current train perplexity3.702462911605835
INFO:root:current mean train loss 1662.5012325252499
INFO:root:current train perplexity3.7042429447174072
INFO:root:current mean train loss 1665.4625389996697
INFO:root:current train perplexity3.7109477519989014
INFO:root:current mean train loss 1666.9467147921923
INFO:root:current train perplexity3.7142183780670166
INFO:root:current mean train loss 1667.2775079870737
INFO:root:current train perplexity3.7171032428741455
INFO:root:current mean train loss 1666.1772915085346
INFO:root:current train perplexity3.7195544242858887
INFO:root:current mean train loss 1666.1551880917307
INFO:root:current train perplexity3.7202208042144775
INFO:root:current mean train loss 1665.8089862691945
INFO:root:current train perplexity3.720682144165039
INFO:root:current mean train loss 1665.9898707249781
INFO:root:current train perplexity3.7233850955963135
INFO:root:current mean train loss 1667.6167742861046
INFO:root:current train perplexity3.7269701957702637
INFO:root:current mean train loss 1667.586454754784
INFO:root:current train perplexity3.730565309524536
INFO:root:current mean train loss 1668.6997028044386
INFO:root:current train perplexity3.733631134033203
INFO:root:current mean train loss 1669.0033654721908
INFO:root:current train perplexity3.733799457550049
INFO:root:current mean train loss 1669.0204440300192
INFO:root:current train perplexity3.733107328414917
INFO:root:current mean train loss 1669.324735029028
INFO:root:current train perplexity3.7337584495544434
INFO:root:current mean train loss 1669.8452760113034
INFO:root:current train perplexity3.7355899810791016
INFO:root:current mean train loss 1670.8771048337835
INFO:root:current train perplexity3.7368998527526855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.49s/it]
INFO:root:final mean train loss: 1670.5382061302812
INFO:root:final train perplexity: 3.7373814582824707
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2272.5634276478004
INFO:root:eval perplexity: 6.2901930809021
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2865.1709335002492
INFO:root:eval perplexity: 10.543813705444336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [14:33:15<19:15:20, 608.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1654.0854111968495
INFO:root:current train perplexity3.6983344554901123
INFO:root:current mean train loss 1651.982875279018
INFO:root:current train perplexity3.693861722946167
INFO:root:current mean train loss 1653.9261540087703
INFO:root:current train perplexity3.687910795211792
INFO:root:current mean train loss 1653.5121803178022
INFO:root:current train perplexity3.6843717098236084
INFO:root:current mean train loss 1655.3400735917164
INFO:root:current train perplexity3.692758798599243
INFO:root:current mean train loss 1655.1607635552446
INFO:root:current train perplexity3.702808380126953
INFO:root:current mean train loss 1656.8441328317062
INFO:root:current train perplexity3.707908868789673
INFO:root:current mean train loss 1657.4935450309522
INFO:root:current train perplexity3.7098772525787354
INFO:root:current mean train loss 1659.4573912227333
INFO:root:current train perplexity3.7105181217193604
INFO:root:current mean train loss 1660.551498937061
INFO:root:current train perplexity3.71231746673584
INFO:root:current mean train loss 1661.6609305508512
INFO:root:current train perplexity3.712150812149048
INFO:root:current mean train loss 1660.5832262983822
INFO:root:current train perplexity3.7130770683288574
INFO:root:current mean train loss 1661.06111908564
INFO:root:current train perplexity3.713207244873047
INFO:root:current mean train loss 1661.8041750917007
INFO:root:current train perplexity3.714918375015259
INFO:root:current mean train loss 1662.9088304377353
INFO:root:current train perplexity3.716310501098633
INFO:root:current mean train loss 1663.481074253158
INFO:root:current train perplexity3.7190773487091064
INFO:root:current mean train loss 1664.7538914046038
INFO:root:current train perplexity3.7212202548980713
INFO:root:current mean train loss 1666.1041210549315
INFO:root:current train perplexity3.721571207046509
INFO:root:current mean train loss 1666.555449045582
INFO:root:current train perplexity3.7240750789642334
INFO:root:current mean train loss 1666.5652002998909
INFO:root:current train perplexity3.724215269088745

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.03s/it]
INFO:root:final mean train loss: 1665.9844426526845
INFO:root:final train perplexity: 3.7239737510681152
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2281.299493018617
INFO:root:eval perplexity: 6.334818363189697
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2874.772275667664
INFO:root:eval perplexity: 10.627371788024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [14:43:23<19:05:21, 608.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.1632972130408
INFO:root:current train perplexity3.6245386600494385
INFO:root:current mean train loss 1646.0021636619997
INFO:root:current train perplexity3.641432762145996
INFO:root:current mean train loss 1652.2539795800078
INFO:root:current train perplexity3.6669788360595703
INFO:root:current mean train loss 1653.0830749834656
INFO:root:current train perplexity3.6688170433044434
INFO:root:current mean train loss 1654.1656205564364
INFO:root:current train perplexity3.6698691844940186
INFO:root:current mean train loss 1653.1700488027816
INFO:root:current train perplexity3.673116683959961
INFO:root:current mean train loss 1652.1302148149427
INFO:root:current train perplexity3.6757237911224365
INFO:root:current mean train loss 1654.1291899301093
INFO:root:current train perplexity3.681576728820801
INFO:root:current mean train loss 1654.027316082583
INFO:root:current train perplexity3.6864635944366455
INFO:root:current mean train loss 1652.8949118148087
INFO:root:current train perplexity3.687410593032837
INFO:root:current mean train loss 1654.9662214006696
INFO:root:current train perplexity3.6915876865386963
INFO:root:current mean train loss 1654.507494474712
INFO:root:current train perplexity3.6917197704315186
INFO:root:current mean train loss 1654.9351841026628
INFO:root:current train perplexity3.6933817863464355
INFO:root:current mean train loss 1656.8097082395511
INFO:root:current train perplexity3.6944596767425537
INFO:root:current mean train loss 1656.912478063684
INFO:root:current train perplexity3.696807384490967
INFO:root:current mean train loss 1659.0552375899824
INFO:root:current train perplexity3.700598955154419
INFO:root:current mean train loss 1659.256813604016
INFO:root:current train perplexity3.701226234436035
INFO:root:current mean train loss 1659.9947026427606
INFO:root:current train perplexity3.703852415084839
INFO:root:current mean train loss 1660.8511940140568
INFO:root:current train perplexity3.7067222595214844
INFO:root:current mean train loss 1660.9915591896604
INFO:root:current train perplexity3.7083346843719482

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.35s/it]
INFO:root:final mean train loss: 1660.6560964733437
INFO:root:final train perplexity: 3.7083470821380615
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2280.9549820270945
INFO:root:eval perplexity: 6.333052635192871
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2873.896037649601
INFO:root:eval perplexity: 10.619717597961426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [14:53:31<18:54:54, 607.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.292085988898
INFO:root:current train perplexity3.6674349308013916
INFO:root:current mean train loss 1645.0059438852163
INFO:root:current train perplexity3.669642448425293
INFO:root:current mean train loss 1647.396913896981
INFO:root:current train perplexity3.6703994274139404
INFO:root:current mean train loss 1645.6871180280855
INFO:root:current train perplexity3.6659302711486816
INFO:root:current mean train loss 1647.9715618095013
INFO:root:current train perplexity3.661728620529175
INFO:root:current mean train loss 1647.5870125393908
INFO:root:current train perplexity3.6634249687194824
INFO:root:current mean train loss 1647.6147754257531
INFO:root:current train perplexity3.666738510131836
INFO:root:current mean train loss 1649.4926071454893
INFO:root:current train perplexity3.668602466583252
INFO:root:current mean train loss 1650.4545624290765
INFO:root:current train perplexity3.673840045928955
INFO:root:current mean train loss 1650.2435681827105
INFO:root:current train perplexity3.674506664276123
INFO:root:current mean train loss 1651.2776387253853
INFO:root:current train perplexity3.676116466522217
INFO:root:current mean train loss 1650.4533685277197
INFO:root:current train perplexity3.6754868030548096
INFO:root:current mean train loss 1650.885715793919
INFO:root:current train perplexity3.6786556243896484
INFO:root:current mean train loss 1651.5281985047043
INFO:root:current train perplexity3.6791725158691406
INFO:root:current mean train loss 1652.2732852999582
INFO:root:current train perplexity3.681405782699585
INFO:root:current mean train loss 1652.868477771723
INFO:root:current train perplexity3.683960199356079
INFO:root:current mean train loss 1653.4450438732947
INFO:root:current train perplexity3.6862025260925293
INFO:root:current mean train loss 1654.7358661619733
INFO:root:current train perplexity3.6878950595855713
INFO:root:current mean train loss 1654.2392852541639
INFO:root:current train perplexity3.687941312789917

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.44s/it]
INFO:root:final mean train loss: 1654.0355611811729
INFO:root:final train perplexity: 3.689021587371826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2284.434162112838
INFO:root:eval perplexity: 6.350907802581787
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2879.2948084621567
INFO:root:eval perplexity: 10.666956901550293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [15:03:37<18:44:02, 607.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1645.5934346516926
INFO:root:current train perplexity3.6413369178771973
INFO:root:current mean train loss 1642.911849975586
INFO:root:current train perplexity3.638495683670044
INFO:root:current mean train loss 1640.4151404038914
INFO:root:current train perplexity3.649723768234253
INFO:root:current mean train loss 1643.9886427659255
INFO:root:current train perplexity3.650108575820923
INFO:root:current mean train loss 1642.6904862783488
INFO:root:current train perplexity3.6521599292755127
INFO:root:current mean train loss 1643.7271246910095
INFO:root:current train perplexity3.655871868133545
INFO:root:current mean train loss 1644.5121210634318
INFO:root:current train perplexity3.658210277557373
INFO:root:current mean train loss 1644.2032952469388
INFO:root:current train perplexity3.6588332653045654
INFO:root:current mean train loss 1644.3034606332262
INFO:root:current train perplexity3.656614303588867
INFO:root:current mean train loss 1645.393265305904
INFO:root:current train perplexity3.6594395637512207
INFO:root:current mean train loss 1647.6262479638865
INFO:root:current train perplexity3.663486957550049
INFO:root:current mean train loss 1648.7150657159818
INFO:root:current train perplexity3.6665408611297607
INFO:root:current mean train loss 1650.0913815136396
INFO:root:current train perplexity3.668632984161377
INFO:root:current mean train loss 1650.055874894305
INFO:root:current train perplexity3.671111822128296
INFO:root:current mean train loss 1649.834992578955
INFO:root:current train perplexity3.672055959701538
INFO:root:current mean train loss 1649.9063835345878
INFO:root:current train perplexity3.672868490219116
INFO:root:current mean train loss 1649.6058290543094
INFO:root:current train perplexity3.6724393367767334
INFO:root:current mean train loss 1649.4160295290367
INFO:root:current train perplexity3.6729249954223633
INFO:root:current mean train loss 1650.1120612205523
INFO:root:current train perplexity3.674504518508911
INFO:root:current mean train loss 1650.1886772251528
INFO:root:current train perplexity3.675926685333252

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.47s/it]
INFO:root:final mean train loss: 1649.6138706717056
INFO:root:final train perplexity: 3.67617130279541
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2285.2673270584
INFO:root:eval perplexity: 6.355192184448242
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2879.8625548883533
INFO:root:eval perplexity: 10.671939849853516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [15:13:45<18:33:54, 607.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1631.4872078731141
INFO:root:current train perplexity3.6523213386535645
INFO:root:current mean train loss 1634.1823266790818
INFO:root:current train perplexity3.6377627849578857
INFO:root:current mean train loss 1641.8810230238469
INFO:root:current train perplexity3.631929874420166
INFO:root:current mean train loss 1635.1691475262396
INFO:root:current train perplexity3.628181219100952
INFO:root:current mean train loss 1636.4402014814611
INFO:root:current train perplexity3.6274449825286865
INFO:root:current mean train loss 1639.2518243627421
INFO:root:current train perplexity3.629857063293457
INFO:root:current mean train loss 1636.9461256551817
INFO:root:current train perplexity3.6328327655792236
INFO:root:current mean train loss 1637.389563038516
INFO:root:current train perplexity3.635369300842285
INFO:root:current mean train loss 1638.4446532614124
INFO:root:current train perplexity3.6375696659088135
INFO:root:current mean train loss 1637.5249086509352
INFO:root:current train perplexity3.640432357788086
INFO:root:current mean train loss 1639.0653786765822
INFO:root:current train perplexity3.643022298812866
INFO:root:current mean train loss 1638.416252088927
INFO:root:current train perplexity3.644623041152954
INFO:root:current mean train loss 1638.5782523345326
INFO:root:current train perplexity3.6466116905212402
INFO:root:current mean train loss 1640.4393963609268
INFO:root:current train perplexity3.649639844894409
INFO:root:current mean train loss 1641.8599763820853
INFO:root:current train perplexity3.6513853073120117
INFO:root:current mean train loss 1642.9746913672898
INFO:root:current train perplexity3.6534526348114014
INFO:root:current mean train loss 1643.259977243516
INFO:root:current train perplexity3.653985023498535
INFO:root:current mean train loss 1643.4103376257274
INFO:root:current train perplexity3.6564018726348877
INFO:root:current mean train loss 1644.5715743826672
INFO:root:current train perplexity3.658268928527832
INFO:root:current mean train loss 1644.2920782505507
INFO:root:current train perplexity3.65874981880188

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.46s/it]
INFO:root:final mean train loss: 1644.0483458149151
INFO:root:final train perplexity: 3.6600594520568848
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 2290.2546265514184
INFO:root:eval perplexity: 6.380890846252441
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 2887.7286065284243
INFO:root:eval perplexity: 10.741174697875977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [15:23:53<18:23:51, 607.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1642.4789906377378
INFO:root:current train perplexity3.606391191482544
INFO:root:current mean train loss 1627.9036070941245
INFO:root:current train perplexity3.6083598136901855
INFO:root:current mean train loss 1627.4007677527948
INFO:root:current train perplexity3.606107711791992
INFO:root:current mean train loss 1627.5971714967936
INFO:root:current train perplexity3.6160671710968018
INFO:root:current mean train loss 1629.8761287398402
INFO:root:current train perplexity3.619143486022949
INFO:root:current mean train loss 1630.2799586481226
INFO:root:current train perplexity3.6212639808654785
INFO:root:current mean train loss 1632.645289181925
INFO:root:current train perplexity3.6252477169036865
INFO:root:current mean train loss 1632.9123864058834
INFO:root:current train perplexity3.628378391265869
INFO:root:current mean train loss 1633.6291055160775
INFO:root:current train perplexity3.6305429935455322
INFO:root:current mean train loss 1635.1086527721575
INFO:root:current train perplexity3.6328163146972656
INFO:root:current mean train loss 1634.4875103164586
INFO:root:current train perplexity3.633340835571289
INFO:root:current mean train loss 1635.6129770328862
INFO:root:current train perplexity3.6343674659729004
INFO:root:current mean train loss 1636.53914979105
INFO:root:current train perplexity3.636545181274414
INFO:root:current mean train loss 1637.3961590657793
INFO:root:current train perplexity3.6383793354034424
INFO:root:current mean train loss 1637.7036444319729
INFO:root:current train perplexity3.6402714252471924
INFO:root:current mean train loss 1638.5139796564217
INFO:root:current train perplexity3.6421661376953125
INFO:root:current mean train loss 1638.7167795211403
INFO:root:current train perplexity3.643681764602661
INFO:root:current mean train loss 1639.0603135011722
INFO:root:current train perplexity3.6442155838012695
INFO:root:current mean train loss 1639.0811381396854
INFO:root:current train perplexity3.6445531845092773
INFO:root:current mean train loss 1639.1889871124863
INFO:root:current train perplexity3.64483904838562

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.55s/it]
INFO:root:final mean train loss: 1638.8268643454717
INFO:root:final train perplexity: 3.645008087158203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 2292.758862218113
INFO:root:eval perplexity: 6.393834590911865
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2889.5190737027647
INFO:root:eval perplexity: 10.756997108459473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [15:34:00<18:13:47, 607.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1614.0040690104167
INFO:root:current train perplexity3.619558095932007
INFO:root:current mean train loss 1622.3130040081
INFO:root:current train perplexity3.619541645050049
INFO:root:current mean train loss 1627.9808549192014
INFO:root:current train perplexity3.619425058364868
INFO:root:current mean train loss 1624.8937308991908
INFO:root:current train perplexity3.6156771183013916
INFO:root:current mean train loss 1627.2980696016975
INFO:root:current train perplexity3.617140769958496
INFO:root:current mean train loss 1628.2204871711256
INFO:root:current train perplexity3.6222610473632812
INFO:root:current mean train loss 1627.6848021172111
INFO:root:current train perplexity3.6210222244262695
INFO:root:current mean train loss 1627.960088607368
INFO:root:current train perplexity3.6216516494750977
INFO:root:current mean train loss 1627.9951754644046
INFO:root:current train perplexity3.6194040775299072
INFO:root:current mean train loss 1629.4304294289086
INFO:root:current train perplexity3.622593879699707
INFO:root:current mean train loss 1630.7574415808
INFO:root:current train perplexity3.625133991241455
INFO:root:current mean train loss 1631.4174962129864
INFO:root:current train perplexity3.6246566772460938
INFO:root:current mean train loss 1631.9133641959559
INFO:root:current train perplexity3.624915361404419
INFO:root:current mean train loss 1632.2070490724448
INFO:root:current train perplexity3.624401569366455
INFO:root:current mean train loss 1633.3262226055194
INFO:root:current train perplexity3.6257762908935547
INFO:root:current mean train loss 1633.492499587632
INFO:root:current train perplexity3.626924514770508
INFO:root:current mean train loss 1632.955218839846
INFO:root:current train perplexity3.6275155544281006
INFO:root:current mean train loss 1633.994989647219
INFO:root:current train perplexity3.6295905113220215
INFO:root:current mean train loss 1634.9822414232337
INFO:root:current train perplexity3.631525993347168
INFO:root:current mean train loss 1635.3050043355793
INFO:root:current train perplexity3.633375406265259

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.69s/it]
INFO:root:final mean train loss: 1634.8423006381884
INFO:root:final train perplexity: 3.633563756942749
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.56s/it]
INFO:root:eval mean loss: 2293.148894614362
INFO:root:eval perplexity: 6.395852565765381
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 2891.8856841824577
INFO:root:eval perplexity: 10.777948379516602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [15:44:07<18:03:16, 607.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1620.4493057250977
INFO:root:current train perplexity3.6055352687835693
INFO:root:current mean train loss 1617.4305894639756
INFO:root:current train perplexity3.5861573219299316
INFO:root:current mean train loss 1617.9346108572825
INFO:root:current train perplexity3.5836944580078125
INFO:root:current mean train loss 1620.7984211168791
INFO:root:current train perplexity3.5861213207244873
INFO:root:current mean train loss 1623.9163291931152
INFO:root:current train perplexity3.590391159057617
INFO:root:current mean train loss 1626.4294235755658
INFO:root:current train perplexity3.592229127883911
INFO:root:current mean train loss 1623.616869399127
INFO:root:current train perplexity3.5944712162017822
INFO:root:current mean train loss 1623.3073431552984
INFO:root:current train perplexity3.5947978496551514
INFO:root:current mean train loss 1623.5970936168324
INFO:root:current train perplexity3.5978777408599854
INFO:root:current mean train loss 1625.0936398875956
INFO:root:current train perplexity3.599760055541992
INFO:root:current mean train loss 1626.164670364945
INFO:root:current train perplexity3.6023569107055664
INFO:root:current mean train loss 1627.261462195445
INFO:root:current train perplexity3.6027681827545166
INFO:root:current mean train loss 1627.6813255310058
INFO:root:current train perplexity3.6047306060791016
INFO:root:current mean train loss 1626.9248251210088
INFO:root:current train perplexity3.6054182052612305
INFO:root:current mean train loss 1627.3957046096389
INFO:root:current train perplexity3.607424259185791
INFO:root:current mean train loss 1627.3410417387756
INFO:root:current train perplexity3.607668161392212
INFO:root:current mean train loss 1627.8669015066964
INFO:root:current train perplexity3.6110780239105225
INFO:root:current mean train loss 1627.889576978362
INFO:root:current train perplexity3.6130833625793457
INFO:root:current mean train loss 1628.6555827039354
INFO:root:current train perplexity3.614220142364502
INFO:root:current mean train loss 1629.5375040690103
INFO:root:current train perplexity3.6169893741607666

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.31s/it]
INFO:root:final mean train loss: 1629.0943262789867
INFO:root:final train perplexity: 3.6171183586120605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 2297.975908428219
INFO:root:eval perplexity: 6.420884132385254
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 2896.492468001995
INFO:root:eval perplexity: 10.81884479522705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [15:54:15<17:53:13, 607.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.2669514134986
INFO:root:current train perplexity3.5537893772125244
INFO:root:current mean train loss 1604.701517637611
INFO:root:current train perplexity3.55856990814209
INFO:root:current mean train loss 1607.9827909630155
INFO:root:current train perplexity3.5621402263641357
INFO:root:current mean train loss 1609.5240121836625
INFO:root:current train perplexity3.5693697929382324
INFO:root:current mean train loss 1612.8606777383047
INFO:root:current train perplexity3.5819454193115234
INFO:root:current mean train loss 1616.123667041261
INFO:root:current train perplexity3.587035894393921
INFO:root:current mean train loss 1616.222363771633
INFO:root:current train perplexity3.5892550945281982
INFO:root:current mean train loss 1614.6279113080302
INFO:root:current train perplexity3.5902445316314697
INFO:root:current mean train loss 1614.7757251275955
INFO:root:current train perplexity3.589641809463501
INFO:root:current mean train loss 1616.1780277355504
INFO:root:current train perplexity3.5897891521453857
INFO:root:current mean train loss 1617.3932695971257
INFO:root:current train perplexity3.591867446899414
INFO:root:current mean train loss 1619.3899466276368
INFO:root:current train perplexity3.59323787689209
INFO:root:current mean train loss 1619.9314562489458
INFO:root:current train perplexity3.5942885875701904
INFO:root:current mean train loss 1621.0403989820543
INFO:root:current train perplexity3.595614194869995
INFO:root:current mean train loss 1621.0611539648307
INFO:root:current train perplexity3.5969157218933105
INFO:root:current mean train loss 1622.0164613765558
INFO:root:current train perplexity3.5985612869262695
INFO:root:current mean train loss 1622.7886514028662
INFO:root:current train perplexity3.5994391441345215
INFO:root:current mean train loss 1623.853507405463
INFO:root:current train perplexity3.600848913192749
INFO:root:current mean train loss 1624.4579049825793
INFO:root:current train perplexity3.601977586746216

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.36s/it]
INFO:root:final mean train loss: 1623.9478001673897
INFO:root:final train perplexity: 3.60245680809021
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2301.691590221216
INFO:root:eval perplexity: 6.440219879150391
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2904.399996277288
INFO:root:eval perplexity: 10.889409065246582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [16:04:24<17:43:44, 607.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.2907889229912
INFO:root:current train perplexity3.6325812339782715
INFO:root:current mean train loss 1612.230155007881
INFO:root:current train perplexity3.567960500717163
INFO:root:current mean train loss 1610.6336321964443
INFO:root:current train perplexity3.5549514293670654
INFO:root:current mean train loss 1612.8723712119327
INFO:root:current train perplexity3.5606844425201416
INFO:root:current mean train loss 1611.5303798804537
INFO:root:current train perplexity3.561856508255005
INFO:root:current mean train loss 1610.584567890093
INFO:root:current train perplexity3.5635323524475098
INFO:root:current mean train loss 1610.8007603747837
INFO:root:current train perplexity3.5659847259521484
INFO:root:current mean train loss 1610.7589113037793
INFO:root:current train perplexity3.566037654876709
INFO:root:current mean train loss 1613.96809810854
INFO:root:current train perplexity3.568993330001831
INFO:root:current mean train loss 1614.4507996540362
INFO:root:current train perplexity3.5720489025115967
INFO:root:current mean train loss 1615.4791545077894
INFO:root:current train perplexity3.575838565826416
INFO:root:current mean train loss 1616.1610195084577
INFO:root:current train perplexity3.5784175395965576
INFO:root:current mean train loss 1615.7271491212546
INFO:root:current train perplexity3.579860210418701
INFO:root:current mean train loss 1616.0038324875736
INFO:root:current train perplexity3.5831096172332764
INFO:root:current mean train loss 1616.250929340109
INFO:root:current train perplexity3.5853383541107178
INFO:root:current mean train loss 1616.1557095526389
INFO:root:current train perplexity3.5840516090393066
INFO:root:current mean train loss 1617.1573903061349
INFO:root:current train perplexity3.585287570953369
INFO:root:current mean train loss 1617.982115559706
INFO:root:current train perplexity3.586082935333252
INFO:root:current mean train loss 1618.4740340833232
INFO:root:current train perplexity3.587038993835449
INFO:root:current mean train loss 1619.2883898377295
INFO:root:current train perplexity3.5884315967559814

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.40s/it]
INFO:root:final mean train loss: 1619.362248884808
INFO:root:final train perplexity: 3.5894434452056885
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 2305.7328257459276
INFO:root:eval perplexity: 6.461314678192139
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 2909.3602429112643
INFO:root:eval perplexity: 10.933903694152832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [16:14:34<17:35:07, 608.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1612.6282171433973
INFO:root:current train perplexity3.5288712978363037
INFO:root:current mean train loss 1595.7310828289003
INFO:root:current train perplexity3.5305821895599365
INFO:root:current mean train loss 1599.6941995992288
INFO:root:current train perplexity3.5283825397491455
INFO:root:current mean train loss 1599.8365312559006
INFO:root:current train perplexity3.535313129425049
INFO:root:current mean train loss 1600.961010572252
INFO:root:current train perplexity3.5432138442993164
INFO:root:current mean train loss 1600.0094122877679
INFO:root:current train perplexity3.544271230697632
INFO:root:current mean train loss 1601.9399058104696
INFO:root:current train perplexity3.5487101078033447
INFO:root:current mean train loss 1603.2139408304656
INFO:root:current train perplexity3.5497381687164307
INFO:root:current mean train loss 1605.537641284268
INFO:root:current train perplexity3.5546748638153076
INFO:root:current mean train loss 1607.720684112975
INFO:root:current train perplexity3.5621049404144287
INFO:root:current mean train loss 1608.5347468230934
INFO:root:current train perplexity3.5635225772857666
INFO:root:current mean train loss 1608.8419463598655
INFO:root:current train perplexity3.563326835632324
INFO:root:current mean train loss 1609.0917771414563
INFO:root:current train perplexity3.5639114379882812
INFO:root:current mean train loss 1609.1704801334465
INFO:root:current train perplexity3.566398859024048
INFO:root:current mean train loss 1610.2916479850464
INFO:root:current train perplexity3.568500518798828
INFO:root:current mean train loss 1611.2881448597475
INFO:root:current train perplexity3.5705406665802
INFO:root:current mean train loss 1610.7105055013221
INFO:root:current train perplexity3.5700018405914307
INFO:root:current mean train loss 1611.8656059454659
INFO:root:current train perplexity3.5718095302581787
INFO:root:current mean train loss 1612.9147901217316
INFO:root:current train perplexity3.57269287109375
INFO:root:current mean train loss 1614.0708601411793
INFO:root:current train perplexity3.573686122894287

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.07s/it]
INFO:root:final mean train loss: 1613.9402375021668
INFO:root:final train perplexity: 3.5741169452667236
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2306.8807836221463
INFO:root:eval perplexity: 6.467321395874023
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 2910.58202995138
INFO:root:eval perplexity: 10.944893836975098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [16:24:44<17:25:22, 608.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.7512130737305
INFO:root:current train perplexity3.5182456970214844
INFO:root:current mean train loss 1591.8431058316617
INFO:root:current train perplexity3.5244100093841553
INFO:root:current mean train loss 1594.6027477633568
INFO:root:current train perplexity3.531752824783325
INFO:root:current mean train loss 1596.9502882683414
INFO:root:current train perplexity3.5382158756256104
INFO:root:current mean train loss 1599.2533572060722
INFO:root:current train perplexity3.5412254333496094
INFO:root:current mean train loss 1600.3643317675069
INFO:root:current train perplexity3.5410149097442627
INFO:root:current mean train loss 1600.7369196385514
INFO:root:current train perplexity3.5431978702545166
INFO:root:current mean train loss 1601.4647928329712
INFO:root:current train perplexity3.5433578491210938
INFO:root:current mean train loss 1603.4094208051574
INFO:root:current train perplexity3.5482256412506104
INFO:root:current mean train loss 1603.690775167087
INFO:root:current train perplexity3.5494744777679443
INFO:root:current mean train loss 1604.0574607557924
INFO:root:current train perplexity3.54921817779541
INFO:root:current mean train loss 1605.129089993467
INFO:root:current train perplexity3.5502049922943115
INFO:root:current mean train loss 1605.3129783043494
INFO:root:current train perplexity3.5517985820770264
INFO:root:current mean train loss 1605.6147434676082
INFO:root:current train perplexity3.5528018474578857
INFO:root:current mean train loss 1606.281081141688
INFO:root:current train perplexity3.555158853530884
INFO:root:current mean train loss 1607.5551041792837
INFO:root:current train perplexity3.5566797256469727
INFO:root:current mean train loss 1608.674114560618
INFO:root:current train perplexity3.5577242374420166
INFO:root:current mean train loss 1608.532646755332
INFO:root:current train perplexity3.5587234497070312
INFO:root:current mean train loss 1608.8208810385172
INFO:root:current train perplexity3.5591061115264893
INFO:root:current mean train loss 1609.0363305187814
INFO:root:current train perplexity3.559967041015625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.46s/it]
INFO:root:final mean train loss: 1608.8808047419177
INFO:root:final train perplexity: 3.5598742961883545
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2310.239251319398
INFO:root:eval perplexity: 6.484920501708984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.75s/it]
INFO:root:eval mean loss: 2911.3750017314937
INFO:root:eval perplexity: 10.952030181884766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [16:34:54<17:15:45, 609.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1594.215771484375
INFO:root:current train perplexity3.530848741531372
INFO:root:current mean train loss 1595.3025804924243
INFO:root:current train perplexity3.521613836288452
INFO:root:current mean train loss 1594.547407502948
INFO:root:current train perplexity3.5155255794525146
INFO:root:current mean train loss 1598.875469218215
INFO:root:current train perplexity3.5254340171813965
INFO:root:current mean train loss 1597.5175111832157
INFO:root:current train perplexity3.5263566970825195
INFO:root:current mean train loss 1595.8007592125277
INFO:root:current train perplexity3.5269014835357666
INFO:root:current mean train loss 1596.2677844513628
INFO:root:current train perplexity3.530014991760254
INFO:root:current mean train loss 1595.5983017067504
INFO:root:current train perplexity3.5298399925231934
INFO:root:current mean train loss 1595.8308415936597
INFO:root:current train perplexity3.533324956893921
INFO:root:current mean train loss 1597.6322350378482
INFO:root:current train perplexity3.5353667736053467
INFO:root:current mean train loss 1598.0132043399722
INFO:root:current train perplexity3.534097194671631
INFO:root:current mean train loss 1600.2450900491215
INFO:root:current train perplexity3.5380215644836426
INFO:root:current mean train loss 1601.126784542521
INFO:root:current train perplexity3.5405192375183105
INFO:root:current mean train loss 1600.6855884593922
INFO:root:current train perplexity3.5399513244628906
INFO:root:current mean train loss 1602.3728411469444
INFO:root:current train perplexity3.5422730445861816
INFO:root:current mean train loss 1602.4832014089957
INFO:root:current train perplexity3.5429656505584717
INFO:root:current mean train loss 1603.0885479718
INFO:root:current train perplexity3.5448484420776367
INFO:root:current mean train loss 1603.2856692911207
INFO:root:current train perplexity3.544881820678711
INFO:root:current mean train loss 1603.8523730730562
INFO:root:current train perplexity3.5458054542541504
INFO:root:current mean train loss 1605.0286142130844
INFO:root:current train perplexity3.5473363399505615

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.96s/it]
INFO:root:final mean train loss: 1604.400777242548
INFO:root:final train perplexity: 3.547309637069702
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2311.9965331165504
INFO:root:eval perplexity: 6.494149684906006
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2915.831232165614
INFO:root:eval perplexity: 10.992230415344238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [16:45:06<17:06:46, 609.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1584.6385632026486
INFO:root:current train perplexity3.467550277709961
INFO:root:current mean train loss 1591.7659690773094
INFO:root:current train perplexity3.484147310256958
INFO:root:current mean train loss 1594.1396770071476
INFO:root:current train perplexity3.5052242279052734
INFO:root:current mean train loss 1592.9049270390217
INFO:root:current train perplexity3.5131912231445312
INFO:root:current mean train loss 1593.1620318780797
INFO:root:current train perplexity3.512789249420166
INFO:root:current mean train loss 1594.8521457947406
INFO:root:current train perplexity3.5168488025665283
INFO:root:current mean train loss 1594.7165076292156
INFO:root:current train perplexity3.518746852874756
INFO:root:current mean train loss 1594.6580226732337
INFO:root:current train perplexity3.5201103687286377
INFO:root:current mean train loss 1596.0195012168278
INFO:root:current train perplexity3.526533603668213
INFO:root:current mean train loss 1596.190355724327
INFO:root:current train perplexity3.5251963138580322
INFO:root:current mean train loss 1596.339967851057
INFO:root:current train perplexity3.5220823287963867
INFO:root:current mean train loss 1596.4690025058494
INFO:root:current train perplexity3.5226123332977295
INFO:root:current mean train loss 1597.8292466757264
INFO:root:current train perplexity3.52471923828125
INFO:root:current mean train loss 1597.8443914432773
INFO:root:current train perplexity3.5260307788848877
INFO:root:current mean train loss 1598.1540542170103
INFO:root:current train perplexity3.5283377170562744
INFO:root:current mean train loss 1598.9918111036761
INFO:root:current train perplexity3.5289816856384277
INFO:root:current mean train loss 1599.4605667894434
INFO:root:current train perplexity3.5309102535247803
INFO:root:current mean train loss 1600.2190270215171
INFO:root:current train perplexity3.533233642578125
INFO:root:current mean train loss 1600.4282716919336
INFO:root:current train perplexity3.533625602722168
INFO:root:current mean train loss 1600.694316721588
INFO:root:current train perplexity3.5356950759887695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.04s/it]
INFO:root:final mean train loss: 1600.217136123357
INFO:root:final train perplexity: 3.535616636276245
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 2314.687258456616
INFO:root:eval perplexity: 6.50830602645874
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2921.316651256372
INFO:root:eval perplexity: 11.041911125183105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [16:55:17<16:57:26, 610.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1588.103915127841
INFO:root:current train perplexity3.485593795776367
INFO:root:current mean train loss 1588.3629187195745
INFO:root:current train perplexity3.4842727184295654
INFO:root:current mean train loss 1586.9721855240123
INFO:root:current train perplexity3.496249198913574
INFO:root:current mean train loss 1590.0456640869752
INFO:root:current train perplexity3.499361515045166
INFO:root:current mean train loss 1591.8959055806927
INFO:root:current train perplexity3.5044069290161133
INFO:root:current mean train loss 1590.9269821854784
INFO:root:current train perplexity3.50784969329834
INFO:root:current mean train loss 1592.3527727249866
INFO:root:current train perplexity3.5079829692840576
INFO:root:current mean train loss 1592.8159757191606
INFO:root:current train perplexity3.5104615688323975
INFO:root:current mean train loss 1592.0348104067452
INFO:root:current train perplexity3.513434648513794
INFO:root:current mean train loss 1592.7402035824887
INFO:root:current train perplexity3.514359951019287
INFO:root:current mean train loss 1592.811759025428
INFO:root:current train perplexity3.515151262283325
INFO:root:current mean train loss 1592.4966072801553
INFO:root:current train perplexity3.515167236328125
INFO:root:current mean train loss 1593.4979792147806
INFO:root:current train perplexity3.5168216228485107
INFO:root:current mean train loss 1593.2854493409075
INFO:root:current train perplexity3.517793893814087
INFO:root:current mean train loss 1593.3028883676357
INFO:root:current train perplexity3.516505479812622
INFO:root:current mean train loss 1594.41860383626
INFO:root:current train perplexity3.517766237258911
INFO:root:current mean train loss 1594.9614892233253
INFO:root:current train perplexity3.518740177154541
INFO:root:current mean train loss 1594.991980882934
INFO:root:current train perplexity3.519296884536743
INFO:root:current mean train loss 1595.392706430605
INFO:root:current train perplexity3.5202064514160156

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.78s/it]
INFO:root:final mean train loss: 1595.61158556224
INFO:root:final train perplexity: 3.522789478302002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2317.290390122867
INFO:root:eval perplexity: 6.522028923034668
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2924.2512826040283
INFO:root:eval perplexity: 11.068584442138672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [17:05:27<16:47:09, 610.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1577.1943969726562
INFO:root:current train perplexity3.5062317848205566
INFO:root:current mean train loss 1568.0222820413524
INFO:root:current train perplexity3.463228702545166
INFO:root:current mean train loss 1577.6634990550854
INFO:root:current train perplexity3.4739484786987305
INFO:root:current mean train loss 1579.9834594726562
INFO:root:current train perplexity3.478368043899536
INFO:root:current mean train loss 1582.5861696096567
INFO:root:current train perplexity3.4816548824310303
INFO:root:current mean train loss 1584.1676101093144
INFO:root:current train perplexity3.4849655628204346
INFO:root:current mean train loss 1583.3379568124747
INFO:root:current train perplexity3.4852027893066406
INFO:root:current mean train loss 1584.8766854568566
INFO:root:current train perplexity3.4873206615448
INFO:root:current mean train loss 1586.3606724458582
INFO:root:current train perplexity3.493452548980713
INFO:root:current mean train loss 1587.8820662186135
INFO:root:current train perplexity3.4976065158843994
INFO:root:current mean train loss 1588.2840889758013
INFO:root:current train perplexity3.49910831451416
INFO:root:current mean train loss 1587.95165687479
INFO:root:current train perplexity3.500295639038086
INFO:root:current mean train loss 1588.1101994765431
INFO:root:current train perplexity3.500868558883667
INFO:root:current mean train loss 1589.3302484297824
INFO:root:current train perplexity3.5015573501586914
INFO:root:current mean train loss 1589.79570696987
INFO:root:current train perplexity3.5025441646575928
INFO:root:current mean train loss 1591.0100759541453
INFO:root:current train perplexity3.505937099456787
INFO:root:current mean train loss 1590.5769541523243
INFO:root:current train perplexity3.506728410720825
INFO:root:current mean train loss 1590.9685248528326
INFO:root:current train perplexity3.5084421634674072
INFO:root:current mean train loss 1591.369038720488
INFO:root:current train perplexity3.5074524879455566
INFO:root:current mean train loss 1591.0085489356693
INFO:root:current train perplexity3.5081136226654053

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.96s/it]
INFO:root:final mean train loss: 1590.499481432016
INFO:root:final train perplexity: 3.508605718612671
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2318.595783206588
INFO:root:eval perplexity: 6.528921604156494
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2926.407157302748
INFO:root:eval perplexity: 11.088217735290527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [17:15:38<16:37:02, 610.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1575.8590383818655
INFO:root:current train perplexity3.4336447715759277
INFO:root:current mean train loss 1571.1109307081178
INFO:root:current train perplexity3.441357374191284
INFO:root:current mean train loss 1577.5243972974786
INFO:root:current train perplexity3.4453024864196777
INFO:root:current mean train loss 1580.6184430279889
INFO:root:current train perplexity3.456608533859253
INFO:root:current mean train loss 1577.482310235638
INFO:root:current train perplexity3.459763526916504
INFO:root:current mean train loss 1579.9805718112395
INFO:root:current train perplexity3.4625136852264404
INFO:root:current mean train loss 1581.4432941936957
INFO:root:current train perplexity3.4681901931762695
INFO:root:current mean train loss 1583.103932629178
INFO:root:current train perplexity3.470823287963867
INFO:root:current mean train loss 1583.3820531142144
INFO:root:current train perplexity3.4730710983276367
INFO:root:current mean train loss 1583.352759521746
INFO:root:current train perplexity3.4757280349731445
INFO:root:current mean train loss 1583.244898571742
INFO:root:current train perplexity3.4791243076324463
INFO:root:current mean train loss 1583.9625885198311
INFO:root:current train perplexity3.4817116260528564
INFO:root:current mean train loss 1584.3149987288054
INFO:root:current train perplexity3.484874963760376
INFO:root:current mean train loss 1584.224063675831
INFO:root:current train perplexity3.4850993156433105
INFO:root:current mean train loss 1584.5028137505178
INFO:root:current train perplexity3.4869320392608643
INFO:root:current mean train loss 1584.8567153323497
INFO:root:current train perplexity3.488044500350952
INFO:root:current mean train loss 1585.3697802794177
INFO:root:current train perplexity3.4917867183685303
INFO:root:current mean train loss 1586.424233288553
INFO:root:current train perplexity3.4942634105682373
INFO:root:current mean train loss 1586.5388549871284
INFO:root:current train perplexity3.4953229427337646
INFO:root:current mean train loss 1586.3538292314004
INFO:root:current train perplexity3.4955365657806396

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.92s/it]
INFO:root:final mean train loss: 1585.7420512096965
INFO:root:final train perplexity: 3.4954569339752197
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2325.1964565845246
INFO:root:eval perplexity: 6.563889503479004
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2933.9972404317655
INFO:root:eval perplexity: 11.157626152038574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [17:25:49<16:27:22, 610.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1568.7821215820313
INFO:root:current train perplexity3.4440364837646484
INFO:root:current mean train loss 1567.5470751953126
INFO:root:current train perplexity3.454791784286499
INFO:root:current mean train loss 1570.8183984375
INFO:root:current train perplexity3.448532819747925
INFO:root:current mean train loss 1572.0848615373884
INFO:root:current train perplexity3.458422899246216
INFO:root:current mean train loss 1575.2609114583333
INFO:root:current train perplexity3.463878631591797
INFO:root:current mean train loss 1575.1978085049716
INFO:root:current train perplexity3.4649710655212402
INFO:root:current mean train loss 1574.1667732121393
INFO:root:current train perplexity3.468172073364258
INFO:root:current mean train loss 1573.8557280273437
INFO:root:current train perplexity3.467604875564575
INFO:root:current mean train loss 1575.0212016027115
INFO:root:current train perplexity3.4672038555145264
INFO:root:current mean train loss 1575.7898646946958
INFO:root:current train perplexity3.468153476715088
INFO:root:current mean train loss 1577.9572258649553
INFO:root:current train perplexity3.4711272716522217
INFO:root:current mean train loss 1577.6678415845788
INFO:root:current train perplexity3.4711127281188965
INFO:root:current mean train loss 1578.0581739257811
INFO:root:current train perplexity3.4727046489715576
INFO:root:current mean train loss 1578.7393413628472
INFO:root:current train perplexity3.4739856719970703
INFO:root:current mean train loss 1578.7089969187768
INFO:root:current train perplexity3.474771022796631
INFO:root:current mean train loss 1579.4796897838962
INFO:root:current train perplexity3.476348400115967
INFO:root:current mean train loss 1580.0090561375473
INFO:root:current train perplexity3.477733612060547
INFO:root:current mean train loss 1580.9266793387276
INFO:root:current train perplexity3.479623794555664
INFO:root:current mean train loss 1581.1586025258657
INFO:root:current train perplexity3.481752634048462
INFO:root:current mean train loss 1581.8958488581732
INFO:root:current train perplexity3.484175443649292

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.59s/it]
INFO:root:final mean train loss: 1581.8820645961866
INFO:root:final train perplexity: 3.484825611114502
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2325.0182763498724
INFO:root:eval perplexity: 6.5629425048828125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2934.808076466229
INFO:root:eval perplexity: 11.165066719055176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [17:36:01<16:17:22, 610.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1559.7620722073227
INFO:root:current train perplexity3.442835807800293
INFO:root:current mean train loss 1563.8374213487089
INFO:root:current train perplexity3.4432120323181152
INFO:root:current mean train loss 1565.569524300679
INFO:root:current train perplexity3.442981719970703
INFO:root:current mean train loss 1568.1295558503277
INFO:root:current train perplexity3.4442946910858154
INFO:root:current mean train loss 1567.9375060120283
INFO:root:current train perplexity3.4474425315856934
INFO:root:current mean train loss 1569.0311449377205
INFO:root:current train perplexity3.448805093765259
INFO:root:current mean train loss 1569.685374841876
INFO:root:current train perplexity3.4553894996643066
INFO:root:current mean train loss 1570.8796120933323
INFO:root:current train perplexity3.456575632095337
INFO:root:current mean train loss 1572.1005853743152
INFO:root:current train perplexity3.456489324569702
INFO:root:current mean train loss 1573.014032784021
INFO:root:current train perplexity3.4586598873138428
INFO:root:current mean train loss 1572.9602025612114
INFO:root:current train perplexity3.457805633544922
INFO:root:current mean train loss 1574.2621625754807
INFO:root:current train perplexity3.460695743560791
INFO:root:current mean train loss 1573.7506578501134
INFO:root:current train perplexity3.462972402572632
INFO:root:current mean train loss 1574.5979638814752
INFO:root:current train perplexity3.466435670852661
INFO:root:current mean train loss 1575.4428723419128
INFO:root:current train perplexity3.4672939777374268
INFO:root:current mean train loss 1575.8142621126158
INFO:root:current train perplexity3.4682610034942627
INFO:root:current mean train loss 1576.3286651263497
INFO:root:current train perplexity3.470667600631714
INFO:root:current mean train loss 1576.9233507589222
INFO:root:current train perplexity3.4724483489990234
INFO:root:current mean train loss 1577.3196786758922
INFO:root:current train perplexity3.4723100662231445
INFO:root:current mean train loss 1577.7985088928253
INFO:root:current train perplexity3.473250389099121

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.37s/it]
INFO:root:final mean train loss: 1577.829677441357
INFO:root:final train perplexity: 3.473698377609253
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 2327.3511430456283
INFO:root:eval perplexity: 6.575343132019043
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2939.8169369528478
INFO:root:eval perplexity: 11.211138725280762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [17:46:12<16:07:42, 611.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.9316856747582
INFO:root:current train perplexity3.4440195560455322
INFO:root:current mean train loss 1570.3362718665082
INFO:root:current train perplexity3.444984197616577
INFO:root:current mean train loss 1565.388975331481
INFO:root:current train perplexity3.440279722213745
INFO:root:current mean train loss 1564.6482639312744
INFO:root:current train perplexity3.4415762424468994
INFO:root:current mean train loss 1563.734298579949
INFO:root:current train perplexity3.4374911785125732
INFO:root:current mean train loss 1566.3228542380136
INFO:root:current train perplexity3.4417262077331543
INFO:root:current mean train loss 1567.954491687797
INFO:root:current train perplexity3.4445176124572754
INFO:root:current mean train loss 1566.798491808833
INFO:root:current train perplexity3.4425110816955566
INFO:root:current mean train loss 1566.3490716580352
INFO:root:current train perplexity3.441898822784424
INFO:root:current mean train loss 1567.248607480429
INFO:root:current train perplexity3.443065881729126
INFO:root:current mean train loss 1568.3373981771433
INFO:root:current train perplexity3.444485902786255
INFO:root:current mean train loss 1569.4926079414986
INFO:root:current train perplexity3.44701886177063
INFO:root:current mean train loss 1570.323858623564
INFO:root:current train perplexity3.4494481086730957
INFO:root:current mean train loss 1571.3718930283035
INFO:root:current train perplexity3.4522507190704346
INFO:root:current mean train loss 1572.5488736957232
INFO:root:current train perplexity3.453813314437866
INFO:root:current mean train loss 1572.5340519914723
INFO:root:current train perplexity3.4562504291534424
INFO:root:current mean train loss 1572.6179371740925
INFO:root:current train perplexity3.4569756984710693
INFO:root:current mean train loss 1573.3098597505168
INFO:root:current train perplexity3.4562935829162598
INFO:root:current mean train loss 1573.6984625490354
INFO:root:current train perplexity3.458630323410034

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.11s/it]
INFO:root:final mean train loss: 1572.9753552166069
INFO:root:final train perplexity: 3.4604156017303467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 2330.6792559251717
INFO:root:eval perplexity: 6.593075752258301
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 2944.744125041556
INFO:root:eval perplexity: 11.256646156311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [17:56:24<15:57:44, 611.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1728.7425537109375
INFO:root:current train perplexity3.5850305557250977
INFO:root:current mean train loss 1548.6161456721843
INFO:root:current train perplexity3.386192560195923
INFO:root:current mean train loss 1551.4389326560556
INFO:root:current train perplexity3.4024906158447266
INFO:root:current mean train loss 1554.9664801410663
INFO:root:current train perplexity3.4020447731018066
INFO:root:current mean train loss 1558.5806148081945
INFO:root:current train perplexity3.413728713989258
INFO:root:current mean train loss 1559.9302878617764
INFO:root:current train perplexity3.4207875728607178
INFO:root:current mean train loss 1561.2947868055194
INFO:root:current train perplexity3.4237942695617676
INFO:root:current mean train loss 1561.8225320552112
INFO:root:current train perplexity3.426506280899048
INFO:root:current mean train loss 1561.7249630893511
INFO:root:current train perplexity3.430047035217285
INFO:root:current mean train loss 1562.8744656544811
INFO:root:current train perplexity3.4312777519226074
INFO:root:current mean train loss 1563.6081083223416
INFO:root:current train perplexity3.4326331615448
INFO:root:current mean train loss 1564.73147181096
INFO:root:current train perplexity3.4346272945404053
INFO:root:current mean train loss 1564.6724100359077
INFO:root:current train perplexity3.4373223781585693
INFO:root:current mean train loss 1565.8473734522122
INFO:root:current train perplexity3.4391672611236572
INFO:root:current mean train loss 1565.0956884870182
INFO:root:current train perplexity3.4388060569763184
INFO:root:current mean train loss 1565.7876106684087
INFO:root:current train perplexity3.4416990280151367
INFO:root:current mean train loss 1566.5913778253826
INFO:root:current train perplexity3.4419870376586914
INFO:root:current mean train loss 1566.844835715319
INFO:root:current train perplexity3.443480968475342
INFO:root:current mean train loss 1566.9473528568112
INFO:root:current train perplexity3.4449594020843506
INFO:root:current mean train loss 1567.7572187797953
INFO:root:current train perplexity3.4450201988220215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.37s/it]
INFO:root:final mean train loss: 1567.6940208815952
INFO:root:final train perplexity: 3.4460225105285645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2334.6434702771776
INFO:root:eval perplexity: 6.614258766174316
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 2947.663579413231
INFO:root:eval perplexity: 11.28369426727295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [18:06:34<15:46:55, 610.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1561.043212890625
INFO:root:current train perplexity3.4061272144317627
INFO:root:current mean train loss 1552.807516841565
INFO:root:current train perplexity3.4165353775024414
INFO:root:current mean train loss 1552.948589919904
INFO:root:current train perplexity3.4089648723602295
INFO:root:current mean train loss 1555.9297159062992
INFO:root:current train perplexity3.410371780395508
INFO:root:current mean train loss 1556.905427631579
INFO:root:current train perplexity3.4092249870300293
INFO:root:current mean train loss 1557.6175718565246
INFO:root:current train perplexity3.411459445953369
INFO:root:current mean train loss 1558.2259318033855
INFO:root:current train perplexity3.411341905593872
INFO:root:current mean train loss 1557.1917882722732
INFO:root:current train perplexity3.4106152057647705
INFO:root:current mean train loss 1558.2753915203812
INFO:root:current train perplexity3.415724515914917
INFO:root:current mean train loss 1557.611753509455
INFO:root:current train perplexity3.417759895324707
INFO:root:current mean train loss 1559.08509547865
INFO:root:current train perplexity3.421969175338745
INFO:root:current mean train loss 1560.6108881041062
INFO:root:current train perplexity3.423859119415283
INFO:root:current mean train loss 1562.2163594062692
INFO:root:current train perplexity3.425431728363037
INFO:root:current mean train loss 1562.5310846771565
INFO:root:current train perplexity3.426649332046509
INFO:root:current mean train loss 1562.7478361358428
INFO:root:current train perplexity3.4294419288635254
INFO:root:current mean train loss 1563.4734283969965
INFO:root:current train perplexity3.430574893951416
INFO:root:current mean train loss 1563.4901396912903
INFO:root:current train perplexity3.4317264556884766
INFO:root:current mean train loss 1563.5789166096342
INFO:root:current train perplexity3.432705879211426
INFO:root:current mean train loss 1564.4494080328443
INFO:root:current train perplexity3.43514084815979
INFO:root:current mean train loss 1565.2586403251066
INFO:root:current train perplexity3.436983108520508

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.17s/it]
INFO:root:final mean train loss: 1564.4715839641838
INFO:root:final train perplexity: 3.437269926071167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 2340.534925528452
INFO:root:eval perplexity: 6.64586877822876
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 2954.100100686364
INFO:root:eval perplexity: 11.343564987182617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: roberta_fair_baseline/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [18:16:45<15:36:39, 610.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1547.1553850446428
INFO:root:current train perplexity3.3568828105926514
INFO:root:current mean train loss 1538.0955982349537
INFO:root:current train perplexity3.368223190307617
INFO:root:current mean train loss 1546.7508176113697
INFO:root:current train perplexity3.3873918056488037
INFO:root:current mean train loss 1555.1326769472948
INFO:root:current train perplexity3.3965418338775635
INFO:root:current mean train loss 1556.900024694684
INFO:root:current train perplexity3.4041385650634766
INFO:root:current mean train loss 1557.6767279223861
INFO:root:current train perplexity3.4098823070526123
INFO:root:current mean train loss 1559.4527030404158
INFO:root:current train perplexity3.41056489944458
INFO:root:current mean train loss 1558.8032789580675
INFO:root:current train perplexity3.411633014678955
INFO:root:current mean train loss 1559.2742936002994
INFO:root:current train perplexity3.4136297702789307
slurmstepd: error: *** JOB 30170730 ON gr023 CANCELLED AT 2023-02-13T23:48:39 ***
