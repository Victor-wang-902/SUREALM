INFO:root:Output: large_distilroberta_roberta_from_scratch_64_low
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 12532.752732402147
INFO:root:current train perplexity20694.57421875
INFO:root:current mean train loss 11862.81145964196
INFO:root:current train perplexity11883.837890625
INFO:root:current mean train loss 11303.228979410535
INFO:root:current train perplexity7611.4970703125
INFO:root:current mean train loss 10779.822599712172
INFO:root:current train perplexity5029.96630859375
INFO:root:current mean train loss 10246.848997800287
INFO:root:current train perplexity3283.22509765625
INFO:root:current mean train loss 9731.82497032815
INFO:root:current train perplexity2168.536865234375
INFO:root:current mean train loss 9252.67471736968
INFO:root:current train perplexity1498.4959716796875
INFO:root:current mean train loss 8842.58003229134
INFO:root:current train perplexity1081.223388671875
INFO:root:current mean train loss 8489.563234322859
INFO:root:current train perplexity813.18505859375
INFO:root:current mean train loss 8175.274831960867
INFO:root:current train perplexity635.95556640625
INFO:root:current mean train loss 7909.0517284889675
INFO:root:current train perplexity511.8545227050781
INFO:root:current mean train loss 7666.087905285654
INFO:root:current train perplexity423.23974609375
INFO:root:current mean train loss 7451.009008018488
INFO:root:current train perplexity358.51531982421875
INFO:root:current mean train loss 7261.183876457514
INFO:root:current train perplexity307.9811706542969
INFO:root:current mean train loss 7086.903039037744
INFO:root:current train perplexity268.4834289550781
INFO:root:current mean train loss 6930.6942073781465
INFO:root:current train perplexity236.81753540039062
INFO:root:current mean train loss 6786.2536740361975
INFO:root:current train perplexity211.68032836914062
INFO:root:current mean train loss 6655.758513301661
INFO:root:current train perplexity190.75137329101562
INFO:root:current mean train loss 6531.983026762647
INFO:root:current train perplexity173.0959014892578


100%|██████████| 1/1 [07:47<00:00, 467.50s/it][A
100%|██████████| 1/1 [07:47<00:00, 467.50s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.57s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.57s/it]
INFO:root:eval mean loss: 4493.426618513045
INFO:root:eval perplexity: 40.71150588989258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/1

  0%|          | 1/200 [08:33<28:23:26, 513.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4124.764587402344
INFO:root:current train perplexity27.49201774597168
INFO:root:current mean train loss 4156.422554805361
INFO:root:current train perplexity26.583148956298828
INFO:root:current mean train loss 4143.7826899775755
INFO:root:current train perplexity26.52227210998535
INFO:root:current mean train loss 4126.61106775984
INFO:root:current train perplexity25.977474212646484
INFO:root:current mean train loss 4107.118263244629
INFO:root:current train perplexity25.499507904052734
INFO:root:current mean train loss 4086.0055773831155
INFO:root:current train perplexity25.067188262939453
INFO:root:current mean train loss 4059.0158037458145
INFO:root:current train perplexity24.544940948486328
INFO:root:current mean train loss 4035.0856981330744
INFO:root:current train perplexity24.092079162597656
INFO:root:current mean train loss 4007.3071522432215
INFO:root:current train perplexity23.603546142578125
INFO:root:current mean train loss 3981.256935619371
INFO:root:current train perplexity23.15092658996582
INFO:root:current mean train loss 3954.080968180979
INFO:root:current train perplexity22.72783088684082
INFO:root:current mean train loss 3935.117184656068
INFO:root:current train perplexity22.333290100097656
INFO:root:current mean train loss 3915.6132483231395
INFO:root:current train perplexity21.9823055267334
INFO:root:current mean train loss 3894.3637923498645
INFO:root:current train perplexity21.63407325744629
INFO:root:current mean train loss 3876.1394029175494
INFO:root:current train perplexity21.317678451538086
INFO:root:current mean train loss 3856.397593797669
INFO:root:current train perplexity20.993408203125
INFO:root:current mean train loss 3839.5033833720895
INFO:root:current train perplexity20.704862594604492
INFO:root:current mean train loss 3818.783742766836
INFO:root:current train perplexity20.4052734375
INFO:root:current mean train loss 3801.7804582906715
INFO:root:current train perplexity20.108190536499023
INFO:root:current mean train loss 3788.081037485525
INFO:root:current train perplexity19.85034942626953


100%|██████████| 1/1 [08:07<00:00, 487.88s/it][A
100%|██████████| 1/1 [08:07<00:00, 487.88s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:48<00:00, 48.36s/it][A
100%|██████████| 1/1 [00:48<00:00, 48.36s/it]
INFO:root:eval mean loss: 3879.3062945758256
INFO:root:eval perplexity: 24.531034469604492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/2

  1%|          | 2/200 [17:32<29:03:27, 528.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3421.203975793087
INFO:root:current train perplexity14.846858024597168
INFO:root:current mean train loss 3397.7291360726035
INFO:root:current train perplexity14.606367111206055
INFO:root:current mean train loss 3395.3554268374464
INFO:root:current train perplexity14.551159858703613
INFO:root:current mean train loss 3401.724583714574
INFO:root:current train perplexity14.552974700927734
INFO:root:current mean train loss 3389.4922872988236
INFO:root:current train perplexity14.441911697387695
INFO:root:current mean train loss 3384.701476478219
INFO:root:current train perplexity14.370394706726074
INFO:root:current mean train loss 3374.6939016525967
INFO:root:current train perplexity14.248634338378906
INFO:root:current mean train loss 3366.2112472554995
INFO:root:current train perplexity14.141427993774414
INFO:root:current mean train loss 3351.0351547845703
INFO:root:current train perplexity13.995943069458008
INFO:root:current mean train loss 3339.1843311436564
INFO:root:current train perplexity13.898115158081055
INFO:root:current mean train loss 3330.271525262055
INFO:root:current train perplexity13.790241241455078
INFO:root:current mean train loss 3319.070860685128
INFO:root:current train perplexity13.683450698852539
INFO:root:current mean train loss 3310.5475577225266
INFO:root:current train perplexity13.581672668457031
INFO:root:current mean train loss 3300.493767729042
INFO:root:current train perplexity13.493497848510742
INFO:root:current mean train loss 3293.1873190667525
INFO:root:current train perplexity13.395729064941406
INFO:root:current mean train loss 3284.1821039029373
INFO:root:current train perplexity13.312470436096191
INFO:root:current mean train loss 3272.0196743256756
INFO:root:current train perplexity13.201278686523438
INFO:root:current mean train loss 3264.837287669504
INFO:root:current train perplexity13.124213218688965
INFO:root:current mean train loss 3254.3878931023683
INFO:root:current train perplexity13.040261268615723
INFO:root:current mean train loss 3247.152623633924
INFO:root:current train perplexity12.962444305419922


100%|██████████| 1/1 [08:14<00:00, 494.96s/it][A
100%|██████████| 1/1 [08:14<00:00, 494.96s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:48<00:00, 48.30s/it][A
100%|██████████| 1/1 [00:48<00:00, 48.30s/it]
INFO:root:eval mean loss: 3641.784868120073
INFO:root:eval perplexity: 20.16632652282715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/3

  2%|▏         | 3/200 [27:08<30:06:04, 550.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3026.8811767578127
INFO:root:current train perplexity11.016059875488281
INFO:root:current mean train loss 3057.6944254557293
INFO:root:current train perplexity11.172990798950195
INFO:root:current mean train loss 3065.6995400390624
INFO:root:current train perplexity11.194046974182129
INFO:root:current mean train loss 3045.1456180245536
INFO:root:current train perplexity11.067286491394043
INFO:root:current mean train loss 3043.323156467014
INFO:root:current train perplexity11.000829696655273
INFO:root:current mean train loss 3037.102186612216
INFO:root:current train perplexity10.951709747314453
INFO:root:current mean train loss 3025.208487830529
INFO:root:current train perplexity10.883748054504395
INFO:root:current mean train loss 3017.5335830078125
INFO:root:current train perplexity10.83707332611084
INFO:root:current mean train loss 3015.3291690602023
INFO:root:current train perplexity10.79560375213623
INFO:root:current mean train loss 3006.60742161801
INFO:root:current train perplexity10.740068435668945
INFO:root:current mean train loss 2999.8388897414434
INFO:root:current train perplexity10.687989234924316
INFO:root:current mean train loss 2998.658811565897
INFO:root:current train perplexity10.658602714538574
INFO:root:current mean train loss 2992.839069140625
INFO:root:current train perplexity10.612548828125
INFO:root:current mean train loss 2987.212622974537
INFO:root:current train perplexity10.563238143920898
INFO:root:current mean train loss 2985.7567051117994
INFO:root:current train perplexity10.541380882263184
INFO:root:current mean train loss 2980.0892565524196
INFO:root:current train perplexity10.500919342041016
INFO:root:current mean train loss 2975.3095484138257
INFO:root:current train perplexity10.452688217163086
INFO:root:current mean train loss 2970.3859698660713
INFO:root:current train perplexity10.41525650024414
INFO:root:current mean train loss 2966.147953441723
INFO:root:current train perplexity10.386125564575195
INFO:root:current mean train loss 2962.176028520633
INFO:root:current train perplexity10.351310729980469


100%|██████████| 1/1 [08:10<00:00, 490.40s/it][A
100%|██████████| 1/1 [08:10<00:00, 490.40s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:47<00:00, 47.10s/it][A
100%|██████████| 1/1 [00:47<00:00, 47.10s/it]
INFO:root:eval mean loss: 3473.0534726621154
INFO:root:eval perplexity: 17.54609489440918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/4

  2%|▏         | 4/200 [36:51<30:40:21, 563.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2878.5069197469684
INFO:root:current train perplexity9.44274616241455
INFO:root:current mean train loss 2855.7787501169537
INFO:root:current train perplexity9.360876083374023
INFO:root:current mean train loss 2840.769674808345
INFO:root:current train perplexity9.316481590270996
INFO:root:current mean train loss 2832.923327204317
INFO:root:current train perplexity9.313920974731445
INFO:root:current mean train loss 2824.2025896680943
INFO:root:current train perplexity9.261173248291016
INFO:root:current mean train loss 2825.327753406774
INFO:root:current train perplexity9.276081085205078
INFO:root:current mean train loss 2821.497822133855
INFO:root:current train perplexity9.256924629211426
INFO:root:current mean train loss 2819.39668076974
INFO:root:current train perplexity9.22745418548584
INFO:root:current mean train loss 2819.7075062964063
INFO:root:current train perplexity9.222503662109375
INFO:root:current mean train loss 2818.931766861104
INFO:root:current train perplexity9.212427139282227
INFO:root:current mean train loss 2815.6922571956275
INFO:root:current train perplexity9.187058448791504
INFO:root:current mean train loss 2809.9459480605988
INFO:root:current train perplexity9.160196304321289
INFO:root:current mean train loss 2807.1272957311808
INFO:root:current train perplexity9.146194458007812
INFO:root:current mean train loss 2800.8681679916103
INFO:root:current train perplexity9.121252059936523
INFO:root:current mean train loss 2796.907821353634
INFO:root:current train perplexity9.098873138427734
INFO:root:current mean train loss 2795.058860949216
INFO:root:current train perplexity9.07912826538086
INFO:root:current mean train loss 2789.8010073766495
INFO:root:current train perplexity9.04559326171875
INFO:root:current mean train loss 2786.9905771666754
INFO:root:current train perplexity9.017660140991211
INFO:root:current mean train loss 2783.6779719773117
INFO:root:current train perplexity8.997479438781738
INFO:root:current mean train loss 2781.1029562239846
INFO:root:current train perplexity8.9723539352417


100%|██████████| 1/1 [08:02<00:00, 482.67s/it][A
100%|██████████| 1/1 [08:02<00:00, 482.67s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.43s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.43s/it]
INFO:root:eval mean loss: 3383.0270922778245
INFO:root:eval perplexity: 16.29032325744629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/5

  2%|▎         | 5/200 [46:32<30:50:41, 569.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2716.985540480841
INFO:root:current train perplexity8.43662166595459
INFO:root:current mean train loss 2697.3328247070312
INFO:root:current train perplexity8.351642608642578
INFO:root:current mean train loss 2688.426827444157
INFO:root:current train perplexity8.321280479431152
INFO:root:current mean train loss 2692.2728430430093
INFO:root:current train perplexity8.329327583312988
INFO:root:current mean train loss 2685.3257849827287
INFO:root:current train perplexity8.313915252685547
INFO:root:current mean train loss 2679.140706937607
INFO:root:current train perplexity8.290846824645996
INFO:root:current mean train loss 2676.4365280775996
INFO:root:current train perplexity8.280538558959961
INFO:root:current mean train loss 2670.4861375458386
INFO:root:current train perplexity8.26102066040039
INFO:root:current mean train loss 2671.9321769610788
INFO:root:current train perplexity8.254785537719727
INFO:root:current mean train loss 2668.8544028677593
INFO:root:current train perplexity8.240362167358398
INFO:root:current mean train loss 2669.682353903007
INFO:root:current train perplexity8.24057674407959
INFO:root:current mean train loss 2666.890683560758
INFO:root:current train perplexity8.226202964782715
INFO:root:current mean train loss 2663.5884625681465
INFO:root:current train perplexity8.212654113769531
INFO:root:current mean train loss 2662.138632008106
INFO:root:current train perplexity8.198925971984863
INFO:root:current mean train loss 2661.009663790063
INFO:root:current train perplexity8.179705619812012
INFO:root:current mean train loss 2659.211089317245
INFO:root:current train perplexity8.164752006530762
INFO:root:current mean train loss 2656.0066644301605
INFO:root:current train perplexity8.142866134643555
INFO:root:current mean train loss 2655.2464403913696
INFO:root:current train perplexity8.130080223083496
INFO:root:current mean train loss 2653.9708665333483
INFO:root:current train perplexity8.117833137512207


100%|██████████| 1/1 [07:58<00:00, 478.63s/it][A
100%|██████████| 1/1 [07:58<00:00, 478.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.96s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.96s/it]
INFO:root:eval mean loss: 3327.2168863199136
INFO:root:eval perplexity: 15.557377815246582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/6

  3%|▎         | 6/200 [56:03<30:42:45, 569.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2529.552734375
INFO:root:current train perplexity7.052079677581787
INFO:root:current mean train loss 2605.241675046411
INFO:root:current train perplexity7.780178070068359
INFO:root:current mean train loss 2593.511877866527
INFO:root:current train perplexity7.708959102630615
INFO:root:current mean train loss 2596.5311932231107
INFO:root:current train perplexity7.708367347717285
INFO:root:current mean train loss 2587.717137819514
INFO:root:current train perplexity7.677061557769775
INFO:root:current mean train loss 2588.003657723615
INFO:root:current train perplexity7.674554347991943
INFO:root:current mean train loss 2588.390087565646
INFO:root:current train perplexity7.658891201019287
INFO:root:current mean train loss 2584.5974894263777
INFO:root:current train perplexity7.651986598968506
INFO:root:current mean train loss 2578.0101257400415
INFO:root:current train perplexity7.623388290405273
INFO:root:current mean train loss 2576.4787688429956
INFO:root:current train perplexity7.608272075653076
INFO:root:current mean train loss 2573.5482425289556
INFO:root:current train perplexity7.591770172119141
INFO:root:current mean train loss 2575.188713163814
INFO:root:current train perplexity7.6018242835998535
INFO:root:current mean train loss 2572.340375736691
INFO:root:current train perplexity7.585213661193848
INFO:root:current mean train loss 2572.403350712793
INFO:root:current train perplexity7.5823893547058105
INFO:root:current mean train loss 2570.1745070485367
INFO:root:current train perplexity7.571245193481445
INFO:root:current mean train loss 2568.540041420954
INFO:root:current train perplexity7.569232940673828
INFO:root:current mean train loss 2567.3540589560725
INFO:root:current train perplexity7.564657688140869
INFO:root:current mean train loss 2565.1326992566414
INFO:root:current train perplexity7.549945831298828
INFO:root:current mean train loss 2560.566815365162
INFO:root:current train perplexity7.5355305671691895
INFO:root:current mean train loss 2557.7545458188124
INFO:root:current train perplexity7.523222923278809


100%|██████████| 1/1 [07:57<00:00, 477.53s/it][A
100%|██████████| 1/1 [07:57<00:00, 477.53s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.33s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.33s/it]
INFO:root:eval mean loss: 3274.8561153927367
INFO:root:eval perplexity: 14.899736404418945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/7

  4%|▎         | 7/200 [1:05:35<30:36:13, 570.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2484.9906616210938
INFO:root:current train perplexity7.389000415802002
INFO:root:current mean train loss 2498.9814804853017
INFO:root:current train perplexity7.271280288696289
INFO:root:current mean train loss 2506.8011373817376
INFO:root:current train perplexity7.276901721954346
INFO:root:current mean train loss 2498.66608241819
INFO:root:current train perplexity7.23763370513916
INFO:root:current mean train loss 2493.7095193817286
INFO:root:current train perplexity7.201193332672119
INFO:root:current mean train loss 2490.1037741407004
INFO:root:current train perplexity7.158878803253174
INFO:root:current mean train loss 2485.6486863812197
INFO:root:current train perplexity7.139829635620117
INFO:root:current mean train loss 2487.415150931951
INFO:root:current train perplexity7.141931533813477
INFO:root:current mean train loss 2481.169344951005
INFO:root:current train perplexity7.1186017990112305
INFO:root:current mean train loss 2482.8141714856515
INFO:root:current train perplexity7.118658542633057
INFO:root:current mean train loss 2485.3813080853233
INFO:root:current train perplexity7.117044925689697
INFO:root:current mean train loss 2485.230153638263
INFO:root:current train perplexity7.1075592041015625
INFO:root:current mean train loss 2487.848289051275
INFO:root:current train perplexity7.1147966384887695
INFO:root:current mean train loss 2486.5079212333435
INFO:root:current train perplexity7.1114044189453125
INFO:root:current mean train loss 2486.036053785316
INFO:root:current train perplexity7.107715606689453
INFO:root:current mean train loss 2485.206111299819
INFO:root:current train perplexity7.104404449462891
INFO:root:current mean train loss 2485.7161436705715
INFO:root:current train perplexity7.10019588470459
INFO:root:current mean train loss 2483.7384011887007
INFO:root:current train perplexity7.091160297393799
INFO:root:current mean train loss 2482.287897728982
INFO:root:current train perplexity7.083549499511719
INFO:root:current mean train loss 2481.2222766100554
INFO:root:current train perplexity7.079949855804443


100%|██████████| 1/1 [07:53<00:00, 473.55s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.56s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.92s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.92s/it]
INFO:root:eval mean loss: 3228.9599272123687
INFO:root:eval perplexity: 14.346205711364746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/8

  4%|▍         | 8/200 [1:14:17<29:36:32, 555.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2448.1990792410716
INFO:root:current train perplexity6.789308071136475
INFO:root:current mean train loss 2442.082472511574
INFO:root:current train perplexity6.7971649169921875
INFO:root:current mean train loss 2428.9019816946475
INFO:root:current train perplexity6.779128074645996
INFO:root:current mean train loss 2425.750496662197
INFO:root:current train perplexity6.7602458000183105
INFO:root:current mean train loss 2426.531885607489
INFO:root:current train perplexity6.7739362716674805
INFO:root:current mean train loss 2422.3138309086594
INFO:root:current train perplexity6.7746500968933105
INFO:root:current mean train loss 2420.8617808424583
INFO:root:current train perplexity6.758548736572266
INFO:root:current mean train loss 2419.374095351031
INFO:root:current train perplexity6.760365962982178
INFO:root:current mean train loss 2418.8884055131925
INFO:root:current train perplexity6.758742332458496
INFO:root:current mean train loss 2420.090830234793
INFO:root:current train perplexity6.762904167175293
INFO:root:current mean train loss 2421.9840348543175
INFO:root:current train perplexity6.765953063964844
INFO:root:current mean train loss 2421.193563076473
INFO:root:current train perplexity6.768111705780029
INFO:root:current mean train loss 2421.2832377198256
INFO:root:current train perplexity6.759703636169434
INFO:root:current mean train loss 2419.7097941537922
INFO:root:current train perplexity6.755439758300781
INFO:root:current mean train loss 2419.1737735123584
INFO:root:current train perplexity6.747578144073486
INFO:root:current mean train loss 2416.611337747481
INFO:root:current train perplexity6.738382339477539
INFO:root:current mean train loss 2416.2791078340024
INFO:root:current train perplexity6.732644557952881
INFO:root:current mean train loss 2415.0316139594966
INFO:root:current train perplexity6.7274370193481445
INFO:root:current mean train loss 2414.65224868816
INFO:root:current train perplexity6.726817607879639
INFO:root:current mean train loss 2415.7126614356225
INFO:root:current train perplexity6.726961135864258


100%|██████████| 1/1 [07:53<00:00, 473.86s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.86s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.74s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.74s/it]
INFO:root:eval mean loss: 3200.2180982251784
INFO:root:eval perplexity: 14.010074615478516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/9

  4%|▍         | 9/200 [1:22:59<28:54:02, 544.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2408.6647526667666
INFO:root:current train perplexity6.483755588531494
INFO:root:current mean train loss 2374.149827655993
INFO:root:current train perplexity6.519829750061035
INFO:root:current mean train loss 2371.961680578807
INFO:root:current train perplexity6.501648426055908
INFO:root:current mean train loss 2368.006289395419
INFO:root:current train perplexity6.508485794067383
INFO:root:current mean train loss 2362.912848818619
INFO:root:current train perplexity6.493283271789551
INFO:root:current mean train loss 2370.4029587455416
INFO:root:current train perplexity6.502871036529541
INFO:root:current mean train loss 2367.8356982272094
INFO:root:current train perplexity6.494018077850342
INFO:root:current mean train loss 2368.8204050266995
INFO:root:current train perplexity6.4992194175720215
INFO:root:current mean train loss 2366.1935867524485
INFO:root:current train perplexity6.488714694976807
INFO:root:current mean train loss 2368.3519490987314
INFO:root:current train perplexity6.49045467376709
INFO:root:current mean train loss 2368.9231545335892
INFO:root:current train perplexity6.488712787628174
INFO:root:current mean train loss 2365.9873524771797
INFO:root:current train perplexity6.476800441741943
INFO:root:current mean train loss 2365.823629263491
INFO:root:current train perplexity6.4738240242004395
INFO:root:current mean train loss 2366.10886739697
INFO:root:current train perplexity6.468133449554443
INFO:root:current mean train loss 2364.8626121331836
INFO:root:current train perplexity6.462850570678711
INFO:root:current mean train loss 2366.4393506394217
INFO:root:current train perplexity6.465056896209717
INFO:root:current mean train loss 2364.5345614897424
INFO:root:current train perplexity6.4612932205200195
INFO:root:current mean train loss 2364.263230206215
INFO:root:current train perplexity6.4567766189575195
INFO:root:current mean train loss 2362.6247183550513
INFO:root:current train perplexity6.451458930969238
INFO:root:current mean train loss 2362.986331001657
INFO:root:current train perplexity6.4509968757629395


100%|██████████| 1/1 [07:55<00:00, 475.88s/it][A
100%|██████████| 1/1 [07:55<00:00, 475.88s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.13s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.13s/it]
INFO:root:eval mean loss: 3166.7168247348914
INFO:root:eval perplexity: 13.628215789794922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/10

  5%|▌         | 10/200 [1:31:42<28:23:54, 538.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2320.482264421988
INFO:root:current train perplexity6.2190070152282715
INFO:root:current mean train loss 2329.072736570821
INFO:root:current train perplexity6.252361297607422
INFO:root:current mean train loss 2319.651090827573
INFO:root:current train perplexity6.227370738983154
INFO:root:current mean train loss 2327.508996813601
INFO:root:current train perplexity6.250521183013916
INFO:root:current mean train loss 2322.529644866488
INFO:root:current train perplexity6.236043930053711
INFO:root:current mean train loss 2319.005147333933
INFO:root:current train perplexity6.237146377563477
INFO:root:current mean train loss 2322.079556634786
INFO:root:current train perplexity6.234899997711182
INFO:root:current mean train loss 2321.141414250447
INFO:root:current train perplexity6.221744060516357
INFO:root:current mean train loss 2321.029978305479
INFO:root:current train perplexity6.217867851257324
INFO:root:current mean train loss 2318.003931067184
INFO:root:current train perplexity6.212955474853516
INFO:root:current mean train loss 2315.5717695787534
INFO:root:current train perplexity6.216070652008057
INFO:root:current mean train loss 2315.1365911452763
INFO:root:current train perplexity6.2178778648376465
INFO:root:current mean train loss 2316.1671086785545
INFO:root:current train perplexity6.216118812561035
INFO:root:current mean train loss 2317.4499578594377
INFO:root:current train perplexity6.223108291625977
INFO:root:current mean train loss 2316.6616800099187
INFO:root:current train perplexity6.217717170715332
INFO:root:current mean train loss 2318.1860377236944
INFO:root:current train perplexity6.2238569259643555
INFO:root:current mean train loss 2318.208398159569
INFO:root:current train perplexity6.224850177764893
INFO:root:current mean train loss 2317.339781369241
INFO:root:current train perplexity6.221517086029053
INFO:root:current mean train loss 2316.1072998046875
INFO:root:current train perplexity6.219693660736084
INFO:root:current mean train loss 2317.0948354276443
INFO:root:current train perplexity6.2210540771484375


100%|██████████| 1/1 [07:52<00:00, 472.06s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.06s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.89s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.89s/it]
INFO:root:eval mean loss: 3145.6682268205705
INFO:root:eval perplexity: 13.39363956451416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/11

  6%|▌         | 11/200 [1:40:22<27:57:33, 532.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2272.0451447242913
INFO:root:current train perplexity6.044062614440918
INFO:root:current mean train loss 2267.208969280284
INFO:root:current train perplexity6.005383491516113
INFO:root:current mean train loss 2264.49328186462
INFO:root:current train perplexity6.02667236328125
INFO:root:current mean train loss 2274.780889165216
INFO:root:current train perplexity6.035356044769287
INFO:root:current mean train loss 2275.8290485648954
INFO:root:current train perplexity6.031132221221924
INFO:root:current mean train loss 2276.23030418422
INFO:root:current train perplexity6.0248541831970215
INFO:root:current mean train loss 2277.6747219430463
INFO:root:current train perplexity6.0378313064575195
INFO:root:current mean train loss 2277.3189265515666
INFO:root:current train perplexity6.042245864868164
INFO:root:current mean train loss 2277.1380270792183
INFO:root:current train perplexity6.041990756988525
INFO:root:current mean train loss 2275.6798520349344
INFO:root:current train perplexity6.038095474243164
INFO:root:current mean train loss 2273.3010407899187
INFO:root:current train perplexity6.028632640838623
INFO:root:current mean train loss 2274.4320088944587
INFO:root:current train perplexity6.0273823738098145
INFO:root:current mean train loss 2271.741510797622
INFO:root:current train perplexity6.021015167236328
INFO:root:current mean train loss 2271.907257124115
INFO:root:current train perplexity6.01984167098999
INFO:root:current mean train loss 2273.0900201194218
INFO:root:current train perplexity6.022906303405762
INFO:root:current mean train loss 2274.194681829041
INFO:root:current train perplexity6.022105693817139
INFO:root:current mean train loss 2273.865383161769
INFO:root:current train perplexity6.02280330657959
INFO:root:current mean train loss 2274.302623718877
INFO:root:current train perplexity6.022132396697998
INFO:root:current mean train loss 2275.243856808287
INFO:root:current train perplexity6.021425724029541


100%|██████████| 1/1 [07:53<00:00, 473.30s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.30s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:46<00:00, 46.11s/it][A
100%|██████████| 1/1 [00:46<00:00, 46.11s/it]
INFO:root:eval mean loss: 3130.021631739161
INFO:root:eval perplexity: 13.221884727478027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/12

  6%|▌         | 12/200 [1:49:03<27:38:10, 529.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2243.0599772135415
INFO:root:current train perplexity5.959152698516846
INFO:root:current mean train loss 2237.015331083131
INFO:root:current train perplexity5.880657196044922
INFO:root:current mean train loss 2247.2131251443197
INFO:root:current train perplexity5.856505393981934
INFO:root:current mean train loss 2241.6608741684713
INFO:root:current train perplexity5.837076663970947
INFO:root:current mean train loss 2238.5665313493523
INFO:root:current train perplexity5.838201522827148
INFO:root:current mean train loss 2238.1154773022026
INFO:root:current train perplexity5.832071781158447
INFO:root:current mean train loss 2238.2683933441517
INFO:root:current train perplexity5.837542533874512
INFO:root:current mean train loss 2241.0745919066167
INFO:root:current train perplexity5.850627899169922
INFO:root:current mean train loss 2242.7915920765877
INFO:root:current train perplexity5.854514122009277
INFO:root:current mean train loss 2242.181070422809
INFO:root:current train perplexity5.849581718444824
INFO:root:current mean train loss 2240.1124046074665
INFO:root:current train perplexity5.842949867248535
INFO:root:current mean train loss 2241.406639673228
INFO:root:current train perplexity5.8446760177612305
INFO:root:current mean train loss 2241.4639014848944
INFO:root:current train perplexity5.844066619873047
INFO:root:current mean train loss 2243.410184074162
INFO:root:current train perplexity5.850653171539307
INFO:root:current mean train loss 2243.3778484233003
INFO:root:current train perplexity5.853485584259033
INFO:root:current mean train loss 2242.344292209851
INFO:root:current train perplexity5.852850914001465
INFO:root:current mean train loss 2240.7562270328094
INFO:root:current train perplexity5.852084159851074
INFO:root:current mean train loss 2239.23700098115
INFO:root:current train perplexity5.8507890701293945
INFO:root:current mean train loss 2240.060536516288
INFO:root:current train perplexity5.854712963104248
INFO:root:current mean train loss 2239.073831755678
INFO:root:current train perplexity5.8527512550354


100%|██████████| 1/1 [07:53<00:00, 473.27s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.61s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.62s/it]
INFO:root:eval mean loss: 3120.4929895716027
INFO:root:eval perplexity: 13.118371963500977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/13

  6%|▋         | 13/200 [1:57:44<27:21:36, 526.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2177.0145629882813
INFO:root:current train perplexity5.56303596496582
INFO:root:current mean train loss 2210.214236450195
INFO:root:current train perplexity5.678651332855225
INFO:root:current mean train loss 2213.6700711337003
INFO:root:current train perplexity5.693745136260986
INFO:root:current mean train loss 2209.472957992554
INFO:root:current train perplexity5.712018966674805
INFO:root:current mean train loss 2209.277676827567
INFO:root:current train perplexity5.7095465660095215
INFO:root:current mean train loss 2211.3294889009917
INFO:root:current train perplexity5.707127094268799
INFO:root:current mean train loss 2209.4557274603076
INFO:root:current train perplexity5.705191135406494
INFO:root:current mean train loss 2213.7129670884874
INFO:root:current train perplexity5.71437931060791
INFO:root:current mean train loss 2215.255138564691
INFO:root:current train perplexity5.713515281677246
INFO:root:current mean train loss 2214.03531918733
INFO:root:current train perplexity5.707647800445557
INFO:root:current mean train loss 2213.840717988856
INFO:root:current train perplexity5.714399337768555
INFO:root:current mean train loss 2212.001939828055
INFO:root:current train perplexity5.710158824920654
INFO:root:current mean train loss 2211.1530231413294
INFO:root:current train perplexity5.704288005828857
INFO:root:current mean train loss 2210.3922277277165
INFO:root:current train perplexity5.699948787689209
INFO:root:current mean train loss 2209.300975960745
INFO:root:current train perplexity5.699614524841309
INFO:root:current mean train loss 2208.928963751542
INFO:root:current train perplexity5.701369762420654
INFO:root:current mean train loss 2209.3019122841915
INFO:root:current train perplexity5.702178478240967
INFO:root:current mean train loss 2208.010835159657
INFO:root:current train perplexity5.700895309448242
INFO:root:current mean train loss 2207.798697268308
INFO:root:current train perplexity5.702201843261719
INFO:root:current mean train loss 2207.849534034729
INFO:root:current train perplexity5.704267978668213


100%|██████████| 1/1 [07:50<00:00, 470.07s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.99s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.99s/it]
INFO:root:eval mean loss: 3108.565892308324
INFO:root:eval perplexity: 12.989938735961914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/14

  7%|▋         | 14/200 [2:06:23<27:04:51, 524.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2182.988469304265
INFO:root:current train perplexity5.528811454772949
INFO:root:current mean train loss 2158.660668588903
INFO:root:current train perplexity5.502874374389648
INFO:root:current mean train loss 2163.77018486699
INFO:root:current train perplexity5.522662162780762
INFO:root:current mean train loss 2170.186302117141
INFO:root:current train perplexity5.548080921173096
INFO:root:current mean train loss 2167.3956134019236
INFO:root:current train perplexity5.544386863708496
INFO:root:current mean train loss 2163.484322034669
INFO:root:current train perplexity5.538749694824219
INFO:root:current mean train loss 2165.205003579707
INFO:root:current train perplexity5.5468597412109375
INFO:root:current mean train loss 2168.4632074777924
INFO:root:current train perplexity5.552650451660156
INFO:root:current mean train loss 2171.5282749554303
INFO:root:current train perplexity5.56514310836792
INFO:root:current mean train loss 2170.5244790711295
INFO:root:current train perplexity5.560329437255859
INFO:root:current mean train loss 2171.691373642983
INFO:root:current train perplexity5.563616752624512
INFO:root:current mean train loss 2172.7392170150342
INFO:root:current train perplexity5.564985752105713
INFO:root:current mean train loss 2172.470752762322
INFO:root:current train perplexity5.565190315246582
INFO:root:current mean train loss 2173.339596139912
INFO:root:current train perplexity5.565847873687744
INFO:root:current mean train loss 2172.8051406977156
INFO:root:current train perplexity5.560632228851318
INFO:root:current mean train loss 2172.7592274672656
INFO:root:current train perplexity5.56077241897583
INFO:root:current mean train loss 2173.3419184233258
INFO:root:current train perplexity5.562097549438477
INFO:root:current mean train loss 2174.3812463175104
INFO:root:current train perplexity5.565631866455078
INFO:root:current mean train loss 2174.9870208756847
INFO:root:current train perplexity5.564943313598633
INFO:root:current mean train loss 2176.4026678003597
INFO:root:current train perplexity5.5671000480651855


100%|██████████| 1/1 [07:53<00:00, 473.25s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.16s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.16s/it]
INFO:root:eval mean loss: 3103.821848459788
INFO:root:eval perplexity: 12.93920612335205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/15

  8%|▊         | 15/200 [2:15:02<26:51:49, 522.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2116.9363945855034
INFO:root:current train perplexity5.410796642303467
INFO:root:current mean train loss 2132.237732726258
INFO:root:current train perplexity5.3870086669921875
INFO:root:current mean train loss 2137.1411387526146
INFO:root:current train perplexity5.420064449310303
INFO:root:current mean train loss 2147.160197629767
INFO:root:current train perplexity5.430197238922119
INFO:root:current mean train loss 2146.572898293382
INFO:root:current train perplexity5.4327192306518555
INFO:root:current mean train loss 2146.9528768931914
INFO:root:current train perplexity5.433602809906006
INFO:root:current mean train loss 2143.8020202450066
INFO:root:current train perplexity5.431424617767334
INFO:root:current mean train loss 2143.801096463393
INFO:root:current train perplexity5.434079647064209
INFO:root:current mean train loss 2143.318852945011
INFO:root:current train perplexity5.443942546844482
INFO:root:current mean train loss 2145.0941658579827
INFO:root:current train perplexity5.450260162353516
INFO:root:current mean train loss 2143.9207581840383
INFO:root:current train perplexity5.450172424316406
INFO:root:current mean train loss 2146.690479721519
INFO:root:current train perplexity5.455489635467529
INFO:root:current mean train loss 2147.851757870907
INFO:root:current train perplexity5.4498610496521
INFO:root:current mean train loss 2147.6712465272167
INFO:root:current train perplexity5.449747562408447
INFO:root:current mean train loss 2148.485131600864
INFO:root:current train perplexity5.447563648223877
INFO:root:current mean train loss 2148.362481273126
INFO:root:current train perplexity5.449597358703613
INFO:root:current mean train loss 2147.4479150183975
INFO:root:current train perplexity5.448424816131592
INFO:root:current mean train loss 2148.9905363493936
INFO:root:current train perplexity5.449670314788818
INFO:root:current mean train loss 2148.2607541048283
INFO:root:current train perplexity5.445919513702393
INFO:root:current mean train loss 2148.2763605029945
INFO:root:current train perplexity5.4459333419799805


100%|██████████| 1/1 [07:49<00:00, 469.16s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.40s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.40s/it]
INFO:root:eval mean loss: 3088.434888501783
INFO:root:eval perplexity: 12.77601432800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/16

  8%|▊         | 16/200 [2:23:38<26:36:34, 520.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2092.6834596445865
INFO:root:current train perplexity5.306926727294922
INFO:root:current mean train loss 2113.9635345280517
INFO:root:current train perplexity5.323618412017822
INFO:root:current mean train loss 2111.8487958732126
INFO:root:current train perplexity5.3245015144348145
INFO:root:current mean train loss 2114.57066686584
INFO:root:current train perplexity5.339819431304932
INFO:root:current mean train loss 2119.72766579792
INFO:root:current train perplexity5.347855091094971
INFO:root:current mean train loss 2121.949239914555
INFO:root:current train perplexity5.35356330871582
INFO:root:current mean train loss 2118.7894410816994
INFO:root:current train perplexity5.336928367614746
INFO:root:current mean train loss 2114.923393675016
INFO:root:current train perplexity5.328218460083008
INFO:root:current mean train loss 2114.5824738424762
INFO:root:current train perplexity5.326601982116699
INFO:root:current mean train loss 2114.401525086895
INFO:root:current train perplexity5.3301191329956055
INFO:root:current mean train loss 2114.5828907572145
INFO:root:current train perplexity5.326693058013916
INFO:root:current mean train loss 2116.2316744419168
INFO:root:current train perplexity5.329348564147949
INFO:root:current mean train loss 2117.7443133098323
INFO:root:current train perplexity5.331699371337891
INFO:root:current mean train loss 2116.8704522353382
INFO:root:current train perplexity5.331585884094238
INFO:root:current mean train loss 2117.5951090384146
INFO:root:current train perplexity5.331795692443848
INFO:root:current mean train loss 2119.255588737891
INFO:root:current train perplexity5.334259510040283
INFO:root:current mean train loss 2119.708259769716
INFO:root:current train perplexity5.334098815917969
INFO:root:current mean train loss 2120.5724598622337
INFO:root:current train perplexity5.333614826202393
INFO:root:current mean train loss 2121.562351375643
INFO:root:current train perplexity5.3333210945129395
INFO:root:current mean train loss 2122.083816288329
INFO:root:current train perplexity5.332553863525391


100%|██████████| 1/1 [07:51<00:00, 471.70s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.24s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.24s/it]
INFO:root:eval mean loss: 3080.648971236862
INFO:root:eval perplexity: 12.694229125976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/17

  8%|▊         | 17/200 [2:32:16<26:25:32, 519.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2048.522232055664
INFO:root:current train perplexity5.143561363220215
INFO:root:current mean train loss 2081.9517192434755
INFO:root:current train perplexity5.222251892089844
INFO:root:current mean train loss 2080.71533203125
INFO:root:current train perplexity5.2128167152404785
INFO:root:current mean train loss 2081.446509292445
INFO:root:current train perplexity5.206581115722656
INFO:root:current mean train loss 2082.4860434610337
INFO:root:current train perplexity5.198652744293213
INFO:root:current mean train loss 2082.299347961841
INFO:root:current train perplexity5.205854415893555
INFO:root:current mean train loss 2082.710210400958
INFO:root:current train perplexity5.2034125328063965
INFO:root:current mean train loss 2085.1254131491414
INFO:root:current train perplexity5.204370021820068
INFO:root:current mean train loss 2087.6683298746743
INFO:root:current train perplexity5.209949970245361
INFO:root:current mean train loss 2087.876093196483
INFO:root:current train perplexity5.213331699371338
INFO:root:current mean train loss 2088.7889057608213
INFO:root:current train perplexity5.212130069732666
INFO:root:current mean train loss 2089.681730122679
INFO:root:current train perplexity5.214304447174072
INFO:root:current mean train loss 2091.0214458963146
INFO:root:current train perplexity5.2159504890441895
INFO:root:current mean train loss 2090.900266180121
INFO:root:current train perplexity5.2136454582214355
INFO:root:current mean train loss 2093.0805709182573
INFO:root:current train perplexity5.21708345413208
INFO:root:current mean train loss 2093.9596265509385
INFO:root:current train perplexity5.218457221984863
INFO:root:current mean train loss 2095.377003168043
INFO:root:current train perplexity5.222921848297119
INFO:root:current mean train loss 2094.9769960271165
INFO:root:current train perplexity5.2257771492004395
INFO:root:current mean train loss 2095.22422984495
INFO:root:current train perplexity5.224820613861084


100%|██████████| 1/1 [07:43<00:00, 463.07s/it][A
100%|██████████| 1/1 [07:43<00:00, 463.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.63s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.63s/it]
INFO:root:eval mean loss: 3082.274071679101
INFO:root:eval perplexity: 12.711255073547363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/18

  9%|▉         | 18/200 [2:40:47<26:08:31, 517.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2003.99013671875
INFO:root:current train perplexity5.185429573059082
INFO:root:current mean train loss 2070.6619442894344
INFO:root:current train perplexity5.1440911293029785
INFO:root:current mean train loss 2070.314933665206
INFO:root:current train perplexity5.126125335693359
INFO:root:current mean train loss 2066.242242331583
INFO:root:current train perplexity5.1134724617004395
INFO:root:current mean train loss 2068.8444194275658
INFO:root:current train perplexity5.1237382888793945
INFO:root:current mean train loss 2067.861477026609
INFO:root:current train perplexity5.1209940910339355
INFO:root:current mean train loss 2067.391771452092
INFO:root:current train perplexity5.11917781829834
INFO:root:current mean train loss 2069.1222567943814
INFO:root:current train perplexity5.120333194732666
INFO:root:current mean train loss 2069.8801395392566
INFO:root:current train perplexity5.122206211090088
INFO:root:current mean train loss 2068.909568154351
INFO:root:current train perplexity5.119791507720947
INFO:root:current mean train loss 2068.381921083061
INFO:root:current train perplexity5.121257781982422
INFO:root:current mean train loss 2068.655519235188
INFO:root:current train perplexity5.123396873474121
INFO:root:current mean train loss 2069.27902983986
INFO:root:current train perplexity5.123405456542969
INFO:root:current mean train loss 2070.7520764113387
INFO:root:current train perplexity5.122734069824219
INFO:root:current mean train loss 2070.301587087828
INFO:root:current train perplexity5.124685287475586
INFO:root:current mean train loss 2070.4806379451306
INFO:root:current train perplexity5.125102996826172
INFO:root:current mean train loss 2071.05830078125
INFO:root:current train perplexity5.126478672027588
INFO:root:current mean train loss 2070.170103512761
INFO:root:current train perplexity5.127598762512207
INFO:root:current mean train loss 2071.0438455597514
INFO:root:current train perplexity5.129134654998779
INFO:root:current mean train loss 2072.7663729289698
INFO:root:current train perplexity5.131286144256592


100%|██████████| 1/1 [07:44<00:00, 464.46s/it][A
100%|██████████| 1/1 [07:44<00:00, 464.46s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.66s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.66s/it]
INFO:root:eval mean loss: 3083.1215417077233
INFO:root:eval perplexity: 12.720144271850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/19

 10%|▉         | 19/200 [2:49:18<25:54:36, 515.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2030.5900268554688
INFO:root:current train perplexity5.033263206481934
INFO:root:current mean train loss 2033.2414110527664
INFO:root:current train perplexity4.987842559814453
INFO:root:current mean train loss 2042.2534680065808
INFO:root:current train perplexity5.00471830368042
INFO:root:current mean train loss 2041.1827513890237
INFO:root:current train perplexity4.993135452270508
INFO:root:current mean train loss 2043.5540488003555
INFO:root:current train perplexity5.00209903717041
INFO:root:current mean train loss 2042.9052117007902
INFO:root:current train perplexity5.0054402351379395
INFO:root:current mean train loss 2043.3524715509445
INFO:root:current train perplexity5.00767707824707
INFO:root:current mean train loss 2044.903777315346
INFO:root:current train perplexity5.019617557525635
INFO:root:current mean train loss 2046.2856039896499
INFO:root:current train perplexity5.022655963897705
INFO:root:current mean train loss 2047.5984959136897
INFO:root:current train perplexity5.026849746704102
INFO:root:current mean train loss 2048.076907999595
INFO:root:current train perplexity5.0289764404296875
INFO:root:current mean train loss 2047.6920988521474
INFO:root:current train perplexity5.029441833496094
INFO:root:current mean train loss 2046.732547441598
INFO:root:current train perplexity5.030874252319336
INFO:root:current mean train loss 2046.2580053009171
INFO:root:current train perplexity5.0258355140686035
INFO:root:current mean train loss 2047.9793524333027
INFO:root:current train perplexity5.028127670288086
INFO:root:current mean train loss 2048.636853091507
INFO:root:current train perplexity5.030364990234375
INFO:root:current mean train loss 2048.5317599558803
INFO:root:current train perplexity5.030229091644287
INFO:root:current mean train loss 2049.9200016247687
INFO:root:current train perplexity5.034007549285889
INFO:root:current mean train loss 2049.819780402074
INFO:root:current train perplexity5.037248134613037
INFO:root:current mean train loss 2050.3564044742006
INFO:root:current train perplexity5.039834499359131


100%|██████████| 1/1 [07:49<00:00, 469.48s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.48s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.30s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.30s/it]
INFO:root:eval mean loss: 3066.4531433288757
INFO:root:eval perplexity: 12.54644775390625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/20
########################best###########
 10%|█         | 20/200 [2:57:54<25:46:32, 515.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1997.0963291266025
INFO:root:current train perplexity4.8863701820373535
INFO:root:current mean train loss 1986.808835256014
INFO:root:current train perplexity4.842784881591797
INFO:root:current mean train loss 2001.2588033636246
INFO:root:current train perplexity4.8758111000061035
INFO:root:current mean train loss 2012.333210542842
INFO:root:current train perplexity4.910567760467529
INFO:root:current mean train loss 2022.3281338980637
INFO:root:current train perplexity4.924628257751465
INFO:root:current mean train loss 2020.370123981766
INFO:root:current train perplexity4.922726631164551
INFO:root:current mean train loss 2023.0914967616027
INFO:root:current train perplexity4.928008556365967
INFO:root:current mean train loss 2025.0360622793153
INFO:root:current train perplexity4.933961868286133
INFO:root:current mean train loss 2025.569433419156
INFO:root:current train perplexity4.936086177825928
INFO:root:current mean train loss 2025.3259481444272
INFO:root:current train perplexity4.9397454261779785
INFO:root:current mean train loss 2023.7900650274078
INFO:root:current train perplexity4.937885761260986
INFO:root:current mean train loss 2024.1882359585918
INFO:root:current train perplexity4.9409003257751465
INFO:root:current mean train loss 2025.9313037739923
INFO:root:current train perplexity4.947425365447998
INFO:root:current mean train loss 2025.2128428543922
INFO:root:current train perplexity4.949938774108887
INFO:root:current mean train loss 2025.1924246336703
INFO:root:current train perplexity4.951172351837158
INFO:root:current mean train loss 2026.1662476299798
INFO:root:current train perplexity4.951043605804443
INFO:root:current mean train loss 2026.2769478221285
INFO:root:current train perplexity4.9490180015563965
INFO:root:current mean train loss 2027.0712322039876
INFO:root:current train perplexity4.951641082763672
INFO:root:current mean train loss 2027.4631479749737
INFO:root:current train perplexity4.953200817108154
INFO:root:current mean train loss 2028.8743640885887
INFO:root:current train perplexity4.9556779861450195


100%|██████████| 1/1 [07:48<00:00, 468.49s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.49s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.68s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.68s/it]
INFO:root:eval mean loss: 3066.9849036047767
INFO:root:eval perplexity: 12.551952362060547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/21

 10%|█         | 21/200 [3:06:30<25:38:37, 515.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1991.8653629847936
INFO:root:current train perplexity4.850587844848633
INFO:root:current mean train loss 2005.9112368852664
INFO:root:current train perplexity4.8584442138671875
INFO:root:current mean train loss 1996.7552342414856
INFO:root:current train perplexity4.846883296966553
INFO:root:current mean train loss 2002.165459450711
INFO:root:current train perplexity4.85827112197876
INFO:root:current mean train loss 2001.9323711729887
INFO:root:current train perplexity4.855419158935547
INFO:root:current mean train loss 2004.9430322441265
INFO:root:current train perplexity4.867757797241211
INFO:root:current mean train loss 2001.831724027308
INFO:root:current train perplexity4.863826751708984
INFO:root:current mean train loss 2003.1083436996219
INFO:root:current train perplexity4.867528438568115
INFO:root:current mean train loss 2004.4944008800471
INFO:root:current train perplexity4.8715715408325195
INFO:root:current mean train loss 2003.4470512358214
INFO:root:current train perplexity4.869484901428223
INFO:root:current mean train loss 2003.5211270650227
INFO:root:current train perplexity4.871264934539795
INFO:root:current mean train loss 2003.9053156763625
INFO:root:current train perplexity4.867093563079834
INFO:root:current mean train loss 2004.7269682671613
INFO:root:current train perplexity4.866495609283447
INFO:root:current mean train loss 2006.529610152793
INFO:root:current train perplexity4.868426322937012
INFO:root:current mean train loss 2006.8947259253198
INFO:root:current train perplexity4.866002559661865
INFO:root:current mean train loss 2006.1871763096983
INFO:root:current train perplexity4.865489959716797
INFO:root:current mean train loss 2005.5776209439632
INFO:root:current train perplexity4.865720748901367
INFO:root:current mean train loss 2007.2851097437135
INFO:root:current train perplexity4.8704657554626465
INFO:root:current mean train loss 2006.895481504243
INFO:root:current train perplexity4.870834827423096
INFO:root:current mean train loss 2006.9573804859247
INFO:root:current train perplexity4.8727617263793945


100%|██████████| 1/1 [07:48<00:00, 468.32s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.32s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.88s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 3075.9185384114585
INFO:root:eval perplexity: 12.644789695739746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/22

 11%|█         | 22/200 [3:15:04<25:28:48, 515.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1990.2266327322345
INFO:root:current train perplexity4.7428131103515625
INFO:root:current mean train loss 1988.907155296017
INFO:root:current train perplexity4.7626519203186035
INFO:root:current mean train loss 1986.759032755981
INFO:root:current train perplexity4.7581562995910645
INFO:root:current mean train loss 1980.5367457821926
INFO:root:current train perplexity4.7526140213012695
INFO:root:current mean train loss 1983.6197757519324
INFO:root:current train perplexity4.762350082397461
INFO:root:current mean train loss 1978.4042474503708
INFO:root:current train perplexity4.76101016998291
INFO:root:current mean train loss 1977.2028621769944
INFO:root:current train perplexity4.757965087890625
INFO:root:current mean train loss 1977.5820866790812
INFO:root:current train perplexity4.761742115020752
INFO:root:current mean train loss 1975.6236390488527
INFO:root:current train perplexity4.7642693519592285
INFO:root:current mean train loss 1978.5499932503774
INFO:root:current train perplexity4.7685112953186035
INFO:root:current mean train loss 1978.5402324637407
INFO:root:current train perplexity4.769992828369141
INFO:root:current mean train loss 1980.239266137841
INFO:root:current train perplexity4.775759220123291
INFO:root:current mean train loss 1983.1864177646921
INFO:root:current train perplexity4.781434059143066
INFO:root:current mean train loss 1983.8751867062317
INFO:root:current train perplexity4.781909465789795
INFO:root:current mean train loss 1984.8760446832039
INFO:root:current train perplexity4.7851409912109375
INFO:root:current mean train loss 1986.001837728589
INFO:root:current train perplexity4.787905216217041
INFO:root:current mean train loss 1986.548218284192
INFO:root:current train perplexity4.790655136108398
INFO:root:current mean train loss 1986.2753317586057
INFO:root:current train perplexity4.7910237312316895
INFO:root:current mean train loss 1987.25413279373
INFO:root:current train perplexity4.794402122497559
INFO:root:current mean train loss 1986.546507675395
INFO:root:current train perplexity4.794362545013428


100%|██████████| 1/1 [07:49<00:00, 469.49s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.49s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.10s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.10s/it]
INFO:root:eval mean loss: 3072.0871713999154
INFO:root:eval perplexity: 12.604889869689941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/23

 12%|█▏        | 23/200 [3:23:41<25:21:26, 515.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1939.5265448676216
INFO:root:current train perplexity4.6503143310546875
INFO:root:current mean train loss 1948.5340968081825
INFO:root:current train perplexity4.664129257202148
INFO:root:current mean train loss 1950.193378316945
INFO:root:current train perplexity4.661194324493408
INFO:root:current mean train loss 1951.0483110476762
INFO:root:current train perplexity4.671677112579346
INFO:root:current mean train loss 1955.1099165935905
INFO:root:current train perplexity4.687039375305176
INFO:root:current mean train loss 1952.261985235699
INFO:root:current train perplexity4.688388824462891
INFO:root:current mean train loss 1955.072685440727
INFO:root:current train perplexity4.69230842590332
INFO:root:current mean train loss 1956.9889908030063
INFO:root:current train perplexity4.7001752853393555
INFO:root:current mean train loss 1958.8037862370522
INFO:root:current train perplexity4.704111576080322
INFO:root:current mean train loss 1958.0041873816288
INFO:root:current train perplexity4.702400207519531
INFO:root:current mean train loss 1960.9767890580204
INFO:root:current train perplexity4.71126127243042
INFO:root:current mean train loss 1961.4662497127758
INFO:root:current train perplexity4.714197635650635
INFO:root:current mean train loss 1965.426664698401
INFO:root:current train perplexity4.722567558288574
INFO:root:current mean train loss 1964.6909478276755
INFO:root:current train perplexity4.717522144317627
INFO:root:current mean train loss 1965.8218407547713
INFO:root:current train perplexity4.721630096435547
INFO:root:current mean train loss 1964.7444898689319
INFO:root:current train perplexity4.718332290649414
INFO:root:current mean train loss 1964.7935234837278
INFO:root:current train perplexity4.717460632324219
INFO:root:current mean train loss 1966.4493698716828
INFO:root:current train perplexity4.7192063331604
INFO:root:current mean train loss 1967.2857478066096
INFO:root:current train perplexity4.721165657043457


100%|██████████| 1/1 [07:54<00:00, 474.22s/it][A
100%|██████████| 1/1 [07:54<00:00, 474.22s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.57s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.57s/it]
INFO:root:eval mean loss: 3076.023754222973
INFO:root:eval perplexity: 12.64588737487793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/24

 12%|█▏        | 24/200 [3:32:22<25:17:27, 517.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1896.7150006975446
INFO:root:current train perplexity4.564975738525391
INFO:root:current mean train loss 1933.1462573470355
INFO:root:current train perplexity4.580173969268799
INFO:root:current mean train loss 1929.430355053593
INFO:root:current train perplexity4.599494934082031
INFO:root:current mean train loss 1933.5706958087337
INFO:root:current train perplexity4.601057529449463
INFO:root:current mean train loss 1931.7920455745164
INFO:root:current train perplexity4.601321220397949
INFO:root:current mean train loss 1932.8689667891704
INFO:root:current train perplexity4.608511924743652
INFO:root:current mean train loss 1933.8970975420227
INFO:root:current train perplexity4.61025333404541
INFO:root:current mean train loss 1936.51716529598
INFO:root:current train perplexity4.614871978759766
INFO:root:current mean train loss 1936.786719869356
INFO:root:current train perplexity4.616827964782715
INFO:root:current mean train loss 1936.7993830267624
INFO:root:current train perplexity4.616515159606934
INFO:root:current mean train loss 1938.2921477150182
INFO:root:current train perplexity4.62573766708374
INFO:root:current mean train loss 1938.566387283339
INFO:root:current train perplexity4.626606464385986
INFO:root:current mean train loss 1941.2602639186453
INFO:root:current train perplexity4.632106304168701
INFO:root:current mean train loss 1942.3719453468702
INFO:root:current train perplexity4.638184070587158
INFO:root:current mean train loss 1943.9667570524887
INFO:root:current train perplexity4.642604827880859
INFO:root:current mean train loss 1945.2072494699216
INFO:root:current train perplexity4.648401737213135
INFO:root:current mean train loss 1946.542363411904
INFO:root:current train perplexity4.650688648223877
INFO:root:current mean train loss 1947.4787445336528
INFO:root:current train perplexity4.651220798492432
INFO:root:current mean train loss 1947.8322907929667
INFO:root:current train perplexity4.651597499847412
INFO:root:current mean train loss 1947.1591030654947
INFO:root:current train perplexity4.650351524353027


100%|██████████| 1/1 [07:51<00:00, 471.30s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.30s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:46<00:00, 46.09s/it][A
100%|██████████| 1/1 [00:46<00:00, 46.09s/it]
INFO:root:eval mean loss: 3077.8955935916383
INFO:root:eval perplexity: 12.665431022644043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/25

 12%|█▎        | 25/200 [3:41:02<25:10:44, 517.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1915.807652791341
INFO:root:current train perplexity4.520885944366455
INFO:root:current mean train loss 1920.9888925860005
INFO:root:current train perplexity4.516147613525391
INFO:root:current mean train loss 1936.439824785505
INFO:root:current train perplexity4.550815582275391
INFO:root:current mean train loss 1926.1471116807725
INFO:root:current train perplexity4.555444717407227
INFO:root:current mean train loss 1929.2282303144348
INFO:root:current train perplexity4.560227394104004
INFO:root:current mean train loss 1925.2011206241054
INFO:root:current train perplexity4.564738750457764
INFO:root:current mean train loss 1928.8475089439978
INFO:root:current train perplexity4.571160793304443
INFO:root:current mean train loss 1928.5394964903098
INFO:root:current train perplexity4.57617712020874
INFO:root:current mean train loss 1925.7920647778558
INFO:root:current train perplexity4.569304943084717
INFO:root:current mean train loss 1927.1021296513545
INFO:root:current train perplexity4.572232723236084
INFO:root:current mean train loss 1926.11985039711
INFO:root:current train perplexity4.573508262634277
INFO:root:current mean train loss 1925.998373011253
INFO:root:current train perplexity4.574451446533203
INFO:root:current mean train loss 1926.4531911214192
INFO:root:current train perplexity4.577755451202393
INFO:root:current mean train loss 1926.7606640034933
INFO:root:current train perplexity4.576676845550537
INFO:root:current mean train loss 1926.9692630553513
INFO:root:current train perplexity4.578711032867432
INFO:root:current mean train loss 1926.1578117630927
INFO:root:current train perplexity4.575959205627441
INFO:root:current mean train loss 1926.5233810499972
INFO:root:current train perplexity4.577229976654053
INFO:root:current mean train loss 1927.7451369424984
INFO:root:current train perplexity4.578646183013916
INFO:root:current mean train loss 1927.4747748793218
INFO:root:current train perplexity4.579146862030029
INFO:root:current mean train loss 1930.070553658658
INFO:root:current train perplexity4.583112716674805


100%|██████████| 1/1 [07:53<00:00, 473.35s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.35s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.78s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.78s/it]
INFO:root:eval mean loss: 3073.2304511542793
INFO:root:eval perplexity: 12.61678409576416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/26

 13%|█▎        | 26/200 [3:50:14<25:32:19, 528.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1900.538077005526
INFO:root:current train perplexity4.448085308074951
INFO:root:current mean train loss 1899.7320452750998
INFO:root:current train perplexity4.4851484298706055
INFO:root:current mean train loss 1898.4596813407677
INFO:root:current train perplexity4.47625732421875
INFO:root:current mean train loss 1908.101540663375
INFO:root:current train perplexity4.492522716522217
INFO:root:current mean train loss 1907.4471183655753
INFO:root:current train perplexity4.49772834777832
INFO:root:current mean train loss 1907.682359508578
INFO:root:current train perplexity4.499065399169922
INFO:root:current mean train loss 1908.392432059587
INFO:root:current train perplexity4.496773719787598
INFO:root:current mean train loss 1908.366052295515
INFO:root:current train perplexity4.4962968826293945
INFO:root:current mean train loss 1907.568633851767
INFO:root:current train perplexity4.504649639129639
INFO:root:current mean train loss 1907.3792841361003
INFO:root:current train perplexity4.505389213562012
INFO:root:current mean train loss 1905.2741298180836
INFO:root:current train perplexity4.5022172927856445
INFO:root:current mean train loss 1907.428706337129
INFO:root:current train perplexity4.506836891174316
INFO:root:current mean train loss 1908.7413000557137
INFO:root:current train perplexity4.508167266845703
INFO:root:current mean train loss 1907.8717803841291
INFO:root:current train perplexity4.505405902862549
INFO:root:current mean train loss 1909.1522157498318
INFO:root:current train perplexity4.509023666381836
INFO:root:current mean train loss 1908.5723116489141
INFO:root:current train perplexity4.509854316711426
INFO:root:current mean train loss 1908.4143993277728
INFO:root:current train perplexity4.509624481201172
INFO:root:current mean train loss 1909.3581779256488
INFO:root:current train perplexity4.5112624168396
INFO:root:current mean train loss 1910.1810574591127
INFO:root:current train perplexity4.513424873352051
INFO:root:current mean train loss 1910.267785600508
INFO:root:current train perplexity4.514394760131836


100%|██████████| 1/1 [07:47<00:00, 467.68s/it][A
100%|██████████| 1/1 [07:47<00:00, 467.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.53s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.53s/it]
INFO:root:eval mean loss: 3085.505613768065
INFO:root:eval perplexity: 12.745182991027832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/27

 14%|█▎        | 27/200 [3:59:20<25:38:16, 533.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1857.8858516298492
INFO:root:current train perplexity4.383152484893799
INFO:root:current mean train loss 1864.7536636545688
INFO:root:current train perplexity4.368354797363281
INFO:root:current mean train loss 1876.403046364008
INFO:root:current train perplexity4.396062850952148
INFO:root:current mean train loss 1877.706441357149
INFO:root:current train perplexity4.4034953117370605
INFO:root:current mean train loss 1884.5796130317788
INFO:root:current train perplexity4.421398639678955
INFO:root:current mean train loss 1882.4832842426915
INFO:root:current train perplexity4.416229248046875
INFO:root:current mean train loss 1882.7129596373954
INFO:root:current train perplexity4.418974876403809
INFO:root:current mean train loss 1885.334726781518
INFO:root:current train perplexity4.423035144805908
INFO:root:current mean train loss 1882.6580628437318
INFO:root:current train perplexity4.42292594909668
INFO:root:current mean train loss 1882.2916257217184
INFO:root:current train perplexity4.422327041625977
INFO:root:current mean train loss 1881.745450861738
INFO:root:current train perplexity4.42296028137207
INFO:root:current mean train loss 1883.4345118072983
INFO:root:current train perplexity4.426982402801514
INFO:root:current mean train loss 1883.8651870218105
INFO:root:current train perplexity4.42805814743042
INFO:root:current mean train loss 1885.2331729939478
INFO:root:current train perplexity4.433682918548584
INFO:root:current mean train loss 1887.1348036359204
INFO:root:current train perplexity4.439426898956299
INFO:root:current mean train loss 1888.6311976147556
INFO:root:current train perplexity4.4441375732421875
INFO:root:current mean train loss 1889.1855442981237
INFO:root:current train perplexity4.445526123046875
INFO:root:current mean train loss 1890.30665867863
INFO:root:current train perplexity4.4469780921936035
INFO:root:current mean train loss 1890.9654213830395
INFO:root:current train perplexity4.448318004608154
INFO:root:current mean train loss 1892.605928789242
INFO:root:current train perplexity4.452279567718506


100%|██████████| 1/1 [07:56<00:00, 476.54s/it][A
100%|██████████| 1/1 [07:56<00:00, 476.54s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.24s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.25s/it]
INFO:root:eval mean loss: 3086.8659191417983
INFO:root:eval perplexity: 12.759490966796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/28

 14%|█▍        | 28/200 [4:08:46<25:57:50, 543.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1849.8434733072916
INFO:root:current train perplexity4.301677703857422
INFO:root:current mean train loss 1858.6331982421875
INFO:root:current train perplexity4.344325542449951
INFO:root:current mean train loss 1860.5897496448863
INFO:root:current train perplexity4.354357719421387
INFO:root:current mean train loss 1859.5070224609376
INFO:root:current train perplexity4.350124835968018
INFO:root:current mean train loss 1861.1681185752468
INFO:root:current train perplexity4.356796741485596
INFO:root:current mean train loss 1863.3016945482336
INFO:root:current train perplexity4.358658313751221
INFO:root:current mean train loss 1866.3417126012732
INFO:root:current train perplexity4.367826461791992
INFO:root:current mean train loss 1864.6609543535785
INFO:root:current train perplexity4.366337299346924
INFO:root:current mean train loss 1865.6618461216517
INFO:root:current train perplexity4.366796970367432
INFO:root:current mean train loss 1867.7299022185496
INFO:root:current train perplexity4.372133731842041
INFO:root:current mean train loss 1869.1867000136265
INFO:root:current train perplexity4.375141620635986
INFO:root:current mean train loss 1871.5788598113365
INFO:root:current train perplexity4.379195213317871
INFO:root:current mean train loss 1870.8053776041668
INFO:root:current train perplexity4.3792619705200195
INFO:root:current mean train loss 1871.4395705788352
INFO:root:current train perplexity4.379986763000488
INFO:root:current mean train loss 1871.5837167306674
INFO:root:current train perplexity4.381557464599609
INFO:root:current mean train loss 1872.4602783203125
INFO:root:current train perplexity4.381320476531982
INFO:root:current mean train loss 1872.6620305940999
INFO:root:current train perplexity4.382984161376953
INFO:root:current mean train loss 1873.2344270604094
INFO:root:current train perplexity4.384861946105957
INFO:root:current mean train loss 1873.1412514973958
INFO:root:current train perplexity4.385616302490234
INFO:root:current mean train loss 1875.1127667004548
INFO:root:current train perplexity4.3906474113464355


100%|██████████| 1/1 [07:53<00:00, 473.28s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.28s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.25s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.25s/it]
INFO:root:eval mean loss: 3090.846703148461
INFO:root:eval perplexity: 12.801458358764648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/29

 14%|█▍        | 29/200 [4:17:27<25:29:18, 536.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1830.1856092370074
INFO:root:current train perplexity4.231463432312012
INFO:root:current mean train loss 1831.8963896433513
INFO:root:current train perplexity4.2627153396606445
INFO:root:current mean train loss 1834.2978151922357
INFO:root:current train perplexity4.2617034912109375
INFO:root:current mean train loss 1838.318599467375
INFO:root:current train perplexity4.267547130584717
INFO:root:current mean train loss 1838.708026917
INFO:root:current train perplexity4.283177852630615
INFO:root:current mean train loss 1840.998218433277
INFO:root:current train perplexity4.291983127593994
INFO:root:current mean train loss 1846.2380219387871
INFO:root:current train perplexity4.2951812744140625
INFO:root:current mean train loss 1847.8508390176175
INFO:root:current train perplexity4.300673007965088
INFO:root:current mean train loss 1849.4314805103525
INFO:root:current train perplexity4.30717658996582
INFO:root:current mean train loss 1849.2413235325967
INFO:root:current train perplexity4.3065290451049805
INFO:root:current mean train loss 1850.548344203404
INFO:root:current train perplexity4.311762809753418
INFO:root:current mean train loss 1852.003650946905
INFO:root:current train perplexity4.314962387084961
INFO:root:current mean train loss 1853.331053270275
INFO:root:current train perplexity4.317859172821045
INFO:root:current mean train loss 1854.428959199752
INFO:root:current train perplexity4.318716526031494
INFO:root:current mean train loss 1854.2074683958979
INFO:root:current train perplexity4.320535659790039
INFO:root:current mean train loss 1854.5426692483413
INFO:root:current train perplexity4.323884010314941
INFO:root:current mean train loss 1855.762831523345
INFO:root:current train perplexity4.32513952255249
INFO:root:current mean train loss 1856.617604800633
INFO:root:current train perplexity4.3264994621276855
INFO:root:current mean train loss 1858.2755347608763
INFO:root:current train perplexity4.33081579208374


100%|██████████| 1/1 [07:50<00:00, 470.57s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.57s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it]
INFO:root:eval mean loss: 3092.6419527437592
INFO:root:eval perplexity: 12.820432662963867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/30

 15%|█▌        | 30/200 [4:26:04<25:04:08, 530.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1867.5166422526042
INFO:root:current train perplexity4.306008338928223
INFO:root:current mean train loss 1829.746094869911
INFO:root:current train perplexity4.232478618621826
INFO:root:current mean train loss 1823.0909674977572
INFO:root:current train perplexity4.210010051727295
INFO:root:current mean train loss 1827.5678588472138
INFO:root:current train perplexity4.220658779144287
INFO:root:current mean train loss 1828.7312369871256
INFO:root:current train perplexity4.237797260284424
INFO:root:current mean train loss 1828.756952731689
INFO:root:current train perplexity4.246416091918945
INFO:root:current mean train loss 1833.2288882501412
INFO:root:current train perplexity4.254858016967773
INFO:root:current mean train loss 1829.3601440946204
INFO:root:current train perplexity4.252668380737305
INFO:root:current mean train loss 1830.5984441995327
INFO:root:current train perplexity4.255105972290039
INFO:root:current mean train loss 1832.9088046133716
INFO:root:current train perplexity4.254999160766602
INFO:root:current mean train loss 1835.0913422266012
INFO:root:current train perplexity4.257423400878906
INFO:root:current mean train loss 1834.8353478420522
INFO:root:current train perplexity4.257198810577393
INFO:root:current mean train loss 1836.362059638163
INFO:root:current train perplexity4.261061668395996
INFO:root:current mean train loss 1836.0840863955668
INFO:root:current train perplexity4.260173320770264
INFO:root:current mean train loss 1836.208083532481
INFO:root:current train perplexity4.260472297668457
INFO:root:current mean train loss 1836.4685246269778
INFO:root:current train perplexity4.259533882141113
INFO:root:current mean train loss 1837.7090035693996
INFO:root:current train perplexity4.264346122741699
INFO:root:current mean train loss 1838.4925346396833
INFO:root:current train perplexity4.265676498413086
INFO:root:current mean train loss 1839.4268505967343
INFO:root:current train perplexity4.267828464508057
INFO:root:current mean train loss 1839.5553211274105
INFO:root:current train perplexity4.270371913909912


100%|██████████| 1/1 [07:53<00:00, 473.44s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.44s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.61s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.61s/it]
INFO:root:eval mean loss: 3100.4388761319915
INFO:root:eval perplexity: 12.90315055847168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/31

 16%|█▌        | 31/200 [4:34:46<24:47:06, 527.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1759.3963341346155
INFO:root:current train perplexity4.105375289916992
INFO:root:current mean train loss 1776.8957190135168
INFO:root:current train perplexity4.155821800231934
INFO:root:current mean train loss 1782.1488960738732
INFO:root:current train perplexity4.149629592895508
INFO:root:current mean train loss 1795.4604900336703
INFO:root:current train perplexity4.162696361541748
INFO:root:current mean train loss 1804.976055306448
INFO:root:current train perplexity4.1765241622924805
INFO:root:current mean train loss 1809.5485842164478
INFO:root:current train perplexity4.17989444732666
INFO:root:current mean train loss 1814.8440768208366
INFO:root:current train perplexity4.182835578918457
INFO:root:current mean train loss 1813.523382686058
INFO:root:current train perplexity4.181855201721191
INFO:root:current mean train loss 1815.7130809719279
INFO:root:current train perplexity4.187973499298096
INFO:root:current mean train loss 1816.0348352551719
INFO:root:current train perplexity4.191739559173584
INFO:root:current mean train loss 1815.355025679977
INFO:root:current train perplexity4.1911234855651855
INFO:root:current mean train loss 1816.034809769788
INFO:root:current train perplexity4.192963600158691
INFO:root:current mean train loss 1817.2858879748994
INFO:root:current train perplexity4.194892883300781
INFO:root:current mean train loss 1818.917001669583
INFO:root:current train perplexity4.198484897613525
INFO:root:current mean train loss 1820.2167183767697
INFO:root:current train perplexity4.2003173828125
INFO:root:current mean train loss 1819.9168940352893
INFO:root:current train perplexity4.200165748596191
INFO:root:current mean train loss 1820.3816486278877
INFO:root:current train perplexity4.202635288238525
INFO:root:current mean train loss 1822.2135985055086
INFO:root:current train perplexity4.207230567932129
INFO:root:current mean train loss 1823.003347507573
INFO:root:current train perplexity4.210904598236084
INFO:root:current mean train loss 1823.599867332514
INFO:root:current train perplexity4.214710235595703


100%|██████████| 1/1 [07:51<00:00, 471.64s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.06s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.06s/it]
INFO:root:eval mean loss: 3102.2623236029
INFO:root:eval perplexity: 12.922574043273926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/32

 16%|█▌        | 32/200 [4:43:22<24:28:54, 524.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1798.622005019077
INFO:root:current train perplexity4.128117084503174
INFO:root:current mean train loss 1794.347569178868
INFO:root:current train perplexity4.113530158996582
INFO:root:current mean train loss 1793.4354664994855
INFO:root:current train perplexity4.1311540603637695
INFO:root:current mean train loss 1797.0846014884748
INFO:root:current train perplexity4.142739295959473
INFO:root:current mean train loss 1799.926956211202
INFO:root:current train perplexity4.14297342300415
INFO:root:current mean train loss 1799.5288771599476
INFO:root:current train perplexity4.140236854553223
INFO:root:current mean train loss 1800.4382958300932
INFO:root:current train perplexity4.144318580627441
INFO:root:current mean train loss 1796.1530917797884
INFO:root:current train perplexity4.1407318115234375
INFO:root:current mean train loss 1794.5460926205237
INFO:root:current train perplexity4.140381813049316
INFO:root:current mean train loss 1795.9175300476786
INFO:root:current train perplexity4.143960952758789
INFO:root:current mean train loss 1796.8917533603862
INFO:root:current train perplexity4.144644260406494
INFO:root:current mean train loss 1799.0177642742167
INFO:root:current train perplexity4.146010398864746
INFO:root:current mean train loss 1799.948071897941
INFO:root:current train perplexity4.1477251052856445
INFO:root:current mean train loss 1802.5803368086258
INFO:root:current train perplexity4.152790069580078
INFO:root:current mean train loss 1802.6954467519654
INFO:root:current train perplexity4.152039051055908
INFO:root:current mean train loss 1804.2584943691013
INFO:root:current train perplexity4.153640270233154
INFO:root:current mean train loss 1804.811325063955
INFO:root:current train perplexity4.155913352966309
INFO:root:current mean train loss 1804.0640761987684
INFO:root:current train perplexity4.15490198135376
INFO:root:current mean train loss 1805.8826061130799
INFO:root:current train perplexity4.157551288604736
INFO:root:current mean train loss 1807.3944279645684
INFO:root:current train perplexity4.161234378814697


100%|██████████| 1/1 [07:46<00:00, 466.46s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.46s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.33s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.33s/it]
INFO:root:eval mean loss: 3111.1764557526276
INFO:root:eval perplexity: 13.0179443359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/33

 16%|█▋        | 33/200 [4:52:10<24:22:30, 525.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1760.5450154622397
INFO:root:current train perplexity4.066216468811035
INFO:root:current mean train loss 1771.89490814209
INFO:root:current train perplexity4.056609153747559
INFO:root:current mean train loss 1772.6865027794472
INFO:root:current train perplexity4.053755283355713
INFO:root:current mean train loss 1776.8807749430339
INFO:root:current train perplexity4.062923908233643
INFO:root:current mean train loss 1776.1916962996772
INFO:root:current train perplexity4.0635175704956055
INFO:root:current mean train loss 1776.8499328613282
INFO:root:current train perplexity4.063655853271484
INFO:root:current mean train loss 1777.3251409357244
INFO:root:current train perplexity4.068355083465576
INFO:root:current mean train loss 1778.2625584652549
INFO:root:current train perplexity4.071928977966309
INFO:root:current mean train loss 1780.6948195346565
INFO:root:current train perplexity4.07866907119751
INFO:root:current mean train loss 1782.155412419637
INFO:root:current train perplexity4.081068515777588
INFO:root:current mean train loss 1782.4347020563089
INFO:root:current train perplexity4.086300373077393
INFO:root:current mean train loss 1783.0558763175175
INFO:root:current train perplexity4.086944580078125
INFO:root:current mean train loss 1783.5328841920882
INFO:root:current train perplexity4.089331150054932
INFO:root:current mean train loss 1786.1531775979436
INFO:root:current train perplexity4.0948805809021
INFO:root:current mean train loss 1786.6326043952001
INFO:root:current train perplexity4.09544563293457
INFO:root:current mean train loss 1786.915755834335
INFO:root:current train perplexity4.097294330596924
INFO:root:current mean train loss 1788.630547801558
INFO:root:current train perplexity4.100785255432129
INFO:root:current mean train loss 1788.9677040100098
INFO:root:current train perplexity4.101815223693848
INFO:root:current mean train loss 1789.8437668667045
INFO:root:current train perplexity4.103912830352783
INFO:root:current mean train loss 1790.1787577726402
INFO:root:current train perplexity4.106355667114258


100%|██████████| 1/1 [07:46<00:00, 466.22s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.22s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.70s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.70s/it]
INFO:root:eval mean loss: 3118.432078318553
INFO:root:eval perplexity: 13.096087455749512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/34

 17%|█▋        | 34/200 [5:01:13<24:28:17, 530.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1737.1956470043629
INFO:root:current train perplexity4.0031280517578125
INFO:root:current mean train loss 1744.5098345912781
INFO:root:current train perplexity3.9993133544921875
INFO:root:current mean train loss 1756.1482616658675
INFO:root:current train perplexity4.017055988311768
INFO:root:current mean train loss 1763.2665529934102
INFO:root:current train perplexity4.023595809936523
INFO:root:current mean train loss 1766.0072418148914
INFO:root:current train perplexity4.02703857421875
INFO:root:current mean train loss 1769.5466319171767
INFO:root:current train perplexity4.03336763381958
INFO:root:current mean train loss 1768.4199173672337
INFO:root:current train perplexity4.035048484802246
INFO:root:current mean train loss 1768.4201078869048
INFO:root:current train perplexity4.0358381271362305
INFO:root:current mean train loss 1769.2976554426934
INFO:root:current train perplexity4.038049697875977
INFO:root:current mean train loss 1770.7243472424354
INFO:root:current train perplexity4.042064666748047
INFO:root:current mean train loss 1770.992188860115
INFO:root:current train perplexity4.042397975921631
INFO:root:current mean train loss 1771.0941243005589
INFO:root:current train perplexity4.042169570922852
INFO:root:current mean train loss 1772.204566232656
INFO:root:current train perplexity4.046360969543457
INFO:root:current mean train loss 1772.7334878848096
INFO:root:current train perplexity4.046376705169678
INFO:root:current mean train loss 1772.8120232893957
INFO:root:current train perplexity4.046709060668945
INFO:root:current mean train loss 1773.4757162903259
INFO:root:current train perplexity4.048171043395996
INFO:root:current mean train loss 1773.4698021922286
INFO:root:current train perplexity4.049896240234375
INFO:root:current mean train loss 1774.5536998776686
INFO:root:current train perplexity4.052192687988281
INFO:root:current mean train loss 1775.0203693534188
INFO:root:current train perplexity4.053597450256348
INFO:root:current mean train loss 1774.369941522331
INFO:root:current train perplexity4.054383754730225


100%|██████████| 1/1 [07:45<00:00, 465.52s/it][A
100%|██████████| 1/1 [07:45<00:00, 465.52s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.61s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.61s/it]
INFO:root:eval mean loss: 3135.663629205377
INFO:root:eval perplexity: 13.283562660217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/35

 18%|█▊        | 35/200 [5:10:36<24:46:03, 540.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1749.603974037982
INFO:root:current train perplexity3.9701645374298096
INFO:root:current mean train loss 1742.3234523497906
INFO:root:current train perplexity3.958420515060425
INFO:root:current mean train loss 1749.2291284677933
INFO:root:current train perplexity3.969433307647705
INFO:root:current mean train loss 1741.5245451176831
INFO:root:current train perplexity3.966790199279785
INFO:root:current mean train loss 1742.2842894025177
INFO:root:current train perplexity3.9640262126922607
INFO:root:current mean train loss 1747.20918926406
INFO:root:current train perplexity3.974402666091919
INFO:root:current mean train loss 1748.4109474204115
INFO:root:current train perplexity3.9759457111358643
INFO:root:current mean train loss 1749.1919194372836
INFO:root:current train perplexity3.9749021530151367
INFO:root:current mean train loss 1752.147051169035
INFO:root:current train perplexity3.982339859008789
INFO:root:current mean train loss 1750.9202411736042
INFO:root:current train perplexity3.982492685317993
INFO:root:current mean train loss 1752.6752153079296
INFO:root:current train perplexity3.987092971801758
INFO:root:current mean train loss 1753.912148633794
INFO:root:current train perplexity3.9903056621551514
INFO:root:current mean train loss 1753.9741741103771
INFO:root:current train perplexity3.9926505088806152
INFO:root:current mean train loss 1753.6572236727436
INFO:root:current train perplexity3.9931836128234863
INFO:root:current mean train loss 1753.9802643189948
INFO:root:current train perplexity3.9948906898498535
INFO:root:current mean train loss 1755.327978730052
INFO:root:current train perplexity3.9960575103759766
INFO:root:current mean train loss 1757.1954002695636
INFO:root:current train perplexity3.99888014793396
INFO:root:current mean train loss 1757.3048302555828
INFO:root:current train perplexity4.000438213348389
INFO:root:current mean train loss 1757.5148498470705
INFO:root:current train perplexity4.001155853271484


100%|██████████| 1/1 [07:43<00:00, 463.86s/it][A
100%|██████████| 1/1 [07:43<00:00, 463.86s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.88s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 3126.9140690983954
INFO:root:eval perplexity: 13.188037872314453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/36

 18%|█▊        | 36/200 [5:19:55<24:52:44, 546.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1749.060946377841
INFO:root:current train perplexity3.8942532539367676
INFO:root:current mean train loss 1734.3595784505208
INFO:root:current train perplexity3.915300130844116
INFO:root:current mean train loss 1729.4804242030139
INFO:root:current train perplexity3.9124462604522705
INFO:root:current mean train loss 1728.9601936953627
INFO:root:current train perplexity3.9118235111236572
INFO:root:current mean train loss 1730.6288088313565
INFO:root:current train perplexity3.9104537963867188
INFO:root:current mean train loss 1729.7372853378026
INFO:root:current train perplexity3.914367198944092
INFO:root:current mean train loss 1731.0156947259231
INFO:root:current train perplexity3.9191181659698486
INFO:root:current mean train loss 1731.5601638386186
INFO:root:current train perplexity3.922711133956909
INFO:root:current mean train loss 1732.8631000260095
INFO:root:current train perplexity3.925220251083374
INFO:root:current mean train loss 1732.8249157969435
INFO:root:current train perplexity3.9270873069763184
INFO:root:current mean train loss 1733.6957557444284
INFO:root:current train perplexity3.9265995025634766
INFO:root:current mean train loss 1734.4570783860613
INFO:root:current train perplexity3.930264949798584
INFO:root:current mean train loss 1735.1173509996258
INFO:root:current train perplexity3.9325106143951416
INFO:root:current mean train loss 1737.5147023495601
INFO:root:current train perplexity3.9360883235931396
INFO:root:current mean train loss 1739.5304407542856
INFO:root:current train perplexity3.9403915405273438
INFO:root:current mean train loss 1739.6914193376178
INFO:root:current train perplexity3.9414169788360596
INFO:root:current mean train loss 1738.9303395267157
INFO:root:current train perplexity3.9420180320739746
INFO:root:current mean train loss 1739.5831892413748
INFO:root:current train perplexity3.9454710483551025
INFO:root:current mean train loss 1740.9450697748784
INFO:root:current train perplexity3.9489197731018066
INFO:root:current mean train loss 1741.7338181779623
INFO:root:current train perplexity3.951612949371338


100%|██████████| 1/1 [07:50<00:00, 470.47s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.47s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it]
INFO:root:eval mean loss: 3140.034454620636
INFO:root:eval perplexity: 13.331541061401367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/37

 18%|█▊        | 37/200 [5:28:33<24:20:39, 537.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1704.6537562779017
INFO:root:current train perplexity3.893298864364624
INFO:root:current mean train loss 1716.4265775680542
INFO:root:current train perplexity3.8729474544525146
INFO:root:current mean train loss 1707.671197723924
INFO:root:current train perplexity3.871185541152954
INFO:root:current mean train loss 1707.275778793707
INFO:root:current train perplexity3.873004674911499
INFO:root:current mean train loss 1709.984486517505
INFO:root:current train perplexity3.87602162361145
INFO:root:current mean train loss 1710.016863042658
INFO:root:current train perplexity3.8761672973632812
INFO:root:current mean train loss 1710.471407944989
INFO:root:current train perplexity3.877753734588623
INFO:root:current mean train loss 1711.009977571257
INFO:root:current train perplexity3.8736913204193115
INFO:root:current mean train loss 1714.6775169188275
INFO:root:current train perplexity3.8789896965026855
INFO:root:current mean train loss 1714.003064911941
INFO:root:current train perplexity3.8760077953338623
INFO:root:current mean train loss 1715.6305887541419
INFO:root:current train perplexity3.878995656967163
INFO:root:current mean train loss 1716.5192124387052
INFO:root:current train perplexity3.882188081741333
INFO:root:current mean train loss 1719.245001081535
INFO:root:current train perplexity3.8866071701049805
INFO:root:current mean train loss 1720.60465350783
INFO:root:current train perplexity3.89058256149292
INFO:root:current mean train loss 1721.6170184992943
INFO:root:current train perplexity3.8935012817382812
INFO:root:current mean train loss 1721.6931801042008
INFO:root:current train perplexity3.8941540718078613
INFO:root:current mean train loss 1722.4370215413612
INFO:root:current train perplexity3.895824670791626
INFO:root:current mean train loss 1723.3015355004204
INFO:root:current train perplexity3.89790940284729
INFO:root:current mean train loss 1723.7605761772172
INFO:root:current train perplexity3.900712490081787
INFO:root:current mean train loss 1725.8344315018396
INFO:root:current train perplexity3.9027254581451416


100%|██████████| 1/1 [07:47<00:00, 467.70s/it][A
100%|██████████| 1/1 [07:47<00:00, 467.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.47s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.47s/it]
INFO:root:eval mean loss: 3149.1296504023553
INFO:root:eval perplexity: 13.431937217712402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/38

 19%|█▉        | 38/200 [5:37:09<23:53:46, 531.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1694.0441867404513
INFO:root:current train perplexity3.8277533054351807
INFO:root:current mean train loss 1689.7689739358837
INFO:root:current train perplexity3.8118984699249268
INFO:root:current mean train loss 1695.6667809311225
INFO:root:current train perplexity3.8168270587921143
INFO:root:current mean train loss 1693.1915576879528
INFO:root:current train perplexity3.809455633163452
INFO:root:current mean train loss 1694.0114419658532
INFO:root:current train perplexity3.814978837966919
INFO:root:current mean train loss 1697.262827014048
INFO:root:current train perplexity3.8186631202697754
INFO:root:current mean train loss 1700.82672355711
INFO:root:current train perplexity3.8271377086639404
INFO:root:current mean train loss 1702.9766139497692
INFO:root:current train perplexity3.8299148082733154
INFO:root:current mean train loss 1705.695140879253
INFO:root:current train perplexity3.837414503097534
INFO:root:current mean train loss 1706.0224070715526
INFO:root:current train perplexity3.8420209884643555
INFO:root:current mean train loss 1707.716070644251
INFO:root:current train perplexity3.8442909717559814
INFO:root:current mean train loss 1707.4880499027702
INFO:root:current train perplexity3.8475747108459473
INFO:root:current mean train loss 1707.397591047785
INFO:root:current train perplexity3.847768545150757
INFO:root:current mean train loss 1708.8250568148815
INFO:root:current train perplexity3.8482375144958496
INFO:root:current mean train loss 1709.2911047489997
INFO:root:current train perplexity3.8539021015167236
INFO:root:current mean train loss 1709.9788878406907
INFO:root:current train perplexity3.8538901805877686
INFO:root:current mean train loss 1710.3739990976444
INFO:root:current train perplexity3.8577640056610107
INFO:root:current mean train loss 1710.7518590364211
INFO:root:current train perplexity3.8573575019836426
INFO:root:current mean train loss 1711.2453154773248
INFO:root:current train perplexity3.8595221042633057
INFO:root:current mean train loss 1711.3854473986785
INFO:root:current train perplexity3.8594415187835693


100%|██████████| 1/1 [07:48<00:00, 468.33s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.33s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.44s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.44s/it]
INFO:root:eval mean loss: 3163.869289455471
INFO:root:eval perplexity: 13.596244812011719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/39

 20%|█▉        | 39/200 [5:45:45<23:33:23, 526.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.060017247354
INFO:root:current train perplexity3.7787766456604004
INFO:root:current mean train loss 1679.0996387622974
INFO:root:current train perplexity3.763101100921631
INFO:root:current mean train loss 1684.4365052667283
INFO:root:current train perplexity3.7750890254974365
INFO:root:current mean train loss 1684.3471440267826
INFO:root:current train perplexity3.7696709632873535
INFO:root:current mean train loss 1685.7497983990293
INFO:root:current train perplexity3.776885986328125
INFO:root:current mean train loss 1684.9279365946813
INFO:root:current train perplexity3.7789969444274902
INFO:root:current mean train loss 1686.6491398652875
INFO:root:current train perplexity3.779116630554199
INFO:root:current mean train loss 1685.7756392511483
INFO:root:current train perplexity3.7817273139953613
INFO:root:current mean train loss 1687.22498011755
INFO:root:current train perplexity3.784214496612549
INFO:root:current mean train loss 1687.0398510082348
INFO:root:current train perplexity3.786806583404541
INFO:root:current mean train loss 1687.2715236857787
INFO:root:current train perplexity3.790039300918579
INFO:root:current mean train loss 1689.0455825464246
INFO:root:current train perplexity3.7933595180511475
INFO:root:current mean train loss 1689.8625642078237
INFO:root:current train perplexity3.794078826904297
INFO:root:current mean train loss 1691.4621736187592
INFO:root:current train perplexity3.798337697982788
INFO:root:current mean train loss 1691.1164797092756
INFO:root:current train perplexity3.799593448638916
INFO:root:current mean train loss 1692.059707622064
INFO:root:current train perplexity3.801957607269287
INFO:root:current mean train loss 1693.2696900442331
INFO:root:current train perplexity3.8038430213928223
INFO:root:current mean train loss 1693.921536570104
INFO:root:current train perplexity3.807629108428955
INFO:root:current mean train loss 1695.1137535349255
INFO:root:current train perplexity3.8110501766204834
INFO:root:current mean train loss 1696.2867944933223
INFO:root:current train perplexity3.8125252723693848


100%|██████████| 1/1 [07:48<00:00, 468.25s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.48s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.48s/it]
INFO:root:eval mean loss: 3168.3812178878097
INFO:root:eval perplexity: 13.646944046020508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/40

 20%|██        | 40/200 [5:54:20<23:15:07, 523.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1645.7996918883505
INFO:root:current train perplexity3.704340934753418
INFO:root:current mean train loss 1662.661824998909
INFO:root:current train perplexity3.7301130294799805
INFO:root:current mean train loss 1661.0709049129143
INFO:root:current train perplexity3.7301807403564453
INFO:root:current mean train loss 1664.2034854134029
INFO:root:current train perplexity3.729640483856201
INFO:root:current mean train loss 1664.2246583050626
INFO:root:current train perplexity3.7304506301879883
INFO:root:current mean train loss 1668.901853571284
INFO:root:current train perplexity3.737231969833374
INFO:root:current mean train loss 1670.9377013530927
INFO:root:current train perplexity3.7450220584869385
INFO:root:current mean train loss 1672.2490303323573
INFO:root:current train perplexity3.746896743774414
INFO:root:current mean train loss 1672.5664855470973
INFO:root:current train perplexity3.7460737228393555
INFO:root:current mean train loss 1673.6630002763104
INFO:root:current train perplexity3.749824047088623
INFO:root:current mean train loss 1673.5689656537809
INFO:root:current train perplexity3.7531652450561523
INFO:root:current mean train loss 1674.470130460964
INFO:root:current train perplexity3.7529947757720947
INFO:root:current mean train loss 1674.6590960803117
INFO:root:current train perplexity3.7522149085998535
INFO:root:current mean train loss 1676.233493420419
INFO:root:current train perplexity3.753455638885498
INFO:root:current mean train loss 1677.1400316507934
INFO:root:current train perplexity3.7578113079071045
INFO:root:current mean train loss 1678.1231252659416
INFO:root:current train perplexity3.7598719596862793
INFO:root:current mean train loss 1678.6875010178585
INFO:root:current train perplexity3.76096773147583
INFO:root:current mean train loss 1680.394704097171
INFO:root:current train perplexity3.764324426651001
INFO:root:current mean train loss 1680.4853537713295
INFO:root:current train perplexity3.7656166553497314
INFO:root:current mean train loss 1681.1178152461193
INFO:root:current train perplexity3.767306327819824


100%|██████████| 1/1 [07:52<00:00, 472.09s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.09s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.38s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.38s/it]
INFO:root:eval mean loss: 3168.5157269085494
INFO:root:eval perplexity: 13.64845085144043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/41

 20%|██        | 41/200 [6:03:00<23:03:35, 522.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1646.7985585530598
INFO:root:current train perplexity3.6754183769226074
INFO:root:current mean train loss 1654.5556422642298
INFO:root:current train perplexity3.6874048709869385
INFO:root:current mean train loss 1650.3162668589
INFO:root:current train perplexity3.6795051097869873
INFO:root:current mean train loss 1647.6983109291155
INFO:root:current train perplexity3.6790730953216553
INFO:root:current mean train loss 1651.4182436543126
INFO:root:current train perplexity3.681152105331421
INFO:root:current mean train loss 1652.3286722682467
INFO:root:current train perplexity3.6869795322418213
INFO:root:current mean train loss 1651.9533817685883
INFO:root:current train perplexity3.6865196228027344
INFO:root:current mean train loss 1652.460596592582
INFO:root:current train perplexity3.6892740726470947
INFO:root:current mean train loss 1653.3256323678154
INFO:root:current train perplexity3.6966021060943604
INFO:root:current mean train loss 1656.0195812547063
INFO:root:current train perplexity3.70068359375
INFO:root:current mean train loss 1657.6695707000956
INFO:root:current train perplexity3.702798366546631
INFO:root:current mean train loss 1659.0670957023085
INFO:root:current train perplexity3.705812454223633
INFO:root:current mean train loss 1659.5845175849067
INFO:root:current train perplexity3.7075388431549072
INFO:root:current mean train loss 1660.3528556933034
INFO:root:current train perplexity3.712286949157715
INFO:root:current mean train loss 1662.2374028496563
INFO:root:current train perplexity3.7176783084869385
INFO:root:current mean train loss 1662.296255623189
INFO:root:current train perplexity3.7184455394744873
INFO:root:current mean train loss 1664.736256869334
INFO:root:current train perplexity3.720838785171509
INFO:root:current mean train loss 1664.9752038900465
INFO:root:current train perplexity3.722679853439331
INFO:root:current mean train loss 1665.1032777939165
INFO:root:current train perplexity3.7238757610321045


100%|██████████| 1/1 [07:48<00:00, 468.67s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.67s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.17s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.17s/it]
INFO:root:eval mean loss: 3175.268529027074
INFO:root:eval perplexity: 13.724690437316895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/42

 21%|██        | 42/200 [6:11:37<22:50:43, 520.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1604.137197641226
INFO:root:current train perplexity3.640846014022827
INFO:root:current mean train loss 1631.3248172186118
INFO:root:current train perplexity3.649709463119507
INFO:root:current mean train loss 1629.1010077391431
INFO:root:current train perplexity3.6375176906585693
INFO:root:current mean train loss 1633.9710474958815
INFO:root:current train perplexity3.635444164276123
INFO:root:current mean train loss 1638.2527020987818
INFO:root:current train perplexity3.64627742767334
INFO:root:current mean train loss 1637.5305130570023
INFO:root:current train perplexity3.6468234062194824
INFO:root:current mean train loss 1637.5996792717042
INFO:root:current train perplexity3.6532833576202393
INFO:root:current mean train loss 1636.932850370902
INFO:root:current train perplexity3.654042959213257
INFO:root:current mean train loss 1637.9583337837773
INFO:root:current train perplexity3.6588385105133057
INFO:root:current mean train loss 1641.5619125115518
INFO:root:current train perplexity3.660397529602051
INFO:root:current mean train loss 1643.9905644348084
INFO:root:current train perplexity3.6663990020751953
INFO:root:current mean train loss 1645.5332873568059
INFO:root:current train perplexity3.6673531532287598
INFO:root:current mean train loss 1646.8388197883926
INFO:root:current train perplexity3.670213460922241
INFO:root:current mean train loss 1647.695759595303
INFO:root:current train perplexity3.6727194786071777
INFO:root:current mean train loss 1647.223519035712
INFO:root:current train perplexity3.6726949214935303
INFO:root:current mean train loss 1647.3388880031912
INFO:root:current train perplexity3.6718509197235107
INFO:root:current mean train loss 1647.9780405875842
INFO:root:current train perplexity3.6734349727630615
INFO:root:current mean train loss 1649.0852677879177
INFO:root:current train perplexity3.6757168769836426
INFO:root:current mean train loss 1649.9332636282447
INFO:root:current train perplexity3.6783432960510254
INFO:root:current mean train loss 1651.283653949233
INFO:root:current train perplexity3.6807596683502197


100%|██████████| 1/1 [07:50<00:00, 470.52s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.52s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.54s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.54s/it]
INFO:root:eval mean loss: 3186.369968357029
INFO:root:eval perplexity: 13.850946426391602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/43

 22%|██▏       | 43/200 [6:20:31<22:52:57, 524.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.934883626302
INFO:root:current train perplexity3.6079554557800293
INFO:root:current mean train loss 1617.7282339242788
INFO:root:current train perplexity3.5897228717803955
INFO:root:current mean train loss 1614.350268023947
INFO:root:current train perplexity3.586543321609497
INFO:root:current mean train loss 1616.5650398023201
INFO:root:current train perplexity3.597520351409912
INFO:root:current mean train loss 1616.3157289017079
INFO:root:current train perplexity3.5923144817352295
INFO:root:current mean train loss 1620.8390026164504
INFO:root:current train perplexity3.593512773513794
INFO:root:current mean train loss 1623.6268444242933
INFO:root:current train perplexity3.5968635082244873
INFO:root:current mean train loss 1624.9640481191138
INFO:root:current train perplexity3.5971579551696777
INFO:root:current mean train loss 1625.7538166827467
INFO:root:current train perplexity3.5997073650360107
INFO:root:current mean train loss 1627.4078618531587
INFO:root:current train perplexity3.6068644523620605
INFO:root:current mean train loss 1628.9293763984754
INFO:root:current train perplexity3.610997438430786
INFO:root:current mean train loss 1629.467959459693
INFO:root:current train perplexity3.612440824508667
INFO:root:current mean train loss 1633.0225385464305
INFO:root:current train perplexity3.618791341781616
INFO:root:current mean train loss 1632.4095486519032
INFO:root:current train perplexity3.620465040206909
INFO:root:current mean train loss 1632.90111894941
INFO:root:current train perplexity3.6250436305999756
INFO:root:current mean train loss 1634.29949847452
INFO:root:current train perplexity3.626955509185791
INFO:root:current mean train loss 1635.1128086956
INFO:root:current train perplexity3.6309638023376465
INFO:root:current mean train loss 1635.4952778546108
INFO:root:current train perplexity3.6342968940734863
INFO:root:current mean train loss 1636.3384673571977
INFO:root:current train perplexity3.6364481449127197
INFO:root:current mean train loss 1636.3260918651838
INFO:root:current train perplexity3.6361052989959717


100%|██████████| 1/1 [07:46<00:00, 466.63s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.26s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.26s/it]
INFO:root:eval mean loss: 3203.6450869815126
INFO:root:eval perplexity: 14.049739837646484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/44

 22%|██▏       | 44/200 [6:29:35<22:59:11, 530.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1606.6509620179522
INFO:root:current train perplexity3.5229506492614746
INFO:root:current mean train loss 1609.5676169882015
INFO:root:current train perplexity3.5391368865966797
INFO:root:current mean train loss 1612.336432700215
INFO:root:current train perplexity3.548882246017456
INFO:root:current mean train loss 1612.4827715519182
INFO:root:current train perplexity3.554339647293091
INFO:root:current mean train loss 1611.6498160479587
INFO:root:current train perplexity3.558502435684204
INFO:root:current mean train loss 1614.7599088958239
INFO:root:current train perplexity3.564518928527832
INFO:root:current mean train loss 1613.841399721974
INFO:root:current train perplexity3.56848406791687
INFO:root:current mean train loss 1613.3577849810702
INFO:root:current train perplexity3.5712530612945557
INFO:root:current mean train loss 1615.5511133043094
INFO:root:current train perplexity3.5755667686462402
INFO:root:current mean train loss 1616.900039495611
INFO:root:current train perplexity3.5780351161956787
INFO:root:current mean train loss 1616.701619699327
INFO:root:current train perplexity3.5773355960845947
INFO:root:current mean train loss 1616.7843078240792
INFO:root:current train perplexity3.580132007598877
INFO:root:current mean train loss 1617.5582102123221
INFO:root:current train perplexity3.580526351928711
INFO:root:current mean train loss 1617.1873801046597
INFO:root:current train perplexity3.582819700241089
INFO:root:current mean train loss 1618.8305973667232
INFO:root:current train perplexity3.58500075340271
INFO:root:current mean train loss 1620.3004757191338
INFO:root:current train perplexity3.588379383087158
INFO:root:current mean train loss 1620.315532635687
INFO:root:current train perplexity3.5905375480651855
INFO:root:current mean train loss 1620.723969396756
INFO:root:current train perplexity3.5934319496154785
INFO:root:current mean train loss 1621.3346305479536
INFO:root:current train perplexity3.5947248935699463
INFO:root:current mean train loss 1622.2043173015577
INFO:root:current train perplexity3.5967493057250977


100%|██████████| 1/1 [07:46<00:00, 466.20s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.20s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.82s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.82s/it]
INFO:root:eval mean loss: 3201.5004091005067
INFO:root:eval perplexity: 14.024903297424316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/45

 22%|██▎       | 45/200 [6:38:26<22:50:26, 530.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1589.550703048706
INFO:root:current train perplexity3.487557888031006
INFO:root:current mean train loss 1583.285096703506
INFO:root:current train perplexity3.5012028217315674
INFO:root:current mean train loss 1585.22858914462
INFO:root:current train perplexity3.5041654109954834
INFO:root:current mean train loss 1591.5335113190033
INFO:root:current train perplexity3.511277437210083
INFO:root:current mean train loss 1593.1162543461242
INFO:root:current train perplexity3.508681058883667
INFO:root:current mean train loss 1593.26875489147
INFO:root:current train perplexity3.514512062072754
INFO:root:current mean train loss 1595.8513343535274
INFO:root:current train perplexity3.5212409496307373
INFO:root:current mean train loss 1596.9339064353424
INFO:root:current train perplexity3.5234577655792236
INFO:root:current mean train loss 1599.1350960908112
INFO:root:current train perplexity3.528019666671753
INFO:root:current mean train loss 1599.8253649952994
INFO:root:current train perplexity3.529726266860962
INFO:root:current mean train loss 1601.4205910818916
INFO:root:current train perplexity3.535006284713745
INFO:root:current mean train loss 1602.7665105341227
INFO:root:current train perplexity3.5411934852600098
INFO:root:current mean train loss 1602.7409915199762
INFO:root:current train perplexity3.543400287628174
INFO:root:current mean train loss 1602.8477878962094
INFO:root:current train perplexity3.544450044631958
INFO:root:current mean train loss 1603.1833229273395
INFO:root:current train perplexity3.546541213989258
INFO:root:current mean train loss 1604.2257991702966
INFO:root:current train perplexity3.549377202987671
INFO:root:current mean train loss 1604.704820046058
INFO:root:current train perplexity3.5507640838623047
INFO:root:current mean train loss 1606.1464662443755
INFO:root:current train perplexity3.55260968208313
INFO:root:current mean train loss 1607.2119625893786
INFO:root:current train perplexity3.5540359020233154
INFO:root:current mean train loss 1608.1321851804155
INFO:root:current train perplexity3.5567853450775146


100%|██████████| 1/1 [07:55<00:00, 475.99s/it][A
100%|██████████| 1/1 [07:55<00:00, 475.99s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.90s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.90s/it]
INFO:root:eval mean loss: 3216.859813426708
INFO:root:eval perplexity: 14.203720092773438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/46

 23%|██▎       | 46/200 [6:47:51<23:08:20, 540.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.4651692708333
INFO:root:current train perplexity3.471374273300171
INFO:root:current mean train loss 1573.8205391056629
INFO:root:current train perplexity3.462326765060425
INFO:root:current mean train loss 1574.968352945674
INFO:root:current train perplexity3.463188648223877
INFO:root:current mean train loss 1577.5421536663387
INFO:root:current train perplexity3.477419853210449
INFO:root:current mean train loss 1580.915883910879
INFO:root:current train perplexity3.478343963623047
INFO:root:current mean train loss 1582.3960477792868
INFO:root:current train perplexity3.482710838317871
INFO:root:current mean train loss 1583.937793434804
INFO:root:current train perplexity3.484522581100464
INFO:root:current mean train loss 1585.5015070447544
INFO:root:current train perplexity3.48862361907959
INFO:root:current mean train loss 1587.2683874470151
INFO:root:current train perplexity3.4953081607818604
INFO:root:current mean train loss 1586.9766823304903
INFO:root:current train perplexity3.4956440925598145
INFO:root:current mean train loss 1587.9750346449323
INFO:root:current train perplexity3.4988842010498047
INFO:root:current mean train loss 1590.2575285650732
INFO:root:current train perplexity3.5031909942626953
INFO:root:current mean train loss 1590.6804122984363
INFO:root:current train perplexity3.504972219467163
INFO:root:current mean train loss 1591.3226277698734
INFO:root:current train perplexity3.5064282417297363
INFO:root:current mean train loss 1591.4353085864966
INFO:root:current train perplexity3.508146047592163
INFO:root:current mean train loss 1592.0621451390537
INFO:root:current train perplexity3.5106067657470703
INFO:root:current mean train loss 1592.8397176130977
INFO:root:current train perplexity3.5118408203125
INFO:root:current mean train loss 1593.4419762313132
INFO:root:current train perplexity3.5133416652679443
INFO:root:current mean train loss 1594.1819797092014
INFO:root:current train perplexity3.5154812335968018
INFO:root:current mean train loss 1594.4456685435466
INFO:root:current train perplexity3.5183608531951904


100%|██████████| 1/1 [07:49<00:00, 469.82s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.43s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.43s/it]
INFO:root:eval mean loss: 3228.6403933230104
INFO:root:eval perplexity: 14.34242057800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/47

 24%|██▎       | 47/200 [6:56:28<22:41:16, 533.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1571.9964998206315
INFO:root:current train perplexity3.4384610652923584
INFO:root:current mean train loss 1568.965701941288
INFO:root:current train perplexity3.448455572128296
INFO:root:current mean train loss 1568.2038930598521
INFO:root:current train perplexity3.4411377906799316
INFO:root:current mean train loss 1567.1212676541888
INFO:root:current train perplexity3.4437012672424316
INFO:root:current mean train loss 1567.4451820955699
INFO:root:current train perplexity3.4449915885925293
INFO:root:current mean train loss 1570.3156166714568
INFO:root:current train perplexity3.4477624893188477
INFO:root:current mean train loss 1569.7659607808023
INFO:root:current train perplexity3.4482455253601074
INFO:root:current mean train loss 1570.6420225368108
INFO:root:current train perplexity3.451453447341919
INFO:root:current mean train loss 1571.940169098648
INFO:root:current train perplexity3.4571726322174072
INFO:root:current mean train loss 1571.7129936141814
INFO:root:current train perplexity3.4590137004852295
INFO:root:current mean train loss 1570.9701408011015
INFO:root:current train perplexity3.460726499557495
INFO:root:current mean train loss 1573.3747041985666
INFO:root:current train perplexity3.461866617202759
INFO:root:current mean train loss 1574.9652504943
INFO:root:current train perplexity3.4655823707580566
INFO:root:current mean train loss 1574.846900426949
INFO:root:current train perplexity3.4678103923797607
INFO:root:current mean train loss 1575.4204777105151
INFO:root:current train perplexity3.4694266319274902
INFO:root:current mean train loss 1576.17812044719
INFO:root:current train perplexity3.470759868621826
INFO:root:current mean train loss 1576.9532774800546
INFO:root:current train perplexity3.473379611968994
INFO:root:current mean train loss 1577.893135995833
INFO:root:current train perplexity3.4760611057281494
INFO:root:current mean train loss 1579.764625862853
INFO:root:current train perplexity3.4780449867248535


100%|██████████| 1/1 [07:42<00:00, 462.57s/it][A
100%|██████████| 1/1 [07:42<00:00, 462.57s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.15s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.15s/it]
INFO:root:eval mean loss: 3236.320736263607
INFO:root:eval perplexity: 14.433571815490723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/48

 24%|██▍       | 48/200 [7:04:57<22:13:22, 526.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1537.7891276041667
INFO:root:current train perplexity3.4168667793273926
INFO:root:current mean train loss 1545.2552044412364
INFO:root:current train perplexity3.380772113800049
INFO:root:current mean train loss 1551.0402048510175
INFO:root:current train perplexity3.401585340499878
INFO:root:current mean train loss 1550.22471671937
INFO:root:current train perplexity3.407383680343628
INFO:root:current mean train loss 1551.2851368364081
INFO:root:current train perplexity3.4084537029266357
INFO:root:current mean train loss 1554.0646655036408
INFO:root:current train perplexity3.4077932834625244
INFO:root:current mean train loss 1556.6800360454777
INFO:root:current train perplexity3.4143004417419434
INFO:root:current mean train loss 1558.5295990630464
INFO:root:current train perplexity3.4237892627716064
INFO:root:current mean train loss 1560.3269462351418
INFO:root:current train perplexity3.425429344177246
INFO:root:current mean train loss 1559.7978727747181
INFO:root:current train perplexity3.423877477645874
INFO:root:current mean train loss 1559.2256531663716
INFO:root:current train perplexity3.4248158931732178
INFO:root:current mean train loss 1561.083460074797
INFO:root:current train perplexity3.426182270050049
INFO:root:current mean train loss 1561.7418861922904
INFO:root:current train perplexity3.4284682273864746
INFO:root:current mean train loss 1561.633594585462
INFO:root:current train perplexity3.431380033493042
INFO:root:current mean train loss 1562.6957219315923
INFO:root:current train perplexity3.4334123134613037
INFO:root:current mean train loss 1563.1040190542492
INFO:root:current train perplexity3.4346940517425537
INFO:root:current mean train loss 1563.4979675104005
INFO:root:current train perplexity3.4366395473480225
INFO:root:current mean train loss 1564.450849936794
INFO:root:current train perplexity3.4387450218200684
INFO:root:current mean train loss 1565.1403238932292
INFO:root:current train perplexity3.4386425018310547
INFO:root:current mean train loss 1566.366656828798
INFO:root:current train perplexity3.4402835369110107


100%|██████████| 1/1 [07:52<00:00, 472.27s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.72s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.72s/it]
INFO:root:eval mean loss: 3245.4700410860078
INFO:root:eval perplexity: 14.542916297912598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/49

 24%|██▍       | 49/200 [7:13:37<21:59:54, 524.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1531.999668121338
INFO:root:current train perplexity3.3379268646240234
INFO:root:current mean train loss 1532.4437930945194
INFO:root:current train perplexity3.3433961868286133
INFO:root:current mean train loss 1535.057973927465
INFO:root:current train perplexity3.357172727584839
INFO:root:current mean train loss 1536.8183108410203
INFO:root:current train perplexity3.359217882156372
INFO:root:current mean train loss 1537.5755903455947
INFO:root:current train perplexity3.360910177230835
INFO:root:current mean train loss 1541.0162415468603
INFO:root:current train perplexity3.367823839187622
INFO:root:current mean train loss 1543.2158523752719
INFO:root:current train perplexity3.3731961250305176
INFO:root:current mean train loss 1543.8525203850752
INFO:root:current train perplexity3.3746047019958496
INFO:root:current mean train loss 1544.304508796105
INFO:root:current train perplexity3.377073287963867
INFO:root:current mean train loss 1545.2976945214004
INFO:root:current train perplexity3.38040828704834
INFO:root:current mean train loss 1546.8978202878966
INFO:root:current train perplexity3.38655424118042
INFO:root:current mean train loss 1547.5644727511456
INFO:root:current train perplexity3.387791633605957
INFO:root:current mean train loss 1548.8628045614664
INFO:root:current train perplexity3.389878511428833
INFO:root:current mean train loss 1549.5981415069855
INFO:root:current train perplexity3.392146587371826
INFO:root:current mean train loss 1549.819141068272
INFO:root:current train perplexity3.393159866333008
INFO:root:current mean train loss 1550.5196263883506
INFO:root:current train perplexity3.3950324058532715
INFO:root:current mean train loss 1550.653413361194
INFO:root:current train perplexity3.396777629852295
INFO:root:current mean train loss 1552.2443073087697
INFO:root:current train perplexity3.399122476577759
INFO:root:current mean train loss 1552.4560606177718
INFO:root:current train perplexity3.4006824493408203
INFO:root:current mean train loss 1552.3348600336237
INFO:root:current train perplexity3.4020586013793945


100%|██████████| 1/1 [07:52<00:00, 472.96s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.96s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it]
INFO:root:eval mean loss: 3254.975173171218
INFO:root:eval perplexity: 14.657389640808105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/50

 25%|██▌       | 50/200 [7:22:17<21:47:48, 523.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1525.5849883410394
INFO:root:current train perplexity3.3130202293395996
INFO:root:current mean train loss 1527.3001225618707
INFO:root:current train perplexity3.317530870437622
INFO:root:current mean train loss 1528.5418730586407
INFO:root:current train perplexity3.322561502456665
INFO:root:current mean train loss 1527.6250916401773
INFO:root:current train perplexity3.3248002529144287
INFO:root:current mean train loss 1530.9804924028222
INFO:root:current train perplexity3.334230422973633
INFO:root:current mean train loss 1531.2916290894666
INFO:root:current train perplexity3.341991662979126
INFO:root:current mean train loss 1531.064395569506
INFO:root:current train perplexity3.344235420227051
INFO:root:current mean train loss 1531.4286988771487
INFO:root:current train perplexity3.344485282897949
INFO:root:current mean train loss 1531.5698535501326
INFO:root:current train perplexity3.345731496810913
INFO:root:current mean train loss 1531.0347875950836
INFO:root:current train perplexity3.3469173908233643
INFO:root:current mean train loss 1530.2798489260606
INFO:root:current train perplexity3.3491828441619873
INFO:root:current mean train loss 1531.1216267584925
INFO:root:current train perplexity3.3492379188537598
INFO:root:current mean train loss 1533.0051809025344
INFO:root:current train perplexity3.351616859436035
INFO:root:current mean train loss 1534.742668270623
INFO:root:current train perplexity3.3522281646728516
INFO:root:current mean train loss 1535.5674557682562
INFO:root:current train perplexity3.3536782264709473
INFO:root:current mean train loss 1535.9103233972776
INFO:root:current train perplexity3.3565080165863037
INFO:root:current mean train loss 1535.8133204398262
INFO:root:current train perplexity3.3591458797454834
INFO:root:current mean train loss 1538.1289786267375
INFO:root:current train perplexity3.363804578781128
INFO:root:current mean train loss 1538.563020630873
INFO:root:current train perplexity3.3670358657836914
INFO:root:current mean train loss 1539.2813170165389
INFO:root:current train perplexity3.3693602085113525


100%|██████████| 1/1 [07:52<00:00, 472.54s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.54s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.91s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.91s/it]
INFO:root:eval mean loss: 3260.4329456409537
INFO:root:eval perplexity: 14.723523139953613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/51

 26%|██▌       | 51/200 [7:30:57<21:36:26, 522.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1500.8246644915957
INFO:root:current train perplexity3.2832345962524414
INFO:root:current mean train loss 1504.382713961314
INFO:root:current train perplexity3.2858712673187256
INFO:root:current mean train loss 1506.9804994970336
INFO:root:current train perplexity3.2957801818847656
INFO:root:current mean train loss 1507.791760720842
INFO:root:current train perplexity3.2988197803497314
INFO:root:current mean train loss 1510.7372879224786
INFO:root:current train perplexity3.3044583797454834
INFO:root:current mean train loss 1511.8255216241305
INFO:root:current train perplexity3.305363655090332
INFO:root:current mean train loss 1514.7261260894684
INFO:root:current train perplexity3.3077192306518555
INFO:root:current mean train loss 1516.011749028537
INFO:root:current train perplexity3.311438798904419
INFO:root:current mean train loss 1518.656295811607
INFO:root:current train perplexity3.314913272857666
INFO:root:current mean train loss 1517.4799424323483
INFO:root:current train perplexity3.316223621368408
INFO:root:current mean train loss 1518.636236308887
INFO:root:current train perplexity3.317044496536255
INFO:root:current mean train loss 1519.9339037415912
INFO:root:current train perplexity3.3189358711242676
INFO:root:current mean train loss 1520.3826518608685
INFO:root:current train perplexity3.319800853729248
INFO:root:current mean train loss 1520.944207790481
INFO:root:current train perplexity3.3226938247680664
INFO:root:current mean train loss 1522.1536112495203
INFO:root:current train perplexity3.324648857116699
INFO:root:current mean train loss 1522.8222091889168
INFO:root:current train perplexity3.326775074005127
INFO:root:current mean train loss 1524.15515246626
INFO:root:current train perplexity3.3287296295166016
INFO:root:current mean train loss 1524.7440768249442
INFO:root:current train perplexity3.330622434616089
INFO:root:current mean train loss 1525.3028884347996
INFO:root:current train perplexity3.3326194286346436
INFO:root:current mean train loss 1525.7696184253402
INFO:root:current train perplexity3.3331191539764404


100%|██████████| 1/1 [07:53<00:00, 473.59s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.59s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.16s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.16s/it]
INFO:root:eval mean loss: 3277.99633862378
INFO:root:eval perplexity: 14.938383102416992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/52

 26%|██▌       | 52/200 [7:39:41<21:29:07, 522.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.366961008095
INFO:root:current train perplexity3.2405152320861816
INFO:root:current mean train loss 1493.3390859801912
INFO:root:current train perplexity3.2568492889404297
INFO:root:current mean train loss 1497.1896260938881
INFO:root:current train perplexity3.262903928756714
INFO:root:current mean train loss 1500.221912991596
INFO:root:current train perplexity3.2695915699005127
INFO:root:current mean train loss 1499.862771991864
INFO:root:current train perplexity3.2684853076934814
INFO:root:current mean train loss 1501.1648409861439
INFO:root:current train perplexity3.2734174728393555
INFO:root:current mean train loss 1502.292733188255
INFO:root:current train perplexity3.2754156589508057
INFO:root:current mean train loss 1501.8285794507804
INFO:root:current train perplexity3.27782940864563
INFO:root:current mean train loss 1502.9618553083858
INFO:root:current train perplexity3.279130697250366
INFO:root:current mean train loss 1504.0928226133328
INFO:root:current train perplexity3.2788679599761963
INFO:root:current mean train loss 1506.2052139465388
INFO:root:current train perplexity3.2837648391723633
INFO:root:current mean train loss 1507.6680638884852
INFO:root:current train perplexity3.288113594055176
INFO:root:current mean train loss 1507.698826945209
INFO:root:current train perplexity3.2885942459106445
INFO:root:current mean train loss 1508.7074054930406
INFO:root:current train perplexity3.290581703186035
INFO:root:current mean train loss 1509.8635647362821
INFO:root:current train perplexity3.294123411178589
INFO:root:current mean train loss 1510.2485598324977
INFO:root:current train perplexity3.295729875564575
INFO:root:current mean train loss 1511.215040962822
INFO:root:current train perplexity3.2965335845947266
INFO:root:current mean train loss 1512.2466645296945
INFO:root:current train perplexity3.2989838123321533
INFO:root:current mean train loss 1512.708367475853
INFO:root:current train perplexity3.299941062927246
INFO:root:current mean train loss 1513.2856653072108
INFO:root:current train perplexity3.3012077808380127


100%|██████████| 1/1 [07:43<00:00, 463.18s/it][A
100%|██████████| 1/1 [07:43<00:00, 463.18s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.64s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.64s/it]
INFO:root:eval mean loss: 3281.5324582394896
INFO:root:eval perplexity: 14.982019424438477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/53

 26%|██▋       | 53/200 [7:48:11<21:11:05, 518.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.1667211914062
INFO:root:current train perplexity3.2263803482055664
INFO:root:current mean train loss 1477.6352435302733
INFO:root:current train perplexity3.221041202545166
INFO:root:current mean train loss 1484.0921850585937
INFO:root:current train perplexity3.2347350120544434
INFO:root:current mean train loss 1485.233840637207
INFO:root:current train perplexity3.23334002494812
INFO:root:current mean train loss 1487.390901123047
INFO:root:current train perplexity3.240997552871704
INFO:root:current mean train loss 1488.2061423746745
INFO:root:current train perplexity3.238994836807251
INFO:root:current mean train loss 1490.4398189871652
INFO:root:current train perplexity3.242597818374634
INFO:root:current mean train loss 1491.647129058838
INFO:root:current train perplexity3.2459707260131836
INFO:root:current mean train loss 1490.9840515136718
INFO:root:current train perplexity3.248443126678467
INFO:root:current mean train loss 1491.9695434570312
INFO:root:current train perplexity3.2496180534362793
INFO:root:current mean train loss 1493.3984806685014
INFO:root:current train perplexity3.2503786087036133
INFO:root:current mean train loss 1494.1137080891926
INFO:root:current train perplexity3.2524991035461426
INFO:root:current mean train loss 1495.422730806791
INFO:root:current train perplexity3.2552361488342285
INFO:root:current mean train loss 1497.2660774448939
INFO:root:current train perplexity3.2565648555755615
INFO:root:current mean train loss 1497.694712565104
INFO:root:current train perplexity3.258869171142578
INFO:root:current mean train loss 1498.2539044952393
INFO:root:current train perplexity3.259934663772583
INFO:root:current mean train loss 1499.0108402745864
INFO:root:current train perplexity3.2613272666931152
INFO:root:current mean train loss 1499.6088944498697
INFO:root:current train perplexity3.2629103660583496
INFO:root:current mean train loss 1499.951232267681
INFO:root:current train perplexity3.264302968978882


100%|██████████| 1/1 [07:53<00:00, 473.12s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.13s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.01s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.01s/it]
INFO:root:eval mean loss: 3305.945970140062
INFO:root:eval perplexity: 15.286789894104004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/54

 27%|██▋       | 54/200 [7:56:50<21:02:46, 518.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1456.9988439223346
INFO:root:current train perplexity3.1893389225006104
INFO:root:current mean train loss 1483.1910807291667
INFO:root:current train perplexity3.2071688175201416
INFO:root:current mean train loss 1478.0338174143146
INFO:root:current train perplexity3.1962575912475586
INFO:root:current mean train loss 1475.668724276666
INFO:root:current train perplexity3.2009098529815674
INFO:root:current mean train loss 1474.5145732047174
INFO:root:current train perplexity3.205566167831421
INFO:root:current mean train loss 1475.069638634097
INFO:root:current train perplexity3.206589460372925
INFO:root:current mean train loss 1473.991238833633
INFO:root:current train perplexity3.207268238067627
INFO:root:current mean train loss 1475.856450930799
INFO:root:current train perplexity3.207897663116455
INFO:root:current mean train loss 1476.55761927928
INFO:root:current train perplexity3.2083868980407715
INFO:root:current mean train loss 1478.8746589485925
INFO:root:current train perplexity3.211059093475342
INFO:root:current mean train loss 1479.7116687215769
INFO:root:current train perplexity3.21297287940979
INFO:root:current mean train loss 1480.3187862385994
INFO:root:current train perplexity3.2144298553466797
INFO:root:current mean train loss 1481.4695292639751
INFO:root:current train perplexity3.217686176300049
INFO:root:current mean train loss 1482.3233135573867
INFO:root:current train perplexity3.2200894355773926
INFO:root:current mean train loss 1483.3423889633964
INFO:root:current train perplexity3.2218027114868164
INFO:root:current mean train loss 1484.9372231088034
INFO:root:current train perplexity3.225069761276245
INFO:root:current mean train loss 1485.667061111554
INFO:root:current train perplexity3.228229284286499
INFO:root:current mean train loss 1486.29863033839
INFO:root:current train perplexity3.231447696685791
INFO:root:current mean train loss 1486.82206669809
INFO:root:current train perplexity3.232905626296997
INFO:root:current mean train loss 1487.528879721835
INFO:root:current train perplexity3.2349343299865723


100%|██████████| 1/1 [07:51<00:00, 471.01s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.01s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.06s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.06s/it]
INFO:root:eval mean loss: 3300.6358426297393
INFO:root:eval perplexity: 15.219976425170898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/55

 28%|██▊       | 55/200 [8:05:27<20:52:50, 518.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1464.7835585650275
INFO:root:current train perplexity3.1627848148345947
INFO:root:current mean train loss 1452.1863394210588
INFO:root:current train perplexity3.1497302055358887
INFO:root:current mean train loss 1462.910424909021
INFO:root:current train perplexity3.160360336303711
INFO:root:current mean train loss 1462.1218210551554
INFO:root:current train perplexity3.1622629165649414
INFO:root:current mean train loss 1459.0342488794283
INFO:root:current train perplexity3.1708662509918213
INFO:root:current mean train loss 1457.9087631854225
INFO:root:current train perplexity3.1759073734283447
INFO:root:current mean train loss 1459.2207943889246
INFO:root:current train perplexity3.174811840057373
INFO:root:current mean train loss 1463.8284850575294
INFO:root:current train perplexity3.180333375930786
INFO:root:current mean train loss 1463.759158200783
INFO:root:current train perplexity3.1793746948242188
INFO:root:current mean train loss 1465.582347142875
INFO:root:current train perplexity3.1810848712921143
INFO:root:current mean train loss 1468.1972101384943
INFO:root:current train perplexity3.183624267578125
INFO:root:current mean train loss 1468.270011242525
INFO:root:current train perplexity3.185292959213257
INFO:root:current mean train loss 1468.5986881101537
INFO:root:current train perplexity3.1880879402160645
INFO:root:current mean train loss 1469.703326315358
INFO:root:current train perplexity3.1911773681640625
INFO:root:current mean train loss 1470.2053244788942
INFO:root:current train perplexity3.192695140838623
INFO:root:current mean train loss 1470.8961967856196
INFO:root:current train perplexity3.1951487064361572
INFO:root:current mean train loss 1472.4911662630527
INFO:root:current train perplexity3.1971566677093506
INFO:root:current mean train loss 1473.8553837090903
INFO:root:current train perplexity3.198726177215576
INFO:root:current mean train loss 1474.2643009963706
INFO:root:current train perplexity3.2006046772003174
INFO:root:current mean train loss 1474.72742955268
INFO:root:current train perplexity3.201024055480957


100%|██████████| 1/1 [07:46<00:00, 466.95s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.95s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.14s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.14s/it]
INFO:root:eval mean loss: 3318.5614603861673
INFO:root:eval perplexity: 15.446694374084473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/56

 28%|██▊       | 56/200 [8:14:00<20:40:25, 516.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.1033097809436
INFO:root:current train perplexity3.1153202056884766
INFO:root:current mean train loss 1451.9590821929326
INFO:root:current train perplexity3.1382226943969727
INFO:root:current mean train loss 1448.9108648414156
INFO:root:current train perplexity3.131387710571289
INFO:root:current mean train loss 1448.2458117015
INFO:root:current train perplexity3.1347360610961914
INFO:root:current mean train loss 1447.8610136112459
INFO:root:current train perplexity3.1414945125579834
INFO:root:current mean train loss 1447.5988151425674
INFO:root:current train perplexity3.1399669647216797
INFO:root:current mean train loss 1448.7722876764112
INFO:root:current train perplexity3.140235185623169
INFO:root:current mean train loss 1450.478378438124
INFO:root:current train perplexity3.1463122367858887
INFO:root:current mean train loss 1450.9308716250644
INFO:root:current train perplexity3.14703631401062
INFO:root:current mean train loss 1452.9433175296563
INFO:root:current train perplexity3.1502671241760254
INFO:root:current mean train loss 1454.237470661312
INFO:root:current train perplexity3.15339994430542
INFO:root:current mean train loss 1455.3187140258472
INFO:root:current train perplexity3.1556577682495117
INFO:root:current mean train loss 1455.977274040143
INFO:root:current train perplexity3.1582937240600586
INFO:root:current mean train loss 1457.2312412897274
INFO:root:current train perplexity3.1597959995269775
INFO:root:current mean train loss 1458.6105469927797
INFO:root:current train perplexity3.1603622436523438
INFO:root:current mean train loss 1459.5587759503696
INFO:root:current train perplexity3.1603291034698486
INFO:root:current mean train loss 1459.8848654402161
INFO:root:current train perplexity3.1624581813812256
INFO:root:current mean train loss 1460.9112771217785
INFO:root:current train perplexity3.1650383472442627
INFO:root:current mean train loss 1461.363803494627
INFO:root:current train perplexity3.166717052459717
INFO:root:current mean train loss 1462.4066922311474
INFO:root:current train perplexity3.169994592666626


100%|██████████| 1/1 [07:49<00:00, 469.89s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.89s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.73s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.73s/it]
INFO:root:eval mean loss: 3326.98621155335
INFO:root:eval perplexity: 15.554414749145508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/57

 28%|██▊       | 57/200 [8:22:37<20:31:43, 516.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1434.3004617130055
INFO:root:current train perplexity3.0772030353546143
INFO:root:current mean train loss 1432.5340002150763
INFO:root:current train perplexity3.093679189682007
INFO:root:current mean train loss 1436.7869608864855
INFO:root:current train perplexity3.095719337463379
INFO:root:current mean train loss 1435.0149532815685
INFO:root:current train perplexity3.1007754802703857
INFO:root:current mean train loss 1436.1962055956196
INFO:root:current train perplexity3.103529453277588
INFO:root:current mean train loss 1438.9084064322458
INFO:root:current train perplexity3.111257314682007
INFO:root:current mean train loss 1439.7352240099879
INFO:root:current train perplexity3.116858720779419
INFO:root:current mean train loss 1439.0240410168965
INFO:root:current train perplexity3.1160855293273926
INFO:root:current mean train loss 1440.9311709074382
INFO:root:current train perplexity3.11808180809021
INFO:root:current mean train loss 1441.490889872401
INFO:root:current train perplexity3.1202805042266846
INFO:root:current mean train loss 1442.3131098943704
INFO:root:current train perplexity3.1221654415130615
INFO:root:current mean train loss 1442.4417493637293
INFO:root:current train perplexity3.1238396167755127
INFO:root:current mean train loss 1443.661150718713
INFO:root:current train perplexity3.124549388885498
INFO:root:current mean train loss 1445.5127433196842
INFO:root:current train perplexity3.126992702484131
INFO:root:current mean train loss 1445.991605836624
INFO:root:current train perplexity3.129331111907959
INFO:root:current mean train loss 1446.616914398816
INFO:root:current train perplexity3.1310577392578125
INFO:root:current mean train loss 1448.0214911810785
INFO:root:current train perplexity3.134359359741211
INFO:root:current mean train loss 1449.3092817763936
INFO:root:current train perplexity3.137225389480591
INFO:root:current mean train loss 1449.7314494947805
INFO:root:current train perplexity3.1387314796447754
INFO:root:current mean train loss 1450.758305557375
INFO:root:current train perplexity3.140944004058838


100%|██████████| 1/1 [07:49<00:00, 469.91s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.17s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.18s/it]
INFO:root:eval mean loss: 3338.19689244909
INFO:root:eval perplexity: 15.698917388916016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/58

 29%|██▉       | 58/200 [8:31:14<20:23:22, 516.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1421.6112807329964
INFO:root:current train perplexity3.0870003700256348
INFO:root:current mean train loss 1430.7387299408783
INFO:root:current train perplexity3.084925651550293
INFO:root:current mean train loss 1432.9030915912829
INFO:root:current train perplexity3.085413694381714
INFO:root:current mean train loss 1431.0399391867898
INFO:root:current train perplexity3.0851290225982666
INFO:root:current mean train loss 1428.1142200587951
INFO:root:current train perplexity3.0870361328125
INFO:root:current mean train loss 1430.2639516977163
INFO:root:current train perplexity3.090276002883911
INFO:root:current mean train loss 1430.022852631729
INFO:root:current train perplexity3.089717149734497
INFO:root:current mean train loss 1431.470688196656
INFO:root:current train perplexity3.093683958053589
INFO:root:current mean train loss 1431.8688648978195
INFO:root:current train perplexity3.095109224319458
INFO:root:current mean train loss 1431.8882025549256
INFO:root:current train perplexity3.0950968265533447
INFO:root:current mean train loss 1432.8448606710829
INFO:root:current train perplexity3.097352981567383
INFO:root:current mean train loss 1434.4437443342892
INFO:root:current train perplexity3.101936101913452
INFO:root:current mean train loss 1435.455062735591
INFO:root:current train perplexity3.1034858226776123
INFO:root:current mean train loss 1435.5349929313797
INFO:root:current train perplexity3.1050944328308105
INFO:root:current mean train loss 1436.5744673295455
INFO:root:current train perplexity3.106254816055298
INFO:root:current mean train loss 1437.3795921542292
INFO:root:current train perplexity3.108198642730713
INFO:root:current mean train loss 1438.0685764935322
INFO:root:current train perplexity3.1097919940948486
INFO:root:current mean train loss 1438.6202960187982
INFO:root:current train perplexity3.110600233078003
INFO:root:current mean train loss 1438.7994191784442
INFO:root:current train perplexity3.1118414402008057


100%|██████████| 1/1 [08:07<00:00, 487.68s/it][A
100%|██████████| 1/1 [08:07<00:00, 487.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.13s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.13s/it]
INFO:root:eval mean loss: 3352.167266387481
INFO:root:eval perplexity: 15.880881309509277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/59

 30%|██▉       | 59/200 [8:40:08<20:26:46, 522.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1430.7865600585938
INFO:root:current train perplexity3.0343503952026367
INFO:root:current mean train loss 1406.941603716682
INFO:root:current train perplexity3.0178773403167725
INFO:root:current mean train loss 1407.728325872138
INFO:root:current train perplexity3.0328893661499023
INFO:root:current mean train loss 1414.3617630510141
INFO:root:current train perplexity3.044376850128174
INFO:root:current mean train loss 1416.1989779496075
INFO:root:current train perplexity3.0510125160217285
INFO:root:current mean train loss 1415.5378337723325
INFO:root:current train perplexity3.0562493801116943
INFO:root:current mean train loss 1416.645524237243
INFO:root:current train perplexity3.0586819648742676
INFO:root:current mean train loss 1417.1565311790532
INFO:root:current train perplexity3.060682773590088
INFO:root:current mean train loss 1419.1068952374922
INFO:root:current train perplexity3.0623536109924316
INFO:root:current mean train loss 1419.6767503691883
INFO:root:current train perplexity3.066187620162964
INFO:root:current mean train loss 1420.071100353005
INFO:root:current train perplexity3.067003011703491
INFO:root:current mean train loss 1420.4136669345862
INFO:root:current train perplexity3.0698037147521973
INFO:root:current mean train loss 1421.7002718857243
INFO:root:current train perplexity3.0717625617980957
INFO:root:current mean train loss 1423.4050592050392
INFO:root:current train perplexity3.0734269618988037
INFO:root:current mean train loss 1424.2272438125501
INFO:root:current train perplexity3.076127529144287
INFO:root:current mean train loss 1425.2735362452927
INFO:root:current train perplexity3.0782787799835205
INFO:root:current mean train loss 1425.4101890916384
INFO:root:current train perplexity3.0792901515960693
INFO:root:current mean train loss 1426.2615895792403
INFO:root:current train perplexity3.0806093215942383
INFO:root:current mean train loss 1426.6672631537874
INFO:root:current train perplexity3.0811495780944824
INFO:root:current mean train loss 1426.8587126626578
INFO:root:current train perplexity3.082728385925293


100%|██████████| 1/1 [07:45<00:00, 465.78s/it][A
100%|██████████| 1/1 [07:45<00:00, 465.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.62s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.62s/it]
INFO:root:eval mean loss: 3365.394435206691
INFO:root:eval perplexity: 16.055099487304688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/60

 30%|███       | 60/200 [8:48:41<20:11:36, 519.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1392.1760960629113
INFO:root:current train perplexity3.007976770401001
INFO:root:current mean train loss 1404.697811351103
INFO:root:current train perplexity3.022320032119751
INFO:root:current mean train loss 1400.8397551236087
INFO:root:current train perplexity3.0277113914489746
INFO:root:current mean train loss 1401.7587546226
INFO:root:current train perplexity3.0277884006500244
INFO:root:current mean train loss 1403.3559564485754
INFO:root:current train perplexity3.029019832611084
INFO:root:current mean train loss 1406.2724875154292
INFO:root:current train perplexity3.02811861038208
INFO:root:current mean train loss 1406.4554194880225
INFO:root:current train perplexity3.029698610305786
INFO:root:current mean train loss 1407.135162735516
INFO:root:current train perplexity3.033304214477539
INFO:root:current mean train loss 1408.0857155675271
INFO:root:current train perplexity3.0345523357391357
INFO:root:current mean train loss 1408.6492949144365
INFO:root:current train perplexity3.037916660308838
INFO:root:current mean train loss 1410.0958186066302
INFO:root:current train perplexity3.042001962661743
INFO:root:current mean train loss 1410.3863143579997
INFO:root:current train perplexity3.0429413318634033
INFO:root:current mean train loss 1411.10350160544
INFO:root:current train perplexity3.0453667640686035
INFO:root:current mean train loss 1411.8141767778027
INFO:root:current train perplexity3.0464720726013184
INFO:root:current mean train loss 1413.0188205925992
INFO:root:current train perplexity3.0472004413604736
INFO:root:current mean train loss 1413.3156994636943
INFO:root:current train perplexity3.0481984615325928
INFO:root:current mean train loss 1413.9270189178071
INFO:root:current train perplexity3.0481390953063965
INFO:root:current mean train loss 1414.8294128808536
INFO:root:current train perplexity3.0505478382110596
INFO:root:current mean train loss 1415.868992248166
INFO:root:current train perplexity3.052520513534546
INFO:root:current mean train loss 1415.7544526084753
INFO:root:current train perplexity3.05430006980896


100%|██████████| 1/1 [07:51<00:00, 471.75s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.75s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.75s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.75s/it]
INFO:root:eval mean loss: 3372.340960345111
INFO:root:eval perplexity: 16.147357940673828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/61

 30%|███       | 61/200 [8:57:20<20:03:13, 519.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1381.8424682617188
INFO:root:current train perplexity2.974688768386841
INFO:root:current mean train loss 1390.3487620634191
INFO:root:current train perplexity2.9844467639923096
INFO:root:current mean train loss 1388.5903284105204
INFO:root:current train perplexity2.9871184825897217
INFO:root:current mean train loss 1391.4842783610027
INFO:root:current train perplexity2.9892361164093018
INFO:root:current mean train loss 1391.4642683956602
INFO:root:current train perplexity2.9909582138061523
INFO:root:current mean train loss 1391.782735568374
INFO:root:current train perplexity2.99194598197937
INFO:root:current mean train loss 1392.3563228583187
INFO:root:current train perplexity2.993339776992798
INFO:root:current mean train loss 1393.8362640712571
INFO:root:current train perplexity2.9974870681762695
INFO:root:current mean train loss 1395.6547327361038
INFO:root:current train perplexity3.0009636878967285
INFO:root:current mean train loss 1394.9534726917234
INFO:root:current train perplexity3.005385160446167
INFO:root:current mean train loss 1395.8593766495987
INFO:root:current train perplexity3.005624294281006
INFO:root:current mean train loss 1397.224942811778
INFO:root:current train perplexity3.009037733078003
INFO:root:current mean train loss 1397.5525344799253
INFO:root:current train perplexity3.011855125427246
INFO:root:current mean train loss 1398.522704895385
INFO:root:current train perplexity3.0146515369415283
INFO:root:current mean train loss 1399.0784844953703
INFO:root:current train perplexity3.0182321071624756
INFO:root:current mean train loss 1399.7008803685505
INFO:root:current train perplexity3.020280361175537
INFO:root:current mean train loss 1400.9674593825212
INFO:root:current train perplexity3.02154541015625
INFO:root:current mean train loss 1402.1002768942838
INFO:root:current train perplexity3.0238544940948486
INFO:root:current mean train loss 1403.6808954376022
INFO:root:current train perplexity3.0269806385040283
INFO:root:current mean train loss 1403.9428468184037
INFO:root:current train perplexity3.027301073074341


100%|██████████| 1/1 [07:43<00:00, 463.21s/it][A
100%|██████████| 1/1 [07:43<00:00, 463.21s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.76s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.76s/it]
INFO:root:eval mean loss: 3384.6240586289414
INFO:root:eval perplexity: 16.31179428100586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/62

 31%|███       | 62/200 [9:05:49<19:47:24, 516.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1367.4187149911556
INFO:root:current train perplexity2.967526435852051
INFO:root:current mean train loss 1371.0816123812806
INFO:root:current train perplexity2.9482479095458984
INFO:root:current mean train loss 1376.9665710690465
INFO:root:current train perplexity2.956181526184082
INFO:root:current mean train loss 1378.4389039814978
INFO:root:current train perplexity2.9601993560791016
INFO:root:current mean train loss 1378.760553557878
INFO:root:current train perplexity2.96502947807312
INFO:root:current mean train loss 1378.4625310363217
INFO:root:current train perplexity2.9708096981048584
INFO:root:current mean train loss 1380.29332019286
INFO:root:current train perplexity2.974147319793701
INFO:root:current mean train loss 1381.4578319210138
INFO:root:current train perplexity2.9786367416381836
INFO:root:current mean train loss 1382.901431871886
INFO:root:current train perplexity2.980600595474243
INFO:root:current mean train loss 1384.7574159315975
INFO:root:current train perplexity2.981938600540161
INFO:root:current mean train loss 1385.6404723251646
INFO:root:current train perplexity2.984910726547241
INFO:root:current mean train loss 1387.1666338110838
INFO:root:current train perplexity2.9867563247680664
INFO:root:current mean train loss 1387.253068611894
INFO:root:current train perplexity2.9889042377471924
INFO:root:current mean train loss 1388.4003004932615
INFO:root:current train perplexity2.992544174194336
INFO:root:current mean train loss 1388.7372047965148
INFO:root:current train perplexity2.992820978164673
INFO:root:current mean train loss 1389.7257578420547
INFO:root:current train perplexity2.9952549934387207
INFO:root:current mean train loss 1390.4773411505596
INFO:root:current train perplexity2.9967010021209717
INFO:root:current mean train loss 1391.326984864674
INFO:root:current train perplexity2.9981133937835693
INFO:root:current mean train loss 1392.4056731798942
INFO:root:current train perplexity2.9999778270721436
INFO:root:current mean train loss 1393.0481457063252
INFO:root:current train perplexity3.0009982585906982


100%|██████████| 1/1 [07:47<00:00, 467.70s/it][A
100%|██████████| 1/1 [07:47<00:00, 467.71s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.39s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.39s/it]
INFO:root:eval mean loss: 3390.7207947693787
INFO:root:eval perplexity: 16.394041061401367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/63

 32%|███▏      | 63/200 [9:14:25<19:38:03, 515.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.5006504603796
INFO:root:current train perplexity2.917870044708252
INFO:root:current mean train loss 1365.1958266314339
INFO:root:current train perplexity2.941575288772583
INFO:root:current mean train loss 1366.962797941985
INFO:root:current train perplexity2.9402382373809814
INFO:root:current mean train loss 1362.7973042256124
INFO:root:current train perplexity2.937161445617676
INFO:root:current mean train loss 1363.7361712516622
INFO:root:current train perplexity2.9419758319854736
INFO:root:current mean train loss 1366.6921223958334
INFO:root:current train perplexity2.9415879249572754
INFO:root:current mean train loss 1366.8854718108676
INFO:root:current train perplexity2.9465084075927734
INFO:root:current mean train loss 1369.0077015269887
INFO:root:current train perplexity2.9472432136535645
INFO:root:current mean train loss 1370.576183661099
INFO:root:current train perplexity2.9504573345184326
INFO:root:current mean train loss 1372.7064567644572
INFO:root:current train perplexity2.9536192417144775
INFO:root:current mean train loss 1373.757845242224
INFO:root:current train perplexity2.9554107189178467
INFO:root:current mean train loss 1375.0714236528445
INFO:root:current train perplexity2.9571807384490967
INFO:root:current mean train loss 1375.892849563238
INFO:root:current train perplexity2.9601640701293945
INFO:root:current mean train loss 1376.9170629348198
INFO:root:current train perplexity2.9637444019317627
INFO:root:current mean train loss 1377.9985040158642
INFO:root:current train perplexity2.9657585620880127
INFO:root:current mean train loss 1378.521717397119
INFO:root:current train perplexity2.9672529697418213
INFO:root:current mean train loss 1379.0785396735826
INFO:root:current train perplexity2.96850848197937
INFO:root:current mean train loss 1379.2070681469588
INFO:root:current train perplexity2.969917058944702
INFO:root:current mean train loss 1379.895454414898
INFO:root:current train perplexity2.9718127250671387
INFO:root:current mean train loss 1381.1219958929846
INFO:root:current train perplexity2.9732954502105713


100%|██████████| 1/1 [07:52<00:00, 472.14s/it][A
100%|██████████| 1/1 [07:52<00:00, 472.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.56s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.56s/it]
INFO:root:eval mean loss: 3404.4969280804244
INFO:root:eval perplexity: 16.58139419555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/64

 32%|███▏      | 64/200 [9:23:05<19:32:06, 517.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.039827193337
INFO:root:current train perplexity2.9026358127593994
INFO:root:current mean train loss 1356.929938168449
INFO:root:current train perplexity2.9100751876831055
INFO:root:current mean train loss 1362.3945899458297
INFO:root:current train perplexity2.9190597534179688
INFO:root:current mean train loss 1361.9037865138484
INFO:root:current train perplexity2.9215056896209717
INFO:root:current mean train loss 1360.1649844191159
INFO:root:current train perplexity2.922006130218506
INFO:root:current mean train loss 1360.4705525646827
INFO:root:current train perplexity2.9245009422302246
INFO:root:current mean train loss 1361.8450110378571
INFO:root:current train perplexity2.928701639175415
INFO:root:current mean train loss 1362.7240756469882
INFO:root:current train perplexity2.929837703704834
INFO:root:current mean train loss 1363.461181365382
INFO:root:current train perplexity2.931063175201416
INFO:root:current mean train loss 1363.3183065644394
INFO:root:current train perplexity2.9335107803344727
INFO:root:current mean train loss 1365.3204602870574
INFO:root:current train perplexity2.9371113777160645
INFO:root:current mean train loss 1365.6679097202111
INFO:root:current train perplexity2.9381601810455322
INFO:root:current mean train loss 1366.3548222610723
INFO:root:current train perplexity2.940295457839966
INFO:root:current mean train loss 1366.9161560014588
INFO:root:current train perplexity2.9424314498901367
INFO:root:current mean train loss 1367.4338980638188
INFO:root:current train perplexity2.9437949657440186
INFO:root:current mean train loss 1367.8626735905993
INFO:root:current train perplexity2.9441356658935547
INFO:root:current mean train loss 1368.439703343815
INFO:root:current train perplexity2.944715738296509
INFO:root:current mean train loss 1368.9917351499107
INFO:root:current train perplexity2.945904016494751
INFO:root:current mean train loss 1369.9176156711628
INFO:root:current train perplexity2.9483273029327393


100%|██████████| 1/1 [07:54<00:00, 474.03s/it][A
100%|██████████| 1/1 [07:54<00:00, 474.03s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.38s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.38s/it]
INFO:root:eval mean loss: 3419.12511657165
INFO:root:eval perplexity: 16.782682418823242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/65

 32%|███▎      | 65/200 [9:31:46<19:26:28, 518.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1297.1622619628906
INFO:root:current train perplexity2.8471245765686035
INFO:root:current mean train loss 1332.1513448861929
INFO:root:current train perplexity2.874171495437622
INFO:root:current mean train loss 1337.7912011240044
INFO:root:current train perplexity2.8923513889312744
INFO:root:current mean train loss 1337.9029733758223
INFO:root:current train perplexity2.8870129585266113
INFO:root:current mean train loss 1338.5120586735188
INFO:root:current train perplexity2.892439842224121
INFO:root:current mean train loss 1342.1238320971293
INFO:root:current train perplexity2.894718885421753
INFO:root:current mean train loss 1344.6133226811492
INFO:root:current train perplexity2.895871639251709
INFO:root:current mean train loss 1344.5626371557062
INFO:root:current train perplexity2.8991758823394775
INFO:root:current mean train loss 1347.5481429218653
INFO:root:current train perplexity2.902601957321167
INFO:root:current mean train loss 1348.7761793558577
INFO:root:current train perplexity2.9030046463012695
INFO:root:current mean train loss 1350.3114121881615
INFO:root:current train perplexity2.904541015625
INFO:root:current mean train loss 1351.9028771441915
INFO:root:current train perplexity2.9076523780822754
INFO:root:current mean train loss 1352.9448835303222
INFO:root:current train perplexity2.910247802734375
INFO:root:current mean train loss 1354.077909411097
INFO:root:current train perplexity2.913567066192627
INFO:root:current mean train loss 1355.011816128027
INFO:root:current train perplexity2.9174253940582275
INFO:root:current mean train loss 1355.984110974251
INFO:root:current train perplexity2.9177591800689697
INFO:root:current mean train loss 1357.400083470523
INFO:root:current train perplexity2.918564796447754
INFO:root:current mean train loss 1358.4558629138928
INFO:root:current train perplexity2.9203875064849854
INFO:root:current mean train loss 1359.5551561579737
INFO:root:current train perplexity2.92254638671875
INFO:root:current mean train loss 1359.899854644006
INFO:root:current train perplexity2.9235596656799316


100%|██████████| 1/1 [07:49<00:00, 469.61s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.61s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.67s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.67s/it]
INFO:root:eval mean loss: 3422.271529830612
INFO:root:eval perplexity: 16.82630157470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/66

 33%|███▎      | 66/200 [9:40:22<19:16:29, 517.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1317.1971377418154
INFO:root:current train perplexity2.870715379714966
INFO:root:current mean train loss 1334.3202065712164
INFO:root:current train perplexity2.8657538890838623
INFO:root:current mean train loss 1334.5971281992365
INFO:root:current train perplexity2.8571910858154297
INFO:root:current mean train loss 1337.8808289524923
INFO:root:current train perplexity2.865556240081787
INFO:root:current mean train loss 1338.3237504755234
INFO:root:current train perplexity2.8660526275634766
INFO:root:current mean train loss 1338.6376097929913
INFO:root:current train perplexity2.869412899017334
INFO:root:current mean train loss 1340.0286590035603
INFO:root:current train perplexity2.872821569442749
INFO:root:current mean train loss 1339.3214793635143
INFO:root:current train perplexity2.8738927841186523
INFO:root:current mean train loss 1339.3391554875438
INFO:root:current train perplexity2.8742640018463135
INFO:root:current mean train loss 1340.7169068840765
INFO:root:current train perplexity2.878202199935913
INFO:root:current mean train loss 1342.0661890102763
INFO:root:current train perplexity2.8816518783569336
INFO:root:current mean train loss 1342.6606695768985
INFO:root:current train perplexity2.882577419281006
INFO:root:current mean train loss 1343.6062652562896
INFO:root:current train perplexity2.88519024848938
INFO:root:current mean train loss 1344.682397812086
INFO:root:current train perplexity2.888040781021118
INFO:root:current mean train loss 1345.597879258115
INFO:root:current train perplexity2.8889894485473633
INFO:root:current mean train loss 1347.0184040458323
INFO:root:current train perplexity2.8916263580322266
INFO:root:current mean train loss 1348.0405073124712
INFO:root:current train perplexity2.893566370010376
INFO:root:current mean train loss 1348.5892248159228
INFO:root:current train perplexity2.8954977989196777
INFO:root:current mean train loss 1349.0077312538613
INFO:root:current train perplexity2.897286891937256
INFO:root:current mean train loss 1349.279643641109
INFO:root:current train perplexity2.8992257118225098


100%|██████████| 1/1 [07:56<00:00, 476.27s/it][A
100%|██████████| 1/1 [07:56<00:00, 476.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.85s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.85s/it]
INFO:root:eval mean loss: 3430.949067720064
INFO:root:eval perplexity: 16.947168350219727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/67

 34%|███▎      | 67/200 [9:49:06<19:11:26, 519.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1333.0559306897615
INFO:root:current train perplexity2.8304145336151123
INFO:root:current mean train loss 1318.0227705361187
INFO:root:current train perplexity2.83306884765625
INFO:root:current mean train loss 1321.232597799862
INFO:root:current train perplexity2.838928699493408
INFO:root:current mean train loss 1321.8422418176774
INFO:root:current train perplexity2.8455986976623535
INFO:root:current mean train loss 1326.1989330831727
INFO:root:current train perplexity2.8476433753967285
INFO:root:current mean train loss 1328.399633698304
INFO:root:current train perplexity2.850148916244507
INFO:root:current mean train loss 1328.2127547787275
INFO:root:current train perplexity2.848839521408081
INFO:root:current mean train loss 1328.685989669345
INFO:root:current train perplexity2.8527071475982666
INFO:root:current mean train loss 1328.67841945457
INFO:root:current train perplexity2.8536441326141357
INFO:root:current mean train loss 1330.192799257063
INFO:root:current train perplexity2.857379913330078
INFO:root:current mean train loss 1331.4638176772864
INFO:root:current train perplexity2.8593556880950928
INFO:root:current mean train loss 1332.5917838956434
INFO:root:current train perplexity2.863438844680786
INFO:root:current mean train loss 1333.4095984537498
INFO:root:current train perplexity2.8664610385894775
INFO:root:current mean train loss 1335.3543466701992
INFO:root:current train perplexity2.868102788925171
INFO:root:current mean train loss 1336.0862528624555
INFO:root:current train perplexity2.8696420192718506
INFO:root:current mean train loss 1336.3929316368153
INFO:root:current train perplexity2.871687889099121
INFO:root:current mean train loss 1336.6607449150783
INFO:root:current train perplexity2.8728272914886475
INFO:root:current mean train loss 1336.7463042475413
INFO:root:current train perplexity2.8733770847320557
INFO:root:current mean train loss 1337.9941076832836
INFO:root:current train perplexity2.874755382537842
INFO:root:current mean train loss 1338.6293001125719
INFO:root:current train perplexity2.8760552406311035


100%|██████████| 1/1 [07:49<00:00, 469.32s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.33s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it]
INFO:root:eval mean loss: 3436.478644660285
INFO:root:eval perplexity: 17.024648666381836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/68

 34%|███▍      | 68/200 [9:57:42<19:00:38, 518.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1316.804219193892
INFO:root:current train perplexity2.8231804370880127
INFO:root:current mean train loss 1314.5987115675402
INFO:root:current train perplexity2.8198132514953613
INFO:root:current mean train loss 1313.1626546223958
INFO:root:current train perplexity2.8279783725738525
INFO:root:current mean train loss 1314.6095348949164
INFO:root:current train perplexity2.8315601348876953
INFO:root:current mean train loss 1316.9343310010302
INFO:root:current train perplexity2.8336739540100098
INFO:root:current mean train loss 1317.541365559896
INFO:root:current train perplexity2.834192991256714
INFO:root:current mean train loss 1318.7645586086594
INFO:root:current train perplexity2.8346779346466064
INFO:root:current mean train loss 1319.372710251966
INFO:root:current train perplexity2.8373794555664062
INFO:root:current mean train loss 1319.7366966202942
INFO:root:current train perplexity2.839050531387329
INFO:root:current mean train loss 1320.8801627433736
INFO:root:current train perplexity2.838737726211548
INFO:root:current mean train loss 1322.9748787396327
INFO:root:current train perplexity2.8403687477111816
INFO:root:current mean train loss 1325.27942285579
INFO:root:current train perplexity2.843097448348999
INFO:root:current mean train loss 1325.4617068834038
INFO:root:current train perplexity2.8442323207855225
INFO:root:current mean train loss 1325.7449418747117
INFO:root:current train perplexity2.845844268798828
INFO:root:current mean train loss 1326.4194761295907
INFO:root:current train perplexity2.847383737564087
INFO:root:current mean train loss 1326.936530895172
INFO:root:current train perplexity2.848928451538086
INFO:root:current mean train loss 1327.1133711616078
INFO:root:current train perplexity2.850498676300049
INFO:root:current mean train loss 1327.97108804643
INFO:root:current train perplexity2.8525328636169434
INFO:root:current mean train loss 1328.27443683141
INFO:root:current train perplexity2.8538150787353516
INFO:root:current mean train loss 1329.206655797934
INFO:root:current train perplexity2.85476016998291


100%|██████████| 1/1 [07:50<00:00, 470.82s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.99s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.99s/it]
INFO:root:eval mean loss: 3461.6624539578643
INFO:root:eval perplexity: 17.382007598876953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/69

 34%|███▍      | 69/200 [10:06:20<18:51:37, 518.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1297.4921451144749
INFO:root:current train perplexity2.777860403060913
INFO:root:current mean train loss 1300.1140200592752
INFO:root:current train perplexity2.7780706882476807
INFO:root:current mean train loss 1300.301097645479
INFO:root:current train perplexity2.7837440967559814
INFO:root:current mean train loss 1299.6754189768146
INFO:root:current train perplexity2.790755271911621
INFO:root:current mean train loss 1302.577127488993
INFO:root:current train perplexity2.7967169284820557
INFO:root:current mean train loss 1303.704055679428
INFO:root:current train perplexity2.8000571727752686
INFO:root:current mean train loss 1305.296322232201
INFO:root:current train perplexity2.8026177883148193
INFO:root:current mean train loss 1305.55543778968
INFO:root:current train perplexity2.804755210876465
INFO:root:current mean train loss 1307.1064570715669
INFO:root:current train perplexity2.807424545288086
INFO:root:current mean train loss 1307.8442649056392
INFO:root:current train perplexity2.8094284534454346
INFO:root:current mean train loss 1308.167043201959
INFO:root:current train perplexity2.812474250793457
INFO:root:current mean train loss 1309.4947532679848
INFO:root:current train perplexity2.8149232864379883
INFO:root:current mean train loss 1310.74755859375
INFO:root:current train perplexity2.8162786960601807
INFO:root:current mean train loss 1312.5059991508463
INFO:root:current train perplexity2.8191068172454834
INFO:root:current mean train loss 1314.3259287295134
INFO:root:current train perplexity2.820781946182251
INFO:root:current mean train loss 1315.042815773845
INFO:root:current train perplexity2.823003053665161
INFO:root:current mean train loss 1316.40745726955
INFO:root:current train perplexity2.8260109424591064
INFO:root:current mean train loss 1316.9187501515546
INFO:root:current train perplexity2.8275842666625977
INFO:root:current mean train loss 1317.7667506291316
INFO:root:current train perplexity2.828416109085083
INFO:root:current mean train loss 1318.633400071707
INFO:root:current train perplexity2.8301937580108643


100%|██████████| 1/1 [07:49<00:00, 469.55s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.55s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.33s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.33s/it]
INFO:root:eval mean loss: 3461.367525484469
INFO:root:eval perplexity: 17.377777099609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/70

 35%|███▌      | 70/200 [10:14:56<18:41:28, 517.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1291.5024592367452
INFO:root:current train perplexity2.781893014907837
INFO:root:current mean train loss 1292.8070772362764
INFO:root:current train perplexity2.7811105251312256
INFO:root:current mean train loss 1294.6320302362674
INFO:root:current train perplexity2.779014825820923
INFO:root:current mean train loss 1295.8959665960394
INFO:root:current train perplexity2.7781248092651367
INFO:root:current mean train loss 1297.254443958493
INFO:root:current train perplexity2.780088186264038
INFO:root:current mean train loss 1297.8566453088524
INFO:root:current train perplexity2.7839853763580322
INFO:root:current mean train loss 1297.6299285224284
INFO:root:current train perplexity2.7863473892211914
INFO:root:current mean train loss 1299.0286314448174
INFO:root:current train perplexity2.7890148162841797
INFO:root:current mean train loss 1300.284023289203
INFO:root:current train perplexity2.7914156913757324
INFO:root:current mean train loss 1301.5855944935304
INFO:root:current train perplexity2.793956756591797
INFO:root:current mean train loss 1302.3065195635331
INFO:root:current train perplexity2.7963685989379883
INFO:root:current mean train loss 1303.4712419181035
INFO:root:current train perplexity2.79825496673584
INFO:root:current mean train loss 1304.9090189789506
INFO:root:current train perplexity2.8005194664001465
INFO:root:current mean train loss 1305.3908571884563
INFO:root:current train perplexity2.801912784576416
INFO:root:current mean train loss 1306.6957315069626
INFO:root:current train perplexity2.804591417312622
INFO:root:current mean train loss 1306.9173321252802
INFO:root:current train perplexity2.8063933849334717
INFO:root:current mean train loss 1307.7230205818207
INFO:root:current train perplexity2.8080873489379883
INFO:root:current mean train loss 1308.3755526257466
INFO:root:current train perplexity2.80832576751709
INFO:root:current mean train loss 1309.3592264348076
INFO:root:current train perplexity2.8092970848083496


100%|██████████| 1/1 [07:50<00:00, 470.58s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.92s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.92s/it]
INFO:root:eval mean loss: 3484.4050124343094
INFO:root:eval perplexity: 17.711166381835938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/71

 36%|███▌      | 71/200 [10:23:33<18:32:52, 517.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1272.4020385742188
INFO:root:current train perplexity2.6581828594207764
INFO:root:current mean train loss 1277.1749520931603
INFO:root:current train perplexity2.742650270462036
INFO:root:current mean train loss 1275.8527328343068
INFO:root:current train perplexity2.74466872215271
INFO:root:current mean train loss 1279.495700811249
INFO:root:current train perplexity2.7498831748962402
INFO:root:current mean train loss 1281.1171908073238
INFO:root:current train perplexity2.752626419067383
INFO:root:current mean train loss 1283.7712247946517
INFO:root:current train perplexity2.756429433822632
INFO:root:current mean train loss 1285.2235131594214
INFO:root:current train perplexity2.7613489627838135
INFO:root:current mean train loss 1287.2781389360725
INFO:root:current train perplexity2.763845920562744
INFO:root:current mean train loss 1288.714866770704
INFO:root:current train perplexity2.767049551010132
INFO:root:current mean train loss 1289.9329863626174
INFO:root:current train perplexity2.7693169116973877
INFO:root:current mean train loss 1291.446561354529
INFO:root:current train perplexity2.770759105682373
INFO:root:current mean train loss 1292.6194161109856
INFO:root:current train perplexity2.772815465927124
INFO:root:current mean train loss 1293.2810959444314
INFO:root:current train perplexity2.7748382091522217
INFO:root:current mean train loss 1293.8701388722723
INFO:root:current train perplexity2.7764060497283936
INFO:root:current mean train loss 1295.119737345667
INFO:root:current train perplexity2.778622627258301
INFO:root:current mean train loss 1296.3696822410877
INFO:root:current train perplexity2.7807083129882812
INFO:root:current mean train loss 1296.2527542589314
INFO:root:current train perplexity2.7812201976776123
INFO:root:current mean train loss 1297.286368223595
INFO:root:current train perplexity2.7838551998138428
INFO:root:current mean train loss 1298.000326602298
INFO:root:current train perplexity2.7851767539978027
INFO:root:current mean train loss 1298.7889571455069
INFO:root:current train perplexity2.786214828491211


100%|██████████| 1/1 [07:56<00:00, 476.14s/it][A
100%|██████████| 1/1 [07:56<00:00, 476.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.53s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.53s/it]
INFO:root:eval mean loss: 3490.039849908502
INFO:root:eval perplexity: 17.79368019104004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/72

 36%|███▌      | 72/200 [10:32:16<18:27:35, 519.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1256.6075545601223
INFO:root:current train perplexity2.6969029903411865
INFO:root:current mean train loss 1266.5643945709476
INFO:root:current train perplexity2.714684247970581
INFO:root:current mean train loss 1267.7672436632918
INFO:root:current train perplexity2.7202560901641846
INFO:root:current mean train loss 1268.5241223031153
INFO:root:current train perplexity2.722593069076538
INFO:root:current mean train loss 1271.9991163610002
INFO:root:current train perplexity2.7298617362976074
INFO:root:current mean train loss 1275.2600349732613
INFO:root:current train perplexity2.7335169315338135
INFO:root:current mean train loss 1278.1196831814932
INFO:root:current train perplexity2.736837863922119
INFO:root:current mean train loss 1280.1119984142679
INFO:root:current train perplexity2.742119312286377
INFO:root:current mean train loss 1280.6139527108805
INFO:root:current train perplexity2.7460055351257324
INFO:root:current mean train loss 1281.5063089058692
INFO:root:current train perplexity2.7466819286346436
INFO:root:current mean train loss 1282.207272765457
INFO:root:current train perplexity2.7501540184020996
INFO:root:current mean train loss 1283.6726814467038
INFO:root:current train perplexity2.7517054080963135
INFO:root:current mean train loss 1284.044360830661
INFO:root:current train perplexity2.7526118755340576
INFO:root:current mean train loss 1284.7334102477796
INFO:root:current train perplexity2.7541821002960205
INFO:root:current mean train loss 1285.3951339668065
INFO:root:current train perplexity2.75703501701355
INFO:root:current mean train loss 1286.2184071932195
INFO:root:current train perplexity2.759416103363037
INFO:root:current mean train loss 1287.1416326253707
INFO:root:current train perplexity2.7615606784820557
INFO:root:current mean train loss 1287.541918647753
INFO:root:current train perplexity2.7625582218170166
INFO:root:current mean train loss 1288.3113846938425
INFO:root:current train perplexity2.764312982559204
INFO:root:current mean train loss 1289.257120577792
INFO:root:current train perplexity2.7663984298706055


100%|██████████| 1/1 [07:48<00:00, 468.98s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.98s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.88s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.88s/it]
INFO:root:eval mean loss: 3487.57556408948
INFO:root:eval perplexity: 17.757545471191406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/73

 36%|███▋      | 73/200 [10:40:52<18:16:53, 518.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1244.902182006836
INFO:root:current train perplexity2.7012531757354736
INFO:root:current mean train loss 1257.0713126046317
INFO:root:current train perplexity2.699253559112549
INFO:root:current mean train loss 1262.3014877319335
INFO:root:current train perplexity2.704766035079956
INFO:root:current mean train loss 1265.3035256778492
INFO:root:current train perplexity2.7119300365448
INFO:root:current mean train loss 1266.5061190518466
INFO:root:current train perplexity2.7150909900665283
INFO:root:current mean train loss 1268.070181387442
INFO:root:current train perplexity2.716778039932251
INFO:root:current mean train loss 1271.6694591522216
INFO:root:current train perplexity2.722820520401001
INFO:root:current mean train loss 1271.8948291675465
INFO:root:current train perplexity2.725403308868408
INFO:root:current mean train loss 1273.941045415969
INFO:root:current train perplexity2.730306386947632
INFO:root:current mean train loss 1275.2503811450715
INFO:root:current train perplexity2.7326035499572754
INFO:root:current mean train loss 1275.8246022151068
INFO:root:current train perplexity2.733320713043213
INFO:root:current mean train loss 1276.399814860026
INFO:root:current train perplexity2.7355735301971436
INFO:root:current mean train loss 1277.326030312815
INFO:root:current train perplexity2.7376677989959717
INFO:root:current mean train loss 1277.4078949430095
INFO:root:current train perplexity2.7382750511169434
INFO:root:current mean train loss 1277.4301575554741
INFO:root:current train perplexity2.7395241260528564
INFO:root:current mean train loss 1278.1702938723874
INFO:root:current train perplexity2.7408318519592285
INFO:root:current mean train loss 1278.8326333394864
INFO:root:current train perplexity2.7429635524749756
INFO:root:current mean train loss 1279.7092692057292
INFO:root:current train perplexity2.7450246810913086
INFO:root:current mean train loss 1280.682302591075
INFO:root:current train perplexity2.7455506324768066
INFO:root:current mean train loss 1281.288879520377
INFO:root:current train perplexity2.7478127479553223


100%|██████████| 1/1 [07:49<00:00, 469.09s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.09s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.08s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.08s/it]
INFO:root:eval mean loss: 3513.040874859234
INFO:root:eval perplexity: 18.13449478149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/74

 37%|███▋      | 74/200 [10:49:27<18:06:22, 517.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1249.0935743900768
INFO:root:current train perplexity2.6735010147094727
INFO:root:current mean train loss 1258.5085558071257
INFO:root:current train perplexity2.688297986984253
INFO:root:current mean train loss 1255.5772781075207
INFO:root:current train perplexity2.696718215942383
INFO:root:current mean train loss 1259.5246506805847
INFO:root:current train perplexity2.7044553756713867
INFO:root:current mean train loss 1259.5975838625718
INFO:root:current train perplexity2.70444393157959
INFO:root:current mean train loss 1261.1155865599053
INFO:root:current train perplexity2.708296537399292
INFO:root:current mean train loss 1262.908568778539
INFO:root:current train perplexity2.7120227813720703
INFO:root:current mean train loss 1263.9915845661844
INFO:root:current train perplexity2.7146036624908447
INFO:root:current mean train loss 1264.4806935189067
INFO:root:current train perplexity2.713238477706909
INFO:root:current mean train loss 1264.8806281174486
INFO:root:current train perplexity2.7158970832824707
INFO:root:current mean train loss 1266.133021763393
INFO:root:current train perplexity2.7168197631835938
INFO:root:current mean train loss 1267.1725513982485
INFO:root:current train perplexity2.718079090118408
INFO:root:current mean train loss 1267.6322928514383
INFO:root:current train perplexity2.7188735008239746
INFO:root:current mean train loss 1268.5531672433447
INFO:root:current train perplexity2.72141170501709
INFO:root:current mean train loss 1269.2546044050544
INFO:root:current train perplexity2.722796678543091
INFO:root:current mean train loss 1269.7412559396575
INFO:root:current train perplexity2.723414421081543
INFO:root:current mean train loss 1270.868293868107
INFO:root:current train perplexity2.7249419689178467
INFO:root:current mean train loss 1270.9122412415072
INFO:root:current train perplexity2.7250216007232666
INFO:root:current mean train loss 1271.5961833865526
INFO:root:current train perplexity2.7269060611724854
INFO:root:current mean train loss 1272.070625753505
INFO:root:current train perplexity2.728198528289795


100%|██████████| 1/1 [07:46<00:00, 466.91s/it][A
100%|██████████| 1/1 [07:46<00:00, 466.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.50s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.50s/it]
INFO:root:eval mean loss: 3513.352761208474
INFO:root:eval perplexity: 18.139169692993164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/75

 38%|███▊      | 75/200 [10:58:01<17:55:23, 516.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1249.732484559755
INFO:root:current train perplexity2.698777198791504
INFO:root:current mean train loss 1251.7862275222253
INFO:root:current train perplexity2.6812264919281006
INFO:root:current mean train loss 1249.9326951521157
INFO:root:current train perplexity2.6768033504486084
INFO:root:current mean train loss 1252.4854638410761
INFO:root:current train perplexity2.6803834438323975
INFO:root:current mean train loss 1254.2204332311446
INFO:root:current train perplexity2.680187225341797
INFO:root:current mean train loss 1255.0425827611198
INFO:root:current train perplexity2.6809585094451904
INFO:root:current mean train loss 1255.9662142337606
INFO:root:current train perplexity2.6838462352752686
INFO:root:current mean train loss 1256.0338950144844
INFO:root:current train perplexity2.685835599899292
INFO:root:current mean train loss 1257.0203935636264
INFO:root:current train perplexity2.6903975009918213
INFO:root:current mean train loss 1257.2508143869513
INFO:root:current train perplexity2.692065477371216
INFO:root:current mean train loss 1257.9462033632303
INFO:root:current train perplexity2.693275213241577
INFO:root:current mean train loss 1258.6980375585604
INFO:root:current train perplexity2.6954424381256104
INFO:root:current mean train loss 1259.5724340705333
INFO:root:current train perplexity2.696768283843994
INFO:root:current mean train loss 1260.379630676003
INFO:root:current train perplexity2.6985373497009277
INFO:root:current mean train loss 1260.6799381830638
INFO:root:current train perplexity2.700188159942627
INFO:root:current mean train loss 1261.3334607290344
INFO:root:current train perplexity2.7020139694213867
INFO:root:current mean train loss 1261.3087220186258
INFO:root:current train perplexity2.7038888931274414
INFO:root:current mean train loss 1262.1291439224124
INFO:root:current train perplexity2.7057454586029053
INFO:root:current mean train loss 1262.5471322335454
INFO:root:current train perplexity2.707423210144043
INFO:root:current mean train loss 1263.2180574643214
INFO:root:current train perplexity2.7091543674468994


100%|██████████| 1/1 [07:50<00:00, 470.33s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.33s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.44s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.44s/it]
INFO:root:eval mean loss: 3526.121973536036
INFO:root:eval perplexity: 18.331241607666016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/76

 38%|███▊      | 76/200 [11:06:38<17:47:12, 516.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1232.8131063272665
INFO:root:current train perplexity2.6518988609313965
INFO:root:current mean train loss 1237.5052650012271
INFO:root:current train perplexity2.6583526134490967
INFO:root:current mean train loss 1239.5765884242107
INFO:root:current train perplexity2.661010503768921
INFO:root:current mean train loss 1241.2092038518022
INFO:root:current train perplexity2.6624503135681152
INFO:root:current mean train loss 1240.0727051775714
INFO:root:current train perplexity2.666529417037964
INFO:root:current mean train loss 1241.322203866923
INFO:root:current train perplexity2.669217586517334
INFO:root:current mean train loss 1243.550793616023
INFO:root:current train perplexity2.6721887588500977
INFO:root:current mean train loss 1243.429207860896
INFO:root:current train perplexity2.671926259994507
INFO:root:current mean train loss 1244.7827649871108
INFO:root:current train perplexity2.673696756362915
INFO:root:current mean train loss 1246.4003755971714
INFO:root:current train perplexity2.676701784133911
INFO:root:current mean train loss 1247.6211167990232
INFO:root:current train perplexity2.6765871047973633
INFO:root:current mean train loss 1248.73435173387
INFO:root:current train perplexity2.677799701690674
INFO:root:current mean train loss 1249.5652266782351
INFO:root:current train perplexity2.680100202560425
INFO:root:current mean train loss 1250.0692746829507
INFO:root:current train perplexity2.6813888549804688
INFO:root:current mean train loss 1251.3226718710703
INFO:root:current train perplexity2.6838173866271973
INFO:root:current mean train loss 1252.157944253074
INFO:root:current train perplexity2.6845223903656006
INFO:root:current mean train loss 1253.2366661825242
INFO:root:current train perplexity2.6864137649536133
INFO:root:current mean train loss 1253.8765379768854
INFO:root:current train perplexity2.6874215602874756
INFO:root:current mean train loss 1254.3684907022573
INFO:root:current train perplexity2.6897494792938232


100%|██████████| 1/1 [07:48<00:00, 468.17s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:43<00:00, 43.93s/it][A
100%|██████████| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 3540.853241425019
INFO:root:eval perplexity: 18.555347442626953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/77

 38%|███▊      | 77/200 [11:15:12<17:37:13, 515.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1221.0056762695312
INFO:root:current train perplexity2.5999622344970703
INFO:root:current mean train loss 1229.8455686216
INFO:root:current train perplexity2.62675404548645
INFO:root:current mean train loss 1234.1402599628154
INFO:root:current train perplexity2.6356983184814453
INFO:root:current mean train loss 1233.8845523982852
INFO:root:current train perplexity2.6351611614227295
INFO:root:current mean train loss 1235.3316488826977
INFO:root:current train perplexity2.636868715286255
INFO:root:current mean train loss 1237.1647704116942
INFO:root:current train perplexity2.643221855163574
INFO:root:current mean train loss 1238.5873766447369
INFO:root:current train perplexity2.645603895187378
INFO:root:current mean train loss 1239.2916882186287
INFO:root:current train perplexity2.6464591026306152
INFO:root:current mean train loss 1239.6166138601775
INFO:root:current train perplexity2.6493260860443115
INFO:root:current mean train loss 1239.0331966719439
INFO:root:current train perplexity2.650115489959717
INFO:root:current mean train loss 1239.925308227539
INFO:root:current train perplexity2.6543490886688232
INFO:root:current mean train loss 1240.8713067120163
INFO:root:current train perplexity2.6572279930114746
INFO:root:current mean train loss 1241.0437034960614
INFO:root:current train perplexity2.658275842666626
INFO:root:current mean train loss 1242.1985244051032
INFO:root:current train perplexity2.6598517894744873
INFO:root:current mean train loss 1242.788399956443
INFO:root:current train perplexity2.661759614944458
INFO:root:current mean train loss 1243.1366278448536
INFO:root:current train perplexity2.66440486907959
INFO:root:current mean train loss 1243.8209881379237
INFO:root:current train perplexity2.6663284301757812
INFO:root:current mean train loss 1244.424524526127
INFO:root:current train perplexity2.668783187866211
INFO:root:current mean train loss 1245.0222909302838
INFO:root:current train perplexity2.6705617904663086
INFO:root:current mean train loss 1245.7481597964368
INFO:root:current train perplexity2.6716270446777344


100%|██████████| 1/1 [07:44<00:00, 464.54s/it][A
100%|██████████| 1/1 [07:44<00:00, 464.54s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.08s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.08s/it]
INFO:root:eval mean loss: 3546.7243498381195
INFO:root:eval perplexity: 18.645429611206055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/78

 39%|███▉      | 78/200 [11:23:44<17:26:12, 514.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1227.6824169921874
INFO:root:current train perplexity2.6185388565063477
INFO:root:current mean train loss 1219.74105859375
INFO:root:current train perplexity2.6077659130096436
INFO:root:current mean train loss 1223.974263780382
INFO:root:current train perplexity2.6161653995513916
INFO:root:current mean train loss 1226.6900345552885
INFO:root:current train perplexity2.6174070835113525
INFO:root:current mean train loss 1228.5052596507353
INFO:root:current train perplexity2.6241490840911865
INFO:root:current mean train loss 1229.1728603980655
INFO:root:current train perplexity2.6270062923431396
INFO:root:current mean train loss 1229.63076328125
INFO:root:current train perplexity2.629459857940674
INFO:root:current mean train loss 1229.9998641231143
INFO:root:current train perplexity2.6319942474365234
INFO:root:current mean train loss 1229.7990880977745
INFO:root:current train perplexity2.6342976093292236
INFO:root:current mean train loss 1230.215475876267
INFO:root:current train perplexity2.6373438835144043
INFO:root:current mean train loss 1230.9139026772104
INFO:root:current train perplexity2.640868902206421
INFO:root:current mean train loss 1231.6767180989584
INFO:root:current train perplexity2.64251708984375
INFO:root:current mean train loss 1232.2858254942603
INFO:root:current train perplexity2.643007516860962
INFO:root:current mean train loss 1233.0433391988502
INFO:root:current train perplexity2.643887996673584
INFO:root:current mean train loss 1233.9012452885142
INFO:root:current train perplexity2.6459457874298096
INFO:root:current mean train loss 1234.9370271676485
INFO:root:current train perplexity2.6482841968536377
INFO:root:current mean train loss 1235.6279602614184
INFO:root:current train perplexity2.6503567695617676
INFO:root:current mean train loss 1236.2119651551177
INFO:root:current train perplexity2.6508023738861084
INFO:root:current mean train loss 1236.5943244327912
INFO:root:current train perplexity2.6517269611358643
INFO:root:current mean train loss 1237.262305575284
INFO:root:current train perplexity2.6535608768463135


100%|██████████| 1/1 [08:04<00:00, 484.71s/it][A
100%|██████████| 1/1 [08:04<00:00, 484.71s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.57s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.57s/it]
INFO:root:eval mean loss: 3563.9467113597975
INFO:root:eval perplexity: 18.912199020385742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/79

 40%|███▉      | 79/200 [11:32:35<17:27:50, 519.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1204.3741455078125
INFO:root:current train perplexity2.593916177749634
INFO:root:current mean train loss 1209.8711762764085
INFO:root:current train perplexity2.5984785556793213
INFO:root:current mean train loss 1212.2055648929816
INFO:root:current train perplexity2.598999500274658
INFO:root:current mean train loss 1215.1329581277412
INFO:root:current train perplexity2.601839303970337
INFO:root:current mean train loss 1215.6113554665405
INFO:root:current train perplexity2.606590509414673
INFO:root:current mean train loss 1217.1278805891086
INFO:root:current train perplexity2.6088521480560303
INFO:root:current mean train loss 1218.3830440913405
INFO:root:current train perplexity2.6113507747650146
INFO:root:current mean train loss 1220.0969414312563
INFO:root:current train perplexity2.615208625793457
INFO:root:current mean train loss 1220.5268978019226
INFO:root:current train perplexity2.6185672283172607
INFO:root:current mean train loss 1222.2725257306595
INFO:root:current train perplexity2.621782064437866
INFO:root:current mean train loss 1224.0041302408229
INFO:root:current train perplexity2.6251862049102783
INFO:root:current mean train loss 1224.7570353973977
INFO:root:current train perplexity2.627110719680786
INFO:root:current mean train loss 1225.495525955962
INFO:root:current train perplexity2.629715919494629
INFO:root:current mean train loss 1225.7967069941378
INFO:root:current train perplexity2.630096912384033
INFO:root:current mean train loss 1226.7437381823747
INFO:root:current train perplexity2.6307482719421387
INFO:root:current mean train loss 1227.2061041647667
INFO:root:current train perplexity2.631782293319702
INFO:root:current mean train loss 1227.9940852398704
INFO:root:current train perplexity2.634340524673462
INFO:root:current mean train loss 1228.474241201968
INFO:root:current train perplexity2.634917736053467
INFO:root:current mean train loss 1229.1355209764777
INFO:root:current train perplexity2.6370606422424316
INFO:root:current mean train loss 1229.3531618599543
INFO:root:current train perplexity2.6377687454223633


100%|██████████| 1/1 [07:49<00:00, 469.49s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.49s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.08s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.08s/it]
INFO:root:eval mean loss: 3561.112106735642
INFO:root:eval perplexity: 18.868030548095703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/80

 40%|████      | 80/200 [11:41:11<17:16:51, 518.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1204.5230009434588
INFO:root:current train perplexity2.5876564979553223
INFO:root:current mean train loss 1201.4463911716293
INFO:root:current train perplexity2.5794200897216797
INFO:root:current mean train loss 1203.899007789877
INFO:root:current train perplexity2.586427688598633
INFO:root:current mean train loss 1203.891822241143
INFO:root:current train perplexity2.5904335975646973
INFO:root:current mean train loss 1207.1497800074892
INFO:root:current train perplexity2.5963268280029297
INFO:root:current mean train loss 1209.266924098907
INFO:root:current train perplexity2.5984137058258057
INFO:root:current mean train loss 1210.1478645660447
INFO:root:current train perplexity2.598832368850708
INFO:root:current mean train loss 1212.215188731318
INFO:root:current train perplexity2.6005234718322754
INFO:root:current mean train loss 1212.931121222215
INFO:root:current train perplexity2.6028780937194824
INFO:root:current mean train loss 1214.0976755979536
INFO:root:current train perplexity2.6071362495422363
INFO:root:current mean train loss 1214.1386897417597
INFO:root:current train perplexity2.609297752380371
INFO:root:current mean train loss 1215.2612191991009
INFO:root:current train perplexity2.6110503673553467
INFO:root:current mean train loss 1216.3344420174742
INFO:root:current train perplexity2.611701011657715
INFO:root:current mean train loss 1216.9880410616147
INFO:root:current train perplexity2.6121208667755127
INFO:root:current mean train loss 1218.0501320768988
INFO:root:current train perplexity2.6143763065338135
INFO:root:current mean train loss 1218.768614665601
INFO:root:current train perplexity2.6153507232666016
INFO:root:current mean train loss 1219.1155228111813
INFO:root:current train perplexity2.617353916168213
INFO:root:current mean train loss 1219.5719425793463
INFO:root:current train perplexity2.618989944458008
INFO:root:current mean train loss 1220.3280917080965
INFO:root:current train perplexity2.6206650733947754
INFO:root:current mean train loss 1221.1190072761135
INFO:root:current train perplexity2.6208770275115967


100%|██████████| 1/1 [07:49<00:00, 469.99s/it][A
100%|██████████| 1/1 [07:49<00:00, 469.99s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.57s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.57s/it]
INFO:root:eval mean loss: 3579.2501774235175
INFO:root:eval perplexity: 19.152446746826172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/81

 40%|████      | 81/200 [11:49:48<17:07:11, 517.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1195.2168739720394
INFO:root:current train perplexity2.5651915073394775
INFO:root:current mean train loss 1196.424173528498
INFO:root:current train perplexity2.5671491622924805
INFO:root:current mean train loss 1197.7730650970902
INFO:root:current train perplexity2.574676513671875
INFO:root:current mean train loss 1196.8379156234416
INFO:root:current train perplexity2.5774786472320557
INFO:root:current mean train loss 1198.506548200335
INFO:root:current train perplexity2.579802989959717
INFO:root:current mean train loss 1200.1620491875542
INFO:root:current train perplexity2.5862677097320557
INFO:root:current mean train loss 1201.8210541313217
INFO:root:current train perplexity2.586303234100342
INFO:root:current mean train loss 1203.1548137861428
INFO:root:current train perplexity2.5892627239227295
INFO:root:current mean train loss 1204.2228612333672
INFO:root:current train perplexity2.5899128913879395
INFO:root:current mean train loss 1205.35542034712
INFO:root:current train perplexity2.590791702270508
INFO:root:current mean train loss 1206.0839924298255
INFO:root:current train perplexity2.592472791671753
INFO:root:current mean train loss 1206.9874378645502
INFO:root:current train perplexity2.5933008193969727
INFO:root:current mean train loss 1208.0276152512122
INFO:root:current train perplexity2.593703269958496
INFO:root:current mean train loss 1209.136463342711
INFO:root:current train perplexity2.5943686962127686
INFO:root:current mean train loss 1210.0875021668307
INFO:root:current train perplexity2.5963761806488037
INFO:root:current mean train loss 1210.1981141530923
INFO:root:current train perplexity2.5977697372436523
INFO:root:current mean train loss 1210.895523326209
INFO:root:current train perplexity2.5992848873138428
INFO:root:current mean train loss 1211.3721011737446
INFO:root:current train perplexity2.600949287414551
INFO:root:current mean train loss 1212.3464579958397
INFO:root:current train perplexity2.60213303565979
INFO:root:current mean train loss 1212.874621186662
INFO:root:current train perplexity2.603766918182373


100%|██████████| 1/1 [07:50<00:00, 470.04s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.04s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.95s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.95s/it]
INFO:root:eval mean loss: 3594.4586991776932
INFO:root:eval perplexity: 19.394229888916016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/82

 41%|████      | 82/200 [11:58:25<16:58:05, 517.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1186.7010537424396
INFO:root:current train perplexity2.5498104095458984
INFO:root:current mean train loss 1187.0068353050112
INFO:root:current train perplexity2.5634765625
INFO:root:current mean train loss 1188.9618503192992
INFO:root:current train perplexity2.5666048526763916
INFO:root:current mean train loss 1191.0090397259662
INFO:root:current train perplexity2.5666587352752686
INFO:root:current mean train loss 1191.0512962728194
INFO:root:current train perplexity2.566026449203491
INFO:root:current mean train loss 1192.4287693995047
INFO:root:current train perplexity2.5670299530029297
INFO:root:current mean train loss 1192.7793498954388
INFO:root:current train perplexity2.5703816413879395
INFO:root:current mean train loss 1194.4006461568017
INFO:root:current train perplexity2.5706002712249756
INFO:root:current mean train loss 1195.7932785051266
INFO:root:current train perplexity2.5708234310150146
INFO:root:current mean train loss 1197.0429093744099
INFO:root:current train perplexity2.571326494216919
INFO:root:current mean train loss 1197.9313866562072
INFO:root:current train perplexity2.573617458343506
INFO:root:current mean train loss 1198.703828771676
INFO:root:current train perplexity2.576202869415283
INFO:root:current mean train loss 1200.0716013661302
INFO:root:current train perplexity2.577622890472412
INFO:root:current mean train loss 1201.3267342922761
INFO:root:current train perplexity2.580228567123413
INFO:root:current mean train loss 1201.9591899240727
INFO:root:current train perplexity2.5815844535827637
INFO:root:current mean train loss 1203.064994050509
INFO:root:current train perplexity2.5830605030059814
INFO:root:current mean train loss 1203.8249384817539
INFO:root:current train perplexity2.5850377082824707
INFO:root:current mean train loss 1204.3866862069942
INFO:root:current train perplexity2.5867741107940674
INFO:root:current mean train loss 1205.0302990380885
INFO:root:current train perplexity2.58817195892334


100%|██████████| 1/1 [07:50<00:00, 470.62s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.62s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.49s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.49s/it]
INFO:root:eval mean loss: 3591.9612212309967
INFO:root:eval perplexity: 19.354320526123047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/83

 42%|████▏     | 83/200 [12:07:02<16:49:12, 517.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1175.3622314453125
INFO:root:current train perplexity2.513580083847046
INFO:root:current mean train loss 1187.199648215554
INFO:root:current train perplexity2.5383872985839844
INFO:root:current mean train loss 1184.6304553803943
INFO:root:current train perplexity2.5427207946777344
INFO:root:current mean train loss 1182.8837567729336
INFO:root:current train perplexity2.543517589569092
INFO:root:current mean train loss 1184.8553824075839
INFO:root:current train perplexity2.5449440479278564
INFO:root:current mean train loss 1184.8309563131893
INFO:root:current train perplexity2.5441253185272217
INFO:root:current mean train loss 1186.3405673668033
INFO:root:current train perplexity2.547257423400879
INFO:root:current mean train loss 1187.6223330215669
INFO:root:current train perplexity2.5502612590789795
INFO:root:current mean train loss 1187.9650993441358
INFO:root:current train perplexity2.5528292655944824
INFO:root:current mean train loss 1189.1706940032623
INFO:root:current train perplexity2.555157423019409
INFO:root:current mean train loss 1190.1381814182396
INFO:root:current train perplexity2.5563268661499023
INFO:root:current mean train loss 1190.8696700362473
INFO:root:current train perplexity2.558602809906006
INFO:root:current mean train loss 1191.6361051701317
INFO:root:current train perplexity2.5603697299957275
INFO:root:current mean train loss 1192.9623403767594
INFO:root:current train perplexity2.561748743057251
INFO:root:current mean train loss 1193.490864119293
INFO:root:current train perplexity2.5640499591827393
INFO:root:current mean train loss 1194.3414791688224
INFO:root:current train perplexity2.566215991973877
INFO:root:current mean train loss 1194.7674697781201
INFO:root:current train perplexity2.5669515132904053
INFO:root:current mean train loss 1195.7556760553728
INFO:root:current train perplexity2.5690882205963135
INFO:root:current mean train loss 1196.2641049885617
INFO:root:current train perplexity2.5706887245178223
INFO:root:current mean train loss 1197.001281610459
INFO:root:current train perplexity2.5718421936035156


100%|██████████| 1/1 [07:51<00:00, 471.02s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.02s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.14s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.14s/it]
INFO:root:eval mean loss: 3606.666209911083
INFO:root:eval perplexity: 19.590503692626953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/84

 42%|████▏     | 84/200 [12:15:39<16:40:25, 517.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1172.2556333188656
INFO:root:current train perplexity2.518819570541382
INFO:root:current mean train loss 1169.449254313792
INFO:root:current train perplexity2.51849365234375
INFO:root:current mean train loss 1176.4282570725495
INFO:root:current train perplexity2.5348598957061768
INFO:root:current mean train loss 1177.949086973791
INFO:root:current train perplexity2.53406023979187
INFO:root:current mean train loss 1177.8337422355276
INFO:root:current train perplexity2.5367674827575684
INFO:root:current mean train loss 1179.4395721840904
INFO:root:current train perplexity2.539292097091675
INFO:root:current mean train loss 1180.9140284293387
INFO:root:current train perplexity2.540635585784912
INFO:root:current mean train loss 1182.4710102317422
INFO:root:current train perplexity2.541062831878662
INFO:root:current mean train loss 1182.3586722469677
INFO:root:current train perplexity2.541170835494995
INFO:root:current mean train loss 1182.8793275045089
INFO:root:current train perplexity2.544473171234131
INFO:root:current mean train loss 1183.7676502736658
INFO:root:current train perplexity2.5445640087127686
INFO:root:current mean train loss 1184.54450497293
INFO:root:current train perplexity2.546412944793701
INFO:root:current mean train loss 1185.104392103772
INFO:root:current train perplexity2.5480175018310547
INFO:root:current mean train loss 1186.1898539240592
INFO:root:current train perplexity2.5492851734161377
INFO:root:current mean train loss 1187.1123677329242
INFO:root:current train perplexity2.5501708984375
INFO:root:current mean train loss 1187.6512577479075
INFO:root:current train perplexity2.5508384704589844
INFO:root:current mean train loss 1188.3911321132405
INFO:root:current train perplexity2.552313804626465
INFO:root:current mean train loss 1188.7330624790777
INFO:root:current train perplexity2.5540213584899902
INFO:root:current mean train loss 1188.9136438395851
INFO:root:current train perplexity2.55513334274292
INFO:root:current mean train loss 1189.7547548508855
INFO:root:current train perplexity2.5563580989837646


100%|██████████| 1/1 [07:50<00:00, 470.19s/it][A
100%|██████████| 1/1 [07:50<00:00, 470.19s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.42s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.42s/it]
INFO:root:eval mean loss: 3623.77038977454
INFO:root:eval perplexity: 19.868867874145508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/85

 42%|████▎     | 85/200 [12:24:17<16:31:57, 517.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1160.819285999645
INFO:root:current train perplexity2.5003774166107178
INFO:root:current mean train loss 1162.8726925320095
INFO:root:current train perplexity2.503286838531494
INFO:root:current mean train loss 1165.9930880186987
INFO:root:current train perplexity2.508146286010742
INFO:root:current mean train loss 1168.4234700757404
INFO:root:current train perplexity2.5120575428009033
INFO:root:current mean train loss 1169.5740139248135
INFO:root:current train perplexity2.5196008682250977
INFO:root:current mean train loss 1170.125052283792
INFO:root:current train perplexity2.520449161529541
INFO:root:current mean train loss 1171.780126915215
INFO:root:current train perplexity2.5220236778259277
INFO:root:current mean train loss 1172.607390537057
INFO:root:current train perplexity2.5241212844848633
INFO:root:current mean train loss 1174.126404242493
INFO:root:current train perplexity2.5269579887390137
INFO:root:current mean train loss 1174.9708319195247
INFO:root:current train perplexity2.5268218517303467
INFO:root:current mean train loss 1175.6315526268036
INFO:root:current train perplexity2.530156135559082
INFO:root:current mean train loss 1176.7137934544703
INFO:root:current train perplexity2.5314431190490723
INFO:root:current mean train loss 1177.8595595773777
INFO:root:current train perplexity2.5334064960479736
INFO:root:current mean train loss 1179.067179271153
INFO:root:current train perplexity2.535465955734253
INFO:root:current mean train loss 1179.7119074686743
INFO:root:current train perplexity2.5371499061584473
INFO:root:current mean train loss 1180.255234239015
INFO:root:current train perplexity2.538846015930176
INFO:root:current mean train loss 1180.628422572375
INFO:root:current train perplexity2.539917230606079
INFO:root:current mean train loss 1181.6017240471797
INFO:root:current train perplexity2.5406553745269775
INFO:root:current mean train loss 1182.1881838982638
INFO:root:current train perplexity2.5421042442321777
INFO:root:current mean train loss 1182.5766175195513
INFO:root:current train perplexity2.542792558670044


100%|██████████| 1/1 [07:53<00:00, 473.26s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.26s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.56s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.56s/it]
INFO:root:eval mean loss: 3624.3894394883164
INFO:root:eval perplexity: 19.8790225982666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/86

 43%|████▎     | 86/200 [12:32:57<16:24:40, 518.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1154.2460377177254
INFO:root:current train perplexity2.487529754638672
INFO:root:current mean train loss 1158.572618188325
INFO:root:current train perplexity2.4950528144836426
INFO:root:current mean train loss 1163.9251914773408
INFO:root:current train perplexity2.5020077228546143
INFO:root:current mean train loss 1164.3486057609073
INFO:root:current train perplexity2.507429599761963
INFO:root:current mean train loss 1165.8514192461191
INFO:root:current train perplexity2.5084726810455322
INFO:root:current mean train loss 1165.4868386008523
INFO:root:current train perplexity2.509256601333618
INFO:root:current mean train loss 1165.7164445147032
INFO:root:current train perplexity2.5104708671569824
INFO:root:current mean train loss 1166.9103047875944
INFO:root:current train perplexity2.512671947479248
INFO:root:current mean train loss 1167.6331941646704
INFO:root:current train perplexity2.5140950679779053
INFO:root:current mean train loss 1168.3236870264536
INFO:root:current train perplexity2.515058994293213
INFO:root:current mean train loss 1169.534827891214
INFO:root:current train perplexity2.517609119415283
INFO:root:current mean train loss 1170.248715580588
INFO:root:current train perplexity2.5201873779296875
INFO:root:current mean train loss 1171.3442434118817
INFO:root:current train perplexity2.520550489425659
INFO:root:current mean train loss 1171.796963705025
INFO:root:current train perplexity2.5221784114837646
INFO:root:current mean train loss 1172.7663768060725
INFO:root:current train perplexity2.5236785411834717
INFO:root:current mean train loss 1173.912057137352
INFO:root:current train perplexity2.5248422622680664
INFO:root:current mean train loss 1174.5848082944904
INFO:root:current train perplexity2.5260424613952637
INFO:root:current mean train loss 1175.2486794085614
INFO:root:current train perplexity2.5268614292144775
INFO:root:current mean train loss 1175.8654317471496
INFO:root:current train perplexity2.5280284881591797
INFO:root:current mean train loss 1176.205031375992
INFO:root:current train perplexity2.529552459716797


100%|██████████| 1/1 [07:48<00:00, 468.09s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.09s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.42s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.42s/it]
INFO:root:eval mean loss: 3643.453886014921
INFO:root:eval perplexity: 20.1940975189209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/87

 44%|████▎     | 87/200 [12:41:31<16:13:57, 517.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1144.7677080203325
INFO:root:current train perplexity2.486466884613037
INFO:root:current mean train loss 1152.0729143807057
INFO:root:current train perplexity2.4785408973693848
INFO:root:current mean train loss 1154.0955792982802
INFO:root:current train perplexity2.479482650756836
INFO:root:current mean train loss 1154.3266453011327
INFO:root:current train perplexity2.484631299972534
INFO:root:current mean train loss 1155.870150641916
INFO:root:current train perplexity2.489055871963501
INFO:root:current mean train loss 1157.3377725673795
INFO:root:current train perplexity2.4925875663757324
INFO:root:current mean train loss 1158.5376125999608
INFO:root:current train perplexity2.49421763420105
INFO:root:current mean train loss 1159.4187079186918
INFO:root:current train perplexity2.4954919815063477
INFO:root:current mean train loss 1160.1508109194815
INFO:root:current train perplexity2.4977333545684814
INFO:root:current mean train loss 1160.2019300090267
INFO:root:current train perplexity2.4987964630126953
INFO:root:current mean train loss 1161.569819802477
INFO:root:current train perplexity2.500495195388794
INFO:root:current mean train loss 1162.4419320435193
INFO:root:current train perplexity2.5028076171875
INFO:root:current mean train loss 1163.5016230193662
INFO:root:current train perplexity2.505610466003418
INFO:root:current mean train loss 1164.4216599152985
INFO:root:current train perplexity2.5067057609558105
INFO:root:current mean train loss 1165.4127419436898
INFO:root:current train perplexity2.508225679397583
INFO:root:current mean train loss 1166.292483872485
INFO:root:current train perplexity2.5089685916900635
INFO:root:current mean train loss 1167.208973535622
INFO:root:current train perplexity2.5103394985198975
INFO:root:current mean train loss 1167.8255458698766
INFO:root:current train perplexity2.5121665000915527
INFO:root:current mean train loss 1168.520711783022
INFO:root:current train perplexity2.5139899253845215
INFO:root:current mean train loss 1168.915497597597
INFO:root:current train perplexity2.514801025390625


100%|██████████| 1/1 [07:47<00:00, 467.28s/it][A
100%|██████████| 1/1 [07:47<00:00, 467.28s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it]
INFO:root:eval mean loss: 3648.6497439822633
INFO:root:eval perplexity: 20.28083610534668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/88

 44%|████▍     | 88/200 [12:50:06<16:03:44, 516.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1155.1009585731908
INFO:root:current train perplexity2.4710779190063477
INFO:root:current mean train loss 1153.7833176832933
INFO:root:current train perplexity2.4817326068878174
INFO:root:current mean train loss 1152.2402623063426
INFO:root:current train perplexity2.477504014968872
INFO:root:current mean train loss 1152.5470466710344
INFO:root:current train perplexity2.482041120529175
INFO:root:current mean train loss 1153.03834573765
INFO:root:current train perplexity2.483217477798462
INFO:root:current mean train loss 1153.9031642881762
INFO:root:current train perplexity2.483466386795044
INFO:root:current mean train loss 1153.8625815851226
INFO:root:current train perplexity2.4861350059509277
INFO:root:current mean train loss 1154.5731341667895
INFO:root:current train perplexity2.488048553466797
INFO:root:current mean train loss 1155.7719582669563
INFO:root:current train perplexity2.487977981567383
INFO:root:current mean train loss 1156.7951024041104
INFO:root:current train perplexity2.4900708198547363
INFO:root:current mean train loss 1157.4482459220712
INFO:root:current train perplexity2.4909470081329346
INFO:root:current mean train loss 1157.8782975839272
INFO:root:current train perplexity2.49249005317688
INFO:root:current mean train loss 1158.255424587883
INFO:root:current train perplexity2.493708848953247
INFO:root:current mean train loss 1158.2169945063984
INFO:root:current train perplexity2.4943509101867676
INFO:root:current mean train loss 1158.8577659989678
INFO:root:current train perplexity2.496493101119995
INFO:root:current mean train loss 1159.5447285140942
INFO:root:current train perplexity2.4981777667999268
INFO:root:current mean train loss 1160.3978148693884
INFO:root:current train perplexity2.499072551727295
INFO:root:current mean train loss 1160.9157177938391
INFO:root:current train perplexity2.499889850616455
INFO:root:current mean train loss 1161.6106621493136
INFO:root:current train perplexity2.500837564468384


100%|██████████| 1/1 [07:55<00:00, 475.95s/it][A
100%|██████████| 1/1 [07:55<00:00, 475.95s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.37s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.37s/it]
INFO:root:eval mean loss: 3655.022356096331
INFO:root:eval perplexity: 20.387727737426758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/89

 44%|████▍     | 89/200 [12:58:49<15:59:05, 518.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1144.5690612792969
INFO:root:current train perplexity2.4546759128570557
INFO:root:current mean train loss 1143.8066667829241
INFO:root:current train perplexity2.4698638916015625
INFO:root:current mean train loss 1147.300752459832
INFO:root:current train perplexity2.4577221870422363
INFO:root:current mean train loss 1148.0619021684695
INFO:root:current train perplexity2.4609103202819824
INFO:root:current mean train loss 1148.5433527381674
INFO:root:current train perplexity2.4639546871185303
INFO:root:current mean train loss 1147.1540911197662
INFO:root:current train perplexity2.468090057373047
INFO:root:current mean train loss 1147.4047949298536
INFO:root:current train perplexity2.4708995819091797
INFO:root:current mean train loss 1148.0701979733585
INFO:root:current train perplexity2.470766305923462
INFO:root:current mean train loss 1148.8800805002597
INFO:root:current train perplexity2.4712817668914795
INFO:root:current mean train loss 1149.4864661233466
INFO:root:current train perplexity2.474116325378418
INFO:root:current mean train loss 1149.7341830890641
INFO:root:current train perplexity2.4746627807617188
INFO:root:current mean train loss 1149.8526508139191
INFO:root:current train perplexity2.4769113063812256
INFO:root:current mean train loss 1150.9880649075649
INFO:root:current train perplexity2.478868007659912
INFO:root:current mean train loss 1151.1815538173769
INFO:root:current train perplexity2.4809181690216064
INFO:root:current mean train loss 1151.906826548806
INFO:root:current train perplexity2.482274293899536
INFO:root:current mean train loss 1152.3926910723328
INFO:root:current train perplexity2.4825968742370605
INFO:root:current mean train loss 1152.788812301295
INFO:root:current train perplexity2.4835944175720215
INFO:root:current mean train loss 1153.5431244752117
INFO:root:current train perplexity2.484905242919922
INFO:root:current mean train loss 1154.1515783179411
INFO:root:current train perplexity2.4853403568267822
INFO:root:current mean train loss 1154.739126676296
INFO:root:current train perplexity2.4866912364959717


100%|██████████| 1/1 [07:53<00:00, 473.29s/it][A
100%|██████████| 1/1 [07:53<00:00, 473.29s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.80s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.80s/it]
INFO:root:eval mean loss: 3667.3955994568787
INFO:root:eval perplexity: 20.596879959106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/90

 45%|████▌     | 90/200 [13:07:29<15:51:26, 518.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1125.6235099003234
INFO:root:current train perplexity2.451655149459839
INFO:root:current mean train loss 1131.2606560758843
INFO:root:current train perplexity2.443239450454712
INFO:root:current mean train loss 1134.9998736652224
INFO:root:current train perplexity2.44199800491333
INFO:root:current mean train loss 1133.579835097478
INFO:root:current train perplexity2.450413942337036
INFO:root:current mean train loss 1135.1923933407088
INFO:root:current train perplexity2.4518022537231445
INFO:root:current mean train loss 1137.6352249462798
INFO:root:current train perplexity2.4535610675811768
INFO:root:current mean train loss 1138.6424971005874
INFO:root:current train perplexity2.4554104804992676
INFO:root:current mean train loss 1138.9265157649872
INFO:root:current train perplexity2.4566729068756104
INFO:root:current mean train loss 1139.457996400619
INFO:root:current train perplexity2.457981824874878
INFO:root:current mean train loss 1140.7250644778283
INFO:root:current train perplexity2.461571455001831
INFO:root:current mean train loss 1141.8838629097006
INFO:root:current train perplexity2.461881637573242
INFO:root:current mean train loss 1143.1713146550978
INFO:root:current train perplexity2.4633445739746094
INFO:root:current mean train loss 1143.8939239278473
INFO:root:current train perplexity2.4643547534942627
INFO:root:current mean train loss 1144.5614652497325
INFO:root:current train perplexity2.4662680625915527
INFO:root:current mean train loss 1145.4164456046487
INFO:root:current train perplexity2.467531204223633
INFO:root:current mean train loss 1146.44782404279
INFO:root:current train perplexity2.469222068786621
INFO:root:current mean train loss 1146.891584589556
INFO:root:current train perplexity2.4699134826660156
INFO:root:current mean train loss 1147.7102397788528
INFO:root:current train perplexity2.4713385105133057
INFO:root:current mean train loss 1147.995704279629
INFO:root:current train perplexity2.4725234508514404
INFO:root:current mean train loss 1148.4966359155792
INFO:root:current train perplexity2.474369525909424


100%|██████████| 1/1 [07:58<00:00, 478.45s/it][A
100%|██████████| 1/1 [07:58<00:00, 478.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 46.00s/it][A
100%|██████████| 1/1 [00:45<00:00, 46.00s/it]
INFO:root:eval mean loss: 3666.9219827737893
INFO:root:eval perplexity: 20.58883285522461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/91

 46%|████▌     | 91/200 [13:16:16<15:46:55, 521.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1130.2308163850205
INFO:root:current train perplexity2.395732879638672
INFO:root:current mean train loss 1128.6734376672196
INFO:root:current train perplexity2.4197580814361572
INFO:root:current mean train loss 1130.819048625667
INFO:root:current train perplexity2.428258180618286
INFO:root:current mean train loss 1131.3030437844336
INFO:root:current train perplexity2.435075283050537
INFO:root:current mean train loss 1131.7524906722954
INFO:root:current train perplexity2.4395251274108887
INFO:root:current mean train loss 1132.9462336166437
INFO:root:current train perplexity2.44069242477417
INFO:root:current mean train loss 1133.9082685063129
INFO:root:current train perplexity2.4426350593566895
INFO:root:current mean train loss 1133.940893260148
INFO:root:current train perplexity2.4457545280456543
INFO:root:current mean train loss 1134.8220066223867
INFO:root:current train perplexity2.4461913108825684
INFO:root:current mean train loss 1135.44686044447
INFO:root:current train perplexity2.447535514831543
INFO:root:current mean train loss 1135.7105479486586
INFO:root:current train perplexity2.4499411582946777
INFO:root:current mean train loss 1136.215161388457
INFO:root:current train perplexity2.4524223804473877
INFO:root:current mean train loss 1137.1733138817654
INFO:root:current train perplexity2.453540802001953
INFO:root:current mean train loss 1138.0921409572925
INFO:root:current train perplexity2.4536943435668945
INFO:root:current mean train loss 1138.9196930986882
INFO:root:current train perplexity2.4551682472229004
INFO:root:current mean train loss 1139.1188312644022
INFO:root:current train perplexity2.456470489501953
INFO:root:current mean train loss 1140.1266510640141
INFO:root:current train perplexity2.4582464694976807
INFO:root:current mean train loss 1141.2707653766645
INFO:root:current train perplexity2.4596364498138428
INFO:root:current mean train loss 1141.9103216995784
INFO:root:current train perplexity2.461275577545166
INFO:root:current mean train loss 1142.2379469680393
INFO:root:current train perplexity2.4626026153564453


100%|██████████| 1/1 [07:48<00:00, 468.81s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.81s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:42<00:00, 42.67s/it][A
100%|██████████| 1/1 [00:42<00:00, 42.67s/it]
INFO:root:eval mean loss: 3676.734718849709
INFO:root:eval perplexity: 20.756166458129883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/92

 46%|████▌     | 92/200 [13:24:49<15:34:05, 518.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1118.3870733351935
INFO:root:current train perplexity2.40022873878479
INFO:root:current mean train loss 1121.157147179352
INFO:root:current train perplexity2.4171364307403564
INFO:root:current mean train loss 1124.735932672885
INFO:root:current train perplexity2.425938606262207
INFO:root:current mean train loss 1126.2516239050663
INFO:root:current train perplexity2.426469564437866
INFO:root:current mean train loss 1125.5628954761744
INFO:root:current train perplexity2.4280004501342773
INFO:root:current mean train loss 1124.6895383357155
INFO:root:current train perplexity2.428741455078125
INFO:root:current mean train loss 1125.828398599524
INFO:root:current train perplexity2.4318032264709473
INFO:root:current mean train loss 1126.8210462017735
INFO:root:current train perplexity2.433425188064575
INFO:root:current mean train loss 1126.8078476075916
INFO:root:current train perplexity2.4349465370178223
INFO:root:current mean train loss 1128.736775462625
INFO:root:current train perplexity2.4372401237487793
INFO:root:current mean train loss 1129.9868099754526
INFO:root:current train perplexity2.439303398132324
INFO:root:current mean train loss 1130.0694367006126
INFO:root:current train perplexity2.4418113231658936
INFO:root:current mean train loss 1130.9950517547197
INFO:root:current train perplexity2.443255662918091
INFO:root:current mean train loss 1131.7311727813474
INFO:root:current train perplexity2.444328546524048
INFO:root:current mean train loss 1132.4207815403654
INFO:root:current train perplexity2.446462631225586
INFO:root:current mean train loss 1133.2380118049723
INFO:root:current train perplexity2.4476077556610107
INFO:root:current mean train loss 1134.308494655038
INFO:root:current train perplexity2.448981761932373
INFO:root:current mean train loss 1134.5572930745489
INFO:root:current train perplexity2.4492156505584717
INFO:root:current mean train loss 1135.2168743237974
INFO:root:current train perplexity2.4499411582946777
INFO:root:current mean train loss 1136.1010689329748
INFO:root:current train perplexity2.450685977935791


100%|██████████| 1/1 [07:41<00:00, 461.10s/it][A
100%|██████████| 1/1 [07:41<00:00, 461.10s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.06s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.06s/it]
INFO:root:eval mean loss: 3692.6549691781624
INFO:root:eval perplexity: 21.030540466308594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/93

 46%|████▋     | 93/200 [13:33:18<15:19:46, 515.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1110.1776329040526
INFO:root:current train perplexity2.4240543842315674
INFO:root:current mean train loss 1109.876389227973
INFO:root:current train perplexity2.418765068054199
INFO:root:current mean train loss 1113.1454871041435
INFO:root:current train perplexity2.4212429523468018
INFO:root:current mean train loss 1114.7322681627775
INFO:root:current train perplexity2.4182779788970947
INFO:root:current mean train loss 1117.1334573109946
INFO:root:current train perplexity2.4182074069976807
INFO:root:current mean train loss 1118.966266816238
INFO:root:current train perplexity2.4188694953918457
INFO:root:current mean train loss 1119.8504309261546
INFO:root:current train perplexity2.4230570793151855
INFO:root:current mean train loss 1120.5631896190155
INFO:root:current train perplexity2.423312187194824
INFO:root:current mean train loss 1121.4124157298695
INFO:root:current train perplexity2.4241116046905518
INFO:root:current mean train loss 1122.5411910699338
INFO:root:current train perplexity2.427502393722534
INFO:root:current mean train loss 1123.4771034523292
INFO:root:current train perplexity2.4297590255737305
INFO:root:current mean train loss 1124.114744283385
INFO:root:current train perplexity2.4307093620300293
INFO:root:current mean train loss 1125.2010872364044
INFO:root:current train perplexity2.4310355186462402
INFO:root:current mean train loss 1126.236364436495
INFO:root:current train perplexity2.432619571685791
INFO:root:current mean train loss 1126.6196097296638
INFO:root:current train perplexity2.4332382678985596
INFO:root:current mean train loss 1127.1779539084132
INFO:root:current train perplexity2.4339561462402344
INFO:root:current mean train loss 1127.7898591177805
INFO:root:current train perplexity2.43500018119812
INFO:root:current mean train loss 1128.352749290895
INFO:root:current train perplexity2.4347729682922363
INFO:root:current mean train loss 1128.9036853871446
INFO:root:current train perplexity2.4366302490234375
INFO:root:current mean train loss 1129.7760002675682
INFO:root:current train perplexity2.4386847019195557


100%|██████████| 1/1 [07:54<00:00, 474.11s/it][A
100%|██████████| 1/1 [07:54<00:00, 474.11s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.36s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.36s/it]
INFO:root:eval mean loss: 3694.253324858061
INFO:root:eval perplexity: 21.058277130126953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/94

 47%|████▋     | 94/200 [13:41:59<15:14:16, 517.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1109.4072605408344
INFO:root:current train perplexity2.406280994415283
INFO:root:current mean train loss 1112.2316717932067
INFO:root:current train perplexity2.406365394592285
INFO:root:current mean train loss 1113.1354460539642
INFO:root:current train perplexity2.4069361686706543
INFO:root:current mean train loss 1113.447130179225
INFO:root:current train perplexity2.40444278717041
INFO:root:current mean train loss 1112.9948433275433
INFO:root:current train perplexity2.4089550971984863
INFO:root:current mean train loss 1113.3793185695731
INFO:root:current train perplexity2.4094059467315674
INFO:root:current mean train loss 1113.9642230653694
INFO:root:current train perplexity2.413027048110962
INFO:root:current mean train loss 1113.4575049808363
INFO:root:current train perplexity2.41434383392334
INFO:root:current mean train loss 1115.0030660469795
INFO:root:current train perplexity2.4153237342834473
INFO:root:current mean train loss 1116.030378366545
INFO:root:current train perplexity2.4172422885894775
INFO:root:current mean train loss 1117.224832929474
INFO:root:current train perplexity2.4180548191070557
INFO:root:current mean train loss 1118.5058029799434
INFO:root:current train perplexity2.4187746047973633
INFO:root:current mean train loss 1119.8919463146624
INFO:root:current train perplexity2.4193060398101807
INFO:root:current mean train loss 1120.9266285770009
INFO:root:current train perplexity2.421053409576416
INFO:root:current mean train loss 1121.5645926455777
INFO:root:current train perplexity2.4222218990325928
INFO:root:current mean train loss 1122.1596312788627
INFO:root:current train perplexity2.4233033657073975
INFO:root:current mean train loss 1122.9850052770055
INFO:root:current train perplexity2.4242968559265137
INFO:root:current mean train loss 1123.4035505274633
INFO:root:current train perplexity2.4251868724823
INFO:root:current mean train loss 1123.5296011177188
INFO:root:current train perplexity2.426811456680298


100%|██████████| 1/1 [07:48<00:00, 468.82s/it][A
100%|██████████| 1/1 [07:48<00:00, 468.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:45<00:00, 45.45s/it][A
100%|██████████| 1/1 [00:45<00:00, 45.45s/it]
INFO:root:eval mean loss: 3703.3252854905686
INFO:root:eval perplexity: 21.21645164489746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/95

 48%|████▊     | 95/200 [13:50:36<15:05:04, 517.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1109.0640956333705
INFO:root:current train perplexity2.344666004180908
INFO:root:current mean train loss 1105.596131977282
INFO:root:current train perplexity2.3795268535614014
INFO:root:current mean train loss 1105.5825178199839
INFO:root:current train perplexity2.3883392810821533
INFO:root:current mean train loss 1106.935032158141
INFO:root:current train perplexity2.3849668502807617
INFO:root:current mean train loss 1107.9447513893606
INFO:root:current train perplexity2.3903214931488037
INFO:root:current mean train loss 1108.9890604575785
INFO:root:current train perplexity2.3956246376037598
INFO:root:current mean train loss 1109.8454339341154
INFO:root:current train perplexity2.397723913192749
INFO:root:current mean train loss 1110.8540557091978
INFO:root:current train perplexity2.3999922275543213
INFO:root:current mean train loss 1111.5962366952358
INFO:root:current train perplexity2.401453971862793
INFO:root:current mean train loss 1111.8878524412994
INFO:root:current train perplexity2.4029862880706787
INFO:root:current mean train loss 1111.6461058246077
INFO:root:current train perplexity2.403268575668335
INFO:root:current mean train loss 1112.406973710394
INFO:root:current train perplexity2.4054107666015625
INFO:root:current mean train loss 1112.9452267792904
INFO:root:current train perplexity2.4067859649658203
INFO:root:current mean train loss 1113.5080426127581
INFO:root:current train perplexity2.4088902473449707
INFO:root:current mean train loss 1114.4263305664062
INFO:root:current train perplexity2.4102299213409424
INFO:root:current mean train loss 1115.143810035373
INFO:root:current train perplexity2.4109275341033936
INFO:root:current mean train loss 1115.7501709286903
INFO:root:current train perplexity2.4122354984283447
INFO:root:current mean train loss 1116.2847202439013
INFO:root:current train perplexity2.412973403930664
INFO:root:current mean train loss 1116.7765001257042
INFO:root:current train perplexity2.413879156112671
INFO:root:current mean train loss 1117.23954580022
INFO:root:current train perplexity2.414952278137207


100%|██████████| 1/1 [07:51<00:00, 471.97s/it][A
100%|██████████| 1/1 [07:51<00:00, 471.98s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|██████████| 1/1 [00:44<00:00, 44.20s/it][A
100%|██████████| 1/1 [00:44<00:00, 44.20s/it]
INFO:root:eval mean loss: 3717.74841125305
INFO:root:eval perplexity: 21.470378875732422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_from_scratch_64_low/96

 48%|████▊     | 96/200 [13:59:14<14:57:00, 517.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1099.4046433971773
INFO:root:current train perplexity2.394380569458008
INFO:root:current mean train loss 1102.7617532278746
INFO:root:current train perplexity2.3908989429473877
INFO:root:current mean train loss 1101.489799994927
INFO:root:current train perplexity2.3886563777923584
INFO:root:current mean train loss 1103.1650313178577
INFO:root:current train perplexity2.3874499797821045
INFO:root:current mean train loss 1102.7648032203904
INFO:root:current train perplexity2.386265993118286
INFO:root:current mean train loss 1101.973010391839
INFO:root:current train perplexity2.3909032344818115
INFO:root:current mean train loss 1102.910373016696
INFO:root:current train perplexity2.394247531890869
INFO:root:current mean train loss 1103.9160227221112
INFO:root:current train perplexity2.3946995735168457
INFO:root:current mean train loss 1104.6317709361604
INFO:root:current train perplexity2.3955328464508057
INFO:root:current mean train loss 1105.8514579338623
INFO:root:current train perplexity2.396186590194702
INFO:root:current mean train loss 1107.2949041742127
INFO:root:current train perplexity2.395965814590454
INFO:root:current mean train loss 1107.5823222867796
INFO:root:current train perplexity2.3972456455230713
INFO:root:current mean train loss 1107.7832675812983
INFO:root:current train perplexity2.399388313293457
INFO:root:current mean train loss 1108.7453752685362
INFO:root:current train perplexity2.3999552726745605
INFO:root:current mean train loss 1109.4310344533433
INFO:root:current train perplexity2.3994691371917725
INFO:root:current mean train loss 1110.1258599538573
INFO:root:current train perplexity2.3998498916625977
slurmstepd: error: *** JOB 25937147 ON ga006 CANCELLED AT 2022-10-16T04:12:23 ***
