INFO:root:Output: large_distilbert_roberta_final
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11344.617808948864
INFO:root:current train perplexity7719.97705078125
INFO:root:current mean train loss 9715.880265585742
INFO:root:current train perplexity2153.012939453125
INFO:root:current mean train loss 8956.63298070391
INFO:root:current train perplexity1186.84228515625
INFO:root:current mean train loss 8521.458755531406
INFO:root:current train perplexity834.4189453125
INFO:root:current mean train loss 8163.857308366733
INFO:root:current train perplexity636.5322875976562
INFO:root:current mean train loss 7894.775506377817
INFO:root:current train perplexity510.40362548828125
INFO:root:current mean train loss 7662.605669930257
INFO:root:current train perplexity424.3449401855469
INFO:root:current mean train loss 7447.146219762008
INFO:root:current train perplexity358.46624755859375
INFO:root:current mean train loss 7250.9260555347955
INFO:root:current train perplexity307.8305358886719
INFO:root:current mean train loss 7079.953218355074
INFO:root:current train perplexity267.1727294921875
INFO:root:current mean train loss 6906.358586818983
INFO:root:current train perplexity233.4654083251953
INFO:root:current mean train loss 6746.919290652367
INFO:root:current train perplexity205.49874877929688
INFO:root:current mean train loss 6600.148309697363
INFO:root:current train perplexity182.78892517089844
INFO:root:current mean train loss 6458.734496634035
INFO:root:current train perplexity163.51805114746094
INFO:root:current mean train loss 6329.923068015396
INFO:root:current train perplexity147.46836853027344
INFO:root:current mean train loss 6208.5581022624
INFO:root:current train perplexity133.9442596435547
INFO:root:current mean train loss 6094.5684460240855
INFO:root:current train perplexity122.2916030883789
INFO:root:current mean train loss 5985.896328716692
INFO:root:current train perplexity112.2740707397461
INFO:root:current mean train loss 5884.396501602406
INFO:root:current train perplexity103.6878662109375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.66s/it]
INFO:root:final mean train loss: 5800.421887681031
INFO:root:final train perplexity: 97.29098510742188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.84s/it]
INFO:root:eval mean loss: 3551.6105160544103
INFO:root:eval perplexity: 17.707983016967773
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.24s/it]
INFO:root:eval mean loss: 3818.3502197265625
INFO:root:eval perplexity: 23.08481788635254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/1
  0%|          | 1/200 [05:58<19:49:06, 358.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3898.2274169921875
INFO:root:current train perplexity22.345417022705078
INFO:root:current mean train loss 3865.485633587015
INFO:root:current train perplexity21.352853775024414
INFO:root:current mean train loss 3825.946130823206
INFO:root:current train perplexity20.626771926879883
INFO:root:current mean train loss 3812.657743429836
INFO:root:current train perplexity20.221927642822266
INFO:root:current mean train loss 3783.4108147254356
INFO:root:current train perplexity19.732004165649414
INFO:root:current mean train loss 3748.1533151079516
INFO:root:current train perplexity19.287250518798828
INFO:root:current mean train loss 3715.9712461000913
INFO:root:current train perplexity18.840206146240234
INFO:root:current mean train loss 3683.7303603188284
INFO:root:current train perplexity18.381044387817383
INFO:root:current mean train loss 3655.113998113894
INFO:root:current train perplexity17.978315353393555
INFO:root:current mean train loss 3629.415068114168
INFO:root:current train perplexity17.594013214111328
INFO:root:current mean train loss 3603.8582451287216
INFO:root:current train perplexity17.245983123779297
INFO:root:current mean train loss 3584.262420763679
INFO:root:current train perplexity16.923858642578125
INFO:root:current mean train loss 3562.7128996598094
INFO:root:current train perplexity16.626134872436523
INFO:root:current mean train loss 3539.9521343381934
INFO:root:current train perplexity16.334842681884766
INFO:root:current mean train loss 3518.5031788281804
INFO:root:current train perplexity16.070405960083008
INFO:root:current mean train loss 3499.3033270118735
INFO:root:current train perplexity15.820627212524414
INFO:root:current mean train loss 3481.9612881499943
INFO:root:current train perplexity15.591014862060547
INFO:root:current mean train loss 3464.235485583752
INFO:root:current train perplexity15.361372947692871
INFO:root:current mean train loss 3445.4261721976527
INFO:root:current train perplexity15.15087604522705
INFO:root:current mean train loss 3429.0357590836625
INFO:root:current train perplexity14.946484565734863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.82s/it]
INFO:root:final mean train loss: 3415.67904236794
INFO:root:final train perplexity: 14.815316200256348
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.78s/it]
INFO:root:eval mean loss: 2803.219477227394
INFO:root:eval perplexity: 9.663985252380371
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it]
INFO:root:eval mean loss: 3145.550649223598
INFO:root:eval perplexity: 13.277188301086426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/2
  1%|          | 2/200 [12:17<20:22:39, 370.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3055.427985913826
INFO:root:current train perplexity11.435494422912598
INFO:root:current mean train loss 3048.0937848772323
INFO:root:current train perplexity11.12922477722168
INFO:root:current mean train loss 3037.5125402360513
INFO:root:current train perplexity11.01177978515625
INFO:root:current mean train loss 3030.6933865017363
INFO:root:current train perplexity10.91152572631836
INFO:root:current mean train loss 3026.948656606344
INFO:root:current train perplexity10.864845275878906
INFO:root:current mean train loss 3015.4349866799075
INFO:root:current train perplexity10.797685623168945
INFO:root:current mean train loss 3008.5425063098587
INFO:root:current train perplexity10.747502326965332
INFO:root:current mean train loss 3001.1512675994413
INFO:root:current train perplexity10.691143989562988
INFO:root:current mean train loss 2994.830753102022
INFO:root:current train perplexity10.608230590820312
INFO:root:current mean train loss 2987.104373388096
INFO:root:current train perplexity10.535511016845703
INFO:root:current mean train loss 2977.9263709216934
INFO:root:current train perplexity10.465825080871582
INFO:root:current mean train loss 2970.713376751434
INFO:root:current train perplexity10.395675659179688
INFO:root:current mean train loss 2964.1694751748782
INFO:root:current train perplexity10.338865280151367
INFO:root:current mean train loss 2955.6286567979887
INFO:root:current train perplexity10.291524887084961
INFO:root:current mean train loss 2947.4459481515505
INFO:root:current train perplexity10.234273910522461
INFO:root:current mean train loss 2939.747402999888
INFO:root:current train perplexity10.170636177062988
INFO:root:current mean train loss 2933.4312439899245
INFO:root:current train perplexity10.113651275634766
INFO:root:current mean train loss 2925.4420111777804
INFO:root:current train perplexity10.057167053222656
INFO:root:current mean train loss 2918.730933456296
INFO:root:current train perplexity10.002568244934082
INFO:root:current mean train loss 2912.7308538429984
INFO:root:current train perplexity9.956053733825684

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.39s/it]
INFO:root:final mean train loss: 2908.4197070361747
INFO:root:final train perplexity: 9.927726745605469
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.41s/it]
INFO:root:eval mean loss: 2556.067411815021
INFO:root:eval perplexity: 7.912215232849121
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.42s/it]
INFO:root:eval mean loss: 2919.392919662151
INFO:root:eval perplexity: 11.024459838867188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/3
  2%|â–         | 3/200 [19:15<21:27:22, 392.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2748.9059765625
INFO:root:current train perplexity8.845479965209961
INFO:root:current mean train loss 2757.4837125651043
INFO:root:current train perplexity8.788041114807129
INFO:root:current mean train loss 2746.7216611328126
INFO:root:current train perplexity8.71581745147705
INFO:root:current mean train loss 2737.8881968470982
INFO:root:current train perplexity8.690401077270508
INFO:root:current mean train loss 2745.802638888889
INFO:root:current train perplexity8.696587562561035
INFO:root:current mean train loss 2742.3549072265623
INFO:root:current train perplexity8.661994934082031
INFO:root:current mean train loss 2735.656435546875
INFO:root:current train perplexity8.633390426635742
INFO:root:current mean train loss 2729.3586471354165
INFO:root:current train perplexity8.597940444946289
INFO:root:current mean train loss 2721.204211856618
INFO:root:current train perplexity8.558871269226074
INFO:root:current mean train loss 2717.365353618421
INFO:root:current train perplexity8.534163475036621
INFO:root:current mean train loss 2713.987380719866
INFO:root:current train perplexity8.511475563049316
INFO:root:current mean train loss 2706.1451740828807
INFO:root:current train perplexity8.482259750366211
INFO:root:current mean train loss 2700.3436119140624
INFO:root:current train perplexity8.454254150390625
INFO:root:current mean train loss 2696.1567583550345
INFO:root:current train perplexity8.419228553771973
INFO:root:current mean train loss 2692.608590550916
INFO:root:current train perplexity8.400443077087402
INFO:root:current mean train loss 2690.3843175088205
INFO:root:current train perplexity8.37448501586914
INFO:root:current mean train loss 2686.7202013790247
INFO:root:current train perplexity8.345117568969727
INFO:root:current mean train loss 2685.55264718192
INFO:root:current train perplexity8.327486991882324
INFO:root:current mean train loss 2682.7229056693413
INFO:root:current train perplexity8.304071426391602
INFO:root:current mean train loss 2679.0726764698516
INFO:root:current train perplexity8.279619216918945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.18s/it]
INFO:root:final mean train loss: 2677.5451159686436
INFO:root:final train perplexity: 8.274052619934082
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.29s/it]
INFO:root:eval mean loss: 2416.7657197992853
INFO:root:eval perplexity: 7.0687408447265625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.29s/it]
INFO:root:eval mean loss: 2791.0663837405805
INFO:root:eval perplexity: 9.920620918273926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/4
  2%|â–         | 4/200 [25:24<20:50:58, 382.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2561.695221402752
INFO:root:current train perplexity7.514408588409424
INFO:root:current mean train loss 2594.4873310020585
INFO:root:current train perplexity7.614819526672363
INFO:root:current mean train loss 2590.788967404026
INFO:root:current train perplexity7.618213653564453
INFO:root:current mean train loss 2582.6613144211938
INFO:root:current train perplexity7.599372386932373
INFO:root:current mean train loss 2575.7476919039414
INFO:root:current train perplexity7.580708026885986
INFO:root:current mean train loss 2568.38783223793
INFO:root:current train perplexity7.557906150817871
INFO:root:current mean train loss 2562.764676987678
INFO:root:current train perplexity7.536055088043213
INFO:root:current mean train loss 2559.3692771782307
INFO:root:current train perplexity7.52601432800293
INFO:root:current mean train loss 2556.9116762858635
INFO:root:current train perplexity7.519953727722168
INFO:root:current mean train loss 2554.489110873675
INFO:root:current train perplexity7.503108978271484
INFO:root:current mean train loss 2552.8038056649775
INFO:root:current train perplexity7.490102291107178
INFO:root:current mean train loss 2551.6218479290515
INFO:root:current train perplexity7.478445053100586
INFO:root:current mean train loss 2547.6804141411185
INFO:root:current train perplexity7.462748050689697
INFO:root:current mean train loss 2546.3333244630694
INFO:root:current train perplexity7.450952053070068
INFO:root:current mean train loss 2544.247599034222
INFO:root:current train perplexity7.4427900314331055
INFO:root:current mean train loss 2542.759676195058
INFO:root:current train perplexity7.436003684997559
INFO:root:current mean train loss 2540.751288951194
INFO:root:current train perplexity7.4210309982299805
INFO:root:current mean train loss 2540.3507792327655
INFO:root:current train perplexity7.413214683532715
INFO:root:current mean train loss 2537.7602799287374
INFO:root:current train perplexity7.403032302856445
INFO:root:current mean train loss 2534.955124172876
INFO:root:current train perplexity7.390967845916748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.58s/it]
INFO:root:final mean train loss: 2534.112320569568
INFO:root:final train perplexity: 7.388515949249268
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.02s/it]
INFO:root:eval mean loss: 2331.5244599470857
INFO:root:eval perplexity: 6.5975871086120605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.79s/it]
INFO:root:eval mean loss: 2708.9724480378713
INFO:root:eval perplexity: 9.273155212402344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/5
  2%|â–Ž         | 5/200 [31:32<20:27:23, 377.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2429.6727498372397
INFO:root:current train perplexity6.905064582824707
INFO:root:current mean train loss 2461.7010776685634
INFO:root:current train perplexity6.985505104064941
INFO:root:current mean train loss 2458.534202898052
INFO:root:current train perplexity6.970177173614502
INFO:root:current mean train loss 2452.244977315267
INFO:root:current train perplexity6.954298973083496
INFO:root:current mean train loss 2455.440552262235
INFO:root:current train perplexity6.960237979888916
INFO:root:current mean train loss 2454.0399918229614
INFO:root:current train perplexity6.964200496673584
INFO:root:current mean train loss 2454.2928862989993
INFO:root:current train perplexity6.96443510055542
INFO:root:current mean train loss 2454.814041449099
INFO:root:current train perplexity6.9506449699401855
INFO:root:current mean train loss 2454.2956715579485
INFO:root:current train perplexity6.9488935470581055
INFO:root:current mean train loss 2450.5573803661314
INFO:root:current train perplexity6.9264631271362305
INFO:root:current mean train loss 2448.93205041639
INFO:root:current train perplexity6.907997131347656
INFO:root:current mean train loss 2449.4508098911597
INFO:root:current train perplexity6.908447742462158
INFO:root:current mean train loss 2444.294742096993
INFO:root:current train perplexity6.8908233642578125
INFO:root:current mean train loss 2441.132934217508
INFO:root:current train perplexity6.878549575805664
INFO:root:current mean train loss 2441.8914849211906
INFO:root:current train perplexity6.872707843780518
INFO:root:current mean train loss 2440.7523999454997
INFO:root:current train perplexity6.862473487854004
INFO:root:current mean train loss 2439.4732696460715
INFO:root:current train perplexity6.855915069580078
INFO:root:current mean train loss 2438.3500784972325
INFO:root:current train perplexity6.8503098487854
INFO:root:current mean train loss 2436.784659416053
INFO:root:current train perplexity6.839572429656982

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.25s/it]
INFO:root:final mean train loss: 2433.853034853875
INFO:root:final train perplexity: 6.826432704925537
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.34s/it]
INFO:root:eval mean loss: 2269.807616321753
INFO:root:eval perplexity: 6.276181221008301
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.90s/it]
INFO:root:eval mean loss: 2658.492444193955
INFO:root:eval perplexity: 8.896186828613281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/6
  3%|â–Ž         | 6/200 [37:38<20:08:44, 373.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2281.260498046875
INFO:root:current train perplexity6.557939052581787
INFO:root:current mean train loss 2375.027829614016
INFO:root:current train perplexity6.519179344177246
INFO:root:current mean train loss 2379.3173591272157
INFO:root:current train perplexity6.546899795532227
INFO:root:current mean train loss 2388.018473983207
INFO:root:current train perplexity6.553472995758057
INFO:root:current mean train loss 2385.6739489776537
INFO:root:current train perplexity6.560853958129883
INFO:root:current mean train loss 2383.7400019297343
INFO:root:current train perplexity6.534408092498779
INFO:root:current mean train loss 2379.2593626507905
INFO:root:current train perplexity6.5091376304626465
INFO:root:current mean train loss 2379.503271345065
INFO:root:current train perplexity6.502865314483643
INFO:root:current mean train loss 2378.6707208945363
INFO:root:current train perplexity6.496621131896973
INFO:root:current mean train loss 2376.752738114335
INFO:root:current train perplexity6.493860721588135
INFO:root:current mean train loss 2375.8970509470996
INFO:root:current train perplexity6.488523006439209
INFO:root:current mean train loss 2373.237828558732
INFO:root:current train perplexity6.481664180755615
INFO:root:current mean train loss 2367.6315785836023
INFO:root:current train perplexity6.4692840576171875
INFO:root:current mean train loss 2366.048845295536
INFO:root:current train perplexity6.463987827301025
INFO:root:current mean train loss 2365.9675191025663
INFO:root:current train perplexity6.458640098571777
INFO:root:current mean train loss 2363.1117695624794
INFO:root:current train perplexity6.448520660400391
INFO:root:current mean train loss 2362.6837686589924
INFO:root:current train perplexity6.445312023162842
INFO:root:current mean train loss 2362.2588736003177
INFO:root:current train perplexity6.442630767822266
INFO:root:current mean train loss 2362.1971034294097
INFO:root:current train perplexity6.439102649688721
INFO:root:current mean train loss 2360.4505775768716
INFO:root:current train perplexity6.4340667724609375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.98s/it]
INFO:root:final mean train loss: 2358.923888052105
INFO:root:final train perplexity: 6.434460639953613
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.82s/it]
INFO:root:eval mean loss: 2215.8927040911735
INFO:root:eval perplexity: 6.008245468139648
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.82s/it]
INFO:root:eval mean loss: 2604.462706653784
INFO:root:eval perplexity: 8.509672164916992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/7
  4%|â–Ž         | 7/200 [43:53<20:03:10, 374.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2292.381890190972
INFO:root:current train perplexity6.129511833190918
INFO:root:current mean train loss 2303.6721232786017
INFO:root:current train perplexity6.212444305419922
INFO:root:current mean train loss 2300.8285712845827
INFO:root:current train perplexity6.191355228424072
INFO:root:current mean train loss 2303.856046472705
INFO:root:current train perplexity6.189122200012207
INFO:root:current mean train loss 2314.335819810201
INFO:root:current train perplexity6.196160793304443
INFO:root:current mean train loss 2310.29596465711
INFO:root:current train perplexity6.177565097808838
INFO:root:current mean train loss 2311.7387485936238
INFO:root:current train perplexity6.176958084106445
INFO:root:current mean train loss 2310.4378226872605
INFO:root:current train perplexity6.172887325286865
INFO:root:current mean train loss 2307.6736633748474
INFO:root:current train perplexity6.1574602127075195
INFO:root:current mean train loss 2309.70448559198
INFO:root:current train perplexity6.1626152992248535
INFO:root:current mean train loss 2311.027659957676
INFO:root:current train perplexity6.167661190032959
INFO:root:current mean train loss 2310.306627522641
INFO:root:current train perplexity6.167771816253662
INFO:root:current mean train loss 2308.9633531492136
INFO:root:current train perplexity6.166043281555176
INFO:root:current mean train loss 2308.148712204512
INFO:root:current train perplexity6.164095878601074
INFO:root:current mean train loss 2306.7273609500344
INFO:root:current train perplexity6.160364627838135
INFO:root:current mean train loss 2305.843440240551
INFO:root:current train perplexity6.159807205200195
INFO:root:current mean train loss 2305.123452468325
INFO:root:current train perplexity6.154093265533447
INFO:root:current mean train loss 2303.258518276725
INFO:root:current train perplexity6.149448394775391
INFO:root:current mean train loss 2301.9253237884823
INFO:root:current train perplexity6.143974781036377
INFO:root:current mean train loss 2301.5078364303636
INFO:root:current train perplexity6.142950057983398

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.34s/it]
INFO:root:final mean train loss: 2300.0870488138435
INFO:root:final train perplexity: 6.142511367797852
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.73s/it]
INFO:root:eval mean loss: 2183.3309750387853
INFO:root:eval perplexity: 5.852001190185547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.77s/it]
INFO:root:eval mean loss: 2573.532872842559
INFO:root:eval perplexity: 8.296015739440918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/8
  4%|â–         | 8/200 [50:02<19:52:30, 372.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2269.893830217634
INFO:root:current train perplexity5.943070888519287
INFO:root:current mean train loss 2258.3179633246527
INFO:root:current train perplexity5.893484115600586
INFO:root:current mean train loss 2251.929645944149
INFO:root:current train perplexity5.891402244567871
INFO:root:current mean train loss 2247.0631981693095
INFO:root:current train perplexity5.885003089904785
INFO:root:current mean train loss 2249.293333838452
INFO:root:current train perplexity5.905002593994141
INFO:root:current mean train loss 2249.2440459349446
INFO:root:current train perplexity5.919708251953125
INFO:root:current mean train loss 2252.2832744448206
INFO:root:current train perplexity5.926862716674805
INFO:root:current mean train loss 2250.401240799054
INFO:root:current train perplexity5.921392917633057
INFO:root:current mean train loss 2250.882320271566
INFO:root:current train perplexity5.931260108947754
INFO:root:current mean train loss 2253.48676183364
INFO:root:current train perplexity5.935359001159668
INFO:root:current mean train loss 2253.3358541147722
INFO:root:current train perplexity5.936075210571289
INFO:root:current mean train loss 2257.6384705396476
INFO:root:current train perplexity5.943185329437256
INFO:root:current mean train loss 2256.3356935570596
INFO:root:current train perplexity5.940308570861816
INFO:root:current mean train loss 2255.9139647523116
INFO:root:current train perplexity5.934092044830322
INFO:root:current mean train loss 2255.2936530242814
INFO:root:current train perplexity5.93066930770874
INFO:root:current mean train loss 2254.111160168974
INFO:root:current train perplexity5.9258317947387695
INFO:root:current mean train loss 2252.09517355636
INFO:root:current train perplexity5.920295238494873
INFO:root:current mean train loss 2251.778671818714
INFO:root:current train perplexity5.917560577392578
INFO:root:current mean train loss 2251.922579881216
INFO:root:current train perplexity5.913082122802734
INFO:root:current mean train loss 2251.972578402576
INFO:root:current train perplexity5.912069320678711

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.73s/it]
INFO:root:final mean train loss: 2250.931189955934
INFO:root:final train perplexity: 5.9087815284729
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it]
INFO:root:eval mean loss: 2158.834409023853
INFO:root:eval perplexity: 5.737138748168945
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.22s/it]
INFO:root:eval mean loss: 2554.155097258006
INFO:root:eval perplexity: 8.164897918701172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/9
  4%|â–         | 9/200 [56:11<19:41:48, 371.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2216.6941504845254
INFO:root:current train perplexity5.7259345054626465
INFO:root:current mean train loss 2223.555414300216
INFO:root:current train perplexity5.725594520568848
INFO:root:current mean train loss 2223.7336711580792
INFO:root:current train perplexity5.746804714202881
INFO:root:current mean train loss 2223.727108001709
INFO:root:current train perplexity5.755320072174072
INFO:root:current mean train loss 2220.3486338927682
INFO:root:current train perplexity5.754836559295654
INFO:root:current mean train loss 2219.0309861777487
INFO:root:current train perplexity5.749956130981445
INFO:root:current mean train loss 2216.670669649276
INFO:root:current train perplexity5.746338367462158
INFO:root:current mean train loss 2216.5408708288314
INFO:root:current train perplexity5.741748332977295
INFO:root:current mean train loss 2215.6710981628703
INFO:root:current train perplexity5.743220806121826
INFO:root:current mean train loss 2214.0135875028723
INFO:root:current train perplexity5.736382007598877
INFO:root:current mean train loss 2213.5944467986947
INFO:root:current train perplexity5.736729145050049
INFO:root:current mean train loss 2214.113546477424
INFO:root:current train perplexity5.736070156097412
INFO:root:current mean train loss 2213.324482780676
INFO:root:current train perplexity5.738147258758545
INFO:root:current mean train loss 2211.4070582643767
INFO:root:current train perplexity5.732349395751953
INFO:root:current mean train loss 2211.155294791398
INFO:root:current train perplexity5.731560230255127
INFO:root:current mean train loss 2210.612185370062
INFO:root:current train perplexity5.729551792144775
INFO:root:current mean train loss 2211.2668805064645
INFO:root:current train perplexity5.728346347808838
INFO:root:current mean train loss 2209.821636391557
INFO:root:current train perplexity5.723062515258789
INFO:root:current mean train loss 2210.488238340835
INFO:root:current train perplexity5.724441051483154
INFO:root:current mean train loss 2210.8837931898775
INFO:root:current train perplexity5.723826885223389

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.75s/it]
INFO:root:final mean train loss: 2210.095267168392
INFO:root:final train perplexity: 5.7213921546936035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.12s/it]
INFO:root:eval mean loss: 2134.4065482498063
INFO:root:eval perplexity: 5.624845027923584
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.47s/it]
INFO:root:eval mean loss: 2533.2794765174813
INFO:root:eval perplexity: 8.025962829589844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/10
  5%|â–Œ         | 10/200 [1:02:18<19:31:45, 370.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2209.405165520267
INFO:root:current train perplexity5.659857749938965
INFO:root:current mean train loss 2190.9077422915125
INFO:root:current train perplexity5.618681907653809
INFO:root:current mean train loss 2183.4890222939416
INFO:root:current train perplexity5.589104175567627
INFO:root:current mean train loss 2175.8586640810254
INFO:root:current train perplexity5.565842151641846
INFO:root:current mean train loss 2186.1814048132665
INFO:root:current train perplexity5.586893081665039
INFO:root:current mean train loss 2182.553520216045
INFO:root:current train perplexity5.578577518463135
INFO:root:current mean train loss 2183.4156479543276
INFO:root:current train perplexity5.5868611335754395
INFO:root:current mean train loss 2182.3910407375142
INFO:root:current train perplexity5.58909797668457
INFO:root:current mean train loss 2181.878381726931
INFO:root:current train perplexity5.581811904907227
INFO:root:current mean train loss 2181.967117986689
INFO:root:current train perplexity5.578031063079834
INFO:root:current mean train loss 2179.6872709326035
INFO:root:current train perplexity5.5702104568481445
INFO:root:current mean train loss 2178.7961370437138
INFO:root:current train perplexity5.575658798217773
INFO:root:current mean train loss 2177.4595202723294
INFO:root:current train perplexity5.576483249664307
INFO:root:current mean train loss 2175.175016103651
INFO:root:current train perplexity5.571332931518555
INFO:root:current mean train loss 2175.399482701083
INFO:root:current train perplexity5.569631099700928
INFO:root:current mean train loss 2176.8636422482473
INFO:root:current train perplexity5.56870698928833
INFO:root:current mean train loss 2177.7897299737397
INFO:root:current train perplexity5.571121692657471
INFO:root:current mean train loss 2176.747309691762
INFO:root:current train perplexity5.56982946395874
INFO:root:current mean train loss 2175.8307685505074
INFO:root:current train perplexity5.568741798400879
INFO:root:current mean train loss 2175.42938818285
INFO:root:current train perplexity5.56611442565918

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.06s/it]
INFO:root:final mean train loss: 2175.093652645386
INFO:root:final train perplexity: 5.565509796142578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.87s/it]
INFO:root:eval mean loss: 2117.00862586921
INFO:root:eval perplexity: 5.546208381652832
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.77s/it]
INFO:root:eval mean loss: 2525.141878168634
INFO:root:eval perplexity: 7.972446441650391
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/11
  6%|â–Œ         | 11/200 [1:08:33<19:30:01, 371.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2130.295236986737
INFO:root:current train perplexity5.4020161628723145
INFO:root:current mean train loss 2146.872572375882
INFO:root:current train perplexity5.453671455383301
INFO:root:current mean train loss 2143.598357514068
INFO:root:current train perplexity5.438355922698975
INFO:root:current mean train loss 2139.246072877874
INFO:root:current train perplexity5.428792953491211
INFO:root:current mean train loss 2144.2647684984245
INFO:root:current train perplexity5.438083171844482
INFO:root:current mean train loss 2146.5595790615666
INFO:root:current train perplexity5.444052219390869
INFO:root:current mean train loss 2149.7818315244626
INFO:root:current train perplexity5.449928283691406
INFO:root:current mean train loss 2148.602104051119
INFO:root:current train perplexity5.4399919509887695
INFO:root:current mean train loss 2146.926202985019
INFO:root:current train perplexity5.435794353485107
INFO:root:current mean train loss 2145.7927007152875
INFO:root:current train perplexity5.436283111572266
INFO:root:current mean train loss 2147.0389997787897
INFO:root:current train perplexity5.439716815948486
INFO:root:current mean train loss 2147.22688496736
INFO:root:current train perplexity5.436804294586182
INFO:root:current mean train loss 2146.9575891094296
INFO:root:current train perplexity5.439069747924805
INFO:root:current mean train loss 2144.147948602233
INFO:root:current train perplexity5.434578895568848
INFO:root:current mean train loss 2143.1157012159056
INFO:root:current train perplexity5.431896209716797
INFO:root:current mean train loss 2143.1730772309465
INFO:root:current train perplexity5.429553508758545
INFO:root:current mean train loss 2145.0453986442812
INFO:root:current train perplexity5.432057857513428
INFO:root:current mean train loss 2144.5488299020594
INFO:root:current train perplexity5.432257652282715
INFO:root:current mean train loss 2144.7623249591975
INFO:root:current train perplexity5.433190822601318

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.37s/it]
INFO:root:final mean train loss: 2144.015372395095
INFO:root:final train perplexity: 5.430665493011475
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.10s/it]
INFO:root:eval mean loss: 2099.372004948609
INFO:root:eval perplexity: 5.467615604400635
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.66s/it]
INFO:root:eval mean loss: 2501.5712466409022
INFO:root:eval perplexity: 7.819443702697754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/12
  6%|â–Œ         | 12/200 [1:14:40<19:19:39, 370.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2008.8817952473958
INFO:root:current train perplexity4.680079460144043
INFO:root:current mean train loss 2110.044066197664
INFO:root:current train perplexity5.269094467163086
INFO:root:current mean train loss 2104.0964259255697
INFO:root:current train perplexity5.276308536529541
INFO:root:current mean train loss 2109.9290203434407
INFO:root:current train perplexity5.275087356567383
INFO:root:current mean train loss 2107.2454476557655
INFO:root:current train perplexity5.276923179626465
INFO:root:current mean train loss 2110.5027607790757
INFO:root:current train perplexity5.286478519439697
INFO:root:current mean train loss 2113.5944613682886
INFO:root:current train perplexity5.296805381774902
INFO:root:current mean train loss 2116.390780583215
INFO:root:current train perplexity5.3026652336120605
INFO:root:current mean train loss 2116.201291969081
INFO:root:current train perplexity5.299842834472656
INFO:root:current mean train loss 2117.1022623427552
INFO:root:current train perplexity5.306180953979492
INFO:root:current mean train loss 2116.493966343158
INFO:root:current train perplexity5.305760860443115
INFO:root:current mean train loss 2115.9774973394647
INFO:root:current train perplexity5.306523323059082
INFO:root:current mean train loss 2114.7316190318475
INFO:root:current train perplexity5.306011199951172
INFO:root:current mean train loss 2115.1008005676504
INFO:root:current train perplexity5.308333396911621
INFO:root:current mean train loss 2115.4422596111012
INFO:root:current train perplexity5.311378002166748
INFO:root:current mean train loss 2116.023045299375
INFO:root:current train perplexity5.312273979187012
INFO:root:current mean train loss 2117.7664031125723
INFO:root:current train perplexity5.312937259674072
INFO:root:current mean train loss 2117.0525563946208
INFO:root:current train perplexity5.313144207000732
INFO:root:current mean train loss 2117.4078596626064
INFO:root:current train perplexity5.312535285949707
INFO:root:current mean train loss 2117.087951307352
INFO:root:current train perplexity5.312006950378418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.90s/it]
INFO:root:final mean train loss: 2115.904550803411
INFO:root:final train perplexity: 5.311511516571045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.49s/it]
INFO:root:eval mean loss: 2087.3565396789118
INFO:root:eval perplexity: 5.414710521697998
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it]
INFO:root:eval mean loss: 2494.857451743268
INFO:root:eval perplexity: 7.776402473449707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/13
  6%|â–‹         | 13/200 [1:20:56<19:18:57, 371.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2086.0473815917967
INFO:root:current train perplexity5.174391269683838
INFO:root:current mean train loss 2087.3848775227866
INFO:root:current train perplexity5.173981666564941
INFO:root:current mean train loss 2091.7443808815697
INFO:root:current train perplexity5.184942245483398
INFO:root:current mean train loss 2092.6739055633543
INFO:root:current train perplexity5.190103054046631
INFO:root:current mean train loss 2098.1395903087796
INFO:root:current train perplexity5.210745811462402
INFO:root:current mean train loss 2096.8259110670824
INFO:root:current train perplexity5.205912113189697
INFO:root:current mean train loss 2095.410075919859
INFO:root:current train perplexity5.208761692047119
INFO:root:current mean train loss 2094.340592787001
INFO:root:current train perplexity5.205965042114258
INFO:root:current mean train loss 2092.0264544231136
INFO:root:current train perplexity5.20161247253418
INFO:root:current mean train loss 2093.812091993249
INFO:root:current train perplexity5.206154823303223
INFO:root:current mean train loss 2091.7313301834406
INFO:root:current train perplexity5.200453281402588
INFO:root:current mean train loss 2090.243540627616
INFO:root:current train perplexity5.201796054840088
INFO:root:current mean train loss 2090.9112305688077
INFO:root:current train perplexity5.20494270324707
INFO:root:current mean train loss 2089.9760665431168
INFO:root:current train perplexity5.203127861022949
INFO:root:current mean train loss 2091.825537281305
INFO:root:current train perplexity5.203554630279541
INFO:root:current mean train loss 2090.5724345157023
INFO:root:current train perplexity5.20387601852417
INFO:root:current mean train loss 2091.581507779345
INFO:root:current train perplexity5.209620475769043
INFO:root:current mean train loss 2091.37344133244
INFO:root:current train perplexity5.206594944000244
INFO:root:current mean train loss 2090.9429348788417
INFO:root:current train perplexity5.207271575927734
INFO:root:current mean train loss 2090.92606455485
INFO:root:current train perplexity5.206339359283447

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.26s/it]
INFO:root:final mean train loss: 2089.8294551847443
INFO:root:final train perplexity: 5.203326225280762
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.37s/it]
INFO:root:eval mean loss: 2078.484877566074
INFO:root:eval perplexity: 5.375978469848633
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.88s/it]
INFO:root:eval mean loss: 2492.047466305131
INFO:root:eval perplexity: 7.758457660675049
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/14
  7%|â–‹         | 14/200 [1:27:04<19:09:36, 370.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2045.3447760504646
INFO:root:current train perplexity5.100305557250977
INFO:root:current mean train loss 2065.040532689895
INFO:root:current train perplexity5.088338375091553
INFO:root:current mean train loss 2069.1953882144976
INFO:root:current train perplexity5.097812175750732
INFO:root:current mean train loss 2067.810044829145
INFO:root:current train perplexity5.106027126312256
INFO:root:current mean train loss 2067.0488739362845
INFO:root:current train perplexity5.104850769042969
INFO:root:current mean train loss 2070.4432202557614
INFO:root:current train perplexity5.116926193237305
INFO:root:current mean train loss 2070.1914639315764
INFO:root:current train perplexity5.115511417388916
INFO:root:current mean train loss 2068.3774216961183
INFO:root:current train perplexity5.117428779602051
INFO:root:current mean train loss 2067.979096516344
INFO:root:current train perplexity5.116569995880127
INFO:root:current mean train loss 2064.4077678668204
INFO:root:current train perplexity5.104806423187256
INFO:root:current mean train loss 2064.264808647428
INFO:root:current train perplexity5.100954055786133
INFO:root:current mean train loss 2066.4765721625577
INFO:root:current train perplexity5.111265659332275
INFO:root:current mean train loss 2065.959413446721
INFO:root:current train perplexity5.108573913574219
INFO:root:current mean train loss 2065.3279526224756
INFO:root:current train perplexity5.106158256530762
INFO:root:current mean train loss 2066.561797819622
INFO:root:current train perplexity5.110494613647461
INFO:root:current mean train loss 2066.606822562978
INFO:root:current train perplexity5.109457492828369
INFO:root:current mean train loss 2067.1037714730405
INFO:root:current train perplexity5.111867427825928
INFO:root:current mean train loss 2069.4852039818115
INFO:root:current train perplexity5.1152801513671875
INFO:root:current mean train loss 2069.114211828474
INFO:root:current train perplexity5.114215850830078
INFO:root:current mean train loss 2067.9907729464458
INFO:root:current train perplexity5.112096786499023

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.70s/it]
INFO:root:final mean train loss: 2067.0349821628374
INFO:root:final train perplexity: 5.11055850982666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.62s/it]
INFO:root:eval mean loss: 2068.8099966928467
INFO:root:eval perplexity: 5.334053039550781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.54s/it]
INFO:root:eval mean loss: 2482.9555945430243
INFO:root:eval perplexity: 7.70068359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/15
  8%|â–Š         | 15/200 [1:33:09<18:57:50, 369.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2030.0895950882523
INFO:root:current train perplexity4.975647926330566
INFO:root:current mean train loss 2047.302645596591
INFO:root:current train perplexity4.976031303405762
INFO:root:current mean train loss 2043.406601793184
INFO:root:current train perplexity4.987819671630859
INFO:root:current mean train loss 2050.769096417616
INFO:root:current train perplexity5.001319408416748
INFO:root:current mean train loss 2043.495684518688
INFO:root:current train perplexity4.9816436767578125
INFO:root:current mean train loss 2046.7400155738803
INFO:root:current train perplexity4.9944658279418945
INFO:root:current mean train loss 2042.8287443108516
INFO:root:current train perplexity4.996026039123535
INFO:root:current mean train loss 2043.8603282493368
INFO:root:current train perplexity5.000976085662842
INFO:root:current mean train loss 2044.5767306254118
INFO:root:current train perplexity5.008385181427002
INFO:root:current mean train loss 2045.7615385875263
INFO:root:current train perplexity5.012419700622559
INFO:root:current mean train loss 2046.6946226984999
INFO:root:current train perplexity5.017617702484131
INFO:root:current mean train loss 2045.3183119854853
INFO:root:current train perplexity5.014650821685791
INFO:root:current mean train loss 2046.4045385820064
INFO:root:current train perplexity5.015427589416504
INFO:root:current mean train loss 2045.9106879861174
INFO:root:current train perplexity5.018587112426758
INFO:root:current mean train loss 2046.1653611971556
INFO:root:current train perplexity5.022316932678223
INFO:root:current mean train loss 2045.2663398261543
INFO:root:current train perplexity5.02225399017334
INFO:root:current mean train loss 2045.0469752245976
INFO:root:current train perplexity5.024625301361084
INFO:root:current mean train loss 2044.9826278773517
INFO:root:current train perplexity5.023364067077637
INFO:root:current mean train loss 2045.7948107343918
INFO:root:current train perplexity5.025026798248291
INFO:root:current mean train loss 2046.1720010060492
INFO:root:current train perplexity5.0252556800842285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.16s/it]
INFO:root:final mean train loss: 2045.5813804853462
INFO:root:final train perplexity: 5.0247578620910645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.28s/it]
INFO:root:eval mean loss: 2058.779717627992
INFO:root:eval perplexity: 5.2909345626831055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.78s/it]
INFO:root:eval mean loss: 2478.252792466617
INFO:root:eval perplexity: 7.670967102050781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/16
  8%|â–Š         | 16/200 [1:39:26<18:59:12, 371.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2025.753036284111
INFO:root:current train perplexity4.9165568351745605
INFO:root:current mean train loss 2019.2805033008954
INFO:root:current train perplexity4.909101486206055
INFO:root:current mean train loss 2028.6223437319823
INFO:root:current train perplexity4.916887283325195
INFO:root:current mean train loss 2026.7066476004463
INFO:root:current train perplexity4.919855117797852
INFO:root:current mean train loss 2023.0018168001925
INFO:root:current train perplexity4.91281795501709
INFO:root:current mean train loss 2023.2618855010398
INFO:root:current train perplexity4.917978763580322
INFO:root:current mean train loss 2021.2427503696674
INFO:root:current train perplexity4.921571731567383
INFO:root:current mean train loss 2024.2951245338845
INFO:root:current train perplexity4.931207656860352
INFO:root:current mean train loss 2024.5077037439007
INFO:root:current train perplexity4.935324192047119
INFO:root:current mean train loss 2025.6537408327836
INFO:root:current train perplexity4.937203407287598
INFO:root:current mean train loss 2025.9469049989787
INFO:root:current train perplexity4.937677383422852
INFO:root:current mean train loss 2024.8677546943384
INFO:root:current train perplexity4.938349723815918
INFO:root:current mean train loss 2025.015320256411
INFO:root:current train perplexity4.942914962768555
INFO:root:current mean train loss 2026.5376409284395
INFO:root:current train perplexity4.94920015335083
INFO:root:current mean train loss 2026.2199959304364
INFO:root:current train perplexity4.945960521697998
INFO:root:current mean train loss 2026.070199598559
INFO:root:current train perplexity4.945852756500244
INFO:root:current mean train loss 2025.8560074373036
INFO:root:current train perplexity4.943379878997803
INFO:root:current mean train loss 2025.8714833962317
INFO:root:current train perplexity4.945319175720215
INFO:root:current mean train loss 2027.0046080727427
INFO:root:current train perplexity4.947014808654785
INFO:root:current mean train loss 2025.984381626851
INFO:root:current train perplexity4.944843769073486

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.37s/it]
INFO:root:final mean train loss: 2025.2118423161817
INFO:root:final train perplexity: 4.9446282386779785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.37s/it]
INFO:root:eval mean loss: 2052.9846615622228
INFO:root:eval perplexity: 5.266180992126465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.24s/it]
INFO:root:eval mean loss: 2470.7850354783077
INFO:root:eval perplexity: 7.6240153312683105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/17
  8%|â–Š         | 17/200 [1:45:23<18:39:33, 367.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1987.442758733576
INFO:root:current train perplexity4.828716278076172
INFO:root:current mean train loss 2005.1028929365443
INFO:root:current train perplexity4.839032173156738
INFO:root:current mean train loss 2005.4733742607964
INFO:root:current train perplexity4.841268062591553
INFO:root:current mean train loss 2008.7310095718226
INFO:root:current train perplexity4.846378326416016
INFO:root:current mean train loss 2006.7966183521708
INFO:root:current train perplexity4.845826625823975
INFO:root:current mean train loss 2004.6154118751992
INFO:root:current train perplexity4.844804763793945
INFO:root:current mean train loss 2006.7747905642486
INFO:root:current train perplexity4.848453044891357
INFO:root:current mean train loss 2007.3178611794099
INFO:root:current train perplexity4.852046489715576
INFO:root:current mean train loss 2007.7643340514587
INFO:root:current train perplexity4.8588643074035645
INFO:root:current mean train loss 2006.6639928161374
INFO:root:current train perplexity4.860225677490234
INFO:root:current mean train loss 2005.2223248201258
INFO:root:current train perplexity4.861189365386963
INFO:root:current mean train loss 2003.9164096819432
INFO:root:current train perplexity4.857906818389893
INFO:root:current mean train loss 2003.3806565563132
INFO:root:current train perplexity4.859148025512695
INFO:root:current mean train loss 2003.360124043841
INFO:root:current train perplexity4.862276554107666
INFO:root:current mean train loss 2004.332996655536
INFO:root:current train perplexity4.8636698722839355
INFO:root:current mean train loss 2004.1071153155501
INFO:root:current train perplexity4.865356922149658
INFO:root:current mean train loss 2004.8110611178863
INFO:root:current train perplexity4.866576194763184
INFO:root:current mean train loss 2005.1745001944387
INFO:root:current train perplexity4.866095542907715
INFO:root:current mean train loss 2006.8770436432403
INFO:root:current train perplexity4.869965553283691

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.32s/it]
INFO:root:final mean train loss: 2005.9438328822334
INFO:root:final train perplexity: 4.870007514953613
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.61s/it]
INFO:root:eval mean loss: 2047.028830670296
INFO:root:eval perplexity: 5.240860939025879
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.04s/it]
INFO:root:eval mean loss: 2470.2067247755986
INFO:root:eval perplexity: 7.620391845703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/18
  9%|â–‰         | 18/200 [1:51:22<18:25:59, 364.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1979.67265625
INFO:root:current train perplexity4.857144832611084
INFO:root:current mean train loss 1996.1044526599703
INFO:root:current train perplexity4.790006160736084
INFO:root:current mean train loss 1985.327128787157
INFO:root:current train perplexity4.778845310211182
INFO:root:current mean train loss 1987.582530737705
INFO:root:current train perplexity4.772485256195068
INFO:root:current mean train loss 1989.2563202281058
INFO:root:current train perplexity4.779123783111572
INFO:root:current mean train loss 1986.7851093556621
INFO:root:current train perplexity4.792495250701904
INFO:root:current mean train loss 1986.655201204158
INFO:root:current train perplexity4.793100357055664
INFO:root:current mean train loss 1988.136401540337
INFO:root:current train perplexity4.798387050628662
INFO:root:current mean train loss 1985.9405318929541
INFO:root:current train perplexity4.787532329559326
INFO:root:current mean train loss 1984.9697430183874
INFO:root:current train perplexity4.790055751800537
INFO:root:current mean train loss 1984.9916428599192
INFO:root:current train perplexity4.793371677398682
INFO:root:current mean train loss 1984.2708522606758
INFO:root:current train perplexity4.795566082000732
INFO:root:current mean train loss 1986.9690328384336
INFO:root:current train perplexity4.801811218261719
INFO:root:current mean train loss 1988.5354935569326
INFO:root:current train perplexity4.8044304847717285
INFO:root:current mean train loss 1988.2369166689834
INFO:root:current train perplexity4.805042266845703
INFO:root:current mean train loss 1989.2301735101744
INFO:root:current train perplexity4.804739475250244
INFO:root:current mean train loss 1989.4825850917543
INFO:root:current train perplexity4.8068342208862305
INFO:root:current mean train loss 1990.3870823834998
INFO:root:current train perplexity4.806200981140137
INFO:root:current mean train loss 1990.2996910031814
INFO:root:current train perplexity4.806294918060303
INFO:root:current mean train loss 1989.508718191232
INFO:root:current train perplexity4.805300235748291

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.93s/it]
INFO:root:final mean train loss: 1988.4359060375004
INFO:root:final train perplexity: 4.803179740905762
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.27s/it]
INFO:root:eval mean loss: 2043.060531291556
INFO:root:eval perplexity: 5.224059581756592
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.33s/it]
INFO:root:eval mean loss: 2465.5216886912676
INFO:root:eval perplexity: 7.591096878051758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/19
 10%|â–‰         | 19/200 [1:57:47<18:38:53, 370.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1985.9650823419745
INFO:root:current train perplexity4.8161091804504395
INFO:root:current mean train loss 1959.2839185370774
INFO:root:current train perplexity4.737517833709717
INFO:root:current mean train loss 1962.5504397830448
INFO:root:current train perplexity4.733616828918457
INFO:root:current mean train loss 1965.2039480268586
INFO:root:current train perplexity4.7302165031433105
INFO:root:current mean train loss 1968.8449796703756
INFO:root:current train perplexity4.734415531158447
INFO:root:current mean train loss 1963.328711498743
INFO:root:current train perplexity4.730849742889404
INFO:root:current mean train loss 1964.8484069235456
INFO:root:current train perplexity4.734612941741943
INFO:root:current mean train loss 1961.8642796228467
INFO:root:current train perplexity4.725937366485596
INFO:root:current mean train loss 1964.61797427435
INFO:root:current train perplexity4.730145454406738
INFO:root:current mean train loss 1967.4971324068347
INFO:root:current train perplexity4.7305684089660645
INFO:root:current mean train loss 1967.5344872521327
INFO:root:current train perplexity4.7289347648620605
INFO:root:current mean train loss 1968.6980719200856
INFO:root:current train perplexity4.73192834854126
INFO:root:current mean train loss 1969.6198294931464
INFO:root:current train perplexity4.732875347137451
INFO:root:current mean train loss 1969.3380204516711
INFO:root:current train perplexity4.7297844886779785
INFO:root:current mean train loss 1968.4781628057424
INFO:root:current train perplexity4.728219032287598
INFO:root:current mean train loss 1969.5609378689378
INFO:root:current train perplexity4.730291843414307
INFO:root:current mean train loss 1970.4194259925775
INFO:root:current train perplexity4.73273229598999
INFO:root:current mean train loss 1969.897434283355
INFO:root:current train perplexity4.731209754943848
INFO:root:current mean train loss 1970.6167332537218
INFO:root:current train perplexity4.734958171844482
INFO:root:current mean train loss 1970.793335278498
INFO:root:current train perplexity4.7352986335754395

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.31s/it]
INFO:root:final mean train loss: 1971.0620185825119
INFO:root:final train perplexity: 4.737770080566406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.74s/it]
INFO:root:eval mean loss: 2039.7628831795766
INFO:root:eval perplexity: 5.210136890411377
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it]
INFO:root:eval mean loss: 2466.7097315145725
INFO:root:eval perplexity: 7.598513603210449
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/20
 10%|â–ˆ         | 20/200 [2:04:06<18:39:43, 373.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1936.9600297976763
INFO:root:current train perplexity4.654639720916748
INFO:root:current mean train loss 1931.326648739602
INFO:root:current train perplexity4.644050598144531
INFO:root:current mean train loss 1944.7910973457113
INFO:root:current train perplexity4.6486945152282715
INFO:root:current mean train loss 1948.9334731200452
INFO:root:current train perplexity4.655576229095459
INFO:root:current mean train loss 1946.2248504569156
INFO:root:current train perplexity4.659914493560791
INFO:root:current mean train loss 1947.2630769237737
INFO:root:current train perplexity4.658466339111328
INFO:root:current mean train loss 1947.3734814758777
INFO:root:current train perplexity4.666177749633789
INFO:root:current mean train loss 1950.3176514002241
INFO:root:current train perplexity4.671218395233154
INFO:root:current mean train loss 1951.0444445058756
INFO:root:current train perplexity4.669219970703125
INFO:root:current mean train loss 1950.0138732455155
INFO:root:current train perplexity4.661920547485352
INFO:root:current mean train loss 1951.1476990862232
INFO:root:current train perplexity4.665792465209961
INFO:root:current mean train loss 1951.5485399361762
INFO:root:current train perplexity4.665499687194824
INFO:root:current mean train loss 1953.1836353268134
INFO:root:current train perplexity4.663959503173828
INFO:root:current mean train loss 1952.3628151219134
INFO:root:current train perplexity4.6645917892456055
INFO:root:current mean train loss 1951.582848671495
INFO:root:current train perplexity4.667492866516113
INFO:root:current mean train loss 1952.7438807794226
INFO:root:current train perplexity4.667245388031006
INFO:root:current mean train loss 1952.8591117183926
INFO:root:current train perplexity4.668537616729736
INFO:root:current mean train loss 1953.0454716476784
INFO:root:current train perplexity4.669849395751953
INFO:root:current mean train loss 1953.2785540980576
INFO:root:current train perplexity4.67114782333374
INFO:root:current mean train loss 1954.406274048922
INFO:root:current train perplexity4.6745781898498535

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.20s/it]
INFO:root:final mean train loss: 1953.7279211862847
INFO:root:final train perplexity: 4.673398017883301
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.68s/it]
INFO:root:eval mean loss: 2035.0710310699246
INFO:root:eval perplexity: 5.190393924713135
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.86s/it]
INFO:root:eval mean loss: 2465.129162943955
INFO:root:eval perplexity: 7.58864688873291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/21
 10%|â–ˆ         | 21/200 [2:14:59<22:44:01, 457.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1952.3926609584264
INFO:root:current train perplexity4.644463539123535
INFO:root:current mean train loss 1933.0791563376401
INFO:root:current train perplexity4.598707675933838
INFO:root:current mean train loss 1931.0776076316833
INFO:root:current train perplexity4.579270839691162
INFO:root:current mean train loss 1925.8135742873287
INFO:root:current train perplexity4.5866498947143555
INFO:root:current mean train loss 1926.7942876983107
INFO:root:current train perplexity4.590782642364502
INFO:root:current mean train loss 1930.3377687742384
INFO:root:current train perplexity4.5987725257873535
INFO:root:current mean train loss 1930.214873523247
INFO:root:current train perplexity4.600778102874756
INFO:root:current mean train loss 1931.5273728143602
INFO:root:current train perplexity4.6017255783081055
INFO:root:current mean train loss 1933.8543481559398
INFO:root:current train perplexity4.598901748657227
INFO:root:current mean train loss 1933.4169584777067
INFO:root:current train perplexity4.599523067474365
INFO:root:current mean train loss 1934.1474266052246
INFO:root:current train perplexity4.601363182067871
INFO:root:current mean train loss 1935.572030354536
INFO:root:current train perplexity4.603030681610107
INFO:root:current mean train loss 1935.7857286003743
INFO:root:current train perplexity4.6054463386535645
INFO:root:current mean train loss 1935.7352719827388
INFO:root:current train perplexity4.608508586883545
INFO:root:current mean train loss 1936.2180828052562
INFO:root:current train perplexity4.610891819000244
INFO:root:current mean train loss 1936.4920732748233
INFO:root:current train perplexity4.6110920906066895
INFO:root:current mean train loss 1935.961275552206
INFO:root:current train perplexity4.611734390258789
INFO:root:current mean train loss 1936.6843806029995
INFO:root:current train perplexity4.614692211151123
INFO:root:current mean train loss 1937.554872249735
INFO:root:current train perplexity4.615588188171387
INFO:root:current mean train loss 1938.6979863141455
INFO:root:current train perplexity4.616797924041748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.69s/it]
INFO:root:final mean train loss: 1938.3713728865769
INFO:root:final train perplexity: 4.617100715637207
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.98s/it]
INFO:root:eval mean loss: 2036.6181874376662
INFO:root:eval perplexity: 5.196895122528076
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.46s/it]
INFO:root:eval mean loss: 2471.5272610711713
INFO:root:eval perplexity: 7.6286702156066895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/22
 11%|â–ˆ         | 22/200 [2:21:38<21:44:12, 439.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1904.233913473887
INFO:root:current train perplexity4.49769401550293
INFO:root:current mean train loss 1904.4327695989884
INFO:root:current train perplexity4.5489420890808105
INFO:root:current mean train loss 1905.61185217777
INFO:root:current train perplexity4.530137062072754
INFO:root:current mean train loss 1909.7920121507414
INFO:root:current train perplexity4.539516448974609
INFO:root:current mean train loss 1912.1564685910248
INFO:root:current train perplexity4.537459373474121
INFO:root:current mean train loss 1912.7089370807428
INFO:root:current train perplexity4.539780616760254
INFO:root:current mean train loss 1916.5791744781993
INFO:root:current train perplexity4.545226573944092
INFO:root:current mean train loss 1917.6607159100097
INFO:root:current train perplexity4.545754909515381
INFO:root:current mean train loss 1918.5585125096202
INFO:root:current train perplexity4.553081512451172
INFO:root:current mean train loss 1919.4976469159494
INFO:root:current train perplexity4.554399490356445
INFO:root:current mean train loss 1921.1512453447183
INFO:root:current train perplexity4.557491779327393
INFO:root:current mean train loss 1920.7872292599104
INFO:root:current train perplexity4.555496692657471
INFO:root:current mean train loss 1920.9892017157736
INFO:root:current train perplexity4.556440353393555
INFO:root:current mean train loss 1920.7103628893447
INFO:root:current train perplexity4.55904483795166
INFO:root:current mean train loss 1920.6014375291709
INFO:root:current train perplexity4.55880069732666
INFO:root:current mean train loss 1921.1535188998678
INFO:root:current train perplexity4.558605194091797
INFO:root:current mean train loss 1922.6414948877857
INFO:root:current train perplexity4.561232566833496
INFO:root:current mean train loss 1923.5516812517626
INFO:root:current train perplexity4.5631022453308105
INFO:root:current mean train loss 1923.2749827680775
INFO:root:current train perplexity4.562601566314697
INFO:root:current mean train loss 1923.6890119271295
INFO:root:current train perplexity4.562314510345459

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.37s/it]
INFO:root:final mean train loss: 1923.2195821772666
INFO:root:final train perplexity: 4.56221866607666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.47s/it]
INFO:root:eval mean loss: 2032.032746876385
INFO:root:eval perplexity: 5.177648067474365
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.51s/it]
INFO:root:eval mean loss: 2466.4841053198415
INFO:root:eval perplexity: 7.597104549407959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/23
 12%|â–ˆâ–        | 23/200 [2:27:49<20:36:24, 419.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1890.7116170247396
INFO:root:current train perplexity4.4728617668151855
INFO:root:current mean train loss 1877.8058992084705
INFO:root:current train perplexity4.470175743103027
INFO:root:current mean train loss 1886.3998375202048
INFO:root:current train perplexity4.488816261291504
INFO:root:current mean train loss 1886.986287121895
INFO:root:current train perplexity4.4759626388549805
INFO:root:current mean train loss 1890.6955292370856
INFO:root:current train perplexity4.475269317626953
INFO:root:current mean train loss 1896.2512517379503
INFO:root:current train perplexity4.482832431793213
INFO:root:current mean train loss 1898.6347222811935
INFO:root:current train perplexity4.484061241149902
INFO:root:current mean train loss 1900.5292865222013
INFO:root:current train perplexity4.489936351776123
INFO:root:current mean train loss 1898.8654916827597
INFO:root:current train perplexity4.488234996795654
INFO:root:current mean train loss 1902.1390045474275
INFO:root:current train perplexity4.492809772491455
INFO:root:current mean train loss 1901.5079783588374
INFO:root:current train perplexity4.490048408508301
INFO:root:current mean train loss 1902.8962892676602
INFO:root:current train perplexity4.491878986358643
INFO:root:current mean train loss 1904.1267774005269
INFO:root:current train perplexity4.494453430175781
INFO:root:current mean train loss 1903.2909706609712
INFO:root:current train perplexity4.495139122009277
INFO:root:current mean train loss 1904.1534892447044
INFO:root:current train perplexity4.495782852172852
INFO:root:current mean train loss 1905.3039101654629
INFO:root:current train perplexity4.499873161315918
INFO:root:current mean train loss 1906.5079636071562
INFO:root:current train perplexity4.502440452575684
INFO:root:current mean train loss 1907.9588102713644
INFO:root:current train perplexity4.5044426918029785
INFO:root:current mean train loss 1908.6366415033895
INFO:root:current train perplexity4.507557392120361

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.46s/it]
INFO:root:final mean train loss: 1908.2337678494744
INFO:root:final train perplexity: 4.508580684661865
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.57s/it]
INFO:root:eval mean loss: 2030.1475258685173
INFO:root:eval perplexity: 5.169755458831787
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.56s/it]
INFO:root:eval mean loss: 2469.3702267044823
INFO:root:eval perplexity: 7.615151882171631
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/24
 12%|â–ˆâ–        | 24/200 [2:40:31<25:31:30, 522.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1754.938929966518
INFO:root:current train perplexity4.215682506561279
INFO:root:current mean train loss 1870.9402003778475
INFO:root:current train perplexity4.416204929351807
INFO:root:current mean train loss 1873.32625030665
INFO:root:current train perplexity4.414848327636719
INFO:root:current mean train loss 1882.3341585339474
INFO:root:current train perplexity4.422909259796143
INFO:root:current mean train loss 1885.0439995992974
INFO:root:current train perplexity4.42252779006958
INFO:root:current mean train loss 1886.7515079415527
INFO:root:current train perplexity4.426477432250977
INFO:root:current mean train loss 1888.4597493757722
INFO:root:current train perplexity4.434841156005859
INFO:root:current mean train loss 1891.4910196307019
INFO:root:current train perplexity4.4428582191467285
INFO:root:current mean train loss 1891.1828353106607
INFO:root:current train perplexity4.4455952644348145
INFO:root:current mean train loss 1892.3201847770379
INFO:root:current train perplexity4.447046756744385
INFO:root:current mean train loss 1892.2429776234328
INFO:root:current train perplexity4.449629783630371
INFO:root:current mean train loss 1892.053199058195
INFO:root:current train perplexity4.448940277099609
INFO:root:current mean train loss 1894.0485475756655
INFO:root:current train perplexity4.4514312744140625
INFO:root:current mean train loss 1893.6102174439318
INFO:root:current train perplexity4.450502395629883
INFO:root:current mean train loss 1894.244510046031
INFO:root:current train perplexity4.4504570960998535
INFO:root:current mean train loss 1894.2235011029259
INFO:root:current train perplexity4.451424598693848
INFO:root:current mean train loss 1893.3616948676688
INFO:root:current train perplexity4.453333377838135
INFO:root:current mean train loss 1894.1274497015963
INFO:root:current train perplexity4.45658016204834
INFO:root:current mean train loss 1894.121999042893
INFO:root:current train perplexity4.458253860473633
INFO:root:current mean train loss 1894.0475299677175
INFO:root:current train perplexity4.457199573516846

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.21s/it]
INFO:root:final mean train loss: 1893.955380192082
INFO:root:final train perplexity: 4.458060264587402
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.43s/it]
INFO:root:eval mean loss: 2027.1193826012577
INFO:root:eval perplexity: 5.157102108001709
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.03s/it]
INFO:root:eval mean loss: 2466.9716147564827
INFO:root:eval perplexity: 7.600151538848877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/25
 12%|â–ˆâ–Ž        | 25/200 [2:49:47<25:52:05, 532.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1937.543980916341
INFO:root:current train perplexity4.383993148803711
INFO:root:current mean train loss 1885.3939464938255
INFO:root:current train perplexity4.372619152069092
INFO:root:current mean train loss 1878.4711587088448
INFO:root:current train perplexity4.364333629608154
INFO:root:current mean train loss 1878.4592149522568
INFO:root:current train perplexity4.3714776039123535
INFO:root:current mean train loss 1879.1321580994804
INFO:root:current train perplexity4.3804755210876465
INFO:root:current mean train loss 1879.140009057431
INFO:root:current train perplexity4.390358924865723
INFO:root:current mean train loss 1879.0794360821064
INFO:root:current train perplexity4.395535945892334
INFO:root:current mean train loss 1879.577111681522
INFO:root:current train perplexity4.396412372589111
INFO:root:current mean train loss 1881.949984948612
INFO:root:current train perplexity4.401798248291016
INFO:root:current mean train loss 1881.043736049107
INFO:root:current train perplexity4.401777267456055
INFO:root:current mean train loss 1880.1560362577438
INFO:root:current train perplexity4.401580810546875
INFO:root:current mean train loss 1879.1112313592985
INFO:root:current train perplexity4.401611804962158
INFO:root:current mean train loss 1879.944382611443
INFO:root:current train perplexity4.402558326721191
INFO:root:current mean train loss 1878.8876081852754
INFO:root:current train perplexity4.4040751457214355
INFO:root:current mean train loss 1878.8486355556531
INFO:root:current train perplexity4.402504920959473
INFO:root:current mean train loss 1879.0597157716124
INFO:root:current train perplexity4.404691696166992
INFO:root:current mean train loss 1879.5596284913313
INFO:root:current train perplexity4.40532112121582
INFO:root:current mean train loss 1879.1803154823674
INFO:root:current train perplexity4.405941009521484
INFO:root:current mean train loss 1880.4027353253282
INFO:root:current train perplexity4.409379482269287
INFO:root:current mean train loss 1879.871101871102
INFO:root:current train perplexity4.40665864944458

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.89s/it]
INFO:root:final mean train loss: 1879.621899857283
INFO:root:final train perplexity: 4.407914638519287
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.16s/it]
INFO:root:eval mean loss: 2030.5965636774158
INFO:root:eval perplexity: 5.171634197235107
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it]
INFO:root:eval mean loss: 2473.575794842226
INFO:root:eval perplexity: 7.64152717590332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/26
 13%|â–ˆâ–Ž        | 26/200 [2:56:56<24:13:51, 501.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1843.9031565596417
INFO:root:current train perplexity4.317144393920898
INFO:root:current mean train loss 1842.856612401651
INFO:root:current train perplexity4.295762538909912
INFO:root:current mean train loss 1848.0936299557509
INFO:root:current train perplexity4.313663005828857
INFO:root:current mean train loss 1853.8435935638518
INFO:root:current train perplexity4.324885368347168
INFO:root:current mean train loss 1852.587002916401
INFO:root:current train perplexity4.317418575286865
INFO:root:current mean train loss 1857.1334731689
INFO:root:current train perplexity4.330136299133301
INFO:root:current mean train loss 1858.907726079551
INFO:root:current train perplexity4.336618900299072
INFO:root:current mean train loss 1859.5072108795125
INFO:root:current train perplexity4.334293365478516
INFO:root:current mean train loss 1859.7444621300442
INFO:root:current train perplexity4.334323883056641
INFO:root:current mean train loss 1858.8340794627143
INFO:root:current train perplexity4.334218502044678
INFO:root:current mean train loss 1858.7527224845776
INFO:root:current train perplexity4.338094711303711
INFO:root:current mean train loss 1860.1221618919808
INFO:root:current train perplexity4.337894439697266
INFO:root:current mean train loss 1861.1569220260878
INFO:root:current train perplexity4.342422008514404
INFO:root:current mean train loss 1861.3211192017966
INFO:root:current train perplexity4.343496799468994
INFO:root:current mean train loss 1862.4868563057069
INFO:root:current train perplexity4.347229480743408
INFO:root:current mean train loss 1863.7017689976578
INFO:root:current train perplexity4.349494457244873
INFO:root:current mean train loss 1864.3161409832514
INFO:root:current train perplexity4.35152006149292
INFO:root:current mean train loss 1864.705233359734
INFO:root:current train perplexity4.352505207061768
INFO:root:current mean train loss 1865.7315952978595
INFO:root:current train perplexity4.356694221496582
INFO:root:current mean train loss 1866.3768848259997
INFO:root:current train perplexity4.360511302947998

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.88s/it]
INFO:root:final mean train loss: 1865.8971522221107
INFO:root:final train perplexity: 4.3604278564453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.72s/it]
INFO:root:eval mean loss: 2025.0028067514406
INFO:root:eval perplexity: 5.1482768058776855
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.90s/it]
INFO:root:eval mean loss: 2469.47607421875
INFO:root:eval perplexity: 7.615816593170166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/27
 14%|â–ˆâ–Ž        | 27/200 [3:06:33<25:10:34, 523.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1840.8600990032328
INFO:root:current train perplexity4.323846817016602
INFO:root:current mean train loss 1836.626328866693
INFO:root:current train perplexity4.290031433105469
INFO:root:current mean train loss 1839.2014543400255
INFO:root:current train perplexity4.289180755615234
INFO:root:current mean train loss 1839.8403050939464
INFO:root:current train perplexity4.28692626953125
INFO:root:current mean train loss 1837.6929092074065
INFO:root:current train perplexity4.276566982269287
INFO:root:current mean train loss 1840.8429733877967
INFO:root:current train perplexity4.280371189117432
INFO:root:current mean train loss 1840.8434272000854
INFO:root:current train perplexity4.2872209548950195
INFO:root:current mean train loss 1843.1999102670472
INFO:root:current train perplexity4.287117958068848
INFO:root:current mean train loss 1842.4859933564157
INFO:root:current train perplexity4.292003154754639
INFO:root:current mean train loss 1844.4115150786142
INFO:root:current train perplexity4.295569896697998
INFO:root:current mean train loss 1846.7537840643092
INFO:root:current train perplexity4.301502227783203
INFO:root:current mean train loss 1848.009595485549
INFO:root:current train perplexity4.300459861755371
INFO:root:current mean train loss 1848.5596584204839
INFO:root:current train perplexity4.302048683166504
INFO:root:current mean train loss 1848.803400907671
INFO:root:current train perplexity4.302257537841797
INFO:root:current mean train loss 1849.2457368324813
INFO:root:current train perplexity4.3050217628479
INFO:root:current mean train loss 1849.6406167731818
INFO:root:current train perplexity4.307720184326172
INFO:root:current mean train loss 1849.5342396182807
INFO:root:current train perplexity4.309829235076904
INFO:root:current mean train loss 1850.6899674451392
INFO:root:current train perplexity4.310636520385742
INFO:root:current mean train loss 1851.780155769077
INFO:root:current train perplexity4.3129072189331055
INFO:root:current mean train loss 1852.6651444245165
INFO:root:current train perplexity4.314769744873047

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.62s/it]
INFO:root:final mean train loss: 1852.7490648355258
INFO:root:final train perplexity: 4.315415382385254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.83s/it]
INFO:root:eval mean loss: 2028.3569344594969
INFO:root:eval perplexity: 5.16226863861084
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.62s/it]
INFO:root:eval mean loss: 2476.8181797325187
INFO:root:eval perplexity: 7.661924839019775
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/28
 14%|â–ˆâ–        | 28/200 [3:16:32<26:06:18, 546.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1843.0363557942708
INFO:root:current train perplexity4.26885986328125
INFO:root:current mean train loss 1832.399021344866
INFO:root:current train perplexity4.246715545654297
INFO:root:current mean train loss 1835.7426194069603
INFO:root:current train perplexity4.251453876495361
INFO:root:current mean train loss 1833.2037724609374
INFO:root:current train perplexity4.243178844451904
INFO:root:current mean train loss 1832.7712492290295
INFO:root:current train perplexity4.23896598815918
INFO:root:current mean train loss 1833.0604305366849
INFO:root:current train perplexity4.233431339263916
INFO:root:current mean train loss 1834.976148546007
INFO:root:current train perplexity4.240585803985596
INFO:root:current mean train loss 1836.073013482863
INFO:root:current train perplexity4.246134281158447
INFO:root:current mean train loss 1834.5920905412947
INFO:root:current train perplexity4.249212265014648
INFO:root:current mean train loss 1837.972298427484
INFO:root:current train perplexity4.255319118499756
INFO:root:current mean train loss 1838.7154644349564
INFO:root:current train perplexity4.260176658630371
INFO:root:current mean train loss 1838.1681712308844
INFO:root:current train perplexity4.261662483215332
INFO:root:current mean train loss 1836.7425297755822
INFO:root:current train perplexity4.260226249694824
INFO:root:current mean train loss 1838.7396891867897
INFO:root:current train perplexity4.264044761657715
INFO:root:current mean train loss 1838.2318438824152
INFO:root:current train perplexity4.265054225921631
INFO:root:current mean train loss 1838.3740082465279
INFO:root:current train perplexity4.266847610473633
INFO:root:current mean train loss 1839.290940779501
INFO:root:current train perplexity4.268070697784424
INFO:root:current mean train loss 1840.5077895301497
INFO:root:current train perplexity4.271826267242432
INFO:root:current mean train loss 1841.6886013020833
INFO:root:current train perplexity4.276249885559082
INFO:root:current mean train loss 1841.3933159859573
INFO:root:current train perplexity4.275300979614258

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.72s/it]
INFO:root:final mean train loss: 1840.9192839760524
INFO:root:final train perplexity: 4.2753143310546875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.76s/it]
INFO:root:eval mean loss: 2030.0660036776928
INFO:root:eval perplexity: 5.169414043426514
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.98s/it]
INFO:root:eval mean loss: 2478.011558153951
INFO:root:eval perplexity: 7.669447422027588
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/29
 14%|â–ˆâ–        | 29/200 [3:26:33<26:44:04, 562.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1826.566424825917
INFO:root:current train perplexity4.24763298034668
INFO:root:current mean train loss 1818.7625045776367
INFO:root:current train perplexity4.22862434387207
INFO:root:current mean train loss 1817.0033473184665
INFO:root:current train perplexity4.223219871520996
INFO:root:current mean train loss 1820.6377161765586
INFO:root:current train perplexity4.227565288543701
INFO:root:current mean train loss 1822.4361076044843
INFO:root:current train perplexity4.225462436676025
INFO:root:current mean train loss 1822.7382492890229
INFO:root:current train perplexity4.224259376525879
INFO:root:current mean train loss 1822.7377035328418
INFO:root:current train perplexity4.224215507507324
INFO:root:current mean train loss 1823.6267805003156
INFO:root:current train perplexity4.227153301239014
INFO:root:current mean train loss 1823.8839122276136
INFO:root:current train perplexity4.228068828582764
INFO:root:current mean train loss 1824.8873844762002
INFO:root:current train perplexity4.226539134979248
INFO:root:current mean train loss 1824.730189284999
INFO:root:current train perplexity4.22544002532959
INFO:root:current mean train loss 1824.5382907534606
INFO:root:current train perplexity4.224483489990234
INFO:root:current mean train loss 1824.8953497446728
INFO:root:current train perplexity4.225686550140381
INFO:root:current mean train loss 1825.2628020363293
INFO:root:current train perplexity4.227950096130371
INFO:root:current mean train loss 1826.7135000765802
INFO:root:current train perplexity4.229572296142578
INFO:root:current mean train loss 1827.8811957584553
INFO:root:current train perplexity4.2315993309021
INFO:root:current mean train loss 1828.1913513472175
INFO:root:current train perplexity4.232578277587891
INFO:root:current mean train loss 1829.255213056292
INFO:root:current train perplexity4.233890533447266
INFO:root:current mean train loss 1829.4755633557825
INFO:root:current train perplexity4.234600067138672

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.60s/it]
INFO:root:final mean train loss: 1828.5080533472506
INFO:root:final train perplexity: 4.233642101287842
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.10s/it]
INFO:root:eval mean loss: 2030.2962443033855
INFO:root:eval perplexity: 5.170376777648926
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.99s/it]
INFO:root:eval mean loss: 2482.982052201075
INFO:root:eval perplexity: 7.700850963592529
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/30
 15%|â–ˆâ–Œ        | 30/200 [3:37:27<27:52:20, 590.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1746.1719428168403
INFO:root:current train perplexity4.030296802520752
INFO:root:current mean train loss 1809.7457622563074
INFO:root:current train perplexity4.154337406158447
INFO:root:current mean train loss 1810.5033578096966
INFO:root:current train perplexity4.180914402008057
INFO:root:current mean train loss 1806.7567762850172
INFO:root:current train perplexity4.16033935546875
INFO:root:current mean train loss 1803.9285593196057
INFO:root:current train perplexity4.158031940460205
INFO:root:current mean train loss 1806.6321186897562
INFO:root:current train perplexity4.157594203948975
INFO:root:current mean train loss 1805.1284350064784
INFO:root:current train perplexity4.162032604217529
INFO:root:current mean train loss 1803.7887663632757
INFO:root:current train perplexity4.1594696044921875
INFO:root:current mean train loss 1803.8421371327931
INFO:root:current train perplexity4.1584248542785645
INFO:root:current mean train loss 1808.1054040218476
INFO:root:current train perplexity4.165605545043945
INFO:root:current mean train loss 1807.9822647200585
INFO:root:current train perplexity4.169203281402588
INFO:root:current mean train loss 1809.5589910013596
INFO:root:current train perplexity4.174506664276123
INFO:root:current mean train loss 1810.3995729861326
INFO:root:current train perplexity4.178338050842285
INFO:root:current mean train loss 1811.8563375101462
INFO:root:current train perplexity4.180892467498779
INFO:root:current mean train loss 1811.3419250098418
INFO:root:current train perplexity4.178099632263184
INFO:root:current mean train loss 1812.8905102911178
INFO:root:current train perplexity4.181639194488525
INFO:root:current mean train loss 1812.984806153254
INFO:root:current train perplexity4.181988716125488
INFO:root:current mean train loss 1813.0893146119806
INFO:root:current train perplexity4.182370185852051
INFO:root:current mean train loss 1814.2562037900773
INFO:root:current train perplexity4.185373306274414
INFO:root:current mean train loss 1814.9921675492733
INFO:root:current train perplexity4.18773889541626

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.91s/it]
INFO:root:final mean train loss: 1815.556095032877
INFO:root:final train perplexity: 4.190587520599365
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.12s/it]
INFO:root:eval mean loss: 2030.9516856957835
INFO:root:eval perplexity: 5.173120021820068
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.04s/it]
INFO:root:eval mean loss: 2482.1739947812775
INFO:root:eval perplexity: 7.695737361907959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/31
 16%|â–ˆâ–Œ        | 31/200 [3:44:34<25:24:33, 541.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1756.283437875601
INFO:root:current train perplexity4.130611419677734
INFO:root:current mean train loss 1799.382565452939
INFO:root:current train perplexity4.131776332855225
INFO:root:current mean train loss 1796.1394485878734
INFO:root:current train perplexity4.110416889190674
INFO:root:current mean train loss 1793.7409218630175
INFO:root:current train perplexity4.115889549255371
INFO:root:current mean train loss 1797.0255694322184
INFO:root:current train perplexity4.119348526000977
INFO:root:current mean train loss 1796.8475485682034
INFO:root:current train perplexity4.114401340484619
INFO:root:current mean train loss 1797.5432337556783
INFO:root:current train perplexity4.119463920593262
INFO:root:current mean train loss 1797.6018870119879
INFO:root:current train perplexity4.124368667602539
INFO:root:current mean train loss 1797.5170029462395
INFO:root:current train perplexity4.125596046447754
INFO:root:current mean train loss 1798.5707487102204
INFO:root:current train perplexity4.125289440155029
INFO:root:current mean train loss 1799.999248541819
INFO:root:current train perplexity4.129291534423828
INFO:root:current mean train loss 1800.2535093588685
INFO:root:current train perplexity4.133022308349609
INFO:root:current mean train loss 1801.3739343241805
INFO:root:current train perplexity4.133014678955078
INFO:root:current mean train loss 1801.2281460999363
INFO:root:current train perplexity4.136655807495117
INFO:root:current mean train loss 1801.732177135152
INFO:root:current train perplexity4.138813495635986
INFO:root:current mean train loss 1802.878096634251
INFO:root:current train perplexity4.1428914070129395
INFO:root:current mean train loss 1802.5033089613123
INFO:root:current train perplexity4.145272254943848
INFO:root:current mean train loss 1802.6460042977803
INFO:root:current train perplexity4.145967960357666
INFO:root:current mean train loss 1803.5679530533355
INFO:root:current train perplexity4.150312423706055
INFO:root:current mean train loss 1803.5406253929573
INFO:root:current train perplexity4.150503158569336

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.68s/it]
INFO:root:final mean train loss: 1803.5238181156037
INFO:root:final train perplexity: 4.150981426239014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.27s/it]
INFO:root:eval mean loss: 2033.5797855025487
INFO:root:eval perplexity: 5.184133529663086
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.99s/it]
INFO:root:eval mean loss: 2489.8075085362643
INFO:root:eval perplexity: 7.744184970855713
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/32
 16%|â–ˆâ–Œ        | 32/200 [3:51:30<23:29:57, 503.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1773.7439788108647
INFO:root:current train perplexity4.034824371337891
INFO:root:current mean train loss 1783.3543624344406
INFO:root:current train perplexity4.0605669021606445
INFO:root:current mean train loss 1785.5400290155608
INFO:root:current train perplexity4.070746898651123
INFO:root:current mean train loss 1786.9262286038856
INFO:root:current train perplexity4.074410438537598
INFO:root:current mean train loss 1781.7382564501622
INFO:root:current train perplexity4.070026874542236
INFO:root:current mean train loss 1784.7520259625346
INFO:root:current train perplexity4.076011657714844
INFO:root:current mean train loss 1785.6367094475968
INFO:root:current train perplexity4.080389022827148
INFO:root:current mean train loss 1785.8604200730254
INFO:root:current train perplexity4.081642150878906
INFO:root:current mean train loss 1784.0971057027543
INFO:root:current train perplexity4.086428165435791
INFO:root:current mean train loss 1785.8400087973473
INFO:root:current train perplexity4.090044975280762
INFO:root:current mean train loss 1785.8997236271946
INFO:root:current train perplexity4.090913772583008
INFO:root:current mean train loss 1785.6078827091194
INFO:root:current train perplexity4.0918989181518555
INFO:root:current mean train loss 1786.0152315859439
INFO:root:current train perplexity4.095317840576172
INFO:root:current mean train loss 1786.668662632923
INFO:root:current train perplexity4.099295616149902
INFO:root:current mean train loss 1787.4846081432995
INFO:root:current train perplexity4.102171421051025
INFO:root:current mean train loss 1788.4158744886179
INFO:root:current train perplexity4.103430271148682
INFO:root:current mean train loss 1788.1209502077945
INFO:root:current train perplexity4.102542400360107
INFO:root:current mean train loss 1789.406624054813
INFO:root:current train perplexity4.105362892150879
INFO:root:current mean train loss 1790.6874656904927
INFO:root:current train perplexity4.107399940490723
INFO:root:current mean train loss 1791.8848450366702
INFO:root:current train perplexity4.110764980316162

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.92s/it]
INFO:root:final mean train loss: 1791.143974792819
INFO:root:final train perplexity: 4.110623836517334
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.15s/it]
INFO:root:eval mean loss: 2034.8986548024711
INFO:root:eval perplexity: 5.189669132232666
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.83s/it]
INFO:root:eval mean loss: 2492.1330341312055
INFO:root:eval perplexity: 7.759003162384033
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/33
 16%|â–ˆâ–‹        | 33/200 [4:01:40<24:50:35, 535.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1753.0551472981772
INFO:root:current train perplexity4.057614803314209
INFO:root:current mean train loss 1762.0288352966309
INFO:root:current train perplexity4.031705379486084
INFO:root:current mean train loss 1766.1140225923978
INFO:root:current train perplexity4.045218467712402
INFO:root:current mean train loss 1769.9437150743272
INFO:root:current train perplexity4.052400588989258
INFO:root:current mean train loss 1771.4721738068954
INFO:root:current train perplexity4.0577392578125
INFO:root:current mean train loss 1770.733131626674
INFO:root:current train perplexity4.058203220367432
INFO:root:current mean train loss 1769.965547873757
INFO:root:current train perplexity4.059848785400391
INFO:root:current mean train loss 1771.0701690673827
INFO:root:current train perplexity4.059469699859619
INFO:root:current mean train loss 1772.1055339014808
INFO:root:current train perplexity4.0577168464660645
INFO:root:current mean train loss 1773.4945755004883
INFO:root:current train perplexity4.058169841766357
INFO:root:current mean train loss 1774.6287131255528
INFO:root:current train perplexity4.057349681854248
INFO:root:current mean train loss 1774.735742292733
INFO:root:current train perplexity4.059662818908691
INFO:root:current mean train loss 1775.9928708999876
INFO:root:current train perplexity4.062167644500732
INFO:root:current mean train loss 1777.3614554012522
INFO:root:current train perplexity4.061834812164307
INFO:root:current mean train loss 1778.7010875963185
INFO:root:current train perplexity4.065306186676025
INFO:root:current mean train loss 1779.0773554092798
INFO:root:current train perplexity4.0669755935668945
INFO:root:current mean train loss 1779.9552543180534
INFO:root:current train perplexity4.069774150848389
INFO:root:current mean train loss 1779.916770657626
INFO:root:current train perplexity4.071180820465088
INFO:root:current mean train loss 1779.9986926663307
INFO:root:current train perplexity4.0714945793151855
INFO:root:current mean train loss 1780.614678456832
INFO:root:current train perplexity4.075191020965576

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.97s/it]
INFO:root:final mean train loss: 1779.9037583620934
INFO:root:final train perplexity: 4.074320316314697
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.42s/it]
INFO:root:eval mean loss: 2036.4807414602726
INFO:root:eval perplexity: 5.196317195892334
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.14s/it]
INFO:root:eval mean loss: 2495.1588368517287
INFO:root:eval perplexity: 7.7783308029174805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/34
 17%|â–ˆâ–‹        | 34/200 [4:08:48<23:13:01, 503.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1757.6904915153207
INFO:root:current train perplexity3.9874236583709717
INFO:root:current mean train loss 1751.9097872804114
INFO:root:current train perplexity3.985689640045166
INFO:root:current mean train loss 1753.4648067322878
INFO:root:current train perplexity3.9914052486419678
INFO:root:current mean train loss 1756.3727276141826
INFO:root:current train perplexity4.000012397766113
INFO:root:current mean train loss 1759.1970754819347
INFO:root:current train perplexity4.005913734436035
INFO:root:current mean train loss 1761.1465969250976
INFO:root:current train perplexity4.012789726257324
INFO:root:current mean train loss 1762.3601515979851
INFO:root:current train perplexity4.018919467926025
INFO:root:current mean train loss 1763.1824252056185
INFO:root:current train perplexity4.023036956787109
INFO:root:current mean train loss 1764.2102185796305
INFO:root:current train perplexity4.024471282958984
INFO:root:current mean train loss 1765.3777436458467
INFO:root:current train perplexity4.023646354675293
INFO:root:current mean train loss 1765.5113620598668
INFO:root:current train perplexity4.023844242095947
INFO:root:current mean train loss 1766.091346967595
INFO:root:current train perplexity4.025351047515869
INFO:root:current mean train loss 1766.2891827540745
INFO:root:current train perplexity4.029413223266602
INFO:root:current mean train loss 1765.5647296226796
INFO:root:current train perplexity4.027106761932373
INFO:root:current mean train loss 1767.6137284554577
INFO:root:current train perplexity4.033497333526611
INFO:root:current mean train loss 1767.7944554224298
INFO:root:current train perplexity4.033072471618652
INFO:root:current mean train loss 1768.9910255391185
INFO:root:current train perplexity4.036540508270264
INFO:root:current mean train loss 1768.104146584944
INFO:root:current train perplexity4.036214828491211
INFO:root:current mean train loss 1768.8277403191803
INFO:root:current train perplexity4.038693428039551
INFO:root:current mean train loss 1769.5806643959243
INFO:root:current train perplexity4.039484024047852

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.43s/it]
INFO:root:final mean train loss: 1768.9686049991826
INFO:root:final train perplexity: 4.039310932159424
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.36s/it]
INFO:root:eval mean loss: 2043.0390759190768
INFO:root:eval perplexity: 5.223968505859375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.21s/it]
INFO:root:eval mean loss: 2507.53489263007
INFO:root:eval perplexity: 7.857875347137451
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/35
 18%|â–ˆâ–Š        | 35/200 [4:15:01<21:16:18, 464.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1742.595361587849
INFO:root:current train perplexity3.9642398357391357
INFO:root:current mean train loss 1747.8598928549854
INFO:root:current train perplexity3.9751617908477783
INFO:root:current mean train loss 1746.8006998697917
INFO:root:current train perplexity3.973097085952759
INFO:root:current mean train loss 1748.400354375694
INFO:root:current train perplexity3.9769794940948486
INFO:root:current mean train loss 1750.1555590919154
INFO:root:current train perplexity3.981428384780884
INFO:root:current mean train loss 1751.9176985101667
INFO:root:current train perplexity3.9836931228637695
INFO:root:current mean train loss 1750.960114141019
INFO:root:current train perplexity3.9806253910064697
INFO:root:current mean train loss 1750.804140182226
INFO:root:current train perplexity3.9822680950164795
INFO:root:current mean train loss 1752.2166483151566
INFO:root:current train perplexity3.9835877418518066
INFO:root:current mean train loss 1753.354792450995
INFO:root:current train perplexity3.9868404865264893
INFO:root:current mean train loss 1753.6345895491745
INFO:root:current train perplexity3.9861347675323486
INFO:root:current mean train loss 1754.7596292415856
INFO:root:current train perplexity3.987248659133911
INFO:root:current mean train loss 1756.130195535132
INFO:root:current train perplexity3.9890849590301514
INFO:root:current mean train loss 1756.708103962574
INFO:root:current train perplexity3.992466449737549
INFO:root:current mean train loss 1756.7024764912514
INFO:root:current train perplexity3.9951868057250977
INFO:root:current mean train loss 1758.0089768394173
INFO:root:current train perplexity3.9986393451690674
INFO:root:current mean train loss 1758.3674704091186
INFO:root:current train perplexity3.9978139400482178
INFO:root:current mean train loss 1758.1609446037573
INFO:root:current train perplexity4.000054836273193
INFO:root:current mean train loss 1757.8719918111058
INFO:root:current train perplexity4.000982284545898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.40s/it]
INFO:root:final mean train loss: 1757.5163795829
INFO:root:final train perplexity: 4.00296688079834
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.98s/it]
INFO:root:eval mean loss: 2042.715226843002
INFO:root:eval perplexity: 5.222599506378174
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.38s/it]
INFO:root:eval mean loss: 2507.08437222961
INFO:root:eval perplexity: 7.854966640472412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/36
 18%|â–ˆâ–Š        | 36/200 [4:21:19<19:58:28, 438.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1744.926924272017
INFO:root:current train perplexity3.908775568008423
INFO:root:current mean train loss 1750.897927224099
INFO:root:current train perplexity3.915562629699707
INFO:root:current mean train loss 1742.1077713085012
INFO:root:current train perplexity3.9176461696624756
INFO:root:current mean train loss 1739.7180960799337
INFO:root:current train perplexity3.924544095993042
INFO:root:current mean train loss 1742.5115533165108
INFO:root:current train perplexity3.937378406524658
INFO:root:current mean train loss 1742.4480331629923
INFO:root:current train perplexity3.9423789978027344
INFO:root:current mean train loss 1739.724869099069
INFO:root:current train perplexity3.9404032230377197
INFO:root:current mean train loss 1738.389835234265
INFO:root:current train perplexity3.936147451400757
INFO:root:current mean train loss 1738.779866586631
INFO:root:current train perplexity3.938155174255371
INFO:root:current mean train loss 1741.1740312628635
INFO:root:current train perplexity3.942776918411255
INFO:root:current mean train loss 1742.6905225382125
INFO:root:current train perplexity3.942591905593872
INFO:root:current mean train loss 1743.0341366167868
INFO:root:current train perplexity3.9475820064544678
INFO:root:current mean train loss 1744.2166561564563
INFO:root:current train perplexity3.9505562782287598
INFO:root:current mean train loss 1746.66853729411
INFO:root:current train perplexity3.953448534011841
INFO:root:current mean train loss 1746.6536353075446
INFO:root:current train perplexity3.9576735496520996
INFO:root:current mean train loss 1745.5897653050804
INFO:root:current train perplexity3.959878921508789
INFO:root:current mean train loss 1745.429159513695
INFO:root:current train perplexity3.960423231124878
INFO:root:current mean train loss 1746.9561413709773
INFO:root:current train perplexity3.9644017219543457
INFO:root:current mean train loss 1747.0142938876138
INFO:root:current train perplexity3.966662883758545
INFO:root:current mean train loss 1747.2761562632866
INFO:root:current train perplexity3.968867540359497

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.97s/it]
INFO:root:final mean train loss: 1746.996053706259
INFO:root:final train perplexity: 3.9698691368103027
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.09s/it]
INFO:root:eval mean loss: 2042.7759014156693
INFO:root:eval perplexity: 5.222856044769287
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.51s/it]
INFO:root:eval mean loss: 2510.3166430317765
INFO:root:eval perplexity: 7.875866413116455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/37
 18%|â–ˆâ–Š        | 37/200 [4:27:28<18:54:11, 417.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1729.1398620605469
INFO:root:current train perplexity3.896780014038086
INFO:root:current mean train loss 1727.0425214767456
INFO:root:current train perplexity3.8960683345794678
INFO:root:current mean train loss 1724.2486781070106
INFO:root:current train perplexity3.9085090160369873
INFO:root:current mean train loss 1727.8033432379002
INFO:root:current train perplexity3.9207794666290283
INFO:root:current mean train loss 1725.9917140497225
INFO:root:current train perplexity3.9192140102386475
INFO:root:current mean train loss 1723.8289746371183
INFO:root:current train perplexity3.907670021057129
INFO:root:current mean train loss 1727.027339668031
INFO:root:current train perplexity3.912102460861206
INFO:root:current mean train loss 1727.6656121893243
INFO:root:current train perplexity3.9127540588378906
INFO:root:current mean train loss 1728.5570637431122
INFO:root:current train perplexity3.9118244647979736
INFO:root:current mean train loss 1729.8155830646383
INFO:root:current train perplexity3.913539171218872
INFO:root:current mean train loss 1730.51831458422
INFO:root:current train perplexity3.9173548221588135
INFO:root:current mean train loss 1730.1666409106965
INFO:root:current train perplexity3.9214720726013184
INFO:root:current mean train loss 1731.2904607418693
INFO:root:current train perplexity3.923191785812378
INFO:root:current mean train loss 1731.8794205447277
INFO:root:current train perplexity3.9268288612365723
INFO:root:current mean train loss 1732.928539799709
INFO:root:current train perplexity3.9285879135131836
INFO:root:current mean train loss 1732.3391369724773
INFO:root:current train perplexity3.929758071899414
INFO:root:current mean train loss 1734.1201151629923
INFO:root:current train perplexity3.9325106143951416
INFO:root:current mean train loss 1734.701657401191
INFO:root:current train perplexity3.9340927600860596
INFO:root:current mean train loss 1735.3250910051543
INFO:root:current train perplexity3.9338836669921875
INFO:root:current mean train loss 1735.9636990875624
INFO:root:current train perplexity3.9350054264068604

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.88s/it]
INFO:root:final mean train loss: 1735.9019397114241
INFO:root:final train perplexity: 3.935262441635132
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.46s/it]
INFO:root:eval mean loss: 2049.0841298204787
INFO:root:eval perplexity: 5.2495856285095215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it]
INFO:root:eval mean loss: 2519.014955344775
INFO:root:eval perplexity: 7.932389736175537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/38
 19%|â–ˆâ–‰        | 38/200 [4:33:37<18:07:39, 402.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1699.0929009331596
INFO:root:current train perplexity3.854525566101074
INFO:root:current mean train loss 1707.3047447467673
INFO:root:current train perplexity3.8650028705596924
INFO:root:current mean train loss 1705.5466796875
INFO:root:current train perplexity3.869215965270996
INFO:root:current mean train loss 1709.309515823143
INFO:root:current train perplexity3.874271869659424
INFO:root:current mean train loss 1712.0144306311447
INFO:root:current train perplexity3.8795695304870605
INFO:root:current mean train loss 1713.8573136915854
INFO:root:current train perplexity3.8815038204193115
INFO:root:current mean train loss 1714.5695736434109
INFO:root:current train perplexity3.886505603790283
INFO:root:current mean train loss 1716.7379633756293
INFO:root:current train perplexity3.8905630111694336
INFO:root:current mean train loss 1717.9874344142936
INFO:root:current train perplexity3.890366792678833
INFO:root:current mean train loss 1717.0553540426588
INFO:root:current train perplexity3.891399383544922
INFO:root:current mean train loss 1718.2835564864308
INFO:root:current train perplexity3.895474433898926
INFO:root:current mean train loss 1720.0813208967318
INFO:root:current train perplexity3.8975796699523926
INFO:root:current mean train loss 1721.6641938849148
INFO:root:current train perplexity3.8971996307373047
INFO:root:current mean train loss 1722.2920076164614
INFO:root:current train perplexity3.8966736793518066
INFO:root:current mean train loss 1722.6452961113214
INFO:root:current train perplexity3.897921562194824
INFO:root:current mean train loss 1722.7040698795256
INFO:root:current train perplexity3.8976547718048096
INFO:root:current mean train loss 1723.4050664003134
INFO:root:current train perplexity3.8977320194244385
INFO:root:current mean train loss 1724.3828647558873
INFO:root:current train perplexity3.899662971496582
INFO:root:current mean train loss 1724.4107416581978
INFO:root:current train perplexity3.899721622467041
INFO:root:current mean train loss 1725.0290143245902
INFO:root:current train perplexity3.9009175300598145

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.63s/it]
INFO:root:final mean train loss: 1724.9219952851188
INFO:root:final train perplexity: 3.9013094902038574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.26s/it]
INFO:root:eval mean loss: 2048.7645042906415
INFO:root:eval perplexity: 5.248227596282959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.41s/it]
INFO:root:eval mean loss: 2521.0640492540724
INFO:root:eval perplexity: 7.945764064788818
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/39
 20%|â–ˆâ–‰        | 39/200 [4:39:43<17:31:30, 391.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1704.9873893491683
INFO:root:current train perplexity3.8529181480407715
INFO:root:current mean train loss 1699.5408023787134
INFO:root:current train perplexity3.8562233448028564
INFO:root:current mean train loss 1694.5452946087787
INFO:root:current train perplexity3.848383903503418
INFO:root:current mean train loss 1694.9251253749785
INFO:root:current train perplexity3.849607467651367
INFO:root:current mean train loss 1696.133387181666
INFO:root:current train perplexity3.842599868774414
INFO:root:current mean train loss 1698.6627664260593
INFO:root:current train perplexity3.845154047012329
INFO:root:current mean train loss 1701.8685675214786
INFO:root:current train perplexity3.8483970165252686
INFO:root:current mean train loss 1702.3328453724778
INFO:root:current train perplexity3.8509697914123535
INFO:root:current mean train loss 1702.7419494487294
INFO:root:current train perplexity3.8494818210601807
INFO:root:current mean train loss 1704.8957977612151
INFO:root:current train perplexity3.8518848419189453
INFO:root:current mean train loss 1707.3583534944753
INFO:root:current train perplexity3.8527872562408447
INFO:root:current mean train loss 1707.1443827066078
INFO:root:current train perplexity3.855203866958618
INFO:root:current mean train loss 1709.2685389208907
INFO:root:current train perplexity3.8586461544036865
INFO:root:current mean train loss 1708.3860192207862
INFO:root:current train perplexity3.8571736812591553
INFO:root:current mean train loss 1709.924465028108
INFO:root:current train perplexity3.8578133583068848
INFO:root:current mean train loss 1711.268096025103
INFO:root:current train perplexity3.8611502647399902
INFO:root:current mean train loss 1712.6669765431097
INFO:root:current train perplexity3.8627030849456787
INFO:root:current mean train loss 1712.6434818748448
INFO:root:current train perplexity3.8632373809814453
INFO:root:current mean train loss 1713.207707750298
INFO:root:current train perplexity3.865788698196411
INFO:root:current mean train loss 1714.5373636570423
INFO:root:current train perplexity3.8687422275543213

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.94s/it]
INFO:root:final mean train loss: 1714.5373254449935
INFO:root:final train perplexity: 3.8694660663604736
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.75s/it]
INFO:root:eval mean loss: 2054.314523683372
INFO:root:eval perplexity: 5.271851062774658
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it]
INFO:root:eval mean loss: 2528.400488021526
INFO:root:eval perplexity: 7.993835926055908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/40
 20%|â–ˆâ–ˆ        | 40/200 [4:45:56<17:10:11, 386.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1698.7387030879154
INFO:root:current train perplexity3.811525583267212
INFO:root:current mean train loss 1698.3622235346106
INFO:root:current train perplexity3.8129379749298096
INFO:root:current mean train loss 1693.6756758057516
INFO:root:current train perplexity3.8003711700439453
INFO:root:current mean train loss 1696.9826280095647
INFO:root:current train perplexity3.8057785034179688
INFO:root:current mean train loss 1697.2253476582887
INFO:root:current train perplexity3.8080480098724365
INFO:root:current mean train loss 1697.5530019177056
INFO:root:current train perplexity3.812407970428467
INFO:root:current mean train loss 1696.2231919930505
INFO:root:current train perplexity3.816955804824829
INFO:root:current mean train loss 1697.7080784847863
INFO:root:current train perplexity3.8230056762695312
INFO:root:current mean train loss 1698.4381093794439
INFO:root:current train perplexity3.8248910903930664
INFO:root:current mean train loss 1699.5114486741095
INFO:root:current train perplexity3.8243980407714844
INFO:root:current mean train loss 1700.2478261528686
INFO:root:current train perplexity3.827451229095459
INFO:root:current mean train loss 1700.1080474465252
INFO:root:current train perplexity3.828192710876465
INFO:root:current mean train loss 1701.38951166912
INFO:root:current train perplexity3.8299076557159424
INFO:root:current mean train loss 1701.5249585545175
INFO:root:current train perplexity3.831275701522827
INFO:root:current mean train loss 1701.5481004076603
INFO:root:current train perplexity3.8301517963409424
INFO:root:current mean train loss 1702.1169110443714
INFO:root:current train perplexity3.8316547870635986
INFO:root:current mean train loss 1702.959203432684
INFO:root:current train perplexity3.8337957859039307
INFO:root:current mean train loss 1704.0717421430359
INFO:root:current train perplexity3.835204601287842
INFO:root:current mean train loss 1704.8030132722067
INFO:root:current train perplexity3.8379204273223877
INFO:root:current mean train loss 1705.4447798071153
INFO:root:current train perplexity3.8401477336883545

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.80s/it]
INFO:root:final mean train loss: 1704.8810850481043
INFO:root:final train perplexity: 3.84009051322937
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.63s/it]
INFO:root:eval mean loss: 2057.96617050712
INFO:root:eval perplexity: 5.287452697753906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.86s/it]
INFO:root:eval mean loss: 2533.5799833257147
INFO:root:eval perplexity: 8.027945518493652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/41
 20%|â–ˆâ–ˆ        | 41/200 [4:52:04<16:49:19, 380.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1673.335563659668
INFO:root:current train perplexity3.7482259273529053
INFO:root:current mean train loss 1672.7747628348213
INFO:root:current train perplexity3.760119915008545
INFO:root:current mean train loss 1675.9121666985588
INFO:root:current train perplexity3.7673394680023193
INFO:root:current mean train loss 1677.8993191189236
INFO:root:current train perplexity3.775256872177124
INFO:root:current mean train loss 1678.6828576364826
INFO:root:current train perplexity3.775503635406494
INFO:root:current mean train loss 1681.1737648368683
INFO:root:current train perplexity3.774536371231079
INFO:root:current mean train loss 1681.4229201393566
INFO:root:current train perplexity3.782121419906616
INFO:root:current mean train loss 1683.3101935458542
INFO:root:current train perplexity3.7843453884124756
INFO:root:current mean train loss 1685.3854623522077
INFO:root:current train perplexity3.789301872253418
INFO:root:current mean train loss 1687.4622367644406
INFO:root:current train perplexity3.790879249572754
INFO:root:current mean train loss 1687.1728102412537
INFO:root:current train perplexity3.7907238006591797
INFO:root:current mean train loss 1687.5600822729411
INFO:root:current train perplexity3.7930145263671875
INFO:root:current mean train loss 1687.8171534597138
INFO:root:current train perplexity3.793952465057373
INFO:root:current mean train loss 1688.7267298657437
INFO:root:current train perplexity3.7946808338165283
INFO:root:current mean train loss 1689.812985017338
INFO:root:current train perplexity3.7976489067077637
INFO:root:current mean train loss 1690.6054304309357
INFO:root:current train perplexity3.7991409301757812
INFO:root:current mean train loss 1691.7510589023807
INFO:root:current train perplexity3.801767349243164
INFO:root:current mean train loss 1693.0219770061942
INFO:root:current train perplexity3.804076671600342
INFO:root:current mean train loss 1692.8506169701427
INFO:root:current train perplexity3.8047454357147217

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.03s/it]
INFO:root:final mean train loss: 1693.9370780171496
INFO:root:final train perplexity: 3.8070662021636963
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.77s/it]
INFO:root:eval mean loss: 2062.225913189827
INFO:root:eval perplexity: 5.3057098388671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.09s/it]
INFO:root:eval mean loss: 2538.281383325022
INFO:root:eval perplexity: 8.059035301208496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/42
 21%|â–ˆâ–ˆ        | 42/200 [4:58:23<16:41:21, 380.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1626.7177264873799
INFO:root:current train perplexity3.699273109436035
INFO:root:current mean train loss 1669.6151263481747
INFO:root:current train perplexity3.7319936752319336
INFO:root:current mean train loss 1677.0576389653022
INFO:root:current train perplexity3.7436790466308594
INFO:root:current mean train loss 1673.0702793499152
INFO:root:current train perplexity3.737001419067383
INFO:root:current mean train loss 1676.666094542127
INFO:root:current train perplexity3.741852045059204
INFO:root:current mean train loss 1676.8234199390076
INFO:root:current train perplexity3.751858949661255
INFO:root:current mean train loss 1676.1856886597675
INFO:root:current train perplexity3.7545809745788574
INFO:root:current mean train loss 1677.3178543155022
INFO:root:current train perplexity3.758863687515259
INFO:root:current mean train loss 1677.7152416421625
INFO:root:current train perplexity3.7635021209716797
INFO:root:current mean train loss 1679.115762098465
INFO:root:current train perplexity3.762460470199585
INFO:root:current mean train loss 1680.677991771039
INFO:root:current train perplexity3.7671666145324707
INFO:root:current mean train loss 1679.282561405864
INFO:root:current train perplexity3.7659196853637695
INFO:root:current mean train loss 1680.7335814926512
INFO:root:current train perplexity3.7686071395874023
INFO:root:current mean train loss 1682.165333779096
INFO:root:current train perplexity3.770685911178589
INFO:root:current mean train loss 1683.5700887476225
INFO:root:current train perplexity3.772766351699829
INFO:root:current mean train loss 1683.3454690694966
INFO:root:current train perplexity3.7738094329833984
INFO:root:current mean train loss 1683.4596859198214
INFO:root:current train perplexity3.7747695446014404
INFO:root:current mean train loss 1683.745568270441
INFO:root:current train perplexity3.776069402694702
INFO:root:current mean train loss 1685.244719533189
INFO:root:current train perplexity3.7785208225250244
INFO:root:current mean train loss 1685.2338980770958
INFO:root:current train perplexity3.7785580158233643

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.47s/it]
INFO:root:final mean train loss: 1684.6423512515069
INFO:root:final train perplexity: 3.7792420387268066
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.75s/it]
INFO:root:eval mean loss: 2064.7129512272827
INFO:root:eval perplexity: 5.316398620605469
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.38s/it]
INFO:root:eval mean loss: 2546.5145211727063
INFO:root:eval perplexity: 8.11376953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/43
 22%|â–ˆâ–ˆâ–       | 43/200 [5:04:39<16:31:15, 378.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1698.6867838541666
INFO:root:current train perplexity3.700596332550049
INFO:root:current mean train loss 1664.0063016451322
INFO:root:current train perplexity3.689579725265503
INFO:root:current mean train loss 1662.579562775985
INFO:root:current train perplexity3.6985976696014404
INFO:root:current mean train loss 1663.524168442235
INFO:root:current train perplexity3.7131736278533936
INFO:root:current mean train loss 1665.1935271507086
INFO:root:current train perplexity3.7170073986053467
INFO:root:current mean train loss 1664.3555334702978
INFO:root:current train perplexity3.7213878631591797
INFO:root:current mean train loss 1666.4102463495165
INFO:root:current train perplexity3.724850654602051
INFO:root:current mean train loss 1666.4981826573203
INFO:root:current train perplexity3.722926616668701
INFO:root:current mean train loss 1667.0604424534074
INFO:root:current train perplexity3.72336483001709
INFO:root:current mean train loss 1667.3810571814097
INFO:root:current train perplexity3.7211191654205322
INFO:root:current mean train loss 1667.938744643128
INFO:root:current train perplexity3.7258098125457764
INFO:root:current mean train loss 1667.5999093654937
INFO:root:current train perplexity3.7271714210510254
INFO:root:current mean train loss 1668.3276789967606
INFO:root:current train perplexity3.7295546531677246
INFO:root:current mean train loss 1668.844671952097
INFO:root:current train perplexity3.7297937870025635
INFO:root:current mean train loss 1669.5871468497323
INFO:root:current train perplexity3.733242988586426
INFO:root:current mean train loss 1670.8993012471915
INFO:root:current train perplexity3.734828233718872
INFO:root:current mean train loss 1671.545310028638
INFO:root:current train perplexity3.7376208305358887
INFO:root:current mean train loss 1672.267442295317
INFO:root:current train perplexity3.740386962890625
INFO:root:current mean train loss 1673.5910491776597
INFO:root:current train perplexity3.743246555328369
INFO:root:current mean train loss 1673.7768428189768
INFO:root:current train perplexity3.7449615001678467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.02s/it]
INFO:root:final mean train loss: 1673.7950294790878
INFO:root:final train perplexity: 3.7470271587371826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.81s/it]
INFO:root:eval mean loss: 2065.6356993330287
INFO:root:eval perplexity: 5.320369720458984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.49s/it]
INFO:root:eval mean loss: 2547.046039121371
INFO:root:eval perplexity: 8.117316246032715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/44
 22%|â–ˆâ–ˆâ–       | 44/200 [5:10:54<16:22:12, 377.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1632.286275660738
INFO:root:current train perplexity3.660386323928833
INFO:root:current mean train loss 1646.6487273131909
INFO:root:current train perplexity3.6758692264556885
INFO:root:current mean train loss 1646.1553821640941
INFO:root:current train perplexity3.682049512863159
INFO:root:current mean train loss 1648.9569753157646
INFO:root:current train perplexity3.6860029697418213
INFO:root:current mean train loss 1650.4336232434982
INFO:root:current train perplexity3.690901279449463
INFO:root:current mean train loss 1651.6771338426217
INFO:root:current train perplexity3.6860921382904053
INFO:root:current mean train loss 1651.9949528548227
INFO:root:current train perplexity3.6890883445739746
INFO:root:current mean train loss 1653.494848534764
INFO:root:current train perplexity3.6912031173706055
INFO:root:current mean train loss 1654.9967905740389
INFO:root:current train perplexity3.6938023567199707
INFO:root:current mean train loss 1656.9145802998366
INFO:root:current train perplexity3.69333553314209
INFO:root:current mean train loss 1658.0802837907413
INFO:root:current train perplexity3.696000576019287
INFO:root:current mean train loss 1659.6414080805225
INFO:root:current train perplexity3.697762966156006
INFO:root:current mean train loss 1660.8500259998998
INFO:root:current train perplexity3.7009289264678955
INFO:root:current mean train loss 1661.9430571444937
INFO:root:current train perplexity3.7063510417938232
INFO:root:current mean train loss 1662.068164416816
INFO:root:current train perplexity3.7087745666503906
INFO:root:current mean train loss 1662.8082731477507
INFO:root:current train perplexity3.7112512588500977
INFO:root:current mean train loss 1664.1477987617163
INFO:root:current train perplexity3.7149202823638916
INFO:root:current mean train loss 1664.6459798829244
INFO:root:current train perplexity3.7161049842834473
INFO:root:current mean train loss 1664.6469582483842
INFO:root:current train perplexity3.717491388320923
INFO:root:current mean train loss 1664.9668425808336
INFO:root:current train perplexity3.7194926738739014

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.57s/it]
INFO:root:final mean train loss: 1664.662361333542
INFO:root:final train perplexity: 3.7201175689697266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.29s/it]
INFO:root:eval mean loss: 2068.6476804043386
INFO:root:eval perplexity: 5.333353042602539
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.14s/it]
INFO:root:eval mean loss: 2551.03570599928
INFO:root:eval perplexity: 8.1439847946167
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [5:16:56<16:03:35, 373.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1639.668451309204
INFO:root:current train perplexity3.656949043273926
INFO:root:current mean train loss 1649.638945788872
INFO:root:current train perplexity3.665618419647217
INFO:root:current mean train loss 1648.7453104654949
INFO:root:current train perplexity3.658766031265259
INFO:root:current mean train loss 1649.3155930068467
INFO:root:current train perplexity3.6646597385406494
INFO:root:current mean train loss 1647.3737874524347
INFO:root:current train perplexity3.664039134979248
INFO:root:current mean train loss 1646.8010130537318
INFO:root:current train perplexity3.664738893508911
INFO:root:current mean train loss 1646.2903519595961
INFO:root:current train perplexity3.6683294773101807
INFO:root:current mean train loss 1646.2455925267404
INFO:root:current train perplexity3.670677661895752
INFO:root:current mean train loss 1647.2948215625904
INFO:root:current train perplexity3.671565532684326
INFO:root:current mean train loss 1647.2697431002416
INFO:root:current train perplexity3.6708316802978516
INFO:root:current mean train loss 1647.8082057407923
INFO:root:current train perplexity3.6734108924865723
INFO:root:current mean train loss 1649.5029053573346
INFO:root:current train perplexity3.676450729370117
INFO:root:current mean train loss 1650.7020718538308
INFO:root:current train perplexity3.68088436126709
INFO:root:current mean train loss 1651.8108005020276
INFO:root:current train perplexity3.6848273277282715
INFO:root:current mean train loss 1652.5316659062287
INFO:root:current train perplexity3.6861395835876465
INFO:root:current mean train loss 1652.3175230684792
INFO:root:current train perplexity3.686352252960205
INFO:root:current mean train loss 1652.8104115266067
INFO:root:current train perplexity3.6864304542541504
INFO:root:current mean train loss 1653.2536221804803
INFO:root:current train perplexity3.688791513442993
INFO:root:current mean train loss 1654.1454112171616
INFO:root:current train perplexity3.689600706100464
INFO:root:current mean train loss 1655.3584765649862
INFO:root:current train perplexity3.6917126178741455

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.49s/it]
INFO:root:final mean train loss: 1654.8946102294306
INFO:root:final train perplexity: 3.6915502548217773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.27s/it]
INFO:root:eval mean loss: 2077.377293796404
INFO:root:eval perplexity: 5.371162414550781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.81s/it]
INFO:root:eval mean loss: 2563.5170344359485
INFO:root:eval perplexity: 8.227984428405762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [5:22:56<15:47:39, 369.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1623.2902168933256
INFO:root:current train perplexity3.603595495223999
INFO:root:current mean train loss 1616.2901199930939
INFO:root:current train perplexity3.6186769008636475
INFO:root:current mean train loss 1631.0922477966526
INFO:root:current train perplexity3.6358754634857178
INFO:root:current mean train loss 1634.1044752065905
INFO:root:current train perplexity3.6368110179901123
INFO:root:current mean train loss 1632.0924945284075
INFO:root:current train perplexity3.635418176651001
INFO:root:current mean train loss 1631.8976716716195
INFO:root:current train perplexity3.634671688079834
INFO:root:current mean train loss 1634.8857656694545
INFO:root:current train perplexity3.6379687786102295
INFO:root:current mean train loss 1637.6120768020767
INFO:root:current train perplexity3.6453938484191895
INFO:root:current mean train loss 1638.8466565481785
INFO:root:current train perplexity3.642982006072998
INFO:root:current mean train loss 1640.6318804850757
INFO:root:current train perplexity3.642526865005493
INFO:root:current mean train loss 1641.4160026387965
INFO:root:current train perplexity3.6433024406433105
INFO:root:current mean train loss 1641.9691414312222
INFO:root:current train perplexity3.647149085998535
INFO:root:current mean train loss 1642.1560990559133
INFO:root:current train perplexity3.6505916118621826
INFO:root:current mean train loss 1642.7950747059706
INFO:root:current train perplexity3.653204917907715
INFO:root:current mean train loss 1642.9458161945845
INFO:root:current train perplexity3.653897285461426
INFO:root:current mean train loss 1643.315193885644
INFO:root:current train perplexity3.6563680171966553
INFO:root:current mean train loss 1642.8955775254685
INFO:root:current train perplexity3.655524969100952
INFO:root:current mean train loss 1644.0169658543084
INFO:root:current train perplexity3.658601999282837
INFO:root:current mean train loss 1644.4423872254617
INFO:root:current train perplexity3.6596829891204834
INFO:root:current mean train loss 1645.2211955964476
INFO:root:current train perplexity3.661999464035034

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.40s/it]
INFO:root:final mean train loss: 1644.708518716463
INFO:root:final train perplexity: 3.6619937419891357
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it]
INFO:root:eval mean loss: 2077.202357082502
INFO:root:eval perplexity: 5.370401859283447
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it]
INFO:root:eval mean loss: 2564.135682018091
INFO:root:eval perplexity: 8.232170104980469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [5:28:53<15:31:41, 365.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1627.5018460020726
INFO:root:current train perplexity3.5981040000915527
INFO:root:current mean train loss 1624.2632458619398
INFO:root:current train perplexity3.6063334941864014
INFO:root:current mean train loss 1624.1264173264472
INFO:root:current train perplexity3.5990147590637207
INFO:root:current mean train loss 1626.3149867992306
INFO:root:current train perplexity3.5956871509552
INFO:root:current mean train loss 1629.3866486453628
INFO:root:current train perplexity3.6002042293548584
INFO:root:current mean train loss 1629.6476368983851
INFO:root:current train perplexity3.6016006469726562
INFO:root:current mean train loss 1628.93677702196
INFO:root:current train perplexity3.6063623428344727
INFO:root:current mean train loss 1629.9517811557703
INFO:root:current train perplexity3.6078290939331055
INFO:root:current mean train loss 1629.9089920961512
INFO:root:current train perplexity3.610541820526123
INFO:root:current mean train loss 1630.4768224192526
INFO:root:current train perplexity3.6150522232055664
INFO:root:current mean train loss 1630.3298795661856
INFO:root:current train perplexity3.6155037879943848
INFO:root:current mean train loss 1630.9959528290967
INFO:root:current train perplexity3.621047019958496
INFO:root:current mean train loss 1631.0425053906552
INFO:root:current train perplexity3.6240317821502686
INFO:root:current mean train loss 1632.6795456085424
INFO:root:current train perplexity3.6276047229766846
INFO:root:current mean train loss 1633.0719489592896
INFO:root:current train perplexity3.630371570587158
INFO:root:current mean train loss 1633.7457198237298
INFO:root:current train perplexity3.6311209201812744
INFO:root:current mean train loss 1634.4933844792126
INFO:root:current train perplexity3.6334993839263916
INFO:root:current mean train loss 1635.5487505241283
INFO:root:current train perplexity3.6345086097717285
INFO:root:current mean train loss 1635.8694064398585
INFO:root:current train perplexity3.635828733444214

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:07<00:00, 307.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:07<00:00, 307.27s/it]
INFO:root:final mean train loss: 1636.012113154686
INFO:root:final train perplexity: 3.636946201324463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.40s/it]
INFO:root:eval mean loss: 2082.861168827571
INFO:root:eval perplexity: 5.395051002502441
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.59s/it]
INFO:root:eval mean loss: 2571.8660087855997
INFO:root:eval perplexity: 8.28465461730957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/48
 24%|â–ˆâ–ˆâ–       | 48/200 [5:34:44<15:14:43, 361.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1626.1189290364584
INFO:root:current train perplexity3.570570707321167
INFO:root:current mean train loss 1615.7178498641304
INFO:root:current train perplexity3.596200942993164
INFO:root:current mean train loss 1614.4842637172965
INFO:root:current train perplexity3.593602418899536
INFO:root:current mean train loss 1619.2153831845237
INFO:root:current train perplexity3.59439754486084
INFO:root:current mean train loss 1618.3415371446724
INFO:root:current train perplexity3.592388153076172
INFO:root:current mean train loss 1618.4948479217232
INFO:root:current train perplexity3.5913424491882324
INFO:root:current mean train loss 1620.9851324314025
INFO:root:current train perplexity3.593608856201172
INFO:root:current mean train loss 1620.0789137620193
INFO:root:current train perplexity3.5976014137268066
INFO:root:current mean train loss 1622.0806403973352
INFO:root:current train perplexity3.5986878871917725
INFO:root:current mean train loss 1622.5821902749317
INFO:root:current train perplexity3.5986249446868896
INFO:root:current mean train loss 1622.7321411734144
INFO:root:current train perplexity3.6009137630462646
INFO:root:current mean train loss 1623.0637081129134
INFO:root:current train perplexity3.6044819355010986
INFO:root:current mean train loss 1623.3248743127895
INFO:root:current train perplexity3.6021130084991455
INFO:root:current mean train loss 1623.791272204729
INFO:root:current train perplexity3.603602409362793
INFO:root:current mean train loss 1624.9594210675243
INFO:root:current train perplexity3.605584144592285
INFO:root:current mean train loss 1625.444794486773
INFO:root:current train perplexity3.6066782474517822
INFO:root:current mean train loss 1626.37949105372
INFO:root:current train perplexity3.6079368591308594
INFO:root:current mean train loss 1627.0366596010615
INFO:root:current train perplexity3.6094000339508057
INFO:root:current mean train loss 1626.861682297047
INFO:root:current train perplexity3.6088101863861084
INFO:root:current mean train loss 1626.9751739581634
INFO:root:current train perplexity3.6093909740448

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.93s/it]
INFO:root:final mean train loss: 1626.442605253788
INFO:root:final train perplexity: 3.60958194732666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.65s/it]
INFO:root:eval mean loss: 2085.8400411402927
INFO:root:eval perplexity: 5.408071041107178
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it]
INFO:root:eval mean loss: 2578.5791271020335
INFO:root:eval perplexity: 8.3305025100708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/49
 24%|â–ˆâ–ˆâ–       | 49/200 [5:40:36<15:02:22, 358.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1623.8799896240234
INFO:root:current train perplexity3.5634703636169434
INFO:root:current mean train loss 1604.0002681847775
INFO:root:current train perplexity3.5433857440948486
INFO:root:current mean train loss 1607.0834140119882
INFO:root:current train perplexity3.532712936401367
INFO:root:current mean train loss 1610.5439986263414
INFO:root:current train perplexity3.539991855621338
INFO:root:current mean train loss 1607.6011604026512
INFO:root:current train perplexity3.5442357063293457
INFO:root:current mean train loss 1608.668632105777
INFO:root:current train perplexity3.550492763519287
INFO:root:current mean train loss 1608.894146110438
INFO:root:current train perplexity3.5523080825805664
INFO:root:current mean train loss 1608.3236684330175
INFO:root:current train perplexity3.5567381381988525
INFO:root:current mean train loss 1608.2012751652644
INFO:root:current train perplexity3.5549447536468506
INFO:root:current mean train loss 1609.407655642268
INFO:root:current train perplexity3.5589780807495117
INFO:root:current mean train loss 1610.5014725322872
INFO:root:current train perplexity3.5605554580688477
INFO:root:current mean train loss 1611.924356736901
INFO:root:current train perplexity3.5653085708618164
INFO:root:current mean train loss 1613.5132143094943
INFO:root:current train perplexity3.5693578720092773
INFO:root:current mean train loss 1613.5649021824559
INFO:root:current train perplexity3.573052406311035
INFO:root:current mean train loss 1614.7147966097186
INFO:root:current train perplexity3.5754079818725586
INFO:root:current mean train loss 1614.5371905692875
INFO:root:current train perplexity3.575579881668091
INFO:root:current mean train loss 1615.6018091837566
INFO:root:current train perplexity3.5775880813598633
INFO:root:current mean train loss 1615.911556041268
INFO:root:current train perplexity3.5775318145751953
INFO:root:current mean train loss 1616.9087391149528
INFO:root:current train perplexity3.580106019973755
INFO:root:current mean train loss 1617.283181642647
INFO:root:current train perplexity3.5809216499328613

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.14s/it]
INFO:root:final mean train loss: 1616.5659475475625
INFO:root:final train perplexity: 3.5815560817718506
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.94s/it]
INFO:root:eval mean loss: 2093.483278531555
INFO:root:eval perplexity: 5.441624164581299
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.16s/it]
INFO:root:eval mean loss: 2588.795431367049
INFO:root:eval perplexity: 8.40076732635498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [5:46:36<14:57:33, 359.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1580.5203707948024
INFO:root:current train perplexity3.5138840675354004
INFO:root:current mean train loss 1590.1517293021184
INFO:root:current train perplexity3.5172433853149414
INFO:root:current mean train loss 1598.2387773751254
INFO:root:current train perplexity3.527186632156372
INFO:root:current mean train loss 1599.5580271198962
INFO:root:current train perplexity3.5251524448394775
INFO:root:current mean train loss 1600.8797857543673
INFO:root:current train perplexity3.5270910263061523
INFO:root:current mean train loss 1601.956405556267
INFO:root:current train perplexity3.531737804412842
INFO:root:current mean train loss 1602.318871355571
INFO:root:current train perplexity3.536371946334839
INFO:root:current mean train loss 1601.8691355726908
INFO:root:current train perplexity3.539592981338501
INFO:root:current mean train loss 1603.5935413733528
INFO:root:current train perplexity3.5420799255371094
INFO:root:current mean train loss 1604.2067011842234
INFO:root:current train perplexity3.5425567626953125
INFO:root:current mean train loss 1605.5779830307138
INFO:root:current train perplexity3.5460379123687744
INFO:root:current mean train loss 1605.0572049744344
INFO:root:current train perplexity3.548412561416626
INFO:root:current mean train loss 1606.0573885866506
INFO:root:current train perplexity3.548750638961792
INFO:root:current mean train loss 1608.0854313923219
INFO:root:current train perplexity3.5552642345428467
INFO:root:current mean train loss 1609.0500481541687
INFO:root:current train perplexity3.555525779724121
INFO:root:current mean train loss 1608.795646889122
INFO:root:current train perplexity3.5554428100585938
INFO:root:current mean train loss 1608.944557499929
INFO:root:current train perplexity3.558177947998047
INFO:root:current mean train loss 1608.3323682869006
INFO:root:current train perplexity3.556699275970459
INFO:root:current mean train loss 1608.2801695225883
INFO:root:current train perplexity3.556525468826294
INFO:root:current mean train loss 1609.2128745285042
INFO:root:current train perplexity3.558901309967041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.89s/it]
INFO:root:final mean train loss: 1608.8234355116635
INFO:root:final train perplexity: 3.5597383975982666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.02s/it]
INFO:root:eval mean loss: 2092.7015160093915
INFO:root:eval perplexity: 5.438180923461914
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.36s/it]
INFO:root:eval mean loss: 2586.966229377909
INFO:root:eval perplexity: 8.388142585754395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [5:52:27<14:45:37, 356.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1616.3685820608428
INFO:root:current train perplexity3.519815444946289
INFO:root:current mean train loss 1599.1495353974492
INFO:root:current train perplexity3.5083377361297607
INFO:root:current mean train loss 1597.0316680678748
INFO:root:current train perplexity3.512063980102539
INFO:root:current mean train loss 1594.2589748361722
INFO:root:current train perplexity3.5106375217437744
INFO:root:current mean train loss 1591.1688905642268
INFO:root:current train perplexity3.509355306625366
INFO:root:current mean train loss 1592.8503810491663
INFO:root:current train perplexity3.511927366256714
INFO:root:current mean train loss 1593.918355672567
INFO:root:current train perplexity3.514878988265991
INFO:root:current mean train loss 1595.075073560909
INFO:root:current train perplexity3.5164148807525635
INFO:root:current mean train loss 1594.8488435458917
INFO:root:current train perplexity3.5164754390716553
INFO:root:current mean train loss 1595.7889400505871
INFO:root:current train perplexity3.51973032951355
INFO:root:current mean train loss 1596.6155151596213
INFO:root:current train perplexity3.521754503250122
INFO:root:current mean train loss 1597.4306170560073
INFO:root:current train perplexity3.524869203567505
INFO:root:current mean train loss 1598.5289528411336
INFO:root:current train perplexity3.5278186798095703
INFO:root:current mean train loss 1598.6973364901228
INFO:root:current train perplexity3.5275039672851562
INFO:root:current mean train loss 1598.4930375790043
INFO:root:current train perplexity3.528742790222168
INFO:root:current mean train loss 1598.1168962773388
INFO:root:current train perplexity3.5284512042999268
INFO:root:current mean train loss 1598.1041473718394
INFO:root:current train perplexity3.5295979976654053
INFO:root:current mean train loss 1598.9176006036328
INFO:root:current train perplexity3.531312942504883
INFO:root:current mean train loss 1599.5859933671204
INFO:root:current train perplexity3.5323259830474854
INFO:root:current mean train loss 1599.4579516825088
INFO:root:current train perplexity3.5327744483947754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.10s/it]
INFO:root:final mean train loss: 1599.3041084816646
INFO:root:final train perplexity: 3.5330958366394043
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.91s/it]
INFO:root:eval mean loss: 2098.3082651990526
INFO:root:eval perplexity: 5.462912082672119
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.35s/it]
INFO:root:eval mean loss: 2595.6740804902206
INFO:root:eval perplexity: 8.448407173156738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [5:58:29<14:43:07, 358.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1583.1971788521273
INFO:root:current train perplexity3.482699155807495
INFO:root:current mean train loss 1583.7164186571465
INFO:root:current train perplexity3.4799602031707764
INFO:root:current mean train loss 1585.6146365324093
INFO:root:current train perplexity3.486994743347168
INFO:root:current mean train loss 1583.9538947122837
INFO:root:current train perplexity3.4833896160125732
INFO:root:current mean train loss 1582.6743942481885
INFO:root:current train perplexity3.4856879711151123
INFO:root:current mean train loss 1583.2305847482044
INFO:root:current train perplexity3.487927198410034
INFO:root:current mean train loss 1583.5772106343795
INFO:root:current train perplexity3.4891817569732666
INFO:root:current mean train loss 1583.0191821881485
INFO:root:current train perplexity3.489565372467041
INFO:root:current mean train loss 1583.440614521031
INFO:root:current train perplexity3.4911136627197266
INFO:root:current mean train loss 1583.0220521323436
INFO:root:current train perplexity3.4936468601226807
INFO:root:current mean train loss 1583.3157584770674
INFO:root:current train perplexity3.495932102203369
INFO:root:current mean train loss 1583.576090460396
INFO:root:current train perplexity3.49765944480896
INFO:root:current mean train loss 1584.7556888761692
INFO:root:current train perplexity3.499791145324707
INFO:root:current mean train loss 1586.8585505178676
INFO:root:current train perplexity3.5019867420196533
INFO:root:current mean train loss 1587.5515909638666
INFO:root:current train perplexity3.503995895385742
INFO:root:current mean train loss 1589.5360850022703
INFO:root:current train perplexity3.5049338340759277
INFO:root:current mean train loss 1589.7286387832833
INFO:root:current train perplexity3.5052475929260254
INFO:root:current mean train loss 1590.2671304425696
INFO:root:current train perplexity3.505392551422119
INFO:root:current mean train loss 1590.9390862009593
INFO:root:current train perplexity3.5072357654571533
INFO:root:current mean train loss 1590.7834216573294
INFO:root:current train perplexity3.509416341781616

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.41s/it]
INFO:root:final mean train loss: 1590.7834216573294
INFO:root:final train perplexity: 3.509416341781616
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.54s/it]
INFO:root:eval mean loss: 2103.4420230946644
INFO:root:eval perplexity: 5.485654354095459
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.39s/it]
INFO:root:eval mean loss: 2601.514540219138
INFO:root:eval perplexity: 8.489073753356934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [6:04:24<14:35:03, 357.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.7027124023436
INFO:root:current train perplexity3.4217700958251953
INFO:root:current mean train loss 1566.7549945068358
INFO:root:current train perplexity3.434526205062866
INFO:root:current mean train loss 1568.8159431966146
INFO:root:current train perplexity3.4490394592285156
INFO:root:current mean train loss 1572.612458190918
INFO:root:current train perplexity3.4474148750305176
INFO:root:current mean train loss 1574.4263806152344
INFO:root:current train perplexity3.4533493518829346
INFO:root:current mean train loss 1578.2365075683595
INFO:root:current train perplexity3.4581730365753174
INFO:root:current mean train loss 1578.9363957868304
INFO:root:current train perplexity3.4626946449279785
INFO:root:current mean train loss 1579.9570401000976
INFO:root:current train perplexity3.4654955863952637
INFO:root:current mean train loss 1578.1668226453994
INFO:root:current train perplexity3.462486982345581
INFO:root:current mean train loss 1578.0627467041015
INFO:root:current train perplexity3.464885950088501
INFO:root:current mean train loss 1579.1218611283737
INFO:root:current train perplexity3.4677793979644775
INFO:root:current mean train loss 1579.7967856852213
INFO:root:current train perplexity3.4710581302642822
INFO:root:current mean train loss 1579.7457065054086
INFO:root:current train perplexity3.471802234649658
INFO:root:current mean train loss 1580.2159036690848
INFO:root:current train perplexity3.475088357925415
INFO:root:current mean train loss 1580.327298095703
INFO:root:current train perplexity3.4768660068511963
INFO:root:current mean train loss 1579.8408441925048
INFO:root:current train perplexity3.4773342609405518
INFO:root:current mean train loss 1580.1580374684054
INFO:root:current train perplexity3.4794023036956787
INFO:root:current mean train loss 1580.5121601019964
INFO:root:current train perplexity3.480877161026001
INFO:root:current mean train loss 1581.5818275210731
INFO:root:current train perplexity3.4827563762664795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.35s/it]
INFO:root:final mean train loss: 1581.7834255047296
INFO:root:final train perplexity: 3.4845778942108154
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.04s/it]
INFO:root:eval mean loss: 2109.93509625374
INFO:root:eval perplexity: 5.514552593231201
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.44s/it]
INFO:root:eval mean loss: 2609.4628707128213
INFO:root:eval perplexity: 8.54472541809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [6:10:24<14:30:58, 357.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1541.1722842945771
INFO:root:current train perplexity3.4928317070007324
INFO:root:current mean train loss 1563.2017874432424
INFO:root:current train perplexity3.430197238922119
INFO:root:current mean train loss 1568.7706130067324
INFO:root:current train perplexity3.4398324489593506
INFO:root:current mean train loss 1566.08465287362
INFO:root:current train perplexity3.4364030361175537
INFO:root:current mean train loss 1564.6085787619904
INFO:root:current train perplexity3.4392542839050293
INFO:root:current mean train loss 1563.764560839655
INFO:root:current train perplexity3.439504623413086
INFO:root:current mean train loss 1563.1951245473308
INFO:root:current train perplexity3.4381322860717773
INFO:root:current mean train loss 1564.6556530983046
INFO:root:current train perplexity3.44342303276062
INFO:root:current mean train loss 1565.6056050145348
INFO:root:current train perplexity3.4452531337738037
INFO:root:current mean train loss 1566.4266848631748
INFO:root:current train perplexity3.446228265762329
INFO:root:current mean train loss 1566.6937494718688
INFO:root:current train perplexity3.4442827701568604
INFO:root:current mean train loss 1568.772537327069
INFO:root:current train perplexity3.447956085205078
INFO:root:current mean train loss 1568.3020899199812
INFO:root:current train perplexity3.4489781856536865
INFO:root:current mean train loss 1569.0190832881015
INFO:root:current train perplexity3.452108860015869
INFO:root:current mean train loss 1569.2350529769649
INFO:root:current train perplexity3.4530677795410156
INFO:root:current mean train loss 1570.6060840101247
INFO:root:current train perplexity3.455219268798828
INFO:root:current mean train loss 1571.0937377703212
INFO:root:current train perplexity3.4560799598693848
INFO:root:current mean train loss 1572.335625392445
INFO:root:current train perplexity3.4585282802581787
INFO:root:current mean train loss 1572.1960736759168
INFO:root:current train perplexity3.4581148624420166
INFO:root:current mean train loss 1573.1253178158215
INFO:root:current train perplexity3.4601762294769287

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.66s/it]
INFO:root:final mean train loss: 1573.2971001190786
INFO:root:final train perplexity: 3.461318254470825
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.63s/it]
INFO:root:eval mean loss: 2110.226640850094
INFO:root:eval perplexity: 5.515854358673096
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.82s/it]
INFO:root:eval mean loss: 2611.6234161160514
INFO:root:eval perplexity: 8.559916496276855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [6:16:19<14:22:52, 357.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1566.7036779067096
INFO:root:current train perplexity3.420865774154663
INFO:root:current mean train loss 1558.3755547822411
INFO:root:current train perplexity3.421111822128296
INFO:root:current mean train loss 1556.0305629632412
INFO:root:current train perplexity3.4191746711730957
INFO:root:current mean train loss 1553.9269749076066
INFO:root:current train perplexity3.416090488433838
INFO:root:current mean train loss 1557.25731859339
INFO:root:current train perplexity3.420825481414795
INFO:root:current mean train loss 1558.1234976664912
INFO:root:current train perplexity3.425626516342163
INFO:root:current mean train loss 1559.5720024229224
INFO:root:current train perplexity3.4312922954559326
INFO:root:current mean train loss 1562.1566983672515
INFO:root:current train perplexity3.4266295433044434
INFO:root:current mean train loss 1563.2028550987336
INFO:root:current train perplexity3.428893804550171
INFO:root:current mean train loss 1563.941624774157
INFO:root:current train perplexity3.427692174911499
INFO:root:current mean train loss 1563.611124595775
INFO:root:current train perplexity3.4262187480926514
INFO:root:current mean train loss 1562.8626310694995
INFO:root:current train perplexity3.427600383758545
INFO:root:current mean train loss 1562.0025306343066
INFO:root:current train perplexity3.427102565765381
INFO:root:current mean train loss 1563.6506487661932
INFO:root:current train perplexity3.429833173751831
INFO:root:current mean train loss 1563.770980345488
INFO:root:current train perplexity3.431285858154297
INFO:root:current mean train loss 1564.3625746904793
INFO:root:current train perplexity3.4334380626678467
INFO:root:current mean train loss 1564.6269382584196
INFO:root:current train perplexity3.43414306640625
INFO:root:current mean train loss 1564.4940453763652
INFO:root:current train perplexity3.435145616531372
INFO:root:current mean train loss 1564.96537562779
INFO:root:current train perplexity3.436304807662964
INFO:root:current mean train loss 1564.8182212727022
INFO:root:current train perplexity3.436736583709717

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.77s/it]
INFO:root:final mean train loss: 1564.9642014496264
INFO:root:final train perplexity: 3.4386303424835205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.24s/it]
INFO:root:eval mean loss: 2120.8113078665224
INFO:root:eval perplexity: 5.563301086425781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.68s/it]
INFO:root:eval mean loss: 2626.5337294991136
INFO:root:eval perplexity: 8.665492057800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [6:22:21<14:20:51, 358.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1531.729714786305
INFO:root:current train perplexity3.3773083686828613
INFO:root:current mean train loss 1538.6334753983858
INFO:root:current train perplexity3.3810272216796875
INFO:root:current mean train loss 1544.1829478959164
INFO:root:current train perplexity3.3939123153686523
INFO:root:current mean train loss 1545.601785773905
INFO:root:current train perplexity3.3938984870910645
INFO:root:current mean train loss 1549.2804407090148
INFO:root:current train perplexity3.4019174575805664
INFO:root:current mean train loss 1550.14026964468
INFO:root:current train perplexity3.402318239212036
INFO:root:current mean train loss 1549.941859654018
INFO:root:current train perplexity3.404654026031494
INFO:root:current mean train loss 1552.4441288568366
INFO:root:current train perplexity3.4068830013275146
INFO:root:current mean train loss 1554.9369871899328
INFO:root:current train perplexity3.4093966484069824
INFO:root:current mean train loss 1555.0421000098581
INFO:root:current train perplexity3.411688804626465
INFO:root:current mean train loss 1554.7589747812724
INFO:root:current train perplexity3.410749912261963
INFO:root:current mean train loss 1554.0329459395023
INFO:root:current train perplexity3.4096949100494385
INFO:root:current mean train loss 1554.9977520912958
INFO:root:current train perplexity3.4107258319854736
INFO:root:current mean train loss 1555.7590947352367
INFO:root:current train perplexity3.41294527053833
INFO:root:current mean train loss 1555.468785922828
INFO:root:current train perplexity3.413748264312744
INFO:root:current mean train loss 1556.1913772081268
INFO:root:current train perplexity3.41513729095459
INFO:root:current mean train loss 1556.433295265535
INFO:root:current train perplexity3.414768934249878
INFO:root:current mean train loss 1556.4757221598818
INFO:root:current train perplexity3.415804386138916
INFO:root:current mean train loss 1557.0812885401895
INFO:root:current train perplexity3.4165267944335938
INFO:root:current mean train loss 1557.4735392732293
INFO:root:current train perplexity3.417947292327881

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.22s/it]
INFO:root:final mean train loss: 1557.1919328205765
INFO:root:final train perplexity: 3.417603015899658
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.84s/it]
INFO:root:eval mean loss: 2120.7113677762077
INFO:root:eval perplexity: 5.562851905822754
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.98s/it]
INFO:root:eval mean loss: 2627.207606971687
INFO:root:eval perplexity: 8.670292854309082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [6:28:16<14:12:10, 357.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1533.643595975988
INFO:root:current train perplexity3.384279727935791
INFO:root:current mean train loss 1530.988002232143
INFO:root:current train perplexity3.378466844558716
INFO:root:current mean train loss 1527.3186709275885
INFO:root:current train perplexity3.3605237007141113
INFO:root:current mean train loss 1531.6653203549592
INFO:root:current train perplexity3.3596739768981934
INFO:root:current mean train loss 1535.2313902765259
INFO:root:current train perplexity3.3669354915618896
INFO:root:current mean train loss 1535.0385460652096
INFO:root:current train perplexity3.3659234046936035
INFO:root:current mean train loss 1537.4302649583644
INFO:root:current train perplexity3.3675472736358643
INFO:root:current mean train loss 1537.5956149101257
INFO:root:current train perplexity3.3707587718963623
INFO:root:current mean train loss 1539.125008719308
INFO:root:current train perplexity3.3725452423095703
INFO:root:current mean train loss 1539.9706929104386
INFO:root:current train perplexity3.372986316680908
INFO:root:current mean train loss 1542.1975160520176
INFO:root:current train perplexity3.3769891262054443
INFO:root:current mean train loss 1542.0306503086874
INFO:root:current train perplexity3.3784780502319336
INFO:root:current mean train loss 1544.1454660313364
INFO:root:current train perplexity3.3813400268554688
INFO:root:current mean train loss 1544.6773353264346
INFO:root:current train perplexity3.3839478492736816
INFO:root:current mean train loss 1544.8986209380855
INFO:root:current train perplexity3.3853697776794434
INFO:root:current mean train loss 1545.9654473285286
INFO:root:current train perplexity3.3870227336883545
INFO:root:current mean train loss 1546.3659697974042
INFO:root:current train perplexity3.3882179260253906
INFO:root:current mean train loss 1547.0864950326772
INFO:root:current train perplexity3.3891067504882812
INFO:root:current mean train loss 1548.3480740206144
INFO:root:current train perplexity3.391324758529663
INFO:root:current mean train loss 1548.3589186629629
INFO:root:current train perplexity3.392869472503662

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.30s/it]
INFO:root:final mean train loss: 1548.2625227027388
INFO:root:final train perplexity: 3.3936030864715576
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.43s/it]
INFO:root:eval mean loss: 2124.942681928053
INFO:root:eval perplexity: 5.581932067871094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it]
INFO:root:eval mean loss: 2633.0623463299257
INFO:root:eval perplexity: 8.712129592895508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [6:34:08<14:02:07, 355.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1523.522025792739
INFO:root:current train perplexity3.3476321697235107
INFO:root:current mean train loss 1512.9891971072634
INFO:root:current train perplexity3.3392131328582764
INFO:root:current mean train loss 1519.5157162314968
INFO:root:current train perplexity3.347856283187866
INFO:root:current mean train loss 1522.0308631797889
INFO:root:current train perplexity3.3446719646453857
INFO:root:current mean train loss 1522.0900221991785
INFO:root:current train perplexity3.3457860946655273
INFO:root:current mean train loss 1525.2165295723157
INFO:root:current train perplexity3.3468384742736816
INFO:root:current mean train loss 1527.5092800168225
INFO:root:current train perplexity3.348109483718872
INFO:root:current mean train loss 1527.519987497512
INFO:root:current train perplexity3.350419282913208
INFO:root:current mean train loss 1528.7656172757768
INFO:root:current train perplexity3.3511059284210205
INFO:root:current mean train loss 1529.5089017141895
INFO:root:current train perplexity3.3526182174682617
INFO:root:current mean train loss 1530.1510179651498
INFO:root:current train perplexity3.3552145957946777
INFO:root:current mean train loss 1531.6873593873615
INFO:root:current train perplexity3.356536865234375
INFO:root:current mean train loss 1532.436349119194
INFO:root:current train perplexity3.357703447341919
INFO:root:current mean train loss 1533.2956394016528
INFO:root:current train perplexity3.3603217601776123
INFO:root:current mean train loss 1534.5094168409355
INFO:root:current train perplexity3.362313985824585
INFO:root:current mean train loss 1535.38356278958
INFO:root:current train perplexity3.363109827041626
INFO:root:current mean train loss 1537.3110424732242
INFO:root:current train perplexity3.364550828933716
INFO:root:current mean train loss 1538.5678699995624
INFO:root:current train perplexity3.3671224117279053
INFO:root:current mean train loss 1539.5508715237484
INFO:root:current train perplexity3.37011456489563

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.59s/it]
INFO:root:final mean train loss: 1539.9758555017454
INFO:root:final train perplexity: 3.3714821338653564
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.74s/it]
INFO:root:eval mean loss: 2129.5058403285684
INFO:root:eval perplexity: 5.602581024169922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.85s/it]
INFO:root:eval mean loss: 2641.477750304743
INFO:root:eval perplexity: 8.772610664367676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [6:40:10<14:00:35, 357.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1588.8781127929688
INFO:root:current train perplexity3.509711742401123
INFO:root:current mean train loss 1520.4926578297334
INFO:root:current train perplexity3.311664342880249
INFO:root:current mean train loss 1518.8213875647818
INFO:root:current train perplexity3.314260244369507
INFO:root:current mean train loss 1520.4655959779852
INFO:root:current train perplexity3.311830997467041
INFO:root:current mean train loss 1523.0045703489388
INFO:root:current train perplexity3.3167521953582764
INFO:root:current mean train loss 1523.4349061274434
INFO:root:current train perplexity3.3188211917877197
INFO:root:current mean train loss 1525.7890996077529
INFO:root:current train perplexity3.3213589191436768
INFO:root:current mean train loss 1527.6708572257278
INFO:root:current train perplexity3.324732780456543
INFO:root:current mean train loss 1530.0594176485056
INFO:root:current train perplexity3.329517364501953
INFO:root:current mean train loss 1530.195718363489
INFO:root:current train perplexity3.3339099884033203
INFO:root:current mean train loss 1529.631841297873
INFO:root:current train perplexity3.336331605911255
INFO:root:current mean train loss 1529.7653200457619
INFO:root:current train perplexity3.338733673095703
INFO:root:current mean train loss 1530.0654893008723
INFO:root:current train perplexity3.3414864540100098
INFO:root:current mean train loss 1530.6310123847927
INFO:root:current train perplexity3.342904567718506
INFO:root:current mean train loss 1531.1169295154523
INFO:root:current train perplexity3.344202756881714
INFO:root:current mean train loss 1531.2654444139584
INFO:root:current train perplexity3.3465843200683594
INFO:root:current mean train loss 1531.7179571525583
INFO:root:current train perplexity3.3488831520080566
INFO:root:current mean train loss 1532.3845891179265
INFO:root:current train perplexity3.3502111434936523
INFO:root:current mean train loss 1532.8256627970875
INFO:root:current train perplexity3.3503105640411377
INFO:root:current mean train loss 1532.4095185577683
INFO:root:current train perplexity3.3508121967315674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.18s/it]
INFO:root:final mean train loss: 1532.1465652011827
INFO:root:final train perplexity: 3.3507139682769775
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it]
INFO:root:eval mean loss: 2137.6391566932625
INFO:root:eval perplexity: 5.6395769119262695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.56s/it]
INFO:root:eval mean loss: 2650.111074461159
INFO:root:eval perplexity: 8.835100173950195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [6:46:04<13:52:07, 356.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1511.4247468647204
INFO:root:current train perplexity3.2848126888275146
INFO:root:current mean train loss 1507.308238822873
INFO:root:current train perplexity3.2774603366851807
INFO:root:current mean train loss 1508.3725875784817
INFO:root:current train perplexity3.2831315994262695
INFO:root:current mean train loss 1512.0724550444504
INFO:root:current train perplexity3.2910544872283936
INFO:root:current mean train loss 1517.085938665349
INFO:root:current train perplexity3.2998838424682617
INFO:root:current mean train loss 1518.7269456455474
INFO:root:current train perplexity3.3051669597625732
INFO:root:current mean train loss 1519.8727339095947
INFO:root:current train perplexity3.3054921627044678
INFO:root:current mean train loss 1521.4526591294332
INFO:root:current train perplexity3.3054051399230957
INFO:root:current mean train loss 1521.4083517258566
INFO:root:current train perplexity3.3071436882019043
INFO:root:current mean train loss 1520.9754714384692
INFO:root:current train perplexity3.3101751804351807
INFO:root:current mean train loss 1520.7881387667521
INFO:root:current train perplexity3.31241774559021
INFO:root:current mean train loss 1519.7446669782241
INFO:root:current train perplexity3.3131470680236816
INFO:root:current mean train loss 1520.3945108214982
INFO:root:current train perplexity3.3157687187194824
INFO:root:current mean train loss 1521.4542583381706
INFO:root:current train perplexity3.317431926727295
INFO:root:current mean train loss 1521.7533248030359
INFO:root:current train perplexity3.3183279037475586
INFO:root:current mean train loss 1522.3745202371524
INFO:root:current train perplexity3.3202877044677734
INFO:root:current mean train loss 1522.3622510791047
INFO:root:current train perplexity3.322511672973633
INFO:root:current mean train loss 1523.0275380399214
INFO:root:current train perplexity3.3240509033203125
INFO:root:current mean train loss 1523.8174159640857
INFO:root:current train perplexity3.3273813724517822
INFO:root:current mean train loss 1524.51520039378
INFO:root:current train perplexity3.3294832706451416

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.26s/it]
INFO:root:final mean train loss: 1524.501043045575
INFO:root:final train perplexity: 3.330557346343994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.35s/it]
INFO:root:eval mean loss: 2140.383489946947
INFO:root:eval perplexity: 5.652115345001221
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.74s/it]
INFO:root:eval mean loss: 2652.535468351756
INFO:root:eval perplexity: 8.8527250289917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [6:51:55<13:42:25, 355.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1492.26759507921
INFO:root:current train perplexity3.261972665786743
INFO:root:current mean train loss 1501.1745039995978
INFO:root:current train perplexity3.2835702896118164
INFO:root:current mean train loss 1507.483178607488
INFO:root:current train perplexity3.290501594543457
INFO:root:current mean train loss 1508.1534122285389
INFO:root:current train perplexity3.2891180515289307
INFO:root:current mean train loss 1506.9568478645535
INFO:root:current train perplexity3.2858824729919434
INFO:root:current mean train loss 1509.558947662809
INFO:root:current train perplexity3.2874653339385986
INFO:root:current mean train loss 1510.8027539523143
INFO:root:current train perplexity3.2897207736968994
INFO:root:current mean train loss 1510.4498891415803
INFO:root:current train perplexity3.290105104446411
INFO:root:current mean train loss 1511.6066441878177
INFO:root:current train perplexity3.2946205139160156
INFO:root:current mean train loss 1511.9905469845503
INFO:root:current train perplexity3.2957327365875244
INFO:root:current mean train loss 1512.5917398460124
INFO:root:current train perplexity3.2988669872283936
INFO:root:current mean train loss 1513.4831656872386
INFO:root:current train perplexity3.300863027572632
INFO:root:current mean train loss 1514.5749323082587
INFO:root:current train perplexity3.302309274673462
INFO:root:current mean train loss 1514.2794056052933
INFO:root:current train perplexity3.3031654357910156
INFO:root:current mean train loss 1514.8133929154335
INFO:root:current train perplexity3.3047406673431396
INFO:root:current mean train loss 1515.9214990139008
INFO:root:current train perplexity3.3052167892456055
INFO:root:current mean train loss 1516.6459687100064
INFO:root:current train perplexity3.306278705596924
INFO:root:current mean train loss 1516.4581930274787
INFO:root:current train perplexity3.3073549270629883
INFO:root:current mean train loss 1516.981217394727
INFO:root:current train perplexity3.308206558227539
INFO:root:current mean train loss 1517.0383257905314
INFO:root:current train perplexity3.308439016342163

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.78s/it]
INFO:root:final mean train loss: 1516.1917825872947
INFO:root:final train perplexity: 3.3087875843048096
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.30s/it]
INFO:root:eval mean loss: 2145.9346858897106
INFO:root:eval perplexity: 5.677562713623047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.85s/it]
INFO:root:eval mean loss: 2661.608997967226
INFO:root:eval perplexity: 8.919008255004883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [6:57:56<13:40:27, 356.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1478.8679498636498
INFO:root:current train perplexity3.2446277141571045
INFO:root:current mean train loss 1490.2437353196487
INFO:root:current train perplexity3.242734909057617
INFO:root:current mean train loss 1493.591534882195
INFO:root:current train perplexity3.243596076965332
INFO:root:current mean train loss 1496.0635398454099
INFO:root:current train perplexity3.254625082015991
INFO:root:current mean train loss 1500.6966237453435
INFO:root:current train perplexity3.2648098468780518
INFO:root:current mean train loss 1502.4015629856324
INFO:root:current train perplexity3.2653512954711914
INFO:root:current mean train loss 1502.3786830463964
INFO:root:current train perplexity3.2683022022247314
INFO:root:current mean train loss 1502.3048359945635
INFO:root:current train perplexity3.26707124710083
INFO:root:current mean train loss 1502.6524313315138
INFO:root:current train perplexity3.268339157104492
INFO:root:current mean train loss 1504.294046760231
INFO:root:current train perplexity3.2713260650634766
INFO:root:current mean train loss 1505.6726177393089
INFO:root:current train perplexity3.273407220840454
INFO:root:current mean train loss 1507.1794153033188
INFO:root:current train perplexity3.2758967876434326
INFO:root:current mean train loss 1507.0663220770152
INFO:root:current train perplexity3.277735710144043
INFO:root:current mean train loss 1506.9083753587224
INFO:root:current train perplexity3.279236078262329
INFO:root:current mean train loss 1507.106269138071
INFO:root:current train perplexity3.27988862991333
INFO:root:current mean train loss 1507.6865554288827
INFO:root:current train perplexity3.282339096069336
INFO:root:current mean train loss 1508.39035848351
INFO:root:current train perplexity3.283968448638916
INFO:root:current mean train loss 1508.265835855052
INFO:root:current train perplexity3.285074472427368
INFO:root:current mean train loss 1508.251032426302
INFO:root:current train perplexity3.2871596813201904
INFO:root:current mean train loss 1508.8700736222117
INFO:root:current train perplexity3.2887537479400635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.21s/it]
INFO:root:final mean train loss: 1508.7268281245076
INFO:root:final train perplexity: 3.2893521785736084
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.69s/it]
INFO:root:eval mean loss: 2147.2942526526485
INFO:root:eval perplexity: 5.683812141418457
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.25s/it]
INFO:root:eval mean loss: 2664.162380786652
INFO:root:eval perplexity: 8.937753677368164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [7:03:50<13:32:41, 355.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1488.4879185267857
INFO:root:current train perplexity3.2399837970733643
INFO:root:current mean train loss 1492.9177210190717
INFO:root:current train perplexity3.24306058883667
INFO:root:current mean train loss 1490.626087782118
INFO:root:current train perplexity3.2356226444244385
INFO:root:current mean train loss 1490.9989637220228
INFO:root:current train perplexity3.2402446269989014
INFO:root:current mean train loss 1491.0003140063995
INFO:root:current train perplexity3.2412161827087402
INFO:root:current mean train loss 1493.3998635810717
INFO:root:current train perplexity3.244935989379883
INFO:root:current mean train loss 1494.9561547122785
INFO:root:current train perplexity3.25007963180542
INFO:root:current mean train loss 1496.7721399084314
INFO:root:current train perplexity3.2518653869628906
INFO:root:current mean train loss 1497.6736110643408
INFO:root:current train perplexity3.2577626705169678
INFO:root:current mean train loss 1497.4468879621054
INFO:root:current train perplexity3.2581992149353027
INFO:root:current mean train loss 1497.9648137458016
INFO:root:current train perplexity3.258988857269287
INFO:root:current mean train loss 1498.382849851429
INFO:root:current train perplexity3.261382818222046
INFO:root:current mean train loss 1499.090516290139
INFO:root:current train perplexity3.262709379196167
INFO:root:current mean train loss 1499.9981503229071
INFO:root:current train perplexity3.2620463371276855
INFO:root:current mean train loss 1499.5533305265465
INFO:root:current train perplexity3.2639307975769043
INFO:root:current mean train loss 1499.8364926477907
INFO:root:current train perplexity3.2656595706939697
INFO:root:current mean train loss 1500.2779044693816
INFO:root:current train perplexity3.265623092651367
INFO:root:current mean train loss 1500.4240688862774
INFO:root:current train perplexity3.2672669887542725
INFO:root:current mean train loss 1500.5999470593458
INFO:root:current train perplexity3.2676308155059814
INFO:root:current mean train loss 1501.5360658907043
INFO:root:current train perplexity3.2692739963531494

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 309.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 309.00s/it]
INFO:root:final mean train loss: 1501.1705362278587
INFO:root:final train perplexity: 3.269794225692749
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.61s/it]
INFO:root:eval mean loss: 2150.451152395695
INFO:root:eval perplexity: 5.69835090637207
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.51s/it]
INFO:root:eval mean loss: 2668.627497246925
INFO:root:eval perplexity: 8.970623970031738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [7:09:42<13:24:01, 354.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1473.553813364314
INFO:root:current train perplexity3.232773542404175
INFO:root:current mean train loss 1475.3155295632102
INFO:root:current train perplexity3.2258565425872803
INFO:root:current mean train loss 1481.794395739193
INFO:root:current train perplexity3.2337167263031006
INFO:root:current mean train loss 1483.328840073381
INFO:root:current train perplexity3.2317872047424316
INFO:root:current mean train loss 1482.2440777099107
INFO:root:current train perplexity3.230234384536743
INFO:root:current mean train loss 1482.8596149815003
INFO:root:current train perplexity3.231881856918335
INFO:root:current mean train loss 1485.9210764789166
INFO:root:current train perplexity3.233198642730713
INFO:root:current mean train loss 1486.767466602059
INFO:root:current train perplexity3.235771894454956
INFO:root:current mean train loss 1487.7622238210788
INFO:root:current train perplexity3.238370656967163
INFO:root:current mean train loss 1488.376306535746
INFO:root:current train perplexity3.239583730697632
INFO:root:current mean train loss 1489.8756096777613
INFO:root:current train perplexity3.2409324645996094
INFO:root:current mean train loss 1490.527961814514
INFO:root:current train perplexity3.2400784492492676
INFO:root:current mean train loss 1491.0113751130598
INFO:root:current train perplexity3.2414050102233887
INFO:root:current mean train loss 1491.1655217110895
INFO:root:current train perplexity3.242048501968384
INFO:root:current mean train loss 1490.252781922495
INFO:root:current train perplexity3.2419991493225098
INFO:root:current mean train loss 1490.5911006050135
INFO:root:current train perplexity3.242946147918701
INFO:root:current mean train loss 1491.6204435284067
INFO:root:current train perplexity3.244938373565674
INFO:root:current mean train loss 1492.8395695528907
INFO:root:current train perplexity3.247222661972046
INFO:root:current mean train loss 1492.9661978442136
INFO:root:current train perplexity3.2484419345855713

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.94s/it]
INFO:root:final mean train loss: 1493.312562851129
INFO:root:final train perplexity: 3.249579429626465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.20s/it]
INFO:root:eval mean loss: 2161.7809530488144
INFO:root:eval perplexity: 5.750833511352539
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.14s/it]
INFO:root:eval mean loss: 2683.4325202238474
INFO:root:eval perplexity: 9.080476760864258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_roberta_final/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [7:15:33<13:15:41, 353.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1481.9688110351562
INFO:root:current train perplexity3.1721389293670654
INFO:root:current mean train loss 1472.6224107008713
INFO:root:current train perplexity3.2100088596343994
INFO:root:current mean train loss 1470.6132004681756
INFO:root:current train perplexity3.190964937210083
INFO:root:current mean train loss 1472.907176770662
INFO:root:current train perplexity3.2013020515441895
slurmstepd: error: *** JOB 26260766 ON ga011 CANCELLED AT 2022-10-25T09:45:16 ***
