INFO:root:Output: distilbert_roberta_not_concat_1e-5
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11510.291577888258
INFO:root:current train perplexity8798.1044921875
INFO:root:current mean train loss 9857.57353417478
INFO:root:current train perplexity2407.989990234375
INFO:root:current mean train loss 9107.761217404368
INFO:root:current train perplexity1337.4207763671875
INFO:root:current mean train loss 8674.1084180177
INFO:root:current train perplexity941.2748413085938
INFO:root:current mean train loss 8355.041096842122
INFO:root:current train perplexity740.42578125
INFO:root:current mean train loss 8107.429227749374
INFO:root:current train perplexity603.7459106445312
INFO:root:current mean train loss 7870.93178522331
INFO:root:current train perplexity500.21826171875
INFO:root:current mean train loss 7632.941807141739
INFO:root:current train perplexity415.1242370605469
INFO:root:current mean train loss 7399.215543855152
INFO:root:current train perplexity346.0990295410156
INFO:root:current mean train loss 7188.914176872185
INFO:root:current train perplexity291.166259765625
INFO:root:current mean train loss 6982.523512586015
INFO:root:current train perplexity247.93589782714844
INFO:root:current mean train loss 6794.884691507245
INFO:root:current train perplexity213.42807006835938
INFO:root:current mean train loss 6623.566276567913
INFO:root:current train perplexity186.1982421875
INFO:root:current mean train loss 6461.593820676879
INFO:root:current train perplexity163.88742065429688
INFO:root:current mean train loss 6315.571327499583
INFO:root:current train perplexity145.8081512451172
INFO:root:current mean train loss 6179.072328377843
INFO:root:current train perplexity130.86477661132812
INFO:root:current mean train loss 6052.47251384661
INFO:root:current train perplexity118.29837799072266
INFO:root:current mean train loss 5933.209679205462
INFO:root:current train perplexity107.70439910888672
INFO:root:current mean train loss 5822.239017143069
INFO:root:current train perplexity98.72692108154297

100%|██████████| 1/1 [07:25<00:00, 445.59s/it][A100%|██████████| 1/1 [07:25<00:00, 445.59s/it]
INFO:root:final mean train loss: 5731.235778900931
INFO:root:final train perplexity: 92.11878967285156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.11s/it][A100%|██████████| 1/1 [00:40<00:00, 40.11s/it]
INFO:root:eval mean loss: 3320.8198389364475
INFO:root:eval perplexity: 14.691312789916992
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:34<00:00, 34.09s/it][A100%|██████████| 1/1 [00:34<00:00, 34.09s/it]
INFO:root:eval mean loss: 3594.7123798343305
INFO:root:eval perplexity: 19.207801818847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/1
  0%|          | 1/200 [08:41<28:49:08, 521.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3680.1388549804688
INFO:root:current train perplexity18.78050994873047
INFO:root:current mean train loss 3643.9129344019398
INFO:root:current train perplexity17.916391372680664
INFO:root:current mean train loss 3607.6048109266494
INFO:root:current train perplexity17.354764938354492
INFO:root:current mean train loss 3595.411699898635
INFO:root:current train perplexity17.037921905517578
INFO:root:current mean train loss 3568.22478719858
INFO:root:current train perplexity16.653547286987305
INFO:root:current mean train loss 3533.9961931095568
INFO:root:current train perplexity16.286741256713867
INFO:root:current mean train loss 3503.6697998046875
INFO:root:current train perplexity15.930788040161133
INFO:root:current mean train loss 3474.3810571425456
INFO:root:current train perplexity15.57811450958252
INFO:root:current mean train loss 3447.756160661286
INFO:root:current train perplexity15.26039981842041
INFO:root:current mean train loss 3423.3664795987993
INFO:root:current train perplexity14.95075798034668
INFO:root:current mean train loss 3399.161445197158
INFO:root:current train perplexity14.670500755310059
INFO:root:current mean train loss 3380.183837453097
INFO:root:current train perplexity14.406277656555176
INFO:root:current mean train loss 3359.306990974828
INFO:root:current train perplexity14.160956382751465
INFO:root:current mean train loss 3337.82162670379
INFO:root:current train perplexity13.926640510559082
INFO:root:current mean train loss 3317.7920411535574
INFO:root:current train perplexity13.715789794921875
INFO:root:current mean train loss 3299.401117410383
INFO:root:current train perplexity13.51164436340332
INFO:root:current mean train loss 3282.3606448031887
INFO:root:current train perplexity13.319452285766602
INFO:root:current mean train loss 3265.580740121695
INFO:root:current train perplexity13.133712768554688
INFO:root:current mean train loss 3247.3856470049213
INFO:root:current train perplexity12.959259033203125
INFO:root:current mean train loss 3231.5564475806123
INFO:root:current train perplexity12.7905912399292

100%|██████████| 1/1 [07:25<00:00, 445.46s/it][A100%|██████████| 1/1 [07:25<00:00, 445.46s/it]
INFO:root:final mean train loss: 3218.923791620867
INFO:root:final train perplexity: 12.684332847595215
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.61s/it][A100%|██████████| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 2633.0373297075853
INFO:root:eval perplexity: 8.420696258544922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.57s/it][A100%|██████████| 1/1 [00:37<00:00, 37.57s/it]
INFO:root:eval mean loss: 2962.359745539672
INFO:root:eval perplexity: 11.420851707458496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/2
  1%|          | 2/200 [17:27<28:50:26, 524.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2873.852901574337
INFO:root:current train perplexity9.893863677978516
INFO:root:current mean train loss 2866.7766957677395
INFO:root:current train perplexity9.643075942993164
INFO:root:current mean train loss 2855.979439796808
INFO:root:current train perplexity9.540961265563965
INFO:root:current mean train loss 2848.891061227243
INFO:root:current train perplexity9.4542236328125
INFO:root:current mean train loss 2845.2691732146723
INFO:root:current train perplexity9.415441513061523
INFO:root:current mean train loss 2833.9344878635084
INFO:root:current train perplexity9.35694408416748
INFO:root:current mean train loss 2826.376961224452
INFO:root:current train perplexity9.308131217956543
INFO:root:current mean train loss 2819.7879940100615
INFO:root:current train perplexity9.264862060546875
INFO:root:current mean train loss 2814.3790149849
INFO:root:current train perplexity9.201177597045898
INFO:root:current mean train loss 2806.6959844754824
INFO:root:current train perplexity9.138851165771484
INFO:root:current mean train loss 2797.83614429869
INFO:root:current train perplexity9.080353736877441
INFO:root:current mean train loss 2790.392845322154
INFO:root:current train perplexity9.018417358398438
INFO:root:current mean train loss 2783.9558873729598
INFO:root:current train perplexity8.970065116882324
INFO:root:current mean train loss 2775.1853866176507
INFO:root:current train perplexity8.9262113571167
INFO:root:current mean train loss 2766.740700167372
INFO:root:current train perplexity8.87424373626709
INFO:root:current mean train loss 2759.466745594321
INFO:root:current train perplexity8.822104454040527
INFO:root:current mean train loss 2753.380611945279
INFO:root:current train perplexity8.774484634399414
INFO:root:current mean train loss 2745.008018321958
INFO:root:current train perplexity8.722469329833984
INFO:root:current mean train loss 2738.2488933090817
INFO:root:current train perplexity8.674872398376465
INFO:root:current mean train loss 2732.248444850742
INFO:root:current train perplexity8.63449764251709

100%|██████████| 1/1 [07:32<00:00, 452.23s/it][A100%|██████████| 1/1 [07:32<00:00, 452.23s/it]
INFO:root:final mean train loss: 2728.1373067558625
INFO:root:final train perplexity: 8.610995292663574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.05s/it][A100%|██████████| 1/1 [00:38<00:00, 38.05s/it]
INFO:root:eval mean loss: 2382.5147601188496
INFO:root:eval perplexity: 6.875510215759277
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.91s/it][A100%|██████████| 1/1 [00:35<00:00, 35.91s/it]
INFO:root:eval mean loss: 2728.176074738198
INFO:root:eval perplexity: 9.420720100402832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/3
  2%|▏         | 3/200 [26:16<28:47:53, 526.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2572.4630615234373
INFO:root:current train perplexity7.690502643585205
INFO:root:current mean train loss 2580.996025390625
INFO:root:current train perplexity7.646805286407471
INFO:root:current mean train loss 2568.6529384765627
INFO:root:current train perplexity7.574399471282959
INFO:root:current mean train loss 2562.4378627232145
INFO:root:current train perplexity7.565474510192871
INFO:root:current mean train loss 2570.7057709418405
INFO:root:current train perplexity7.575759410858154
INFO:root:current mean train loss 2566.9102943004264
INFO:root:current train perplexity7.544238567352295
INFO:root:current mean train loss 2559.949619140625
INFO:root:current train perplexity7.516870498657227
INFO:root:current mean train loss 2553.3869833984377
INFO:root:current train perplexity7.484068870544434
INFO:root:current mean train loss 2544.6794436465993
INFO:root:current train perplexity7.445935249328613
INFO:root:current mean train loss 2541.362046669408
INFO:root:current train perplexity7.4274492263793945
INFO:root:current mean train loss 2538.1346234421503
INFO:root:current train perplexity7.408599853515625
INFO:root:current mean train loss 2530.4850778065556
INFO:root:current train perplexity7.382987976074219
INFO:root:current mean train loss 2524.607752050781
INFO:root:current train perplexity7.357570648193359
INFO:root:current mean train loss 2520.4689868164064
INFO:root:current train perplexity7.327780246734619
INFO:root:current mean train loss 2517.216058139143
INFO:root:current train perplexity7.312870979309082
INFO:root:current mean train loss 2514.686723396547
INFO:root:current train perplexity7.289159297943115
INFO:root:current mean train loss 2510.9930608575996
INFO:root:current train perplexity7.263728141784668
INFO:root:current mean train loss 2509.921944824219
INFO:root:current train perplexity7.249507427215576
INFO:root:current mean train loss 2507.2439040065456
INFO:root:current train perplexity7.230268955230713
INFO:root:current mean train loss 2503.5683106720753
INFO:root:current train perplexity7.208869934082031

100%|██████████| 1/1 [07:26<00:00, 446.38s/it][A100%|██████████| 1/1 [07:26<00:00, 446.38s/it]
INFO:root:final mean train loss: 2501.969274539149
INFO:root:final train perplexity: 7.203368186950684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.19s/it][A100%|██████████| 1/1 [00:39<00:00, 39.19s/it]
INFO:root:eval mean loss: 2246.5915955888463
INFO:root:eval perplexity: 6.159374237060547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.18s/it][A100%|██████████| 1/1 [00:39<00:00, 39.18s/it]
INFO:root:eval mean loss: 2603.3229699101007
INFO:root:eval perplexity: 8.501700401306152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/4
  2%|▏         | 4/200 [35:03<28:40:09, 526.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2389.4469467846316
INFO:root:current train perplexity6.5614471435546875
INFO:root:current mean train loss 2423.4739332370414
INFO:root:current train perplexity6.661097049713135
INFO:root:current mean train loss 2418.8582516788097
INFO:root:current train perplexity6.657289981842041
INFO:root:current mean train loss 2408.584317324272
INFO:root:current train perplexity6.628063678741455
INFO:root:current mean train loss 2403.0787024161036
INFO:root:current train perplexity6.617864608764648
INFO:root:current mean train loss 2395.086880477017
INFO:root:current train perplexity6.5934977531433105
INFO:root:current mean train loss 2389.538851118874
INFO:root:current train perplexity6.574178695678711
INFO:root:current mean train loss 2386.3861055336843
INFO:root:current train perplexity6.566106796264648
INFO:root:current mean train loss 2383.9504987283285
INFO:root:current train perplexity6.560448169708252
INFO:root:current mean train loss 2380.9562754744456
INFO:root:current train perplexity6.542966365814209
INFO:root:current mean train loss 2378.368401338815
INFO:root:current train perplexity6.527168273925781
INFO:root:current mean train loss 2377.1894060541854
INFO:root:current train perplexity6.517314434051514
INFO:root:current mean train loss 2373.45591402242
INFO:root:current train perplexity6.5042619705200195
INFO:root:current mean train loss 2372.2164963873615
INFO:root:current train perplexity6.494770526885986
INFO:root:current mean train loss 2370.0831154041252
INFO:root:current train perplexity6.487174987792969
INFO:root:current mean train loss 2368.5984803609354
INFO:root:current train perplexity6.481167316436768
INFO:root:current mean train loss 2367.004639330923
INFO:root:current train perplexity6.470428466796875
INFO:root:current mean train loss 2366.6483997804808
INFO:root:current train perplexity6.464169979095459
INFO:root:current mean train loss 2363.9681661939903
INFO:root:current train perplexity6.454543113708496
INFO:root:current mean train loss 2361.074686799973
INFO:root:current train perplexity6.44332218170166

100%|██████████| 1/1 [07:30<00:00, 450.26s/it][A100%|██████████| 1/1 [07:30<00:00, 450.26s/it]
INFO:root:final mean train loss: 2360.1500710137734
INFO:root:final train perplexity: 6.440622806549072
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.59s/it][A100%|██████████| 1/1 [00:38<00:00, 38.59s/it]
INFO:root:eval mean loss: 2154.4056136760305
INFO:root:eval perplexity: 5.716614246368408
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.07s/it][A100%|██████████| 1/1 [00:36<00:00, 36.07s/it]
INFO:root:eval mean loss: 2514.1079521449747
INFO:root:eval perplexity: 7.900452613830566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/5
  2%|▎         | 5/200 [43:50<28:32:19, 526.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2262.667172386533
INFO:root:current train perplexity6.046259880065918
INFO:root:current mean train loss 2292.09576814071
INFO:root:current train perplexity6.109908103942871
INFO:root:current mean train loss 2287.9806909695476
INFO:root:current train perplexity6.091808319091797
INFO:root:current mean train loss 2281.2562987009683
INFO:root:current train perplexity6.074705600738525
INFO:root:current mean train loss 2284.7548828125
INFO:root:current train perplexity6.082057476043701
INFO:root:current mean train loss 2282.3536188831067
INFO:root:current train perplexity6.079996585845947
INFO:root:current mean train loss 2282.077401144463
INFO:root:current train perplexity6.077727794647217
INFO:root:current mean train loss 2281.4624179139428
INFO:root:current train perplexity6.061270713806152
INFO:root:current mean train loss 2281.5796410746166
INFO:root:current train perplexity6.062721252441406
INFO:root:current mean train loss 2277.7896228573186
INFO:root:current train perplexity6.043023109436035
INFO:root:current mean train loss 2276.039302586629
INFO:root:current train perplexity6.026908874511719
INFO:root:current mean train loss 2277.7051839055243
INFO:root:current train perplexity6.032805442810059
INFO:root:current mean train loss 2272.699112651504
INFO:root:current train perplexity6.017494201660156
INFO:root:current mean train loss 2269.6056247777333
INFO:root:current train perplexity6.006805419921875
INFO:root:current mean train loss 2270.240941790558
INFO:root:current train perplexity6.001736164093018
INFO:root:current mean train loss 2269.034184542569
INFO:root:current train perplexity5.992733478546143
INFO:root:current mean train loss 2267.555998523558
INFO:root:current train perplexity5.98604679107666
INFO:root:current mean train loss 2266.2591567103636
INFO:root:current train perplexity5.980309963226318
INFO:root:current mean train loss 2264.316261566875
INFO:root:current train perplexity5.969301700592041

100%|██████████| 1/1 [07:24<00:00, 444.20s/it][A100%|██████████| 1/1 [07:24<00:00, 444.20s/it]
INFO:root:final mean train loss: 2261.0261645372384
INFO:root:final train perplexity: 5.955987453460693
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.48s/it][A100%|██████████| 1/1 [00:37<00:00, 37.48s/it]
INFO:root:eval mean loss: 2086.4243653209496
INFO:root:eval perplexity: 5.410628318786621
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.36s/it][A100%|██████████| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 2455.2610564536235
INFO:root:eval perplexity: 7.527331829071045
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/6
  3%|▎         | 6/200 [52:31<28:16:22, 524.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2142.57275390625
INFO:root:current train perplexity5.849416732788086
INFO:root:current mean train loss 2199.469283000077
INFO:root:current train perplexity5.675562381744385
INFO:root:current mean train loss 2203.7871233432447
INFO:root:current train perplexity5.699462890625
INFO:root:current mean train loss 2208.2633555466155
INFO:root:current train perplexity5.688695430755615
INFO:root:current mean train loss 2206.956595328086
INFO:root:current train perplexity5.698487281799316
INFO:root:current mean train loss 2208.7453769219373
INFO:root:current train perplexity5.693251609802246
INFO:root:current mean train loss 2207.60208835697
INFO:root:current train perplexity5.686119556427002
INFO:root:current mean train loss 2207.042208117867
INFO:root:current train perplexity5.677549362182617
INFO:root:current mean train loss 2206.471169461025
INFO:root:current train perplexity5.673409938812256
INFO:root:current mean train loss 2204.3672362739317
INFO:root:current train perplexity5.669737339019775
INFO:root:current mean train loss 2203.1057653284215
INFO:root:current train perplexity5.663341999053955
INFO:root:current mean train loss 2200.4006425266803
INFO:root:current train perplexity5.656734466552734
INFO:root:current mean train loss 2194.363341929414
INFO:root:current train perplexity5.6429877281188965
INFO:root:current mean train loss 2192.7291782387947
INFO:root:current train perplexity5.637969493865967
INFO:root:current mean train loss 2192.0962720894117
INFO:root:current train perplexity5.6311774253845215
INFO:root:current mean train loss 2189.151003822972
INFO:root:current train perplexity5.621677875518799
INFO:root:current mean train loss 2188.9115273108114
INFO:root:current train perplexity5.619787693023682
INFO:root:current mean train loss 2188.457827254648
INFO:root:current train perplexity5.617360591888428
INFO:root:current mean train loss 2188.1339286876214
INFO:root:current train perplexity5.613333702087402
INFO:root:current mean train loss 2186.207151843397
INFO:root:current train perplexity5.607903003692627

100%|██████████| 1/1 [07:28<00:00, 448.89s/it][A100%|██████████| 1/1 [07:28<00:00, 448.89s/it]
INFO:root:final mean train loss: 2184.740872458623
INFO:root:final train perplexity: 5.6079912185668945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.77s/it][A100%|██████████| 1/1 [00:37<00:00, 37.77s/it]
INFO:root:eval mean loss: 2034.5029894240358
INFO:root:eval perplexity: 5.1880083084106445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.39s/it][A100%|██████████| 1/1 [00:35<00:00, 35.39s/it]
INFO:root:eval mean loss: 2404.1151058115856
INFO:root:eval perplexity: 7.217380523681641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/7
  4%|▎         | 7/200 [1:01:15<28:07:26, 524.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2109.780815972222
INFO:root:current train perplexity5.305215358734131
INFO:root:current mean train loss 2124.480939444849
INFO:root:current train perplexity5.389625072479248
INFO:root:current mean train loss 2125.2827243629945
INFO:root:current train perplexity5.38734769821167
INFO:root:current mean train loss 2128.115043208284
INFO:root:current train perplexity5.385705947875977
INFO:root:current mean train loss 2138.2869245173256
INFO:root:current train perplexity5.393451690673828
INFO:root:current mean train loss 2135.6601703894185
INFO:root:current train perplexity5.383184909820557
INFO:root:current mean train loss 2136.6679993663406
INFO:root:current train perplexity5.381312847137451
INFO:root:current mean train loss 2134.7682797742755
INFO:root:current train perplexity5.375082969665527
INFO:root:current mean train loss 2134.6336304307856
INFO:root:current train perplexity5.372776031494141
INFO:root:current mean train loss 2135.8442211275787
INFO:root:current train perplexity5.374124050140381
INFO:root:current mean train loss 2136.6513349311995
INFO:root:current train perplexity5.376441478729248
INFO:root:current mean train loss 2136.690511795617
INFO:root:current train perplexity5.379528045654297
INFO:root:current mean train loss 2135.61579271138
INFO:root:current train perplexity5.378850936889648
INFO:root:current mean train loss 2135.59580945028
INFO:root:current train perplexity5.380395412445068
INFO:root:current mean train loss 2134.606030032396
INFO:root:current train perplexity5.378764629364014
INFO:root:current mean train loss 2134.8546591295085
INFO:root:current train perplexity5.382844924926758
INFO:root:current mean train loss 2135.3896626966402
INFO:root:current train perplexity5.383321285247803
INFO:root:current mean train loss 2135.1863757878436
INFO:root:current train perplexity5.3860321044921875
INFO:root:current mean train loss 2135.199323899675
INFO:root:current train perplexity5.386892795562744
INFO:root:current mean train loss 2136.5591532495396
INFO:root:current train perplexity5.393490314483643

100%|██████████| 1/1 [07:22<00:00, 442.65s/it][A100%|██████████| 1/1 [07:22<00:00, 442.65s/it]
INFO:root:final mean train loss: 2135.9190462682805
INFO:root:final train perplexity: 5.39602518081665
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.37s/it][A100%|██████████| 1/1 [00:40<00:00, 40.38s/it]
INFO:root:eval mean loss: 2027.0271099983377
INFO:root:eval perplexity: 5.156717300415039
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.36s/it][A100%|██████████| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 2400.5682931453625
INFO:root:eval perplexity: 7.196364402770996
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/8
  4%|▍         | 8/200 [1:09:57<27:55:52, 523.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2133.1977294921876
INFO:root:current train perplexity5.3382487297058105
INFO:root:current mean train loss 2133.539234302662
INFO:root:current train perplexity5.343266487121582
INFO:root:current mean train loss 2122.6084264876995
INFO:root:current train perplexity5.320928573608398
INFO:root:current mean train loss 2118.8551229448462
INFO:root:current train perplexity5.318976879119873
INFO:root:current mean train loss 2119.692747339709
INFO:root:current train perplexity5.330692291259766
INFO:root:current mean train loss 2118.6112341194507
INFO:root:current train perplexity5.338830947875977
INFO:root:current mean train loss 2121.8178968534694
INFO:root:current train perplexity5.346363544464111
INFO:root:current mean train loss 2118.4687629544005
INFO:root:current train perplexity5.3350629806518555
INFO:root:current mean train loss 2120.076835001871
INFO:root:current train perplexity5.3481974601745605
INFO:root:current mean train loss 2122.3331425467914
INFO:root:current train perplexity5.350860118865967
INFO:root:current mean train loss 2121.6469127415457
INFO:root:current train perplexity5.34917688369751
INFO:root:current mean train loss 2125.8222136778977
INFO:root:current train perplexity5.355743408203125
INFO:root:current mean train loss 2123.6048007733425
INFO:root:current train perplexity5.349122047424316
INFO:root:current mean train loss 2123.4322293970913
INFO:root:current train perplexity5.344804763793945
INFO:root:current mean train loss 2123.211209372278
INFO:root:current train perplexity5.343440532684326
INFO:root:current mean train loss 2122.3791942882226
INFO:root:current train perplexity5.340526103973389
INFO:root:current mean train loss 2120.215764167622
INFO:root:current train perplexity5.334714412689209
INFO:root:current mean train loss 2119.685731422798
INFO:root:current train perplexity5.331420421600342
INFO:root:current mean train loss 2120.0058768706363
INFO:root:current train perplexity5.328402042388916
INFO:root:current mean train loss 2119.7159989073602
INFO:root:current train perplexity5.326128959655762

100%|██████████| 1/1 [07:25<00:00, 445.51s/it][A100%|██████████| 1/1 [07:25<00:00, 445.51s/it]
INFO:root:final mean train loss: 2118.7132316277716
INFO:root:final train perplexity: 5.323249816894531
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.40s/it][A100%|██████████| 1/1 [00:38<00:00, 38.40s/it]
INFO:root:eval mean loss: 1994.931939740553
INFO:root:eval perplexity: 5.024511814117432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.48s/it][A100%|██████████| 1/1 [00:35<00:00, 35.48s/it]
INFO:root:eval mean loss: 2377.5708475315823
INFO:root:eval perplexity: 7.061583042144775
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/9
  4%|▍         | 9/200 [1:18:39<27:45:20, 523.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2088.0665682279146
INFO:root:current train perplexity5.17453145980835
INFO:root:current mean train loss 2099.9178635446647
INFO:root:current train perplexity5.196167469024658
INFO:root:current mean train loss 2099.2760145399307
INFO:root:current train perplexity5.211021423339844
INFO:root:current mean train loss 2100.7963537736373
INFO:root:current train perplexity5.224584579467773
INFO:root:current mean train loss 2102.122679313727
INFO:root:current train perplexity5.242806911468506
INFO:root:current mean train loss 2106.3936568550444
INFO:root:current train perplexity5.261434555053711
INFO:root:current mean train loss 2108.974468582247
INFO:root:current train perplexity5.278327941894531
INFO:root:current mean train loss 2113.3397679430373
INFO:root:current train perplexity5.293018817901611
INFO:root:current mean train loss 2115.3745845024573
INFO:root:current train perplexity5.306290149688721
INFO:root:current mean train loss 2116.2268874224496
INFO:root:current train perplexity5.31044864654541
INFO:root:current mean train loss 2117.056333012454
INFO:root:current train perplexity5.315914630889893
INFO:root:current mean train loss 2119.293305820889
INFO:root:current train perplexity5.322630882263184
INFO:root:current mean train loss 2120.180151893689
INFO:root:current train perplexity5.331305027008057
INFO:root:current mean train loss 2119.1578786274385
INFO:root:current train perplexity5.329576015472412
INFO:root:current mean train loss 2119.8778261126895
INFO:root:current train perplexity5.332925319671631
INFO:root:current mean train loss 2119.1308129694044
INFO:root:current train perplexity5.330184459686279
INFO:root:current mean train loss 2120.080430887513
INFO:root:current train perplexity5.330470561981201
INFO:root:current mean train loss 2118.695119361355
INFO:root:current train perplexity5.325759410858154
INFO:root:current mean train loss 2119.633437286443
INFO:root:current train perplexity5.328251838684082
INFO:root:current mean train loss 2119.9887803499814
INFO:root:current train perplexity5.327603340148926

100%|██████████| 1/1 [07:24<00:00, 444.58s/it][A100%|██████████| 1/1 [07:24<00:00, 444.58s/it]
INFO:root:final mean train loss: 2118.994256785706
INFO:root:final train perplexity: 5.324429512023926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.27s/it][A100%|██████████| 1/1 [00:39<00:00, 39.27s/it]
INFO:root:eval mean loss: 1998.0195788660794
INFO:root:eval perplexity: 5.037082195281982
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.18s/it][A100%|██████████| 1/1 [00:36<00:00, 36.18s/it]
INFO:root:eval mean loss: 2379.9321505499224
INFO:root:eval perplexity: 7.07530403137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/10
  5%|▌         | 10/200 [1:27:21<27:35:55, 522.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2117.5051605666895
INFO:root:current train perplexity5.266143798828125
INFO:root:current mean train loss 2101.3949891942493
INFO:root:current train perplexity5.236090183258057
INFO:root:current mean train loss 2092.1756083548735
INFO:root:current train perplexity5.201017379760742
INFO:root:current mean train loss 2085.062009403053
INFO:root:current train perplexity5.1810832023620605
INFO:root:current mean train loss 2093.043545005164
INFO:root:current train perplexity5.1920485496521
INFO:root:current mean train loss 2089.4932906380436
INFO:root:current train perplexity5.1843342781066895
INFO:root:current mean train loss 2090.703039787988
INFO:root:current train perplexity5.193277359008789
INFO:root:current mean train loss 2088.760975057654
INFO:root:current train perplexity5.191333293914795
INFO:root:current mean train loss 2088.7695714250394
INFO:root:current train perplexity5.186895370483398
INFO:root:current mean train loss 2087.958043715533
INFO:root:current train perplexity5.179873466491699
INFO:root:current mean train loss 2084.698785851446
INFO:root:current train perplexity5.168533802032471
INFO:root:current mean train loss 2083.1548316244184
INFO:root:current train perplexity5.1705474853515625
INFO:root:current mean train loss 2081.5740149147027
INFO:root:current train perplexity5.170042514801025
INFO:root:current mean train loss 2079.0856167644893
INFO:root:current train perplexity5.164236545562744
INFO:root:current mean train loss 2079.0286098243914
INFO:root:current train perplexity5.161620140075684
INFO:root:current mean train loss 2080.0622480792404
INFO:root:current train perplexity5.15931510925293
INFO:root:current mean train loss 2080.935904967421
INFO:root:current train perplexity5.161405086517334
INFO:root:current mean train loss 2078.747514223365
INFO:root:current train perplexity5.155410289764404
INFO:root:current mean train loss 2077.3032355882574
INFO:root:current train perplexity5.152134895324707
INFO:root:current mean train loss 2077.577386068535
INFO:root:current train perplexity5.152438640594482

100%|██████████| 1/1 [07:25<00:00, 445.02s/it][A100%|██████████| 1/1 [07:25<00:00, 445.02s/it]
INFO:root:final mean train loss: 2077.3843656800577
INFO:root:final train perplexity: 5.152423858642578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.97s/it][A100%|██████████| 1/1 [00:37<00:00, 37.97s/it]
INFO:root:eval mean loss: 1972.9529042345412
INFO:root:eval perplexity: 4.935937404632568
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.42s/it][A100%|██████████| 1/1 [00:35<00:00, 35.42s/it]
INFO:root:eval mean loss: 2363.1212430913397
INFO:root:eval perplexity: 6.978190898895264
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/11
  6%|▌         | 11/200 [1:36:02<27:25:02, 522.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2028.1305598769077
INFO:root:current train perplexity4.982231616973877
INFO:root:current mean train loss 2048.128163327453
INFO:root:current train perplexity5.044351577758789
INFO:root:current mean train loss 2044.3496102286385
INFO:root:current train perplexity5.028234481811523
INFO:root:current mean train loss 2036.8565740239435
INFO:root:current train perplexity5.006550312042236
INFO:root:current mean train loss 2042.6749069151074
INFO:root:current train perplexity5.018826961517334
INFO:root:current mean train loss 2047.9314235648199
INFO:root:current train perplexity5.0362677574157715
INFO:root:current mean train loss 2050.851086319014
INFO:root:current train perplexity5.040839672088623
INFO:root:current mean train loss 2048.191979017573
INFO:root:current train perplexity5.025990962982178
INFO:root:current mean train loss 2046.9439923219702
INFO:root:current train perplexity5.023679256439209
INFO:root:current mean train loss 2045.8767435255688
INFO:root:current train perplexity5.024163722991943
INFO:root:current mean train loss 2047.3065882449212
INFO:root:current train perplexity5.028146266937256
INFO:root:current mean train loss 2047.7830027279524
INFO:root:current train perplexity5.026758670806885
INFO:root:current mean train loss 2049.137007883882
INFO:root:current train perplexity5.0351481437683105
INFO:root:current mean train loss 2048.2313742369283
INFO:root:current train perplexity5.0381760597229
INFO:root:current mean train loss 2048.155018946364
INFO:root:current train perplexity5.03942346572876
INFO:root:current mean train loss 2048.8722654248845
INFO:root:current train perplexity5.039984226226807
INFO:root:current mean train loss 2051.8677726555547
INFO:root:current train perplexity5.047008991241455
INFO:root:current mean train loss 2052.0565101478296
INFO:root:current train perplexity5.049831867218018
INFO:root:current mean train loss 2052.709081591124
INFO:root:current train perplexity5.052452564239502

100%|██████████| 1/1 [07:21<00:00, 441.46s/it][A100%|██████████| 1/1 [07:21<00:00, 441.46s/it]
INFO:root:final mean train loss: 2052.8456080788264
INFO:root:final train perplexity: 5.0536017417907715
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.89s/it][A100%|██████████| 1/1 [00:37<00:00, 37.89s/it]
INFO:root:eval mean loss: 1972.3724092524103
INFO:root:eval perplexity: 4.933619976043701
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.54s/it][A100%|██████████| 1/1 [00:36<00:00, 36.54s/it]
INFO:root:eval mean loss: 2365.42311388381
INFO:root:eval perplexity: 6.9914093017578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/12
  6%|▌         | 12/200 [1:44:40<27:12:41, 521.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1916.816650390625
INFO:root:current train perplexity4.360498905181885
INFO:root:current mean train loss 2034.2620304440989
INFO:root:current train perplexity4.963807582855225
INFO:root:current mean train loss 2028.0674555736223
INFO:root:current train perplexity4.968549728393555
INFO:root:current mean train loss 2032.2841857305848
INFO:root:current train perplexity4.961942195892334
INFO:root:current mean train loss 2027.706262782549
INFO:root:current train perplexity4.955804347991943
INFO:root:current mean train loss 2031.7487411954057
INFO:root:current train perplexity4.967996597290039
INFO:root:current mean train loss 2033.9898744396507
INFO:root:current train perplexity4.974451541900635
INFO:root:current mean train loss 2038.1181937552788
INFO:root:current train perplexity4.98539400100708
INFO:root:current mean train loss 2038.1233778178023
INFO:root:current train perplexity4.983575820922852
INFO:root:current mean train loss 2039.2678890460618
INFO:root:current train perplexity4.99040412902832
INFO:root:current mean train loss 2038.7586123465542
INFO:root:current train perplexity4.990325450897217
INFO:root:current mean train loss 2037.6085010296847
INFO:root:current train perplexity4.988448619842529
INFO:root:current mean train loss 2035.7151876371895
INFO:root:current train perplexity4.985256195068359
INFO:root:current mean train loss 2036.1996371429514
INFO:root:current train perplexity4.987864017486572
INFO:root:current mean train loss 2036.6810170484287
INFO:root:current train perplexity4.9912190437316895
INFO:root:current mean train loss 2037.2378178213885
INFO:root:current train perplexity4.992020130157471
INFO:root:current mean train loss 2039.9573024242875
INFO:root:current train perplexity4.996665000915527
INFO:root:current mean train loss 2039.3162937847503
INFO:root:current train perplexity4.997039318084717
INFO:root:current mean train loss 2039.0384393523814
INFO:root:current train perplexity4.99404764175415
INFO:root:current mean train loss 2038.6254657017703
INFO:root:current train perplexity4.99315881729126

100%|██████████| 1/1 [07:25<00:00, 445.92s/it][A100%|██████████| 1/1 [07:25<00:00, 445.92s/it]
INFO:root:final mean train loss: 2037.3378102605054
INFO:root:final train perplexity: 4.992129802703857
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.61s/it][A100%|██████████| 1/1 [00:38<00:00, 38.61s/it]
INFO:root:eval mean loss: 1958.400608793218
INFO:root:eval perplexity: 4.878152847290039
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.50s/it][A100%|██████████| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2352.6705110157636
INFO:root:eval perplexity: 6.918491840362549
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/13
  6%|▋         | 13/200 [1:53:24<27:06:09, 521.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2007.7986938476563
INFO:root:current train perplexity4.864991664886475
INFO:root:current mean train loss 2034.3117279052735
INFO:root:current train perplexity4.961482524871826
INFO:root:current mean train loss 2029.8071993741123
INFO:root:current train perplexity4.9379353523254395
INFO:root:current mean train loss 2027.2780170440674
INFO:root:current train perplexity4.929501056671143
INFO:root:current mean train loss 2030.9754138764881
INFO:root:current train perplexity4.9423418045043945
INFO:root:current mean train loss 2028.3567685640776
INFO:root:current train perplexity4.932713508605957
INFO:root:current mean train loss 2025.3982740832914
INFO:root:current train perplexity4.929178714752197
INFO:root:current mean train loss 2022.8655809190539
INFO:root:current train perplexity4.920827388763428
INFO:root:current mean train loss 2019.5885783870046
INFO:root:current train perplexity4.912831783294678
INFO:root:current mean train loss 2020.5497042448624
INFO:root:current train perplexity4.914031505584717
INFO:root:current mean train loss 2018.2521882898668
INFO:root:current train perplexity4.907726287841797
INFO:root:current mean train loss 2016.6779670715332
INFO:root:current train perplexity4.908419132232666
INFO:root:current mean train loss 2017.224109086834
INFO:root:current train perplexity4.910909652709961
INFO:root:current mean train loss 2016.4266361120976
INFO:root:current train perplexity4.909671306610107
INFO:root:current mean train loss 2017.9650509256712
INFO:root:current train perplexity4.909110069274902
INFO:root:current mean train loss 2016.5307336104543
INFO:root:current train perplexity4.908534049987793
INFO:root:current mean train loss 2017.8941055109472
INFO:root:current train perplexity4.915276527404785
INFO:root:current mean train loss 2017.2067853350973
INFO:root:current train perplexity4.910640239715576
INFO:root:current mean train loss 2016.5884894402473
INFO:root:current train perplexity4.910472393035889
INFO:root:current mean train loss 2016.8772955576578
INFO:root:current train perplexity4.910808563232422

100%|██████████| 1/1 [07:25<00:00, 445.18s/it][A100%|██████████| 1/1 [07:25<00:00, 445.19s/it]
INFO:root:final mean train loss: 2015.7539975718903
INFO:root:final train perplexity: 4.907813549041748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.67s/it][A100%|██████████| 1/1 [00:38<00:00, 38.67s/it]
INFO:root:eval mean loss: 1950.950338160738
INFO:root:eval perplexity: 4.848830699920654
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.01s/it][A100%|██████████| 1/1 [00:36<00:00, 36.02s/it]
INFO:root:eval mean loss: 2351.060451209968
INFO:root:eval perplexity: 6.909339904785156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/14
  7%|▋         | 14/200 [2:02:06<26:57:49, 521.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1978.8271682326858
INFO:root:current train perplexity4.837090492248535
INFO:root:current mean train loss 1991.206418225365
INFO:root:current train perplexity4.800791263580322
INFO:root:current mean train loss 1999.0448435851793
INFO:root:current train perplexity4.823940277099609
INFO:root:current mean train loss 1994.837666044603
INFO:root:current train perplexity4.82053279876709
INFO:root:current mean train loss 1993.4527537609947
INFO:root:current train perplexity4.816988468170166
INFO:root:current mean train loss 1996.8515784123313
INFO:root:current train perplexity4.828454971313477
INFO:root:current mean train loss 1996.656631349956
INFO:root:current train perplexity4.827349662780762
INFO:root:current mean train loss 1995.7679870688285
INFO:root:current train perplexity4.8323774337768555
INFO:root:current mean train loss 1994.555580348809
INFO:root:current train perplexity4.828436851501465
INFO:root:current mean train loss 1990.6234024031567
INFO:root:current train perplexity4.81587553024292
INFO:root:current mean train loss 1991.6635358437047
INFO:root:current train perplexity4.81676721572876
INFO:root:current mean train loss 1993.3853287373915
INFO:root:current train perplexity4.8245978355407715
INFO:root:current mean train loss 1992.7340777681638
INFO:root:current train perplexity4.821573734283447
INFO:root:current mean train loss 1992.3387879376637
INFO:root:current train perplexity4.820191860198975
INFO:root:current mean train loss 1993.461883396263
INFO:root:current train perplexity4.8238911628723145
INFO:root:current mean train loss 1993.1026886125314
INFO:root:current train perplexity4.821417808532715
INFO:root:current mean train loss 1993.5511552161681
INFO:root:current train perplexity4.823496341705322
INFO:root:current mean train loss 1996.1711647152283
INFO:root:current train perplexity4.827836513519287
INFO:root:current mean train loss 1995.3219408794273
INFO:root:current train perplexity4.82499885559082
INFO:root:current mean train loss 1994.2596399625306
INFO:root:current train perplexity4.823154449462891

100%|██████████| 1/1 [07:22<00:00, 442.42s/it][A100%|██████████| 1/1 [07:22<00:00, 442.42s/it]
INFO:root:final mean train loss: 1993.2075376601995
INFO:root:final train perplexity: 4.821258544921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.12s/it][A100%|██████████| 1/1 [00:39<00:00, 39.12s/it]
INFO:root:eval mean loss: 1941.6939502472574
INFO:root:eval perplexity: 4.812647342681885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:34<00:00, 34.97s/it][A100%|██████████| 1/1 [00:34<00:00, 34.97s/it]
INFO:root:eval mean loss: 2346.921718732685
INFO:root:eval perplexity: 6.885871887207031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/15
  8%|▊         | 15/200 [2:10:45<26:46:22, 520.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1956.5300722475406
INFO:root:current train perplexity4.694610118865967
INFO:root:current mean train loss 1975.607573273894
INFO:root:current train perplexity4.704123020172119
INFO:root:current mean train loss 1973.8550371785802
INFO:root:current train perplexity4.722325801849365
INFO:root:current mean train loss 1984.3557170286017
INFO:root:current train perplexity4.74705171585083
INFO:root:current mean train loss 1975.5582977160484
INFO:root:current train perplexity4.722497463226318
INFO:root:current mean train loss 1978.0756086769518
INFO:root:current train perplexity4.7319769859313965
INFO:root:current mean train loss 1973.0280752386157
INFO:root:current train perplexity4.728704929351807
INFO:root:current mean train loss 1973.3017714118453
INFO:root:current train perplexity4.73055362701416
INFO:root:current mean train loss 1975.1129666402132
INFO:root:current train perplexity4.741514205932617
INFO:root:current mean train loss 1975.1333797302887
INFO:root:current train perplexity4.7410125732421875
INFO:root:current mean train loss 1974.7133281787387
INFO:root:current train perplexity4.740828514099121
INFO:root:current mean train loss 1972.9688221420738
INFO:root:current train perplexity4.736574649810791
INFO:root:current mean train loss 1974.4876398454633
INFO:root:current train perplexity4.739047527313232
INFO:root:current mean train loss 1974.209144941637
INFO:root:current train perplexity4.742672443389893
INFO:root:current mean train loss 1974.4480832610202
INFO:root:current train perplexity4.746052265167236
INFO:root:current mean train loss 1972.887905125759
INFO:root:current train perplexity4.743405818939209
INFO:root:current mean train loss 1972.0625484148277
INFO:root:current train perplexity4.743271350860596
INFO:root:current mean train loss 1971.7522586487582
INFO:root:current train perplexity4.741196155548096
INFO:root:current mean train loss 1972.353137233368
INFO:root:current train perplexity4.742030143737793
INFO:root:current mean train loss 1973.0278349674345
INFO:root:current train perplexity4.743404865264893

100%|██████████| 1/1 [07:27<00:00, 447.83s/it][A100%|██████████| 1/1 [07:27<00:00, 447.83s/it]
INFO:root:final mean train loss: 1972.6939330377545
INFO:root:final train perplexity: 4.743834018707275
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.49s/it][A100%|██████████| 1/1 [00:37<00:00, 37.49s/it]
INFO:root:eval mean loss: 1927.3540480593417
INFO:root:eval perplexity: 4.7571234703063965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.91s/it][A100%|██████████| 1/1 [00:35<00:00, 35.91s/it]
INFO:root:eval mean loss: 2332.437334209469
INFO:root:eval perplexity: 6.8043599128723145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/16
  8%|▊         | 16/200 [2:19:28<26:40:04, 521.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1944.8536376953125
INFO:root:current train perplexity4.613592624664307
INFO:root:current mean train loss 1948.9016598707053
INFO:root:current train perplexity4.644278049468994
INFO:root:current mean train loss 1960.059558150513
INFO:root:current train perplexity4.659214973449707
INFO:root:current mean train loss 1959.109126252948
INFO:root:current train perplexity4.665233135223389
INFO:root:current mean train loss 1955.4586974708898
INFO:root:current train perplexity4.658529281616211
INFO:root:current mean train loss 1953.8000417732733
INFO:root:current train perplexity4.656253337860107
INFO:root:current mean train loss 1951.4208649636737
INFO:root:current train perplexity4.657959938049316
INFO:root:current mean train loss 1954.3564776112598
INFO:root:current train perplexity4.666723251342773
INFO:root:current mean train loss 1953.93571687648
INFO:root:current train perplexity4.668178558349609
INFO:root:current mean train loss 1954.6737000203157
INFO:root:current train perplexity4.668539047241211
INFO:root:current mean train loss 1955.1339268617603
INFO:root:current train perplexity4.6696248054504395
INFO:root:current mean train loss 1954.4938376904756
INFO:root:current train perplexity4.671716690063477
INFO:root:current mean train loss 1954.4088540065954
INFO:root:current train perplexity4.675045967102051
INFO:root:current mean train loss 1956.649872071025
INFO:root:current train perplexity4.683635234832764
INFO:root:current mean train loss 1956.8894789000096
INFO:root:current train perplexity4.682639122009277
INFO:root:current mean train loss 1956.4183199643937
INFO:root:current train perplexity4.681336879730225
INFO:root:current mean train loss 1956.1555737553065
INFO:root:current train perplexity4.6788740158081055
INFO:root:current mean train loss 1956.663143560753
INFO:root:current train perplexity4.682467937469482
INFO:root:current mean train loss 1957.4202461997052
INFO:root:current train perplexity4.682775974273682
INFO:root:current mean train loss 1956.7599406481877
INFO:root:current train perplexity4.6819939613342285

100%|██████████| 1/1 [07:30<00:00, 450.92s/it][A100%|██████████| 1/1 [07:30<00:00, 450.92s/it]
INFO:root:final mean train loss: 1956.054442128207
INFO:root:final train perplexity: 4.68194580078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.75s/it][A100%|██████████| 1/1 [00:37<00:00, 37.75s/it]
INFO:root:eval mean loss: 1922.095269818678
INFO:root:eval perplexity: 4.7369232177734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.81s/it][A100%|██████████| 1/1 [00:35<00:00, 35.81s/it]
INFO:root:eval mean loss: 2329.754454267786
INFO:root:eval perplexity: 6.789368629455566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/17
  8%|▊         | 17/200 [2:28:15<26:35:57, 523.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1920.9365997314453
INFO:root:current train perplexity4.580875396728516
INFO:root:current mean train loss 1941.1485556744515
INFO:root:current train perplexity4.601691246032715
INFO:root:current mean train loss 1942.803825378418
INFO:root:current train perplexity4.608448028564453
INFO:root:current mean train loss 1944.1994726436653
INFO:root:current train perplexity4.606784820556641
INFO:root:current mean train loss 1941.378598572778
INFO:root:current train perplexity4.602841854095459
INFO:root:current mean train loss 1940.9312108876754
INFO:root:current train perplexity4.6079301834106445
INFO:root:current mean train loss 1943.9186557503633
INFO:root:current train perplexity4.614543437957764
INFO:root:current mean train loss 1944.5477742616295
INFO:root:current train perplexity4.618230819702148
INFO:root:current mean train loss 1945.391000971064
INFO:root:current train perplexity4.626012325286865
INFO:root:current mean train loss 1944.6980024947811
INFO:root:current train perplexity4.628629207611084
INFO:root:current mean train loss 1943.999023998485
INFO:root:current train perplexity4.632068157196045
INFO:root:current mean train loss 1942.7268953162813
INFO:root:current train perplexity4.629013538360596
INFO:root:current mean train loss 1942.3612736293248
INFO:root:current train perplexity4.630721092224121
INFO:root:current mean train loss 1942.1942286422686
INFO:root:current train perplexity4.633074760437012
INFO:root:current mean train loss 1944.406750422652
INFO:root:current train perplexity4.6389546394348145
INFO:root:current mean train loss 1943.9208546982004
INFO:root:current train perplexity4.639540672302246
INFO:root:current mean train loss 1944.9010317834068
INFO:root:current train perplexity4.64176082611084
INFO:root:current mean train loss 1945.430274161183
INFO:root:current train perplexity4.641966819763184
INFO:root:current mean train loss 1947.0541321059404
INFO:root:current train perplexity4.645448207855225

100%|██████████| 1/1 [07:26<00:00, 446.96s/it][A100%|██████████| 1/1 [07:26<00:00, 446.96s/it]
INFO:root:final mean train loss: 1945.9427551792778
INFO:root:final train perplexity: 4.6447319984436035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.49s/it][A100%|██████████| 1/1 [00:40<00:00, 40.49s/it]
INFO:root:eval mean loss: 1912.8845271117298
INFO:root:eval perplexity: 4.701747894287109
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.49s/it][A100%|██████████| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2320.317540811309
INFO:root:eval perplexity: 6.736897945404053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/18
  9%|▉         | 18/200 [2:37:01<26:29:55, 524.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1915.6041748046875
INFO:root:current train perplexity4.614956855773926
INFO:root:current mean train loss 1934.7440034412202
INFO:root:current train perplexity4.564806938171387
INFO:root:current mean train loss 1926.7194389529345
INFO:root:current train perplexity4.56319522857666
INFO:root:current mean train loss 1932.5281598200563
INFO:root:current train perplexity4.570290565490723
INFO:root:current mean train loss 1932.9819468557098
INFO:root:current train perplexity4.572251319885254
INFO:root:current mean train loss 1932.4746874516554
INFO:root:current train perplexity4.591536045074463
INFO:root:current mean train loss 1929.9529877970042
INFO:root:current train perplexity4.583430290222168
INFO:root:current mean train loss 1931.4336250900376
INFO:root:current train perplexity4.588493824005127
INFO:root:current mean train loss 1929.1904735115004
INFO:root:current train perplexity4.578011989593506
INFO:root:current mean train loss 1929.275067577046
INFO:root:current train perplexity4.584072589874268
INFO:root:current mean train loss 1930.4149768734453
INFO:root:current train perplexity4.591132640838623
INFO:root:current mean train loss 1928.902034431561
INFO:root:current train perplexity4.590237617492676
INFO:root:current mean train loss 1932.4062058318204
INFO:root:current train perplexity4.5992536544799805
INFO:root:current mean train loss 1933.0679999925167
INFO:root:current train perplexity4.598569393157959
INFO:root:current mean train loss 1932.2403418490046
INFO:root:current train perplexity4.597192764282227
INFO:root:current mean train loss 1933.199180709484
INFO:root:current train perplexity4.596892356872559
INFO:root:current mean train loss 1933.1926605699962
INFO:root:current train perplexity4.597928524017334
INFO:root:current mean train loss 1934.0286587443868
INFO:root:current train perplexity4.597188472747803
INFO:root:current mean train loss 1933.540831336024
INFO:root:current train perplexity4.595815658569336
INFO:root:current mean train loss 1932.9208130203208
INFO:root:current train perplexity4.595433235168457

100%|██████████| 1/1 [07:27<00:00, 447.50s/it][A100%|██████████| 1/1 [07:27<00:00, 447.50s/it]
INFO:root:final mean train loss: 1931.611911513982
INFO:root:final train perplexity: 4.592496871948242
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.96s/it][A100%|██████████| 1/1 [00:38<00:00, 38.96s/it]
INFO:root:eval mean loss: 1899.0935500124667
INFO:root:eval perplexity: 4.649567604064941
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.39s/it][A100%|██████████| 1/1 [00:36<00:00, 36.39s/it]
INFO:root:eval mean loss: 2310.821321528009
INFO:root:eval perplexity: 6.684507846832275
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/19
 10%|▉         | 19/200 [2:45:47<26:22:15, 524.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1940.1867453835227
INFO:root:current train perplexity4.644720077514648
INFO:root:current mean train loss 1904.8170015929174
INFO:root:current train perplexity4.537023544311523
INFO:root:current mean train loss 1906.3789078995987
INFO:root:current train perplexity4.5275983810424805
INFO:root:current mean train loss 1915.2277236843702
INFO:root:current train perplexity4.546695232391357
INFO:root:current mean train loss 1917.726429437574
INFO:root:current train perplexity4.546914100646973
INFO:root:current mean train loss 1910.3794258673072
INFO:root:current train perplexity4.5365190505981445
INFO:root:current mean train loss 1910.115835895109
INFO:root:current train perplexity4.533796787261963
INFO:root:current mean train loss 1908.0540695401771
INFO:root:current train perplexity4.528746604919434
INFO:root:current mean train loss 1909.6049595296818
INFO:root:current train perplexity4.5286407470703125
INFO:root:current mean train loss 1912.7908004793842
INFO:root:current train perplexity4.530429363250732
INFO:root:current mean train loss 1911.8466681015702
INFO:root:current train perplexity4.525412082672119
INFO:root:current mean train loss 1911.6848356685537
INFO:root:current train perplexity4.523582935333252
INFO:root:current mean train loss 1912.954984724229
INFO:root:current train perplexity4.525807857513428
INFO:root:current mean train loss 1912.5766840716894
INFO:root:current train perplexity4.5225701332092285
INFO:root:current mean train loss 1910.8238649864572
INFO:root:current train perplexity4.517846584320068
INFO:root:current mean train loss 1911.3255779652338
INFO:root:current train perplexity4.5178141593933105
INFO:root:current mean train loss 1911.7368007523505
INFO:root:current train perplexity4.518575191497803
INFO:root:current mean train loss 1911.2648153803489
INFO:root:current train perplexity4.517290115356445
INFO:root:current mean train loss 1911.7306712848817
INFO:root:current train perplexity4.519937038421631
INFO:root:current mean train loss 1912.0601033698013
INFO:root:current train perplexity4.520817279815674

100%|██████████| 1/1 [07:20<00:00, 440.99s/it][A100%|██████████| 1/1 [07:20<00:00, 440.99s/it]
INFO:root:final mean train loss: 1911.8365737060917
INFO:root:final train perplexity: 4.5213799476623535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.54s/it][A100%|██████████| 1/1 [00:37<00:00, 37.54s/it]
INFO:root:eval mean loss: 1912.180567531721
INFO:root:eval perplexity: 4.699070453643799
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.13s/it][A100%|██████████| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 2320.9124703914563
INFO:root:eval perplexity: 6.740193843841553
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/20
 10%|█         | 20/200 [2:54:24<26:06:50, 522.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1871.6865735176282
INFO:root:current train perplexity4.419559478759766
INFO:root:current mean train loss 1874.1484664807217
INFO:root:current train perplexity4.437650680541992
INFO:root:current mean train loss 1888.6952752149255
INFO:root:current train perplexity4.447155952453613
INFO:root:current mean train loss 1892.633620900742
INFO:root:current train perplexity4.45325231552124
INFO:root:current mean train loss 1888.1993719635357
INFO:root:current train perplexity4.450929164886475
INFO:root:current mean train loss 1889.7230052487969
INFO:root:current train perplexity4.4514031410217285
INFO:root:current mean train loss 1889.171510699396
INFO:root:current train perplexity4.456232070922852
INFO:root:current mean train loss 1891.5043385341783
INFO:root:current train perplexity4.459057331085205
INFO:root:current mean train loss 1891.818183035049
INFO:root:current train perplexity4.455828666687012
INFO:root:current mean train loss 1918.6721885608026
INFO:root:current train perplexity4.5479888916015625
INFO:root:current mean train loss 1931.1433146589645
INFO:root:current train perplexity4.5926899909973145
INFO:root:current mean train loss 1929.5082169717816
INFO:root:current train perplexity4.585046291351318
INFO:root:current mean train loss 1928.184867852729
INFO:root:current train perplexity4.572938442230225
INFO:root:current mean train loss 1924.2870110076608
INFO:root:current train perplexity4.56242561340332
INFO:root:current mean train loss 1923.094179239598
INFO:root:current train perplexity4.563694953918457
INFO:root:current mean train loss 1926.262260253589
INFO:root:current train perplexity4.570749282836914
INFO:root:current mean train loss 1924.3033635696308
INFO:root:current train perplexity4.564526557922363
INFO:root:current mean train loss 1922.7851247321332
INFO:root:current train perplexity4.559662818908691
INFO:root:current mean train loss 1922.7859213965428
INFO:root:current train perplexity4.560088634490967
INFO:root:current mean train loss 1924.145711724693
INFO:root:current train perplexity4.564244747161865

100%|██████████| 1/1 [07:29<00:00, 449.48s/it][A100%|██████████| 1/1 [07:29<00:00, 449.48s/it]
INFO:root:final mean train loss: 1922.7032043180018
INFO:root:final train perplexity: 4.560321807861328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.69s/it][A100%|██████████| 1/1 [00:37<00:00, 37.69s/it]
INFO:root:eval mean loss: 1888.6520463659408
INFO:root:eval perplexity: 4.610448360443115
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.55s/it][A100%|██████████| 1/1 [00:35<00:00, 35.55s/it]
INFO:root:eval mean loss: 2302.3029482144834
INFO:root:eval perplexity: 6.637857437133789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/21
 10%|█         | 21/200 [3:03:09<26:00:30, 523.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1892.7613372802734
INFO:root:current train perplexity4.431652069091797
INFO:root:current mean train loss 1889.9559036646133
INFO:root:current train perplexity4.444815158843994
INFO:root:current mean train loss 1886.3738570213318
INFO:root:current train perplexity4.42078161239624
INFO:root:current mean train loss 1880.7642122761588
INFO:root:current train perplexity4.4261040687561035
INFO:root:current mean train loss 1881.3211265697814
INFO:root:current train perplexity4.428594589233398
INFO:root:current mean train loss 1883.5193700996235
INFO:root:current train perplexity4.431697845458984
INFO:root:current mean train loss 1882.1526072432355
INFO:root:current train perplexity4.42921781539917
INFO:root:current mean train loss 1883.6137427274512
INFO:root:current train perplexity4.430739402770996
INFO:root:current mean train loss 1884.7544703973788
INFO:root:current train perplexity4.424147605895996
INFO:root:current mean train loss 1884.753776773748
INFO:root:current train perplexity4.426215648651123
INFO:root:current mean train loss 1884.6670654759262
INFO:root:current train perplexity4.425151824951172
INFO:root:current mean train loss 1887.4037881079016
INFO:root:current train perplexity4.431363105773926
INFO:root:current mean train loss 1887.0983952807774
INFO:root:current train perplexity4.431838035583496
INFO:root:current mean train loss 1886.589040480532
INFO:root:current train perplexity4.433105945587158
INFO:root:current mean train loss 1886.518181350205
INFO:root:current train perplexity4.433448314666748
INFO:root:current mean train loss 1886.622940024251
INFO:root:current train perplexity4.433071136474609
INFO:root:current mean train loss 1885.339463312269
INFO:root:current train perplexity4.430994033813477
INFO:root:current mean train loss 1885.4745593233913
INFO:root:current train perplexity4.4317708015441895
INFO:root:current mean train loss 1886.1231976870833
INFO:root:current train perplexity4.4319167137146
INFO:root:current mean train loss 1886.7826884451088
INFO:root:current train perplexity4.431464195251465

100%|██████████| 1/1 [07:21<00:00, 441.32s/it][A100%|██████████| 1/1 [07:21<00:00, 441.32s/it]
INFO:root:final mean train loss: 1886.3562650694969
INFO:root:final train perplexity: 4.431367874145508
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.10s/it][A100%|██████████| 1/1 [00:38<00:00, 38.10s/it]
INFO:root:eval mean loss: 1887.6772776935118
INFO:root:eval perplexity: 4.606812953948975
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.39s/it][A100%|██████████| 1/1 [00:35<00:00, 35.39s/it]
INFO:root:eval mean loss: 2305.4542327231547
INFO:root:eval perplexity: 6.655077934265137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/22
 11%|█         | 22/200 [3:11:46<25:46:29, 521.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1862.452877514983
INFO:root:current train perplexity4.351736068725586
INFO:root:current mean train loss 1852.1949265320177
INFO:root:current train perplexity4.36379337310791
INFO:root:current mean train loss 1851.8851322830815
INFO:root:current train perplexity4.341231822967529
INFO:root:current mean train loss 1856.750760894039
INFO:root:current train perplexity4.35273551940918
INFO:root:current mean train loss 1858.8267889365586
INFO:root:current train perplexity4.350051403045654
INFO:root:current mean train loss 1859.2135152500546
INFO:root:current train perplexity4.351696491241455
INFO:root:current mean train loss 1862.6903614877415
INFO:root:current train perplexity4.355789661407471
INFO:root:current mean train loss 1862.1145062169005
INFO:root:current train perplexity4.350688934326172
INFO:root:current mean train loss 1862.3823705019956
INFO:root:current train perplexity4.355419635772705
INFO:root:current mean train loss 1863.0876055851747
INFO:root:current train perplexity4.355932712554932
INFO:root:current mean train loss 1864.3346813020592
INFO:root:current train perplexity4.357571601867676
INFO:root:current mean train loss 1864.5239030946957
INFO:root:current train perplexity4.357585906982422
INFO:root:current mean train loss 1864.3537527655208
INFO:root:current train perplexity4.357203960418701
INFO:root:current mean train loss 1863.571291374101
INFO:root:current train perplexity4.357857704162598
INFO:root:current mean train loss 1862.6495079563656
INFO:root:current train perplexity4.354823112487793
INFO:root:current mean train loss 1862.8313820966853
INFO:root:current train perplexity4.353426456451416
INFO:root:current mean train loss 1864.0159219672278
INFO:root:current train perplexity4.354971885681152
INFO:root:current mean train loss 1865.7454418821383
INFO:root:current train perplexity4.3595757484436035
INFO:root:current mean train loss 1864.756852296095
INFO:root:current train perplexity4.356637001037598
INFO:root:current mean train loss 1865.058257422469
INFO:root:current train perplexity4.356029033660889

100%|██████████| 1/1 [07:28<00:00, 448.02s/it][A100%|██████████| 1/1 [07:28<00:00, 448.02s/it]
INFO:root:final mean train loss: 1864.4761428871482
INFO:root:final train perplexity: 4.355505466461182
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.95s/it][A100%|██████████| 1/1 [00:37<00:00, 37.95s/it]
INFO:root:eval mean loss: 1882.3535731971688
INFO:root:eval perplexity: 4.587009429931641
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.13s/it][A100%|██████████| 1/1 [00:35<00:00, 35.13s/it]
INFO:root:eval mean loss: 2299.8721123012247
INFO:root:eval perplexity: 6.624605655670166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/23
 12%|█▏        | 23/200 [3:20:29<25:39:35, 521.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1827.0157335069443
INFO:root:current train perplexity4.252731800079346
INFO:root:current mean train loss 1811.536390445107
INFO:root:current train perplexity4.240079879760742
INFO:root:current mean train loss 1822.7028711779365
INFO:root:current train perplexity4.26689338684082
INFO:root:current mean train loss 1824.0546446188903
INFO:root:current train perplexity4.257739067077637
INFO:root:current mean train loss 1828.5470229791135
INFO:root:current train perplexity4.260164260864258
INFO:root:current mean train loss 1835.1501024149231
INFO:root:current train perplexity4.271280765533447
INFO:root:current mean train loss 1837.7731912364131
INFO:root:current train perplexity4.273482799530029
INFO:root:current mean train loss 1839.8388331932358
INFO:root:current train perplexity4.279686450958252
INFO:root:current mean train loss 1837.4911042288447
INFO:root:current train perplexity4.275623798370361
INFO:root:current mean train loss 1840.6448566475299
INFO:root:current train perplexity4.2797932624816895
INFO:root:current mean train loss 1840.1955509290783
INFO:root:current train perplexity4.277792930603027
INFO:root:current mean train loss 1841.3580974675026
INFO:root:current train perplexity4.278869152069092
INFO:root:current mean train loss 1842.3793319820434
INFO:root:current train perplexity4.280669689178467
INFO:root:current mean train loss 1840.7895268062894
INFO:root:current train perplexity4.278661251068115
INFO:root:current mean train loss 1841.8792095414744
INFO:root:current train perplexity4.280072212219238
INFO:root:current mean train loss 1842.3913904346011
INFO:root:current train perplexity4.281810283660889
INFO:root:current mean train loss 1843.3583099545813
INFO:root:current train perplexity4.283507823944092
INFO:root:current mean train loss 1844.8581138568218
INFO:root:current train perplexity4.285679340362549
INFO:root:current mean train loss 1845.4672459516576
INFO:root:current train perplexity4.288390636444092

100%|██████████| 1/1 [07:22<00:00, 442.53s/it][A100%|██████████| 1/1 [07:22<00:00, 442.53s/it]
INFO:root:final mean train loss: 1845.4289328863208
INFO:root:final train perplexity: 4.290523052215576
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.54s/it][A100%|██████████| 1/1 [00:39<00:00, 39.54s/it]
INFO:root:eval mean loss: 1873.5100972060616
INFO:root:eval perplexity: 4.554300785064697
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.54s/it][A100%|██████████| 1/1 [00:36<00:00, 36.54s/it]
INFO:root:eval mean loss: 2291.4885665136026
INFO:root:eval perplexity: 6.579104423522949
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/24
 12%|█▏        | 24/200 [3:29:10<25:29:59, 521.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1663.9955182756696
INFO:root:current train perplexity3.9127886295318604
INFO:root:current mean train loss 1808.618621540961
INFO:root:current train perplexity4.203029632568359
INFO:root:current mean train loss 1810.3164463503924
INFO:root:current train perplexity4.199754238128662
INFO:root:current mean train loss 1821.3203466955924
INFO:root:current train perplexity4.214810848236084
INFO:root:current mean train loss 1824.7698314769848
INFO:root:current train perplexity4.21721076965332
INFO:root:current mean train loss 1826.6663437943016
INFO:root:current train perplexity4.221667289733887
INFO:root:current mean train loss 1829.4683467858706
INFO:root:current train perplexity4.233222484588623
INFO:root:current mean train loss 1834.4887538192295
INFO:root:current train perplexity4.247511386871338
INFO:root:current mean train loss 1833.6673804830293
INFO:root:current train perplexity4.248310089111328
INFO:root:current mean train loss 1836.0139419908955
INFO:root:current train perplexity4.253834247589111
INFO:root:current mean train loss 1835.9156814166072
INFO:root:current train perplexity4.256160259246826
INFO:root:current mean train loss 1835.1745914228347
INFO:root:current train perplexity4.253657341003418
INFO:root:current mean train loss 1837.2963655814713
INFO:root:current train perplexity4.256597518920898
INFO:root:current mean train loss 1836.2782852885007
INFO:root:current train perplexity4.253750801086426
INFO:root:current mean train loss 1836.6317781558168
INFO:root:current train perplexity4.25283670425415
INFO:root:current mean train loss 1836.4707225655275
INFO:root:current train perplexity4.2532639503479
INFO:root:current mean train loss 1835.2864321012562
INFO:root:current train perplexity4.253865718841553
INFO:root:current mean train loss 1835.925384932676
INFO:root:current train perplexity4.256526470184326
INFO:root:current mean train loss 1835.4455253630629
INFO:root:current train perplexity4.256483554840088
INFO:root:current mean train loss 1835.0799671287116
INFO:root:current train perplexity4.254525661468506

100%|██████████| 1/1 [07:28<00:00, 448.10s/it][A100%|██████████| 1/1 [07:28<00:00, 448.10s/it]
INFO:root:final mean train loss: 1834.8785462872404
INFO:root:final train perplexity: 4.254946231842041
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 1867.2568575811724
INFO:root:eval perplexity: 4.531312942504883
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.01s/it][A100%|██████████| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2287.905569522939
INFO:root:eval perplexity: 6.559752464294434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/25
 12%|█▎        | 25/200 [3:37:57<25:25:51, 523.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1890.7229868570964
INFO:root:current train perplexity4.23018217086792
INFO:root:current mean train loss 1833.8682004867062
INFO:root:current train perplexity4.199821949005127
INFO:root:current mean train loss 1831.4295185634069
INFO:root:current train perplexity4.206227779388428
INFO:root:current mean train loss 1830.512599238643
INFO:root:current train perplexity4.209946155548096
INFO:root:current mean train loss 1834.9570560095444
INFO:root:current train perplexity4.2308149337768555
INFO:root:current mean train loss 1835.1099795275973
INFO:root:current train perplexity4.240650653839111
INFO:root:current mean train loss 1834.5360555404272
INFO:root:current train perplexity4.243833065032959
INFO:root:current mean train loss 1834.3778170590901
INFO:root:current train perplexity4.242519855499268
INFO:root:current mean train loss 1835.9324589701532
INFO:root:current train perplexity4.245059490203857
INFO:root:current mean train loss 1835.8444215188294
INFO:root:current train perplexity4.247710227966309
INFO:root:current mean train loss 1834.1348457336426
INFO:root:current train perplexity4.244710445404053
INFO:root:current mean train loss 1832.1028561846642
INFO:root:current train perplexity4.241356372833252
INFO:root:current mean train loss 1832.3597327338325
INFO:root:current train perplexity4.240393161773682
INFO:root:current mean train loss 1831.0121542939246
INFO:root:current train perplexity4.240758895874023
INFO:root:current mean train loss 1831.0346072764878
INFO:root:current train perplexity4.239490985870361
INFO:root:current mean train loss 1830.834669778979
INFO:root:current train perplexity4.2401885986328125
INFO:root:current mean train loss 1830.6480001816021
INFO:root:current train perplexity4.238528251647949
INFO:root:current mean train loss 1829.4547224642229
INFO:root:current train perplexity4.236356735229492
INFO:root:current mean train loss 1830.9492247732062
INFO:root:current train perplexity4.240596771240234
INFO:root:current mean train loss 1830.1578917949462
INFO:root:current train perplexity4.23713493347168

100%|██████████| 1/1 [07:30<00:00, 450.65s/it][A100%|██████████| 1/1 [07:30<00:00, 450.65s/it]
INFO:root:final mean train loss: 1830.0130631855143
INFO:root:final train perplexity: 4.238639831542969
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.43s/it][A100%|██████████| 1/1 [00:39<00:00, 39.43s/it]
INFO:root:eval mean loss: 1877.908261995789
INFO:root:eval perplexity: 4.570538520812988
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.25s/it][A100%|██████████| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2297.3925798564937
INFO:root:eval perplexity: 6.611114501953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/26
 13%|█▎        | 26/200 [3:46:47<25:22:53, 525.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1807.562380907012
INFO:root:current train perplexity4.19447660446167
INFO:root:current mean train loss 1801.7013095287566
INFO:root:current train perplexity4.158176898956299
INFO:root:current mean train loss 1810.3691355598419
INFO:root:current train perplexity4.186573505401611
INFO:root:current mean train loss 1815.5621409486116
INFO:root:current train perplexity4.195867538452148
INFO:root:current mean train loss 1812.8692961885274
INFO:root:current train perplexity4.18398380279541
INFO:root:current mean train loss 1816.361700202533
INFO:root:current train perplexity4.192904949188232
INFO:root:current mean train loss 1818.2499969530031
INFO:root:current train perplexity4.199570178985596
INFO:root:current mean train loss 1817.9966042707806
INFO:root:current train perplexity4.1946024894714355
INFO:root:current mean train loss 1819.285226937565
INFO:root:current train perplexity4.19813871383667
INFO:root:current mean train loss 1818.321348865278
INFO:root:current train perplexity4.197803974151611
INFO:root:current mean train loss 1818.0924207117555
INFO:root:current train perplexity4.20098876953125
INFO:root:current mean train loss 1819.213516596427
INFO:root:current train perplexity4.2000813484191895
INFO:root:current mean train loss 1821.4031830940585
INFO:root:current train perplexity4.208280563354492
INFO:root:current mean train loss 1822.3966761737684
INFO:root:current train perplexity4.212071418762207
INFO:root:current mean train loss 1824.5860537251692
INFO:root:current train perplexity4.2191057205200195
INFO:root:current mean train loss 1826.4629547891486
INFO:root:current train perplexity4.223549842834473
INFO:root:current mean train loss 1827.9465869854748
INFO:root:current train perplexity4.228419303894043
INFO:root:current mean train loss 1828.895164739531
INFO:root:current train perplexity4.231252193450928
INFO:root:current mean train loss 1830.2574976103128
INFO:root:current train perplexity4.2364373207092285
INFO:root:current mean train loss 1830.6955885260697
INFO:root:current train perplexity4.23942756652832

100%|██████████| 1/1 [07:35<00:00, 455.58s/it][A100%|██████████| 1/1 [07:35<00:00, 455.58s/it]
INFO:root:final mean train loss: 1830.1004795953593
INFO:root:final train perplexity: 4.238932132720947
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.02s/it][A100%|██████████| 1/1 [00:39<00:00, 39.02s/it]
INFO:root:eval mean loss: 1867.1840603875776
INFO:root:eval perplexity: 4.531046390533447
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.62s/it][A100%|██████████| 1/1 [00:36<00:00, 36.62s/it]
INFO:root:eval mean loss: 2292.229224671709
INFO:root:eval perplexity: 6.583110332489014
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/27
 14%|█▎        | 27/200 [3:55:40<25:21:28, 527.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1830.6701744342672
INFO:root:current train perplexity4.28894567489624
INFO:root:current mean train loss 1818.8090248590784
INFO:root:current train perplexity4.229849815368652
INFO:root:current mean train loss 1823.6857895962028
INFO:root:current train perplexity4.23681640625
INFO:root:current mean train loss 1821.9884885649442
INFO:root:current train perplexity4.226805686950684
INFO:root:current mean train loss 1821.4189359839827
INFO:root:current train perplexity4.221886157989502
INFO:root:current mean train loss 1829.311229199919
INFO:root:current train perplexity4.241559982299805
INFO:root:current mean train loss 1827.827621135306
INFO:root:current train perplexity4.24332332611084
INFO:root:current mean train loss 1829.6338096759564
INFO:root:current train perplexity4.2414326667785645
INFO:root:current mean train loss 1827.6819715522108
INFO:root:current train perplexity4.242059707641602
INFO:root:current mean train loss 1829.0701335994586
INFO:root:current train perplexity4.243805408477783
INFO:root:current mean train loss 1832.2718915452579
INFO:root:current train perplexity4.252569675445557
INFO:root:current mean train loss 1833.5789700048574
INFO:root:current train perplexity4.251752853393555
INFO:root:current mean train loss 1834.023124658436
INFO:root:current train perplexity4.252969264984131
INFO:root:current mean train loss 1834.7499526280894
INFO:root:current train perplexity4.2548041343688965
INFO:root:current mean train loss 1835.1083836182645
INFO:root:current train perplexity4.257198333740234
INFO:root:current mean train loss 1835.053669333305
INFO:root:current train perplexity4.258347988128662
INFO:root:current mean train loss 1834.6118862764108
INFO:root:current train perplexity4.259287357330322
INFO:root:current mean train loss 1836.2514143630235
INFO:root:current train perplexity4.261739253997803
INFO:root:current mean train loss 1837.5225346527263
INFO:root:current train perplexity4.264606952667236
INFO:root:current mean train loss 1838.626004991541
INFO:root:current train perplexity4.267195224761963

100%|██████████| 1/1 [07:31<00:00, 451.08s/it][A100%|██████████| 1/1 [07:31<00:00, 451.08s/it]
INFO:root:final mean train loss: 1838.8678820634573
INFO:root:final train perplexity: 4.2683634757995605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.16s/it][A100%|██████████| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 1865.5538728321699
INFO:root:eval perplexity: 4.5250725746154785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.77s/it][A100%|██████████| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2286.870301591589
INFO:root:eval perplexity: 6.554171085357666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/28
 14%|█▍        | 28/200 [4:04:30<25:14:11, 528.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1857.9080436197917
INFO:root:current train perplexity4.319147109985352
INFO:root:current mean train loss 1848.5421728515626
INFO:root:current train perplexity4.300769805908203
INFO:root:current mean train loss 1851.000643643466
INFO:root:current train perplexity4.302651882171631
INFO:root:current mean train loss 1846.048390625
INFO:root:current train perplexity4.286183834075928
INFO:root:current mean train loss 1840.485023386102
INFO:root:current train perplexity4.264669418334961
INFO:root:current mean train loss 1839.8795579993207
INFO:root:current train perplexity4.256099700927734
INFO:root:current mean train loss 1840.7425911458333
INFO:root:current train perplexity4.2597808837890625
INFO:root:current mean train loss 1840.7285620904738
INFO:root:current train perplexity4.261641979217529
INFO:root:current mean train loss 1837.7203618861606
INFO:root:current train perplexity4.259629726409912
INFO:root:current mean train loss 1841.2261790114183
INFO:root:current train perplexity4.266172409057617
INFO:root:current mean train loss 1843.50419410883
INFO:root:current train perplexity4.276224136352539
INFO:root:current mean train loss 1842.3724404712434
INFO:root:current train perplexity4.275758743286133
INFO:root:current mean train loss 1840.2948884612438
INFO:root:current train perplexity4.272130489349365
INFO:root:current mean train loss 1843.3430541548296
INFO:root:current train perplexity4.279505729675293
INFO:root:current mean train loss 1842.1671735136388
INFO:root:current train perplexity4.2782721519470215
INFO:root:current mean train loss 1842.539585813492
INFO:root:current train perplexity4.280854225158691
INFO:root:current mean train loss 1844.3109444233908
INFO:root:current train perplexity4.28496789932251
INFO:root:current mean train loss 1846.5831126210387
INFO:root:current train perplexity4.29231071472168
INFO:root:current mean train loss 2007.8584716145833
INFO:root:current train perplexity4.875260829925537
INFO:root:current mean train loss 2022.5787789878361
INFO:root:current train perplexity4.93227481842041

100%|██████████| 1/1 [07:27<00:00, 447.91s/it][A100%|██████████| 1/1 [07:27<00:00, 447.91s/it]
INFO:root:final mean train loss: 2021.7735233124133
INFO:root:final train perplexity: 4.9311842918396
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.96s/it][A100%|██████████| 1/1 [00:38<00:00, 38.96s/it]
INFO:root:eval mean loss: 1921.8115567687555
INFO:root:eval perplexity: 4.735836029052734
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.31s/it][A100%|██████████| 1/1 [00:37<00:00, 37.31s/it]
INFO:root:eval mean loss: 2346.682336252632
INFO:root:eval perplexity: 6.884516716003418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/29
 14%|█▍        | 29/200 [4:13:16<25:04:10, 527.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1936.4830919348676
INFO:root:current train perplexity4.633901596069336
INFO:root:current mean train loss 1918.70690091451
INFO:root:current train perplexity4.577307224273682
INFO:root:current mean train loss 1909.3102015665133
INFO:root:current train perplexity4.543886184692383
INFO:root:current mean train loss 1908.4609038683832
INFO:root:current train perplexity4.532015323638916
INFO:root:current mean train loss 1905.041764422161
INFO:root:current train perplexity4.510693073272705
INFO:root:current mean train loss 1901.2098141232052
INFO:root:current train perplexity4.4945902824401855
INFO:root:current mean train loss 1897.7809224211412
INFO:root:current train perplexity4.482378005981445
INFO:root:current mean train loss 1895.803460939966
INFO:root:current train perplexity4.475341320037842
INFO:root:current mean train loss 1892.8200869709922
INFO:root:current train perplexity4.464860916137695
INFO:root:current mean train loss 1893.8347948135868
INFO:root:current train perplexity4.463089942932129
INFO:root:current mean train loss 1892.0469424069584
INFO:root:current train perplexity4.456165313720703
INFO:root:current mean train loss 1889.9639625293296
INFO:root:current train perplexity4.448495864868164
INFO:root:current mean train loss 1888.158796375381
INFO:root:current train perplexity4.442170143127441
INFO:root:current mean train loss 1887.4898736011023
INFO:root:current train perplexity4.440951347351074
INFO:root:current mean train loss 1887.77725661536
INFO:root:current train perplexity4.4384613037109375
INFO:root:current mean train loss 1887.6925773428911
INFO:root:current train perplexity4.436135292053223
INFO:root:current mean train loss 1887.044085419206
INFO:root:current train perplexity4.433804988861084
INFO:root:current mean train loss 1887.8640757969447
INFO:root:current train perplexity4.43420934677124
INFO:root:current mean train loss 1886.7814436220972
INFO:root:current train perplexity4.430397987365723

100%|██████████| 1/1 [07:28<00:00, 448.77s/it][A100%|██████████| 1/1 [07:28<00:00, 448.77s/it]
INFO:root:final mean train loss: 1884.7322393235568
INFO:root:final train perplexity: 4.425692081451416
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.73s/it][A100%|██████████| 1/1 [00:38<00:00, 38.73s/it]
INFO:root:eval mean loss: 1876.9284334656195
INFO:root:eval perplexity: 4.566915512084961
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.75s/it][A100%|██████████| 1/1 [00:36<00:00, 36.75s/it]
INFO:root:eval mean loss: 2300.933853474069
INFO:root:eval perplexity: 6.6303911209106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/30
 15%|█▌        | 30/200 [4:22:03<24:54:26, 527.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1824.6461859809028
INFO:root:current train perplexity4.290830135345459
INFO:root:current mean train loss 1855.9918291284403
INFO:root:current train perplexity4.308310508728027
INFO:root:current mean train loss 1857.823198382364
INFO:root:current train perplexity4.3401923179626465
INFO:root:current mean train loss 1853.2246026591577
INFO:root:current train perplexity4.315707683563232
INFO:root:current mean train loss 1848.4969822666756
INFO:root:current train perplexity4.307033061981201
INFO:root:current mean train loss 1851.3962184104096
INFO:root:current train perplexity4.3070068359375
INFO:root:current mean train loss 1848.817504081037
INFO:root:current train perplexity4.308186054229736
INFO:root:current mean train loss 1846.0320962279068
INFO:root:current train perplexity4.300661563873291
INFO:root:current mean train loss 1847.0410441432807
INFO:root:current train perplexity4.3027143478393555
INFO:root:current mean train loss 1851.1093420987606
INFO:root:current train perplexity4.309321880340576
INFO:root:current mean train loss 1849.62761561958
INFO:root:current train perplexity4.3085246086120605
INFO:root:current mean train loss 1850.6971577540296
INFO:root:current train perplexity4.312286853790283
INFO:root:current mean train loss 1850.2799941600108
INFO:root:current train perplexity4.311988353729248
INFO:root:current mean train loss 1850.775288417924
INFO:root:current train perplexity4.311304092407227
INFO:root:current mean train loss 1849.8802385404654
INFO:root:current train perplexity4.307107925415039
INFO:root:current mean train loss 1851.0757305127568
INFO:root:current train perplexity4.309524059295654
INFO:root:current mean train loss 1850.6992777746757
INFO:root:current train perplexity4.308288097381592
INFO:root:current mean train loss 1849.915577129009
INFO:root:current train perplexity4.305665969848633
INFO:root:current mean train loss 1850.5539641203704
INFO:root:current train perplexity4.306944847106934
INFO:root:current mean train loss 1851.03905105391
INFO:root:current train perplexity4.30852746963501

100%|██████████| 1/1 [07:31<00:00, 451.78s/it][A100%|██████████| 1/1 [07:31<00:00, 451.78s/it]
INFO:root:final mean train loss: 1850.886406279548
INFO:root:final train perplexity: 4.309041976928711
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.57s/it][A100%|██████████| 1/1 [00:40<00:00, 40.57s/it]
INFO:root:eval mean loss: 1868.7537716263575
INFO:root:eval perplexity: 4.536805152893066
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.38s/it][A100%|██████████| 1/1 [00:41<00:00, 41.38s/it]
INFO:root:eval mean loss: 2292.2553009682515
INFO:root:eval perplexity: 6.583251953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/31
 16%|█▌        | 31/200 [4:30:59<24:52:55, 530.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1781.5876981295073
INFO:root:current train perplexity4.21589469909668
INFO:root:current mean train loss 1830.1834028940352
INFO:root:current train perplexity4.233343124389648
INFO:root:current mean train loss 1830.2377173499724
INFO:root:current train perplexity4.222212791442871
INFO:root:current mean train loss 1826.8506578316717
INFO:root:current train perplexity4.224796295166016
INFO:root:current mean train loss 1830.5792703404672
INFO:root:current train perplexity4.229689598083496
INFO:root:current mean train loss 1829.0489808289271
INFO:root:current train perplexity4.220031261444092
INFO:root:current mean train loss 1827.8659685518794
INFO:root:current train perplexity4.219029426574707
INFO:root:current mean train loss 1826.8112944295583
INFO:root:current train perplexity4.220427989959717
INFO:root:current mean train loss 1826.6909658510517
INFO:root:current train perplexity4.221590518951416
INFO:root:current mean train loss 1827.3862928221602
INFO:root:current train perplexity4.2200236320495605
INFO:root:current mean train loss 1828.6311044674403
INFO:root:current train perplexity4.2234954833984375
INFO:root:current mean train loss 1828.969664443238
INFO:root:current train perplexity4.227639198303223
INFO:root:current mean train loss 1829.4652490911437
INFO:root:current train perplexity4.225491046905518
INFO:root:current mean train loss 1828.0545388246194
INFO:root:current train perplexity4.225064754486084
INFO:root:current mean train loss 1827.6842627398262
INFO:root:current train perplexity4.224363803863525
INFO:root:current mean train loss 1828.2224133892735
INFO:root:current train perplexity4.226505756378174
INFO:root:current mean train loss 1827.8331334863642
INFO:root:current train perplexity4.228937149047852
INFO:root:current mean train loss 1827.732924371854
INFO:root:current train perplexity4.2288408279418945
INFO:root:current mean train loss 1827.9637650388486
INFO:root:current train perplexity4.230982303619385
INFO:root:current mean train loss 1827.7953771985333
INFO:root:current train perplexity4.230709075927734

100%|██████████| 1/1 [07:27<00:00, 447.91s/it][A100%|██████████| 1/1 [07:27<00:00, 447.91s/it]
INFO:root:final mean train loss: 1827.989400751111
INFO:root:final train perplexity: 4.231874942779541
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 1866.13765765251
INFO:root:eval perplexity: 4.5272111892700195
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.16s/it][A100%|██████████| 1/1 [00:37<00:00, 37.16s/it]
INFO:root:eval mean loss: 2290.302933063913
INFO:root:eval perplexity: 6.572693347930908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/32
 16%|█▌        | 32/200 [4:39:45<24:40:50, 528.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1798.5088458393896
INFO:root:current train perplexity4.114178657531738
INFO:root:current mean train loss 1810.440031891936
INFO:root:current train perplexity4.147915363311768
INFO:root:current mean train loss 1817.1437199596514
INFO:root:current train perplexity4.173161506652832
INFO:root:current mean train loss 1816.8629285628872
INFO:root:current train perplexity4.171433448791504
INFO:root:current mean train loss 1807.840937422845
INFO:root:current train perplexity4.154588222503662
INFO:root:current mean train loss 1810.013634557004
INFO:root:current train perplexity4.1578874588012695
INFO:root:current mean train loss 1809.043885131646
INFO:root:current train perplexity4.156301021575928
INFO:root:current mean train loss 1807.6947041199635
INFO:root:current train perplexity4.152437210083008
INFO:root:current mean train loss 1805.2699028187092
INFO:root:current train perplexity4.155267238616943
INFO:root:current mean train loss 1807.0407149152059
INFO:root:current train perplexity4.159013271331787
INFO:root:current mean train loss 1806.6299800708218
INFO:root:current train perplexity4.158361434936523
INFO:root:current mean train loss 1805.394689845288
INFO:root:current train perplexity4.156290054321289
INFO:root:current mean train loss 1804.7928855693447
INFO:root:current train perplexity4.156473636627197
INFO:root:current mean train loss 1805.672009159182
INFO:root:current train perplexity4.1612229347229
INFO:root:current mean train loss 1805.991233778099
INFO:root:current train perplexity4.162515640258789
INFO:root:current mean train loss 1806.6097904188016
INFO:root:current train perplexity4.162749290466309
INFO:root:current mean train loss 1805.6921439469768
INFO:root:current train perplexity4.159806251525879
INFO:root:current mean train loss 1806.265017379787
INFO:root:current train perplexity4.160314559936523
INFO:root:current mean train loss 1807.2006540531318
INFO:root:current train perplexity4.161227226257324
INFO:root:current mean train loss 1807.8275492025662
INFO:root:current train perplexity4.162760257720947

100%|██████████| 1/1 [07:25<00:00, 445.83s/it][A100%|██████████| 1/1 [07:25<00:00, 445.83s/it]
INFO:root:final mean train loss: 1806.9802834899872
INFO:root:final train perplexity: 4.162288188934326
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.89s/it][A100%|██████████| 1/1 [00:38<00:00, 38.89s/it]
INFO:root:eval mean loss: 1860.0816416638963
INFO:root:eval perplexity: 4.5050787925720215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.48s/it][A100%|██████████| 1/1 [00:36<00:00, 36.48s/it]
INFO:root:eval mean loss: 2285.011327865276
INFO:root:eval perplexity: 6.544161796569824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/33
 16%|█▋        | 33/200 [4:48:29<24:27:38, 527.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1772.4120361328125
INFO:root:current train perplexity4.120853424072266
INFO:root:current mean train loss 1773.9163513183594
INFO:root:current train perplexity4.069806098937988
INFO:root:current mean train loss 1780.7649165226862
INFO:root:current train perplexity4.092389106750488
INFO:root:current mean train loss 1783.833403184679
INFO:root:current train perplexity4.097145080566406
INFO:root:current mean train loss 1784.3596143639606
INFO:root:current train perplexity4.099297523498535
INFO:root:current mean train loss 1783.9996233258928
INFO:root:current train perplexity4.101015090942383
INFO:root:current mean train loss 1781.5809886585582
INFO:root:current train perplexity4.097352027893066
INFO:root:current mean train loss 1782.0819692511307
INFO:root:current train perplexity4.0949859619140625
INFO:root:current mean train loss 1783.627604923692
INFO:root:current train perplexity4.094837665557861
INFO:root:current mean train loss 1785.133075205485
INFO:root:current train perplexity4.095645904541016
INFO:root:current mean train loss 1785.7873530549823
INFO:root:current train perplexity4.09323787689209
INFO:root:current mean train loss 1786.6117698932517
INFO:root:current train perplexity4.097849369049072
INFO:root:current mean train loss 1787.87383781312
INFO:root:current train perplexity4.100387096405029
INFO:root:current mean train loss 1789.3758540433996
INFO:root:current train perplexity4.100454807281494
INFO:root:current mean train loss 1790.7948188677226
INFO:root:current train perplexity4.104214191436768
INFO:root:current mean train loss 1790.8414700239132
INFO:root:current train perplexity4.1048383712768555
INFO:root:current mean train loss 1791.3111692129849
INFO:root:current train perplexity4.106342792510986
INFO:root:current mean train loss 1790.5831063704056
INFO:root:current train perplexity4.105540752410889
INFO:root:current mean train loss 1790.6507632675991
INFO:root:current train perplexity4.105813026428223
INFO:root:current mean train loss 1790.5687626429967
INFO:root:current train perplexity4.107290267944336

100%|██████████| 1/1 [07:27<00:00, 447.54s/it][A100%|██████████| 1/1 [07:27<00:00, 447.54s/it]
INFO:root:final mean train loss: 1789.65750172856
INFO:root:final train perplexity: 4.105772018432617
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.49s/it][A100%|██████████| 1/1 [00:39<00:00, 39.49s/it]
INFO:root:eval mean loss: 1850.079496343085
INFO:root:eval perplexity: 4.4687628746032715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.17s/it][A100%|██████████| 1/1 [00:37<00:00, 37.17s/it]
INFO:root:eval mean loss: 2280.120928825216
INFO:root:eval perplexity: 6.5179033279418945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/34
 17%|█▋        | 34/200 [4:57:15<24:18:09, 527.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1785.5932759867087
INFO:root:current train perplexity4.075944423675537
INFO:root:current mean train loss 1769.6271869206832
INFO:root:current train perplexity4.041816234588623
INFO:root:current mean train loss 1770.0165861384532
INFO:root:current train perplexity4.0438971519470215
INFO:root:current mean train loss 1773.6402500466263
INFO:root:current train perplexity4.054736137390137
INFO:root:current mean train loss 1775.153232278564
INFO:root:current train perplexity4.056524753570557
INFO:root:current mean train loss 1776.684790335247
INFO:root:current train perplexity4.06217622756958
INFO:root:current mean train loss 1775.7706718951947
INFO:root:current train perplexity4.061591625213623
INFO:root:current mean train loss 1774.9450444794684
INFO:root:current train perplexity4.060490608215332
INFO:root:current mean train loss 1775.116621967868
INFO:root:current train perplexity4.0591912269592285
INFO:root:current mean train loss 1775.3835585407737
INFO:root:current train perplexity4.05545711517334
INFO:root:current mean train loss 1774.3180859919046
INFO:root:current train perplexity4.051828384399414
INFO:root:current mean train loss 1773.661996638866
INFO:root:current train perplexity4.049400329589844
INFO:root:current mean train loss 1773.9138031603306
INFO:root:current train perplexity4.053677558898926
INFO:root:current mean train loss 1772.329376819087
INFO:root:current train perplexity4.04861307144165
INFO:root:current mean train loss 1774.5189830493346
INFO:root:current train perplexity4.055489540100098
INFO:root:current mean train loss 1774.5622661544617
INFO:root:current train perplexity4.054622650146484
INFO:root:current mean train loss 1775.7482727451131
INFO:root:current train perplexity4.05807638168335
INFO:root:current mean train loss 1774.4733964343654
INFO:root:current train perplexity4.0565185546875
INFO:root:current mean train loss 1775.0811651816189
INFO:root:current train perplexity4.058640480041504
INFO:root:current mean train loss 1781.2186013792402
INFO:root:current train perplexity4.076712131500244

100%|██████████| 1/1 [07:22<00:00, 442.91s/it][A100%|██████████| 1/1 [07:22<00:00, 442.91s/it]
INFO:root:final mean train loss: 1780.8066115386548
INFO:root:final train perplexity: 4.077192783355713
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.55s/it][A100%|██████████| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 1876.0046391047485
INFO:root:eval perplexity: 4.563503265380859
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.51s/it][A100%|██████████| 1/1 [00:36<00:00, 36.51s/it]
INFO:root:eval mean loss: 2297.8939312874004
INFO:root:eval perplexity: 6.6138410568237305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/35
 18%|█▊        | 35/200 [5:05:56<24:03:54, 525.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.9016996343084
INFO:root:current train perplexity4.298108100891113
INFO:root:current mean train loss 1887.3950465880719
INFO:root:current train perplexity4.438154697418213
INFO:root:current mean train loss 1948.5313857720823
INFO:root:current train perplexity4.659293174743652
INFO:root:current mean train loss 2269.122188355112
INFO:root:current train perplexity5.999523639678955
INFO:root:current mean train loss 2433.6001557261357
INFO:root:current train perplexity6.828999996185303
INFO:root:current mean train loss 2502.6329816310895
INFO:root:current train perplexity7.203022003173828
INFO:root:current mean train loss 2512.7791945047957
INFO:root:current train perplexity7.260760307312012
INFO:root:current mean train loss 2706.6112057472055
INFO:root:current train perplexity8.467525482177734
INFO:root:current mean train loss 3175.60471175021
INFO:root:current train perplexity12.243306159973145
INFO:root:current mean train loss 3106.538116884903
INFO:root:current train perplexity11.592445373535156
INFO:root:current mean train loss 3063.5728660457967
INFO:root:current train perplexity11.198379516601562
INFO:root:current mean train loss 3024.5863952125537
INFO:root:current train perplexity10.847982406616211
INFO:root:current mean train loss 3028.5735134024526
INFO:root:current train perplexity10.87049674987793
INFO:root:current mean train loss 3081.96828755142
INFO:root:current train perplexity11.345344543457031
INFO:root:current mean train loss 3133.053251008592
INFO:root:current train perplexity11.825855255126953
INFO:root:current mean train loss 3172.005883344892
INFO:root:current train perplexity12.191033363342285
INFO:root:current mean train loss 3197.585916818666
INFO:root:current train perplexity12.428122520446777
INFO:root:current mean train loss 3214.9813588290176
INFO:root:current train perplexity12.616353988647461
INFO:root:current mean train loss 3245.0795072819387
INFO:root:current train perplexity12.93032455444336

100%|██████████| 1/1 [07:30<00:00, 450.48s/it][A100%|██████████| 1/1 [07:30<00:00, 450.48s/it]
INFO:root:final mean train loss: 3242.422360695797
INFO:root:final train perplexity: 12.921760559082031
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.88s/it][A100%|██████████| 1/1 [00:39<00:00, 39.88s/it]
INFO:root:eval mean loss: 2375.077041084885
INFO:root:eval perplexity: 6.834253311157227
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.02s/it][A100%|██████████| 1/1 [00:37<00:00, 37.02s/it]
INFO:root:eval mean loss: 2774.5302128352173
INFO:root:eval perplexity: 9.786665916442871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/36
 18%|█▊        | 36/200 [5:14:46<23:58:59, 526.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2778.368829900568
INFO:root:current train perplexity8.763545989990234
INFO:root:current mean train loss 2922.957521730715
INFO:root:current train perplexity9.76367473602295
INFO:root:current mean train loss 2856.8289731283326
INFO:root:current train perplexity9.38607406616211
INFO:root:current mean train loss 2809.7539823967545
INFO:root:current train perplexity9.09927749633789
INFO:root:current mean train loss 2748.4505113290757
INFO:root:current train perplexity8.685919761657715
INFO:root:current mean train loss 2752.7752229276234
INFO:root:current train perplexity8.733291625976562
INFO:root:current mean train loss 2710.7961825356742
INFO:root:current train perplexity8.471101760864258
INFO:root:current mean train loss 2677.7689327449234
INFO:root:current train perplexity8.253108978271484
INFO:root:current mean train loss 2681.282912925786
INFO:root:current train perplexity8.278619766235352
INFO:root:current mean train loss 2722.0723789855756
INFO:root:current train perplexity8.539629936218262
INFO:root:current mean train loss 2781.225263314478
INFO:root:current train perplexity8.929357528686523
INFO:root:current mean train loss 2825.210541952633
INFO:root:current train perplexity9.258882522583008
INFO:root:current mean train loss 2872.756715379206
INFO:root:current train perplexity9.609436988830566
INFO:root:current mean train loss 2904.6469428602927
INFO:root:current train perplexity9.834253311157227
INFO:root:current mean train loss 2921.2672810382155
INFO:root:current train perplexity9.981751441955566
INFO:root:current mean train loss 2953.3075363997355
INFO:root:current train perplexity10.2610445022583
INFO:root:current mean train loss 2981.059995095971
INFO:root:current train perplexity10.492768287658691
INFO:root:current mean train loss 3012.2558262711864
INFO:root:current train perplexity10.750382423400879
INFO:root:current mean train loss 3035.7986906189603
INFO:root:current train perplexity10.961788177490234
INFO:root:current mean train loss 3064.990723550538
INFO:root:current train perplexity11.223986625671387

100%|██████████| 1/1 [07:32<00:00, 452.88s/it][A100%|██████████| 1/1 [07:32<00:00, 452.88s/it]
INFO:root:final mean train loss: 3081.069376935396
INFO:root:final train perplexity: 11.376765251159668
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.38s/it][A100%|██████████| 1/1 [00:38<00:00, 38.38s/it]
INFO:root:eval mean loss: 3262.757196088209
INFO:root:eval perplexity: 14.017001152038574
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.20s/it][A100%|██████████| 1/1 [00:36<00:00, 36.20s/it]
INFO:root:eval mean loss: 3652.2408581456393
INFO:root:eval perplexity: 20.1380672454834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/37
 18%|█▊        | 37/200 [5:23:35<23:53:03, 527.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3844.7699672154017
INFO:root:current train perplexity20.57992935180664
INFO:root:current mean train loss 3768.883743286133
INFO:root:current train perplexity19.449588775634766
INFO:root:current mean train loss 3790.3711708470396
INFO:root:current train perplexity20.01716423034668
INFO:root:current mean train loss 3843.1895201148054
INFO:root:current train perplexity20.885787963867188
INFO:root:current mean train loss 4059.284135194582
INFO:root:current train perplexity24.838117599487305
INFO:root:current mean train loss 4036.937298861417
INFO:root:current train perplexity24.332061767578125
INFO:root:current mean train loss 4050.242404427498
INFO:root:current train perplexity24.50904083251953
INFO:root:current mean train loss 4042.3265672620837
INFO:root:current train perplexity24.33706283569336
INFO:root:current mean train loss 4031.7120408505057
INFO:root:current train perplexity24.080995559692383
INFO:root:current mean train loss 4081.5971487637225
INFO:root:current train perplexity25.015050888061523
INFO:root:current mean train loss 4353.932100644835
INFO:root:current train perplexity31.042192459106445
INFO:root:current mean train loss 4596.040101179839
INFO:root:current train perplexity37.70915985107422
INFO:root:current mean train loss 4868.371648235508
INFO:root:current train perplexity46.69908142089844
INFO:root:current mean train loss 5091.4858499549955
INFO:root:current train perplexity55.768211364746094
INFO:root:current mean train loss 5305.703396666284
INFO:root:current train perplexity65.97181701660156
INFO:root:current mean train loss 5508.890250001278
INFO:root:current train perplexity77.64029693603516
INFO:root:current mean train loss 5638.3354916584285
INFO:root:current train perplexity85.80304718017578
INFO:root:current mean train loss 5744.814450723154
INFO:root:current train perplexity93.31156158447266
INFO:root:current mean train loss 5863.7490342555475
INFO:root:current train perplexity102.31024169921875
INFO:root:current mean train loss 5979.38138399876
INFO:root:current train perplexity112.00249481201172

100%|██████████| 1/1 [07:31<00:00, 451.36s/it][A100%|██████████| 1/1 [07:31<00:00, 451.36s/it]
INFO:root:final mean train loss: 6026.830361416769
INFO:root:final train perplexity: 116.32186126708984
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.89s/it][A100%|██████████| 1/1 [00:38<00:00, 38.89s/it]
INFO:root:eval mean loss: 7582.86898998504
INFO:root:eval perplexity: 462.2791748046875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.17s/it][A100%|██████████| 1/1 [00:37<00:00, 37.17s/it]
INFO:root:eval mean loss: 7799.935252521055
INFO:root:eval perplexity: 609.4463500976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/38
 19%|█▉        | 38/200 [5:32:25<23:46:08, 528.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7219.1572482638885
INFO:root:current train perplexity308.8096618652344
INFO:root:current mean train loss 7267.138917699353
INFO:root:current train perplexity315.64739990234375
INFO:root:current mean train loss 7479.735499043368
INFO:root:current train perplexity377.6079406738281
INFO:root:current mean train loss 7554.466018455616
INFO:root:current train perplexity397.70941162109375
INFO:root:current mean train loss 7588.937099499649
INFO:root:current train perplexity407.316162109375
INFO:root:current mean train loss 7569.075345828555
INFO:root:current train perplexity399.2621765136719
INFO:root:current mean train loss 7501.852472444283
INFO:root:current train perplexity379.78009033203125
INFO:root:current mean train loss 7449.991501284606
INFO:root:current train perplexity363.4380798339844
INFO:root:current mean train loss 7529.9608438886835
INFO:root:current train perplexity385.4215087890625
INFO:root:current mean train loss 7437.282314401455
INFO:root:current train perplexity359.74468994140625
INFO:root:current mean train loss 7371.0777147502995
INFO:root:current train perplexity341.4919128417969
INFO:root:current mean train loss 7361.789793429312
INFO:root:current train perplexity337.71319580078125
INFO:root:current mean train loss 7361.002347279744
INFO:root:current train perplexity335.5617370605469
INFO:root:current mean train loss 7378.288785504763
INFO:root:current train perplexity339.24114990234375
INFO:root:current mean train loss 7395.224365741241
INFO:root:current train perplexity343.8712158203125
INFO:root:current mean train loss 7407.450474059466
INFO:root:current train perplexity347.036865234375
INFO:root:current mean train loss 7423.484960047018
INFO:root:current train perplexity350.6524658203125
INFO:root:current mean train loss 7436.75899696678
INFO:root:current train perplexity353.9247741699219
INFO:root:current mean train loss 7445.8124044609585
INFO:root:current train perplexity356.452392578125
INFO:root:current mean train loss 7454.640396298602
INFO:root:current train perplexity358.6639099121094

100%|██████████| 1/1 [07:35<00:00, 455.39s/it][A100%|██████████| 1/1 [07:35<00:00, 455.39s/it]
INFO:root:final mean train loss: 7455.6284476399005
INFO:root:final train perplexity: 359.23095703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.87s/it][A100%|██████████| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 7420.981673869681
INFO:root:eval perplexity: 405.5188903808594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.79s/it][A100%|██████████| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 7534.696622375055
INFO:root:eval perplexity: 490.0416564941406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/39
 20%|█▉        | 39/200 [5:41:19<23:41:37, 529.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7584.34716796875
INFO:root:current train perplexity403.4468688964844
INFO:root:current mean train loss 7468.019543306327
INFO:root:current train perplexity376.4248352050781
INFO:root:current mean train loss 7506.272833671279
INFO:root:current train perplexity391.3710021972656
INFO:root:current mean train loss 7575.605151771841
INFO:root:current train perplexity413.58685302734375
INFO:root:current mean train loss 7631.416356999121
INFO:root:current train perplexity426.9776611328125
INFO:root:current mean train loss 7644.421039187611
INFO:root:current train perplexity428.807861328125
INFO:root:current mean train loss 7628.4700009441085
INFO:root:current train perplexity420.207275390625
INFO:root:current mean train loss 7588.051142655019
INFO:root:current train perplexity407.5164489746094
INFO:root:current mean train loss 7553.404197179524
INFO:root:current train perplexity395.2379150390625
INFO:root:current mean train loss 7516.590081799799
INFO:root:current train perplexity382.0605163574219
INFO:root:current mean train loss 7495.614290456539
INFO:root:current train perplexity372.9571228027344
INFO:root:current mean train loss 7454.413695658751
INFO:root:current train perplexity362.2710266113281
INFO:root:current mean train loss 7426.268177062698
INFO:root:current train perplexity353.0917053222656
INFO:root:current mean train loss 7389.937037889478
INFO:root:current train perplexity343.5697937011719
INFO:root:current mean train loss 7366.478280501881
INFO:root:current train perplexity335.72320556640625
INFO:root:current mean train loss 7344.708963743398
INFO:root:current train perplexity329.7249450683594
INFO:root:current mean train loss 7330.583542218995
INFO:root:current train perplexity325.09088134765625
INFO:root:current mean train loss 7310.289939577274
INFO:root:current train perplexity320.1414794921875
INFO:root:current mean train loss 7291.996181598667
INFO:root:current train perplexity315.842529296875
INFO:root:current mean train loss 7278.774059921716
INFO:root:current train perplexity312.18548583984375

100%|██████████| 1/1 [07:33<00:00, 453.15s/it][A100%|██████████| 1/1 [07:33<00:00, 453.16s/it]
INFO:root:final mean train loss: 7275.14419981956
INFO:root:final train perplexity: 311.5399475097656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.05s/it][A100%|██████████| 1/1 [00:39<00:00, 39.05s/it]
INFO:root:eval mean loss: 6755.351723528923
INFO:root:eval perplexity: 236.63771057128906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.24s/it][A100%|██████████| 1/1 [00:36<00:00, 36.24s/it]
INFO:root:eval mean loss: 6833.27675504211
INFO:root:eval perplexity: 275.2920837402344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/40
 20%|██        | 40/200 [5:50:10<23:33:33, 530.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7663.836141465586
INFO:root:current train perplexity418.42657470703125
INFO:root:current mean train loss 8099.6772788276885
INFO:root:current train perplexity591.4541015625
INFO:root:current mean train loss 8268.161678847446
INFO:root:current train perplexity676.8217163085938
INFO:root:current mean train loss 8380.540108632915
INFO:root:current train perplexity735.248291015625
INFO:root:current mean train loss 8455.610595193437
INFO:root:current train perplexity781.6353759765625
INFO:root:current mean train loss 8563.915201822916
INFO:root:current train perplexity855.0921630859375
INFO:root:current mean train loss 8732.40495199167
INFO:root:current train perplexity987.9072875976562
INFO:root:current mean train loss 8722.718797637195
INFO:root:current train perplexity982.4673461914062
INFO:root:current mean train loss 8637.955708613303
INFO:root:current train perplexity918.460205078125
INFO:root:current mean train loss 8565.827401805094
INFO:root:current train perplexity863.3448486328125
INFO:root:current mean train loss 8495.313926831122
INFO:root:current train perplexity817.4982299804688
INFO:root:current mean train loss 8432.699693778493
INFO:root:current train perplexity779.2525634765625
INFO:root:current mean train loss 8372.960395389464
INFO:root:current train perplexity741.2919921875
INFO:root:current mean train loss 8329.265125033991
INFO:root:current train perplexity717.0453491210938
INFO:root:current mean train loss 8295.256566210806
INFO:root:current train perplexity696.99365234375
INFO:root:current mean train loss 8254.614085259658
INFO:root:current train perplexity674.8076171875
INFO:root:current mean train loss 8199.48546381775
INFO:root:current train perplexity645.7501831054688
INFO:root:current mean train loss 8137.459842641143
INFO:root:current train perplexity613.4091186523438
INFO:root:current mean train loss 8080.828884837347
INFO:root:current train perplexity586.975341796875
INFO:root:current mean train loss 8029.429926829364
INFO:root:current train perplexity563.8473510742188

100%|██████████| 1/1 [07:30<00:00, 450.30s/it][A100%|██████████| 1/1 [07:30<00:00, 450.30s/it]
INFO:root:final mean train loss: 8025.205967151452
INFO:root:final train perplexity: 563.1100463867188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.71s/it][A100%|██████████| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 6737.287294644836
INFO:root:eval perplexity: 233.20361328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.43s/it][A100%|██████████| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 6817.165500505596
INFO:root:eval perplexity: 271.66973876953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/41
 20%|██        | 41/200 [5:59:00<23:25:17, 530.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7028.062367757161
INFO:root:current train perplexity257.0908203125
INFO:root:current mean train loss 6999.647951709981
INFO:root:current train perplexity255.1341552734375
INFO:root:current mean train loss 6992.881030933277
INFO:root:current train perplexity253.19927978515625
INFO:root:current mean train loss 6977.243943339647
INFO:root:current train perplexity250.64649963378906
INFO:root:current mean train loss 6969.603419150076
INFO:root:current train perplexity248.569091796875
INFO:root:current mean train loss 6973.122179274591
INFO:root:current train perplexity246.9751739501953
INFO:root:current mean train loss 6957.349219311242
INFO:root:current train perplexity245.75425720214844
INFO:root:current mean train loss 6957.419235459524
INFO:root:current train perplexity244.85289001464844
INFO:root:current mean train loss 6952.58611188616
INFO:root:current train perplexity243.5872802734375
INFO:root:current mean train loss 6944.973253855264
INFO:root:current train perplexity240.90936279296875
INFO:root:current mean train loss 6931.941131814553
INFO:root:current train perplexity238.6282501220703
INFO:root:current mean train loss 6915.138414261732
INFO:root:current train perplexity235.771240234375
INFO:root:current mean train loss 6901.358811366706
INFO:root:current train perplexity233.26165771484375
INFO:root:current mean train loss 6894.486384788011
INFO:root:current train perplexity231.5011444091797
INFO:root:current mean train loss 6884.49325022978
INFO:root:current train perplexity229.6110076904297
INFO:root:current mean train loss 6877.43484596501
INFO:root:current train perplexity228.1191864013672
INFO:root:current mean train loss 6872.5944009457
INFO:root:current train perplexity227.05252075195312
INFO:root:current mean train loss 6868.594052049259
INFO:root:current train perplexity225.97372436523438
INFO:root:current mean train loss 6860.321123726761
INFO:root:current train perplexity224.78672790527344

100%|██████████| 1/1 [07:26<00:00, 446.70s/it][A100%|██████████| 1/1 [07:26<00:00, 446.70s/it]
INFO:root:final mean train loss: 6857.473344103591
INFO:root:final train perplexity: 224.05764770507812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 6413.101340868794
INFO:root:eval perplexity: 179.3925018310547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.72s/it][A100%|██████████| 1/1 [00:36<00:00, 36.72s/it]
INFO:root:eval mean loss: 6517.89468188996
INFO:root:eval perplexity: 212.4163360595703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/42
 21%|██        | 42/200 [6:07:45<23:11:57, 528.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6603.428109975962
INFO:root:current train perplexity202.3892059326172
INFO:root:current mean train loss 6747.448609478706
INFO:root:current train perplexity204.83114624023438
INFO:root:current mean train loss 7005.369961304284
INFO:root:current train perplexity248.1115264892578
INFO:root:current mean train loss 7118.011197708666
INFO:root:current train perplexity272.7041320800781
INFO:root:current mean train loss 7090.431400622351
INFO:root:current train perplexity265.126708984375
INFO:root:current mean train loss 7024.670125563474
INFO:root:current train perplexity254.4619140625
INFO:root:current mean train loss 6982.29973459166
INFO:root:current train perplexity247.36801147460938
INFO:root:current mean train loss 6954.911610136527
INFO:root:current train perplexity242.3287811279297
INFO:root:current mean train loss 6928.381836538092
INFO:root:current train perplexity238.21192932128906
INFO:root:current mean train loss 6923.685227058803
INFO:root:current train perplexity235.98179626464844
INFO:root:current mean train loss 6915.428260253424
INFO:root:current train perplexity234.46759033203125
INFO:root:current mean train loss 6903.200747645019
INFO:root:current train perplexity232.95343017578125
INFO:root:current mean train loss 6899.604961549361
INFO:root:current train perplexity231.88218688964844
INFO:root:current mean train loss 6897.496627028989
INFO:root:current train perplexity230.9486541748047
INFO:root:current mean train loss 6895.7565242392075
INFO:root:current train perplexity230.1072998046875
INFO:root:current mean train loss 6888.737734233002
INFO:root:current train perplexity229.26287841796875
INFO:root:current mean train loss 6884.566593025903
INFO:root:current train perplexity228.66371154785156
INFO:root:current mean train loss 6880.17516726412
INFO:root:current train perplexity227.98390197753906
INFO:root:current mean train loss 6879.947001150545
INFO:root:current train perplexity227.4468536376953
INFO:root:current mean train loss 6875.815275265045
INFO:root:current train perplexity226.7241668701172

100%|██████████| 1/1 [07:24<00:00, 444.46s/it][A100%|██████████| 1/1 [07:24<00:00, 444.46s/it]
INFO:root:final mean train loss: 6870.27723159059
INFO:root:final train perplexity: 226.33314514160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.07s/it][A100%|██████████| 1/1 [00:38<00:00, 38.07s/it]
INFO:root:eval mean loss: 6482.225018007535
INFO:root:eval perplexity: 189.71292114257812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.89s/it][A100%|██████████| 1/1 [00:35<00:00, 35.89s/it]
INFO:root:eval mean loss: 6565.392849103779
INFO:root:eval perplexity: 220.87518310546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/43
 22%|██▏       | 43/200 [6:16:26<22:57:01, 526.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7021.462760416666
INFO:root:current train perplexity223.32177734375
INFO:root:current mean train loss 6842.018487079327
INFO:root:current train perplexity214.42774963378906
INFO:root:current mean train loss 6800.110427989131
INFO:root:current train perplexity210.53578186035156
INFO:root:current mean train loss 6776.646895714962
INFO:root:current train perplexity209.3888397216797
INFO:root:current mean train loss 6767.304129950945
INFO:root:current train perplexity207.61180114746094
INFO:root:current mean train loss 6748.343037846404
INFO:root:current train perplexity206.06129455566406
INFO:root:current mean train loss 6745.711380053323
INFO:root:current train perplexity205.05807495117188
INFO:root:current mean train loss 6745.469822212115
INFO:root:current train perplexity204.5332794189453
INFO:root:current mean train loss 6743.10314264872
INFO:root:current train perplexity203.88291931152344
INFO:root:current mean train loss 6746.472815335182
INFO:root:current train perplexity203.70567321777344
INFO:root:current mean train loss 6739.905549340109
INFO:root:current train perplexity203.3281707763672
INFO:root:current mean train loss 6736.05109020672
INFO:root:current train perplexity203.23085021972656
INFO:root:current mean train loss 6737.614518626143
INFO:root:current train perplexity203.53594970703125
INFO:root:current mean train loss 6740.051652079417
INFO:root:current train perplexity203.6455535888672
INFO:root:current mean train loss 6739.94608555507
INFO:root:current train perplexity203.90771484375
INFO:root:current mean train loss 6774.243684576695
INFO:root:current train perplexity208.98326110839844
INFO:root:current mean train loss 6805.301713477761
INFO:root:current train perplexity214.37315368652344
INFO:root:current mean train loss 6830.501270942467
INFO:root:current train perplexity218.82937622070312
INFO:root:current mean train loss 6854.37545065958
INFO:root:current train perplexity222.7344207763672
INFO:root:current mean train loss 6874.345165256638
INFO:root:current train perplexity226.560546875

100%|██████████| 1/1 [07:26<00:00, 446.37s/it][A100%|██████████| 1/1 [07:26<00:00, 446.37s/it]
INFO:root:final mean train loss: 6881.578519958712
INFO:root:final train perplexity: 228.3607940673828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.56s/it][A100%|██████████| 1/1 [00:39<00:00, 39.56s/it]
INFO:root:eval mean loss: 6937.868392619681
INFO:root:eval perplexity: 274.3008117675781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.42s/it][A100%|██████████| 1/1 [00:36<00:00, 36.42s/it]
INFO:root:eval mean loss: 7050.805586145279
INFO:root:eval perplexity: 329.2013244628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/44
 22%|██▏       | 44/200 [6:25:11<22:47:09, 525.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7201.005215259309
INFO:root:current train perplexity306.23809814453125
INFO:root:current mean train loss 7220.237394371811
INFO:root:current train perplexity301.2971496582031
INFO:root:current mean train loss 7190.595199028972
INFO:root:current train perplexity296.9916687011719
INFO:root:current mean train loss 7186.706277017291
INFO:root:current train perplexity294.6043395996094
INFO:root:current mean train loss 7208.998771104237
INFO:root:current train perplexity300.0555419921875
INFO:root:current mean train loss 7225.046128742002
INFO:root:current train perplexity300.8638610839844
INFO:root:current mean train loss 7199.2349855402335
INFO:root:current train perplexity295.5141906738281
INFO:root:current mean train loss 7161.735197953271
INFO:root:current train perplexity286.12677001953125
INFO:root:current mean train loss 7127.3220632794055
INFO:root:current train perplexity277.87677001953125
INFO:root:current mean train loss 7102.008095569066
INFO:root:current train perplexity270.4694519042969
INFO:root:current mean train loss 7076.309593164249
INFO:root:current train perplexity264.8203125
INFO:root:current mean train loss 7055.911687503406
INFO:root:current train perplexity259.7613830566406
INFO:root:current mean train loss 7068.59676700644
INFO:root:current train perplexity262.26300048828125
INFO:root:current mean train loss 7090.915542206059
INFO:root:current train perplexity267.6030578613281
INFO:root:current mean train loss 7093.979634588804
INFO:root:current train perplexity268.88623046875
INFO:root:current mean train loss 7098.951066454226
INFO:root:current train perplexity270.039306640625
INFO:root:current mean train loss 7102.56195420414
INFO:root:current train perplexity270.72027587890625
INFO:root:current mean train loss 7101.7334635603
INFO:root:current train perplexity270.4586181640625
INFO:root:current mean train loss 7097.414479931561
INFO:root:current train perplexity269.96783447265625
INFO:root:current mean train loss 7093.517568093542
INFO:root:current train perplexity269.4678039550781

100%|██████████| 1/1 [07:33<00:00, 453.44s/it][A100%|██████████| 1/1 [07:33<00:00, 453.44s/it]
INFO:root:final mean train loss: 7090.624329752111
INFO:root:final train perplexity: 269.32135009765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6738.310088029145
INFO:root:eval perplexity: 233.3968505859375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.14s/it][A100%|██████████| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 6844.869534539838
INFO:root:eval perplexity: 277.9284362792969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/45
 22%|██▎       | 45/200 [6:34:04<22:44:00, 528.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7048.114532470703
INFO:root:current train perplexity263.3699645996094
INFO:root:current mean train loss 7116.651638124047
INFO:root:current train perplexity271.4990539550781
INFO:root:current mean train loss 7137.140367912523
INFO:root:current train perplexity274.524169921875
INFO:root:current mean train loss 7145.043962751116
INFO:root:current train perplexity277.6292419433594
INFO:root:current mean train loss 7146.435769969019
INFO:root:current train perplexity279.57769775390625
INFO:root:current mean train loss 7139.694058032746
INFO:root:current train perplexity278.8411865234375
INFO:root:current mean train loss 7122.7014689617845
INFO:root:current train perplexity276.7836608886719
INFO:root:current mean train loss 7134.824519771556
INFO:root:current train perplexity280.26763916015625
INFO:root:current mean train loss 7147.283248901367
INFO:root:current train perplexity282.32269287109375
INFO:root:current mean train loss 7139.627512318465
INFO:root:current train perplexity280.404541015625
INFO:root:current mean train loss 7118.75345146925
INFO:root:current train perplexity276.1537780761719
INFO:root:current mean train loss 7105.769415471972
INFO:root:current train perplexity272.7374267578125
INFO:root:current mean train loss 7081.01930333391
INFO:root:current train perplexity267.75927734375
INFO:root:current mean train loss 7056.137130782052
INFO:root:current train perplexity262.771240234375
INFO:root:current mean train loss 7035.8170446177
INFO:root:current train perplexity258.3551330566406
INFO:root:current mean train loss 7014.641376153892
INFO:root:current train perplexity254.31744384765625
INFO:root:current mean train loss 6999.785038287823
INFO:root:current train perplexity250.96102905273438
INFO:root:current mean train loss 6981.321114122732
INFO:root:current train perplexity247.63389587402344
INFO:root:current mean train loss 6969.325395706897
INFO:root:current train perplexity244.79942321777344
INFO:root:current mean train loss 6957.580855794934
INFO:root:current train perplexity242.14894104003906

100%|██████████| 1/1 [07:30<00:00, 450.40s/it][A100%|██████████| 1/1 [07:30<00:00, 450.40s/it]
INFO:root:final mean train loss: 6953.1977264512025
INFO:root:final train perplexity: 241.63980102539062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.64s/it][A100%|██████████| 1/1 [00:38<00:00, 38.64s/it]
INFO:root:eval mean loss: 6381.746117990913
INFO:root:eval perplexity: 174.89794921875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.38s/it][A100%|██████████| 1/1 [00:36<00:00, 36.38s/it]
INFO:root:eval mean loss: 6493.201150231327
INFO:root:eval perplexity: 208.14747619628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/46
 23%|██▎       | 46/200 [6:42:51<22:35:03, 527.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6699.898642457561
INFO:root:current train perplexity198.54042053222656
INFO:root:current mean train loss 6625.451573830283
INFO:root:current train perplexity194.80125427246094
INFO:root:current mean train loss 6664.71561874444
INFO:root:current train perplexity195.287109375
INFO:root:current mean train loss 6768.945426560449
INFO:root:current train perplexity210.1861114501953
INFO:root:current mean train loss 6745.833612834589
INFO:root:current train perplexity207.42141723632812
INFO:root:current mean train loss 6735.600405248225
INFO:root:current train perplexity205.71051025390625
INFO:root:current mean train loss 6731.272308215171
INFO:root:current train perplexity203.7850341796875
INFO:root:current mean train loss 6715.239204045294
INFO:root:current train perplexity201.12814331054688
INFO:root:current mean train loss 6717.842359978008
INFO:root:current train perplexity200.1960906982422
INFO:root:current mean train loss 6783.157087195782
INFO:root:current train perplexity209.45054626464844
INFO:root:current mean train loss 6828.787881320103
INFO:root:current train perplexity216.75372314453125
INFO:root:current mean train loss 6856.434256092559
INFO:root:current train perplexity222.1004180908203
INFO:root:current mean train loss 6869.660511883416
INFO:root:current train perplexity225.17691040039062
INFO:root:current mean train loss 6882.451464278037
INFO:root:current train perplexity227.6622314453125
INFO:root:current mean train loss 6894.667012958411
INFO:root:current train perplexity229.9349822998047
INFO:root:current mean train loss 6906.923522987824
INFO:root:current train perplexity232.54457092285156
INFO:root:current mean train loss 6922.636323419375
INFO:root:current train perplexity235.54537963867188
INFO:root:current mean train loss 6937.477526999403
INFO:root:current train perplexity238.27635192871094
INFO:root:current mean train loss 6950.446069193165
INFO:root:current train perplexity240.68638610839844
INFO:root:current mean train loss 6963.27775488232
INFO:root:current train perplexity243.15475463867188

100%|██████████| 1/1 [07:29<00:00, 449.29s/it][A100%|██████████| 1/1 [07:29<00:00, 449.29s/it]
INFO:root:final mean train loss: 6961.208562084357
INFO:root:final train perplexity: 243.17239379882812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.20s/it][A100%|██████████| 1/1 [00:38<00:00, 38.20s/it]
INFO:root:eval mean loss: 6931.589421265514
INFO:root:eval perplexity: 272.91046142578125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.24s/it][A100%|██████████| 1/1 [00:36<00:00, 36.24s/it]
INFO:root:eval mean loss: 6958.751181744515
INFO:root:eval perplexity: 305.20654296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/47
 24%|██▎       | 47/200 [6:51:37<22:24:45, 527.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7226.796511280293
INFO:root:current train perplexity294.5794677734375
INFO:root:current mean train loss 7188.67272332702
INFO:root:current train perplexity292.0553283691406
INFO:root:current mean train loss 7200.737180159396
INFO:root:current train perplexity292.3475341796875
INFO:root:current mean train loss 7217.118306375628
INFO:root:current train perplexity292.6849365234375
INFO:root:current mean train loss 7222.869531838291
INFO:root:current train perplexity292.50244140625
INFO:root:current mean train loss 7219.437113784229
INFO:root:current train perplexity291.9500732421875
INFO:root:current mean train loss 7206.639045431142
INFO:root:current train perplexity291.44140625
INFO:root:current mean train loss 7207.82182078732
INFO:root:current train perplexity291.18402099609375
INFO:root:current mean train loss 7201.471563870233
INFO:root:current train perplexity290.7415771484375
INFO:root:current mean train loss 7195.69902793869
INFO:root:current train perplexity290.44989013671875
INFO:root:current mean train loss 7192.35404926571
INFO:root:current train perplexity289.9945983886719
INFO:root:current mean train loss 7185.217929540771
INFO:root:current train perplexity289.6511535644531
INFO:root:current mean train loss 7179.1398222325215
INFO:root:current train perplexity289.2670593261719
INFO:root:current mean train loss 7181.306098555973
INFO:root:current train perplexity289.372314453125
INFO:root:current mean train loss 7177.122952021967
INFO:root:current train perplexity288.9938049316406
INFO:root:current mean train loss 7178.318332791478
INFO:root:current train perplexity288.8542175292969
INFO:root:current mean train loss 7177.090365676071
INFO:root:current train perplexity288.65692138671875
INFO:root:current mean train loss 7180.36892228344
INFO:root:current train perplexity288.7008361816406
INFO:root:current mean train loss 7179.647739036568
INFO:root:current train perplexity288.67645263671875

100%|██████████| 1/1 [07:31<00:00, 451.46s/it][A100%|██████████| 1/1 [07:31<00:00, 451.46s/it]
INFO:root:final mean train loss: 7178.067940900979
INFO:root:final train perplexity: 288.5634765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.21s/it][A100%|██████████| 1/1 [00:39<00:00, 39.21s/it]
INFO:root:eval mean loss: 6911.419539214871
INFO:root:eval perplexity: 268.4925231933594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.42s/it][A100%|██████████| 1/1 [00:37<00:00, 37.42s/it]
INFO:root:eval mean loss: 6937.818128220579
INFO:root:eval perplexity: 299.9988708496094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/48
 24%|██▍       | 48/200 [7:00:28<22:18:15, 528.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7255.002083333334
INFO:root:current train perplexity292.4582214355469
INFO:root:current mean train loss 7146.160670006793
INFO:root:current train perplexity287.37091064453125
INFO:root:current mean train loss 7147.116578851745
INFO:root:current train perplexity287.9126892089844
INFO:root:current mean train loss 7168.297631448413
INFO:root:current train perplexity288.24896240234375
INFO:root:current mean train loss 7166.012947100904
INFO:root:current train perplexity287.8962097167969
INFO:root:current mean train loss 7168.529745335255
INFO:root:current train perplexity287.9430236816406
INFO:root:current mean train loss 7175.830169429624
INFO:root:current train perplexity287.90032958984375
INFO:root:current mean train loss 7164.247354403409
INFO:root:current train perplexity287.5929870605469
INFO:root:current mean train loss 7171.981165524348
INFO:root:current train perplexity287.72314453125
INFO:root:current mean train loss 7174.271733051571
INFO:root:current train perplexity287.7174072265625
INFO:root:current mean train loss 7168.669074719058
INFO:root:current train perplexity287.1039733886719
INFO:root:current mean train loss 7162.636674957959
INFO:root:current train perplexity286.6597900390625
INFO:root:current mean train loss 7167.903114149305
INFO:root:current train perplexity286.7591857910156
INFO:root:current mean train loss 7168.677345235266
INFO:root:current train perplexity286.99200439453125
INFO:root:current mean train loss 7170.8630997405035
INFO:root:current train perplexity287.01513671875
INFO:root:current mean train loss 7171.108438080136
INFO:root:current train perplexity286.9700927734375
INFO:root:current mean train loss 7173.66032677051
INFO:root:current train perplexity287.055908203125
INFO:root:current mean train loss 7174.596700756196
INFO:root:current train perplexity287.1249084472656
INFO:root:current mean train loss 7173.881387202996
INFO:root:current train perplexity286.92083740234375
INFO:root:current mean train loss 7173.770068486863
INFO:root:current train perplexity286.98638916015625

100%|██████████| 1/1 [07:26<00:00, 446.64s/it][A100%|██████████| 1/1 [07:26<00:00, 446.64s/it]
INFO:root:final mean train loss: 7171.248991796481
INFO:root:final train perplexity: 287.0148620605469
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.24s/it][A100%|██████████| 1/1 [00:39<00:00, 39.24s/it]
INFO:root:eval mean loss: 6924.876946199025
INFO:root:eval perplexity: 271.432373046875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.90s/it][A100%|██████████| 1/1 [00:36<00:00, 36.90s/it]
INFO:root:eval mean loss: 6951.5256313026375
INFO:root:eval perplexity: 303.3990173339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/49
 24%|██▍       | 49/200 [7:09:13<22:07:06, 527.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7241.376754760742
INFO:root:current train perplexity289.04864501953125
INFO:root:current mean train loss 7166.464177911932
INFO:root:current train perplexity284.9245910644531
INFO:root:current mean train loss 7208.8177700700435
INFO:root:current train perplexity287.4905700683594
INFO:root:current mean train loss 7215.463261248117
INFO:root:current train perplexity288.142333984375
INFO:root:current mean train loss 7193.490345142506
INFO:root:current train perplexity287.6962585449219
INFO:root:current mean train loss 7189.842374184974
INFO:root:current train perplexity288.0579833984375
INFO:root:current mean train loss 7183.0423336753365
INFO:root:current train perplexity286.9464416503906
INFO:root:current mean train loss 7170.545485533
INFO:root:current train perplexity286.2912292480469
INFO:root:current mean train loss 7171.258269089919
INFO:root:current train perplexity285.9320068359375
INFO:root:current mean train loss 7170.067834420265
INFO:root:current train perplexity285.8959045410156
INFO:root:current mean train loss 7171.554229026617
INFO:root:current train perplexity285.69677734375
INFO:root:current mean train loss 7172.344463874089
INFO:root:current train perplexity286.1448059082031
INFO:root:current mean train loss 7171.86188536805
INFO:root:current train perplexity285.8861999511719
INFO:root:current mean train loss 7166.218141847902
INFO:root:current train perplexity285.8620910644531
INFO:root:current mean train loss 7168.517217369719
INFO:root:current train perplexity286.0664978027344
INFO:root:current mean train loss 7166.7603903190275
INFO:root:current train perplexity285.90966796875
INFO:root:current mean train loss 7168.955038332471
INFO:root:current train perplexity286.0521545410156
INFO:root:current mean train loss 7170.399708666372
INFO:root:current train perplexity286.0484924316406
INFO:root:current mean train loss 7171.1629350820485
INFO:root:current train perplexity286.13604736328125
INFO:root:current mean train loss 7172.2826706153755
INFO:root:current train perplexity286.30364990234375

100%|██████████| 1/1 [07:32<00:00, 452.86s/it][A100%|██████████| 1/1 [07:32<00:00, 452.86s/it]
INFO:root:final mean train loss: 7167.770848969168
INFO:root:final train perplexity: 286.2281799316406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.70s/it][A100%|██████████| 1/1 [00:38<00:00, 38.70s/it]
INFO:root:eval mean loss: 6917.78032365082
INFO:root:eval perplexity: 269.87786865234375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.44s/it][A100%|██████████| 1/1 [00:37<00:00, 37.44s/it]
INFO:root:eval mean loss: 6943.0321417193045
INFO:root:eval perplexity: 301.2878112792969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/50
 25%|██▌       | 50/200 [7:18:04<22:01:21, 528.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7115.106305803572
INFO:root:current train perplexity286.4167175292969
INFO:root:current mean train loss 7159.25865470323
INFO:root:current train perplexity287.83197021484375
INFO:root:current mean train loss 7180.377623776355
INFO:root:current train perplexity287.9468688964844
INFO:root:current mean train loss 7185.217482427472
INFO:root:current train perplexity286.9765319824219
INFO:root:current mean train loss 7185.71758747738
INFO:root:current train perplexity286.4684753417969
INFO:root:current mean train loss 7185.562928691371
INFO:root:current train perplexity287.0441589355469
INFO:root:current mean train loss 7180.799250198623
INFO:root:current train perplexity287.2915954589844
INFO:root:current mean train loss 7170.157098136056
INFO:root:current train perplexity286.51287841796875
INFO:root:current mean train loss 7173.918170043801
INFO:root:current train perplexity286.5223083496094
INFO:root:current mean train loss 7177.060380169915
INFO:root:current train perplexity286.787353515625
INFO:root:current mean train loss 7176.505185369995
INFO:root:current train perplexity286.5366516113281
INFO:root:current mean train loss 7169.122907487489
INFO:root:current train perplexity286.2530212402344
INFO:root:current mean train loss 7174.536231328813
INFO:root:current train perplexity286.58966064453125
INFO:root:current mean train loss 7172.907349628197
INFO:root:current train perplexity286.5223083496094
INFO:root:current mean train loss 7177.470543397386
INFO:root:current train perplexity286.67633056640625
INFO:root:current mean train loss 7175.539229883695
INFO:root:current train perplexity286.46697998046875
INFO:root:current mean train loss 7172.877188530454
INFO:root:current train perplexity286.6987609863281
INFO:root:current mean train loss 7171.572896286717
INFO:root:current train perplexity286.4902038574219
INFO:root:current mean train loss 7171.671294819413
INFO:root:current train perplexity286.5033264160156
INFO:root:current mean train loss 7172.146712857555
INFO:root:current train perplexity286.5245056152344

100%|██████████| 1/1 [07:31<00:00, 451.75s/it][A100%|██████████| 1/1 [07:31<00:00, 451.75s/it]
INFO:root:final mean train loss: 7169.3678499915295
INFO:root:final train perplexity: 286.5892639160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.41s/it][A100%|██████████| 1/1 [00:39<00:00, 39.41s/it]
INFO:root:eval mean loss: 6919.629773728391
INFO:root:eval perplexity: 270.281982421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.64s/it][A100%|██████████| 1/1 [00:37<00:00, 37.64s/it]
INFO:root:eval mean loss: 6943.9324803994905
INFO:root:eval perplexity: 301.5108642578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/51
 26%|██▌       | 51/200 [7:26:56<21:54:35, 529.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7294.542214133523
INFO:root:current train perplexity292.6849365234375
INFO:root:current mean train loss 7222.051922533886
INFO:root:current train perplexity289.58612060546875
INFO:root:current mean train loss 7201.086504713933
INFO:root:current train perplexity288.3798522949219
INFO:root:current mean train loss 7188.11845756489
INFO:root:current train perplexity287.746337890625
INFO:root:current mean train loss 7174.706626793857
INFO:root:current train perplexity287.39105224609375
INFO:root:current mean train loss 7176.554802237467
INFO:root:current train perplexity287.0409851074219
INFO:root:current mean train loss 7177.18689954603
INFO:root:current train perplexity287.1816101074219
INFO:root:current mean train loss 7181.2159261280185
INFO:root:current train perplexity287.479736328125
INFO:root:current mean train loss 7177.8784162772445
INFO:root:current train perplexity286.976806640625
INFO:root:current mean train loss 7176.583329289596
INFO:root:current train perplexity286.9209899902344
INFO:root:current mean train loss 7178.067911402146
INFO:root:current train perplexity287.15789794921875
INFO:root:current mean train loss 7176.935569907135
INFO:root:current train perplexity287.21282958984375
INFO:root:current mean train loss 7176.796867286236
INFO:root:current train perplexity287.1428527832031
INFO:root:current mean train loss 7177.943366524067
INFO:root:current train perplexity287.11669921875
INFO:root:current mean train loss 7174.24343584797
INFO:root:current train perplexity286.93890380859375
INFO:root:current mean train loss 7172.896687046017
INFO:root:current train perplexity286.89813232421875
INFO:root:current mean train loss 7171.228291121136
INFO:root:current train perplexity286.9527282714844
INFO:root:current mean train loss 7171.92513177511
INFO:root:current train perplexity286.9097595214844
INFO:root:current mean train loss 7173.067409764788
INFO:root:current train perplexity286.85955810546875
INFO:root:current mean train loss 7172.278885834578
INFO:root:current train perplexity286.97503662109375

100%|██████████| 1/1 [07:32<00:00, 452.77s/it][A100%|██████████| 1/1 [07:32<00:00, 452.77s/it]
INFO:root:final mean train loss: 7171.027174464392
INFO:root:final train perplexity: 286.9646301269531
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.22s/it][A100%|██████████| 1/1 [00:39<00:00, 39.22s/it]
INFO:root:eval mean loss: 6925.796757258422
INFO:root:eval perplexity: 271.6343688964844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.70s/it][A100%|██████████| 1/1 [00:36<00:00, 36.70s/it]
INFO:root:eval mean loss: 6945.070007757092
INFO:root:eval perplexity: 301.79278564453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/52
 26%|██▌       | 52/200 [7:35:47<21:47:06, 529.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7172.960037415286
INFO:root:current train perplexity285.26458740234375
INFO:root:current mean train loss 7189.792912717726
INFO:root:current train perplexity287.5090637207031
INFO:root:current mean train loss 7188.247506832487
INFO:root:current train perplexity287.77996826171875
INFO:root:current mean train loss 7192.159180962386
INFO:root:current train perplexity289.04339599609375
INFO:root:current mean train loss 7180.956964528338
INFO:root:current train perplexity288.689697265625
INFO:root:current mean train loss 7176.3894373793955
INFO:root:current train perplexity287.9228515625
INFO:root:current mean train loss 7174.632201969711
INFO:root:current train perplexity287.6407165527344
INFO:root:current mean train loss 7169.89586264268
INFO:root:current train perplexity287.2861022949219
INFO:root:current mean train loss 7167.608413367957
INFO:root:current train perplexity286.91400146484375
INFO:root:current mean train loss 7160.403029231307
INFO:root:current train perplexity286.6543273925781
INFO:root:current mean train loss 7157.280455584892
INFO:root:current train perplexity286.49554443359375
INFO:root:current mean train loss 7154.75912834161
INFO:root:current train perplexity286.29888916015625
INFO:root:current mean train loss 7156.206093125853
INFO:root:current train perplexity286.20947265625
INFO:root:current mean train loss 7163.360476545553
INFO:root:current train perplexity286.4919738769531
INFO:root:current mean train loss 7162.190206783652
INFO:root:current train perplexity286.26202392578125
INFO:root:current mean train loss 7171.112442566034
INFO:root:current train perplexity286.6014404296875
INFO:root:current mean train loss 7171.597858467488
INFO:root:current train perplexity286.6312255859375
INFO:root:current mean train loss 7173.3594223767
INFO:root:current train perplexity286.5347595214844
INFO:root:current mean train loss 7173.095034363798
INFO:root:current train perplexity286.4697265625
INFO:root:current mean train loss 7168.152380069458
INFO:root:current train perplexity286.314453125

100%|██████████| 1/1 [08:18<00:00, 498.63s/it][A100%|██████████| 1/1 [08:18<00:00, 498.63s/it]
INFO:root:final mean train loss: 7168.152380069458
INFO:root:final train perplexity: 286.314453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.03s/it][A100%|██████████| 1/1 [00:45<00:00, 45.03s/it]
INFO:root:eval mean loss: 6853.923843708444
INFO:root:eval perplexity: 256.2865295410156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.60s/it][A100%|██████████| 1/1 [00:42<00:00, 42.60s/it]
INFO:root:eval mean loss: 6885.901665870179
INFO:root:eval perplexity: 287.4638366699219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/53
 26%|██▋       | 53/200 [7:45:36<22:21:40, 547.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7207.640546875
INFO:root:current train perplexity282.859130859375
INFO:root:current mean train loss 7156.641806640625
INFO:root:current train perplexity280.3748474121094
INFO:root:current mean train loss 7146.2574430338545
INFO:root:current train perplexity281.3961181640625
INFO:root:current mean train loss 7170.167548828125
INFO:root:current train perplexity282.25872802734375
INFO:root:current mean train loss 7165.715083984375
INFO:root:current train perplexity281.6462097167969
INFO:root:current mean train loss 7173.59970703125
INFO:root:current train perplexity281.3419189453125
INFO:root:current mean train loss 7165.625169503348
INFO:root:current train perplexity280.52008056640625
INFO:root:current mean train loss 7162.144985351562
INFO:root:current train perplexity279.76348876953125
INFO:root:current mean train loss 7153.479780273437
INFO:root:current train perplexity278.5460510253906
INFO:root:current mean train loss 7146.735676757812
INFO:root:current train perplexity278.04693603515625
INFO:root:current mean train loss 7146.241746715199
INFO:root:current train perplexity277.941162109375
INFO:root:current mean train loss 7141.123181152344
INFO:root:current train perplexity277.3426208496094
INFO:root:current mean train loss 7135.028991135818
INFO:root:current train perplexity276.3329162597656
INFO:root:current mean train loss 7128.153365652902
INFO:root:current train perplexity275.5557861328125
INFO:root:current mean train loss 7121.3290703125
INFO:root:current train perplexity274.6016845703125
INFO:root:current mean train loss 7113.943885192871
INFO:root:current train perplexity273.6457824707031
INFO:root:current mean train loss 7108.1982421875
INFO:root:current train perplexity272.8309631347656
INFO:root:current mean train loss 7104.46145046658
INFO:root:current train perplexity272.2048645019531
INFO:root:current mean train loss 7103.590750925165
INFO:root:current train perplexity271.6470642089844

100%|██████████| 1/1 [08:14<00:00, 494.31s/it][A100%|██████████| 1/1 [08:14<00:00, 494.31s/it]
INFO:root:final mean train loss: 7098.813604234627
INFO:root:final train perplexity: 271.0675048828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.64s/it][A100%|██████████| 1/1 [00:41<00:00, 41.64s/it]
INFO:root:eval mean loss: 6779.241796182402
INFO:root:eval perplexity: 241.2568817138672
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.87s/it][A100%|██████████| 1/1 [00:36<00:00, 36.90s/it]
INFO:root:eval mean loss: 6821.162124092697
INFO:root:eval perplexity: 272.5638732910156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/54
 27%|██▋       | 54/200 [7:55:11<22:32:44, 555.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6818.036305147059
INFO:root:current train perplexity252.91751098632812
INFO:root:current mean train loss 7032.954460470086
INFO:root:current train perplexity256.1168518066406
INFO:root:current mean train loss 7051.346891201037
INFO:root:current train perplexity258.01226806640625
INFO:root:current mean train loss 7043.805979827977
INFO:root:current train perplexity257.7774963378906
INFO:root:current mean train loss 7028.613969761691
INFO:root:current train perplexity256.9993896484375
INFO:root:current mean train loss 7022.824665475399
INFO:root:current train perplexity256.67816162109375
INFO:root:current mean train loss 7022.863301034491
INFO:root:current train perplexity256.744384765625
INFO:root:current mean train loss 7019.527456796984
INFO:root:current train perplexity256.4882507324219
INFO:root:current mean train loss 7019.230844672774
INFO:root:current train perplexity256.17840576171875
INFO:root:current mean train loss 7020.073757092591
INFO:root:current train perplexity255.93165588378906
INFO:root:current mean train loss 7022.7956886254
INFO:root:current train perplexity255.5939483642578
INFO:root:current mean train loss 7026.051375318235
INFO:root:current train perplexity255.5939483642578
INFO:root:current mean train loss 7020.41922094867
INFO:root:current train perplexity255.22335815429688
INFO:root:current mean train loss 7017.117373247082
INFO:root:current train perplexity254.94857788085938
INFO:root:current mean train loss 7015.425467330298
INFO:root:current train perplexity254.73143005371094
INFO:root:current mean train loss 7018.8276190157385
INFO:root:current train perplexity254.89279174804688
INFO:root:current mean train loss 7018.754217276399
INFO:root:current train perplexity254.72401428222656
INFO:root:current mean train loss 7020.5118421711195
INFO:root:current train perplexity254.7687225341797
INFO:root:current mean train loss 7020.042713188377
INFO:root:current train perplexity254.66378784179688
INFO:root:current mean train loss 7020.971701337947
INFO:root:current train perplexity254.6948699951172

100%|██████████| 1/1 [07:53<00:00, 473.82s/it][A100%|██████████| 1/1 [07:53<00:00, 473.82s/it]
INFO:root:final mean train loss: 7019.609956480672
INFO:root:final train perplexity: 254.6426544189453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.66s/it][A100%|██████████| 1/1 [00:43<00:00, 43.66s/it]
INFO:root:eval mean loss: 6756.85378920102
INFO:root:eval perplexity: 236.92539978027344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.25s/it][A100%|██████████| 1/1 [00:41<00:00, 41.25s/it]
INFO:root:eval mean loss: 6801.454583783522
INFO:root:eval perplexity: 268.1833801269531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/55
 28%|██▊       | 55/200 [8:04:32<22:27:17, 557.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7055.32479319853
INFO:root:current train perplexity254.31442260742188
INFO:root:current mean train loss 7005.153520726446
INFO:root:current train perplexity251.86602783203125
INFO:root:current mean train loss 6992.738878038194
INFO:root:current train perplexity250.85394287109375
INFO:root:current mean train loss 6986.859763870696
INFO:root:current train perplexity250.54541015625
INFO:root:current mean train loss 6991.728081347206
INFO:root:current train perplexity250.1068572998047
INFO:root:current mean train loss 6985.933863493387
INFO:root:current train perplexity249.76791381835938
INFO:root:current mean train loss 6982.330072733882
INFO:root:current train perplexity249.6248016357422
INFO:root:current mean train loss 7002.5210752565135
INFO:root:current train perplexity249.80137634277344
INFO:root:current mean train loss 7003.095605937126
INFO:root:current train perplexity249.7306365966797
INFO:root:current mean train loss 7008.3787452322
INFO:root:current train perplexity249.72694396972656
INFO:root:current mean train loss 7009.331239799927
INFO:root:current train perplexity249.72409057617188
INFO:root:current mean train loss 7003.24783244461
INFO:root:current train perplexity249.63861083984375
INFO:root:current mean train loss 6999.822700883813
INFO:root:current train perplexity249.56076049804688
INFO:root:current mean train loss 7004.127665781367
INFO:root:current train perplexity249.845458984375
INFO:root:current mean train loss 7002.648045921592
INFO:root:current train perplexity249.92172241210938
INFO:root:current mean train loss 7003.248767837863
INFO:root:current train perplexity250.21981811523438
INFO:root:current mean train loss 7002.970477212745
INFO:root:current train perplexity250.1521759033203
INFO:root:current mean train loss 7000.410504861411
INFO:root:current train perplexity250.09182739257812
INFO:root:current mean train loss 7000.785814125119
INFO:root:current train perplexity250.1278533935547
INFO:root:current mean train loss 6999.595318357356
INFO:root:current train perplexity250.16363525390625

100%|██████████| 1/1 [07:59<00:00, 479.71s/it][A100%|██████████| 1/1 [07:59<00:00, 479.71s/it]
INFO:root:final mean train loss: 6997.092161670094
INFO:root:final train perplexity: 250.1574249267578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.87s/it][A100%|██████████| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 6752.145447210217
INFO:root:eval perplexity: 236.02455139160156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.52s/it][A100%|██████████| 1/1 [00:40<00:00, 40.52s/it]
INFO:root:eval mean loss: 6796.907222233765
INFO:root:eval perplexity: 267.18255615234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/56
 28%|██▊       | 56/200 [8:13:59<22:24:36, 560.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6937.188639322917
INFO:root:current train perplexity247.68043518066406
INFO:root:current mean train loss 6964.0474764590235
INFO:root:current train perplexity248.05084228515625
INFO:root:current mean train loss 6969.494519967007
INFO:root:current train perplexity248.46043395996094
INFO:root:current mean train loss 6978.536762987446
INFO:root:current train perplexity248.97454833984375
INFO:root:current mean train loss 6984.1096954684035
INFO:root:current train perplexity249.456298828125
INFO:root:current mean train loss 6989.2260768772685
INFO:root:current train perplexity249.8336639404297
INFO:root:current mean train loss 6983.453913300451
INFO:root:current train perplexity249.6436004638672
INFO:root:current mean train loss 6991.411457249708
INFO:root:current train perplexity249.7257537841797
INFO:root:current mean train loss 7001.595142548289
INFO:root:current train perplexity250.35086059570312
INFO:root:current mean train loss 6998.790515021195
INFO:root:current train perplexity250.46083068847656
INFO:root:current mean train loss 6998.977715605673
INFO:root:current train perplexity250.4250030517578
INFO:root:current mean train loss 6997.268004045395
INFO:root:current train perplexity250.38536071777344
INFO:root:current mean train loss 6999.580767027003
INFO:root:current train perplexity250.32603454589844
INFO:root:current mean train loss 6999.327299511936
INFO:root:current train perplexity250.3329620361328
INFO:root:current mean train loss 6995.536567588086
INFO:root:current train perplexity250.10781860351562
INFO:root:current mean train loss 6994.366610755158
INFO:root:current train perplexity249.69384765625
INFO:root:current mean train loss 6996.271768589586
INFO:root:current train perplexity249.73468017578125
INFO:root:current mean train loss 6993.653324495378
INFO:root:current train perplexity249.5218505859375
INFO:root:current mean train loss 6993.604037671782
INFO:root:current train perplexity249.21412658691406
INFO:root:current mean train loss 6992.5093842100205
INFO:root:current train perplexity249.1183624267578

100%|██████████| 1/1 [07:40<00:00, 460.70s/it][A100%|██████████| 1/1 [07:40<00:00, 460.71s/it]
INFO:root:final mean train loss: 6991.765943749409
INFO:root:final train perplexity: 249.1080322265625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.36s/it][A100%|██████████| 1/1 [00:39<00:00, 39.36s/it]
INFO:root:eval mean loss: 6744.986993018617
INFO:root:eval perplexity: 234.66119384765625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.26s/it][A100%|██████████| 1/1 [00:36<00:00, 36.26s/it]
INFO:root:eval mean loss: 6794.503553891012
INFO:root:eval perplexity: 266.6552429199219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/57
 28%|██▊       | 57/200 [8:22:58<21:59:54, 553.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6928.158727309283
INFO:root:current train perplexity246.51173400878906
INFO:root:current mean train loss 6917.139221191406
INFO:root:current train perplexity244.79486083984375
INFO:root:current mean train loss 6927.36546576201
INFO:root:current train perplexity244.05767822265625
INFO:root:current mean train loss 6955.636889913808
INFO:root:current train perplexity245.4492950439453
INFO:root:current mean train loss 6960.118560530182
INFO:root:current train perplexity245.5911865234375
INFO:root:current mean train loss 6961.595099650638
INFO:root:current train perplexity245.7186279296875
INFO:root:current mean train loss 6971.409705247707
INFO:root:current train perplexity246.06005859375
INFO:root:current mean train loss 6967.110767364502
INFO:root:current train perplexity246.14479064941406
INFO:root:current mean train loss 6971.156884540611
INFO:root:current train perplexity246.17730712890625
INFO:root:current mean train loss 6973.905696143789
INFO:root:current train perplexity246.1146240234375
INFO:root:current mean train loss 6977.947615377019
INFO:root:current train perplexity246.2647705078125
INFO:root:current mean train loss 6974.690267066433
INFO:root:current train perplexity246.27040100097656
INFO:root:current mean train loss 6981.11639115487
INFO:root:current train perplexity246.60496520996094
INFO:root:current mean train loss 6978.609892906501
INFO:root:current train perplexity246.50973510742188
INFO:root:current mean train loss 6978.311682760878
INFO:root:current train perplexity246.7256317138672
INFO:root:current mean train loss 6981.3268874810665
INFO:root:current train perplexity246.9193572998047
INFO:root:current mean train loss 6981.517574612185
INFO:root:current train perplexity246.99790954589844
INFO:root:current mean train loss 6983.744113559636
INFO:root:current train perplexity247.09097290039062
INFO:root:current mean train loss 6985.84067889914
INFO:root:current train perplexity247.11972045898438
INFO:root:current mean train loss 6982.73745082452
INFO:root:current train perplexity247.01345825195312

100%|██████████| 1/1 [07:29<00:00, 449.66s/it][A100%|██████████| 1/1 [07:29<00:00, 449.66s/it]
INFO:root:final mean train loss: 6981.520399839062
INFO:root:final train perplexity: 247.1019287109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.41s/it][A100%|██████████| 1/1 [00:39<00:00, 39.41s/it]
INFO:root:eval mean loss: 6742.90197840481
INFO:root:eval perplexity: 234.26553344726562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.51s/it][A100%|██████████| 1/1 [00:36<00:00, 36.51s/it]
INFO:root:eval mean loss: 6791.145367561503
INFO:root:eval perplexity: 265.919921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/58
 29%|██▉       | 58/200 [8:31:46<21:32:26, 546.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6953.690090762868
INFO:root:current train perplexity248.32374572753906
INFO:root:current mean train loss 6906.115371621621
INFO:root:current train perplexity245.58380126953125
INFO:root:current mean train loss 6928.85604783443
INFO:root:current train perplexity247.10971069335938
INFO:root:current mean train loss 6943.945918729708
INFO:root:current train perplexity246.74740600585938
INFO:root:current mean train loss 6943.5158927996135
INFO:root:current train perplexity246.98553466796875
INFO:root:current mean train loss 6957.034077857906
INFO:root:current train perplexity247.19444274902344
INFO:root:current mean train loss 6963.348413976962
INFO:root:current train perplexity246.81224060058594
INFO:root:current mean train loss 6956.954908937102
INFO:root:current train perplexity246.33006286621094
INFO:root:current mean train loss 6962.657371115819
INFO:root:current train perplexity246.56581115722656
INFO:root:current mean train loss 6964.753101701301
INFO:root:current train perplexity246.82130432128906
INFO:root:current mean train loss 6961.5867700532835
INFO:root:current train perplexity246.50244140625
INFO:root:current mean train loss 6966.056275959257
INFO:root:current train perplexity246.4530792236328
INFO:root:current mean train loss 6967.623073093993
INFO:root:current train perplexity246.48410034179688
INFO:root:current mean train loss 6966.8863912313855
INFO:root:current train perplexity246.4532012939453
INFO:root:current mean train loss 6968.064863478536
INFO:root:current train perplexity246.2721710205078
INFO:root:current mean train loss 6971.385057361495
INFO:root:current train perplexity246.41053771972656
INFO:root:current mean train loss 6978.617132151799
INFO:root:current train perplexity246.59437561035156
INFO:root:current mean train loss 6978.659527365634
INFO:root:current train perplexity246.34793090820312
INFO:root:current mean train loss 6977.915815908903
INFO:root:current train perplexity246.32913208007812

100%|██████████| 1/1 [07:29<00:00, 449.22s/it][A100%|██████████| 1/1 [07:29<00:00, 449.22s/it]
INFO:root:final mean train loss: 6976.995967678391
INFO:root:final train perplexity: 246.22097778320312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.97s/it][A100%|██████████| 1/1 [00:38<00:00, 38.97s/it]
INFO:root:eval mean loss: 6748.093569924646
INFO:root:eval perplexity: 235.25184631347656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.42s/it][A100%|██████████| 1/1 [00:36<00:00, 36.42s/it]
INFO:root:eval mean loss: 6801.778818982712
INFO:root:eval perplexity: 268.2548828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/59
 30%|██▉       | 59/200 [8:40:33<21:09:46, 540.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7090.958984375
INFO:root:current train perplexity271.3165588378906
INFO:root:current mean train loss 6984.454666436887
INFO:root:current train perplexity244.8258056640625
INFO:root:current mean train loss 6977.297073213181
INFO:root:current train perplexity245.80921936035156
INFO:root:current mean train loss 6992.837337670737
INFO:root:current train perplexity246.52853393554688
INFO:root:current mean train loss 6994.444286137671
INFO:root:current train perplexity246.25796508789062
INFO:root:current mean train loss 6993.978945545941
INFO:root:current train perplexity246.48974609375
INFO:root:current mean train loss 6999.8008299159055
INFO:root:current train perplexity246.3879852294922
INFO:root:current mean train loss 7002.964733851941
INFO:root:current train perplexity246.47576904296875
INFO:root:current mean train loss 7006.9214835226385
INFO:root:current train perplexity246.74598693847656
INFO:root:current mean train loss 6998.850891248614
INFO:root:current train perplexity246.54747009277344
INFO:root:current mean train loss 6991.188256787207
INFO:root:current train perplexity246.37905883789062
INFO:root:current mean train loss 6986.481940683133
INFO:root:current train perplexity246.1572265625
INFO:root:current mean train loss 6983.141160809459
INFO:root:current train perplexity246.169677734375
INFO:root:current mean train loss 6984.724311230919
INFO:root:current train perplexity246.45272827148438
INFO:root:current mean train loss 6984.463986645752
INFO:root:current train perplexity246.407958984375
INFO:root:current mean train loss 6980.34737667485
INFO:root:current train perplexity246.27639770507812
INFO:root:current mean train loss 6978.119806601611
INFO:root:current train perplexity246.21334838867188
INFO:root:current mean train loss 6979.098797772382
INFO:root:current train perplexity246.2584228515625
INFO:root:current mean train loss 6981.198525618237
INFO:root:current train perplexity246.3091583251953
INFO:root:current mean train loss 6977.426043874458
INFO:root:current train perplexity246.1044158935547

100%|██████████| 1/1 [07:26<00:00, 446.39s/it][A100%|██████████| 1/1 [07:26<00:00, 446.39s/it]
INFO:root:final mean train loss: 6976.551241214384
INFO:root:final train perplexity: 246.1348114013672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.30s/it][A100%|██████████| 1/1 [00:40<00:00, 40.30s/it]
INFO:root:eval mean loss: 6754.758879100177
INFO:root:eval perplexity: 236.52410888671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.94s/it][A100%|██████████| 1/1 [00:36<00:00, 36.94s/it]
INFO:root:eval mean loss: 6812.203617609985
INFO:root:eval perplexity: 270.5638732910156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/60
 30%|███       | 60/200 [8:49:19<20:50:41, 536.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6974.16455078125
INFO:root:current train perplexity241.7320098876953
INFO:root:current mean train loss 6995.531438747374
INFO:root:current train perplexity246.9725799560547
INFO:root:current mean train loss 6988.133869327911
INFO:root:current train perplexity246.55287170410156
INFO:root:current mean train loss 6987.973443010384
INFO:root:current train perplexity245.94651794433594
INFO:root:current mean train loss 6999.258762259472
INFO:root:current train perplexity246.6924591064453
INFO:root:current mean train loss 6995.743967515655
INFO:root:current train perplexity246.36073303222656
INFO:root:current mean train loss 7004.960997450525
INFO:root:current train perplexity247.21282958984375
INFO:root:current mean train loss 7013.624935484397
INFO:root:current train perplexity247.455078125
INFO:root:current mean train loss 7011.024648365957
INFO:root:current train perplexity247.5915069580078
INFO:root:current mean train loss 7002.171247513432
INFO:root:current train perplexity247.30255126953125
INFO:root:current mean train loss 6996.183883652018
INFO:root:current train perplexity247.07717895507812
INFO:root:current mean train loss 6988.354671965762
INFO:root:current train perplexity246.73916625976562
INFO:root:current mean train loss 6986.110287873641
INFO:root:current train perplexity246.61965942382812
INFO:root:current mean train loss 6988.216363381942
INFO:root:current train perplexity246.65176391601562
INFO:root:current mean train loss 6986.154452409267
INFO:root:current train perplexity246.2907257080078
INFO:root:current mean train loss 6986.232003026765
INFO:root:current train perplexity246.42041015625
INFO:root:current mean train loss 6981.730472670726
INFO:root:current train perplexity246.314453125
INFO:root:current mean train loss 6981.069463759817
INFO:root:current train perplexity246.11709594726562
INFO:root:current mean train loss 6978.24163989486
INFO:root:current train perplexity245.9951934814453
INFO:root:current mean train loss 6978.292267243274
INFO:root:current train perplexity246.09666442871094

100%|██████████| 1/1 [07:18<00:00, 438.89s/it][A100%|██████████| 1/1 [07:18<00:00, 438.89s/it]
INFO:root:final mean train loss: 6976.3605246647285
INFO:root:final train perplexity: 246.0976104736328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.50s/it][A100%|██████████| 1/1 [00:38<00:00, 38.50s/it]
INFO:root:eval mean loss: 6748.024860787899
INFO:root:eval perplexity: 235.2387237548828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.76s/it][A100%|██████████| 1/1 [00:35<00:00, 35.76s/it]
INFO:root:eval mean loss: 6802.718968168218
INFO:root:eval perplexity: 268.4620361328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/61
 30%|███       | 61/200 [8:57:54<20:27:37, 529.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6928.795084635417
INFO:root:current train perplexity242.1905059814453
INFO:root:current mean train loss 6948.1113065831805
INFO:root:current train perplexity245.40481567382812
INFO:root:current mean train loss 6963.714837543035
INFO:root:current train perplexity245.16400146484375
INFO:root:current mean train loss 6969.569677443731
INFO:root:current train perplexity245.2217559814453
INFO:root:current mean train loss 6969.046689094754
INFO:root:current train perplexity245.0756378173828
INFO:root:current mean train loss 6979.6880948650305
INFO:root:current train perplexity245.35263061523438
INFO:root:current mean train loss 6980.197842963837
INFO:root:current train perplexity245.0936279296875
INFO:root:current mean train loss 6977.880341239597
INFO:root:current train perplexity245.09632873535156
INFO:root:current mean train loss 6975.079029138008
INFO:root:current train perplexity245.07691955566406
INFO:root:current mean train loss 6975.301122942541
INFO:root:current train perplexity245.16119384765625
INFO:root:current mean train loss 6973.242362828789
INFO:root:current train perplexity245.30291748046875
INFO:root:current mean train loss 6973.436005068497
INFO:root:current train perplexity245.23040771484375
INFO:root:current mean train loss 6975.234115452442
INFO:root:current train perplexity245.10134887695312
INFO:root:current mean train loss 6971.668353600417
INFO:root:current train perplexity244.9683837890625
INFO:root:current mean train loss 6969.843130127633
INFO:root:current train perplexity244.67921447753906
INFO:root:current mean train loss 6974.614168802897
INFO:root:current train perplexity244.7787628173828
INFO:root:current mean train loss 6975.5102174940785
INFO:root:current train perplexity244.6705780029297
INFO:root:current mean train loss 6971.590425412226
INFO:root:current train perplexity244.44784545898438
INFO:root:current mean train loss 6972.71895025914
INFO:root:current train perplexity244.49168395996094
INFO:root:current mean train loss 6972.373222414127
INFO:root:current train perplexity244.45367431640625

100%|██████████| 1/1 [07:33<00:00, 453.16s/it][A100%|██████████| 1/1 [07:33<00:00, 453.16s/it]
INFO:root:final mean train loss: 6967.700791567188
INFO:root:final train perplexity: 244.4215087890625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.76s/it][A100%|██████████| 1/1 [00:38<00:00, 38.76s/it]
INFO:root:eval mean loss: 6734.272469594969
INFO:root:eval perplexity: 232.63540649414062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.44s/it][A100%|██████████| 1/1 [00:36<00:00, 36.44s/it]
INFO:root:eval mean loss: 6793.601552976784
INFO:root:eval perplexity: 266.45745849609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/62
 31%|███       | 62/200 [9:06:45<20:19:24, 530.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6895.036031471109
INFO:root:current train perplexity241.6808319091797
INFO:root:current mean train loss 6963.589307598039
INFO:root:current train perplexity243.9942626953125
INFO:root:current mean train loss 6981.405254137846
INFO:root:current train perplexity244.7154998779297
INFO:root:current mean train loss 6979.213009583038
INFO:root:current train perplexity245.9477996826172
INFO:root:current mean train loss 6979.099302178187
INFO:root:current train perplexity245.32046508789062
INFO:root:current mean train loss 6987.446435635172
INFO:root:current train perplexity245.5636749267578
INFO:root:current mean train loss 6981.426505820492
INFO:root:current train perplexity245.45455932617188
INFO:root:current mean train loss 6984.952786510209
INFO:root:current train perplexity245.77687072753906
INFO:root:current mean train loss 6985.8329957915075
INFO:root:current train perplexity246.0800018310547
INFO:root:current mean train loss 6989.0907583166645
INFO:root:current train perplexity246.2789764404297
INFO:root:current mean train loss 6993.172268221748
INFO:root:current train perplexity246.5578155517578
INFO:root:current mean train loss 6996.585942581852
INFO:root:current train perplexity246.73374938964844
INFO:root:current mean train loss 6992.843751169069
INFO:root:current train perplexity246.75234985351562
INFO:root:current mean train loss 6988.20092430594
INFO:root:current train perplexity246.51690673828125
INFO:root:current mean train loss 6987.178223664401
INFO:root:current train perplexity246.3682403564453
INFO:root:current mean train loss 6984.50423952632
INFO:root:current train perplexity246.18035888671875
INFO:root:current mean train loss 6984.871366395852
INFO:root:current train perplexity246.18504333496094
INFO:root:current mean train loss 6981.466835035029
INFO:root:current train perplexity246.02064514160156
INFO:root:current mean train loss 6976.929472740573
INFO:root:current train perplexity245.8763885498047
INFO:root:current mean train loss 6977.292827490959
INFO:root:current train perplexity245.94287109375

100%|██████████| 1/1 [07:27<00:00, 447.53s/it][A100%|██████████| 1/1 [07:27<00:00, 447.53s/it]
INFO:root:final mean train loss: 6975.670607389372
INFO:root:final train perplexity: 245.96363830566406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.44s/it][A100%|██████████| 1/1 [00:38<00:00, 38.44s/it]
INFO:root:eval mean loss: 6735.887575839428
INFO:root:eval perplexity: 232.9396514892578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.50s/it][A100%|██████████| 1/1 [00:36<00:00, 36.51s/it]
INFO:root:eval mean loss: 6795.285494757037
INFO:root:eval perplexity: 266.8265686035156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/63
 32%|███▏      | 63/200 [9:15:30<20:06:56, 528.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6973.739501953125
INFO:root:current train perplexity246.57708740234375
INFO:root:current mean train loss 6980.240734145221
INFO:root:current train perplexity244.90963745117188
INFO:root:current mean train loss 6991.861543330439
INFO:root:current train perplexity246.59507751464844
INFO:root:current mean train loss 6985.635065192146
INFO:root:current train perplexity246.69540405273438
INFO:root:current mean train loss 6984.285909449801
INFO:root:current train perplexity246.77835083007812
INFO:root:current mean train loss 6987.8997592859105
INFO:root:current train perplexity246.59461975097656
INFO:root:current mean train loss 6986.8463765158585
INFO:root:current train perplexity246.80552673339844
INFO:root:current mean train loss 6990.468592101258
INFO:root:current train perplexity246.496337890625
INFO:root:current mean train loss 6983.690562702048
INFO:root:current train perplexity246.4486083984375
INFO:root:current mean train loss 6981.074819285599
INFO:root:current train perplexity246.30140686035156
INFO:root:current mean train loss 6982.057442410192
INFO:root:current train perplexity246.30282592773438
INFO:root:current mean train loss 6980.136347739717
INFO:root:current train perplexity246.39515686035156
INFO:root:current mean train loss 6980.624374461737
INFO:root:current train perplexity246.31703186035156
INFO:root:current mean train loss 6986.364494112112
INFO:root:current train perplexity246.3787078857422
INFO:root:current mean train loss 6980.288451982356
INFO:root:current train perplexity246.26394653320312
INFO:root:current mean train loss 6978.030046402269
INFO:root:current train perplexity246.1767120361328
INFO:root:current mean train loss 6979.725214609843
INFO:root:current train perplexity246.0946807861328
INFO:root:current mean train loss 6977.122374039989
INFO:root:current train perplexity246.0340118408203
INFO:root:current mean train loss 6977.466743607954
INFO:root:current train perplexity246.06991577148438
INFO:root:current mean train loss 6979.399827242623
INFO:root:current train perplexity246.17588806152344

100%|██████████| 1/1 [07:29<00:00, 449.59s/it][A100%|██████████| 1/1 [07:29<00:00, 449.59s/it]
INFO:root:final mean train loss: 6976.53327133171
INFO:root:final train perplexity: 246.1311798095703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.50s/it][A100%|██████████| 1/1 [00:38<00:00, 38.50s/it]
INFO:root:eval mean loss: 6732.21281790226
INFO:root:eval perplexity: 232.24790954589844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.67s/it][A100%|██████████| 1/1 [00:35<00:00, 35.67s/it]
INFO:root:eval mean loss: 6791.821773015015
INFO:root:eval perplexity: 266.06793212890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/64
 32%|███▏      | 64/200 [9:24:16<19:56:32, 527.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6913.006218570402
INFO:root:current train perplexity245.81777954101562
INFO:root:current mean train loss 6929.016121114639
INFO:root:current train perplexity244.86082458496094
INFO:root:current mean train loss 6947.297014508928
INFO:root:current train perplexity245.30397033691406
INFO:root:current mean train loss 6960.631312328408
INFO:root:current train perplexity245.80909729003906
INFO:root:current mean train loss 6958.369508590542
INFO:root:current train perplexity245.774169921875
INFO:root:current mean train loss 6958.559894724233
INFO:root:current train perplexity245.8429718017578
INFO:root:current mean train loss 6972.764108982669
INFO:root:current train perplexity246.2853240966797
INFO:root:current mean train loss 6972.775363946354
INFO:root:current train perplexity246.4374542236328
INFO:root:current mean train loss 6972.227116839593
INFO:root:current train perplexity246.3526153564453
INFO:root:current mean train loss 6972.718375502628
INFO:root:current train perplexity246.3223114013672
INFO:root:current mean train loss 6978.066494742554
INFO:root:current train perplexity246.4783477783203
INFO:root:current mean train loss 6982.307767744313
INFO:root:current train perplexity246.4058380126953
INFO:root:current mean train loss 6981.3364618237665
INFO:root:current train perplexity246.2504425048828
INFO:root:current mean train loss 6981.067610935247
INFO:root:current train perplexity246.2876739501953
INFO:root:current mean train loss 6976.32706043523
INFO:root:current train perplexity246.18011474609375
INFO:root:current mean train loss 6975.515046877462
INFO:root:current train perplexity246.05197143554688
INFO:root:current mean train loss 6976.932384190223
INFO:root:current train perplexity246.0994873046875
INFO:root:current mean train loss 6978.996625749773
INFO:root:current train perplexity246.20372009277344
INFO:root:current mean train loss 6977.362292266991
INFO:root:current train perplexity246.20372009277344

100%|██████████| 1/1 [07:18<00:00, 438.44s/it][A100%|██████████| 1/1 [07:18<00:00, 438.44s/it]
INFO:root:final mean train loss: 6976.9203929198975
INFO:root:final train perplexity: 246.2064208984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.10s/it][A100%|██████████| 1/1 [00:38<00:00, 38.10s/it]
INFO:root:eval mean loss: 6740.306496911015
INFO:root:eval perplexity: 233.7740936279297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.02s/it][A100%|██████████| 1/1 [00:35<00:00, 35.02s/it]
INFO:root:eval mean loss: 6797.008669589428
INFO:root:eval perplexity: 267.2049560546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/65
 32%|███▎      | 65/200 [9:32:50<19:38:22, 523.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7087.5263671875
INFO:root:current train perplexity249.8735809326172
INFO:root:current mean train loss 6954.712407038762
INFO:root:current train perplexity246.6396484375
INFO:root:current mean train loss 6970.929622874541
INFO:root:current train perplexity244.64573669433594
INFO:root:current mean train loss 6962.788811934622
INFO:root:current train perplexity244.74560546875
INFO:root:current mean train loss 6975.002523592203
INFO:root:current train perplexity244.93533325195312
INFO:root:current mean train loss 6960.893470400855
INFO:root:current train perplexity244.2705078125
INFO:root:current mean train loss 6963.844418557274
INFO:root:current train perplexity244.64747619628906
INFO:root:current mean train loss 6964.2304167314005
INFO:root:current train perplexity244.70314025878906
INFO:root:current mean train loss 6972.048413328864
INFO:root:current train perplexity245.3529815673828
INFO:root:current mean train loss 6975.348992001694
INFO:root:current train perplexity245.41734313964844
INFO:root:current mean train loss 6975.842415007937
INFO:root:current train perplexity245.5758514404297
INFO:root:current mean train loss 6976.179010363592
INFO:root:current train perplexity245.6734161376953
INFO:root:current mean train loss 6976.925681079345
INFO:root:current train perplexity245.77593994140625
INFO:root:current mean train loss 6977.583804265122
INFO:root:current train perplexity245.7285919189453
INFO:root:current mean train loss 6978.514159808471
INFO:root:current train perplexity245.80230712890625
INFO:root:current mean train loss 6980.543308014565
INFO:root:current train perplexity245.88412475585938
INFO:root:current mean train loss 6980.015306582178
INFO:root:current train perplexity245.79034423828125
INFO:root:current mean train loss 6974.5414680874965
INFO:root:current train perplexity245.58216857910156
INFO:root:current mean train loss 6975.981067192264
INFO:root:current train perplexity245.61297607421875
INFO:root:current mean train loss 6975.7899975175615
INFO:root:current train perplexity245.4215545654297

100%|██████████| 1/1 [07:35<00:00, 455.42s/it][A100%|██████████| 1/1 [07:35<00:00, 455.42s/it]
INFO:root:final mean train loss: 6972.890576122628
INFO:root:final train perplexity: 245.42471313476562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.90s/it][A100%|██████████| 1/1 [00:44<00:00, 44.90s/it]
INFO:root:eval mean loss: 6742.503308884641
INFO:root:eval perplexity: 234.19003295898438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.13s/it][A100%|██████████| 1/1 [00:43<00:00, 43.13s/it]
INFO:root:eval mean loss: 6801.350308032746
INFO:root:eval perplexity: 268.1603698730469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/66
 33%|███▎      | 66/200 [9:41:56<19:44:42, 530.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7088.036225818452
INFO:root:current train perplexity251.08981323242188
INFO:root:current mean train loss 6961.371485182077
INFO:root:current train perplexity246.9928436279297
INFO:root:current mean train loss 6986.7196183010465
INFO:root:current train perplexity247.0714111328125
INFO:root:current mean train loss 6971.898629161799
INFO:root:current train perplexity246.20770263671875
INFO:root:current mean train loss 6982.750088145784
INFO:root:current train perplexity246.16275024414062
INFO:root:current mean train loss 6968.663398025132
INFO:root:current train perplexity245.99424743652344
INFO:root:current mean train loss 6964.716283432719
INFO:root:current train perplexity245.34140014648438
INFO:root:current mean train loss 6972.3411216788745
INFO:root:current train perplexity245.64646911621094
INFO:root:current mean train loss 6969.861927622564
INFO:root:current train perplexity245.6219940185547
INFO:root:current mean train loss 6975.258633724383
INFO:root:current train perplexity245.78038024902344
INFO:root:current mean train loss 6973.372217131642
INFO:root:current train perplexity245.60348510742188
INFO:root:current mean train loss 6970.591000641169
INFO:root:current train perplexity245.49635314941406
INFO:root:current mean train loss 6971.890150715346
INFO:root:current train perplexity245.45315551757812
INFO:root:current mean train loss 6970.254824041328
INFO:root:current train perplexity245.14552307128906
INFO:root:current mean train loss 6973.856255978954
INFO:root:current train perplexity245.40365600585938
INFO:root:current mean train loss 6976.371268709422
INFO:root:current train perplexity245.35638427734375
INFO:root:current mean train loss 6972.001470868195
INFO:root:current train perplexity245.27706909179688
INFO:root:current mean train loss 6971.515659613779
INFO:root:current train perplexity245.22303771972656
INFO:root:current mean train loss 6975.3908754418935
INFO:root:current train perplexity245.3279571533203
INFO:root:current mean train loss 6975.699413452467
INFO:root:current train perplexity245.2921600341797

100%|██████████| 1/1 [08:27<00:00, 507.56s/it][A100%|██████████| 1/1 [08:27<00:00, 507.56s/it]
INFO:root:final mean train loss: 6971.576920548293
INFO:root:final train perplexity: 245.17031860351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.98s/it][A100%|██████████| 1/1 [00:44<00:00, 44.98s/it]
INFO:root:eval mean loss: 6725.277310851618
INFO:root:eval perplexity: 230.94821166992188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.71s/it][A100%|██████████| 1/1 [00:42<00:00, 42.72s/it]
INFO:root:eval mean loss: 6781.6137288411455
INFO:root:eval perplexity: 263.8443603515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/67
 34%|███▎      | 67/200 [9:51:54<20:20:42, 550.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6971.4085115131575
INFO:root:current train perplexity242.28013610839844
INFO:root:current mean train loss 6988.304376132246
INFO:root:current train perplexity243.74203491210938
INFO:root:current mean train loss 6983.6139500722165
INFO:root:current train perplexity245.13348388671875
INFO:root:current mean train loss 6973.34571901581
INFO:root:current train perplexity244.7760772705078
INFO:root:current mean train loss 6985.9155217697635
INFO:root:current train perplexity245.51612854003906
INFO:root:current mean train loss 6980.200667257202
INFO:root:current train perplexity245.05215454101562
INFO:root:current mean train loss 6973.71277352934
INFO:root:current train perplexity244.75973510742188
INFO:root:current mean train loss 6966.939466357554
INFO:root:current train perplexity244.66392517089844
INFO:root:current mean train loss 6966.83008453442
INFO:root:current train perplexity244.64573669433594
INFO:root:current mean train loss 6971.9270222547975
INFO:root:current train perplexity244.9098663330078
INFO:root:current mean train loss 6973.608608238499
INFO:root:current train perplexity245.12928771972656
INFO:root:current mean train loss 6976.889463079416
INFO:root:current train perplexity245.10333251953125
INFO:root:current mean train loss 6977.824285011106
INFO:root:current train perplexity245.07598876953125
INFO:root:current mean train loss 6978.8244260323245
INFO:root:current train perplexity244.94151306152344
INFO:root:current mean train loss 6979.368015337057
INFO:root:current train perplexity245.09457397460938
INFO:root:current mean train loss 6977.686856789459
INFO:root:current train perplexity245.03158569335938
INFO:root:current mean train loss 6974.744102170615
INFO:root:current train perplexity245.03988647460938
INFO:root:current mean train loss 6971.925070741783
INFO:root:current train perplexity244.97586059570312
INFO:root:current mean train loss 6971.801902596658
INFO:root:current train perplexity244.91990661621094
INFO:root:current mean train loss 6971.917735443272
INFO:root:current train perplexity244.8728485107422

100%|██████████| 1/1 [08:26<00:00, 506.37s/it][A100%|██████████| 1/1 [08:26<00:00, 506.37s/it]
INFO:root:final mean train loss: 6970.023599767949
INFO:root:final train perplexity: 244.86993408203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.96s/it][A100%|██████████| 1/1 [00:41<00:00, 41.96s/it]
INFO:root:eval mean loss: 6745.282145182292
INFO:root:eval perplexity: 234.71726989746094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.78s/it][A100%|██████████| 1/1 [00:36<00:00, 36.78s/it]
INFO:root:eval mean loss: 6803.5036551834
INFO:root:eval perplexity: 268.63555908203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/68
 34%|███▍      | 68/200 [10:01:42<20:35:50, 561.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7015.776056463068
INFO:root:current train perplexity247.12066650390625
INFO:root:current mean train loss 6974.715495841734
INFO:root:current train perplexity243.05032348632812
INFO:root:current mean train loss 6967.691436887255
INFO:root:current train perplexity244.07862854003906
INFO:root:current mean train loss 6974.518495543574
INFO:root:current train perplexity245.57455444335938
INFO:root:current mean train loss 6983.078776399382
INFO:root:current train perplexity245.91708374023438
INFO:root:current mean train loss 6974.2068614512955
INFO:root:current train perplexity245.2412872314453
INFO:root:current mean train loss 6974.665555671517
INFO:root:current train perplexity245.57760620117188
INFO:root:current mean train loss 6975.2992297444125
INFO:root:current train perplexity245.522216796875
INFO:root:current mean train loss 6985.0854280884505
INFO:root:current train perplexity245.20982360839844
INFO:root:current mean train loss 6994.5403821375985
INFO:root:current train perplexity245.1929931640625
INFO:root:current mean train loss 6991.734120445793
INFO:root:current train perplexity244.76498413085938
INFO:root:current mean train loss 6988.0046828497025
INFO:root:current train perplexity244.64247131347656
INFO:root:current mean train loss 6985.993380384712
INFO:root:current train perplexity244.5886993408203
INFO:root:current mean train loss 6980.153079234894
INFO:root:current train perplexity244.25186157226562
INFO:root:current mean train loss 6972.022392477448
INFO:root:current train perplexity244.2239227294922
INFO:root:current mean train loss 6968.843364399116
INFO:root:current train perplexity244.0305633544922
INFO:root:current mean train loss 6969.555322708176
INFO:root:current train perplexity244.09002685546875
INFO:root:current mean train loss 6968.3680650151355
INFO:root:current train perplexity244.15895080566406
INFO:root:current mean train loss 6967.814921137971
INFO:root:current train perplexity244.131591796875
INFO:root:current mean train loss 6966.299744495284
INFO:root:current train perplexity244.13031005859375

100%|██████████| 1/1 [07:32<00:00, 452.18s/it][A100%|██████████| 1/1 [07:32<00:00, 452.18s/it]
INFO:root:final mean train loss: 6966.37808961623
INFO:root:final train perplexity: 244.1666259765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.18s/it][A100%|██████████| 1/1 [00:38<00:00, 38.18s/it]
INFO:root:eval mean loss: 6733.418910682624
INFO:root:eval perplexity: 232.47471618652344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.55s/it][A100%|██████████| 1/1 [00:36<00:00, 36.55s/it]
INFO:root:eval mean loss: 6793.864946081283
INFO:root:eval perplexity: 266.5152893066406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/69
 34%|███▍      | 69/200 [10:10:31<20:05:08, 551.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6944.586337619357
INFO:root:current train perplexity246.46142578125
INFO:root:current mean train loss 6958.72238372093
INFO:root:current train perplexity245.468017578125
INFO:root:current mean train loss 6963.976399141199
INFO:root:current train perplexity244.74000549316406
INFO:root:current mean train loss 6970.581780546455
INFO:root:current train perplexity244.67991638183594
INFO:root:current mean train loss 6977.744155107918
INFO:root:current train perplexity245.22608947753906
INFO:root:current mean train loss 6978.032019981971
INFO:root:current train perplexity245.15487670898438
INFO:root:current mean train loss 6976.401319231306
INFO:root:current train perplexity245.45994567871094
INFO:root:current mean train loss 6981.315812343143
INFO:root:current train perplexity245.3342742919922
INFO:root:current mean train loss 6986.568654471581
INFO:root:current train perplexity245.2671356201172
INFO:root:current mean train loss 6979.738165710198
INFO:root:current train perplexity245.2877197265625
INFO:root:current mean train loss 6979.414167717322
INFO:root:current train perplexity245.074462890625
INFO:root:current mean train loss 6980.729764658437
INFO:root:current train perplexity245.0325164794922
INFO:root:current mean train loss 6981.066981285623
INFO:root:current train perplexity245.05015563964844
INFO:root:current mean train loss 6976.4942505950485
INFO:root:current train perplexity244.8831329345703
INFO:root:current mean train loss 6976.675563314687
INFO:root:current train perplexity244.9313507080078
INFO:root:current mean train loss 6974.714949047292
INFO:root:current train perplexity244.76988220214844
INFO:root:current mean train loss 6975.224472410941
INFO:root:current train perplexity244.79171752929688
INFO:root:current mean train loss 6975.716925834156
INFO:root:current train perplexity244.66439819335938
INFO:root:current mean train loss 6973.847327859992
INFO:root:current train perplexity244.46311950683594
INFO:root:current mean train loss 6968.932636005641
INFO:root:current train perplexity244.3401641845703

100%|██████████| 1/1 [07:26<00:00, 446.31s/it][A100%|██████████| 1/1 [07:26<00:00, 446.31s/it]
INFO:root:final mean train loss: 6967.21131903898
INFO:root:final train perplexity: 244.32711791992188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.47s/it][A100%|██████████| 1/1 [00:39<00:00, 39.47s/it]
INFO:root:eval mean loss: 6731.1549444536795
INFO:root:eval perplexity: 232.04920959472656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.10s/it][A100%|██████████| 1/1 [00:37<00:00, 37.10s/it]
INFO:root:eval mean loss: 6790.716738869958
INFO:root:eval perplexity: 265.82611083984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/70
 35%|███▌      | 70/200 [10:19:16<19:38:37, 543.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6954.855951544944
INFO:root:current train perplexity245.85821533203125
INFO:root:current mean train loss 6956.172099764385
INFO:root:current train perplexity243.10769653320312
INFO:root:current mean train loss 6975.449240714209
INFO:root:current train perplexity243.5870361328125
INFO:root:current mean train loss 6968.774210716581
INFO:root:current train perplexity243.26239013671875
INFO:root:current mean train loss 6966.988681660596
INFO:root:current train perplexity243.22410583496094
INFO:root:current mean train loss 6961.9492345010085
INFO:root:current train perplexity242.7109832763672
INFO:root:current mean train loss 6949.860241716936
INFO:root:current train perplexity241.94615173339844
INFO:root:current mean train loss 6950.470165953739
INFO:root:current train perplexity241.7090606689453
INFO:root:current mean train loss 6947.523572614947
INFO:root:current train perplexity241.54074096679688
INFO:root:current mean train loss 6947.2584948100985
INFO:root:current train perplexity241.26734924316406
INFO:root:current mean train loss 6940.3915087487085
INFO:root:current train perplexity240.9688720703125
INFO:root:current mean train loss 6946.565991067205
INFO:root:current train perplexity240.8954620361328
INFO:root:current mean train loss 6944.946561045384
INFO:root:current train perplexity240.57989501953125
INFO:root:current mean train loss 6945.018175733441
INFO:root:current train perplexity240.3815155029297
INFO:root:current mean train loss 6943.882440960271
INFO:root:current train perplexity240.34071350097656
INFO:root:current mean train loss 6942.84155841921
INFO:root:current train perplexity240.1758575439453
INFO:root:current mean train loss 6943.608791028345
INFO:root:current train perplexity239.9661407470703
INFO:root:current mean train loss 6943.494590422373
INFO:root:current train perplexity239.78427124023438
INFO:root:current mean train loss 6946.641191344213
INFO:root:current train perplexity239.67007446289062

100%|██████████| 1/1 [07:19<00:00, 439.40s/it][A100%|██████████| 1/1 [07:19<00:00, 439.40s/it]
INFO:root:final mean train loss: 6942.096681534252
INFO:root:final train perplexity: 239.5321807861328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.44s/it][A100%|██████████| 1/1 [00:37<00:00, 37.44s/it]
INFO:root:eval mean loss: 6692.642344373337
INFO:root:eval perplexity: 224.92889404296875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.78s/it][A100%|██████████| 1/1 [00:36<00:00, 36.78s/it]
INFO:root:eval mean loss: 6753.620763034685
INFO:root:eval perplexity: 257.841552734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/71
 36%|███▌      | 71/200 [10:27:53<19:11:37, 535.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6986.180826822917
INFO:root:current train perplexity231.15081787109375
INFO:root:current mean train loss 6986.8057239460495
INFO:root:current train perplexity239.33432006835938
INFO:root:current mean train loss 6950.003387154885
INFO:root:current train perplexity238.22044372558594
INFO:root:current mean train loss 6929.748509625204
INFO:root:current train perplexity237.5189208984375
INFO:root:current mean train loss 6917.424932169797
INFO:root:current train perplexity237.25856018066406
INFO:root:current mean train loss 6923.330393674345
INFO:root:current train perplexity237.2671661376953
INFO:root:current mean train loss 6919.65751501908
INFO:root:current train perplexity236.9964599609375
INFO:root:current mean train loss 6926.549983124557
INFO:root:current train perplexity236.9865264892578
INFO:root:current mean train loss 6928.631691149388
INFO:root:current train perplexity237.33448791503906
INFO:root:current mean train loss 6929.746276451262
INFO:root:current train perplexity237.3570098876953
INFO:root:current mean train loss 6926.754885239345
INFO:root:current train perplexity237.09469604492188
INFO:root:current mean train loss 6927.8027851456545
INFO:root:current train perplexity237.2207794189453
INFO:root:current mean train loss 6928.079681750752
INFO:root:current train perplexity237.03353881835938
INFO:root:current mean train loss 6920.997190700374
INFO:root:current train perplexity236.98245239257812
INFO:root:current mean train loss 6926.486646931677
INFO:root:current train perplexity236.94155883789062
INFO:root:current mean train loss 6931.173927661749
INFO:root:current train perplexity237.07423400878906
INFO:root:current mean train loss 6929.906706965578
INFO:root:current train perplexity236.87998962402344
INFO:root:current mean train loss 6930.445590413889
INFO:root:current train perplexity237.0413360595703
INFO:root:current mean train loss 6928.225778438192
INFO:root:current train perplexity237.0872344970703
INFO:root:current mean train loss 6933.12585513264
INFO:root:current train perplexity237.20913696289062

100%|██████████| 1/1 [07:39<00:00, 459.70s/it][A100%|██████████| 1/1 [07:39<00:00, 459.70s/it]
INFO:root:final mean train loss: 6928.982003154745
INFO:root:final train perplexity: 237.0657501220703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.01s/it][A100%|██████████| 1/1 [00:45<00:00, 45.01s/it]
INFO:root:eval mean loss: 6686.577273105053
INFO:root:eval perplexity: 223.82774353027344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.90s/it][A100%|██████████| 1/1 [00:41<00:00, 41.94s/it]
INFO:root:eval mean loss: 6748.000528105607
INFO:root:eval perplexity: 256.6529235839844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/72
 36%|███▌      | 72/200 [10:37:02<19:11:21, 539.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6943.811162533967
INFO:root:current train perplexity238.97988891601562
INFO:root:current mean train loss 6866.252758987551
INFO:root:current train perplexity235.56536865234375
INFO:root:current mean train loss 6874.711130184977
INFO:root:current train perplexity235.02120971679688
INFO:root:current mean train loss 6882.9388000677245
INFO:root:current train perplexity234.40689086914062
INFO:root:current mean train loss 6870.352292036052
INFO:root:current train perplexity232.293212890625
INFO:root:current mean train loss 6866.734935169694
INFO:root:current train perplexity231.26437377929688
INFO:root:current mean train loss 6861.804249379264
INFO:root:current train perplexity230.16798400878906
INFO:root:current mean train loss 6862.410287944113
INFO:root:current train perplexity228.9664764404297
INFO:root:current mean train loss 6858.612048977635
INFO:root:current train perplexity227.3539276123047
INFO:root:current mean train loss 6860.029765582679
INFO:root:current train perplexity226.27703857421875
INFO:root:current mean train loss 6856.028293106213
INFO:root:current train perplexity225.45269775390625
INFO:root:current mean train loss 6857.364097805821
INFO:root:current train perplexity224.83935546875
INFO:root:current mean train loss 6852.581099403363
INFO:root:current train perplexity224.0148162841797
INFO:root:current mean train loss 6852.36318602962
INFO:root:current train perplexity223.32923889160156
INFO:root:current mean train loss 6847.372244968267
INFO:root:current train perplexity222.50692749023438
INFO:root:current mean train loss 6838.992944768754
INFO:root:current train perplexity221.5765838623047
INFO:root:current mean train loss 6836.852737925043
INFO:root:current train perplexity221.00140380859375
INFO:root:current mean train loss 6833.307067127194
INFO:root:current train perplexity220.2552947998047
INFO:root:current mean train loss 6833.287381237572
INFO:root:current train perplexity219.82846069335938
INFO:root:current mean train loss 6830.075889519956
INFO:root:current train perplexity219.23033142089844

100%|██████████| 1/1 [08:03<00:00, 483.84s/it][A100%|██████████| 1/1 [08:03<00:00, 483.84s/it]
INFO:root:final mean train loss: 6828.095950589779
INFO:root:final train perplexity: 218.9226837158203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 6460.6282292359265
INFO:root:eval perplexity: 186.42628479003906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.91s/it][A100%|██████████| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 6560.407249937666
INFO:root:eval perplexity: 219.97169494628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/73
 36%|███▋      | 73/200 [10:46:24<19:16:24, 546.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6753.426745605469
INFO:root:current train perplexity209.84942626953125
INFO:root:current mean train loss 6740.896819196429
INFO:root:current train perplexity207.15573120117188
INFO:root:current mean train loss 6774.068035888672
INFO:root:current train perplexity207.22537231445312
INFO:root:current mean train loss 6781.489434455423
INFO:root:current train perplexity206.67050170898438
INFO:root:current mean train loss 6764.518568004261
INFO:root:current train perplexity205.33233642578125
INFO:root:current mean train loss 6754.302888997396
INFO:root:current train perplexity205.01670837402344
INFO:root:current mean train loss 6748.648567962647
INFO:root:current train perplexity204.71701049804688
INFO:root:current mean train loss 6746.65630608636
INFO:root:current train perplexity204.67005920410156
INFO:root:current mean train loss 6745.102031598773
INFO:root:current train perplexity205.0050811767578
INFO:root:current mean train loss 6747.523638006981
INFO:root:current train perplexity205.68521118164062
INFO:root:current mean train loss 6753.311802790715
INFO:root:current train perplexity206.6245880126953
INFO:root:current mean train loss 6756.21939504523
INFO:root:current train perplexity207.4036102294922
INFO:root:current mean train loss 6760.235021972656
INFO:root:current train perplexity207.8328857421875
INFO:root:current mean train loss 6759.974248265508
INFO:root:current train perplexity207.8283233642578
INFO:root:current mean train loss 6762.252226426866
INFO:root:current train perplexity208.21676635742188
INFO:root:current mean train loss 6760.463282201197
INFO:root:current train perplexity208.08197021484375
INFO:root:current mean train loss 6759.268309058212
INFO:root:current train perplexity207.83258056640625
INFO:root:current mean train loss 6763.159339922324
INFO:root:current train perplexity207.7089385986328
INFO:root:current mean train loss 6763.698952849015
INFO:root:current train perplexity207.55162048339844
INFO:root:current mean train loss 6761.873382379591
INFO:root:current train perplexity207.31936645507812

100%|██████████| 1/1 [07:28<00:00, 448.46s/it][A100%|██████████| 1/1 [07:28<00:00, 448.46s/it]
INFO:root:final mean train loss: 6758.321778944269
INFO:root:final train perplexity: 207.19346618652344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.83s/it][A100%|██████████| 1/1 [00:38<00:00, 38.83s/it]
INFO:root:eval mean loss: 6385.832926432292
INFO:root:eval perplexity: 175.47744750976562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.65s/it][A100%|██████████| 1/1 [00:36<00:00, 36.65s/it]
INFO:root:eval mean loss: 6494.200403091755
INFO:root:eval perplexity: 208.3186492919922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/74
 37%|███▋      | 74/200 [10:55:10<18:54:45, 540.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6596.62511136239
INFO:root:current train perplexity202.080810546875
INFO:root:current mean train loss 6703.269789385948
INFO:root:current train perplexity203.6221466064453
INFO:root:current mean train loss 6725.389258952457
INFO:root:current train perplexity204.33343505859375
INFO:root:current mean train loss 6730.3411307882525
INFO:root:current train perplexity204.59658813476562
INFO:root:current mean train loss 6743.412273916165
INFO:root:current train perplexity204.95611572265625
INFO:root:current mean train loss 6743.841730251346
INFO:root:current train perplexity204.48257446289062
INFO:root:current mean train loss 6750.623159841134
INFO:root:current train perplexity204.8195343017578
INFO:root:current mean train loss 6753.315197479772
INFO:root:current train perplexity204.98025512695312
INFO:root:current mean train loss 6752.444956971995
INFO:root:current train perplexity204.98973083496094
INFO:root:current mean train loss 6748.823347292973
INFO:root:current train perplexity204.7668914794922
INFO:root:current mean train loss 6749.767485273031
INFO:root:current train perplexity204.7306671142578
INFO:root:current mean train loss 6749.025727399795
INFO:root:current train perplexity204.55677795410156
INFO:root:current mean train loss 6752.09053946338
INFO:root:current train perplexity204.7741241455078
INFO:root:current mean train loss 6750.378593203068
INFO:root:current train perplexity204.65640258789062
INFO:root:current mean train loss 6752.229203642438
INFO:root:current train perplexity204.7311553955078
INFO:root:current mean train loss 6750.849048337648
INFO:root:current train perplexity204.60165405273438
INFO:root:current mean train loss 6746.036822064066
INFO:root:current train perplexity204.3275909423828
INFO:root:current mean train loss 6745.316237283011
INFO:root:current train perplexity204.31336975097656
INFO:root:current mean train loss 6745.31506419965
INFO:root:current train perplexity204.3172607421875
INFO:root:current mean train loss 6742.5462916573515
INFO:root:current train perplexity204.20535278320312

100%|██████████| 1/1 [07:29<00:00, 449.17s/it][A100%|██████████| 1/1 [07:29<00:00, 449.17s/it]
INFO:root:final mean train loss: 6740.031059045835
INFO:root:final train perplexity: 204.2240447998047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.78s/it][A100%|██████████| 1/1 [00:38<00:00, 38.78s/it]
INFO:root:eval mean loss: 6385.064764793883
INFO:root:eval perplexity: 175.36819458007812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.98s/it][A100%|██████████| 1/1 [00:35<00:00, 35.98s/it]
INFO:root:eval mean loss: 6494.127638796543
INFO:root:eval perplexity: 208.3060302734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/75
 38%|███▊      | 75/200 [11:03:56<18:37:03, 536.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6746.400463207348
INFO:root:current train perplexity205.47601318359375
INFO:root:current mean train loss 6772.1867479346265
INFO:root:current train perplexity205.26673889160156
INFO:root:current mean train loss 6768.332252223996
INFO:root:current train perplexity205.3336944580078
INFO:root:current mean train loss 6753.282991623496
INFO:root:current train perplexity204.70216369628906
INFO:root:current mean train loss 6755.899647901832
INFO:root:current train perplexity204.79354858398438
INFO:root:current mean train loss 6740.802404317291
INFO:root:current train perplexity203.86026000976562
INFO:root:current mean train loss 6746.876303290755
INFO:root:current train perplexity203.94874572753906
INFO:root:current mean train loss 6751.421121759932
INFO:root:current train perplexity203.89720153808594
INFO:root:current mean train loss 6748.076911000965
INFO:root:current train perplexity203.44395446777344
INFO:root:current mean train loss 6737.693419031539
INFO:root:current train perplexity203.01882934570312
INFO:root:current mean train loss 6734.045646568028
INFO:root:current train perplexity202.95997619628906
INFO:root:current mean train loss 6727.8974692557495
INFO:root:current train perplexity202.4638214111328
INFO:root:current mean train loss 6724.6447918710755
INFO:root:current train perplexity202.2159423828125
INFO:root:current mean train loss 6725.92642447348
INFO:root:current train perplexity202.08380126953125
INFO:root:current mean train loss 6726.565219998198
INFO:root:current train perplexity202.04371643066406
INFO:root:current mean train loss 6727.493802488683
INFO:root:current train perplexity202.05642700195312
INFO:root:current mean train loss 6724.717278739173
INFO:root:current train perplexity201.8228302001953
INFO:root:current mean train loss 6723.558974135957
INFO:root:current train perplexity201.59063720703125
INFO:root:current mean train loss 6723.799688740245
INFO:root:current train perplexity201.45851135253906
INFO:root:current mean train loss 6724.771392853185
INFO:root:current train perplexity201.3885040283203

100%|██████████| 1/1 [07:30<00:00, 450.13s/it][A100%|██████████| 1/1 [07:30<00:00, 450.13s/it]
INFO:root:final mean train loss: 6722.090167424081
INFO:root:final train perplexity: 201.35287475585938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.64s/it][A100%|██████████| 1/1 [00:38<00:00, 38.64s/it]
INFO:root:eval mean loss: 6359.863362630208
INFO:root:eval perplexity: 171.82823181152344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.18s/it][A100%|██████████| 1/1 [00:36<00:00, 36.18s/it]
INFO:root:eval mean loss: 6472.98635323166
INFO:root:eval perplexity: 204.71690368652344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/76
 38%|███▊      | 76/200 [11:12:44<18:22:36, 533.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6687.376727764423
INFO:root:current train perplexity199.2955322265625
INFO:root:current mean train loss 6710.16447408786
INFO:root:current train perplexity198.49630737304688
INFO:root:current mean train loss 6707.286320742053
INFO:root:current train perplexity198.84237670898438
INFO:root:current mean train loss 6687.249675311701
INFO:root:current train perplexity198.1736297607422
INFO:root:current mean train loss 6685.649991845405
INFO:root:current train perplexity197.917236328125
INFO:root:current mean train loss 6692.406773807636
INFO:root:current train perplexity197.8785400390625
INFO:root:current mean train loss 6687.910067214634
INFO:root:current train perplexity197.49847412109375
INFO:root:current mean train loss 6685.901398669604
INFO:root:current train perplexity197.32640075683594
INFO:root:current mean train loss 6689.512818615846
INFO:root:current train perplexity197.31190490722656
INFO:root:current mean train loss 6684.514671102422
INFO:root:current train perplexity197.1982879638672
INFO:root:current mean train loss 6686.913571085873
INFO:root:current train perplexity197.13650512695312
INFO:root:current mean train loss 6690.297138614478
INFO:root:current train perplexity197.05728149414062
INFO:root:current mean train loss 6695.845390337553
INFO:root:current train perplexity197.02374267578125
INFO:root:current mean train loss 6698.191283740902
INFO:root:current train perplexity196.99302673339844
INFO:root:current mean train loss 6694.1707579461145
INFO:root:current train perplexity196.79971313476562
INFO:root:current mean train loss 6693.6026452506285
INFO:root:current train perplexity196.72952270507812
INFO:root:current mean train loss 6693.293345572609
INFO:root:current train perplexity196.55203247070312
INFO:root:current mean train loss 6692.3432630819025
INFO:root:current train perplexity196.54640197753906
INFO:root:current mean train loss 6694.358179214453
INFO:root:current train perplexity196.5965576171875

100%|██████████| 1/1 [07:31<00:00, 451.45s/it][A100%|██████████| 1/1 [07:31<00:00, 451.45s/it]
INFO:root:final mean train loss: 6690.802704826967
INFO:root:final train perplexity: 196.4418487548828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:50<00:00, 50.35s/it][A100%|██████████| 1/1 [00:50<00:00, 50.35s/it]
INFO:root:eval mean loss: 6345.001322861259
INFO:root:eval perplexity: 169.77415466308594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.07s/it][A100%|██████████| 1/1 [00:47<00:00, 47.07s/it]
INFO:root:eval mean loss: 6461.988481237533
INFO:root:eval perplexity: 202.8741455078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/77
 38%|███▊      | 77/200 [11:21:55<18:24:55, 538.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6657.4066162109375
INFO:root:current train perplexity197.39942932128906
INFO:root:current mean train loss 6703.043316876447
INFO:root:current train perplexity197.12588500976562
INFO:root:current mean train loss 6685.225351186899
INFO:root:current train perplexity196.43435668945312
INFO:root:current mean train loss 6699.3382964691555
INFO:root:current train perplexity196.67991638183594
INFO:root:current mean train loss 6689.4673390108
INFO:root:current train perplexity195.71620178222656
INFO:root:current mean train loss 6685.805979330708
INFO:root:current train perplexity196.2564697265625
INFO:root:current mean train loss 6694.181316174959
INFO:root:current train perplexity196.1724395751953
INFO:root:current mean train loss 6689.098354878399
INFO:root:current train perplexity196.05723571777344
INFO:root:current mean train loss 6676.940461715849
INFO:root:current train perplexity195.52731323242188
INFO:root:current mean train loss 6678.919353468303
INFO:root:current train perplexity195.32984924316406
INFO:root:current mean train loss 6685.833442324683
INFO:root:current train perplexity195.42227172851562
INFO:root:current mean train loss 6687.974874668604
INFO:root:current train perplexity195.3975830078125
INFO:root:current mean train loss 6689.612174937267
INFO:root:current train perplexity195.3418731689453
INFO:root:current mean train loss 6686.133872309227
INFO:root:current train perplexity195.2238006591797
INFO:root:current mean train loss 6685.051126306707
INFO:root:current train perplexity195.10374450683594
INFO:root:current mean train loss 6684.376838178154
INFO:root:current train perplexity195.1716766357422
INFO:root:current mean train loss 6685.0974151459495
INFO:root:current train perplexity195.1457977294922
INFO:root:current mean train loss 6682.165743754117
INFO:root:current train perplexity195.00059509277344
INFO:root:current mean train loss 6683.1071639609545
INFO:root:current train perplexity195.06643676757812
INFO:root:current mean train loss 6684.414260576356
INFO:root:current train perplexity195.0999298095703

100%|██████████| 1/1 [09:15<00:00, 555.42s/it][A100%|██████████| 1/1 [09:15<00:00, 555.43s/it]
INFO:root:final mean train loss: 6681.904030203999
INFO:root:final train perplexity: 195.0671844482422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:49<00:00, 49.69s/it][A100%|██████████| 1/1 [00:49<00:00, 49.69s/it]
INFO:root:eval mean loss: 6338.267280308068
INFO:root:eval perplexity: 168.8515167236328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.77s/it][A100%|██████████| 1/1 [00:46<00:00, 46.77s/it]
INFO:root:eval mean loss: 6454.741971929023
INFO:root:eval perplexity: 201.66909790039062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/78
 39%|███▉      | 78/200 [11:32:50<19:26:35, 573.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6537.17427734375
INFO:root:current train perplexity193.46971130371094
INFO:root:current mean train loss 6675.3241015625
INFO:root:current train perplexity194.74069213867188
INFO:root:current mean train loss 6657.321304253473
INFO:root:current train perplexity194.67208862304688
INFO:root:current mean train loss 6653.188490084134
INFO:root:current train perplexity194.161376953125
INFO:root:current mean train loss 6661.026951976103
INFO:root:current train perplexity194.03994750976562
INFO:root:current mean train loss 6659.54556640625
INFO:root:current train perplexity194.24656677246094
INFO:root:current mean train loss 6665.67506171875
INFO:root:current train perplexity194.19090270996094
INFO:root:current mean train loss 6666.56891096444
INFO:root:current train perplexity194.2010955810547
INFO:root:current mean train loss 6670.416574928977
INFO:root:current train perplexity194.21044921875
INFO:root:current mean train loss 6668.72646484375
INFO:root:current train perplexity194.07852172851562
INFO:root:current mean train loss 6665.779328315549
INFO:root:current train perplexity193.9001922607422
INFO:root:current mean train loss 6669.249549913195
INFO:root:current train perplexity194.04660034179688
INFO:root:current mean train loss 6674.063734853316
INFO:root:current train perplexity194.2704620361328
INFO:root:current mean train loss 6676.040617629717
INFO:root:current train perplexity194.27880859375
INFO:root:current mean train loss 6677.0226031387065
INFO:root:current train perplexity194.15211486816406
INFO:root:current mean train loss 6680.313301101434
INFO:root:current train perplexity194.29612731933594
INFO:root:current mean train loss 6677.11624248798
INFO:root:current train perplexity194.1515655517578
INFO:root:current mean train loss 6674.891623075181
INFO:root:current train perplexity193.9615936279297
INFO:root:current mean train loss 6676.8552367829625
INFO:root:current train perplexity194.0039520263672
INFO:root:current mean train loss 6676.748961292614
INFO:root:current train perplexity193.95095825195312

100%|██████████| 1/1 [09:17<00:00, 557.36s/it][A100%|██████████| 1/1 [09:17<00:00, 557.36s/it]
INFO:root:final mean train loss: 6674.725394244634
INFO:root:final train perplexity: 193.96519470214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:50<00:00, 50.82s/it][A100%|██████████| 1/1 [00:50<00:00, 50.83s/it]
INFO:root:eval mean loss: 6332.252115885417
INFO:root:eval perplexity: 168.0315399169922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:48<00:00, 48.30s/it][A100%|██████████| 1/1 [00:48<00:00, 48.30s/it]
INFO:root:eval mean loss: 6449.376943601784
INFO:root:eval perplexity: 200.78164672851562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/79
 40%|███▉      | 79/200 [11:43:50<20:08:54, 599.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6668.858793712798
INFO:root:current train perplexity192.96620178222656
INFO:root:current mean train loss 6629.222398354974
INFO:root:current train perplexity191.8820343017578
INFO:root:current mean train loss 6641.028530152376
INFO:root:current train perplexity192.1929168701172
INFO:root:current mean train loss 6651.086271587171
INFO:root:current train perplexity193.0869598388672
INFO:root:current mean train loss 6662.295340559601
INFO:root:current train perplexity193.5625457763672
INFO:root:current mean train loss 6660.7627178346975
INFO:root:current train perplexity193.20042419433594
INFO:root:current mean train loss 6658.024131133177
INFO:root:current train perplexity192.82998657226562
INFO:root:current mean train loss 6665.702430087601
INFO:root:current train perplexity193.09645080566406
INFO:root:current mean train loss 6663.018406231443
INFO:root:current train perplexity192.86134338378906
INFO:root:current mean train loss 6664.109503031284
INFO:root:current train perplexity192.85398864746094
INFO:root:current mean train loss 6671.147192898273
INFO:root:current train perplexity193.09083557128906
INFO:root:current mean train loss 6667.006752989547
INFO:root:current train perplexity192.89950561523438
INFO:root:current mean train loss 6666.785370905042
INFO:root:current train perplexity193.0889892578125
INFO:root:current mean train loss 6661.659458029643
INFO:root:current train perplexity192.96656799316406
INFO:root:current mean train loss 6662.785403776764
INFO:root:current train perplexity193.0535430908203
INFO:root:current mean train loss 6667.857671398751
INFO:root:current train perplexity193.2328643798828
INFO:root:current mean train loss 6671.871965935692
INFO:root:current train perplexity193.43817138671875
INFO:root:current mean train loss 6671.240251753552
INFO:root:current train perplexity193.32382202148438
INFO:root:current mean train loss 6669.251870949545
INFO:root:current train perplexity193.31027221679688
INFO:root:current mean train loss 6670.537258222837
INFO:root:current train perplexity193.37342834472656

100%|██████████| 1/1 [07:55<00:00, 475.41s/it][A100%|██████████| 1/1 [07:55<00:00, 475.41s/it]
INFO:root:final mean train loss: 6671.14011898993
INFO:root:final train perplexity: 193.4170379638672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 6331.255480177859
INFO:root:eval perplexity: 167.89602661132812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.34s/it][A100%|██████████| 1/1 [00:37<00:00, 37.34s/it]
INFO:root:eval mean loss: 6449.097087454288
INFO:root:eval perplexity: 200.73541259765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/80
 40%|████      | 80/200 [11:53:04<19:31:50, 585.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6643.044806011652
INFO:root:current train perplexity193.04295349121094
INFO:root:current mean train loss 6696.192097214033
INFO:root:current train perplexity194.39732360839844
INFO:root:current mean train loss 6706.5382857746135
INFO:root:current train perplexity193.4224853515625
INFO:root:current mean train loss 6703.090442200557
INFO:root:current train perplexity193.67120361328125
INFO:root:current mean train loss 6704.286972145629
INFO:root:current train perplexity193.96371459960938
INFO:root:current mean train loss 6698.398677709917
INFO:root:current train perplexity193.75599670410156
INFO:root:current mean train loss 6701.669403956042
INFO:root:current train perplexity193.9626007080078
INFO:root:current mean train loss 6699.073482146533
INFO:root:current train perplexity193.87124633789062
INFO:root:current mean train loss 6691.385917263897
INFO:root:current train perplexity193.557373046875
INFO:root:current mean train loss 6684.939997413484
INFO:root:current train perplexity193.22622680664062
INFO:root:current mean train loss 6673.952011036355
INFO:root:current train perplexity192.72665405273438
INFO:root:current mean train loss 6674.123510721122
INFO:root:current train perplexity192.71177673339844
INFO:root:current mean train loss 6673.8696036971305
INFO:root:current train perplexity192.6571044921875
INFO:root:current mean train loss 6673.415742920461
INFO:root:current train perplexity192.64552307128906
INFO:root:current mean train loss 6667.4878943732865
INFO:root:current train perplexity192.5460662841797
INFO:root:current mean train loss 6669.444538578917
INFO:root:current train perplexity192.66978454589844
INFO:root:current mean train loss 6666.209930033623
INFO:root:current train perplexity192.601806640625
INFO:root:current mean train loss 6665.483541674069
INFO:root:current train perplexity192.69229125976562
INFO:root:current mean train loss 6663.244091770609
INFO:root:current train perplexity192.49319458007812
INFO:root:current mean train loss 6667.1245555867945
INFO:root:current train perplexity192.50291442871094

100%|██████████| 1/1 [07:30<00:00, 450.44s/it][A100%|██████████| 1/1 [07:30<00:00, 450.45s/it]
INFO:root:final mean train loss: 6664.861580514451
INFO:root:final train perplexity: 192.4609832763672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.91s/it][A100%|██████████| 1/1 [00:39<00:00, 39.91s/it]
INFO:root:eval mean loss: 6322.666117783134
INFO:root:eval perplexity: 166.73304748535156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.62s/it][A100%|██████████| 1/1 [00:36<00:00, 36.62s/it]
INFO:root:eval mean loss: 6441.543887307458
INFO:root:eval perplexity: 199.4926300048828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/81
 40%|████      | 81/200 [12:01:54<18:48:29, 568.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6611.489501953125
INFO:root:current train perplexity190.58908081054688
INFO:root:current mean train loss 6634.624339710583
INFO:root:current train perplexity190.75027465820312
INFO:root:current mean train loss 6662.702325350996
INFO:root:current train perplexity190.986083984375
INFO:root:current mean train loss 6652.39347936752
INFO:root:current train perplexity190.7808380126953
INFO:root:current mean train loss 6647.515834263393
INFO:root:current train perplexity190.89877319335938
INFO:root:current mean train loss 6660.885709974501
INFO:root:current train perplexity191.34515380859375
INFO:root:current mean train loss 6657.935203055658
INFO:root:current train perplexity191.47996520996094
INFO:root:current mean train loss 6652.139314317212
INFO:root:current train perplexity191.400634765625
INFO:root:current mean train loss 6655.932017426512
INFO:root:current train perplexity191.3130340576172
INFO:root:current mean train loss 6657.566708924341
INFO:root:current train perplexity191.26022338867188
INFO:root:current mean train loss 6656.395246427742
INFO:root:current train perplexity191.121826171875
INFO:root:current mean train loss 6656.946571817203
INFO:root:current train perplexity191.259033203125
INFO:root:current mean train loss 6658.663791190121
INFO:root:current train perplexity191.45448303222656
INFO:root:current mean train loss 6656.4909423118415
INFO:root:current train perplexity191.38046264648438
INFO:root:current mean train loss 6653.091114075203
INFO:root:current train perplexity191.0436553955078
INFO:root:current mean train loss 6654.662984315514
INFO:root:current train perplexity191.04702758789062
INFO:root:current mean train loss 6656.278265832432
INFO:root:current train perplexity191.24855041503906
INFO:root:current mean train loss 6656.884853603604
INFO:root:current train perplexity191.3147735595703
INFO:root:current mean train loss 6654.687964595965
INFO:root:current train perplexity191.13113403320312
INFO:root:current mean train loss 6659.194223751423
INFO:root:current train perplexity191.27655029296875

100%|██████████| 1/1 [07:35<00:00, 455.32s/it][A100%|██████████| 1/1 [07:35<00:00, 455.32s/it]
INFO:root:final mean train loss: 6657.086531169235
INFO:root:final train perplexity: 191.28375244140625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.42s/it][A100%|██████████| 1/1 [00:39<00:00, 39.42s/it]
INFO:root:eval mean loss: 6317.870110261525
INFO:root:eval perplexity: 166.08729553222656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.30s/it][A100%|██████████| 1/1 [00:37<00:00, 37.30s/it]
INFO:root:eval mean loss: 6437.7305094401045
INFO:root:eval perplexity: 198.86825561523438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/82
 41%|████      | 82/200 [12:10:48<18:18:34, 558.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6638.553154401881
INFO:root:current train perplexity190.73854064941406
INFO:root:current mean train loss 6656.5072306104275
INFO:root:current train perplexity191.2091522216797
INFO:root:current mean train loss 6658.957577858362
INFO:root:current train perplexity190.6961669921875
INFO:root:current mean train loss 6660.740014462071
INFO:root:current train perplexity190.6325225830078
INFO:root:current mean train loss 6652.040601625888
INFO:root:current train perplexity190.4609832763672
INFO:root:current mean train loss 6652.737428198778
INFO:root:current train perplexity190.5656280517578
INFO:root:current mean train loss 6650.683295708198
INFO:root:current train perplexity190.3683624267578
INFO:root:current mean train loss 6656.021007792796
INFO:root:current train perplexity190.45407104492188
INFO:root:current mean train loss 6659.772665982818
INFO:root:current train perplexity190.43618774414062
INFO:root:current mean train loss 6653.599708703109
INFO:root:current train perplexity190.37481689453125
INFO:root:current mean train loss 6645.4102933976155
INFO:root:current train perplexity190.2990264892578
INFO:root:current mean train loss 6644.32329621359
INFO:root:current train perplexity190.16360473632812
INFO:root:current mean train loss 6650.899228644021
INFO:root:current train perplexity190.32054138183594
INFO:root:current mean train loss 6651.764341027122
INFO:root:current train perplexity190.29830932617188
INFO:root:current mean train loss 6651.211957886805
INFO:root:current train perplexity190.44264221191406
INFO:root:current mean train loss 6649.549267363564
INFO:root:current train perplexity190.44744873046875
INFO:root:current mean train loss 6647.414640188921
INFO:root:current train perplexity190.38516235351562
INFO:root:current mean train loss 6650.673897568234
INFO:root:current train perplexity190.45162963867188
INFO:root:current mean train loss 6654.20242469171
INFO:root:current train perplexity190.5183868408203

100%|██████████| 1/1 [07:30<00:00, 450.12s/it][A100%|██████████| 1/1 [07:30<00:00, 450.12s/it]
INFO:root:final mean train loss: 6652.027897898741
INFO:root:final train perplexity: 190.5216522216797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.14s/it][A100%|██████████| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 6311.852878435284
INFO:root:eval perplexity: 165.28050231933594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.21s/it][A100%|██████████| 1/1 [00:37<00:00, 37.21s/it]
INFO:root:eval mean loss: 6432.566628746952
INFO:root:eval perplexity: 198.0258026123047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/83
 42%|████▏     | 83/200 [12:19:38<17:52:30, 550.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6680.579931640625
INFO:root:current train perplexity198.40130615234375
INFO:root:current mean train loss 6600.950594815341
INFO:root:current train perplexity187.6860809326172
INFO:root:current mean train loss 6615.464227585566
INFO:root:current train perplexity189.1877899169922
INFO:root:current mean train loss 6622.077091733871
INFO:root:current train perplexity189.4700927734375
INFO:root:current mean train loss 6623.24149914253
INFO:root:current train perplexity189.38682556152344
INFO:root:current mean train loss 6635.797249348959
INFO:root:current train perplexity189.6848907470703
INFO:root:current mean train loss 6638.591673603996
INFO:root:current train perplexity189.6910400390625
INFO:root:current mean train loss 6642.841610502861
INFO:root:current train perplexity190.3828887939453
INFO:root:current mean train loss 6637.243711419753
INFO:root:current train perplexity190.21783447265625
INFO:root:current mean train loss 6642.776555524553
INFO:root:current train perplexity189.87428283691406
INFO:root:current mean train loss 6651.928024443069
INFO:root:current train perplexity189.94935607910156
INFO:root:current mean train loss 6656.042805109797
INFO:root:current train perplexity189.93966674804688
INFO:root:current mean train loss 6654.40686539579
INFO:root:current train perplexity189.80340576171875
INFO:root:current mean train loss 6649.779497405773
INFO:root:current train perplexity189.5469970703125
INFO:root:current mean train loss 6653.327807097739
INFO:root:current train perplexity189.63650512695312
INFO:root:current mean train loss 6649.174963459747
INFO:root:current train perplexity189.4675750732422
INFO:root:current mean train loss 6649.489978709725
INFO:root:current train perplexity189.49105834960938
INFO:root:current mean train loss 6651.1579387107095
INFO:root:current train perplexity189.68109130859375
INFO:root:current mean train loss 6651.01958898049
INFO:root:current train perplexity189.68814086914062
INFO:root:current mean train loss 6649.335237289349
INFO:root:current train perplexity189.62420654296875

100%|██████████| 1/1 [07:38<00:00, 458.32s/it][A100%|██████████| 1/1 [07:38<00:00, 458.32s/it]
INFO:root:final mean train loss: 6645.9872634433705
INFO:root:final train perplexity: 189.6155242919922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.19s/it][A100%|██████████| 1/1 [00:39<00:00, 39.19s/it]
INFO:root:eval mean loss: 6308.90805248504
INFO:root:eval perplexity: 164.8871612548828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.38s/it][A100%|██████████| 1/1 [00:37<00:00, 37.38s/it]
INFO:root:eval mean loss: 6430.634552651263
INFO:root:eval perplexity: 197.71160888671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/84
 42%|████▏     | 84/200 [12:28:35<17:35:59, 546.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6682.529369212963
INFO:root:current train perplexity191.1290283203125
INFO:root:current mean train loss 6705.550200695128
INFO:root:current train perplexity190.7603759765625
INFO:root:current mean train loss 6671.425409123761
INFO:root:current train perplexity190.29847717285156
INFO:root:current mean train loss 6665.9980185039185
INFO:root:current train perplexity189.56381225585938
INFO:root:current mean train loss 6671.014073249049
INFO:root:current train perplexity189.48211669921875
INFO:root:current mean train loss 6669.742222708136
INFO:root:current train perplexity189.843505859375
INFO:root:current mean train loss 6660.701416405004
INFO:root:current train perplexity189.2233428955078
INFO:root:current mean train loss 6650.540750327759
INFO:root:current train perplexity188.89852905273438
INFO:root:current mean train loss 6647.322788150885
INFO:root:current train perplexity188.6728515625
INFO:root:current mean train loss 6649.664079355448
INFO:root:current train perplexity188.7881317138672
INFO:root:current mean train loss 6641.77049212284
INFO:root:current train perplexity188.5298614501953
INFO:root:current mean train loss 6645.890428301076
INFO:root:current train perplexity188.80828857421875
INFO:root:current mean train loss 6643.943656243633
INFO:root:current train perplexity188.76345825195312
INFO:root:current mean train loss 6643.377301949887
INFO:root:current train perplexity189.0433349609375
INFO:root:current mean train loss 6640.321736967305
INFO:root:current train perplexity188.80514526367188
INFO:root:current mean train loss 6642.454010109692
INFO:root:current train perplexity189.0149383544922
INFO:root:current mean train loss 6644.159326141864
INFO:root:current train perplexity188.96717834472656
INFO:root:current mean train loss 6645.570123351096
INFO:root:current train perplexity188.9687042236328
INFO:root:current mean train loss 6643.75408958932
INFO:root:current train perplexity188.88140869140625
INFO:root:current mean train loss 6641.573202658764
INFO:root:current train perplexity188.81820678710938

100%|██████████| 1/1 [07:33<00:00, 453.60s/it][A100%|██████████| 1/1 [07:33<00:00, 453.60s/it]
INFO:root:final mean train loss: 6640.365862886291
INFO:root:final train perplexity: 188.77615356445312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.47s/it][A100%|██████████| 1/1 [00:39<00:00, 39.47s/it]
INFO:root:eval mean loss: 6301.183524490249
INFO:root:eval perplexity: 163.85968017578125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.06s/it][A100%|██████████| 1/1 [00:38<00:00, 38.06s/it]
INFO:root:eval mean loss: 6423.783209185228
INFO:root:eval perplexity: 196.60096740722656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/85
 42%|████▎     | 85/200 [12:37:29<17:19:31, 542.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6649.939098011364
INFO:root:current train perplexity190.74281311035156
INFO:root:current mean train loss 6614.440368652344
INFO:root:current train perplexity187.3810577392578
INFO:root:current mean train loss 6622.042628554047
INFO:root:current train perplexity187.6277313232422
INFO:root:current mean train loss 6630.174904047057
INFO:root:current train perplexity187.90081787109375
INFO:root:current mean train loss 6631.723738386824
INFO:root:current train perplexity187.9290313720703
INFO:root:current mean train loss 6646.571464089786
INFO:root:current train perplexity188.3076629638672
INFO:root:current mean train loss 6637.21437745657
INFO:root:current train perplexity188.13023376464844
INFO:root:current mean train loss 6634.163816390499
INFO:root:current train perplexity187.9028778076172
INFO:root:current mean train loss 6632.5689940249185
INFO:root:current train perplexity187.7397918701172
INFO:root:current mean train loss 6629.754387807038
INFO:root:current train perplexity187.68527221679688
INFO:root:current mean train loss 6636.018736623713
INFO:root:current train perplexity187.86488342285156
INFO:root:current mean train loss 6638.15338518903
INFO:root:current train perplexity187.97994995117188
INFO:root:current mean train loss 6637.581036239575
INFO:root:current train perplexity188.00558471679688
INFO:root:current mean train loss 6635.822253272647
INFO:root:current train perplexity187.81857299804688
INFO:root:current mean train loss 6635.065121637487
INFO:root:current train perplexity187.6051025390625
INFO:root:current mean train loss 6634.036735890443
INFO:root:current train perplexity187.45960998535156
INFO:root:current mean train loss 6638.232009330805
INFO:root:current train perplexity187.49044799804688
INFO:root:current mean train loss 6637.301288569739
INFO:root:current train perplexity187.44754028320312
INFO:root:current mean train loss 6634.856298881084
INFO:root:current train perplexity187.3162078857422
INFO:root:current mean train loss 6632.345926418226
INFO:root:current train perplexity187.29994201660156

100%|██████████| 1/1 [07:22<00:00, 442.28s/it][A100%|██████████| 1/1 [07:22<00:00, 442.28s/it]
INFO:root:final mean train loss: 6630.194892671694
INFO:root:final train perplexity: 187.26699829101562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.11s/it][A100%|██████████| 1/1 [00:39<00:00, 39.11s/it]
INFO:root:eval mean loss: 6296.796571988586
INFO:root:eval perplexity: 163.2790069580078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.23s/it][A100%|██████████| 1/1 [00:36<00:00, 36.23s/it]
INFO:root:eval mean loss: 6422.649805380098
INFO:root:eval perplexity: 196.41786193847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/86
 43%|████▎     | 86/200 [12:46:09<16:57:45, 535.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6650.989465932377
INFO:root:current train perplexity185.585205078125
INFO:root:current mean train loss 6651.509074145963
INFO:root:current train perplexity185.91140747070312
INFO:root:current mean train loss 6647.531070402299
INFO:root:current train perplexity186.26385498046875
INFO:root:current mean train loss 6640.103168012032
INFO:root:current train perplexity186.4333953857422
INFO:root:current mean train loss 6617.771005626356
INFO:root:current train perplexity186.1170196533203
INFO:root:current mean train loss 6604.1619814296455
INFO:root:current train perplexity185.63865661621094
INFO:root:current mean train loss 6603.758348796804
INFO:root:current train perplexity185.62060546875
INFO:root:current mean train loss 6603.031735714726
INFO:root:current train perplexity185.8507843017578
INFO:root:current mean train loss 6605.692968636578
INFO:root:current train perplexity185.94244384765625
INFO:root:current mean train loss 6613.200742533006
INFO:root:current train perplexity186.1421356201172
INFO:root:current mean train loss 6617.74432516862
INFO:root:current train perplexity186.32791137695312
INFO:root:current mean train loss 6616.348532296377
INFO:root:current train perplexity186.31236267089844
INFO:root:current mean train loss 6613.106073196496
INFO:root:current train perplexity186.19566345214844
INFO:root:current mean train loss 6615.4768789320815
INFO:root:current train perplexity186.26429748535156
INFO:root:current mean train loss 6618.7204634962145
INFO:root:current train perplexity186.3385772705078
INFO:root:current mean train loss 6617.623765690062
INFO:root:current train perplexity186.32400512695312
INFO:root:current mean train loss 6619.1052188769945
INFO:root:current train perplexity186.25009155273438
INFO:root:current mean train loss 6622.449830418619
INFO:root:current train perplexity186.26217651367188
INFO:root:current mean train loss 6622.375657251226
INFO:root:current train perplexity186.2529296875
INFO:root:current mean train loss 6624.0621957268295
INFO:root:current train perplexity186.297607421875

100%|██████████| 1/1 [07:37<00:00, 457.91s/it][A100%|██████████| 1/1 [07:37<00:00, 457.91s/it]
INFO:root:final mean train loss: 6623.6534854121355
INFO:root:final train perplexity: 186.30267333984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.03s/it][A100%|██████████| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 6291.526774088542
INFO:root:eval perplexity: 162.58421325683594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.92s/it][A100%|██████████| 1/1 [00:37<00:00, 37.92s/it]
INFO:root:eval mean loss: 6419.003142661237
INFO:root:eval perplexity: 195.82981872558594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/87
 44%|████▎     | 87/200 [12:55:07<16:50:25, 536.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6621.341584034455
INFO:root:current train perplexity185.978271484375
INFO:root:current mean train loss 6649.320905021067
INFO:root:current train perplexity186.74855041503906
INFO:root:current mean train loss 6625.681103164344
INFO:root:current train perplexity186.69921875
INFO:root:current mean train loss 6642.652568514385
INFO:root:current train perplexity186.69912719726562
INFO:root:current mean train loss 6637.45346618397
INFO:root:current train perplexity186.23890686035156
INFO:root:current mean train loss 6631.4913731347315
INFO:root:current train perplexity185.77902221679688
INFO:root:current mean train loss 6626.768970230688
INFO:root:current train perplexity185.54263305664062
INFO:root:current mean train loss 6628.768770585636
INFO:root:current train perplexity185.2833251953125
INFO:root:current mean train loss 6634.179742556769
INFO:root:current train perplexity185.3388214111328
INFO:root:current mean train loss 6629.813858001023
INFO:root:current train perplexity185.2373046875
INFO:root:current mean train loss 6624.675451501624
INFO:root:current train perplexity185.1370849609375
INFO:root:current mean train loss 6628.589623235887
INFO:root:current train perplexity185.14210510253906
INFO:root:current mean train loss 6629.764793240782
INFO:root:current train perplexity185.03549194335938
INFO:root:current mean train loss 6630.663327952082
INFO:root:current train perplexity185.09506225585938
INFO:root:current mean train loss 6626.059663806136
INFO:root:current train perplexity184.92919921875
INFO:root:current mean train loss 6623.834524949996
INFO:root:current train perplexity184.87374877929688
INFO:root:current mean train loss 6622.591403456496
INFO:root:current train perplexity184.9058380126953
INFO:root:current mean train loss 6617.05604743743
INFO:root:current train perplexity184.79177856445312
INFO:root:current mean train loss 6618.272852238501
INFO:root:current train perplexity184.89808654785156
INFO:root:current mean train loss 6615.006679183914
INFO:root:current train perplexity184.7837677001953

100%|██████████| 1/1 [07:39<00:00, 459.52s/it][A100%|██████████| 1/1 [07:39<00:00, 459.52s/it]
INFO:root:final mean train loss: 6613.257677194631
INFO:root:final train perplexity: 184.7805938720703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.15s/it][A100%|██████████| 1/1 [00:40<00:00, 40.15s/it]
INFO:root:eval mean loss: 6286.260056515957
INFO:root:eval perplexity: 161.89266967773438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.60s/it][A100%|██████████| 1/1 [00:37<00:00, 37.60s/it]
INFO:root:eval mean loss: 6415.888302201075
INFO:root:eval perplexity: 195.3291015625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/88
 44%|████▍     | 88/200 [13:04:07<16:43:16, 537.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6621.261307565789
INFO:root:current train perplexity185.94642639160156
INFO:root:current mean train loss 6603.403771033654
INFO:root:current train perplexity185.37655639648438
INFO:root:current mean train loss 6603.970881885593
INFO:root:current train perplexity184.95204162597656
INFO:root:current mean train loss 6608.741523684731
INFO:root:current train perplexity184.95883178710938
INFO:root:current mean train loss 6610.436665482955
INFO:root:current train perplexity184.77847290039062
INFO:root:current mean train loss 6615.605746947216
INFO:root:current train perplexity184.92868041992188
INFO:root:current mean train loss 6627.910156952563
INFO:root:current train perplexity185.4347381591797
INFO:root:current mean train loss 6628.977133696934
INFO:root:current train perplexity185.197998046875
INFO:root:current mean train loss 6633.718765821403
INFO:root:current train perplexity185.14520263671875
INFO:root:current mean train loss 6628.057192211055
INFO:root:current train perplexity184.93661499023438
INFO:root:current mean train loss 6625.544686429795
INFO:root:current train perplexity184.8094940185547
INFO:root:current mean train loss 6619.767690082374
INFO:root:current train perplexity184.62875366210938
INFO:root:current mean train loss 6615.401665057915
INFO:root:current train perplexity184.51319885253906
INFO:root:current mean train loss 6607.386439082101
INFO:root:current train perplexity184.25418090820312
INFO:root:current mean train loss 6608.838577158236
INFO:root:current train perplexity184.09593200683594
INFO:root:current mean train loss 6605.255897029291
INFO:root:current train perplexity183.91957092285156
INFO:root:current mean train loss 6610.573811416851
INFO:root:current train perplexity184.123046875
INFO:root:current mean train loss 6612.3651293741295
INFO:root:current train perplexity184.20947265625
INFO:root:current mean train loss 6609.214261419855
INFO:root:current train perplexity184.0823211669922

100%|██████████| 1/1 [07:37<00:00, 457.49s/it][A100%|██████████| 1/1 [07:37<00:00, 457.49s/it]
INFO:root:final mean train loss: 6609.017941812043
INFO:root:final train perplexity: 184.16326904296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.98s/it][A100%|██████████| 1/1 [00:39<00:00, 39.98s/it]
INFO:root:eval mean loss: 6286.785322473404
INFO:root:eval perplexity: 161.9616241455078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.11s/it][A100%|██████████| 1/1 [00:37<00:00, 37.11s/it]
INFO:root:eval mean loss: 6418.886242589207
INFO:root:eval perplexity: 195.81105041503906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/89
 44%|████▍     | 89/200 [13:13:04<16:34:04, 537.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6747.3494873046875
INFO:root:current train perplexity190.7295379638672
INFO:root:current mean train loss 6648.212938581194
INFO:root:current train perplexity186.7921905517578
INFO:root:current mean train loss 6634.45192732901
INFO:root:current train perplexity185.41209411621094
INFO:root:current mean train loss 6635.034256372696
INFO:root:current train perplexity184.97991943359375
INFO:root:current mean train loss 6613.032023902078
INFO:root:current train perplexity184.3291473388672
INFO:root:current mean train loss 6608.837356567383
INFO:root:current train perplexity184.48240661621094
INFO:root:current mean train loss 6613.024457943985
INFO:root:current train perplexity184.22528076171875
INFO:root:current mean train loss 6611.711246790511
INFO:root:current train perplexity183.88107299804688
INFO:root:current mean train loss 6619.282515802995
INFO:root:current train perplexity184.12384033203125
INFO:root:current mean train loss 6618.202961168791
INFO:root:current train perplexity184.0104522705078
INFO:root:current mean train loss 6614.227884526309
INFO:root:current train perplexity183.7139434814453
INFO:root:current mean train loss 6615.631329653074
INFO:root:current train perplexity183.78289794921875
INFO:root:current mean train loss 6608.750949167183
INFO:root:current train perplexity183.64544677734375
INFO:root:current mean train loss 6609.419839254239
INFO:root:current train perplexity183.46348571777344
INFO:root:current mean train loss 6607.912389133875
INFO:root:current train perplexity183.46339416503906
INFO:root:current mean train loss 6607.931118112392
INFO:root:current train perplexity183.3560791015625
INFO:root:current mean train loss 6608.073441498333
INFO:root:current train perplexity183.36997985839844
INFO:root:current mean train loss 6606.513063519915
INFO:root:current train perplexity183.42428588867188
INFO:root:current mean train loss 6607.491909136572
INFO:root:current train perplexity183.4754638671875
INFO:root:current mean train loss 6603.939726378629
INFO:root:current train perplexity183.1969451904297

100%|██████████| 1/1 [07:26<00:00, 446.57s/it][A100%|██████████| 1/1 [07:26<00:00, 446.57s/it]
INFO:root:final mean train loss: 6602.737887153106
INFO:root:final train perplexity: 183.2527618408203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.34s/it][A100%|██████████| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 6307.140519378879
INFO:root:eval perplexity: 164.6513671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.23s/it][A100%|██████████| 1/1 [00:42<00:00, 42.23s/it]
INFO:root:eval mean loss: 6445.586887224346
INFO:root:eval perplexity: 200.15704345703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/90
 45%|████▌     | 90/200 [13:21:56<16:22:05, 535.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6635.272679822198
INFO:root:current train perplexity188.82981872558594
INFO:root:current mean train loss 6617.928442193556
INFO:root:current train perplexity186.34950256347656
INFO:root:current mean train loss 6601.29397942822
INFO:root:current train perplexity183.82215881347656
INFO:root:current mean train loss 6594.378413516338
INFO:root:current train perplexity183.98455810546875
INFO:root:current mean train loss 6591.21159650896
INFO:root:current train perplexity183.4993438720703
INFO:root:current mean train loss 6581.534572896975
INFO:root:current train perplexity183.16751098632812
INFO:root:current mean train loss 6582.2098010234495
INFO:root:current train perplexity183.09642028808594
INFO:root:current mean train loss 6589.934322487998
INFO:root:current train perplexity183.575927734375
INFO:root:current mean train loss 6597.021201065855
INFO:root:current train perplexity183.77536010742188
INFO:root:current mean train loss 6595.61574841059
INFO:root:current train perplexity183.8247833251953
INFO:root:current mean train loss 6596.843325304452
INFO:root:current train perplexity183.94325256347656
INFO:root:current mean train loss 6602.915007923218
INFO:root:current train perplexity183.9869384765625
INFO:root:current mean train loss 6606.229791354124
INFO:root:current train perplexity184.38038635253906
INFO:root:current mean train loss 6607.7224067820025
INFO:root:current train perplexity184.43359375
INFO:root:current mean train loss 6607.848754114
INFO:root:current train perplexity184.4150390625
INFO:root:current mean train loss 6611.8125756851905
INFO:root:current train perplexity184.58888244628906
INFO:root:current mean train loss 6616.670855874003
INFO:root:current train perplexity184.76023864746094
INFO:root:current mean train loss 6617.03499612538
INFO:root:current train perplexity184.92030334472656
INFO:root:current mean train loss 6616.969141639472
INFO:root:current train perplexity185.0101776123047
INFO:root:current mean train loss 6619.060848095678
INFO:root:current train perplexity185.25311279296875

100%|██████████| 1/1 [08:28<00:00, 508.11s/it][A100%|██████████| 1/1 [08:28<00:00, 508.11s/it]
INFO:root:final mean train loss: 6617.265708596311
INFO:root:final train perplexity: 185.36585998535156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.58s/it][A100%|██████████| 1/1 [00:45<00:00, 45.58s/it]
INFO:root:eval mean loss: 6392.904826712101
INFO:root:eval perplexity: 176.4843292236328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.40s/it][A100%|██████████| 1/1 [00:43<00:00, 43.40s/it]
INFO:root:eval mean loss: 6537.8441014932405
INFO:root:eval perplexity: 215.92884826660156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/91
 46%|████▌     | 91/200 [13:31:55<16:48:04, 554.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6660.387313179348
INFO:root:current train perplexity191.02452087402344
INFO:root:current mean train loss 6692.255147019478
INFO:root:current train perplexity193.0692901611328
INFO:root:current mean train loss 6696.006885559578
INFO:root:current train perplexity194.3112335205078
INFO:root:current mean train loss 6698.352129809429
INFO:root:current train perplexity194.5394744873047
INFO:root:current mean train loss 6707.879926604541
INFO:root:current train perplexity195.09397888183594
INFO:root:current mean train loss 6715.251100868532
INFO:root:current train perplexity195.72479248046875
INFO:root:current mean train loss 6712.253046844766
INFO:root:current train perplexity196.7860107421875
INFO:root:current mean train loss 6709.458654490617
INFO:root:current train perplexity197.22348022460938
INFO:root:current mean train loss 6707.162078785276
INFO:root:current train perplexity197.68162536621094
INFO:root:current mean train loss 6711.044581213663
INFO:root:current train perplexity198.41815185546875
INFO:root:current mean train loss 6719.3442396816745
INFO:root:current train perplexity199.3215789794922
INFO:root:current mean train loss 6723.312691733475
INFO:root:current train perplexity200.4777069091797
INFO:root:current mean train loss 6720.356423367275
INFO:root:current train perplexity200.91371154785156
INFO:root:current mean train loss 6730.356395250975
INFO:root:current train perplexity201.7447967529297
INFO:root:current mean train loss 6735.22059169152
INFO:root:current train perplexity202.7073516845703
INFO:root:current mean train loss 6739.50348997918
INFO:root:current train perplexity203.4970245361328
INFO:root:current mean train loss 6741.327089998006
INFO:root:current train perplexity203.99261474609375
INFO:root:current mean train loss 6744.549193916452
INFO:root:current train perplexity204.56838989257812
INFO:root:current mean train loss 6747.587755990571
INFO:root:current train perplexity205.2620391845703
INFO:root:current mean train loss 6751.496579773012
INFO:root:current train perplexity205.89569091796875

100%|██████████| 1/1 [08:20<00:00, 500.93s/it][A100%|██████████| 1/1 [08:20<00:00, 500.93s/it]
INFO:root:final mean train loss: 6751.682943570151
INFO:root:final train perplexity: 206.1107177734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.12s/it][A100%|██████████| 1/1 [00:45<00:00, 45.12s/it]
INFO:root:eval mean loss: 6586.808304590536
INFO:root:eval perplexity: 206.46739196777344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.51s/it][A100%|██████████| 1/1 [00:42<00:00, 42.51s/it]
INFO:root:eval mean loss: 6717.5271480912015
INFO:root:eval perplexity: 250.30299377441406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/92
 46%|████▌     | 92/200 [13:41:47<16:58:26, 565.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6830.566708519345
INFO:root:current train perplexity215.19638061523438
INFO:root:current mean train loss 6833.136368265913
INFO:root:current train perplexity216.50570678710938
INFO:root:current mean train loss 6816.715226206036
INFO:root:current train perplexity215.7259063720703
INFO:root:current mean train loss 6817.39900379864
INFO:root:current train perplexity216.683349609375
INFO:root:current mean train loss 6807.371969070599
INFO:root:current train perplexity216.3737030029297
INFO:root:current mean train loss 6825.581823101687
INFO:root:current train perplexity217.0788116455078
INFO:root:current mean train loss 6826.330281391402
INFO:root:current train perplexity217.12269592285156
INFO:root:current mean train loss 6824.924206334986
INFO:root:current train perplexity216.8914337158203
INFO:root:current mean train loss 6818.992012669286
INFO:root:current train perplexity217.008544921875
INFO:root:current mean train loss 6819.83661490784
INFO:root:current train perplexity216.9424285888672
INFO:root:current mean train loss 6822.373450177857
INFO:root:current train perplexity216.75608825683594
INFO:root:current mean train loss 6823.487869800623
INFO:root:current train perplexity216.6414031982422
INFO:root:current mean train loss 6819.764576915702
INFO:root:current train perplexity216.52706909179688
INFO:root:current mean train loss 6819.670136819057
INFO:root:current train perplexity216.5474090576172
INFO:root:current mean train loss 6822.017158930707
INFO:root:current train perplexity216.65225219726562
INFO:root:current mean train loss 6819.6740586762235
INFO:root:current train perplexity216.4696807861328
INFO:root:current mean train loss 6815.123694001804
INFO:root:current train perplexity216.17930603027344
INFO:root:current mean train loss 6814.421693313953
INFO:root:current train perplexity216.0365753173828
INFO:root:current mean train loss 6814.558488650278
INFO:root:current train perplexity216.08717346191406
INFO:root:current mean train loss 6811.9608919801485
INFO:root:current train perplexity215.95738220214844

100%|██████████| 1/1 [08:25<00:00, 505.89s/it][A100%|██████████| 1/1 [08:25<00:00, 505.89s/it]
INFO:root:final mean train loss: 6810.68284803151
INFO:root:final train perplexity: 215.9346160888672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.70s/it][A100%|██████████| 1/1 [00:45<00:00, 45.70s/it]
INFO:root:eval mean loss: 6543.73518707059
INFO:root:eval perplexity: 199.394775390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.84s/it][A100%|██████████| 1/1 [00:42<00:00, 42.84s/it]
INFO:root:eval mean loss: 6658.251909837655
INFO:root:eval perplexity: 238.39772033691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/93
 46%|████▋     | 93/200 [13:51:44<17:05:45, 575.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6868.437145996094
INFO:root:current train perplexity216.45046997070312
INFO:root:current mean train loss 6894.833873155382
INFO:root:current train perplexity216.8158416748047
INFO:root:current mean train loss 6849.80767124721
INFO:root:current train perplexity215.14877319335938
INFO:root:current mean train loss 6831.711023591694
INFO:root:current train perplexity214.6309051513672
INFO:root:current mean train loss 6825.733792114258
INFO:root:current train perplexity214.7805938720703
INFO:root:current mean train loss 6825.425647393588
INFO:root:current train perplexity215.03318786621094
INFO:root:current mean train loss 6821.372682818244
INFO:root:current train perplexity214.9932098388672
INFO:root:current mean train loss 6813.528083683894
INFO:root:current train perplexity214.55947875976562
INFO:root:current mean train loss 6814.753181041371
INFO:root:current train perplexity214.4023895263672
INFO:root:current mean train loss 6808.617576630261
INFO:root:current train perplexity214.0782470703125
INFO:root:current mean train loss 6807.989444082755
INFO:root:current train perplexity214.1059112548828
INFO:root:current mean train loss 6810.927151747881
INFO:root:current train perplexity214.40280151367188
INFO:root:current mean train loss 6807.880952072143
INFO:root:current train perplexity214.22866821289062
INFO:root:current mean train loss 6805.8117587324505
INFO:root:current train perplexity214.08917236328125
INFO:root:current mean train loss 6802.238982659417
INFO:root:current train perplexity214.0455780029297
INFO:root:current mean train loss 6800.183141935325
INFO:root:current train perplexity213.86317443847656
INFO:root:current mean train loss 6798.558438546317
INFO:root:current train perplexity213.66317749023438
INFO:root:current mean train loss 6799.905503588045
INFO:root:current train perplexity213.6895751953125
INFO:root:current mean train loss 6799.0264832841585
INFO:root:current train perplexity213.6827392578125
INFO:root:current mean train loss 6798.043610667219
INFO:root:current train perplexity213.51133728027344

100%|██████████| 1/1 [08:24<00:00, 504.87s/it][A100%|██████████| 1/1 [08:24<00:00, 504.87s/it]
INFO:root:final mean train loss: 6796.439046839542
INFO:root:final train perplexity: 213.52099609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.59s/it][A100%|██████████| 1/1 [00:45<00:00, 45.59s/it]
INFO:root:eval mean loss: 6525.1015070921985
INFO:root:eval perplexity: 196.41065979003906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.67s/it][A100%|██████████| 1/1 [00:43<00:00, 43.67s/it]
INFO:root:eval mean loss: 6633.485085778202
INFO:root:eval perplexity: 233.59246826171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/94
 47%|████▋     | 94/200 [14:01:41<17:07:37, 581.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6793.399247946199
INFO:root:current train perplexity212.4770050048828
INFO:root:current mean train loss 6789.420675364848
INFO:root:current train perplexity213.57496643066406
INFO:root:current mean train loss 6790.07644643045
INFO:root:current train perplexity214.40127563476562
INFO:root:current mean train loss 6778.154769167191
INFO:root:current train perplexity213.12449645996094
INFO:root:current mean train loss 6779.299563985475
INFO:root:current train perplexity212.7974090576172
INFO:root:current mean train loss 6778.386025995865
INFO:root:current train perplexity212.55848693847656
INFO:root:current mean train loss 6771.350686816266
INFO:root:current train perplexity212.29339599609375
INFO:root:current mean train loss 6769.024191670915
INFO:root:current train perplexity211.8499755859375
INFO:root:current mean train loss 6766.981365837514
INFO:root:current train perplexity211.68386840820312
INFO:root:current mean train loss 6771.815923845756
INFO:root:current train perplexity211.7812957763672
INFO:root:current mean train loss 6773.625473592753
INFO:root:current train perplexity211.5846710205078
INFO:root:current mean train loss 6778.0777550157945
INFO:root:current train perplexity211.63238525390625
INFO:root:current mean train loss 6778.9632215419
INFO:root:current train perplexity211.60525512695312
INFO:root:current mean train loss 6779.927188073215
INFO:root:current train perplexity211.60292053222656
INFO:root:current mean train loss 6778.113988393454
INFO:root:current train perplexity211.5234375
INFO:root:current mean train loss 6779.045558138795
INFO:root:current train perplexity211.49105834960938
INFO:root:current mean train loss 6779.174512639493
INFO:root:current train perplexity211.3271484375
INFO:root:current mean train loss 6784.180989039893
INFO:root:current train perplexity211.4292449951172
INFO:root:current mean train loss 6785.365760236145
INFO:root:current train perplexity211.39486694335938

100%|██████████| 1/1 [08:24<00:00, 504.03s/it][A100%|██████████| 1/1 [08:24<00:00, 504.03s/it]
INFO:root:final mean train loss: 6784.448056650474
INFO:root:final train perplexity: 211.5098114013672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.14s/it][A100%|██████████| 1/1 [00:46<00:00, 46.14s/it]
INFO:root:eval mean loss: 6512.476242173648
INFO:root:eval perplexity: 194.41429138183594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.09s/it][A100%|██████████| 1/1 [00:43<00:00, 43.09s/it]
INFO:root:eval mean loss: 6617.949674132868
INFO:root:eval perplexity: 230.6279754638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/95
 48%|████▊     | 95/200 [14:11:36<17:05:19, 585.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6815.54411969866
INFO:root:current train perplexity214.96961975097656
INFO:root:current mean train loss 6762.859580592105
INFO:root:current train perplexity211.18641662597656
INFO:root:current mean train loss 6769.953503760222
INFO:root:current train perplexity211.6107940673828
INFO:root:current mean train loss 6769.468494974124
INFO:root:current train perplexity211.506591796875
INFO:root:current mean train loss 6769.895231827446
INFO:root:current train perplexity211.08876037597656
INFO:root:current mean train loss 6770.957920415856
INFO:root:current train perplexity210.42849731445312
INFO:root:current mean train loss 6761.6725223305175
INFO:root:current train perplexity209.8876495361328
INFO:root:current mean train loss 6751.098542542017
INFO:root:current train perplexity209.70138549804688
INFO:root:current mean train loss 6761.058477378301
INFO:root:current train perplexity209.84011840820312
INFO:root:current mean train loss 6758.8515918823505
INFO:root:current train perplexity209.7701873779297
INFO:root:current mean train loss 6767.438689403045
INFO:root:current train perplexity209.9044647216797
INFO:root:current mean train loss 6771.201980563425
INFO:root:current train perplexity209.69268798828125
INFO:root:current mean train loss 6772.994883504299
INFO:root:current train perplexity209.8369140625
INFO:root:current mean train loss 6775.717160298944
INFO:root:current train perplexity209.8611297607422
INFO:root:current mean train loss 6774.340376577417
INFO:root:current train perplexity209.8135986328125
INFO:root:current mean train loss 6775.065089761188
INFO:root:current train perplexity209.91297912597656
INFO:root:current mean train loss 6774.746978646317
INFO:root:current train perplexity209.94921875
INFO:root:current mean train loss 6776.5791690786355
INFO:root:current train perplexity209.93600463867188
INFO:root:current mean train loss 6778.285894324525
INFO:root:current train perplexity209.90257263183594
INFO:root:current mean train loss 6775.645912672659
INFO:root:current train perplexity209.8685302734375

100%|██████████| 1/1 [08:29<00:00, 509.76s/it][A100%|██████████| 1/1 [08:29<00:00, 509.76s/it]
INFO:root:final mean train loss: 6774.8704014642035
INFO:root:final train perplexity: 209.91708374023438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.57s/it][A100%|██████████| 1/1 [00:45<00:00, 45.57s/it]
INFO:root:eval mean loss: 6499.773169118462
INFO:root:eval perplexity: 192.4261016845703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.24s/it][A100%|██████████| 1/1 [00:43<00:00, 43.24s/it]
INFO:root:eval mean loss: 6602.878708859707
INFO:root:eval perplexity: 227.7880859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/96
 48%|████▊     | 96/200 [14:21:38<17:03:32, 590.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6764.555034022177
INFO:root:current train perplexity210.5360870361328
INFO:root:current mean train loss 6815.6361596493325
INFO:root:current train perplexity210.84832763671875
INFO:root:current mean train loss 6781.90137987013
INFO:root:current train perplexity209.15313720703125
INFO:root:current mean train loss 6781.265666304758
INFO:root:current train perplexity209.27195739746094
INFO:root:current mean train loss 6771.827084994925
INFO:root:current train perplexity208.58822631835938
INFO:root:current mean train loss 6775.724080633533
INFO:root:current train perplexity208.7786865234375
INFO:root:current mean train loss 6785.592529683786
INFO:root:current train perplexity209.16281127929688
INFO:root:current mean train loss 6778.301890069254
INFO:root:current train perplexity208.8107452392578
INFO:root:current mean train loss 6775.693874685056
INFO:root:current train perplexity208.95765686035156
INFO:root:current mean train loss 6777.026010023664
INFO:root:current train perplexity208.8351287841797
INFO:root:current mean train loss 6778.296855582414
INFO:root:current train perplexity208.96661376953125
INFO:root:current mean train loss 6779.246587211953
INFO:root:current train perplexity209.2833251953125
INFO:root:current mean train loss 6778.722288551609
INFO:root:current train perplexity209.14007568359375
INFO:root:current mean train loss 6775.383865001057
INFO:root:current train perplexity208.99591064453125
INFO:root:current mean train loss 6775.351372101022
INFO:root:current train perplexity209.04595947265625
INFO:root:current mean train loss 6771.689871879593
INFO:root:current train perplexity208.8428955078125
INFO:root:current mean train loss 6772.1620204605115
INFO:root:current train perplexity208.90684509277344
INFO:root:current mean train loss 6771.358433133395
INFO:root:current train perplexity208.7865447998047
INFO:root:current mean train loss 6768.520414476379
INFO:root:current train perplexity208.64930725097656
INFO:root:current mean train loss 6769.475070094025
INFO:root:current train perplexity208.7417449951172

100%|██████████| 1/1 [08:26<00:00, 506.17s/it][A100%|██████████| 1/1 [08:26<00:00, 506.17s/it]
INFO:root:final mean train loss: 6767.471879629192
INFO:root:final train perplexity: 208.69517517089844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.01s/it][A100%|██████████| 1/1 [00:45<00:00, 45.01s/it]
INFO:root:eval mean loss: 6494.2773922318265
INFO:root:eval perplexity: 191.57220458984375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.37s/it][A100%|██████████| 1/1 [00:43<00:00, 43.37s/it]
INFO:root:eval mean loss: 6595.822343542221
INFO:root:eval perplexity: 226.47036743164062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/97
 48%|████▊     | 97/200 [14:31:35<16:57:08, 592.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6801.573588053386
INFO:root:current train perplexity207.0219268798828
INFO:root:current mean train loss 6752.911301071579
INFO:root:current train perplexity208.88392639160156
INFO:root:current mean train loss 6738.845403855847
INFO:root:current train perplexity208.17198181152344
INFO:root:current mean train loss 6749.566949252425
INFO:root:current train perplexity208.2230224609375
INFO:root:current mean train loss 6765.354371207101
INFO:root:current train perplexity208.92576599121094
INFO:root:current mean train loss 6774.124109866845
INFO:root:current train perplexity209.2308349609375
INFO:root:current mean train loss 6779.015240704572
INFO:root:current train perplexity209.406005859375
INFO:root:current mean train loss 6785.836813534007
INFO:root:current train perplexity209.11663818359375
INFO:root:current mean train loss 6781.386628924675
INFO:root:current train perplexity208.90863037109375
INFO:root:current mean train loss 6782.596846568434
INFO:root:current train perplexity209.14974975585938
INFO:root:current mean train loss 6786.028282573205
INFO:root:current train perplexity209.34341430664062
INFO:root:current mean train loss 6784.099692740091
INFO:root:current train perplexity209.01077270507812
INFO:root:current mean train loss 6783.0310782408105
INFO:root:current train perplexity208.6959686279297
INFO:root:current mean train loss 6781.181417855735
INFO:root:current train perplexity208.62005615234375
INFO:root:current mean train loss 6777.06239175533
INFO:root:current train perplexity208.6129913330078
INFO:root:current mean train loss 6776.371525254361
INFO:root:current train perplexity208.65357971191406
INFO:root:current mean train loss 6774.10135124725
INFO:root:current train perplexity208.6486053466797
INFO:root:current mean train loss 6769.732620763015
INFO:root:current train perplexity208.6007537841797
INFO:root:current mean train loss 6770.275726714692
INFO:root:current train perplexity208.56863403320312
INFO:root:current mean train loss 6767.880086597231
INFO:root:current train perplexity208.46144104003906

100%|██████████| 1/1 [08:24<00:00, 504.78s/it][A100%|██████████| 1/1 [08:24<00:00, 504.78s/it]
INFO:root:final mean train loss: 6765.592916006761
INFO:root:final train perplexity: 208.3858184814453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.51s/it][A100%|██████████| 1/1 [00:46<00:00, 46.52s/it]
INFO:root:eval mean loss: 6496.465846284907
INFO:root:eval perplexity: 191.91177368164062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.18s/it][A100%|██████████| 1/1 [00:45<00:00, 45.18s/it]
INFO:root:eval mean loss: 6596.4251042359265
INFO:root:eval perplexity: 226.58270263671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/98
 49%|████▉     | 98/200 [14:41:34<16:50:35, 594.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6806.873978365385
INFO:root:current train perplexity210.87196350097656
INFO:root:current mean train loss 6821.640151515152
INFO:root:current train perplexity210.6993865966797
INFO:root:current mean train loss 6803.152878095519
INFO:root:current train perplexity209.83641052246094
INFO:root:current mean train loss 6787.5503504922945
INFO:root:current train perplexity209.3406219482422
INFO:root:current mean train loss 6772.56462953629
INFO:root:current train perplexity208.67666625976562
INFO:root:current mean train loss 6767.334901306693
INFO:root:current train perplexity208.6581573486328
INFO:root:current mean train loss 6769.571065848214
INFO:root:current train perplexity208.6528778076172
INFO:root:current mean train loss 6775.758321844362
INFO:root:current train perplexity208.7324981689453
INFO:root:current mean train loss 6778.554010680094
INFO:root:current train perplexity208.74134826660156
INFO:root:current mean train loss 6777.042841240285
INFO:root:current train perplexity208.94081115722656
INFO:root:current mean train loss 6771.70322449017
INFO:root:current train perplexity208.81332397460938
INFO:root:current mean train loss 6776.252469068535
INFO:root:current train perplexity209.16619873046875
INFO:root:current mean train loss 6768.499395534832
INFO:root:current train perplexity209.06668090820312
INFO:root:current mean train loss 6768.350373812385
INFO:root:current train perplexity209.02581787109375
INFO:root:current mean train loss 6768.879270544475
INFO:root:current train perplexity209.06369018554688
INFO:root:current mean train loss 6770.819986147165
INFO:root:current train perplexity209.07875061035156
INFO:root:current mean train loss 6771.329470486111
INFO:root:current train perplexity209.08143615722656
INFO:root:current mean train loss 6771.297877290634
INFO:root:current train perplexity209.22067260742188
INFO:root:current mean train loss 6772.7411748073055
INFO:root:current train perplexity209.34361267089844
INFO:root:current mean train loss 6772.916490736323
INFO:root:current train perplexity209.3621826171875

100%|██████████| 1/1 [08:28<00:00, 508.16s/it][A100%|██████████| 1/1 [08:28<00:00, 508.16s/it]
INFO:root:final mean train loss: 6772.232135997778
INFO:root:final train perplexity: 209.48060607910156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.73s/it][A100%|██████████| 1/1 [00:44<00:00, 44.73s/it]
INFO:root:eval mean loss: 6506.329494611591
INFO:root:eval perplexity: 193.4497833251953
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.62s/it][A100%|██████████| 1/1 [00:42<00:00, 42.62s/it]
INFO:root:eval mean loss: 6602.84587887162
INFO:root:eval perplexity: 227.7819061279297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/99
 50%|████▉     | 99/200 [14:51:32<16:42:31, 595.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6705.113805259146
INFO:root:current train perplexity208.74462890625
INFO:root:current mean train loss 6754.3103912688875
INFO:root:current train perplexity209.82962036132812
INFO:root:current mean train loss 6772.174494750111
INFO:root:current train perplexity210.0229034423828
INFO:root:current mean train loss 6773.793301088023
INFO:root:current train perplexity209.9237823486328
INFO:root:current mean train loss 6777.411606911307
INFO:root:current train perplexity210.51129150390625
INFO:root:current mean train loss 6776.77794193648
INFO:root:current train perplexity210.39617919921875
INFO:root:current mean train loss 6773.41406965955
INFO:root:current train perplexity210.69204711914062
INFO:root:current mean train loss 6772.988420491329
INFO:root:current train perplexity210.52032470703125
INFO:root:current mean train loss 6776.274601181619
INFO:root:current train perplexity210.96620178222656
INFO:root:current mean train loss 6780.664026699338
INFO:root:current train perplexity211.2946014404297
INFO:root:current mean train loss 6779.515504057879
INFO:root:current train perplexity211.46080017089844
INFO:root:current mean train loss 6783.998749966952
INFO:root:current train perplexity211.7303009033203
INFO:root:current mean train loss 6781.8169303334635
INFO:root:current train perplexity211.90817260742188
INFO:root:current mean train loss 6784.775897985257
INFO:root:current train perplexity212.09872436523438
INFO:root:current mean train loss 6788.814298930921
INFO:root:current train perplexity212.4788360595703
INFO:root:current mean train loss 6791.928018639875
INFO:root:current train perplexity212.92327880859375
INFO:root:current mean train loss 6794.861934847838
INFO:root:current train perplexity213.31869506835938
INFO:root:current mean train loss 6798.519612082194
INFO:root:current train perplexity213.62661743164062
INFO:root:current mean train loss 6801.895172605606
INFO:root:current train perplexity213.96609497070312
INFO:root:current mean train loss 6803.403369091354
INFO:root:current train perplexity214.37387084960938

100%|██████████| 1/1 [08:25<00:00, 505.72s/it][A100%|██████████| 1/1 [08:25<00:00, 505.72s/it]
INFO:root:final mean train loss: 6801.578456922907
INFO:root:final train perplexity: 214.38870239257812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.20s/it][A100%|██████████| 1/1 [00:45<00:00, 45.20s/it]
INFO:root:eval mean loss: 6576.219480690381
INFO:root:eval perplexity: 204.70578002929688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.39s/it][A100%|██████████| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 6661.27750824191
INFO:root:eval perplexity: 238.9912872314453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/100
 50%|█████     | 100/200 [15:01:29<16:33:15, 595.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6886.196343315973
INFO:root:current train perplexity223.97348022460938
INFO:root:current mean train loss 6882.999435654837
INFO:root:current train perplexity223.63827514648438
INFO:root:current mean train loss 6882.59337276599
INFO:root:current train perplexity223.96685791015625
INFO:root:current mean train loss 6874.201898789944
INFO:root:current train perplexity224.6375732421875
INFO:root:current mean train loss 6863.253092121743
INFO:root:current train perplexity224.55746459960938
INFO:root:current mean train loss 6868.101472832324
INFO:root:current train perplexity225.36424255371094
INFO:root:current mean train loss 6873.92669773784
INFO:root:current train perplexity226.56454467773438
INFO:root:current mean train loss 6888.803009376956
INFO:root:current train perplexity227.7088165283203
INFO:root:current mean train loss 6889.050139260637
INFO:root:current train perplexity228.55699157714844
INFO:root:current mean train loss 6901.481589010886
INFO:root:current train perplexity229.69017028808594
INFO:root:current mean train loss 6903.064185214542
INFO:root:current train perplexity230.7069549560547
INFO:root:current mean train loss 6909.783872220992
INFO:root:current train perplexity231.91116333007812
INFO:root:current mean train loss 6913.21595751239
INFO:root:current train perplexity233.1300048828125
INFO:root:current mean train loss 6921.173086803074
INFO:root:current train perplexity234.52159118652344
INFO:root:current mean train loss 6930.285690786045
INFO:root:current train perplexity235.9619903564453
INFO:root:current mean train loss 6936.93708256381
INFO:root:current train perplexity237.6901092529297
INFO:root:current mean train loss 6946.622983935863
INFO:root:current train perplexity239.50933837890625
INFO:root:current mean train loss 6959.46929989317
INFO:root:current train perplexity241.92364501953125
INFO:root:current mean train loss 6970.773936323394
INFO:root:current train perplexity244.413818359375

100%|██████████| 1/1 [08:27<00:00, 507.20s/it][A100%|██████████| 1/1 [08:27<00:00, 507.20s/it]
INFO:root:final mean train loss: 6980.763033021897
INFO:root:final train perplexity: 246.95420837402344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.83s/it][A100%|██████████| 1/1 [00:45<00:00, 45.83s/it]
INFO:root:eval mean loss: 7084.895403922872
INFO:root:eval perplexity: 308.9566650390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.11s/it][A100%|██████████| 1/1 [00:43<00:00, 43.12s/it]
INFO:root:eval mean loss: 7167.496306723737
INFO:root:eval perplexity: 362.3475646972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/101
 50%|█████     | 101/200 [15:11:27<16:24:40, 596.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7388.811370849609
INFO:root:current train perplexity320.8905334472656
INFO:root:current mean train loss 7376.107712318158
INFO:root:current train perplexity338.09796142578125
INFO:root:current mean train loss 7409.413067853009
INFO:root:current train perplexity345.9703369140625
INFO:root:current mean train loss 7388.530406324169
INFO:root:current train perplexity338.8031921386719
INFO:root:current mean train loss 7366.303808358999
INFO:root:current train perplexity330.0884704589844
INFO:root:current mean train loss 7337.818183366642
INFO:root:current train perplexity325.45819091796875
INFO:root:current mean train loss 7325.744624150264
INFO:root:current train perplexity322.8663330078125
INFO:root:current mean train loss 7318.408757556084
INFO:root:current train perplexity322.0480041503906
INFO:root:current mean train loss 7311.74780811983
INFO:root:current train perplexity321.099609375
INFO:root:current mean train loss 7309.0106110552
INFO:root:current train perplexity320.25616455078125
INFO:root:current mean train loss 7304.605125607468
INFO:root:current train perplexity319.433349609375
INFO:root:current mean train loss 7308.3890026461695
INFO:root:current train perplexity319.2579345703125
INFO:root:current mean train loss 7300.801133005242
INFO:root:current train perplexity318.56024169921875
INFO:root:current mean train loss 7301.904869752089
INFO:root:current train perplexity318.7269287109375
INFO:root:current mean train loss 7309.658953478107
INFO:root:current train perplexity319.1502990722656
INFO:root:current mean train loss 7309.788642822909
INFO:root:current train perplexity319.09326171875
INFO:root:current mean train loss 7311.455069664681
INFO:root:current train perplexity319.3165283203125
INFO:root:current mean train loss 7308.888996542195
INFO:root:current train perplexity319.2532043457031
INFO:root:current mean train loss 7310.063750010755
INFO:root:current train perplexity319.4618225097656
INFO:root:current mean train loss 7308.640535549729
INFO:root:current train perplexity319.3824768066406

100%|██████████| 1/1 [08:30<00:00, 510.96s/it][A100%|██████████| 1/1 [08:30<00:00, 510.96s/it]
INFO:root:final mean train loss: 7307.076576683056
INFO:root:final train perplexity: 319.49078369140625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.55s/it][A100%|██████████| 1/1 [00:45<00:00, 45.55s/it]
INFO:root:eval mean loss: 7110.030294215426
INFO:root:eval perplexity: 315.3053283691406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.73s/it][A100%|██████████| 1/1 [00:42<00:00, 42.73s/it]
INFO:root:eval mean loss: 7183.019714788342
INFO:root:eval perplexity: 367.0016784667969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/102
 51%|█████     | 102/200 [15:21:29<16:17:10, 598.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7267.571747750947
INFO:root:current train perplexity324.62030029296875
INFO:root:current mean train loss 7297.864478089756
INFO:root:current train perplexity321.41534423828125
INFO:root:current mean train loss 7293.293448648739
INFO:root:current train perplexity320.90582275390625
INFO:root:current mean train loss 7297.012869803397
INFO:root:current train perplexity321.5871887207031
INFO:root:current mean train loss 7292.329269585378
INFO:root:current train perplexity321.3592529296875
INFO:root:current mean train loss 7293.305924234874
INFO:root:current train perplexity321.5235595703125
INFO:root:current mean train loss 7311.96374993829
INFO:root:current train perplexity322.5538024902344
INFO:root:current mean train loss 7319.761760716874
INFO:root:current train perplexity323.18609619140625
INFO:root:current mean train loss 7324.022068788453
INFO:root:current train perplexity323.1993408203125
INFO:root:current mean train loss 7319.208242794581
INFO:root:current train perplexity322.2640075683594
INFO:root:current mean train loss 7316.420941924311
INFO:root:current train perplexity321.4912109375
INFO:root:current mean train loss 7311.186315282298
INFO:root:current train perplexity320.1260681152344
INFO:root:current mean train loss 7306.826508088124
INFO:root:current train perplexity319.50799560546875
INFO:root:current mean train loss 7303.1634452802655
INFO:root:current train perplexity318.4154968261719
INFO:root:current mean train loss 7302.0364766197445
INFO:root:current train perplexity317.93682861328125
INFO:root:current mean train loss 7299.229080030985
INFO:root:current train perplexity317.3155517578125
INFO:root:current mean train loss 7296.247571451126
INFO:root:current train perplexity316.5279235839844
INFO:root:current mean train loss 7293.528978210203
INFO:root:current train perplexity315.9422607421875
INFO:root:current mean train loss 7287.8337965745195
INFO:root:current train perplexity315.38397216796875
INFO:root:current mean train loss 7290.374272756483
INFO:root:current train perplexity315.0196533203125

100%|██████████| 1/1 [08:28<00:00, 508.92s/it][A100%|██████████| 1/1 [08:28<00:00, 508.94s/it]
INFO:root:final mean train loss: 7288.675143258714
INFO:root:final train perplexity: 314.8847961425781
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.20s/it][A100%|██████████| 1/1 [00:45<00:00, 45.20s/it]
INFO:root:eval mean loss: 7052.551079066932
INFO:root:eval perplexity: 300.9752197265625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.82s/it][A100%|██████████| 1/1 [00:42<00:00, 42.82s/it]
INFO:root:eval mean loss: 7113.140902039007
INFO:root:eval perplexity: 346.51171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/103
 52%|█████▏    | 103/200 [15:31:29<16:07:56, 598.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7226.667509765625
INFO:root:current train perplexity303.8353576660156
INFO:root:current mean train loss 7216.1926171875
INFO:root:current train perplexity302.9943542480469
INFO:root:current mean train loss 7233.80683203125
INFO:root:current train perplexity303.6062316894531
INFO:root:current mean train loss 7237.3111077008925
INFO:root:current train perplexity304.3363342285156
INFO:root:current mean train loss 7241.2149446614585
INFO:root:current train perplexity303.9851989746094
INFO:root:current mean train loss 7232.20483931108
INFO:root:current train perplexity302.791259765625
INFO:root:current mean train loss 7239.0214002403845
INFO:root:current train perplexity302.85162353515625
INFO:root:current mean train loss 7250.192962890625
INFO:root:current train perplexity303.3583679199219
INFO:root:current mean train loss 7239.5738189338235
INFO:root:current train perplexity303.3923645019531
INFO:root:current mean train loss 7243.374616570723
INFO:root:current train perplexity303.2396240234375
INFO:root:current mean train loss 7247.3433519345235
INFO:root:current train perplexity302.8821105957031
INFO:root:current mean train loss 7244.498466796875
INFO:root:current train perplexity302.7073974609375
INFO:root:current mean train loss 7241.259401171875
INFO:root:current train perplexity302.399658203125
INFO:root:current mean train loss 7251.7186765769675
INFO:root:current train perplexity303.4837951660156
INFO:root:current mean train loss 7250.448019261853
INFO:root:current train perplexity303.7162780761719
INFO:root:current mean train loss 7247.517064957157
INFO:root:current train perplexity303.6175537109375
INFO:root:current mean train loss 7247.8792143110795
INFO:root:current train perplexity303.5968322753906
INFO:root:current mean train loss 7247.594202287946
INFO:root:current train perplexity303.8372497558594
INFO:root:current mean train loss 7250.5572563872465
INFO:root:current train perplexity304.3633117675781
INFO:root:current mean train loss 7247.375700871395
INFO:root:current train perplexity304.2875671386719

100%|██████████| 1/1 [08:25<00:00, 505.78s/it][A100%|██████████| 1/1 [08:25<00:00, 505.78s/it]
INFO:root:final mean train loss: 7246.102784311176
INFO:root:final train perplexity: 304.48089599609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.96s/it][A100%|██████████| 1/1 [00:45<00:00, 45.96s/it]
INFO:root:eval mean loss: 7049.337497575909
INFO:root:eval perplexity: 300.1936340332031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.63s/it][A100%|██████████| 1/1 [00:43<00:00, 43.63s/it]
INFO:root:eval mean loss: 7120.24405145307
INFO:root:eval perplexity: 348.5413513183594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/104
 52%|█████▏    | 104/200 [15:41:27<15:57:39, 598.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7323.4004124883395
INFO:root:current train perplexity305.9460144042969
INFO:root:current mean train loss 7258.985603012725
INFO:root:current train perplexity301.74169921875
INFO:root:current mean train loss 7254.227429336376
INFO:root:current train perplexity302.0144958496094
INFO:root:current mean train loss 7277.784710543682
INFO:root:current train perplexity302.3232421875
INFO:root:current mean train loss 7262.490387028239
INFO:root:current train perplexity300.9198303222656
INFO:root:current mean train loss 7263.147373098545
INFO:root:current train perplexity302.12640380859375
INFO:root:current mean train loss 7264.711316704929
INFO:root:current train perplexity301.79754638671875
INFO:root:current mean train loss 7252.708913711091
INFO:root:current train perplexity300.7760925292969
INFO:root:current mean train loss 7247.599788467777
INFO:root:current train perplexity300.50543212890625
INFO:root:current mean train loss 7241.485380849276
INFO:root:current train perplexity300.2886962890625
INFO:root:current mean train loss 7244.486079179358
INFO:root:current train perplexity299.80267333984375
INFO:root:current mean train loss 7238.935146040863
INFO:root:current train perplexity299.4369201660156
INFO:root:current mean train loss 7240.00745679077
INFO:root:current train perplexity299.5108947753906
INFO:root:current mean train loss 7240.440363249817
INFO:root:current train perplexity299.8145446777344
INFO:root:current mean train loss 7237.655430206736
INFO:root:current train perplexity299.83526611328125
INFO:root:current mean train loss 7232.417374212268
INFO:root:current train perplexity299.64959716796875
INFO:root:current mean train loss 7235.696853207483
INFO:root:current train perplexity299.9293518066406
INFO:root:current mean train loss 7232.170520136973
INFO:root:current train perplexity300.0741271972656
INFO:root:current mean train loss 7234.00808135545
INFO:root:current train perplexity300.4830627441406
INFO:root:current mean train loss 7230.820410553429
INFO:root:current train perplexity300.47991943359375

100%|██████████| 1/1 [08:48<00:00, 528.69s/it][A100%|██████████| 1/1 [08:48<00:00, 528.70s/it]
INFO:root:final mean train loss: 7229.488952482823
INFO:root:final train perplexity: 300.5147399902344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:49<00:00, 49.28s/it][A100%|██████████| 1/1 [00:49<00:00, 49.28s/it]
INFO:root:eval mean loss: 7057.405720162899
INFO:root:eval perplexity: 302.15997314453125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.05s/it][A100%|██████████| 1/1 [00:46<00:00, 46.05s/it]
INFO:root:eval mean loss: 7128.962275944703
INFO:root:eval perplexity: 351.0486145019531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/105
 52%|█████▎    | 105/200 [15:51:54<16:01:10, 607.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7249.232038225447
INFO:root:current train perplexity299.31103515625
INFO:root:current mean train loss 7253.794521165931
INFO:root:current train perplexity299.622314453125
INFO:root:current mean train loss 7250.571395659111
INFO:root:current train perplexity301.16888427734375
INFO:root:current mean train loss 7259.49742380778
INFO:root:current train perplexity303.7908935546875
INFO:root:current mean train loss 7248.618029886041
INFO:root:current train perplexity304.9551696777344
INFO:root:current mean train loss 7241.838093796821
INFO:root:current train perplexity304.50311279296875
INFO:root:current mean train loss 7236.778226082785
INFO:root:current train perplexity304.1584777832031
INFO:root:current mean train loss 7236.573383564852
INFO:root:current train perplexity303.9853515625
INFO:root:current mean train loss 7243.37096560379
INFO:root:current train perplexity303.56744384765625
INFO:root:current mean train loss 7242.153649306879
INFO:root:current train perplexity303.6847229003906
INFO:root:current mean train loss 7241.1041358863295
INFO:root:current train perplexity304.0249328613281
INFO:root:current mean train loss 7245.8863001643
INFO:root:current train perplexity304.39727783203125
INFO:root:current mean train loss 7250.829577294466
INFO:root:current train perplexity304.8650207519531
INFO:root:current mean train loss 7247.223203096775
INFO:root:current train perplexity304.72491455078125
INFO:root:current mean train loss 7246.077713382854
INFO:root:current train perplexity303.9441833496094
INFO:root:current mean train loss 7239.964835427024
INFO:root:current train perplexity303.3819274902344
INFO:root:current mean train loss 7239.768538450119
INFO:root:current train perplexity303.42578125
INFO:root:current mean train loss 7243.412019601317
INFO:root:current train perplexity303.72454833984375
INFO:root:current mean train loss 7243.053858406731
INFO:root:current train perplexity303.0580749511719

100%|██████████| 1/1 [09:05<00:00, 545.94s/it][A100%|██████████| 1/1 [09:05<00:00, 545.94s/it]
INFO:root:final mean train loss: 7239.612237958191
INFO:root:final train perplexity: 302.92529296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:49<00:00, 49.77s/it][A100%|██████████| 1/1 [00:49<00:00, 49.77s/it]
INFO:root:eval mean loss: 7053.117156333112
INFO:root:eval perplexity: 301.1131591796875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.71s/it][A100%|██████████| 1/1 [00:47<00:00, 47.72s/it]
INFO:root:eval mean loss: 7123.836878566877
INFO:root:eval perplexity: 349.5722961425781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/106
 53%|█████▎    | 106/200 [16:02:40<16:09:31, 618.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7319.9736328125
INFO:root:current train perplexity327.3788757324219
INFO:root:current mean train loss 7196.575470877166
INFO:root:current train perplexity290.5478210449219
INFO:root:current mean train loss 7211.590431630908
INFO:root:current train perplexity293.7847900390625
INFO:root:current mean train loss 7228.668435942691
INFO:root:current train perplexity299.0079040527344
INFO:root:current mean train loss 7236.440055866194
INFO:root:current train perplexity301.69580078125
INFO:root:current mean train loss 7234.946629202533
INFO:root:current train perplexity301.912109375
INFO:root:current mean train loss 7241.465151667793
INFO:root:current train perplexity301.7666015625
INFO:root:current mean train loss 7241.380844050909
INFO:root:current train perplexity301.9006042480469
INFO:root:current mean train loss 7238.100222622113
INFO:root:current train perplexity302.50885009765625
INFO:root:current mean train loss 7237.713538234427
INFO:root:current train perplexity302.47119140625
INFO:root:current mean train loss 7242.747303965566
INFO:root:current train perplexity302.4345703125
INFO:root:current mean train loss 7239.2317597461115
INFO:root:current train perplexity302.2527770996094
INFO:root:current mean train loss 7246.313261897637
INFO:root:current train perplexity302.7709045410156
INFO:root:current mean train loss 7242.966037618298
INFO:root:current train perplexity302.5730285644531
INFO:root:current mean train loss 7237.7756795508785
INFO:root:current train perplexity302.50811767578125
INFO:root:current mean train loss 7242.899742294199
INFO:root:current train perplexity302.9743957519531
INFO:root:current mean train loss 7241.831970557952
INFO:root:current train perplexity303.22674560546875
INFO:root:current mean train loss 7241.654398205559
INFO:root:current train perplexity303.685302734375
INFO:root:current mean train loss 7244.067432969097
INFO:root:current train perplexity303.9573669433594
INFO:root:current mean train loss 7246.251708342238
INFO:root:current train perplexity304.1345520019531

100%|██████████| 1/1 [09:04<00:00, 544.83s/it][A100%|██████████| 1/1 [09:04<00:00, 544.83s/it]
INFO:root:final mean train loss: 7244.191200768052
INFO:root:final train perplexity: 304.0218811035156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.31s/it][A100%|██████████| 1/1 [00:46<00:00, 46.31s/it]
INFO:root:eval mean loss: 7058.243203886857
INFO:root:eval perplexity: 302.3647766113281
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.71s/it][A100%|██████████| 1/1 [00:43<00:00, 43.71s/it]
INFO:root:eval mean loss: 7125.8715638505655
INFO:root:eval perplexity: 350.15753173828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/107
 54%|█████▎    | 107/200 [16:13:18<16:07:52, 624.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7263.931287977431
INFO:root:current train perplexity312.3123474121094
INFO:root:current mean train loss 7272.149645789195
INFO:root:current train perplexity313.3587646484375
INFO:root:current mean train loss 7250.8700970291
INFO:root:current train perplexity313.2367248535156
INFO:root:current mean train loss 7247.9231939735655
INFO:root:current train perplexity313.0832214355469
INFO:root:current mean train loss 7254.644063995215
INFO:root:current train perplexity311.1754455566406
INFO:root:current mean train loss 7263.870567763634
INFO:root:current train perplexity308.7688903808594
INFO:root:current mean train loss 7255.564160788329
INFO:root:current train perplexity307.4699401855469
INFO:root:current mean train loss 7252.124978918219
INFO:root:current train perplexity307.5937194824219
INFO:root:current mean train loss 7250.3930711816165
INFO:root:current train perplexity307.4248046875
INFO:root:current mean train loss 7256.7796069708265
INFO:root:current train perplexity307.6500244140625
INFO:root:current mean train loss 7259.138488169971
INFO:root:current train perplexity307.7380676269531
INFO:root:current mean train loss 7263.818643696193
INFO:root:current train perplexity307.7260437011719
INFO:root:current mean train loss 7263.1416244131005
INFO:root:current train perplexity307.67352294921875
INFO:root:current mean train loss 7263.404040138349
INFO:root:current train perplexity307.15167236328125
INFO:root:current mean train loss 7264.686558560693
INFO:root:current train perplexity307.1109619140625
INFO:root:current mean train loss 7265.296385753767
INFO:root:current train perplexity307.5092468261719
INFO:root:current mean train loss 7267.23965314528
INFO:root:current train perplexity307.6639709472656
INFO:root:current mean train loss 7265.859578782105
INFO:root:current train perplexity307.7391052246094
INFO:root:current mean train loss 7265.602152573656
INFO:root:current train perplexity307.5868225097656
INFO:root:current mean train loss 7261.697020466062
INFO:root:current train perplexity307.2928771972656

100%|██████████| 1/1 [08:25<00:00, 505.89s/it][A100%|██████████| 1/1 [08:25<00:00, 505.89s/it]
INFO:root:final mean train loss: 7256.6943589603425
INFO:root:final train perplexity: 307.036865234375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.64s/it][A100%|██████████| 1/1 [00:45<00:00, 45.64s/it]
INFO:root:eval mean loss: 7035.853261095413
INFO:root:eval perplexity: 296.9358825683594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.68s/it][A100%|██████████| 1/1 [00:43<00:00, 43.68s/it]
INFO:root:eval mean loss: 7104.850039651208
INFO:root:eval perplexity: 344.157958984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/108
 54%|█████▍    | 108/200 [16:23:16<15:45:17, 616.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7220.56181640625
INFO:root:current train perplexity305.013916015625
INFO:root:current mean train loss 7254.639615885417
INFO:root:current train perplexity303.5910339355469
INFO:root:current mean train loss 7256.685239361702
INFO:root:current train perplexity301.22662353515625
INFO:root:current mean train loss 7232.72107917444
INFO:root:current train perplexity298.31903076171875
INFO:root:current mean train loss 7236.303185614224
INFO:root:current train perplexity298.96368408203125
INFO:root:current mean train loss 7242.307254855432
INFO:root:current train perplexity300.7205810546875
INFO:root:current mean train loss 7253.222903850886
INFO:root:current train perplexity302.6189270019531
INFO:root:current mean train loss 7271.215944541879
INFO:root:current train perplexity304.76837158203125
INFO:root:current mean train loss 7274.828146636415
INFO:root:current train perplexity306.35345458984375
INFO:root:current mean train loss 7275.236616915942
INFO:root:current train perplexity307.8902893066406
INFO:root:current mean train loss 7277.805258812651
INFO:root:current train perplexity308.62432861328125
INFO:root:current mean train loss 7274.480638250276
INFO:root:current train perplexity308.3594055175781
INFO:root:current mean train loss 7267.791912718244
INFO:root:current train perplexity308.1426086425781
INFO:root:current mean train loss 7265.68298147823
INFO:root:current train perplexity306.8377990722656
INFO:root:current mean train loss 7261.38481836618
INFO:root:current train perplexity306.8022766113281
INFO:root:current mean train loss 7267.365815541022
INFO:root:current train perplexity308.3669128417969
INFO:root:current mean train loss 7272.109270474962
INFO:root:current train perplexity309.3133850097656
INFO:root:current mean train loss 7272.9901015399855
INFO:root:current train perplexity310.1856994628906
INFO:root:current mean train loss 7274.735978744466
INFO:root:current train perplexity311.0392761230469
INFO:root:current mean train loss 7279.441604085917
INFO:root:current train perplexity312.0685119628906

100%|██████████| 1/1 [08:24<00:00, 504.26s/it][A100%|██████████| 1/1 [08:24<00:00, 504.26s/it]
INFO:root:final mean train loss: 7279.048612301241
INFO:root:final train perplexity: 312.5013732910156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.35s/it][A100%|██████████| 1/1 [00:46<00:00, 46.35s/it]
INFO:root:eval mean loss: 7132.26232996731
INFO:root:eval perplexity: 321.0292053222656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.94s/it][A100%|██████████| 1/1 [00:42<00:00, 42.94s/it]
INFO:root:eval mean loss: 7210.613725378158
INFO:root:eval perplexity: 375.4224548339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/109
 55%|█████▍    | 109/200 [16:33:12<15:25:46, 610.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7392.999708909255
INFO:root:current train perplexity329.49359130859375
INFO:root:current mean train loss 7359.0835442794
INFO:root:current train perplexity331.92156982421875
INFO:root:current mean train loss 7353.151216052827
INFO:root:current train perplexity331.3616027832031
INFO:root:current mean train loss 7370.958074396307
INFO:root:current train perplexity331.91033935546875
INFO:root:current mean train loss 7362.309023696765
INFO:root:current train perplexity328.2689514160156
INFO:root:current mean train loss 7352.221021569293
INFO:root:current train perplexity324.9636535644531
INFO:root:current mean train loss 7335.398197103863
INFO:root:current train perplexity323.4841003417969
INFO:root:current mean train loss 7320.679781000665
INFO:root:current train perplexity321.9237976074219
INFO:root:current mean train loss 7315.4955458663435
INFO:root:current train perplexity320.0030517578125
INFO:root:current mean train loss 7314.296822684152
INFO:root:current train perplexity319.274658203125
INFO:root:current mean train loss 7308.292141178262
INFO:root:current train perplexity318.9924011230469
INFO:root:current mean train loss 7305.9278975592715
INFO:root:current train perplexity318.829833984375
INFO:root:current mean train loss 7306.871002879768
INFO:root:current train perplexity318.90081787109375
INFO:root:current mean train loss 7304.052648420165
INFO:root:current train perplexity318.45819091796875
INFO:root:current mean train loss 7306.4255334102745
INFO:root:current train perplexity318.19451904296875
INFO:root:current mean train loss 7302.09285492258
INFO:root:current train perplexity317.761474609375
INFO:root:current mean train loss 7301.774671208194
INFO:root:current train perplexity317.1961975097656
INFO:root:current mean train loss 7296.422321476349
INFO:root:current train perplexity316.5437927246094
INFO:root:current mean train loss 7292.500159508723
INFO:root:current train perplexity316.0138244628906
INFO:root:current mean train loss 7293.1780648153335
INFO:root:current train perplexity315.8320007324219

100%|██████████| 1/1 [08:27<00:00, 507.53s/it][A100%|██████████| 1/1 [08:27<00:00, 507.53s/it]
INFO:root:final mean train loss: 7292.580416449981
INFO:root:final train perplexity: 315.8568420410156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.79s/it][A100%|██████████| 1/1 [00:47<00:00, 47.80s/it]
INFO:root:eval mean loss: 7083.006179701352
INFO:root:eval perplexity: 308.48486328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.83s/it][A100%|██████████| 1/1 [00:42<00:00, 42.83s/it]
INFO:root:eval mean loss: 7154.903966159685
INFO:root:eval perplexity: 358.61566162109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/110
 55%|█████▌    | 110/200 [16:43:13<15:11:16, 607.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7291.815804744112
INFO:root:current train perplexity313.7943420410156
INFO:root:current mean train loss 7244.311910595414
INFO:root:current train perplexity309.0866394042969
INFO:root:current mean train loss 7246.660510208527
INFO:root:current train perplexity309.59332275390625
INFO:root:current mean train loss 7254.4177027756605
INFO:root:current train perplexity309.8287048339844
INFO:root:current mean train loss 7255.981922141525
INFO:root:current train perplexity310.3453369140625
INFO:root:current mean train loss 7264.412837077109
INFO:root:current train perplexity311.497314453125
INFO:root:current mean train loss 7263.662877195441
INFO:root:current train perplexity311.49017333984375
INFO:root:current mean train loss 7262.44040682908
INFO:root:current train perplexity311.8560791015625
INFO:root:current mean train loss 7271.947594329869
INFO:root:current train perplexity311.9569091796875
INFO:root:current mean train loss 7263.712754571401
INFO:root:current train perplexity311.7774353027344
INFO:root:current mean train loss 7267.637950187091
INFO:root:current train perplexity312.2782287597656
INFO:root:current mean train loss 7272.157170591852
INFO:root:current train perplexity312.4161682128906
INFO:root:current mean train loss 7271.605215182353
INFO:root:current train perplexity312.5647277832031
INFO:root:current mean train loss 7271.701851331378
INFO:root:current train perplexity312.53057861328125
INFO:root:current mean train loss 7276.274709889806
INFO:root:current train perplexity312.78955078125
INFO:root:current mean train loss 7280.389610781648
INFO:root:current train perplexity313.19775390625
INFO:root:current mean train loss 7282.093290974667
INFO:root:current train perplexity313.3865661621094
INFO:root:current mean train loss 7283.981427371131
INFO:root:current train perplexity313.63232421875
INFO:root:current mean train loss 7282.396150494081
INFO:root:current train perplexity313.65655517578125
INFO:root:current mean train loss 7286.151152433024
INFO:root:current train perplexity313.77069091796875

100%|██████████| 1/1 [08:22<00:00, 502.20s/it][A100%|██████████| 1/1 [08:22<00:00, 502.20s/it]
INFO:root:final mean train loss: 7284.393931794275
INFO:root:final train perplexity: 313.8226318359375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.24s/it][A100%|██████████| 1/1 [00:45<00:00, 45.24s/it]
INFO:root:eval mean loss: 7075.797506995235
INFO:root:eval perplexity: 306.690673828125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.26s/it][A100%|██████████| 1/1 [00:43<00:00, 43.26s/it]
INFO:root:eval mean loss: 7139.674817673704
INFO:root:eval perplexity: 354.1538391113281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/111
 56%|█████▌    | 111/200 [16:53:06<14:54:47, 603.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7382.925696084666
INFO:root:current train perplexity320.3721008300781
INFO:root:current mean train loss 7310.038272324428
INFO:root:current train perplexity314.2276611328125
INFO:root:current mean train loss 7307.699626789226
INFO:root:current train perplexity314.4324035644531
INFO:root:current mean train loss 7291.028970510848
INFO:root:current train perplexity313.9398193359375
INFO:root:current mean train loss 7293.526000474216
INFO:root:current train perplexity313.6680908203125
INFO:root:current mean train loss 7288.1842228495625
INFO:root:current train perplexity313.9598693847656
INFO:root:current mean train loss 7296.846895356915
INFO:root:current train perplexity315.1338195800781
INFO:root:current mean train loss 7296.236477839734
INFO:root:current train perplexity315.6949768066406
INFO:root:current mean train loss 7304.025534464059
INFO:root:current train perplexity316.8724060058594
INFO:root:current mean train loss 7306.500793333228
INFO:root:current train perplexity317.6243896484375
INFO:root:current mean train loss 7303.92094429817
INFO:root:current train perplexity317.5547180175781
INFO:root:current mean train loss 7303.587229427962
INFO:root:current train perplexity318.04693603515625
INFO:root:current mean train loss 7308.4036308988625
INFO:root:current train perplexity318.3582763671875
INFO:root:current mean train loss 7304.253017761319
INFO:root:current train perplexity317.8730163574219
INFO:root:current mean train loss 7296.75913177942
INFO:root:current train perplexity317.6490783691406
INFO:root:current mean train loss 7296.533619672624
INFO:root:current train perplexity317.5978698730469
INFO:root:current mean train loss 7295.603640156992
INFO:root:current train perplexity317.3706359863281
INFO:root:current mean train loss 7300.166609983028
INFO:root:current train perplexity317.4835510253906
INFO:root:current mean train loss 7299.638654528848
INFO:root:current train perplexity317.4428405761719

100%|██████████| 1/1 [08:22<00:00, 502.89s/it][A100%|██████████| 1/1 [08:22<00:00, 502.89s/it]
INFO:root:final mean train loss: 7299.511924601298
INFO:root:final train perplexity: 317.5892333984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.09s/it][A100%|██████████| 1/1 [00:45<00:00, 45.09s/it]
INFO:root:eval mean loss: 7089.2944197418
INFO:root:eval perplexity: 310.05853271484375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.48s/it][A100%|██████████| 1/1 [00:43<00:00, 43.48s/it]
INFO:root:eval mean loss: 7156.4882180504765
INFO:root:eval perplexity: 359.0831604003906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/112
 56%|█████▌    | 112/200 [17:03:00<14:40:45, 600.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7104.152180989583
INFO:root:current train perplexity295.71337890625
INFO:root:current mean train loss 7250.583458168992
INFO:root:current train perplexity317.0599670410156
INFO:root:current mean train loss 7272.025691290794
INFO:root:current train perplexity316.51116943359375
INFO:root:current mean train loss 7311.013085292904
INFO:root:current train perplexity317.5421447753906
INFO:root:current mean train loss 7304.228910611818
INFO:root:current train perplexity317.6683044433594
INFO:root:current mean train loss 7306.6214917526095
INFO:root:current train perplexity318.0020446777344
INFO:root:current mean train loss 7306.98373043636
INFO:root:current train perplexity317.895751953125
INFO:root:current mean train loss 7297.68279430232
INFO:root:current train perplexity316.3880310058594
INFO:root:current mean train loss 7305.429732497276
INFO:root:current train perplexity317.09185791015625
INFO:root:current mean train loss 7301.958068374515
INFO:root:current train perplexity317.5865173339844
INFO:root:current mean train loss 7303.466653749689
INFO:root:current train perplexity318.26251220703125
INFO:root:current mean train loss 7302.368990554879
INFO:root:current train perplexity318.7168884277344
INFO:root:current mean train loss 7302.827428499065
INFO:root:current train perplexity318.6921081542969
INFO:root:current mean train loss 7301.199728016476
INFO:root:current train perplexity318.86175537109375
INFO:root:current mean train loss 7303.212233898899
INFO:root:current train perplexity319.0190124511719
INFO:root:current mean train loss 7303.537633067199
INFO:root:current train perplexity319.21148681640625
INFO:root:current mean train loss 7305.924271324762
INFO:root:current train perplexity319.4493408203125
INFO:root:current mean train loss 7306.391333767616
INFO:root:current train perplexity319.7352294921875
INFO:root:current mean train loss 7308.23675141041
INFO:root:current train perplexity320.2467041015625
INFO:root:current mean train loss 7311.95463192369
INFO:root:current train perplexity320.6874084472656

100%|██████████| 1/1 [08:20<00:00, 500.33s/it][A100%|██████████| 1/1 [08:20<00:00, 500.34s/it]
INFO:root:final mean train loss: 7312.448672357618
INFO:root:final train perplexity: 320.8482971191406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.31s/it][A100%|██████████| 1/1 [00:45<00:00, 45.31s/it]
INFO:root:eval mean loss: 7133.683263034685
INFO:root:eval perplexity: 321.3981628417969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.67s/it][A100%|██████████| 1/1 [00:42<00:00, 42.67s/it]
INFO:root:eval mean loss: 7202.915754169437
INFO:root:eval perplexity: 373.0540771484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/113
 56%|█████▋    | 113/200 [17:12:51<14:26:36, 597.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7410.467016601562
INFO:root:current train perplexity321.6341247558594
INFO:root:current mean train loss 7334.264538574219
INFO:root:current train perplexity317.5421447753906
INFO:root:current mean train loss 7313.1988747336645
INFO:root:current train perplexity318.8027648925781
INFO:root:current mean train loss 7327.551861572266
INFO:root:current train perplexity319.7956237792969
INFO:root:current mean train loss 7342.086632719494
INFO:root:current train perplexity320.3428955078125
INFO:root:current mean train loss 7335.062088716947
INFO:root:current train perplexity319.9995422363281
INFO:root:current mean train loss 7329.865968371976
INFO:root:current train perplexity319.392822265625
INFO:root:current mean train loss 7322.630055745442
INFO:root:current train perplexity318.8362121582031
INFO:root:current mean train loss 7319.041324671303
INFO:root:current train perplexity319.3443908691406
INFO:root:current mean train loss 7318.44623439623
INFO:root:current train perplexity320.1221008300781
INFO:root:current mean train loss 7308.626442344516
INFO:root:current train perplexity319.5518798828125
INFO:root:current mean train loss 7304.926152256557
INFO:root:current train perplexity318.9072265625
INFO:root:current mean train loss 7297.1531306032275
INFO:root:current train perplexity318.4645690917969
INFO:root:current mean train loss 7299.6522882634945
INFO:root:current train perplexity318.8039855957031
INFO:root:current mean train loss 7304.316632509903
INFO:root:current train perplexity319.17938232421875
INFO:root:current mean train loss 7305.233293714022
INFO:root:current train perplexity319.4781188964844
INFO:root:current mean train loss 7308.170549105421
INFO:root:current train perplexity319.9270935058594
INFO:root:current mean train loss 7308.588551507994
INFO:root:current train perplexity319.95135498046875
INFO:root:current mean train loss 7307.499706494677
INFO:root:current train perplexity319.624267578125
INFO:root:current mean train loss 7307.414743550618
INFO:root:current train perplexity319.62548828125

100%|██████████| 1/1 [08:21<00:00, 501.23s/it][A100%|██████████| 1/1 [08:21<00:00, 501.23s/it]
INFO:root:final mean train loss: 7308.089875267902
INFO:root:final train perplexity: 319.7466735839844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.41s/it][A100%|██████████| 1/1 [00:45<00:00, 45.41s/it]
INFO:root:eval mean loss: 7112.134318899601
INFO:root:eval perplexity: 315.842529296875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.11s/it][A100%|██████████| 1/1 [00:43<00:00, 43.11s/it]
INFO:root:eval mean loss: 7180.121891968639
INFO:root:eval perplexity: 366.12823486328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/114
 57%|█████▋    | 114/200 [17:22:44<14:14:23, 596.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7256.735998205237
INFO:root:current train perplexity320.24639892578125
INFO:root:current mean train loss 7339.600610886177
INFO:root:current train perplexity327.2622985839844
INFO:root:current mean train loss 7326.053513152689
INFO:root:current train perplexity327.63702392578125
INFO:root:current mean train loss 7316.245367848201
INFO:root:current train perplexity327.0677490234375
INFO:root:current mean train loss 7328.583719563429
INFO:root:current train perplexity326.39593505859375
INFO:root:current mean train loss 7331.63065115078
INFO:root:current train perplexity325.47882080078125
INFO:root:current mean train loss 7320.1164393642075
INFO:root:current train perplexity324.9698486328125
INFO:root:current mean train loss 7315.456017586075
INFO:root:current train perplexity324.6735534667969
INFO:root:current mean train loss 7316.3143592023225
INFO:root:current train perplexity324.3076171875
INFO:root:current mean train loss 7320.648488047792
INFO:root:current train perplexity324.11065673828125
INFO:root:current mean train loss 7318.6890378269645
INFO:root:current train perplexity324.3083801269531
INFO:root:current mean train loss 7314.9919435740985
INFO:root:current train perplexity324.09442138671875
INFO:root:current mean train loss 7318.326818837788
INFO:root:current train perplexity324.3083801269531
INFO:root:current mean train loss 7318.67791259583
INFO:root:current train perplexity324.27484130859375
INFO:root:current mean train loss 7317.02733729395
INFO:root:current train perplexity323.9057922363281
INFO:root:current mean train loss 7320.519130332019
INFO:root:current train perplexity323.6073913574219
INFO:root:current mean train loss 7324.039291279303
INFO:root:current train perplexity324.164306640625
INFO:root:current mean train loss 7325.69351904325
INFO:root:current train perplexity324.1448059082031
INFO:root:current mean train loss 7327.892924732921
INFO:root:current train perplexity324.4516296386719
INFO:root:current mean train loss 7328.628614339991
INFO:root:current train perplexity324.69677734375

100%|██████████| 1/1 [08:18<00:00, 498.42s/it][A100%|██████████| 1/1 [08:18<00:00, 498.42s/it]
INFO:root:final mean train loss: 7328.245422763411
INFO:root:final train perplexity: 324.8731689453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.38s/it][A100%|██████████| 1/1 [00:44<00:00, 44.38s/it]
INFO:root:eval mean loss: 7130.39791285738
INFO:root:eval perplexity: 320.5450744628906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.16s/it][A100%|██████████| 1/1 [00:43<00:00, 43.16s/it]
INFO:root:eval mean loss: 7196.518113156582
INFO:root:eval perplexity: 371.0969543457031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/115
 57%|█████▊    | 115/200 [17:32:32<14:01:11, 593.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7307.151891637732
INFO:root:current train perplexity322.1885681152344
INFO:root:current mean train loss 7301.511360465706
INFO:root:current train perplexity316.0597839355469
INFO:root:current mean train loss 7299.877266470841
INFO:root:current train perplexity315.2189025878906
INFO:root:current mean train loss 7301.400418211511
INFO:root:current train perplexity317.1850280761719
INFO:root:current mean train loss 7297.509089129612
INFO:root:current train perplexity318.7176513671875
INFO:root:current mean train loss 7310.991687760887
INFO:root:current train perplexity319.5401306152344
INFO:root:current mean train loss 7307.024892637854
INFO:root:current train perplexity320.1159973144531
INFO:root:current mean train loss 7310.449531534939
INFO:root:current train perplexity320.4169921875
INFO:root:current mean train loss 7319.441518314549
INFO:root:current train perplexity322.0827331542969
INFO:root:current mean train loss 7317.639880806145
INFO:root:current train perplexity322.7177734375
INFO:root:current mean train loss 7318.671815238822
INFO:root:current train perplexity323.2427978515625
INFO:root:current mean train loss 7319.949897858671
INFO:root:current train perplexity323.2514343261719
INFO:root:current mean train loss 7318.109325548869
INFO:root:current train perplexity322.68963623046875
INFO:root:current mean train loss 7318.636203422152
INFO:root:current train perplexity322.62640380859375
INFO:root:current mean train loss 7318.028434826879
INFO:root:current train perplexity322.4958190917969
INFO:root:current mean train loss 7319.203036392978
INFO:root:current train perplexity322.42431640625
INFO:root:current mean train loss 7320.046700824705
INFO:root:current train perplexity322.6117858886719
INFO:root:current mean train loss 7322.816397620171
INFO:root:current train perplexity322.9642639160156
INFO:root:current mean train loss 7325.752490392395
INFO:root:current train perplexity323.3013916015625
INFO:root:current mean train loss 7326.635860384547
INFO:root:current train perplexity323.6121520996094

100%|██████████| 1/1 [08:12<00:00, 492.87s/it][A100%|██████████| 1/1 [08:12<00:00, 492.87s/it]
INFO:root:final mean train loss: 7323.792003021779
INFO:root:final train perplexity: 323.733642578125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.87s/it][A100%|██████████| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 7157.080955992354
INFO:root:eval perplexity: 327.541748046875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.05s/it][A100%|██████████| 1/1 [00:42<00:00, 42.05s/it]
INFO:root:eval mean loss: 7234.66898167387
INFO:root:eval perplexity: 382.9210205078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/116
 58%|█████▊    | 116/200 [17:42:13<13:46:02, 590.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7329.761299240757
INFO:root:current train perplexity325.02734375
INFO:root:current mean train loss 7326.415455957603
INFO:root:current train perplexity322.9005126953125
INFO:root:current mean train loss 7334.222515711485
INFO:root:current train perplexity321.7109680175781
INFO:root:current mean train loss 7347.485409471867
INFO:root:current train perplexity321.4922790527344
INFO:root:current mean train loss 7324.610904118565
INFO:root:current train perplexity319.0039367675781
INFO:root:current mean train loss 7329.793900845556
INFO:root:current train perplexity319.33343505859375
INFO:root:current mean train loss 7321.448050804536
INFO:root:current train perplexity319.2999572753906
INFO:root:current mean train loss 7318.1326864715065
INFO:root:current train perplexity319.7669372558594
INFO:root:current mean train loss 7332.39305911847
INFO:root:current train perplexity321.046630859375
INFO:root:current mean train loss 7338.730140379602
INFO:root:current train perplexity322.5854797363281
INFO:root:current mean train loss 7332.886896099586
INFO:root:current train perplexity322.4170837402344
INFO:root:current mean train loss 7336.16612654115
INFO:root:current train perplexity322.12371826171875
INFO:root:current mean train loss 7337.436413948785
INFO:root:current train perplexity322.6834716796875
INFO:root:current mean train loss 7339.665670515933
INFO:root:current train perplexity323.2388000488281
INFO:root:current mean train loss 7331.844173885218
INFO:root:current train perplexity323.1819152832031
INFO:root:current mean train loss 7326.86371109912
INFO:root:current train perplexity322.9510192871094
INFO:root:current mean train loss 7324.22102367828
INFO:root:current train perplexity322.7553405761719
INFO:root:current mean train loss 7325.1636199865015
INFO:root:current train perplexity322.6953125
INFO:root:current mean train loss 7326.186278905415
INFO:root:current train perplexity323.168212890625
INFO:root:current mean train loss 7326.346341036831
INFO:root:current train perplexity323.75494384765625

100%|██████████| 1/1 [08:18<00:00, 498.13s/it][A100%|██████████| 1/1 [08:18<00:00, 498.13s/it]
INFO:root:final mean train loss: 7323.8627930918665
INFO:root:final train perplexity: 323.75152587890625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.20s/it][A100%|██████████| 1/1 [00:44<00:00, 44.20s/it]
INFO:root:eval mean loss: 7115.671625664893
INFO:root:eval perplexity: 316.7480773925781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.79s/it][A100%|██████████| 1/1 [00:42<00:00, 42.79s/it]
INFO:root:eval mean loss: 7185.40212904477
INFO:root:eval perplexity: 367.7210998535156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/117
 58%|█████▊    | 117/200 [17:52:01<13:35:20, 589.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7349.432933460583
INFO:root:current train perplexity326.8472900390625
INFO:root:current mean train loss 7357.712355593418
INFO:root:current train perplexity327.3006896972656
INFO:root:current mean train loss 7325.943778143988
INFO:root:current train perplexity326.1378479003906
INFO:root:current mean train loss 7324.704039898115
INFO:root:current train perplexity323.1319885253906
INFO:root:current mean train loss 7314.4042728611685
INFO:root:current train perplexity321.0960998535156
INFO:root:current mean train loss 7321.276984182345
INFO:root:current train perplexity320.7932434082031
INFO:root:current mean train loss 7321.719426354696
INFO:root:current train perplexity320.4128723144531
INFO:root:current mean train loss 7313.002968105568
INFO:root:current train perplexity319.8729248046875
INFO:root:current mean train loss 7308.4924156945035
INFO:root:current train perplexity319.7335510253906
INFO:root:current mean train loss 7314.895101076196
INFO:root:current train perplexity319.7138977050781
INFO:root:current mean train loss 7318.741430843577
INFO:root:current train perplexity320.3102111816406
INFO:root:current mean train loss 7318.81781067511
INFO:root:current train perplexity320.6051330566406
INFO:root:current mean train loss 7322.053994504561
INFO:root:current train perplexity321.4378662109375
INFO:root:current mean train loss 7317.105738219336
INFO:root:current train perplexity321.4446105957031
INFO:root:current mean train loss 7311.64105749643
INFO:root:current train perplexity321.2008361816406
INFO:root:current mean train loss 7316.9563821557185
INFO:root:current train perplexity321.5989990234375
INFO:root:current mean train loss 7320.946193315406
INFO:root:current train perplexity322.36297607421875
INFO:root:current mean train loss 7325.9159787581275
INFO:root:current train perplexity322.7674865722656
INFO:root:current mean train loss 7324.077995688228
INFO:root:current train perplexity322.7447204589844

100%|██████████| 1/1 [08:18<00:00, 498.90s/it][A100%|██████████| 1/1 [08:18<00:00, 498.90s/it]
INFO:root:final mean train loss: 7318.921412203928
INFO:root:final train perplexity: 322.49151611328125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.71s/it][A100%|██████████| 1/1 [00:44<00:00, 44.71s/it]
INFO:root:eval mean loss: 7139.730008172651
INFO:root:eval perplexity: 322.9748840332031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.93s/it][A100%|██████████| 1/1 [00:42<00:00, 42.93s/it]
INFO:root:eval mean loss: 7214.099336664727
INFO:root:eval perplexity: 376.50006103515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/118
 59%|█████▉    | 118/200 [18:01:51<13:25:28, 589.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7256.15546875
INFO:root:current train perplexity312.9629211425781
INFO:root:current mean train loss 7276.121493675595
INFO:root:current train perplexity328.03955078125
INFO:root:current mean train loss 7307.235034775153
INFO:root:current train perplexity323.2012023925781
INFO:root:current mean train loss 7316.944814613217
INFO:root:current train perplexity321.9907531738281
INFO:root:current mean train loss 7310.424858940973
INFO:root:current train perplexity320.82763671875
INFO:root:current mean train loss 7296.775397393255
INFO:root:current train perplexity320.6184387207031
INFO:root:current mean train loss 7304.298495609504
INFO:root:current train perplexity320.6302185058594
INFO:root:current mean train loss 7307.766404864805
INFO:root:current train perplexity321.6922607421875
INFO:root:current mean train loss 7305.870270040761
INFO:root:current train perplexity322.6454772949219
INFO:root:current mean train loss 7306.7762921918165
INFO:root:current train perplexity322.53717041015625
INFO:root:current mean train loss 7312.920052569185
INFO:root:current train perplexity322.5906982421875
INFO:root:current mean train loss 7309.759062588377
INFO:root:current train perplexity322.3998718261719
INFO:root:current mean train loss 7309.511734958506
INFO:root:current train perplexity322.3934020996094
INFO:root:current mean train loss 7316.241188861949
INFO:root:current train perplexity323.386962890625
INFO:root:current mean train loss 7320.3160371719305
INFO:root:current train perplexity324.0785217285156
INFO:root:current mean train loss 7316.580809086898
INFO:root:current train perplexity323.9624938964844
INFO:root:current mean train loss 7319.082026078174
INFO:root:current train perplexity324.0103759765625
INFO:root:current mean train loss 7325.4088589397
INFO:root:current train perplexity324.2618408203125
INFO:root:current mean train loss 7320.420263266101
INFO:root:current train perplexity324.2779235839844
INFO:root:current mean train loss 7327.423237317504
INFO:root:current train perplexity324.5703125

100%|██████████| 1/1 [08:23<00:00, 503.94s/it][A100%|██████████| 1/1 [08:23<00:00, 503.94s/it]
INFO:root:final mean train loss: 7325.906654438705
INFO:root:final train perplexity: 324.27423095703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.24s/it][A100%|██████████| 1/1 [00:44<00:00, 44.24s/it]
INFO:root:eval mean loss: 7143.718885056516
INFO:root:eval perplexity: 324.0191955566406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.76s/it][A100%|██████████| 1/1 [00:42<00:00, 42.76s/it]
INFO:root:eval mean loss: 7211.626356625387
INFO:root:eval perplexity: 375.7353210449219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/119
 60%|█████▉    | 119/200 [18:11:44<13:17:23, 590.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7329.678111683239
INFO:root:current train perplexity330.65985107421875
INFO:root:current mean train loss 7324.862028528432
INFO:root:current train perplexity325.6402587890625
INFO:root:current mean train loss 7327.04127076295
INFO:root:current train perplexity321.1351318359375
INFO:root:current mean train loss 7329.153467403436
INFO:root:current train perplexity319.860107421875
INFO:root:current mean train loss 7339.536290173282
INFO:root:current train perplexity319.4688415527344
INFO:root:current mean train loss 7326.039967036339
INFO:root:current train perplexity319.7986755371094
INFO:root:current mean train loss 7334.565698948703
INFO:root:current train perplexity321.2137145996094
INFO:root:current mean train loss 7336.0933645148025
INFO:root:current train perplexity321.83123779296875
INFO:root:current mean train loss 7338.895092595233
INFO:root:current train perplexity322.6922302246094
INFO:root:current mean train loss 7336.96961323041
INFO:root:current train perplexity322.78887939453125
INFO:root:current mean train loss 7329.128243104819
INFO:root:current train perplexity322.6736145019531
INFO:root:current mean train loss 7322.6567287071075
INFO:root:current train perplexity322.0824279785156
INFO:root:current mean train loss 7323.981987136865
INFO:root:current train perplexity322.4486083984375
INFO:root:current mean train loss 7320.326382404737
INFO:root:current train perplexity321.8421630859375
INFO:root:current mean train loss 7316.52834331872
INFO:root:current train perplexity321.3148193359375
INFO:root:current mean train loss 7314.322174834202
INFO:root:current train perplexity321.2660827636719
INFO:root:current mean train loss 7311.702652071613
INFO:root:current train perplexity321.3760986328125
INFO:root:current mean train loss 7312.134941996044
INFO:root:current train perplexity321.3240051269531
INFO:root:current mean train loss 7310.6113656438665
INFO:root:current train perplexity321.0789489746094
INFO:root:current mean train loss 7312.357069255658
INFO:root:current train perplexity321.1068115234375

100%|██████████| 1/1 [08:11<00:00, 491.88s/it][A100%|██████████| 1/1 [08:11<00:00, 491.88s/it]
INFO:root:final mean train loss: 7314.657183225385
INFO:root:final train perplexity: 321.40814208984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.14s/it][A100%|██████████| 1/1 [00:44<00:00, 44.14s/it]
INFO:root:eval mean loss: 7129.21115220523
INFO:root:eval perplexity: 320.23736572265625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.06s/it][A100%|██████████| 1/1 [00:43<00:00, 43.07s/it]
INFO:root:eval mean loss: 7200.195627631871
INFO:root:eval perplexity: 372.22088623046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/120
 60%|██████    | 120/200 [18:21:26<13:03:56, 587.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7335.590494791667
INFO:root:current train perplexity326.4280090332031
INFO:root:current mean train loss 7257.680962651753
INFO:root:current train perplexity317.514892578125
INFO:root:current mean train loss 7281.298474682923
INFO:root:current train perplexity320.1376647949219
INFO:root:current mean train loss 7276.207580026272
INFO:root:current train perplexity320.08807373046875
INFO:root:current mean train loss 7290.419983049189
INFO:root:current train perplexity321.4643859863281
INFO:root:current mean train loss 7314.861532858882
INFO:root:current train perplexity321.6772155761719
INFO:root:current mean train loss 7311.0910335057215
INFO:root:current train perplexity321.4197692871094
INFO:root:current mean train loss 7314.595425617388
INFO:root:current train perplexity322.42462158203125
INFO:root:current mean train loss 7320.590054426773
INFO:root:current train perplexity322.7496337890625
INFO:root:current mean train loss 7318.809892193324
INFO:root:current train perplexity322.467529296875
INFO:root:current mean train loss 7320.748838275987
INFO:root:current train perplexity322.6676330566406
INFO:root:current mean train loss 7323.369999725636
INFO:root:current train perplexity323.1024169921875
INFO:root:current mean train loss 7325.0850980818705
INFO:root:current train perplexity323.3379211425781
INFO:root:current mean train loss 7326.376634046514
INFO:root:current train perplexity323.3528747558594
INFO:root:current mean train loss 7322.642780698945
INFO:root:current train perplexity323.2458801269531
INFO:root:current mean train loss 7315.558427182322
INFO:root:current train perplexity322.7193298339844
INFO:root:current mean train loss 7313.501843194688
INFO:root:current train perplexity322.26507568359375
INFO:root:current mean train loss 7315.038292874407
INFO:root:current train perplexity322.125732421875
INFO:root:current mean train loss 7315.96373283714
INFO:root:current train perplexity321.97601318359375
INFO:root:current mean train loss 7316.466182934986
INFO:root:current train perplexity321.5780029296875

100%|██████████| 1/1 [08:20<00:00, 500.35s/it][A100%|██████████| 1/1 [08:20<00:00, 500.35s/it]
INFO:root:final mean train loss: 7315.345261628136
INFO:root:final train perplexity: 321.5827331542969
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.20s/it][A100%|██████████| 1/1 [00:45<00:00, 45.20s/it]
INFO:root:eval mean loss: 7109.632315561281
INFO:root:eval perplexity: 315.203857421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.19s/it][A100%|██████████| 1/1 [00:43<00:00, 43.19s/it]
INFO:root:eval mean loss: 7179.410317278923
INFO:root:eval perplexity: 365.9140930175781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/121
 60%|██████    | 121/200 [18:31:17<12:55:27, 588.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7189.436985560826
INFO:root:current train perplexity318.4309997558594
INFO:root:current mean train loss 7258.85879281851
INFO:root:current train perplexity323.1281433105469
INFO:root:current mean train loss 7274.73556137085
INFO:root:current train perplexity323.20135498046875
INFO:root:current mean train loss 7274.800583742977
INFO:root:current train perplexity322.27813720703125
INFO:root:current mean train loss 7274.859631990132
INFO:root:current train perplexity322.5284118652344
INFO:root:current mean train loss 7294.021474714759
INFO:root:current train perplexity322.1681213378906
INFO:root:current mean train loss 7300.126056205936
INFO:root:current train perplexity322.000244140625
INFO:root:current mean train loss 7305.1519458912035
INFO:root:current train perplexity321.79412841796875
INFO:root:current mean train loss 7305.921627436843
INFO:root:current train perplexity321.29058837890625
INFO:root:current mean train loss 7306.608228867024
INFO:root:current train perplexity321.0596618652344
INFO:root:current mean train loss 7299.383423776338
INFO:root:current train perplexity320.533447265625
INFO:root:current mean train loss 7292.216403208802
INFO:root:current train perplexity319.5345153808594
INFO:root:current mean train loss 7294.149260502712
INFO:root:current train perplexity319.3424377441406
INFO:root:current mean train loss 7292.983495661642
INFO:root:current train perplexity319.3948059082031
INFO:root:current mean train loss 7300.257498940268
INFO:root:current train perplexity319.76556396484375
INFO:root:current mean train loss 7308.146555295028
INFO:root:current train perplexity320.1390380859375
INFO:root:current mean train loss 7306.799129762511
INFO:root:current train perplexity319.7603759765625
INFO:root:current mean train loss 7310.486203830172
INFO:root:current train perplexity319.9693298339844
INFO:root:current mean train loss 7306.7043543848495
INFO:root:current train perplexity319.8645324707031
INFO:root:current mean train loss 7307.689899218351
INFO:root:current train perplexity319.8027648925781

100%|██████████| 1/1 [08:18<00:00, 498.76s/it][A100%|██████████| 1/1 [08:18<00:00, 498.77s/it]
INFO:root:final mean train loss: 7308.582146241096
INFO:root:final train perplexity: 319.87078857421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.56s/it][A100%|██████████| 1/1 [00:45<00:00, 45.56s/it]
INFO:root:eval mean loss: 7138.4084957474515
INFO:root:eval perplexity: 322.6297912597656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.41s/it][A100%|██████████| 1/1 [00:42<00:00, 42.41s/it]
INFO:root:eval mean loss: 7214.0906462973735
INFO:root:eval perplexity: 376.49737548828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/122
 61%|██████    | 122/200 [18:41:06<12:45:46, 589.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7304.369582084761
INFO:root:current train perplexity318.3921203613281
INFO:root:current mean train loss 7271.575680771315
INFO:root:current train perplexity314.7162170410156
INFO:root:current mean train loss 7291.022439474588
INFO:root:current train perplexity317.5760803222656
INFO:root:current mean train loss 7286.592874235506
INFO:root:current train perplexity316.67724609375
INFO:root:current mean train loss 7288.6071565720795
INFO:root:current train perplexity317.1537170410156
INFO:root:current mean train loss 7300.430909481348
INFO:root:current train perplexity318.2758483886719
INFO:root:current mean train loss 7294.234340174591
INFO:root:current train perplexity318.24322509765625
INFO:root:current mean train loss 7292.9752385187585
INFO:root:current train perplexity318.28662109375
INFO:root:current mean train loss 7293.147931880011
INFO:root:current train perplexity318.2010498046875
INFO:root:current mean train loss 7290.447000156571
INFO:root:current train perplexity317.9095458984375
INFO:root:current mean train loss 7289.514176538472
INFO:root:current train perplexity317.7757263183594
INFO:root:current mean train loss 7287.966856817456
INFO:root:current train perplexity316.9988708496094
INFO:root:current mean train loss 7292.987545184235
INFO:root:current train perplexity317.6113586425781
INFO:root:current mean train loss 7298.502656918586
INFO:root:current train perplexity318.242919921875
INFO:root:current mean train loss 7305.023278385947
INFO:root:current train perplexity318.9609069824219
INFO:root:current mean train loss 7306.912113099968
INFO:root:current train perplexity319.0783386230469
INFO:root:current mean train loss 7305.81629563518
INFO:root:current train perplexity318.78375244140625
INFO:root:current mean train loss 7304.838356323587
INFO:root:current train perplexity318.6914978027344
INFO:root:current mean train loss 7305.719885325598
INFO:root:current train perplexity318.818115234375
INFO:root:current mean train loss 7307.142695183809
INFO:root:current train perplexity318.9989318847656

100%|██████████| 1/1 [08:17<00:00, 497.77s/it][A100%|██████████| 1/1 [08:17<00:00, 497.77s/it]
INFO:root:final mean train loss: 7304.907400034079
INFO:root:final train perplexity: 318.9444885253906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.69s/it][A100%|██████████| 1/1 [00:44<00:00, 44.69s/it]
INFO:root:eval mean loss: 7097.249781831782
INFO:root:eval perplexity: 312.06121826171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.10s/it][A100%|██████████| 1/1 [00:46<00:00, 46.10s/it]
INFO:root:eval mean loss: 7163.566762937721
INFO:root:eval perplexity: 361.1790466308594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/123
 62%|██████▏   | 123/200 [18:50:58<12:36:44, 589.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7338.816324869792
INFO:root:current train perplexity318.9479675292969
INFO:root:current mean train loss 7314.639478824013
INFO:root:current train perplexity320.0647277832031
INFO:root:current mean train loss 7316.655254916487
INFO:root:current train perplexity320.2204284667969
INFO:root:current mean train loss 7308.149674479167
INFO:root:current train perplexity319.55523681640625
INFO:root:current mean train loss 7330.573124601403
INFO:root:current train perplexity320.2854919433594
INFO:root:current mean train loss 7320.520137049788
INFO:root:current train perplexity320.3609313964844
INFO:root:current mean train loss 7319.204367640398
INFO:root:current train perplexity319.5553894042969
INFO:root:current mean train loss 7320.013275069225
INFO:root:current train perplexity319.4574279785156
INFO:root:current mean train loss 7320.447480688203
INFO:root:current train perplexity319.5806579589844
INFO:root:current mean train loss 7318.86119988952
INFO:root:current train perplexity319.08868408203125
INFO:root:current mean train loss 7317.922819757024
INFO:root:current train perplexity319.43701171875
INFO:root:current mean train loss 7313.48643727022
INFO:root:current train perplexity319.5335998535156
INFO:root:current mean train loss 7320.779513762718
INFO:root:current train perplexity319.8067321777344
INFO:root:current mean train loss 7318.567371571493
INFO:root:current train perplexity320.14239501953125
INFO:root:current mean train loss 7316.747551056523
INFO:root:current train perplexity319.990234375
INFO:root:current mean train loss 7314.4478263807
INFO:root:current train perplexity319.58355712890625
INFO:root:current mean train loss 7312.084560778014
INFO:root:current train perplexity319.6385803222656
INFO:root:current mean train loss 7314.564016399703
INFO:root:current train perplexity319.6330871582031
INFO:root:current mean train loss 7310.359300078539
INFO:root:current train perplexity319.6112976074219

100%|██████████| 1/1 [08:22<00:00, 502.56s/it][A100%|██████████| 1/1 [08:22<00:00, 502.56s/it]
INFO:root:final mean train loss: 7306.814490429392
INFO:root:final train perplexity: 319.4248046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.68s/it][A100%|██████████| 1/1 [00:44<00:00, 44.68s/it]
INFO:root:eval mean loss: 7109.788312763187
INFO:root:eval perplexity: 315.2435302734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.46s/it][A100%|██████████| 1/1 [00:43<00:00, 43.46s/it]
INFO:root:eval mean loss: 7178.914328284298
INFO:root:eval perplexity: 365.76495361328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/124
 62%|██████▏   | 124/200 [19:00:52<12:28:46, 591.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7271.388950892857
INFO:root:current train perplexity311.1747131347656
INFO:root:current mean train loss 7329.872859776577
INFO:root:current train perplexity325.9677734375
INFO:root:current mean train loss 7330.946888209541
INFO:root:current train perplexity323.83984375
INFO:root:current mean train loss 7343.702930959894
INFO:root:current train perplexity323.9071960449219
INFO:root:current mean train loss 7335.304560330927
INFO:root:current train perplexity323.3687438964844
INFO:root:current mean train loss 7333.240177553316
INFO:root:current train perplexity322.7762756347656
INFO:root:current mean train loss 7325.300327558691
INFO:root:current train perplexity321.0674743652344
INFO:root:current mean train loss 7324.689015260343
INFO:root:current train perplexity321.05657958984375
INFO:root:current mean train loss 7321.770990043177
INFO:root:current train perplexity320.5806884765625
INFO:root:current mean train loss 7317.255820613975
INFO:root:current train perplexity319.9335021972656
INFO:root:current mean train loss 7313.550557232187
INFO:root:current train perplexity320.0117492675781
INFO:root:current mean train loss 7316.609849166525
INFO:root:current train perplexity319.91900634765625
INFO:root:current mean train loss 7319.911479504323
INFO:root:current train perplexity320.232177734375
INFO:root:current mean train loss 7322.777100169759
INFO:root:current train perplexity320.1741638183594
INFO:root:current mean train loss 7320.16771437178
INFO:root:current train perplexity320.11065673828125
INFO:root:current mean train loss 7316.200223501265
INFO:root:current train perplexity319.8160400390625
INFO:root:current mean train loss 7310.135415552563
INFO:root:current train perplexity319.70550537109375
INFO:root:current mean train loss 7312.046848969775
INFO:root:current train perplexity319.75323486328125
INFO:root:current mean train loss 7309.332535203808
INFO:root:current train perplexity319.56634521484375
INFO:root:current mean train loss 7309.318711951445
INFO:root:current train perplexity319.6238098144531

100%|██████████| 1/1 [08:22<00:00, 502.40s/it][A100%|██████████| 1/1 [08:22<00:00, 502.40s/it]
INFO:root:final mean train loss: 7307.455302197585
INFO:root:final train perplexity: 319.5861511230469
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.68s/it][A100%|██████████| 1/1 [00:46<00:00, 46.68s/it]
INFO:root:eval mean loss: 7100.889937596964
INFO:root:eval perplexity: 312.9817199707031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.19s/it][A100%|██████████| 1/1 [00:43<00:00, 43.19s/it]
INFO:root:eval mean loss: 7171.281915759364
INFO:root:eval perplexity: 363.47723388671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/125
 62%|██████▎   | 125/200 [19:10:47<12:20:21, 592.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7535.6904296875
INFO:root:current train perplexity333.83697509765625
INFO:root:current mean train loss 7367.790806924143
INFO:root:current train perplexity325.0330810546875
INFO:root:current mean train loss 7348.116221836635
INFO:root:current train perplexity322.80352783203125
INFO:root:current mean train loss 7333.018804856289
INFO:root:current train perplexity320.7059020996094
INFO:root:current mean train loss 7317.396459039652
INFO:root:current train perplexity319.5223083496094
INFO:root:current mean train loss 7304.954400681358
INFO:root:current train perplexity318.0725402832031
INFO:root:current mean train loss 7312.8607670710635
INFO:root:current train perplexity318.77752685546875
INFO:root:current mean train loss 7316.496136912983
INFO:root:current train perplexity320.01861572265625
INFO:root:current mean train loss 7308.803142066141
INFO:root:current train perplexity320.2274475097656
INFO:root:current mean train loss 7311.85973804028
INFO:root:current train perplexity319.8961181640625
INFO:root:current mean train loss 7307.4996156692505
INFO:root:current train perplexity320.1410217285156
INFO:root:current mean train loss 7310.766573760009
INFO:root:current train perplexity320.2112731933594
INFO:root:current mean train loss 7315.794320299735
INFO:root:current train perplexity320.2816467285156
INFO:root:current mean train loss 7315.746330883568
INFO:root:current train perplexity320.5357360839844
INFO:root:current mean train loss 7318.066348643785
INFO:root:current train perplexity320.7262268066406
INFO:root:current mean train loss 7316.5637290333825
INFO:root:current train perplexity320.5728759765625
INFO:root:current mean train loss 7317.115634260506
INFO:root:current train perplexity320.5329895019531
INFO:root:current mean train loss 7319.087976725638
INFO:root:current train perplexity320.7618713378906
INFO:root:current mean train loss 7316.499135602984
INFO:root:current train perplexity320.8409729003906
INFO:root:current mean train loss 7314.444222495858
INFO:root:current train perplexity320.8579406738281

100%|██████████| 1/1 [08:21<00:00, 501.62s/it][A100%|██████████| 1/1 [08:21<00:00, 501.62s/it]
INFO:root:final mean train loss: 7312.306742688832
INFO:root:final train perplexity: 320.8123474121094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.83s/it][A100%|██████████| 1/1 [00:44<00:00, 44.83s/it]
INFO:root:eval mean loss: 7134.59393700133
INFO:root:eval perplexity: 321.63519287109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.61s/it][A100%|██████████| 1/1 [00:43<00:00, 43.61s/it]
INFO:root:eval mean loss: 7210.806148015015
INFO:root:eval perplexity: 375.4820861816406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/126
 63%|██████▎   | 126/200 [19:20:40<12:10:36, 592.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7343.830375857469
INFO:root:current train perplexity325.3779602050781
INFO:root:current mean train loss 7349.5043460494235
INFO:root:current train perplexity323.7300720214844
INFO:root:current mean train loss 7348.634526549533
INFO:root:current train perplexity323.8140563964844
INFO:root:current mean train loss 7337.889231751741
INFO:root:current train perplexity322.6813049316406
INFO:root:current mean train loss 7340.56925843254
INFO:root:current train perplexity323.66448974609375
INFO:root:current mean train loss 7341.292945283618
INFO:root:current train perplexity323.25836181640625
INFO:root:current mean train loss 7323.425965593311
INFO:root:current train perplexity322.9642639160156
INFO:root:current mean train loss 7324.192844735788
INFO:root:current train perplexity322.21881103515625
INFO:root:current mean train loss 7321.339821106755
INFO:root:current train perplexity322.14599609375
INFO:root:current mean train loss 7311.591844613443
INFO:root:current train perplexity321.4847717285156
INFO:root:current mean train loss 7317.5279267793885
INFO:root:current train perplexity321.6535949707031
INFO:root:current mean train loss 7319.054922867825
INFO:root:current train perplexity321.7109680175781
INFO:root:current mean train loss 7315.01605898406
INFO:root:current train perplexity321.2417297363281
INFO:root:current mean train loss 7311.484418694072
INFO:root:current train perplexity320.7291564941406
INFO:root:current mean train loss 7309.359449885605
INFO:root:current train perplexity320.6326599121094
INFO:root:current mean train loss 7312.2239319283335
INFO:root:current train perplexity320.6971740722656
INFO:root:current mean train loss 7317.453746881665
INFO:root:current train perplexity321.0555114746094
INFO:root:current mean train loss 7316.233279802826
INFO:root:current train perplexity321.0642395019531
INFO:root:current mean train loss 7313.906433801687
INFO:root:current train perplexity320.9535827636719
INFO:root:current mean train loss 7315.5831252918115
INFO:root:current train perplexity320.91790771484375

100%|██████████| 1/1 [08:16<00:00, 496.25s/it][A100%|██████████| 1/1 [08:16<00:00, 496.25s/it]
INFO:root:final mean train loss: 7312.889889746439
INFO:root:final train perplexity: 320.96002197265625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.80s/it][A100%|██████████| 1/1 [00:44<00:00, 44.81s/it]
INFO:root:eval mean loss: 7120.874961907137
INFO:root:eval perplexity: 318.08453369140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.46s/it][A100%|██████████| 1/1 [00:42<00:00, 42.46s/it]
INFO:root:eval mean loss: 7194.160691281582
INFO:root:eval perplexity: 370.3786926269531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/127
 64%|██████▎   | 127/200 [19:30:26<11:58:23, 590.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7377.417278421336
INFO:root:current train perplexity322.8116760253906
INFO:root:current mean train loss 7290.790171949169
INFO:root:current train perplexity319.2793884277344
INFO:root:current mean train loss 7324.107832561168
INFO:root:current train perplexity322.2804260253906
INFO:root:current mean train loss 7319.046704510737
INFO:root:current train perplexity321.495361328125
INFO:root:current mean train loss 7311.187922182042
INFO:root:current train perplexity321.7468566894531
INFO:root:current mean train loss 7318.136700373824
INFO:root:current train perplexity321.7055969238281
INFO:root:current mean train loss 7317.840331289181
INFO:root:current train perplexity321.8619384765625
INFO:root:current mean train loss 7317.233357210587
INFO:root:current train perplexity321.25
INFO:root:current mean train loss 7325.594494941907
INFO:root:current train perplexity321.6816711425781
INFO:root:current mean train loss 7320.190625917438
INFO:root:current train perplexity321.0146484375
INFO:root:current mean train loss 7317.782099646296
INFO:root:current train perplexity320.8229064941406
INFO:root:current mean train loss 7315.37546466834
INFO:root:current train perplexity320.7929382324219
INFO:root:current mean train loss 7308.736956525115
INFO:root:current train perplexity320.6120300292969
INFO:root:current mean train loss 7305.835759158689
INFO:root:current train perplexity320.2796630859375
INFO:root:current mean train loss 7304.962147151492
INFO:root:current train perplexity320.306396484375
INFO:root:current mean train loss 7307.47853850339
INFO:root:current train perplexity320.3913269042969
INFO:root:current mean train loss 7309.574273232528
INFO:root:current train perplexity320.47705078125
INFO:root:current mean train loss 7310.673410947276
INFO:root:current train perplexity320.4234313964844
INFO:root:current mean train loss 7307.647370534513
INFO:root:current train perplexity320.4248046875
INFO:root:current mean train loss 7313.545174245084
INFO:root:current train perplexity320.91729736328125

100%|██████████| 1/1 [08:13<00:00, 493.54s/it][A100%|██████████| 1/1 [08:13<00:00, 493.54s/it]
INFO:root:final mean train loss: 7312.577499689745
INFO:root:final train perplexity: 320.88104248046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.26s/it][A100%|██████████| 1/1 [00:44<00:00, 44.26s/it]
INFO:root:eval mean loss: 7144.514572251773
INFO:root:eval perplexity: 324.2276916503906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.99s/it][A100%|██████████| 1/1 [00:41<00:00, 41.99s/it]
INFO:root:eval mean loss: 7221.105222012135
INFO:root:eval perplexity: 378.6746826171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/128
 64%|██████▍   | 128/200 [19:40:08<11:45:36, 588.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7302.60763671875
INFO:root:current train perplexity327.9227294921875
INFO:root:current mean train loss 7333.622131696428
INFO:root:current train perplexity325.0752258300781
INFO:root:current mean train loss 7333.578432173295
INFO:root:current train perplexity321.6177062988281
INFO:root:current mean train loss 7325.001061197917
INFO:root:current train perplexity321.07281494140625
INFO:root:current mean train loss 7311.985803865132
INFO:root:current train perplexity320.4574890136719
INFO:root:current mean train loss 7308.524853940217
INFO:root:current train perplexity320.1298828125
INFO:root:current mean train loss 7307.560211226852
INFO:root:current train perplexity321.0545959472656
INFO:root:current mean train loss 7311.975921748992
INFO:root:current train perplexity320.5851135253906
INFO:root:current mean train loss 7319.202477120536
INFO:root:current train perplexity321.3054504394531
INFO:root:current mean train loss 7314.983341846955
INFO:root:current train perplexity321.6407165527344
INFO:root:current mean train loss 7313.829697038517
INFO:root:current train perplexity321.7264709472656
INFO:root:current mean train loss 7316.516574551197
INFO:root:current train perplexity322.16412353515625
INFO:root:current mean train loss 7315.734579120711
INFO:root:current train perplexity322.1885681152344
INFO:root:current mean train loss 7313.701766690341
INFO:root:current train perplexity321.8993835449219
INFO:root:current mean train loss 7322.244520987818
INFO:root:current train perplexity322.5125732421875
INFO:root:current mean train loss 7324.61064453125
INFO:root:current train perplexity322.71978759765625
INFO:root:current mean train loss 7325.659803229944
INFO:root:current train perplexity322.77520751953125
INFO:root:current mean train loss 7326.733214403609
INFO:root:current train perplexity323.12353515625
INFO:root:current mean train loss 7328.32322109375
INFO:root:current train perplexity323.3332824707031
INFO:root:current mean train loss 7323.305348348496
INFO:root:current train perplexity322.99505615234375

100%|██████████| 1/1 [08:16<00:00, 496.82s/it][A100%|██████████| 1/1 [08:16<00:00, 496.82s/it]
INFO:root:final mean train loss: 7321.0899641582355
INFO:root:final train perplexity: 323.0440368652344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.25s/it][A100%|██████████| 1/1 [00:47<00:00, 47.25s/it]
INFO:root:eval mean loss: 7136.2556083084
INFO:root:eval perplexity: 322.0681457519531
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.29s/it][A100%|██████████| 1/1 [00:43<00:00, 43.29s/it]
INFO:root:eval mean loss: 7209.623978418661
INFO:root:eval perplexity: 375.1171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/129
 64%|██████▍   | 129/200 [19:49:58<11:36:34, 588.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7297.9766208814535
INFO:root:current train perplexity320.1072998046875
INFO:root:current mean train loss 7320.16707611084
INFO:root:current train perplexity323.89190673828125
INFO:root:current mean train loss 7309.927849756528
INFO:root:current train perplexity323.1287536621094
INFO:root:current mean train loss 7313.695666254783
INFO:root:current train perplexity324.6031188964844
INFO:root:current mean train loss 7321.733879771659
INFO:root:current train perplexity325.1960144042969
INFO:root:current mean train loss 7328.100220551362
INFO:root:current train perplexity325.5654296875
INFO:root:current mean train loss 7333.051212376942
INFO:root:current train perplexity326.0058288574219
INFO:root:current mean train loss 7337.020256890191
INFO:root:current train perplexity326.6531677246094
INFO:root:current mean train loss 7333.925126558996
INFO:root:current train perplexity327.0248718261719
INFO:root:current mean train loss 7335.307692004788
INFO:root:current train perplexity326.8102111816406
INFO:root:current mean train loss 7341.530657534197
INFO:root:current train perplexity327.10626220703125
INFO:root:current mean train loss 7334.402344569264
INFO:root:current train perplexity326.77435302734375
INFO:root:current mean train loss 7335.477394316587
INFO:root:current train perplexity326.75067138671875
INFO:root:current mean train loss 7335.034173022742
INFO:root:current train perplexity326.7069091796875
INFO:root:current mean train loss 7335.249830148815
INFO:root:current train perplexity326.7835693359375
INFO:root:current mean train loss 7336.888647951672
INFO:root:current train perplexity326.8691101074219
INFO:root:current mean train loss 7335.815228545637
INFO:root:current train perplexity326.7628479003906
INFO:root:current mean train loss 7339.280891145979
INFO:root:current train perplexity326.9989929199219
INFO:root:current mean train loss 7341.1350043460125
INFO:root:current train perplexity327.1566467285156

100%|██████████| 1/1 [08:17<00:00, 497.04s/it][A100%|██████████| 1/1 [08:17<00:00, 497.04s/it]
INFO:root:final mean train loss: 7335.557649567219
INFO:root:final train perplexity: 326.7533264160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.01s/it][A100%|██████████| 1/1 [00:44<00:00, 44.01s/it]
INFO:root:eval mean loss: 7138.699080230496
INFO:root:eval perplexity: 322.7054748535156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.46s/it][A100%|██████████| 1/1 [00:42<00:00, 42.46s/it]
INFO:root:eval mean loss: 7210.733107546543
INFO:root:eval perplexity: 375.4593505859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/130
 65%|██████▌   | 130/200 [19:59:44<11:25:51, 587.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7347.553873697917
INFO:root:current train perplexity339.43450927734375
INFO:root:current mean train loss 7354.4459665280965
INFO:root:current train perplexity332.2040710449219
INFO:root:current mean train loss 7376.222794090161
INFO:root:current train perplexity331.0675354003906
INFO:root:current mean train loss 7361.847082638046
INFO:root:current train perplexity329.5180969238281
INFO:root:current mean train loss 7348.278834858267
INFO:root:current train perplexity329.93505859375
INFO:root:current mean train loss 7340.2077267390105
INFO:root:current train perplexity328.927978515625
INFO:root:current mean train loss 7340.090560537254
INFO:root:current train perplexity328.0245361328125
INFO:root:current mean train loss 7341.104901958083
INFO:root:current train perplexity327.93463134765625
INFO:root:current mean train loss 7334.628213964965
INFO:root:current train perplexity327.5625
INFO:root:current mean train loss 7342.148858635864
INFO:root:current train perplexity327.6106262207031
INFO:root:current mean train loss 7335.602970724418
INFO:root:current train perplexity326.9330139160156
INFO:root:current mean train loss 7328.53188797974
INFO:root:current train perplexity326.29931640625
INFO:root:current mean train loss 7331.238017521583
INFO:root:current train perplexity326.1097106933594
INFO:root:current mean train loss 7333.1802335991215
INFO:root:current train perplexity326.3999938964844
INFO:root:current mean train loss 7329.206277169092
INFO:root:current train perplexity326.28997802734375
INFO:root:current mean train loss 7330.892080783528
INFO:root:current train perplexity326.4370422363281
INFO:root:current mean train loss 7334.13074587768
INFO:root:current train perplexity326.7938537597656
INFO:root:current mean train loss 7336.326286445381
INFO:root:current train perplexity326.77264404296875
INFO:root:current mean train loss 7337.460988244541
INFO:root:current train perplexity326.9498596191406
INFO:root:current mean train loss 7337.34446566838
INFO:root:current train perplexity327.03997802734375

100%|██████████| 1/1 [08:17<00:00, 497.22s/it][A100%|██████████| 1/1 [08:17<00:00, 497.23s/it]
INFO:root:final mean train loss: 7337.0431156283485
INFO:root:final train perplexity: 327.1366882324219
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.55s/it][A100%|██████████| 1/1 [00:43<00:00, 43.55s/it]
INFO:root:eval mean loss: 7151.169826642841
INFO:root:eval perplexity: 325.9784851074219
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.23s/it][A100%|██████████| 1/1 [00:43<00:00, 43.23s/it]
INFO:root:eval mean loss: 7225.662958672706
INFO:root:eval perplexity: 380.0964050292969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/131
 66%|██████▌   | 131/200 [20:09:31<11:15:35, 587.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7293.463134765625
INFO:root:current train perplexity334.24713134765625
INFO:root:current mean train loss 7341.625162760417
INFO:root:current train perplexity326.34942626953125
INFO:root:current mean train loss 7316.077349367395
INFO:root:current train perplexity328.6903076171875
INFO:root:current mean train loss 7323.782168148486
INFO:root:current train perplexity329.21795654296875
INFO:root:current mean train loss 7323.745030076291
INFO:root:current train perplexity329.0647888183594
INFO:root:current mean train loss 7328.0169719507485
INFO:root:current train perplexity328.55694580078125
INFO:root:current mean train loss 7337.597250648962
INFO:root:current train perplexity328.85833740234375
INFO:root:current mean train loss 7357.713318375517
INFO:root:current train perplexity329.1534423828125
INFO:root:current mean train loss 7349.418490726203
INFO:root:current train perplexity328.5097961425781
INFO:root:current mean train loss 7354.100327032431
INFO:root:current train perplexity328.8470764160156
INFO:root:current mean train loss 7356.244387145163
INFO:root:current train perplexity329.2057189941406
INFO:root:current mean train loss 7348.439524675982
INFO:root:current train perplexity329.1524963378906
INFO:root:current mean train loss 7350.555645741996
INFO:root:current train perplexity328.9388122558594
INFO:root:current mean train loss 7354.160017793175
INFO:root:current train perplexity329.39556884765625
INFO:root:current mean train loss 7352.881500372546
INFO:root:current train perplexity329.1363525390625
INFO:root:current mean train loss 7349.46178415281
INFO:root:current train perplexity328.97442626953125
INFO:root:current mean train loss 7350.194495094365
INFO:root:current train perplexity329.1986389160156
INFO:root:current mean train loss 7349.624128958303
INFO:root:current train perplexity329.15533447265625
INFO:root:current mean train loss 7345.356324980319
INFO:root:current train perplexity329.1810607910156
INFO:root:current mean train loss 7348.5222008250585
INFO:root:current train perplexity329.3820495605469

100%|██████████| 1/1 [08:04<00:00, 484.65s/it][A100%|██████████| 1/1 [08:04<00:00, 484.65s/it]
INFO:root:final mean train loss: 7345.891667306874
INFO:root:final train perplexity: 329.4293212890625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.60s/it][A100%|██████████| 1/1 [00:38<00:00, 38.60s/it]
INFO:root:eval mean loss: 7156.148984652039
INFO:root:eval perplexity: 327.29473876953125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.05s/it][A100%|██████████| 1/1 [00:37<00:00, 37.05s/it]
INFO:root:eval mean loss: 7230.108770708665
INFO:root:eval perplexity: 381.4881896972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/132
 66%|██████▌   | 132/200 [20:18:53<10:57:21, 580.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7363.648335301599
INFO:root:current train perplexity330.60028076171875
INFO:root:current mean train loss 7399.7316570148605
INFO:root:current train perplexity330.4853820800781
INFO:root:current mean train loss 7377.915515287423
INFO:root:current train perplexity329.5583190917969
INFO:root:current mean train loss 7362.677799858783
INFO:root:current train perplexity328.2514343261719
INFO:root:current mean train loss 7349.201070471219
INFO:root:current train perplexity328.16864013671875
INFO:root:current mean train loss 7345.912909688651
INFO:root:current train perplexity328.76239013671875
INFO:root:current mean train loss 7341.248983190368
INFO:root:current train perplexity329.22393798828125
INFO:root:current mean train loss 7347.908494910835
INFO:root:current train perplexity328.9869689941406
INFO:root:current mean train loss 7355.597936012567
INFO:root:current train perplexity329.86474609375
INFO:root:current mean train loss 7357.753337710432
INFO:root:current train perplexity329.9926452636719
INFO:root:current mean train loss 7360.613776553511
INFO:root:current train perplexity330.5057067871094
INFO:root:current mean train loss 7360.826999774442
INFO:root:current train perplexity330.9537353515625
INFO:root:current mean train loss 7363.125284012344
INFO:root:current train perplexity330.94000244140625
INFO:root:current mean train loss 7357.628295443968
INFO:root:current train perplexity330.9622497558594
INFO:root:current mean train loss 7364.540893131713
INFO:root:current train perplexity331.5468444824219
INFO:root:current mean train loss 7360.407532568961
INFO:root:current train perplexity331.4521484375
INFO:root:current mean train loss 7358.364278912907
INFO:root:current train perplexity331.8549499511719
INFO:root:current mean train loss 7361.232266958459
INFO:root:current train perplexity331.9915466308594
INFO:root:current mean train loss 7362.077512197759
INFO:root:current train perplexity332.1354675292969
INFO:root:current mean train loss 7360.073496003281
INFO:root:current train perplexity332.1099853515625

100%|██████████| 1/1 [07:25<00:00, 445.82s/it][A100%|██████████| 1/1 [07:25<00:00, 445.82s/it]
INFO:root:final mean train loss: 7356.411472614883
INFO:root:final train perplexity: 332.1758728027344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.91s/it][A100%|██████████| 1/1 [00:38<00:00, 38.91s/it]
INFO:root:eval mean loss: 7163.344269448138
INFO:root:eval perplexity: 329.2061767578125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.71s/it][A100%|██████████| 1/1 [00:37<00:00, 37.71s/it]
INFO:root:eval mean loss: 7239.01500858821
INFO:root:eval perplexity: 384.29180908203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/133
 66%|██████▋   | 133/200 [20:27:38<10:29:09, 563.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7351.073828125
INFO:root:current train perplexity331.4254455566406
INFO:root:current mean train loss 7395.440911865235
INFO:root:current train perplexity332.4324035644531
INFO:root:current mean train loss 7377.437293419471
INFO:root:current train perplexity332.6082458496094
INFO:root:current mean train loss 7372.674473741319
INFO:root:current train perplexity332.20025634765625
INFO:root:current mean train loss 7366.171092688519
INFO:root:current train perplexity332.408935546875
INFO:root:current mean train loss 7375.035689871652
INFO:root:current train perplexity331.9256896972656
INFO:root:current mean train loss 7373.32900242661
INFO:root:current train perplexity331.7582702636719
INFO:root:current mean train loss 7383.667860814145
INFO:root:current train perplexity331.9608459472656
INFO:root:current mean train loss 7373.321143713662
INFO:root:current train perplexity331.4350891113281
INFO:root:current mean train loss 7371.975974019369
INFO:root:current train perplexity331.8321838378906
INFO:root:current mean train loss 7369.824013764003
INFO:root:current train perplexity332.28167724609375
INFO:root:current mean train loss 7365.131934435614
INFO:root:current train perplexity332.0659484863281
INFO:root:current mean train loss 7358.367824203249
INFO:root:current train perplexity331.5550537109375
INFO:root:current mean train loss 7361.510476146025
INFO:root:current train perplexity331.8551025390625
INFO:root:current mean train loss 7367.297959920805
INFO:root:current train perplexity332.3325500488281
INFO:root:current mean train loss 7367.989862217048
INFO:root:current train perplexity332.9549560546875
INFO:root:current mean train loss 7369.027586419898
INFO:root:current train perplexity333.3306579589844
INFO:root:current mean train loss 7367.0963975386185
INFO:root:current train perplexity333.642822265625
INFO:root:current mean train loss 7366.625965536794
INFO:root:current train perplexity333.8654479980469
INFO:root:current mean train loss 7364.55989641462
INFO:root:current train perplexity333.8751525878906

100%|██████████| 1/1 [07:27<00:00, 447.66s/it][A100%|██████████| 1/1 [07:27<00:00, 447.66s/it]
INFO:root:final mean train loss: 7363.113101745698
INFO:root:final train perplexity: 333.937255859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.56s/it][A100%|██████████| 1/1 [00:39<00:00, 39.56s/it]
INFO:root:eval mean loss: 7180.925157912234
INFO:root:eval perplexity: 333.9227600097656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.78s/it][A100%|██████████| 1/1 [00:37<00:00, 37.78s/it]
INFO:root:eval mean loss: 7257.090499120402
INFO:root:eval perplexity: 390.0450744628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/134
 67%|██████▋   | 134/200 [20:36:25<10:07:53, 552.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7410.243030894886
INFO:root:current train perplexity338.9056396484375
INFO:root:current mean train loss 7372.387088409251
INFO:root:current train perplexity335.9400329589844
INFO:root:current mean train loss 7368.80864310695
INFO:root:current train perplexity335.1988525390625
INFO:root:current mean train loss 7383.531571203581
INFO:root:current train perplexity336.3181457519531
INFO:root:current mean train loss 7360.681090924725
INFO:root:current train perplexity334.7658386230469
INFO:root:current mean train loss 7356.216022564179
INFO:root:current train perplexity334.78704833984375
INFO:root:current mean train loss 7345.03960054699
INFO:root:current train perplexity333.58697509765625
INFO:root:current mean train loss 7350.108839587355
INFO:root:current train perplexity333.4207763671875
INFO:root:current mean train loss 7349.5097483653435
INFO:root:current train perplexity333.46514892578125
INFO:root:current mean train loss 7357.213847696232
INFO:root:current train perplexity334.0093994140625
INFO:root:current mean train loss 7359.734712308496
INFO:root:current train perplexity333.9851989746094
INFO:root:current mean train loss 7359.622513789693
INFO:root:current train perplexity334.184326171875
INFO:root:current mean train loss 7363.9876721411265
INFO:root:current train perplexity334.8042907714844
INFO:root:current mean train loss 7365.157303155637
INFO:root:current train perplexity335.2549743652344
INFO:root:current mean train loss 7364.499884954723
INFO:root:current train perplexity335.37921142578125
INFO:root:current mean train loss 7370.5709800550885
INFO:root:current train perplexity335.75860595703125
INFO:root:current mean train loss 7375.5703855820475
INFO:root:current train perplexity335.7613220214844
INFO:root:current mean train loss 7372.867653798977
INFO:root:current train perplexity335.801025390625
INFO:root:current mean train loss 7373.968139453333
INFO:root:current train perplexity335.9515686035156
INFO:root:current mean train loss 7372.257747544022
INFO:root:current train perplexity336.0509033203125

100%|██████████| 1/1 [07:26<00:00, 446.82s/it][A100%|██████████| 1/1 [07:26<00:00, 446.82s/it]
INFO:root:final mean train loss: 7371.538321706183
INFO:root:final train perplexity: 336.16485595703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.72s/it][A100%|██████████| 1/1 [00:38<00:00, 38.72s/it]
INFO:root:eval mean loss: 7181.892926155253
INFO:root:eval perplexity: 334.18463134765625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.58s/it][A100%|██████████| 1/1 [00:37<00:00, 37.58s/it]
INFO:root:eval mean loss: 7257.298254134807
INFO:root:eval perplexity: 390.1114807128906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/135
 68%|██████▊   | 135/200 [20:45:11<9:49:53, 544.51s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7391.830872880651
INFO:root:current train perplexity341.1288146972656
INFO:root:current mean train loss 7363.503792988885
INFO:root:current train perplexity340.4485778808594
INFO:root:current mean train loss 7362.1891857328865
INFO:root:current train perplexity338.51348876953125
INFO:root:current mean train loss 7368.023192120082
INFO:root:current train perplexity337.8404235839844
INFO:root:current mean train loss 7380.430746101657
INFO:root:current train perplexity337.17864990234375
INFO:root:current mean train loss 7383.347032335069
INFO:root:current train perplexity336.84759521484375
INFO:root:current mean train loss 7376.919977457448
INFO:root:current train perplexity336.9815979003906
INFO:root:current mean train loss 7378.6219227211905
INFO:root:current train perplexity337.19744873046875
INFO:root:current mean train loss 7387.5883843680085
INFO:root:current train perplexity337.34478759765625
INFO:root:current mean train loss 7379.2751528703475
INFO:root:current train perplexity337.0516662597656
INFO:root:current mean train loss 7376.975045436043
INFO:root:current train perplexity337.20068359375
INFO:root:current mean train loss 7375.84536860736
INFO:root:current train perplexity336.8980407714844
INFO:root:current mean train loss 7376.7820020436875
INFO:root:current train perplexity336.9994201660156
INFO:root:current mean train loss 7373.320730726551
INFO:root:current train perplexity336.6088562011719
INFO:root:current mean train loss 7379.810080818064
INFO:root:current train perplexity336.9831848144531
INFO:root:current mean train loss 7380.4943694494
INFO:root:current train perplexity337.13348388671875
INFO:root:current mean train loss 7381.304200659958
INFO:root:current train perplexity337.2456970214844
INFO:root:current mean train loss 7380.397831911493
INFO:root:current train perplexity337.4117126464844
INFO:root:current mean train loss 7382.330249049218
INFO:root:current train perplexity337.57891845703125

100%|██████████| 1/1 [07:23<00:00, 443.86s/it][A100%|██████████| 1/1 [07:23<00:00, 443.86s/it]
INFO:root:final mean train loss: 7376.947833439706
INFO:root:final train perplexity: 337.60321044921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.30s/it][A100%|██████████| 1/1 [00:38<00:00, 38.30s/it]
INFO:root:eval mean loss: 7192.369443636414
INFO:root:eval perplexity: 337.0298156738281
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.55s/it][A100%|██████████| 1/1 [00:37<00:00, 37.55s/it]
INFO:root:eval mean loss: 7269.506540717808
INFO:root:eval perplexity: 394.0467224121094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/136
 68%|██████▊   | 136/200 [20:53:53<9:33:38, 537.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7468.729580965909
INFO:root:current train perplexity356.02532958984375
INFO:root:current mean train loss 7403.108037725226
INFO:root:current train perplexity339.559326171875
INFO:root:current mean train loss 7370.658156842417
INFO:root:current train perplexity337.3467102050781
INFO:root:current mean train loss 7384.474446091238
INFO:root:current train perplexity336.7928466796875
INFO:root:current mean train loss 7362.835777115648
INFO:root:current train perplexity336.1390380859375
INFO:root:current mean train loss 7363.10157874419
INFO:root:current train perplexity336.2463073730469
INFO:root:current mean train loss 7376.7099609375
INFO:root:current train perplexity336.98175048828125
INFO:root:current mean train loss 7368.933328663415
INFO:root:current train perplexity336.6919860839844
INFO:root:current mean train loss 7364.1798891944745
INFO:root:current train perplexity335.7936706542969
INFO:root:current mean train loss 7364.739216541747
INFO:root:current train perplexity335.8412170410156
INFO:root:current mean train loss 7363.77089273847
INFO:root:current train perplexity335.79638671875
INFO:root:current mean train loss 7361.9599574215235
INFO:root:current train perplexity335.5084533691406
INFO:root:current mean train loss 7364.026956673204
INFO:root:current train perplexity335.9443664550781
INFO:root:current mean train loss 7370.919115894356
INFO:root:current train perplexity336.2049255371094
INFO:root:current mean train loss 7371.097384598135
INFO:root:current train perplexity336.4742126464844
INFO:root:current mean train loss 7372.2202041797655
INFO:root:current train perplexity336.60595703125
INFO:root:current mean train loss 7373.022929820861
INFO:root:current train perplexity336.9203796386719
INFO:root:current mean train loss 7375.374666678843
INFO:root:current train perplexity336.9801330566406
INFO:root:current mean train loss 7376.775738164774
INFO:root:current train perplexity337.3589172363281
INFO:root:current mean train loss 7379.694692630658
INFO:root:current train perplexity337.58905029296875

100%|██████████| 1/1 [07:28<00:00, 448.12s/it][A100%|██████████| 1/1 [07:28<00:00, 448.13s/it]
INFO:root:final mean train loss: 7376.328165012962
INFO:root:final train perplexity: 337.4380798339844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.48s/it][A100%|██████████| 1/1 [00:38<00:00, 38.48s/it]
INFO:root:eval mean loss: 7189.48568920379
INFO:root:eval perplexity: 336.24420166015625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.71s/it][A100%|██████████| 1/1 [00:36<00:00, 36.71s/it]
INFO:root:eval mean loss: 7268.492815166501
INFO:root:eval perplexity: 393.7186279296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/137
 68%|██████▊   | 137/200 [21:02:39<9:20:49, 534.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7325.820277622768
INFO:root:current train perplexity334.7283020019531
INFO:root:current mean train loss 7327.9241371154785
INFO:root:current train perplexity334.2366027832031
INFO:root:current mean train loss 7351.206309536047
INFO:root:current train perplexity334.9805908203125
INFO:root:current mean train loss 7359.71296059213
INFO:root:current train perplexity334.98553466796875
INFO:root:current mean train loss 7365.23439211266
INFO:root:current train perplexity335.2861328125
INFO:root:current mean train loss 7374.840317234848
INFO:root:current train perplexity336.0964050292969
INFO:root:current mean train loss 7370.007486719994
INFO:root:current train perplexity335.5890808105469
INFO:root:current mean train loss 7369.212496914706
INFO:root:current train perplexity335.981689453125
INFO:root:current mean train loss 7371.752603576955
INFO:root:current train perplexity335.8484191894531
INFO:root:current mean train loss 7375.9356047531655
INFO:root:current train perplexity336.248046875
INFO:root:current mean train loss 7371.892968560007
INFO:root:current train perplexity336.0571594238281
INFO:root:current mean train loss 7367.212881967531
INFO:root:current train perplexity336.04962158203125
INFO:root:current mean train loss 7369.913330078125
INFO:root:current train perplexity336.4595947265625
INFO:root:current mean train loss 7369.5669309317345
INFO:root:current train perplexity336.4142150878906
INFO:root:current mean train loss 7374.53555084143
INFO:root:current train perplexity336.74371337890625
INFO:root:current mean train loss 7374.555412252536
INFO:root:current train perplexity336.3665771484375
INFO:root:current mean train loss 7374.11833532085
INFO:root:current train perplexity336.23699951171875
INFO:root:current mean train loss 7372.756612707068
INFO:root:current train perplexity336.1416015625
INFO:root:current mean train loss 7372.976882233401
INFO:root:current train perplexity336.2519226074219
INFO:root:current mean train loss 7374.5729967805855
INFO:root:current train perplexity336.3511657714844

100%|██████████| 1/1 [07:24<00:00, 444.11s/it][A100%|██████████| 1/1 [07:24<00:00, 444.11s/it]
INFO:root:final mean train loss: 7372.821119038192
INFO:root:final train perplexity: 336.5053405761719
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.12s/it][A100%|██████████| 1/1 [00:39<00:00, 39.12s/it]
INFO:root:eval mean loss: 7179.70204801086
INFO:root:eval perplexity: 333.59271240234375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.97s/it][A100%|██████████| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 7257.825193581006
INFO:root:eval perplexity: 390.2806091308594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/138
 69%|██████▉   | 138/200 [21:11:21<9:08:19, 530.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7429.468207465277
INFO:root:current train perplexity335.1467590332031
INFO:root:current mean train loss 7375.570255253233
INFO:root:current train perplexity334.11358642578125
INFO:root:current mean train loss 7370.826452885842
INFO:root:current train perplexity331.22955322265625
INFO:root:current mean train loss 7382.128219825634
INFO:root:current train perplexity332.2238464355469
INFO:root:current mean train loss 7386.390890537219
INFO:root:current train perplexity333.50616455078125
INFO:root:current mean train loss 7377.94735521789
INFO:root:current train perplexity334.17254638671875
INFO:root:current mean train loss 7374.34738372093
INFO:root:current train perplexity334.8387756347656
INFO:root:current mean train loss 7379.536538511955
INFO:root:current train perplexity335.1771240234375
INFO:root:current mean train loss 7384.55346188517
INFO:root:current train perplexity335.4510192871094
INFO:root:current mean train loss 7377.42834046379
INFO:root:current train perplexity335.3734436035156
INFO:root:current mean train loss 7371.286829489384
INFO:root:current train perplexity335.13653564453125
INFO:root:current mean train loss 7374.246154731851
INFO:root:current train perplexity335.722412109375
INFO:root:current mean train loss 7377.019991685492
INFO:root:current train perplexity336.02911376953125
INFO:root:current mean train loss 7371.023819775209
INFO:root:current train perplexity336.0252685546875
INFO:root:current mean train loss 7373.071051511138
INFO:root:current train perplexity336.146728515625
INFO:root:current mean train loss 7376.89234710002
INFO:root:current train perplexity336.3085021972656
INFO:root:current mean train loss 7375.057743636018
INFO:root:current train perplexity336.4400329589844
INFO:root:current mean train loss 7375.475118362733
INFO:root:current train perplexity336.7825622558594
INFO:root:current mean train loss 7373.478114413957
INFO:root:current train perplexity336.5809326171875
INFO:root:current mean train loss 7373.811587453808
INFO:root:current train perplexity336.5706481933594

100%|██████████| 1/1 [07:13<00:00, 433.56s/it][A100%|██████████| 1/1 [07:13<00:00, 433.56s/it]
INFO:root:final mean train loss: 7372.9082203613525
INFO:root:final train perplexity: 336.5284423828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.99s/it][A100%|██████████| 1/1 [00:35<00:00, 35.99s/it]
INFO:root:eval mean loss: 7178.815183815381
INFO:root:eval perplexity: 333.3533935546875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.91s/it][A100%|██████████| 1/1 [00:35<00:00, 35.91s/it]
INFO:root:eval mean loss: 7256.813495608932
INFO:root:eval perplexity: 389.9562072753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/139
 70%|██████▉   | 139/200 [21:19:49<8:52:32, 523.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7364.3746692288305
INFO:root:current train perplexity340.75262451171875
INFO:root:current mean train loss 7352.11397147473
INFO:root:current train perplexity338.0905456542969
INFO:root:current mean train loss 7343.518131634662
INFO:root:current train perplexity335.5375671386719
INFO:root:current mean train loss 7369.937419069406
INFO:root:current train perplexity336.3739318847656
INFO:root:current mean train loss 7382.9583544710495
INFO:root:current train perplexity337.05535888671875
INFO:root:current mean train loss 7381.258126146853
INFO:root:current train perplexity337.3869323730469
INFO:root:current mean train loss 7376.667598482345
INFO:root:current train perplexity337.0474853515625
INFO:root:current mean train loss 7372.332469549705
INFO:root:current train perplexity336.6058044433594
INFO:root:current mean train loss 7377.586461467698
INFO:root:current train perplexity336.8428039550781
INFO:root:current mean train loss 7376.299166165865
INFO:root:current train perplexity336.9896240234375
INFO:root:current mean train loss 7380.387944510652
INFO:root:current train perplexity337.0852355957031
INFO:root:current mean train loss 7374.065555749785
INFO:root:current train perplexity336.3069152832031
INFO:root:current mean train loss 7380.786025638248
INFO:root:current train perplexity336.6995544433594
INFO:root:current mean train loss 7382.039540026157
INFO:root:current train perplexity337.04296875
INFO:root:current mean train loss 7378.795256190685
INFO:root:current train perplexity336.7742004394531
INFO:root:current mean train loss 7379.690019868858
INFO:root:current train perplexity336.9455871582031
INFO:root:current mean train loss 7378.5776208540165
INFO:root:current train perplexity336.973388671875
INFO:root:current mean train loss 7379.4138610354885
INFO:root:current train perplexity336.72747802734375
INFO:root:current mean train loss 7378.2313223243655
INFO:root:current train perplexity336.9510498046875
INFO:root:current mean train loss 7374.317272314603
INFO:root:current train perplexity336.8909912109375

100%|██████████| 1/1 [07:25<00:00, 445.45s/it][A100%|██████████| 1/1 [07:25<00:00, 445.46s/it]
INFO:root:final mean train loss: 7374.743218849479
INFO:root:final train perplexity: 337.01629638671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.52s/it][A100%|██████████| 1/1 [00:38<00:00, 38.52s/it]
INFO:root:eval mean loss: 7213.978399614915
INFO:root:eval perplexity: 342.9751281738281
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.90s/it][A100%|██████████| 1/1 [00:36<00:00, 36.90s/it]
INFO:root:eval mean loss: 7301.319161056626
INFO:root:eval perplexity: 404.4888000488281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/140
 70%|███████   | 140/200 [21:28:32<8:43:37, 523.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7365.788407337816
INFO:root:current train perplexity338.2571105957031
INFO:root:current mean train loss 7363.168064223987
INFO:root:current train perplexity337.9325866699219
INFO:root:current mean train loss 7376.3037161878365
INFO:root:current train perplexity338.2806701660156
INFO:root:current mean train loss 7386.632659187418
INFO:root:current train perplexity339.6135559082031
INFO:root:current mean train loss 7383.622758391506
INFO:root:current train perplexity340.3540954589844
INFO:root:current mean train loss 7394.052531978627
INFO:root:current train perplexity341.23876953125
INFO:root:current mean train loss 7391.096858028811
INFO:root:current train perplexity341.06390380859375
INFO:root:current mean train loss 7396.970681186818
INFO:root:current train perplexity341.1336975097656
INFO:root:current mean train loss 7389.315422466048
INFO:root:current train perplexity340.74334716796875
INFO:root:current mean train loss 7385.9462182392745
INFO:root:current train perplexity340.6523742675781
INFO:root:current mean train loss 7385.040071644752
INFO:root:current train perplexity340.1475830078125
INFO:root:current mean train loss 7383.680912137537
INFO:root:current train perplexity340.2241516113281
INFO:root:current mean train loss 7389.2609491057465
INFO:root:current train perplexity340.2173156738281
INFO:root:current mean train loss 7386.769263916901
INFO:root:current train perplexity340.0608215332031
INFO:root:current mean train loss 7388.040275774912
INFO:root:current train perplexity340.08349609375
INFO:root:current mean train loss 7389.195398467186
INFO:root:current train perplexity339.93695068359375
INFO:root:current mean train loss 7388.372129057475
INFO:root:current train perplexity339.8680725097656
INFO:root:current mean train loss 7388.0777253724
INFO:root:current train perplexity339.76776123046875
INFO:root:current mean train loss 7385.893227001147
INFO:root:current train perplexity339.8233337402344
INFO:root:current mean train loss 7388.788119246226
INFO:root:current train perplexity340.1826171875

100%|██████████| 1/1 [07:17<00:00, 437.10s/it][A100%|██████████| 1/1 [07:17<00:00, 437.10s/it]
INFO:root:final mean train loss: 7386.625714323704
INFO:root:final train perplexity: 340.1915283203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.77s/it][A100%|██████████| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 7176.562044617132
INFO:root:eval perplexity: 332.74609375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.91s/it][A100%|██████████| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 7260.362926293772
INFO:root:eval perplexity: 391.0956726074219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/141
 70%|███████   | 141/200 [21:37:06<8:31:49, 520.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7365.0500081380205
INFO:root:current train perplexity338.20806884765625
INFO:root:current mean train loss 7362.066503408004
INFO:root:current train perplexity338.0669860839844
INFO:root:current mean train loss 7379.9522705078125
INFO:root:current train perplexity337.9867248535156
INFO:root:current mean train loss 7393.40986032197
INFO:root:current train perplexity338.9122619628906
INFO:root:current mean train loss 7398.53653249433
INFO:root:current train perplexity339.0466003417969
INFO:root:current mean train loss 7388.576842032823
INFO:root:current train perplexity339.02008056640625
INFO:root:current mean train loss 7379.851032125539
INFO:root:current train perplexity338.70208740234375
INFO:root:current mean train loss 7381.440130339196
INFO:root:current train perplexity338.8435974121094
INFO:root:current mean train loss 7386.972695486887
INFO:root:current train perplexity339.31005859375
INFO:root:current mean train loss 7390.773073250031
INFO:root:current train perplexity339.54620361328125
INFO:root:current mean train loss 7390.577108341412
INFO:root:current train perplexity339.34063720703125
INFO:root:current mean train loss 7386.27410500823
INFO:root:current train perplexity339.11126708984375
INFO:root:current mean train loss 7386.293929865331
INFO:root:current train perplexity339.2294921875
INFO:root:current mean train loss 7381.7568072562235
INFO:root:current train perplexity339.0249328613281
INFO:root:current mean train loss 7383.624020499979
INFO:root:current train perplexity339.18194580078125
INFO:root:current mean train loss 7383.196481499158
INFO:root:current train perplexity338.98565673828125
INFO:root:current mean train loss 7378.598425235388
INFO:root:current train perplexity338.7896423339844
INFO:root:current mean train loss 7381.40491755768
INFO:root:current train perplexity339.0517578125
INFO:root:current mean train loss 7383.363340739962
INFO:root:current train perplexity339.20458984375

100%|██████████| 1/1 [07:15<00:00, 435.29s/it][A100%|██████████| 1/1 [07:15<00:00, 435.29s/it]
INFO:root:final mean train loss: 7384.38886418345
INFO:root:final train perplexity: 339.5915222167969
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.63s/it][A100%|██████████| 1/1 [00:36<00:00, 36.63s/it]
INFO:root:eval mean loss: 7198.0221094442595
INFO:root:eval perplexity: 338.57501220703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.29s/it][A100%|██████████| 1/1 [00:36<00:00, 36.29s/it]
INFO:root:eval mean loss: 7265.040163730053
INFO:root:eval perplexity: 392.602783203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/142
 71%|███████   | 142/200 [21:45:36<8:20:17, 517.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7074.240196814904
INFO:root:current train perplexity327.4233703613281
INFO:root:current mean train loss 7333.199421840432
INFO:root:current train perplexity340.16021728515625
INFO:root:current mean train loss 7368.8306695642605
INFO:root:current train perplexity340.0082702636719
INFO:root:current mean train loss 7361.462957705172
INFO:root:current train perplexity340.3231201171875
INFO:root:current mean train loss 7381.10931233921
INFO:root:current train perplexity340.8917236328125
INFO:root:current mean train loss 7386.073426839668
INFO:root:current train perplexity340.8592224121094
INFO:root:current mean train loss 7387.605426533187
INFO:root:current train perplexity340.75146484375
INFO:root:current mean train loss 7391.407499808249
INFO:root:current train perplexity340.99560546875
INFO:root:current mean train loss 7388.781811553467
INFO:root:current train perplexity341.5164794921875
INFO:root:current mean train loss 7395.304576794393
INFO:root:current train perplexity341.96185302734375
INFO:root:current mean train loss 7395.575865313425
INFO:root:current train perplexity342.1730651855469
INFO:root:current mean train loss 7393.926688058035
INFO:root:current train perplexity342.2221984863281
INFO:root:current mean train loss 7395.688713658672
INFO:root:current train perplexity341.8966369628906
INFO:root:current mean train loss 7400.796033058835
INFO:root:current train perplexity342.3343200683594
INFO:root:current mean train loss 7404.673281789079
INFO:root:current train perplexity342.79150390625
INFO:root:current mean train loss 7411.859686751281
INFO:root:current train perplexity343.4826354980469
INFO:root:current mean train loss 7405.351509827379
INFO:root:current train perplexity343.24591064453125
INFO:root:current mean train loss 7406.513708645743
INFO:root:current train perplexity343.32415771484375
INFO:root:current mean train loss 7405.669233217992
INFO:root:current train perplexity343.27813720703125
INFO:root:current mean train loss 7400.8732270770715
INFO:root:current train perplexity343.1102294921875

100%|██████████| 1/1 [07:25<00:00, 445.82s/it][A100%|██████████| 1/1 [07:25<00:00, 445.82s/it]
INFO:root:final mean train loss: 7397.31189611207
INFO:root:final train perplexity: 343.0726013183594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.41s/it][A100%|██████████| 1/1 [00:38<00:00, 38.41s/it]
INFO:root:eval mean loss: 7241.21580646055
INFO:root:eval perplexity: 350.61834716796875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 37.00s/it][A100%|██████████| 1/1 [00:36<00:00, 37.00s/it]
INFO:root:eval mean loss: 7300.555355856604
INFO:root:eval perplexity: 404.23486328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/143
 72%|███████▏  | 143/200 [21:54:20<8:13:25, 519.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7505.344970703125
INFO:root:current train perplexity344.5539855957031
INFO:root:current mean train loss 7422.035133713943
INFO:root:current train perplexity340.1060485839844
INFO:root:current mean train loss 7441.666866932745
INFO:root:current train perplexity343.9873046875
INFO:root:current mean train loss 7431.85889263731
INFO:root:current train perplexity344.4252014160156
INFO:root:current mean train loss 7418.420751953125
INFO:root:current train perplexity345.3031921386719
INFO:root:current mean train loss 7414.8822385392095
INFO:root:current train perplexity344.6936950683594
INFO:root:current mean train loss 7425.659113808284
INFO:root:current train perplexity345.3647766113281
INFO:root:current mean train loss 7422.451280233305
INFO:root:current train perplexity344.95989990234375
INFO:root:current mean train loss 7416.60589526073
INFO:root:current train perplexity345.256591796875
INFO:root:current mean train loss 7415.497063487063
INFO:root:current train perplexity345.4499206542969
INFO:root:current mean train loss 7411.967668670358
INFO:root:current train perplexity345.6100769042969
INFO:root:current mean train loss 7410.651035761201
INFO:root:current train perplexity345.9025573730469
INFO:root:current mean train loss 7406.93072638783
INFO:root:current train perplexity345.8509216308594
INFO:root:current mean train loss 7403.365007122298
INFO:root:current train perplexity345.57562255859375
INFO:root:current mean train loss 7404.366168255572
INFO:root:current train perplexity345.3548889160156
INFO:root:current mean train loss 7404.956382442299
INFO:root:current train perplexity345.25115966796875
INFO:root:current mean train loss 7405.1266290021085
INFO:root:current train perplexity345.4798889160156
INFO:root:current mean train loss 7406.185634934971
INFO:root:current train perplexity345.52752685546875
INFO:root:current mean train loss 7405.104258719689
INFO:root:current train perplexity345.1206359863281
INFO:root:current mean train loss 7405.900849558776
INFO:root:current train perplexity344.98883056640625

100%|██████████| 1/1 [07:22<00:00, 442.13s/it][A100%|██████████| 1/1 [07:22<00:00, 442.13s/it]
INFO:root:final mean train loss: 7404.426257712037
INFO:root:final train perplexity: 345.004150390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.38s/it][A100%|██████████| 1/1 [00:36<00:00, 36.38s/it]
INFO:root:eval mean loss: 7227.68689744016
INFO:root:eval perplexity: 346.8008117675781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.40s/it][A100%|██████████| 1/1 [00:36<00:00, 36.40s/it]
INFO:root:eval mean loss: 7291.261977608322
INFO:root:eval perplexity: 401.1579284667969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/144
 72%|███████▏  | 144/200 [22:02:57<8:04:07, 518.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7383.53992478391
INFO:root:current train perplexity343.7903747558594
INFO:root:current mean train loss 7431.216238839285
INFO:root:current train perplexity344.74578857421875
INFO:root:current mean train loss 7382.8289967896
INFO:root:current train perplexity340.9354553222656
INFO:root:current mean train loss 7389.751908096182
INFO:root:current train perplexity340.7471008300781
INFO:root:current mean train loss 7389.607909063899
INFO:root:current train perplexity339.2607116699219
INFO:root:current mean train loss 7383.87889464551
INFO:root:current train perplexity339.685791015625
INFO:root:current mean train loss 7381.56249849063
INFO:root:current train perplexity339.8912353515625
INFO:root:current mean train loss 7368.201176450594
INFO:root:current train perplexity338.9822692871094
INFO:root:current mean train loss 7371.950142852531
INFO:root:current train perplexity338.75958251953125
INFO:root:current mean train loss 7372.428040130841
INFO:root:current train perplexity338.1848449707031
INFO:root:current mean train loss 7373.371323666577
INFO:root:current train perplexity338.2924499511719
INFO:root:current mean train loss 7371.413314539968
INFO:root:current train perplexity338.188720703125
INFO:root:current mean train loss 7377.2110322586705
INFO:root:current train perplexity338.3890686035156
INFO:root:current mean train loss 7375.295758876787
INFO:root:current train perplexity338.62359619140625
INFO:root:current mean train loss 7374.395278350682
INFO:root:current train perplexity338.32373046875
INFO:root:current mean train loss 7379.1244599552565
INFO:root:current train perplexity338.28533935546875
INFO:root:current mean train loss 7380.080320338589
INFO:root:current train perplexity338.5210876464844
INFO:root:current mean train loss 7381.2990276858545
INFO:root:current train perplexity338.4376525878906
INFO:root:current mean train loss 7383.108254358842
INFO:root:current train perplexity338.76751708984375
INFO:root:current mean train loss 7383.655467546225
INFO:root:current train perplexity338.6258544921875

100%|██████████| 1/1 [07:09<00:00, 429.68s/it][A100%|██████████| 1/1 [07:09<00:00, 429.68s/it]
INFO:root:final mean train loss: 7380.418908623692
INFO:root:final train perplexity: 338.5291748046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.24s/it][A100%|██████████| 1/1 [00:37<00:00, 37.24s/it]
INFO:root:eval mean loss: 7213.846201795212
INFO:root:eval perplexity: 342.9383239746094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.39s/it][A100%|██████████| 1/1 [00:35<00:00, 35.39s/it]
INFO:root:eval mean loss: 7274.476707945479
INFO:root:eval perplexity: 395.6603088378906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/145
 72%|███████▎  | 145/200 [22:11:22<7:51:37, 514.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7411.901260375977
INFO:root:current train perplexity341.40057373046875
INFO:root:current mean train loss 7417.449462890625
INFO:root:current train perplexity340.0976257324219
INFO:root:current mean train loss 7382.2950439453125
INFO:root:current train perplexity339.2131652832031
INFO:root:current mean train loss 7388.789809677627
INFO:root:current train perplexity339.2474365234375
INFO:root:current mean train loss 7378.603852370689
INFO:root:current train perplexity339.268798828125
INFO:root:current mean train loss 7370.425325867132
INFO:root:current train perplexity340.0369873046875
INFO:root:current mean train loss 7374.15097156203
INFO:root:current train perplexity339.8346862792969
INFO:root:current mean train loss 7377.503453119887
INFO:root:current train perplexity339.9886474609375
INFO:root:current mean train loss 7375.142117535626
INFO:root:current train perplexity340.12371826171875
INFO:root:current mean train loss 7377.196300712364
INFO:root:current train perplexity340.2453918457031
INFO:root:current mean train loss 7381.658472505727
INFO:root:current train perplexity340.67462158203125
INFO:root:current mean train loss 7375.262143269438
INFO:root:current train perplexity340.6867980957031
INFO:root:current mean train loss 7382.710318649872
INFO:root:current train perplexity341.1414794921875
INFO:root:current mean train loss 7384.055555953309
INFO:root:current train perplexity341.1380615234375
INFO:root:current mean train loss 7385.7136343867405
INFO:root:current train perplexity341.08489990234375
INFO:root:current mean train loss 7390.983375022479
INFO:root:current train perplexity341.3245544433594
INFO:root:current mean train loss 7390.11386343149
INFO:root:current train perplexity341.2578125
INFO:root:current mean train loss 7390.4673084077385
INFO:root:current train perplexity341.6085205078125
INFO:root:current mean train loss 7389.708830084412
INFO:root:current train perplexity341.3798828125
INFO:root:current mean train loss 7390.824868382844
INFO:root:current train perplexity341.0951232910156

100%|██████████| 1/1 [07:05<00:00, 425.12s/it][A100%|██████████| 1/1 [07:05<00:00, 425.12s/it]
INFO:root:final mean train loss: 7390.333849192748
INFO:root:final train perplexity: 341.1885070800781
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.36s/it][A100%|██████████| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 7188.565159574468
INFO:root:eval perplexity: 335.9938659667969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.13s/it][A100%|██████████| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 7268.084811163287
INFO:root:eval perplexity: 393.586669921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/146
 73%|███████▎  | 146/200 [22:19:42<7:39:05, 510.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7413.1386477623455
INFO:root:current train perplexity337.5151672363281
INFO:root:current mean train loss 7374.449453448722
INFO:root:current train perplexity336.0148620605469
INFO:root:current mean train loss 7374.235002293705
INFO:root:current train perplexity336.8962707519531
INFO:root:current mean train loss 7373.946280091453
INFO:root:current train perplexity337.8773193359375
INFO:root:current mean train loss 7381.705324803469
INFO:root:current train perplexity337.2640380859375
INFO:root:current mean train loss 7379.294950449118
INFO:root:current train perplexity337.1707763671875
INFO:root:current mean train loss 7376.566694486509
INFO:root:current train perplexity337.0503845214844
INFO:root:current mean train loss 7364.30966221691
INFO:root:current train perplexity336.1531677246094
INFO:root:current mean train loss 7376.429668656002
INFO:root:current train perplexity336.8578796386719
INFO:root:current mean train loss 7372.105583229803
INFO:root:current train perplexity336.8172607421875
INFO:root:current mean train loss 7371.458084148791
INFO:root:current train perplexity336.9205322265625
INFO:root:current mean train loss 7369.723934629022
INFO:root:current train perplexity336.9031982421875
INFO:root:current mean train loss 7371.998813792935
INFO:root:current train perplexity336.91845703125
INFO:root:current mean train loss 7368.733202559287
INFO:root:current train perplexity337.037841796875
INFO:root:current mean train loss 7371.371731054292
INFO:root:current train perplexity337.37548828125
INFO:root:current mean train loss 7370.180057494268
INFO:root:current train perplexity337.1784973144531
INFO:root:current mean train loss 7372.889261820996
INFO:root:current train perplexity337.1881408691406
INFO:root:current mean train loss 7374.161893610068
INFO:root:current train perplexity337.0232238769531
INFO:root:current mean train loss 7374.07531316454
INFO:root:current train perplexity336.8649597167969
INFO:root:current mean train loss 7376.412682199647
INFO:root:current train perplexity336.9007873535156

100%|██████████| 1/1 [07:28<00:00, 448.03s/it][A100%|██████████| 1/1 [07:28<00:00, 448.03s/it]
INFO:root:final mean train loss: 7374.337615582057
INFO:root:final train perplexity: 336.9084777832031
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.09s/it][A100%|██████████| 1/1 [00:39<00:00, 39.09s/it]
INFO:root:eval mean loss: 7189.035800365691
INFO:root:eval perplexity: 336.1217346191406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.75s/it][A100%|██████████| 1/1 [00:37<00:00, 37.75s/it]
INFO:root:eval mean loss: 7271.918411146665
INFO:root:eval perplexity: 394.8289794921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat_1e-5/147
 74%|███████▎  | 147/200 [22:28:29<7:35:06, 515.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7368.661043128189
INFO:root:current train perplexity338.7184143066406
INFO:root:current mean train loss 7350.048566721906
INFO:root:current train perplexity334.9972229003906
INFO:root:current mean train loss 7359.640026937395
INFO:root:current train perplexity334.62457275390625
INFO:root:current mean train loss 7379.773324630967
INFO:root:current train perplexity335.3758544921875
INFO:root:current mean train loss 7370.052412776105
INFO:root:current train perplexity334.6976623535156
INFO:root:current mean train loss 7366.908859610159
INFO:root:current train perplexity334.37799072265625
INFO:root:current mean train loss 7367.1508502249735
INFO:root:current train perplexity334.15533447265625
INFO:root:current mean train loss 7366.447679256736
INFO:root:current train perplexity334.41180419921875
INFO:root:current mean train loss 7367.604688478737
INFO:root:current train perplexity334.67694091796875
INFO:root:current mean train loss 7373.887640515406
INFO:root:current train perplexity335.2193298339844
INFO:root:current mean train loss 7367.437001935337
INFO:root:current train perplexity335.3155517578125
INFO:root:current mean train loss 7368.978898750522
INFO:root:current train perplexity335.0935363769531
slurmstepd: error: *** JOB 29972143 ON ga016 CANCELLED AT 2023-02-10T04:56:00 ***
