INFO:root:Output: small_topk_10
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'cls.predictions.decoder.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.bias', 'cls.predictions.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 97912.87665719698
INFO:root:current train perplexity15298.2578125
INFO:root:current mean train loss 81539.22974246231
INFO:root:current train perplexity3079.819091796875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.78s/it]
INFO:root:final mean train loss: 75159.27287046371
INFO:root:final train perplexity: 1657.6014404296875
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.80s/it]
INFO:root:eval mean loss: 44215.057291666664
INFO:root:eval perplexity: 97.1317138671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/1
  0%|          | 1/200 [10:00<33:10:00, 600.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42958.06740196078
INFO:root:current train perplexity69.90116119384766
INFO:root:current mean train loss 39161.93941432119
INFO:root:current train perplexity47.48564147949219

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:20<00:00, 500.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:20<00:00, 500.48s/it]
INFO:root:final mean train loss: 36561.021090599796
INFO:root:final train perplexity: 36.82200241088867
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.28s/it]
INFO:root:eval mean loss: 31779.553803943454
INFO:root:eval perplexity: 26.81728744506836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/2
  1%|          | 2/200 [19:51<32:44:06, 595.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31214.220703125
INFO:root:current train perplexity22.06040382385254
INFO:root:current mean train loss 29734.158260012136
INFO:root:current train perplexity18.733366012573242
INFO:root:current mean train loss 28827.798395166257
INFO:root:current train perplexity17.133014678955078

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.29s/it]
INFO:root:final mean train loss: 28442.573714717742
INFO:root:final train perplexity: 16.532941818237305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.64s/it]
INFO:root:eval mean loss: 28549.864304315477
INFO:root:eval perplexity: 19.197647094726562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/3
  2%|â–         | 3/200 [29:51<32:41:20, 597.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26397.66356534091
INFO:root:current train perplexity13.438523292541504
INFO:root:current mean train loss 25916.3947202621
INFO:root:current train perplexity12.855594635009766

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.07s/it]
INFO:root:final mean train loss: 25546.56261813256
INFO:root:final train perplexity: 12.425049781799316
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.93s/it]
INFO:root:eval mean loss: 27137.817289806546
INFO:root:eval perplexity: 16.587459564208984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/4
  2%|â–         | 4/200 [40:29<33:23:08, 613.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24881.73604910714
INFO:root:current train perplexity11.3196439743042
INFO:root:current mean train loss 24407.85824328271
INFO:root:current train perplexity11.05038833618164
INFO:root:current mean train loss 24125.96356053744
INFO:root:current train perplexity10.792285919189453

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.51s/it]
INFO:root:final mean train loss: 24012.908368510583
INFO:root:final train perplexity: 10.680788040161133
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.36s/it]
INFO:root:eval mean loss: 26331.1689453125
INFO:root:eval perplexity: 15.258890151977539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/5
  2%|â–Ž         | 5/200 [50:33<33:02:24, 609.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23370.85881223517
INFO:root:current train perplexity10.018960952758789
INFO:root:current mean train loss 23143.884188286163
INFO:root:current train perplexity9.788355827331543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.42s/it]
INFO:root:final mean train loss: 22994.9378937752
INFO:root:final train perplexity: 9.660470008850098
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.35s/it]
INFO:root:eval mean loss: 25768.506928943454
INFO:root:eval perplexity: 14.395695686340332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/6
  3%|â–Ž         | 6/200 [1:00:49<32:58:23, 611.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22383.766157670456
INFO:root:current train perplexity9.14881420135498
INFO:root:current mean train loss 22419.982826576575
INFO:root:current train perplexity9.119842529296875
INFO:root:current mean train loss 22298.192072719194
INFO:root:current train perplexity9.00794506072998

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.60s/it]
INFO:root:final mean train loss: 22249.67806514617
INFO:root:final train perplexity: 8.975831031799316
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.61s/it]
INFO:root:eval mean loss: 25327.205264136905
INFO:root:eval perplexity: 13.752991676330566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/7
  4%|â–Ž         | 7/200 [1:10:48<32:34:50, 607.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21885.026165674604
INFO:root:current train perplexity8.645076751708984
INFO:root:current mean train loss 21789.05708397239
INFO:root:current train perplexity8.548179626464844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.95s/it]
INFO:root:final mean train loss: 21671.060255481352
INFO:root:final train perplexity: 8.47791862487793
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.25s/it]
INFO:root:eval mean loss: 25028.723074776786
INFO:root:eval perplexity: 13.334636688232422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/8
  4%|â–         | 8/200 [1:20:54<32:23:37, 607.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21437.746875
INFO:root:current train perplexity8.208579063415527
INFO:root:current mean train loss 21360.86980298913
INFO:root:current train perplexity8.190814018249512
INFO:root:current mean train loss 21242.42945130814
INFO:root:current train perplexity8.114130020141602

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:20<00:00, 500.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:20<00:00, 500.87s/it]
INFO:root:final mean train loss: 21204.84274981099
INFO:root:final train perplexity: 8.09689712524414
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.79s/it]
INFO:root:eval mean loss: 24725.848539806546
INFO:root:eval perplexity: 12.923128128051758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/9
  4%|â–         | 9/200 [1:30:58<32:09:50, 606.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20979.962628264926
INFO:root:current train perplexity7.880105495452881
INFO:root:current mean train loss 20910.45442318488
INFO:root:current train perplexity7.843679904937744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:22<00:00, 502.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:22<00:00, 502.72s/it]
INFO:root:final mean train loss: 20815.90920331401
INFO:root:final train perplexity: 7.792171955108643
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.21s/it]
INFO:root:eval mean loss: 24484.551083519345
INFO:root:eval perplexity: 12.60439395904541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/10
  5%|â–Œ         | 10/200 [1:41:01<31:56:27, 605.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20772.892064144737
INFO:root:current train perplexity7.626833438873291
INFO:root:current mean train loss 20604.168658088234
INFO:root:current train perplexity7.583808422088623
INFO:root:current mean train loss 20514.25579694635
INFO:root:current train perplexity7.5517497062683105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.26s/it]
INFO:root:final mean train loss: 20486.68666125882
INFO:root:final train perplexity: 7.54320764541626
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.96s/it]
INFO:root:eval mean loss: 24282.54162016369
INFO:root:eval perplexity: 12.343606948852539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/11
  6%|â–Œ         | 11/200 [1:51:07<31:47:28, 605.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20302.033175616198
INFO:root:current train perplexity7.359244346618652
INFO:root:current mean train loss 20244.622612847223
INFO:root:current train perplexity7.350895404815674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.06s/it]
INFO:root:final mean train loss: 20194.40502142137
INFO:root:final train perplexity: 7.328854560852051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.70s/it]
INFO:root:eval mean loss: 24107.529715401786
INFO:root:eval perplexity: 12.122041702270508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/12
  6%|â–Œ         | 12/200 [2:01:14<31:38:28, 605.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20074.475373641304
INFO:root:current train perplexity7.213304042816162
INFO:root:current mean train loss 19979.79309578252
INFO:root:current train perplexity7.180319309234619
INFO:root:current mean train loss 19948.877347253365
INFO:root:current train perplexity7.150223731994629

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.34s/it]
INFO:root:final mean train loss: 19943.04181105091
INFO:root:final train perplexity: 7.149388313293457
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.77s/it]
INFO:root:eval mean loss: 23968.36111886161
INFO:root:eval perplexity: 11.948695182800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/13
  6%|â–‹         | 13/200 [2:11:41<31:48:27, 612.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19816.445494791667
INFO:root:current train perplexity7.016193389892578
INFO:root:current mean train loss 19754.336629464287
INFO:root:current train perplexity7.008006572723389

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.04s/it]
INFO:root:final mean train loss: 19715.31978484123
INFO:root:final train perplexity: 6.990598201751709
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.81s/it]
INFO:root:eval mean loss: 23814.579985119046
INFO:root:eval perplexity: 11.760028839111328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/14
  7%|â–‹         | 14/200 [2:21:55<31:39:40, 612.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19514.97822627315
INFO:root:current train perplexity6.88165283203125
INFO:root:current mean train loss 19480.92681163878
INFO:root:current train perplexity6.85984468460083
INFO:root:current mean train loss 19528.847114193282
INFO:root:current train perplexity6.855700969696045

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.44s/it]
INFO:root:final mean train loss: 19508.367849042337
INFO:root:final train perplexity: 6.84935188293457
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.92s/it]
INFO:root:eval mean loss: 23709.155598958332
INFO:root:eval perplexity: 11.632412910461426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/15
  8%|â–Š         | 15/200 [2:32:06<31:27:22, 612.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19328.487242879746
INFO:root:current train perplexity6.736505031585693
INFO:root:current mean train loss 19333.072658432262
INFO:root:current train perplexity6.729516506195068

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.56s/it]
INFO:root:final mean train loss: 19318.396610383064
INFO:root:final train perplexity: 6.722207546234131
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.84s/it]
INFO:root:eval mean loss: 23604.658784412204
INFO:root:eval perplexity: 11.507285118103027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/16
  8%|â–Š         | 16/200 [2:42:23<31:21:48, 613.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19120.05941280242
INFO:root:current train perplexity6.6551690101623535
INFO:root:current mean train loss 19156.029833611643
INFO:root:current train perplexity6.61224365234375
INFO:root:current mean train loss 19151.40564968885
INFO:root:current train perplexity6.608212947845459

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.33s/it]
INFO:root:final mean train loss: 19148.300966324347
INFO:root:final train perplexity: 6.610370635986328
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.83s/it]
INFO:root:eval mean loss: 23487.00027901786
INFO:root:eval perplexity: 11.36800765991211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/17
  8%|â–Š         | 17/200 [2:52:32<31:07:35, 612.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18997.086643448794
INFO:root:current train perplexity6.510045528411865
INFO:root:current mean train loss 19007.407669484288
INFO:root:current train perplexity6.502359867095947

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.61s/it]
INFO:root:final mean train loss: 18988.967296969506
INFO:root:final train perplexity: 6.507297992706299
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.67s/it]
INFO:root:eval mean loss: 23423.93396577381
INFO:root:eval perplexity: 11.294053077697754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/18
  9%|â–‰         | 18/200 [3:02:41<30:54:21, 611.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18925.188783482143
INFO:root:current train perplexity6.456560134887695
INFO:root:current mean train loss 18920.24800347222
INFO:root:current train perplexity6.445659637451172
INFO:root:current mean train loss 18849.444074135637
INFO:root:current train perplexity6.414247035980225

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:51<00:00, 531.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:51<00:00, 531.63s/it]
INFO:root:final mean train loss: 18849.236966040826
INFO:root:final train perplexity: 6.418228626251221
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:43<00:00, 103.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:43<00:00, 103.20s/it]
INFO:root:eval mean loss: 23343.72314453125
INFO:root:eval perplexity: 11.200684547424316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/19
 10%|â–‰         | 19/200 [3:13:20<31:09:20, 619.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18707.352819683907
INFO:root:current train perplexity6.314146041870117
INFO:root:current mean train loss 18738.264705882353
INFO:root:current train perplexity6.3287577629089355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.34s/it]
INFO:root:final mean train loss: 18707.832413211945
INFO:root:final train perplexity: 6.329335689544678
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.42s/it]
INFO:root:eval mean loss: 23246.448056175595
INFO:root:eval perplexity: 11.088484764099121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/20
 10%|â–ˆ         | 20/200 [3:24:12<31:27:40, 629.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18725.93444511218
INFO:root:current train perplexity6.27377986907959
INFO:root:current mean train loss 18615.562289231115
INFO:root:current train perplexity6.257636547088623
INFO:root:current mean train loss 18604.212155138597
INFO:root:current train perplexity6.254141330718994

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.78s/it]
INFO:root:final mean train loss: 18582.049891318045
INFO:root:final train perplexity: 6.2512969970703125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:42<00:00, 102.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:42<00:00, 102.75s/it]
INFO:root:eval mean loss: 23203.530412946428
INFO:root:eval perplexity: 11.039341926574707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/21
 10%|â–ˆ         | 21/200 [3:34:39<31:15:44, 628.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18456.904876373625
INFO:root:current train perplexity6.177357196807861
INFO:root:current mean train loss 18473.233137679974
INFO:root:current train perplexity6.170708179473877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.91s/it]
INFO:root:final mean train loss: 18461.075010238153
INFO:root:final train perplexity: 6.177150249481201
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.50s/it]
INFO:root:eval mean loss: 23127.500558035714
INFO:root:eval perplexity: 10.9528169631958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/22
 11%|â–ˆ         | 22/200 [3:44:58<30:55:59, 625.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18391.835347020347
INFO:root:current train perplexity6.1271538734436035
INFO:root:current mean train loss 18364.95435423951
INFO:root:current train perplexity6.112368106842041
INFO:root:current mean train loss 18366.290268132718
INFO:root:current train perplexity6.107782363891602

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.62s/it]
INFO:root:final mean train loss: 18343.54201187626
INFO:root:final train perplexity: 6.105955123901367
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.53s/it]
INFO:root:eval mean loss: 23088.04296875
INFO:root:eval perplexity: 10.908180236816406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/23
 12%|â–ˆâ–        | 23/200 [3:55:15<30:38:32, 623.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18298.157997532893
INFO:root:current train perplexity6.04849910736084
INFO:root:current mean train loss 18264.329747596155
INFO:root:current train perplexity6.04340124130249

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.28s/it]
INFO:root:final mean train loss: 18243.40685247606
INFO:root:final train perplexity: 6.0459465980529785
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.48s/it]
INFO:root:eval mean loss: 23005.16052827381
INFO:root:eval perplexity: 10.815007209777832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/24
 12%|â–ˆâ–        | 24/200 [4:05:34<30:23:57, 621.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18089.213555518618
INFO:root:current train perplexity5.97343111038208
INFO:root:current mean train loss 18141.818744685374
INFO:root:current train perplexity5.985158443450928
INFO:root:current mean train loss 18154.388395116395
INFO:root:current train perplexity5.985854625701904

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.04s/it]
INFO:root:final mean train loss: 18141.00613501764
INFO:root:final train perplexity: 5.985188961029053
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:42<00:00, 102.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:42<00:00, 102.77s/it]
INFO:root:eval mean loss: 23001.55343191964
INFO:root:eval perplexity: 10.810975074768066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/25
 12%|â–ˆâ–Ž        | 25/200 [4:15:49<30:07:37, 619.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18093.89131549874
INFO:root:current train perplexity5.9456400871276855
INFO:root:current mean train loss 18076.702202418342
INFO:root:current train perplexity5.929144382476807

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.06s/it]
INFO:root:final mean train loss: 18046.7308625252
INFO:root:final train perplexity: 5.929792404174805
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.76s/it]
INFO:root:eval mean loss: 22944.47130766369
INFO:root:eval perplexity: 10.747294425964355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/26
 13%|â–ˆâ–Ž        | 26/200 [4:26:38<30:23:14, 628.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17982.478898590685
INFO:root:current train perplexity5.9022216796875
INFO:root:current mean train loss 17987.812435326985
INFO:root:current train perplexity5.887851715087891

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.92s/it]
INFO:root:final mean train loss: 17956.969387915826
INFO:root:final train perplexity: 5.877526760101318
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.38s/it]
INFO:root:eval mean loss: 22893.76355561756
INFO:root:eval perplexity: 10.6910400390625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/27
 14%|â–ˆâ–Ž        | 27/200 [4:37:34<30:36:25, 636.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18063.96484375
INFO:root:current train perplexity5.840439319610596
INFO:root:current mean train loss 17909.28695767597
INFO:root:current train perplexity5.846006870269775
INFO:root:current mean train loss 17903.619823737685
INFO:root:current train perplexity5.832284450531006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.79s/it]
INFO:root:final mean train loss: 17876.293775989165
INFO:root:final train perplexity: 5.830943584442139
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.05s/it]
INFO:root:eval mean loss: 22842.93115234375
INFO:root:eval perplexity: 10.634940147399902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/28
 14%|â–ˆâ–        | 28/200 [4:48:11<30:25:27, 636.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17868.44776278409
INFO:root:current train perplexity5.8102946281433105
INFO:root:current mean train loss 17831.820829133063
INFO:root:current train perplexity5.791284561157227

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.61s/it]
INFO:root:final mean train loss: 17793.157179309477
INFO:root:final train perplexity: 5.783324241638184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.49s/it]
INFO:root:eval mean loss: 22825.084937686013
INFO:root:eval perplexity: 10.615317344665527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/29
 14%|â–ˆâ–        | 29/200 [4:58:35<30:04:04, 633.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17813.092912946428
INFO:root:current train perplexity5.736204147338867
INFO:root:current mean train loss 17824.626588054907
INFO:root:current train perplexity5.7484235763549805
INFO:root:current mean train loss 17758.311094127417
INFO:root:current train perplexity5.739163875579834

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.69s/it]
INFO:root:final mean train loss: 17712.84876669607
INFO:root:final train perplexity: 5.737696170806885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.74s/it]
INFO:root:eval mean loss: 22786.614885602678
INFO:root:eval perplexity: 10.573135375976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/30
 15%|â–ˆâ–Œ        | 30/200 [5:08:54<29:41:41, 628.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17667.62284825212
INFO:root:current train perplexity5.7038140296936035
INFO:root:current mean train loss 17664.501989976416
INFO:root:current train perplexity5.701831340789795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.99s/it]
INFO:root:final mean train loss: 17641.059928647934
INFO:root:final train perplexity: 5.697213649749756
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.33s/it]
INFO:root:eval mean loss: 22750.941010974704
INFO:root:eval perplexity: 10.534170150756836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/31
 16%|â–ˆâ–Œ        | 31/200 [5:19:23<29:31:34, 628.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17493.509410511364
INFO:root:current train perplexity5.623810768127441
INFO:root:current mean train loss 17578.801414695947
INFO:root:current train perplexity5.64193868637085
INFO:root:current mean train loss 17581.414303169433
INFO:root:current train perplexity5.65936279296875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.69s/it]
INFO:root:final mean train loss: 17569.685952463456
INFO:root:final train perplexity: 5.6572465896606445
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.94s/it]
INFO:root:eval mean loss: 22735.9140625
INFO:root:eval perplexity: 10.517800331115723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/32
 16%|â–ˆâ–Œ        | 32/200 [5:29:36<29:07:01, 623.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17502.68777901786
INFO:root:current train perplexity5.601919651031494
INFO:root:current mean train loss 17525.336932036043
INFO:root:current train perplexity5.623326301574707

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.46s/it]
INFO:root:final mean train loss: 17504.643727948587
INFO:root:final train perplexity: 5.621070384979248
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.75s/it]
INFO:root:eval mean loss: 22700.892531622023
INFO:root:eval perplexity: 10.479747772216797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/33
 16%|â–ˆâ–‹        | 33/200 [5:39:49<28:48:14, 620.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17276.251041666666
INFO:root:current train perplexity5.53264856338501
INFO:root:current mean train loss 17439.643461277174
INFO:root:current train perplexity5.585267066955566
INFO:root:current mean train loss 17423.259429505815
INFO:root:current train perplexity5.573847770690918

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.71s/it]
INFO:root:final mean train loss: 17437.0393578314
INFO:root:final train perplexity: 5.583713054656982
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.89s/it]
INFO:root:eval mean loss: 22640.352353050595
INFO:root:eval perplexity: 10.414288520812988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/34
 17%|â–ˆâ–‹        | 34/200 [5:49:59<28:28:15, 617.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17370.411366021453
INFO:root:current train perplexity5.545406818389893
INFO:root:current mean train loss 17395.17041892777
INFO:root:current train perplexity5.549534320831299

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.59s/it]
INFO:root:final mean train loss: 17373.252331149193
INFO:root:final train perplexity: 5.548694133758545
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.78s/it]
INFO:root:eval mean loss: 22630.281319754464
INFO:root:eval perplexity: 10.403438568115234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/35
 18%|â–ˆâ–Š        | 35/200 [6:00:21<28:22:01, 618.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17521.66097861842
INFO:root:current train perplexity5.580424785614014
INFO:root:current mean train loss 17365.695903361346
INFO:root:current train perplexity5.532711505889893
INFO:root:current mean train loss 17335.05966395548
INFO:root:current train perplexity5.517558574676514

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.92s/it]
INFO:root:final mean train loss: 17317.497200258316
INFO:root:final train perplexity: 5.518263816833496
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.64s/it]
INFO:root:eval mean loss: 22615.84228515625
INFO:root:eval perplexity: 10.387906074523926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/36
 18%|â–ˆâ–Š        | 36/200 [6:10:34<28:06:49, 617.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17225.822939590667
INFO:root:current train perplexity5.467711448669434
INFO:root:current mean train loss 17250.43844229715
INFO:root:current train perplexity5.485037326812744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.79s/it]
INFO:root:final mean train loss: 17256.962768554688
INFO:root:final train perplexity: 5.485415935516357
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.56s/it]
INFO:root:eval mean loss: 22588.447358630954
INFO:root:eval perplexity: 10.358497619628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/37
 18%|â–ˆâ–Š        | 37/200 [6:20:44<27:50:59, 615.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17156.585767663044
INFO:root:current train perplexity5.432734966278076
INFO:root:current mean train loss 17152.680370299797
INFO:root:current train perplexity5.445554256439209
INFO:root:current mean train loss 17217.791365961322
INFO:root:current train perplexity5.457258701324463

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.63s/it]
INFO:root:final mean train loss: 17200.263014270415
INFO:root:final train perplexity: 5.4548234939575195
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.90s/it]
INFO:root:eval mean loss: 22558.629975818454
INFO:root:eval perplexity: 10.326577186584473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/38
 19%|â–ˆâ–‰        | 38/200 [6:31:03<27:43:53, 616.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17166.110677083332
INFO:root:current train perplexity5.418478488922119
INFO:root:current mean train loss 17151.620256696428
INFO:root:current train perplexity5.421065807342529

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.99s/it]
INFO:root:final mean train loss: 17148.869778540826
INFO:root:final train perplexity: 5.427243232727051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.41s/it]
INFO:root:eval mean loss: 22549.12560453869
INFO:root:eval perplexity: 10.316423416137695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/39
 20%|â–ˆâ–‰        | 39/200 [6:41:14<27:28:58, 614.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17024.306785300927
INFO:root:current train perplexity5.366212844848633
INFO:root:current mean train loss 17100.0135257751
INFO:root:current train perplexity5.39837121963501
INFO:root:current mean train loss 17101.988900743392
INFO:root:current train perplexity5.395882606506348

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.93s/it]
INFO:root:final mean train loss: 17092.686606130294
INFO:root:final train perplexity: 5.397252082824707
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.39s/it]
INFO:root:eval mean loss: 22537.283063616072
INFO:root:eval perplexity: 10.303789138793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/40
 20%|â–ˆâ–ˆ        | 40/200 [6:51:24<27:15:12, 613.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17085.49587124209
INFO:root:current train perplexity5.379667282104492
INFO:root:current mean train loss 17101.31562609113
INFO:root:current train perplexity5.386573314666748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:02<00:00, 542.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:02<00:00, 542.93s/it]
INFO:root:final mean train loss: 17046.427935200354
INFO:root:final train perplexity: 5.372682094573975
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.82s/it]
INFO:root:eval mean loss: 22530.07203311012
INFO:root:eval perplexity: 10.296102523803711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/41
 20%|â–ˆâ–ˆ        | 41/200 [7:02:10<27:30:46, 622.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16913.118636592742
INFO:root:current train perplexity5.311096668243408
INFO:root:current mean train loss 16970.046927182728
INFO:root:current train perplexity5.3289289474487305
INFO:root:current mean train loss 17015.27071496212
INFO:root:current train perplexity5.345326900482178

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.24s/it]
INFO:root:final mean train loss: 16994.53156108241
INFO:root:final train perplexity: 5.345251560211182
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:41<00:00, 101.01s/it]
INFO:root:eval mean loss: 22503.966169084822
INFO:root:eval perplexity: 10.268320083618164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/42
 21%|â–ˆâ–ˆ        | 42/200 [7:12:31<27:18:50, 622.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16948.569971291417
INFO:root:current train perplexity5.323326110839844
INFO:root:current mean train loss 16981.436619492826
INFO:root:current train perplexity5.326751708984375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.17s/it]
INFO:root:final mean train loss: 16954.030340379286
INFO:root:final train perplexity: 5.323941707611084
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.83s/it]
INFO:root:eval mean loss: 22498.089378720237
INFO:root:eval perplexity: 10.262076377868652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/43
 22%|â–ˆâ–ˆâ–       | 43/200 [7:22:48<27:04:12, 620.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17038.311746651787
INFO:root:current train perplexity5.336396217346191
INFO:root:current mean train loss 16946.87863136574
INFO:root:current train perplexity5.298099994659424
INFO:root:current mean train loss 16916.708140791223
INFO:root:current train perplexity5.297065734863281

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.47s/it]
INFO:root:final mean train loss: 16900.541066815775
INFO:root:final train perplexity: 5.295928001403809
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.26s/it]
INFO:root:eval mean loss: 22490.36755952381
INFO:root:eval perplexity: 10.253878593444824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/44
 22%|â–ˆâ–ˆâ–       | 44/200 [7:32:57<26:45:07, 617.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16848.33783450072
INFO:root:current train perplexity5.262519359588623
INFO:root:current mean train loss 16866.9474327373
INFO:root:current train perplexity5.269024848937988

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.57s/it]
INFO:root:final mean train loss: 16859.639636624244
INFO:root:final train perplexity: 5.274605751037598
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.75s/it]
INFO:root:eval mean loss: 22463.650855654763
INFO:root:eval perplexity: 10.225565910339355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [7:43:10<26:31:26, 616.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16858.882787459937
INFO:root:current train perplexity5.2659149169921875
INFO:root:current mean train loss 16823.44124466052
INFO:root:current train perplexity5.25430154800415
INFO:root:current mean train loss 16833.69526755361
INFO:root:current train perplexity5.253757476806641

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.34s/it]
INFO:root:final mean train loss: 16817.44902580015
INFO:root:final train perplexity: 5.252702236175537
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.57s/it]
INFO:root:eval mean loss: 22439.455636160714
INFO:root:eval perplexity: 10.199992179870605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [7:53:16<26:13:17, 612.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16818.95477764423
INFO:root:current train perplexity5.230538368225098
INFO:root:current mean train loss 16807.146821825918
INFO:root:current train perplexity5.235276699066162

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.19s/it]
INFO:root:final mean train loss: 16775.669555664062
INFO:root:final train perplexity: 5.231100559234619
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.28s/it]
INFO:root:eval mean loss: 22444.07214936756
INFO:root:eval perplexity: 10.204867362976074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [8:04:00<26:26:57, 622.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16744.280182594477
INFO:root:current train perplexity5.210370063781738
INFO:root:current mean train loss 16748.558409364072
INFO:root:current train perplexity5.209500312805176
INFO:root:current mean train loss 16745.133744855968
INFO:root:current train perplexity5.210276126861572

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.97s/it]
INFO:root:final mean train loss: 16733.932113155242
INFO:root:final train perplexity: 5.209610939025879
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:40<00:00, 100.56s/it]
INFO:root:eval mean loss: 22440.60181826637
INFO:root:eval perplexity: 10.201203346252441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/48
 24%|â–ˆâ–ˆâ–       | 48/200 [8:14:34<26:25:33, 625.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16690.275010279605
INFO:root:current train perplexity5.179048538208008
INFO:root:current mean train loss 16698.51015625
INFO:root:current train perplexity5.182951927185059

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.38s/it]
INFO:root:final mean train loss: 16698.174887380294
INFO:root:final train perplexity: 5.191269397735596
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.51s/it]
INFO:root:eval mean loss: 22415.414713541668
INFO:root:eval perplexity: 10.174642562866211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/49
 24%|â–ˆâ–ˆâ–       | 49/200 [8:24:54<26:10:33, 624.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16677.37169630984
INFO:root:current train perplexity5.146255016326904
INFO:root:current mean train loss 16665.494439572703
INFO:root:current train perplexity5.167400360107422
INFO:root:current mean train loss 16666.208403181932
INFO:root:current train perplexity5.1690897941589355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.62s/it]
INFO:root:final mean train loss: 16655.859926285284
INFO:root:final train perplexity: 5.16964864730835
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.85s/it]
INFO:root:eval mean loss: 22419.57763671875
INFO:root:eval perplexity: 10.179032325744629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [8:35:04<25:49:47, 619.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16639.799469302397
INFO:root:current train perplexity5.140417575836182
INFO:root:current mean train loss 16611.040991088255
INFO:root:current train perplexity5.150664329528809

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.43s/it]
INFO:root:final mean train loss: 16621.39367282006
INFO:root:final train perplexity: 5.152103900909424
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.98s/it]
INFO:root:eval mean loss: 22396.76097470238
INFO:root:eval perplexity: 10.155019760131836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [8:45:25<25:39:48, 620.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16591.87113204657
INFO:root:current train perplexity5.126504898071289
INFO:root:current mean train loss 16603.97148566846
INFO:root:current train perplexity5.131397724151611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.18s/it]
INFO:root:final mean train loss: 16582.046882875504
INFO:root:final train perplexity: 5.1321492195129395
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.40s/it]
INFO:root:eval mean loss: 22406.08272879464
INFO:root:eval perplexity: 10.164822578430176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [8:55:41<25:26:48, 618.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16673.329427083332
INFO:root:current train perplexity5.24329137802124
INFO:root:current mean train loss 16520.023134101943
INFO:root:current train perplexity5.099847793579102
INFO:root:current mean train loss 16556.80092556958
INFO:root:current train perplexity5.113687515258789

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.62s/it]
INFO:root:final mean train loss: 16547.32736107611
INFO:root:final train perplexity: 5.114603042602539
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.14s/it]
INFO:root:eval mean loss: 22367.526041666668
INFO:root:eval perplexity: 10.12433910369873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [9:05:55<25:12:44, 617.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16506.763405539772
INFO:root:current train perplexity5.105229377746582
INFO:root:current mean train loss 16555.341437752017
INFO:root:current train perplexity5.110553741455078

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.39s/it]
INFO:root:final mean train loss: 16513.57302561114
INFO:root:final train perplexity: 5.097603797912598
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.50s/it]
INFO:root:eval mean loss: 22371.948800223214
INFO:root:eval perplexity: 10.128974914550781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [9:16:07<24:58:18, 615.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16353.505161830357
INFO:root:current train perplexity5.081920623779297
INFO:root:current mean train loss 16471.168507228387
INFO:root:current train perplexity5.079310894012451
INFO:root:current mean train loss 16474.33514020984
INFO:root:current train perplexity5.080782413482666

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.17s/it]
INFO:root:final mean train loss: 16475.299399099044
INFO:root:final train perplexity: 5.078397274017334
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.75s/it]
INFO:root:eval mean loss: 22364.058175223214
INFO:root:eval perplexity: 10.120707511901855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [9:26:27<24:51:04, 616.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16484.516585010595
INFO:root:current train perplexity5.0620527267456055
INFO:root:current mean train loss 16433.30305375393
INFO:root:current train perplexity5.059739589691162

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.45s/it]
INFO:root:final mean train loss: 16450.239403509324
INFO:root:final train perplexity: 5.065859317779541
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.12s/it]
INFO:root:eval mean loss: 22361.86397879464
INFO:root:eval perplexity: 10.118412971496582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [9:36:36<24:35:16, 614.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16564.31756036932
INFO:root:current train perplexity5.093014717102051
INFO:root:current mean train loss 16409.000140765766
INFO:root:current train perplexity5.043792247772217
INFO:root:current mean train loss 16443.961904805983
INFO:root:current train perplexity5.052460670471191

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.65s/it]
INFO:root:final mean train loss: 16415.879638671875
INFO:root:final train perplexity: 5.048720359802246
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.09s/it]
INFO:root:eval mean loss: 22354.866536458332
INFO:root:eval perplexity: 10.11108684539795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [9:46:42<24:19:02, 612.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16353.091719370039
INFO:root:current train perplexity5.007543563842773
INFO:root:current mean train loss 16382.516044382668
INFO:root:current train perplexity5.0281476974487305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.36s/it]
INFO:root:final mean train loss: 16382.262392105595
INFO:root:final train perplexity: 5.032008647918701
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.06s/it]
INFO:root:eval mean loss: 22353.08863467262
INFO:root:eval perplexity: 10.109223365783691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [9:56:48<24:04:31, 610.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16367.984244791667
INFO:root:current train perplexity5.031342029571533
INFO:root:current mean train loss 16302.949864130434
INFO:root:current train perplexity4.993777751922607
INFO:root:current mean train loss 16334.571030159883
INFO:root:current train perplexity5.010092258453369

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.65s/it]
INFO:root:final mean train loss: 16351.139341292843
INFO:root:final train perplexity: 5.016584396362305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.83s/it]
INFO:root:eval mean loss: 22337.60816592262
INFO:root:eval perplexity: 10.09304141998291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [10:06:56<23:52:10, 609.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16328.405448344216
INFO:root:current train perplexity4.984291076660156
INFO:root:current mean train loss 16322.380917851797
INFO:root:current train perplexity5.001117706298828

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.45s/it]
INFO:root:final mean train loss: 16319.348916330646
INFO:root:final train perplexity: 5.000879764556885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.53s/it]
INFO:root:eval mean loss: 22328.976585751487
INFO:root:eval perplexity: 10.084029197692871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [10:16:57<23:36:23, 607.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16269.518606085527
INFO:root:current train perplexity4.984658718109131
INFO:root:current mean train loss 16266.415129332983
INFO:root:current train perplexity4.980879783630371
INFO:root:current mean train loss 16287.608911244291
INFO:root:current train perplexity4.982485294342041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.69s/it]
INFO:root:final mean train loss: 16289.076849168347
INFO:root:final train perplexity: 4.985970497131348
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.20s/it]
INFO:root:eval mean loss: 22328.076962425595
INFO:root:eval perplexity: 10.083088874816895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [10:27:07<23:28:00, 607.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16285.141560299297
INFO:root:current train perplexity4.973869323730469
INFO:root:current mean train loss 16295.271221673976
INFO:root:current train perplexity4.977780342102051

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.99s/it]
INFO:root:final mean train loss: 16264.512537802419
INFO:root:final train perplexity: 4.973905086517334
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.49s/it]
INFO:root:eval mean loss: 22322.062453497023
INFO:root:eval perplexity: 10.076814651489258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [10:37:17<23:19:25, 608.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16151.486158288044
INFO:root:current train perplexity4.921940803527832
INFO:root:current mean train loss 16245.166150597053
INFO:root:current train perplexity4.950762748718262
INFO:root:current mean train loss 16245.717974880885
INFO:root:current train perplexity4.959199905395508

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.64s/it]
INFO:root:final mean train loss: 16231.018909085182
INFO:root:final train perplexity: 4.95750093460083
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.86s/it]
INFO:root:eval mean loss: 22321.481282552082
INFO:root:eval perplexity: 10.07620906829834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [10:47:35<23:15:53, 611.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16197.6323046875
INFO:root:current train perplexity4.948038101196289
INFO:root:current mean train loss 16229.670859375
INFO:root:current train perplexity4.942532062530518

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.34s/it]
INFO:root:final mean train loss: 16205.931286227318
INFO:root:final train perplexity: 4.945248126983643
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.27s/it]
INFO:root:eval mean loss: 22311.18677920387
INFO:root:eval perplexity: 10.065478324890137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [10:57:45<23:04:51, 610.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16292.172019675925
INFO:root:current train perplexity4.945906639099121
INFO:root:current mean train loss 16163.375169168306
INFO:root:current train perplexity4.9213690757751465
INFO:root:current mean train loss 16184.146170326267
INFO:root:current train perplexity4.929601192474365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.29s/it]
INFO:root:final mean train loss: 16178.760277532761
INFO:root:final train perplexity: 4.932012557983398
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.89s/it]
INFO:root:eval mean loss: 22326.190848214286
INFO:root:eval perplexity: 10.081123352050781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [11:07:57<22:55:13, 611.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16170.26093997231
INFO:root:current train perplexity4.9135541915893555
INFO:root:current mean train loss 16164.332102173534
INFO:root:current train perplexity4.917275428771973

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.11s/it]
INFO:root:final mean train loss: 16153.961910124748
INFO:root:final train perplexity: 4.91996431350708
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.71s/it]
INFO:root:eval mean loss: 22319.765043712796
INFO:root:eval perplexity: 10.074417114257812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [11:18:04<22:42:20, 610.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16075.54142515121
INFO:root:current train perplexity4.8734259605407715
INFO:root:current mean train loss 16133.900226622138
INFO:root:current train perplexity4.8973469734191895
INFO:root:current mean train loss 16132.81844815341
INFO:root:current train perplexity4.904143333435059

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.80s/it]
INFO:root:final mean train loss: 16123.156498078377
INFO:root:final train perplexity: 4.905038356781006
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.06s/it]
INFO:root:eval mean loss: 22307.473330543155
INFO:root:eval perplexity: 10.06161117553711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [11:28:16<22:33:54, 610.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16100.271484375
INFO:root:current train perplexity4.884284973144531
INFO:root:current mean train loss 16112.94962431694
INFO:root:current train perplexity4.888554096221924

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.81s/it]
INFO:root:final mean train loss: 16101.007158833165
INFO:root:final train perplexity: 4.894334316253662
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.57s/it]
INFO:root:eval mean loss: 22301.767996651786
INFO:root:eval perplexity: 10.055672645568848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [11:38:56<22:43:03, 619.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16156.817466517858
INFO:root:current train perplexity4.900620937347412
INFO:root:current mean train loss 16098.506293402777
INFO:root:current train perplexity4.881600856781006
INFO:root:current mean train loss 16092.160210272606
INFO:root:current train perplexity4.88129186630249

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.41s/it]
INFO:root:final mean train loss: 16075.97177419355
INFO:root:final train perplexity: 4.882263660430908
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.82s/it]
INFO:root:eval mean loss: 22301.717145647322
INFO:root:eval perplexity: 10.055620193481445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [11:49:13<22:30:48, 618.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16042.854290140087
INFO:root:current train perplexity4.870603084564209
INFO:root:current mean train loss 16028.611061789772
INFO:root:current train perplexity4.865670204162598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:45<00:00, 525.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:45<00:00, 525.16s/it]
INFO:root:final mean train loss: 16050.202172064011
INFO:root:final train perplexity: 4.869870662689209
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.16s/it]
INFO:root:eval mean loss: 22312.725399925595
INFO:root:eval perplexity: 10.067084312438965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [11:59:36<22:23:10, 619.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15924.112705328525
INFO:root:current train perplexity4.847175121307373
INFO:root:current mean train loss 15990.261276135341
INFO:root:current train perplexity4.843623161315918
INFO:root:current mean train loss 16032.96620848588
INFO:root:current train perplexity4.855259418487549

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.48s/it]
INFO:root:final mean train loss: 16025.97724766885
INFO:root:final train perplexity: 4.858248710632324
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.30s/it]
INFO:root:eval mean loss: 22288.410784040178
INFO:root:eval perplexity: 10.041779518127441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [12:09:47<22:07:10, 617.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15986.080045930632
INFO:root:current train perplexity4.83506441116333
INFO:root:current mean train loss 15994.642179319371
INFO:root:current train perplexity4.839651107788086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.16s/it]
INFO:root:final mean train loss: 16001.989072738155
INFO:root:final train perplexity: 4.846767902374268
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.92s/it]
INFO:root:eval mean loss: 22287.630673363095
INFO:root:eval perplexity: 10.04096794128418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [12:19:59<21:53:43, 615.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15995.60964752907
INFO:root:current train perplexity4.818729400634766
INFO:root:current mean train loss 15987.717070039336
INFO:root:current train perplexity4.833196640014648
INFO:root:current mean train loss 15992.997938368055
INFO:root:current train perplexity4.835872650146484

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.54s/it]
INFO:root:final mean train loss: 15978.80554592994
INFO:root:final train perplexity: 4.835696697235107
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.62s/it]
INFO:root:eval mean loss: 22287.23560732887
INFO:root:eval perplexity: 10.040558815002441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [12:30:07<21:38:26, 613.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15952.089740953947
INFO:root:current train perplexity4.819350242614746
INFO:root:current mean train loss 15959.180178285256
INFO:root:current train perplexity4.823570728302002

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.55s/it]
INFO:root:final mean train loss: 15955.246920677924
INFO:root:final train perplexity: 4.824472904205322
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.46s/it]
INFO:root:eval mean loss: 22287.09495907738
INFO:root:eval perplexity: 10.040412902832031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [12:40:16<21:25:07, 611.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15929.03478224734
INFO:root:current train perplexity4.800919055938721
INFO:root:current mean train loss 15928.138286564626
INFO:root:current train perplexity4.800083637237549
INFO:root:current mean train loss 15947.863854535679
INFO:root:current train perplexity4.814549922943115

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.95s/it]
INFO:root:final mean train loss: 15935.222427860383
INFO:root:final train perplexity: 4.8149542808532715
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.55s/it]
INFO:root:eval mean loss: 22307.78443545387
INFO:root:eval perplexity: 10.061934471130371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [12:50:59<21:34:19, 621.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15920.196782275883
INFO:root:current train perplexity4.801475524902344
INFO:root:current mean train loss 15922.394251531094
INFO:root:current train perplexity4.802926063537598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.37s/it]
INFO:root:final mean train loss: 15912.4894547001
INFO:root:final train perplexity: 4.80417013168335
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.90s/it]
INFO:root:eval mean loss: 22295.48558407738
INFO:root:eval perplexity: 10.049135208129883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [13:01:21<21:24:43, 621.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15886.953507965687
INFO:root:current train perplexity4.794983386993408
INFO:root:current mean train loss 15903.46405473924
INFO:root:current train perplexity4.790708541870117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.48s/it]
INFO:root:final mean train loss: 15889.811200541835
INFO:root:final train perplexity: 4.793436527252197
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.41s/it]
INFO:root:eval mean loss: 22299.52367001488
INFO:root:eval perplexity: 10.053338050842285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [13:11:39<21:11:47, 620.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15469.099283854166
INFO:root:current train perplexity4.65925931930542
INFO:root:current mean train loss 15875.123966550364
INFO:root:current train perplexity4.77571964263916
INFO:root:current mean train loss 15865.438572775554
INFO:root:current train perplexity4.777088642120361

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.85s/it]
INFO:root:final mean train loss: 15864.866183373237
INFO:root:final train perplexity: 4.781657695770264
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.64s/it]
INFO:root:eval mean loss: 22282.078218005954
INFO:root:eval perplexity: 10.035202980041504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [13:21:59<21:01:03, 620.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15828.314399857954
INFO:root:current train perplexity4.780303478240967
INFO:root:current mean train loss 15847.015454889113
INFO:root:current train perplexity4.772928714752197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.58s/it]
INFO:root:final mean train loss: 15848.347821635585
INFO:root:final train perplexity: 4.773873329162598
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.69s/it]
INFO:root:eval mean loss: 22285.564964657737
INFO:root:eval perplexity: 10.038825988769531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [13:32:17<20:49:56, 619.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15768.71665736607
INFO:root:current train perplexity4.732554912567139
INFO:root:current mean train loss 15856.444500219042
INFO:root:current train perplexity4.765209197998047
INFO:root:current mean train loss 15856.24008812651
INFO:root:current train perplexity4.769784450531006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.37s/it]
INFO:root:final mean train loss: 15826.917697045112
INFO:root:final train perplexity: 4.763792991638184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.75s/it]
INFO:root:eval mean loss: 22290.472144717263
INFO:root:eval perplexity: 10.043922424316406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [13:42:30<20:35:21, 617.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15751.471332097457
INFO:root:current train perplexity4.729369640350342
INFO:root:current mean train loss 15794.058231377752
INFO:root:current train perplexity4.744615077972412

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.62s/it]
INFO:root:final mean train loss: 15805.370081747731
INFO:root:final train perplexity: 4.753679275512695
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.88s/it]
INFO:root:eval mean loss: 22297.540690104168
INFO:root:eval perplexity: 10.05127239227295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [13:53:08<20:37:10, 623.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15867.355823863636
INFO:root:current train perplexity4.8129401206970215
INFO:root:current mean train loss 15812.927883938626
INFO:root:current train perplexity4.7416887283325195
INFO:root:current mean train loss 15816.065249185427
INFO:root:current train perplexity4.7448248863220215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:48<00:00, 528.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:48<00:00, 528.56s/it]
INFO:root:final mean train loss: 15786.932447864163
INFO:root:final train perplexity: 4.74504280090332
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.87s/it]
INFO:root:eval mean loss: 22292.24976748512
INFO:root:eval perplexity: 10.04577350616455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [14:03:38<20:30:11, 625.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15768.032583085318
INFO:root:current train perplexity4.735748767852783
INFO:root:current mean train loss 15764.010298840107
INFO:root:current train perplexity4.739222049713135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.71s/it]
INFO:root:final mean train loss: 15766.361859721523
INFO:root:final train perplexity: 4.7354254722595215
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.91s/it]
INFO:root:eval mean loss: 22279.136742001487
INFO:root:eval perplexity: 10.032147407531738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [14:13:57<20:15:54, 623.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15754.474609375
INFO:root:current train perplexity4.697323799133301
INFO:root:current mean train loss 15736.05078125
INFO:root:current train perplexity4.7236433029174805
INFO:root:current mean train loss 15764.975654069767
INFO:root:current train perplexity4.729844570159912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.08s/it]
INFO:root:final mean train loss: 15749.687263734879
INFO:root:final train perplexity: 4.7276434898376465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.95s/it]
INFO:root:eval mean loss: 22292.034086681546
INFO:root:eval perplexity: 10.045546531677246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [14:24:38<20:15:37, 628.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15686.080413362874
INFO:root:current train perplexity4.704733371734619
INFO:root:current mean train loss 15734.995374485405
INFO:root:current train perplexity4.715993881225586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.87s/it]
INFO:root:final mean train loss: 15728.761612430695
INFO:root:final train perplexity: 4.717896461486816
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.03s/it]
INFO:root:eval mean loss: 22280.185686383928
INFO:root:eval perplexity: 10.033238410949707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [14:34:58<20:00:14, 626.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15513.851408305922
INFO:root:current train perplexity4.706361293792725
INFO:root:current mean train loss 15733.014238117123
INFO:root:current train perplexity4.7092132568359375
INFO:root:current mean train loss 15724.446186501142
INFO:root:current train perplexity4.7088165283203125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.27s/it]
INFO:root:final mean train loss: 15710.1824202999
INFO:root:final train perplexity: 4.70925760269165
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.30s/it]
INFO:root:eval mean loss: 22278.4658203125
INFO:root:eval perplexity: 10.031452178955078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [14:45:46<20:02:12, 632.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15643.088839678698
INFO:root:current train perplexity4.702646255493164
INFO:root:current mean train loss 15688.843732867324
INFO:root:current train perplexity4.6987457275390625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:46<00:00, 526.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:46<00:00, 526.16s/it]
INFO:root:final mean train loss: 15694.865620274697
INFO:root:final train perplexity: 4.7021484375
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.93s/it]
INFO:root:eval mean loss: 22284.57361421131
INFO:root:eval perplexity: 10.037792205810547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [14:56:10<19:46:30, 630.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15724.069675611414
INFO:root:current train perplexity4.691656112670898
INFO:root:current mean train loss 15684.327156377032
INFO:root:current train perplexity4.6917524337768555
INFO:root:current mean train loss 15689.11554967769
INFO:root:current train perplexity4.694330215454102

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.46s/it]
INFO:root:final mean train loss: 15678.349971648186
INFO:root:final train perplexity: 4.694495677947998
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.37s/it]
INFO:root:eval mean loss: 22294.019717261905
INFO:root:eval perplexity: 10.047611236572266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [15:06:23<19:26:35, 624.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15658.100091145834
INFO:root:current train perplexity4.672176361083984
INFO:root:current mean train loss 15653.009229910715
INFO:root:current train perplexity4.6805033683776855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.95s/it]
INFO:root:final mean train loss: 15656.947013608871
INFO:root:final train perplexity: 4.684595108032227
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.67s/it]
INFO:root:eval mean loss: 22276.434151785714
INFO:root:eval perplexity: 10.029342651367188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [15:16:35<19:09:02, 621.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15615.108940972223
INFO:root:current train perplexity4.667803764343262
INFO:root:current mean train loss 15603.5982175812
INFO:root:current train perplexity4.660698890686035
INFO:root:current mean train loss 15639.541024229075
INFO:root:current train perplexity4.672842502593994

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.10s/it]
INFO:root:final mean train loss: 15642.402327998992
INFO:root:final train perplexity: 4.677880764007568
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.02s/it]
INFO:root:eval mean loss: 22285.923432849704
INFO:root:eval perplexity: 10.039194107055664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [15:27:05<19:03:26, 623.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15681.439453125
INFO:root:current train perplexity4.671320915222168
INFO:root:current mean train loss 15623.61920063722
INFO:root:current train perplexity4.665908336639404

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:45<00:00, 525.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:45<00:00, 525.58s/it]
INFO:root:final mean train loss: 15623.484879032258
INFO:root:final train perplexity: 4.66916036605835
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.79s/it]
INFO:root:eval mean loss: 22273.727446056546
INFO:root:eval perplexity: 10.026533126831055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [15:37:28<18:52:38, 623.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15615.526398689517
INFO:root:current train perplexity4.66544771194458
INFO:root:current mean train loss 15595.174491591126
INFO:root:current train perplexity4.659703254699707
INFO:root:current mean train loss 15610.94929484578
INFO:root:current train perplexity4.660068035125732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.33s/it]
INFO:root:final mean train loss: 15601.863930979083
INFO:root:final train perplexity: 4.659213066101074
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.62s/it]
INFO:root:eval mean loss: 22286.956635974704
INFO:root:eval perplexity: 10.040268898010254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [15:47:47<18:40:02, 622.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15619.447465643825
INFO:root:current train perplexity4.660245895385742
INFO:root:current mean train loss 15599.905230746243
INFO:root:current train perplexity4.652820110321045

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.02s/it]
INFO:root:final mean train loss: 15590.985989478326
INFO:root:final train perplexity: 4.654217720031738
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.37s/it]
INFO:root:eval mean loss: 22275.437593005954
INFO:root:eval perplexity: 10.02830696105957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [15:58:03<18:26:38, 620.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15567.871456473214
INFO:root:current train perplexity4.618529796600342
INFO:root:current mean train loss 15552.52970920139
INFO:root:current train perplexity4.6370697021484375
INFO:root:current mean train loss 15573.840425531915
INFO:root:current train perplexity4.640771389007568

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.08s/it]
INFO:root:final mean train loss: 15564.800903320312
INFO:root:final train perplexity: 4.642212390899658
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.98s/it]
INFO:root:eval mean loss: 22299.200032552082
INFO:root:eval perplexity: 10.052997589111328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [16:08:16<18:12:17, 618.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15587.723745061063
INFO:root:current train perplexity4.632246494293213
INFO:root:current mean train loss 15571.683619861296
INFO:root:current train perplexity4.638058662414551

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.71s/it]
INFO:root:final mean train loss: 15551.357855027722
INFO:root:final train perplexity: 4.636061668395996
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.86s/it]
INFO:root:eval mean loss: 22290.852027529763
INFO:root:eval perplexity: 10.044318199157715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [16:18:19<17:53:54, 613.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15481.993239182691
INFO:root:current train perplexity4.584427833557129
INFO:root:current mean train loss 15542.327197616907
INFO:root:current train perplexity4.614908695220947
INFO:root:current mean train loss 15561.265600483786
INFO:root:current train perplexity4.630463600158691

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.09s/it]
INFO:root:final mean train loss: 15538.746239446824
INFO:root:final train perplexity: 4.630298137664795
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.72s/it]
INFO:root:eval mean loss: 22296.940336681546
INFO:root:eval perplexity: 10.05064868927002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [16:28:36<17:44:59, 614.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15567.51304945055
INFO:root:current train perplexity4.629326343536377
INFO:root:current mean train loss 15522.429007485274
INFO:root:current train perplexity4.622532844543457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.67s/it]
INFO:root:final mean train loss: 15522.154505575856
INFO:root:final train perplexity: 4.622726917266846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.87s/it]
INFO:root:eval mean loss: 22296.515159970237
INFO:root:eval perplexity: 10.050206184387207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [16:38:56<17:38:06, 616.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15508.910110828489
INFO:root:current train perplexity4.62472677230835
INFO:root:current mean train loss 15522.804667012675
INFO:root:current train perplexity4.620848655700684
INFO:root:current mean train loss 15516.009078414352
INFO:root:current train perplexity4.616149425506592

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.55s/it]
INFO:root:final mean train loss: 15507.052064957157
INFO:root:final train perplexity: 4.615846157073975
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.06s/it]
INFO:root:eval mean loss: 22296.886672247023
INFO:root:eval perplexity: 10.050594329833984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [16:49:15<17:29:01, 617.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15531.142413651316
INFO:root:current train perplexity4.605766296386719
INFO:root:current mean train loss 15501.86477363782
INFO:root:current train perplexity4.606665134429932

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.32s/it]
INFO:root:final mean train loss: 15487.373444587955
INFO:root:final train perplexity: 4.60689640045166
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.23s/it]
INFO:root:eval mean loss: 22286.37081473214
INFO:root:eval perplexity: 10.039661407470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [17:00:01<17:33:21, 625.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15413.989901928191
INFO:root:current train perplexity4.559098243713379
INFO:root:current mean train loss 15476.434065423044
INFO:root:current train perplexity4.593171119689941
INFO:root:current mean train loss 15490.605448981529
INFO:root:current train perplexity4.602813720703125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.66s/it]
INFO:root:final mean train loss: 15479.942666330646
INFO:root:final train perplexity: 4.603519916534424
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.36s/it]
INFO:root:eval mean loss: 22291.398507254464
INFO:root:eval perplexity: 10.04488754272461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [17:10:19<17:18:59, 623.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15416.497612847223
INFO:root:current train perplexity4.579233646392822
INFO:root:current mean train loss 15482.246952536118
INFO:root:current train perplexity4.595522880554199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.31s/it]
INFO:root:final mean train loss: 15462.468049080142
INFO:root:final train perplexity: 4.595592021942139
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.47s/it]
INFO:root:eval mean loss: 22281.114095052082
INFO:root:eval perplexity: 10.034200668334961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [17:20:34<17:04:27, 620.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15498.430127910538
INFO:root:current train perplexity4.583525657653809
INFO:root:current mean train loss 15475.811542839405
INFO:root:current train perplexity4.591620922088623

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:58<00:00, 538.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:58<00:00, 538.44s/it]
INFO:root:final mean train loss: 15453.065185546875
INFO:root:final train perplexity: 4.59133243560791
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.01s/it]
INFO:root:eval mean loss: 22286.745628720237
INFO:root:eval perplexity: 10.040051460266113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [17:31:12<17:02:29, 626.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15455.486002604166
INFO:root:current train perplexity4.500038146972656
INFO:root:current mean train loss 15467.408952138956
INFO:root:current train perplexity4.576224327087402
INFO:root:current mean train loss 15430.695856103755
INFO:root:current train perplexity4.5762224197387695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.44s/it]
INFO:root:final mean train loss: 15432.6841765373
INFO:root:final train perplexity: 4.582112789154053
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.57s/it]
INFO:root:eval mean loss: 22284.02315848214
INFO:root:eval perplexity: 10.037221908569336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [17:41:26<16:46:04, 622.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15397.762446732955
INFO:root:current train perplexity4.551020622253418
INFO:root:current mean train loss 15418.400592237904
INFO:root:current train perplexity4.574906349182129

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.92s/it]
INFO:root:final mean train loss: 15421.16505875126
INFO:root:final train perplexity: 4.576909065246582
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.63s/it]
INFO:root:eval mean loss: 22286.398484002977
INFO:root:eval perplexity: 10.039690017700195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [17:51:38<16:30:47, 619.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15246.6103515625
INFO:root:current train perplexity4.5230255126953125
INFO:root:current mean train loss 15329.050251898365
INFO:root:current train perplexity4.552892684936523
INFO:root:current mean train loss 15415.664350279287
INFO:root:current train perplexity4.571900844573975

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.44s/it]
INFO:root:final mean train loss: 15407.57359264743
INFO:root:final train perplexity: 4.5707783699035645
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.70s/it]
INFO:root:eval mean loss: 22297.215866815477
INFO:root:eval perplexity: 10.050936698913574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [18:02:13<16:28:14, 624.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15404.31794557733
INFO:root:current train perplexity4.566812992095947
INFO:root:current mean train loss 15436.087558962265
INFO:root:current train perplexity4.569545745849609

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.62s/it]
INFO:root:final mean train loss: 15394.123204385081
INFO:root:final train perplexity: 4.564717769622803
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.46s/it]
INFO:root:eval mean loss: 22296.24597749256
INFO:root:eval perplexity: 10.049927711486816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [18:12:26<16:12:16, 620.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15106.140802556818
INFO:root:current train perplexity4.513220310211182
INFO:root:current mean train loss 15353.2055884009
INFO:root:current train perplexity4.545316219329834
INFO:root:current mean train loss 15390.9842083827
INFO:root:current train perplexity4.557220458984375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.41s/it]
INFO:root:final mean train loss: 15378.877657982612
INFO:root:final train perplexity: 4.557859897613525
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.11s/it]
INFO:root:eval mean loss: 22300.64674014137
INFO:root:eval perplexity: 10.054505348205566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [18:22:35<15:56:25, 617.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15322.413829985118
INFO:root:current train perplexity4.538017749786377
INFO:root:current mean train loss 15373.964891679449
INFO:root:current train perplexity4.551591396331787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.14s/it]
INFO:root:final mean train loss: 15367.047272712955
INFO:root:final train perplexity: 4.552543640136719
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.79s/it]
INFO:root:eval mean loss: 22292.868954613095
INFO:root:eval perplexity: 10.046415328979492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [18:32:50<15:45:20, 616.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15102.36875
INFO:root:current train perplexity4.5247063636779785
INFO:root:current mean train loss 15336.20398267663
INFO:root:current train perplexity4.538288593292236
INFO:root:current mean train loss 15361.40484647529
INFO:root:current train perplexity4.544212341308594

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.75s/it]
INFO:root:final mean train loss: 15356.441075478831
INFO:root:final train perplexity: 4.547783851623535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.30s/it]
INFO:root:eval mean loss: 22302.217750186013
INFO:root:eval perplexity: 10.056139945983887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [18:43:00<15:32:22, 614.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15273.954932369403
INFO:root:current train perplexity4.524055480957031
INFO:root:current mean train loss 15334.387876590568
INFO:root:current train perplexity4.54003381729126

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.41s/it]
INFO:root:final mean train loss: 15343.577329574093
INFO:root:final train perplexity: 4.542016983032227
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.20s/it]
INFO:root:eval mean loss: 22304.810221354168
INFO:root:eval perplexity: 10.05884075164795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [18:53:07<15:18:37, 612.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15325.937551398027
INFO:root:current train perplexity4.518063545227051
INFO:root:current mean train loss 15372.593290441177
INFO:root:current train perplexity4.534229755401611
INFO:root:current mean train loss 15331.974297231734
INFO:root:current train perplexity4.5318684577941895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:26<00:00, 506.64s/it]
INFO:root:final mean train loss: 15325.08068453881
INFO:root:final train perplexity: 4.53373908996582
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.17s/it]
INFO:root:eval mean loss: 22309.09640066964
INFO:root:eval perplexity: 10.063300132751465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [19:03:11<15:04:15, 609.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15354.296063490317
INFO:root:current train perplexity4.537801742553711
INFO:root:current mean train loss 15334.55629797149
INFO:root:current train perplexity4.533873081207275

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.28s/it]
INFO:root:final mean train loss: 15317.77716655116
INFO:root:final train perplexity: 4.530474662780762
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.50s/it]
INFO:root:eval mean loss: 22313.351120721727
INFO:root:eval perplexity: 10.067734718322754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [19:13:17<14:52:34, 608.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15239.389988111414
INFO:root:current train perplexity4.504891395568848
INFO:root:current mean train loss 15286.683069740853
INFO:root:current train perplexity4.521553993225098
INFO:root:current mean train loss 15308.313603559416
INFO:root:current train perplexity4.5251383781433105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:24<00:00, 504.15s/it]
INFO:root:final mean train loss: 15303.026067918347
INFO:root:final train perplexity: 4.523886680603027
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.84s/it]
INFO:root:eval mean loss: 22310.56519717262
INFO:root:eval perplexity: 10.06483268737793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [19:23:20<14:40:14, 607.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15290.3696875
INFO:root:current train perplexity4.519915580749512
INFO:root:current mean train loss 15308.611729910714
INFO:root:current train perplexity4.523581027984619

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.88s/it]
INFO:root:final mean train loss: 15295.060342111896
INFO:root:final train perplexity: 4.520334243774414
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.60s/it]
INFO:root:eval mean loss: 22308.434058779763
INFO:root:eval perplexity: 10.062613487243652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [19:33:28<14:30:28, 607.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15263.692708333334
INFO:root:current train perplexity4.537704944610596
INFO:root:current mean train loss 15273.770284817914
INFO:root:current train perplexity4.515038967132568
INFO:root:current mean train loss 15284.736960524504
INFO:root:current train perplexity4.5101094245910645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.90s/it]
INFO:root:final mean train loss: 15279.270629882812
INFO:root:final train perplexity: 4.51330041885376
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.29s/it]
INFO:root:eval mean loss: 22299.21728515625
INFO:root:eval perplexity: 10.053016662597656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [19:43:35<14:20:22, 607.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15250.613948773735
INFO:root:current train perplexity4.502084732055664
INFO:root:current mean train loss 15285.18170063722
INFO:root:current train perplexity4.505334854125977

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.61s/it]
INFO:root:final mean train loss: 15270.15734469506
INFO:root:final train perplexity: 4.5092453956604
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.19s/it]
INFO:root:eval mean loss: 22320.165364583332
INFO:root:eval perplexity: 10.074837684631348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [19:53:44<14:10:54, 607.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15323.79775705645
INFO:root:current train perplexity4.516362190246582
INFO:root:current mean train loss 15293.564236939408
INFO:root:current train perplexity4.50687837600708
INFO:root:current mean train loss 15274.669114414231
INFO:root:current train perplexity4.505602836608887

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:30<00:00, 510.96s/it]
INFO:root:final mean train loss: 15258.31062563004
INFO:root:final train perplexity: 4.503979206085205
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.32s/it]
INFO:root:eval mean loss: 22319.08849516369
INFO:root:eval perplexity: 10.073715209960938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [20:03:52<14:00:48, 607.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15261.228715643825
INFO:root:current train perplexity4.4940972328186035
INFO:root:current mean train loss 15273.030967170424
INFO:root:current train perplexity4.499164581298828

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:28<00:00, 508.62s/it]
INFO:root:final mean train loss: 15244.755859375
INFO:root:final train perplexity: 4.497961521148682
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.28s/it]
INFO:root:eval mean loss: 22315.18126860119
INFO:root:eval perplexity: 10.069640159606934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [20:13:56<13:48:51, 606.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15197.282505580357
INFO:root:current train perplexity4.470522880554199
INFO:root:current mean train loss 15246.343742766203
INFO:root:current train perplexity4.483654022216797
INFO:root:current mean train loss 15246.81429105718
INFO:root:current train perplexity4.494323253631592

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:27<00:00, 507.22s/it]
INFO:root:final mean train loss: 15235.785959551411
INFO:root:final train perplexity: 4.493983745574951
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.60s/it]
INFO:root:eval mean loss: 22312.849446614582
INFO:root:eval perplexity: 10.067211151123047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [20:23:58<13:37:01, 605.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15173.322669719828
INFO:root:current train perplexity4.482137203216553
INFO:root:current mean train loss 15226.240385820522
INFO:root:current train perplexity4.486414432525635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.40s/it]
INFO:root:final mean train loss: 15222.714761057208
INFO:root:final train perplexity: 4.488193511962891
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.82s/it]
INFO:root:eval mean loss: 22326.870419456845
INFO:root:eval perplexity: 10.081831932067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [20:34:31<13:38:04, 613.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15184.125525841346
INFO:root:current train perplexity4.469675064086914
INFO:root:current mean train loss 15196.115395964478
INFO:root:current train perplexity4.480788230895996
INFO:root:current mean train loss 15224.91879821522
INFO:root:current train perplexity4.4846014976501465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.36s/it]
INFO:root:final mean train loss: 15215.079322076614
INFO:root:final train perplexity: 4.484814643859863
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.21s/it]
INFO:root:eval mean loss: 22317.127464657737
INFO:root:eval perplexity: 10.071669578552246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [20:44:43<13:27:10, 613.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15177.238431490385
INFO:root:current train perplexity4.4695305824279785
INFO:root:current mean train loss 15196.763610520287
INFO:root:current train perplexity4.469438552856445

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.53s/it]
INFO:root:final mean train loss: 15202.535042055191
INFO:root:final train perplexity: 4.479269981384277
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.39s/it]
INFO:root:eval mean loss: 22321.109793526786
INFO:root:eval perplexity: 10.075821876525879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [20:54:54<13:16:15, 612.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15182.141306322674
INFO:root:current train perplexity4.49129581451416
INFO:root:current mean train loss 15222.10372732736
INFO:root:current train perplexity4.48124885559082
INFO:root:current mean train loss 15206.256747524434
INFO:root:current train perplexity4.475802421569824

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.16s/it]
INFO:root:final mean train loss: 15194.317796276462
INFO:root:final train perplexity: 4.475641250610352
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.87s/it]
INFO:root:eval mean loss: 22328.283854166668
INFO:root:eval perplexity: 10.083308219909668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [21:05:05<13:05:38, 612.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15161.879451069079
INFO:root:current train perplexity4.46649694442749
INFO:root:current mean train loss 15202.601933092948
INFO:root:current train perplexity4.469483375549316

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.65s/it]
INFO:root:final mean train loss: 15182.846451297883
INFO:root:final train perplexity: 4.470579624176025
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.25s/it]
INFO:root:eval mean loss: 22332.432152157737
INFO:root:eval perplexity: 10.08763599395752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [21:15:19<12:55:53, 612.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15159.525078956118
INFO:root:current train perplexity4.449696063995361
INFO:root:current mean train loss 15186.177714445153
INFO:root:current train perplexity4.462558269500732
INFO:root:current mean train loss 15186.402466314525
INFO:root:current train perplexity4.466395854949951

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.65s/it]
INFO:root:final mean train loss: 15172.602038967994
INFO:root:final train perplexity: 4.466064929962158
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.89s/it]
INFO:root:eval mean loss: 22325.72828311012
INFO:root:eval perplexity: 10.080636978149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [21:25:50<12:52:40, 618.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15156.290058791035
INFO:root:current train perplexity4.458384990692139
INFO:root:current mean train loss 15168.260133676193
INFO:root:current train perplexity4.457618713378906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.54s/it]
INFO:root:final mean train loss: 15160.563929403981
INFO:root:final train perplexity: 4.4607648849487305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.99s/it]
INFO:root:eval mean loss: 22327.026878720237
INFO:root:eval perplexity: 10.081993103027344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [21:36:07<12:41:47, 617.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15143.879461550245
INFO:root:current train perplexity4.446384906768799
INFO:root:current mean train loss 15136.09643393005
INFO:root:current train perplexity4.449191570281982

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.47s/it]
INFO:root:final mean train loss: 15148.631107453377
INFO:root:final train perplexity: 4.4555182456970215
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.70s/it]
INFO:root:eval mean loss: 22329.18298921131
INFO:root:eval perplexity: 10.084242820739746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [21:46:26<12:32:05, 618.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14931.135416666666
INFO:root:current train perplexity4.44143009185791
INFO:root:current mean train loss 15139.235948877427
INFO:root:current train perplexity4.453807353973389
INFO:root:current mean train loss 15137.694845866687
INFO:root:current train perplexity4.447233200073242

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.38s/it]
INFO:root:final mean train loss: 15139.82353358115
INFO:root:final train perplexity: 4.4516496658325195
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.53s/it]
INFO:root:eval mean loss: 22327.763695126487
INFO:root:eval perplexity: 10.082761764526367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [21:56:47<12:22:48, 619.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15162.867116477273
INFO:root:current train perplexity4.454445838928223
INFO:root:current mean train loss 15138.863785282258
INFO:root:current train perplexity4.449926853179932

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.06s/it]
INFO:root:final mean train loss: 15131.26455393145
INFO:root:final train perplexity: 4.447892665863037
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.97s/it]
INFO:root:eval mean loss: 22328.334937686013
INFO:root:eval perplexity: 10.08336067199707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [22:07:07<12:12:53, 619.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15172.937918526786
INFO:root:current train perplexity4.409132957458496
INFO:root:current mean train loss 15150.019129672897
INFO:root:current train perplexity4.454287528991699
INFO:root:current mean train loss 15132.476704030798
INFO:root:current train perplexity4.444481372833252

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.89s/it]
INFO:root:final mean train loss: 15121.339753181705
INFO:root:final train perplexity: 4.443540573120117
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.63s/it]
INFO:root:eval mean loss: 22349.34054129464
INFO:root:eval perplexity: 10.105304718017578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [22:17:19<12:00:00, 617.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15156.805366128177
INFO:root:current train perplexity4.451523780822754
INFO:root:current mean train loss 15103.187917649371
INFO:root:current train perplexity4.439038276672363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:29<00:00, 509.91s/it]
INFO:root:final mean train loss: 15113.043039629536
INFO:root:final train perplexity: 4.439906597137451
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.37s/it]
INFO:root:eval mean loss: 22330.40555245536
INFO:root:eval perplexity: 10.085519790649414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [22:27:28<11:46:47, 614.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15013.11825284091
INFO:root:current train perplexity4.453216075897217
INFO:root:current mean train loss 15110.41993067286
INFO:root:current train perplexity4.432347297668457
INFO:root:current mean train loss 15107.14727117891
INFO:root:current train perplexity4.434582710266113

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.73s/it]
INFO:root:final mean train loss: 15104.651300245716
INFO:root:final train perplexity: 4.436233043670654
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.23s/it]
INFO:root:eval mean loss: 22339.754348028273
INFO:root:eval perplexity: 10.095282554626465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [22:37:37<11:34:46, 613.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15006.395042782739
INFO:root:current train perplexity4.42399787902832
INFO:root:current mean train loss 15064.294712183666
INFO:root:current train perplexity4.423811912536621

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.72s/it]
INFO:root:final mean train loss: 15097.330298639114
INFO:root:final train perplexity: 4.433030605316162
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.41s/it]
INFO:root:eval mean loss: 22334.417829241072
INFO:root:eval perplexity: 10.089712142944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [22:48:01<11:28:11, 616.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15066.208919270834
INFO:root:current train perplexity4.443305492401123
INFO:root:current mean train loss 15101.057820991848
INFO:root:current train perplexity4.434595584869385
INFO:root:current mean train loss 15090.450694949128
INFO:root:current train perplexity4.424413681030273

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.31s/it]
INFO:root:final mean train loss: 15084.883702431956
INFO:root:final train perplexity: 4.4275922775268555
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.17s/it]
INFO:root:eval mean loss: 22346.882719494046
INFO:root:eval perplexity: 10.102734565734863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [22:58:38<11:24:46, 622.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15044.268671291977
INFO:root:current train perplexity4.410970687866211
INFO:root:current mean train loss 15114.509847492514
INFO:root:current train perplexity4.431668281555176

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.25s/it]
INFO:root:final mean train loss: 15079.702258694557
INFO:root:final train perplexity: 4.42533016204834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.54s/it]
INFO:root:eval mean loss: 22346.592750186013
INFO:root:eval perplexity: 10.102431297302246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [23:09:26<11:22:49, 630.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15124.163805509868
INFO:root:current train perplexity4.424844264984131
INFO:root:current mean train loss 15112.533350840336
INFO:root:current train perplexity4.434753894805908
INFO:root:current mean train loss 15103.14878977597
INFO:root:current train perplexity4.424079895019531

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:37<00:00, 517.60s/it]
INFO:root:final mean train loss: 15068.682542370212
INFO:root:final train perplexity: 4.420522689819336
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.91s/it]
INFO:root:eval mean loss: 22351.234514508928
INFO:root:eval perplexity: 10.107282638549805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [23:20:12<11:17:21, 635.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15049.614120268487
INFO:root:current train perplexity4.408514976501465
INFO:root:current mean train loss 15086.226488258406
INFO:root:current train perplexity4.416769504547119

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:50<00:00, 530.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:50<00:00, 530.79s/it]
INFO:root:final mean train loss: 15060.07593954763
INFO:root:final train perplexity: 4.416770935058594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.76s/it]
INFO:root:eval mean loss: 22348.215029761905
INFO:root:eval perplexity: 10.104126930236816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [23:31:03<11:11:44, 639.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14957.66359544837
INFO:root:current train perplexity4.384689807891846
INFO:root:current mean train loss 15051.421859120936
INFO:root:current train perplexity4.415004730224609
INFO:root:current mean train loss 15063.14598514574
INFO:root:current train perplexity4.415985584259033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.89s/it]
INFO:root:final mean train loss: 15056.82749889743
INFO:root:final train perplexity: 4.415356159210205
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.27s/it]
INFO:root:eval mean loss: 22349.246907552082
INFO:root:eval perplexity: 10.105206489562988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [23:41:41<11:00:18, 639.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14989.743515625
INFO:root:current train perplexity4.38969087600708
INFO:root:current mean train loss 15041.061936383929
INFO:root:current train perplexity4.3994598388671875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.22s/it]
INFO:root:final mean train loss: 15042.902623330394
INFO:root:final train perplexity: 4.409296035766602
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.31s/it]
INFO:root:eval mean loss: 22345.899739583332
INFO:root:eval perplexity: 10.101705551147461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [23:52:07<10:45:58, 635.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15061.534215856482
INFO:root:current train perplexity4.414703845977783
INFO:root:current mean train loss 15008.11138195128
INFO:root:current train perplexity4.397069931030273
INFO:root:current mean train loss 15044.003708356278
INFO:root:current train perplexity4.403343677520752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.57s/it]
INFO:root:final mean train loss: 15031.666184948337
INFO:root:final train perplexity: 4.404411792755127
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.62s/it]
INFO:root:eval mean loss: 22354.638369605655
INFO:root:eval perplexity: 10.110845565795898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [24:02:26<10:30:19, 630.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14989.515934038765
INFO:root:current train perplexity4.399955749511719
INFO:root:current mean train loss 15017.672060492318
INFO:root:current train perplexity4.3988847732543945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:01<00:00, 541.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:01<00:00, 541.53s/it]
INFO:root:final mean train loss: 15029.385104271674
INFO:root:final train perplexity: 4.403421401977539
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.29s/it]
INFO:root:eval mean loss: 22353.086286272322
INFO:root:eval perplexity: 10.109223365783691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [24:13:11<10:24:18, 634.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15009.748991935483
INFO:root:current train perplexity4.404445648193359
INFO:root:current mean train loss 15036.95610687023
INFO:root:current train perplexity4.405786991119385
INFO:root:current mean train loss 15032.816592261905
INFO:root:current train perplexity4.402276992797852

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:59<00:00, 539.85s/it]
INFO:root:final mean train loss: 15021.765955771169
INFO:root:final train perplexity: 4.400113582611084
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.03s/it]
INFO:root:eval mean loss: 22367.29113188244
INFO:root:eval perplexity: 10.124095916748047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [24:24:13<10:21:30, 642.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15081.248717526356
INFO:root:current train perplexity4.390408515930176
INFO:root:current mean train loss 15034.217559981216
INFO:root:current train perplexity4.391836166381836

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.63s/it]
INFO:root:final mean train loss: 15015.573659589214
INFO:root:final train perplexity: 4.397427082061768
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.25s/it]
INFO:root:eval mean loss: 22351.059058779763
INFO:root:eval perplexity: 10.107102394104004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [24:34:54<10:10:02, 642.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14959.58125
INFO:root:current train perplexity4.365053653717041
INFO:root:current mean train loss 14995.611103877314
INFO:root:current train perplexity4.381246566772461
INFO:root:current mean train loss 15010.465662400266
INFO:root:current train perplexity4.394283771514893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.08s/it]
INFO:root:final mean train loss: 15008.495660597278
INFO:root:final train perplexity: 4.394358158111572
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.14s/it]
INFO:root:eval mean loss: 22357.713843936013
INFO:root:eval perplexity: 10.114066123962402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [24:45:50<10:03:13, 646.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14975.694493085488
INFO:root:current train perplexity4.377603054046631
INFO:root:current mean train loss 15019.783438126671
INFO:root:current train perplexity4.392284393310547

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:03<00:00, 543.77s/it]
INFO:root:final mean train loss: 14997.542893932712
INFO:root:final train perplexity: 4.389613151550293
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.93s/it]
INFO:root:eval mean loss: 22365.56677827381
INFO:root:eval perplexity: 10.12228775024414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [24:56:35<9:52:10, 646.00s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15003.562575120191
INFO:root:current train perplexity4.379838943481445
INFO:root:current mean train loss 15000.858145514838
INFO:root:current train perplexity4.388241291046143
INFO:root:current mean train loss 15003.774969763337
INFO:root:current train perplexity4.387569904327393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:57<00:00, 537.89s/it]
INFO:root:final mean train loss: 14993.645212481098
INFO:root:final train perplexity: 4.38792610168457
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.79s/it]
INFO:root:eval mean loss: 22359.39318266369
INFO:root:eval perplexity: 10.115821838378906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [25:07:11<9:38:47, 643.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15036.101905906593
INFO:root:current train perplexity4.378996849060059
INFO:root:current mean train loss 14993.034368864528
INFO:root:current train perplexity4.376522541046143

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.77s/it]
INFO:root:final mean train loss: 14988.70082141507
INFO:root:final train perplexity: 4.385786533355713
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.05s/it]
INFO:root:eval mean loss: 22355.790341331845
INFO:root:eval perplexity: 10.112052917480469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [25:17:35<9:23:03, 637.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14976.123410247093
INFO:root:current train perplexity4.380378723144531
INFO:root:current mean train loss 14979.291159036276
INFO:root:current train perplexity4.376129150390625
INFO:root:current mean train loss 14998.916220582561
INFO:root:current train perplexity4.383486747741699

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.75s/it]
INFO:root:final mean train loss: 14983.061551001763
INFO:root:final train perplexity: 4.383347988128662
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.46s/it]
INFO:root:eval mean loss: 22363.72400483631
INFO:root:eval perplexity: 10.120359420776367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [25:28:18<9:13:47, 639.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14995.786657072369
INFO:root:current train perplexity4.380400657653809
INFO:root:current mean train loss 14974.136212940704
INFO:root:current train perplexity4.3762102127075195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.96s/it]
INFO:root:final mean train loss: 14970.822667275706
INFO:root:final train perplexity: 4.3780598640441895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.68s/it]
INFO:root:eval mean loss: 22360.702776227678
INFO:root:eval perplexity: 10.117194175720215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [25:39:02<9:04:22, 640.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14953.001932347075
INFO:root:current train perplexity4.364330291748047
INFO:root:current mean train loss 14945.453636532739
INFO:root:current train perplexity4.3658447265625
INFO:root:current mean train loss 14977.481888126265
INFO:root:current train perplexity4.376319408416748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.63s/it]
INFO:root:final mean train loss: 14966.959460842994
INFO:root:final train perplexity: 4.376391410827637
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.62s/it]
INFO:root:eval mean loss: 22362.923037574405
INFO:root:eval perplexity: 10.11952018737793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [25:49:29<8:50:21, 636.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14945.907502761995
INFO:root:current train perplexity4.363057613372803
INFO:root:current mean train loss 14941.403997526695
INFO:root:current train perplexity4.3697285652160645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.57s/it]
INFO:root:final mean train loss: 14962.082432900706
INFO:root:final train perplexity: 4.374287128448486
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.80s/it]
INFO:root:eval mean loss: 22384.914039248513
INFO:root:eval perplexity: 10.142578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [25:59:55<8:37:06, 633.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14943.915977328432
INFO:root:current train perplexity4.369849681854248
INFO:root:current mean train loss 14935.403572537252
INFO:root:current train perplexity4.363248825073242

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:58<00:00, 538.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:58<00:00, 538.70s/it]
INFO:root:final mean train loss: 14952.194438319053
INFO:root:final train perplexity: 4.370023727416992
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.59s/it]
INFO:root:eval mean loss: 22375.204287574405
INFO:root:eval perplexity: 10.132390022277832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [26:10:32<8:27:40, 634.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15053.493489583334
INFO:root:current train perplexity4.392698287963867
INFO:root:current mean train loss 14938.543850500606
INFO:root:current train perplexity4.360786437988281
INFO:root:current mean train loss 14956.364835090824
INFO:root:current train perplexity4.366732597351074

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.15s/it]
INFO:root:final mean train loss: 14947.093265656502
INFO:root:final train perplexity: 4.367825508117676
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.57s/it]
INFO:root:eval mean loss: 22381.148344494046
INFO:root:eval perplexity: 10.138625144958496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [26:21:05<8:16:30, 633.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14894.718359375
INFO:root:current train perplexity4.340267181396484
INFO:root:current mean train loss 14919.1728515625
INFO:root:current train perplexity4.35308837890625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:50<00:00, 530.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:50<00:00, 530.44s/it]
INFO:root:final mean train loss: 14935.910223191784
INFO:root:final train perplexity: 4.363009929656982
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.37s/it]
INFO:root:eval mean loss: 22372.07177734375
INFO:root:eval perplexity: 10.129107475280762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [26:31:30<8:04:00, 631.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15011.77650669643
INFO:root:current train perplexity4.402552127838135
INFO:root:current mean train loss 14900.86007775993
INFO:root:current train perplexity4.355863094329834
INFO:root:current mean train loss 14947.680975430254
INFO:root:current train perplexity4.362511157989502

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.35s/it]
INFO:root:final mean train loss: 14931.89207015499
INFO:root:final train perplexity: 4.361281394958496
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.87s/it]
INFO:root:eval mean loss: 22375.285226004464
INFO:root:eval perplexity: 10.132474899291992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [26:42:02<7:53:42, 631.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14968.70986162606
INFO:root:current train perplexity4.362478256225586
INFO:root:current mean train loss 14921.634415536557
INFO:root:current train perplexity4.356251239776611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.95s/it]
INFO:root:final mean train loss: 14926.014388545867
INFO:root:final train perplexity: 4.358753204345703
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.19s/it]
INFO:root:eval mean loss: 22377.195707775296
INFO:root:eval perplexity: 10.134477615356445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [26:52:41<7:44:43, 633.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14915.930397727272
INFO:root:current train perplexity4.31590461730957
INFO:root:current mean train loss 14921.149959529843
INFO:root:current train perplexity4.346893310546875
INFO:root:current mean train loss 14929.280426170024
INFO:root:current train perplexity4.3531999588012695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.05s/it]
INFO:root:final mean train loss: 14922.551277406754
INFO:root:final train perplexity: 4.357264518737793
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.07s/it]
INFO:root:eval mean loss: 22375.519066220237
INFO:root:eval perplexity: 10.132720947265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [27:03:03<7:31:43, 630.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14859.404327876984
INFO:root:current train perplexity4.346903324127197
INFO:root:current mean train loss 14929.174025833972
INFO:root:current train perplexity4.357265949249268

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:00<00:00, 540.96s/it]
INFO:root:final mean train loss: 14920.102113785282
INFO:root:final train perplexity: 4.356212615966797
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.63s/it]
INFO:root:eval mean loss: 22372.311337425595
INFO:root:eval perplexity: 10.129359245300293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [27:13:46<7:23:55, 634.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14992.509765625
INFO:root:current train perplexity4.354877471923828
INFO:root:current mean train loss 14935.761531929347
INFO:root:current train perplexity4.362369537353516
INFO:root:current mean train loss 14930.64812863372
INFO:root:current train perplexity4.356152534484863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:54<00:00, 534.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:54<00:00, 534.63s/it]
INFO:root:final mean train loss: 14913.764979208669
INFO:root:final train perplexity: 4.353490829467773
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.50s/it]
INFO:root:eval mean loss: 22380.161946614582
INFO:root:eval perplexity: 10.137592315673828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [27:24:19<7:13:02, 633.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14879.322498833955
INFO:root:current train perplexity4.332056045532227
INFO:root:current mean train loss 14897.800822183757
INFO:root:current train perplexity4.343616008758545

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.81s/it]
INFO:root:final mean train loss: 14908.758726058468
INFO:root:final train perplexity: 4.351341247558594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.47s/it]
INFO:root:eval mean loss: 22375.58800688244
INFO:root:eval perplexity: 10.132793426513672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [27:34:49<7:01:41, 632.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14888.301243832237
INFO:root:current train perplexity4.329994201660156
INFO:root:current mean train loss 14922.034491530987
INFO:root:current train perplexity4.349305152893066
INFO:root:current mean train loss 14905.935662813927
INFO:root:current train perplexity4.350019454956055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.92s/it]
INFO:root:final mean train loss: 14902.347408171623
INFO:root:final train perplexity: 4.348590850830078
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.78s/it]
INFO:root:eval mean loss: 22388.357352120536
INFO:root:eval perplexity: 10.146193504333496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [27:45:17<6:50:17, 631.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14903.047975352112
INFO:root:current train perplexity4.333140850067139
INFO:root:current mean train loss 14900.046498081141
INFO:root:current train perplexity4.338384628295898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.44s/it]
INFO:root:final mean train loss: 14896.654793031754
INFO:root:final train perplexity: 4.346149921417236
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.04s/it]
INFO:root:eval mean loss: 22382.257998511905
INFO:root:eval perplexity: 10.139789581298828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [27:55:47<6:39:33, 630.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14898.677989130434
INFO:root:current train perplexity4.3597846031188965
INFO:root:current mean train loss 14937.670684070123
INFO:root:current train perplexity4.348052978515625
INFO:root:current mean train loss 14907.385304267096
INFO:root:current train perplexity4.343551158905029

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.99s/it]
INFO:root:final mean train loss: 14895.64476357737
INFO:root:final train perplexity: 4.345716953277588
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.45s/it]
INFO:root:eval mean loss: 22381.967354910714
INFO:root:eval perplexity: 10.139485359191895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [28:06:03<6:26:15, 626.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.7252734375
INFO:root:current train perplexity4.347367763519287
INFO:root:current mean train loss 14880.857661830358
INFO:root:current train perplexity4.340890884399414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.99s/it]
INFO:root:final mean train loss: 14888.082995999244
INFO:root:final train perplexity: 4.3424763679504395
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.19s/it]
INFO:root:eval mean loss: 22388.882765997023
INFO:root:eval perplexity: 10.146744728088379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [28:16:20<6:14:12, 623.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14924.354094328704
INFO:root:current train perplexity4.332790851593018
INFO:root:current mean train loss 14922.653950848917
INFO:root:current train perplexity4.354927062988281
INFO:root:current mean train loss 14898.267182337555
INFO:root:current train perplexity4.342261791229248

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.77s/it]
INFO:root:final mean train loss: 14880.979389805947
INFO:root:final train perplexity: 4.33943510055542
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.23s/it]
INFO:root:eval mean loss: 22381.877952938987
INFO:root:eval perplexity: 10.13939094543457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [28:26:32<6:01:39, 619.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14915.545972606804
INFO:root:current train perplexity4.347128868103027
INFO:root:current mean train loss 14904.193386653282
INFO:root:current train perplexity4.341790199279785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:32<00:00, 512.71s/it]
INFO:root:final mean train loss: 14876.86918000252
INFO:root:final train perplexity: 4.337676525115967
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.35s/it]
INFO:root:eval mean loss: 22388.26697358631
INFO:root:eval perplexity: 10.14609432220459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [28:36:41<5:49:31, 616.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14896.291362147178
INFO:root:current train perplexity4.333297252655029
INFO:root:current mean train loss 14891.572951455153
INFO:root:current train perplexity4.330236911773682
INFO:root:current mean train loss 14885.017070819806
INFO:root:current train perplexity4.335988998413086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:34<00:00, 514.28s/it]
INFO:root:final mean train loss: 14870.5171843498
INFO:root:final train perplexity: 4.334960460662842
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.07s/it]
INFO:root:eval mean loss: 22386.861979166668
INFO:root:eval perplexity: 10.144620895385742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [28:46:51<5:38:04, 614.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14873.682346573794
INFO:root:current train perplexity4.331940650939941
INFO:root:current mean train loss 14889.516393442624
INFO:root:current train perplexity4.33305025100708

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.02s/it]
INFO:root:final mean train loss: 14868.557451801915
INFO:root:final train perplexity: 4.334122180938721
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.09s/it]
INFO:root:eval mean loss: 22384.160598028273
INFO:root:eval perplexity: 10.141786575317383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [28:57:17<5:29:45, 618.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15016.517996651786
INFO:root:current train perplexity4.369121551513672
INFO:root:current mean train loss 14859.523379629629
INFO:root:current train perplexity4.325575351715088
INFO:root:current mean train loss 14877.714303523937
INFO:root:current train perplexity4.3319597244262695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.73s/it]
INFO:root:final mean train loss: 14859.808231476814
INFO:root:final train perplexity: 4.33038330078125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.82s/it]
INFO:root:eval mean loss: 22387.783389136905
INFO:root:eval perplexity: 10.145590782165527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [29:07:44<5:20:45, 620.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14856.107253502156
INFO:root:current train perplexity4.329944133758545
INFO:root:current mean train loss 14856.847995696859
INFO:root:current train perplexity4.328202724456787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:33<00:00, 513.44s/it]
INFO:root:final mean train loss: 14856.719210716987
INFO:root:final train perplexity: 4.329063892364502
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.58s/it]
INFO:root:eval mean loss: 22393.76720610119
INFO:root:eval perplexity: 10.151875495910645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [29:18:13<5:11:31, 623.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14789.288110977564
INFO:root:current train perplexity4.301196098327637
INFO:root:current mean train loss 14837.30497555081
INFO:root:current train perplexity4.327933311462402
INFO:root:current mean train loss 14865.597096463127
INFO:root:current train perplexity4.3306965827941895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.13s/it]
INFO:root:final mean train loss: 14857.47299095892
INFO:root:final train perplexity: 4.3293867111206055
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.28s/it]
INFO:root:eval mean loss: 22393.631998697918
INFO:root:eval perplexity: 10.151734352111816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [29:29:11<5:06:13, 633.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14890.76038804945
INFO:root:current train perplexity4.342774868011475
INFO:root:current mean train loss 14845.52896964987
INFO:root:current train perplexity4.324863910675049

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.27s/it]
INFO:root:final mean train loss: 14848.291618101059
INFO:root:final train perplexity: 4.325466632843018
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.45s/it]
INFO:root:eval mean loss: 22393.025692894345
INFO:root:eval perplexity: 10.151095390319824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [29:39:43<4:55:27, 633.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14810.594522165698
INFO:root:current train perplexity4.316430568695068
INFO:root:current mean train loss 14871.66172694493
INFO:root:current train perplexity4.3291754722595215
INFO:root:current mean train loss 14862.201388888889
INFO:root:current train perplexity4.325193881988525

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:41<00:00, 521.69s/it]
INFO:root:final mean train loss: 14847.989757907006
INFO:root:final train perplexity: 4.325338363647461
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.59s/it]
INFO:root:eval mean loss: 22394.481305803572
INFO:root:eval perplexity: 10.15262508392334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [29:50:12<4:44:23, 631.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14870.299105674341
INFO:root:current train perplexity4.3221116065979
INFO:root:current mean train loss 14863.843179086538
INFO:root:current train perplexity4.323592185974121

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.65s/it]
INFO:root:final mean train loss: 14845.798265026462
INFO:root:final train perplexity: 4.324403762817383
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.89s/it]
INFO:root:eval mean loss: 22393.500139508928
INFO:root:eval perplexity: 10.151594161987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [30:00:23<4:31:05, 625.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14776.773790724734
INFO:root:current train perplexity4.303300380706787
INFO:root:current mean train loss 14838.052694515307
INFO:root:current train perplexity4.315127849578857
INFO:root:current mean train loss 14858.832462202683
INFO:root:current train perplexity4.325222492218018

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:40<00:00, 520.34s/it]
INFO:root:final mean train loss: 14846.433062153477
INFO:root:final train perplexity: 4.324674129486084
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.07s/it]
INFO:root:eval mean loss: 22396.91564360119
INFO:root:eval perplexity: 10.155181884765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [30:10:40<4:19:40, 623.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14824.97991635101
INFO:root:current train perplexity4.311511516571045
INFO:root:current mean train loss 14838.853304608983
INFO:root:current train perplexity4.3167219161987305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.31s/it]
INFO:root:final mean train loss: 14835.659124558972
INFO:root:final train perplexity: 4.3200812339782715
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.55s/it]
INFO:root:eval mean loss: 22391.448544456845
INFO:root:eval perplexity: 10.14943790435791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [30:21:35<4:13:02, 632.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14840.519914215687
INFO:root:current train perplexity4.302890777587891
INFO:root:current mean train loss 14826.036333298842
INFO:root:current train perplexity4.308764934539795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:42<00:00, 522.76s/it]
INFO:root:final mean train loss: 14830.641904769405
INFO:root:final train perplexity: 4.317944049835205
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.81s/it]
INFO:root:eval mean loss: 22399.836356026786
INFO:root:eval perplexity: 10.158252716064453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [30:31:57<4:01:15, 629.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14833.562825520834
INFO:root:current train perplexity4.290335655212402
INFO:root:current mean train loss 14824.180199484223
INFO:root:current train perplexity4.314464092254639
INFO:root:current mean train loss 14848.111246343904
INFO:root:current train perplexity4.316157817840576

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.52s/it]
INFO:root:final mean train loss: 14826.633383474042
INFO:root:final train perplexity: 4.316236972808838
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:39<00:00, 99.61s/it]
INFO:root:eval mean loss: 22403.10328311012
INFO:root:eval perplexity: 10.161689758300781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [30:42:45<3:52:54, 635.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14897.173757102273
INFO:root:current train perplexity4.312793731689453
INFO:root:current mean train loss 14871.505424647177
INFO:root:current train perplexity4.3216447830200195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:48<00:00, 528.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:48<00:00, 528.06s/it]
INFO:root:final mean train loss: 14836.434893208165
INFO:root:final train perplexity: 4.3204121589660645
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.72s/it]
INFO:root:eval mean loss: 22401.613909040178
INFO:root:eval perplexity: 10.16012191772461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [30:53:15<3:41:41, 633.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14767.081612723214
INFO:root:current train perplexity4.304989814758301
INFO:root:current mean train loss 14816.933666764018
INFO:root:current train perplexity4.318440914154053
INFO:root:current mean train loss 14836.977869301027
INFO:root:current train perplexity4.315821170806885

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:55<00:00, 535.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:55<00:00, 535.09s/it]
INFO:root:final mean train loss: 14828.187563004032
INFO:root:final train perplexity: 4.316898822784424
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.77s/it]
INFO:root:eval mean loss: 22398.952078683036
INFO:root:eval perplexity: 10.15732479095459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [31:03:49<3:31:14, 633.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14793.14206501589
INFO:root:current train perplexity4.310255527496338
INFO:root:current mean train loss 14808.259777908805
INFO:root:current train perplexity4.306222438812256

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.55s/it]
INFO:root:final mean train loss: 14820.61281265751
INFO:root:final train perplexity: 4.3136749267578125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.92s/it]
INFO:root:eval mean loss: 22401.74781436012
INFO:root:eval perplexity: 10.160263061523438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [31:14:15<3:19:56, 631.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14929.863458806818
INFO:root:current train perplexity4.303796768188477
INFO:root:current mean train loss 14849.635566230292
INFO:root:current train perplexity4.317300319671631
INFO:root:current mean train loss 14834.90310278436
INFO:root:current train perplexity4.314548969268799

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:02<00:00, 542.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:02<00:00, 542.36s/it]
INFO:root:final mean train loss: 14819.312354303176
INFO:root:final train perplexity: 4.313121795654297
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.75s/it]
INFO:root:eval mean loss: 22405.00525483631
INFO:root:eval perplexity: 10.163688659667969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [31:24:57<3:10:20, 634.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14767.244807167659
INFO:root:current train perplexity4.301474094390869
INFO:root:current mean train loss 14805.887665356595
INFO:root:current train perplexity4.302703380584717

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.73s/it]
INFO:root:final mean train loss: 14817.684786888862
INFO:root:final train perplexity: 4.312428951263428
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.52s/it]
INFO:root:eval mean loss: 22403.053083147322
INFO:root:eval perplexity: 10.161633491516113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [31:35:14<2:58:20, 629.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14805.404036458332
INFO:root:current train perplexity4.330522537231445
INFO:root:current mean train loss 14837.57038892663
INFO:root:current train perplexity4.319385528564453
INFO:root:current mean train loss 14832.467768895349
INFO:root:current train perplexity4.314022541046143

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.68s/it]
INFO:root:final mean train loss: 14819.260423229587
INFO:root:final train perplexity: 4.313098907470703
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.58s/it]
INFO:root:eval mean loss: 22399.39013671875
INFO:root:eval perplexity: 10.157785415649414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [31:45:34<2:47:05, 626.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14837.216986357276
INFO:root:current train perplexity4.3001790046691895
INFO:root:current mean train loss 14813.494772174401
INFO:root:current train perplexity4.303253650665283

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:36<00:00, 516.05s/it]
INFO:root:final mean train loss: 14808.094399729083
INFO:root:final train perplexity: 4.308351516723633
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.90s/it]
INFO:root:eval mean loss: 22401.389508928572
INFO:root:eval perplexity: 10.159887313842773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [31:55:48<2:35:42, 622.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14733.491570723685
INFO:root:current train perplexity4.279988765716553
INFO:root:current mean train loss 14798.305352219013
INFO:root:current train perplexity4.306304931640625
INFO:root:current mean train loss 14830.71146814355
INFO:root:current train perplexity4.311529159545898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:38<00:00, 518.16s/it]
INFO:root:final mean train loss: 14807.933262978831
INFO:root:final train perplexity: 4.308282852172852
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.46s/it]
INFO:root:eval mean loss: 22402.790713355655
INFO:root:eval perplexity: 10.161359786987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [32:06:04<2:24:47, 620.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14834.350503411091
INFO:root:current train perplexity4.307472229003906
INFO:root:current mean train loss 14838.469326800074
INFO:root:current train perplexity4.309479713439941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:31<00:00, 511.94s/it]
INFO:root:final mean train loss: 14808.236702211441
INFO:root:final train perplexity: 4.308412551879883
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.26s/it]
INFO:root:eval mean loss: 22404.906691778273
INFO:root:eval perplexity: 10.163586616516113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [32:16:14<2:13:48, 617.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.683466372282
INFO:root:current train perplexity4.296792507171631
INFO:root:current mean train loss 14802.90178004319
INFO:root:current train perplexity4.30288028717041
INFO:root:current mean train loss 14818.623265835202
INFO:root:current train perplexity4.307992458343506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.63s/it]
INFO:root:final mean train loss: 14805.25486312374
INFO:root:final train perplexity: 4.307145118713379
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.53s/it]
INFO:root:eval mean loss: 22403.686569940477
INFO:root:eval perplexity: 10.16230297088623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [32:27:01<2:05:15, 626.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14826.686796875
INFO:root:current train perplexity4.3102803230285645
INFO:root:current mean train loss 14794.116378348213
INFO:root:current train perplexity4.301390171051025

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:56<00:00, 536.03s/it]
INFO:root:final mean train loss: 14803.75279186618
INFO:root:final train perplexity: 4.306507110595703
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.79s/it]
INFO:root:eval mean loss: 22402.97828311012
INFO:root:eval perplexity: 10.161559104919434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [32:37:36<1:55:18, 628.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14752.3955078125
INFO:root:current train perplexity4.291415214538574
INFO:root:current mean train loss 14787.108567605806
INFO:root:current train perplexity4.30344820022583
INFO:root:current mean train loss 14804.707319486508
INFO:root:current train perplexity4.3038225173950195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:39<00:00, 519.30s/it]
INFO:root:final mean train loss: 14802.686338363155
INFO:root:final train perplexity: 4.30605411529541
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:37<00:00, 97.28s/it]
INFO:root:eval mean loss: 22406.204892113095
INFO:root:eval perplexity: 10.16495132446289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [32:47:56<1:44:22, 626.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.777912381329
INFO:root:current train perplexity4.309621334075928
INFO:root:current mean train loss 14806.84875283694
INFO:root:current train perplexity4.302756309509277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:52<00:00, 532.87s/it]
INFO:root:final mean train loss: 14799.563311176915
INFO:root:final train perplexity: 4.304728031158447
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.57s/it]
INFO:root:eval mean loss: 22404.774065290178
INFO:root:eval perplexity: 10.163446426391602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [32:58:28<1:34:11, 627.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14927.148216985886
INFO:root:current train perplexity4.316568374633789
INFO:root:current mean train loss 14850.99230677481
INFO:root:current train perplexity4.308932304382324
INFO:root:current mean train loss 14801.0980663217
INFO:root:current train perplexity4.301690101623535

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:43<00:00, 523.20s/it]
INFO:root:final mean train loss: 14794.869908486644
INFO:root:final train perplexity: 4.302735805511475
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:34<00:00, 94.57s/it]
INFO:root:eval mean loss: 22405.61193266369
INFO:root:eval perplexity: 10.164328575134277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [33:08:49<1:23:28, 626.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14794.030555817018
INFO:root:current train perplexity4.303572654724121
INFO:root:current mean train loss 14791.885176528347
INFO:root:current train perplexity4.3000264167785645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:49<00:00, 529.98s/it]
INFO:root:final mean train loss: 14794.683995400706
INFO:root:final train perplexity: 4.302656173706055
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.82s/it]
INFO:root:eval mean loss: 22406.433430989582
INFO:root:eval perplexity: 10.165190696716309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [33:19:19<1:13:08, 626.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14804.788839285715
INFO:root:current train perplexity4.324535369873047
INFO:root:current mean train loss 14806.722547743055
INFO:root:current train perplexity4.302132606506348
INFO:root:current mean train loss 14806.267760970744
INFO:root:current train perplexity4.300077438354492

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:35<00:00, 515.43s/it]
INFO:root:final mean train loss: 14789.878406155494
INFO:root:final train perplexity: 4.300619125366211
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.26s/it]
INFO:root:eval mean loss: 22403.04910714286
INFO:root:eval perplexity: 10.161628723144531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [33:29:30<1:02:13, 622.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14757.437084680316
INFO:root:current train perplexity4.293445587158203
INFO:root:current mean train loss 14788.481565424465
INFO:root:current train perplexity4.299125671386719

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it]
INFO:root:final mean train loss: 14790.180939705142
INFO:root:final train perplexity: 4.300746440887451
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.52s/it]
INFO:root:eval mean loss: 22404.69703311012
INFO:root:eval perplexity: 10.163363456726074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [33:40:17<52:28, 629.79s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14872.826822916666
INFO:root:current train perplexity4.32146692276001
INFO:root:current mean train loss 14796.320923729767
INFO:root:current train perplexity4.300326347351074
INFO:root:current mean train loss 14802.858521018567
INFO:root:current train perplexity4.301332950592041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.64s/it]
INFO:root:final mean train loss: 14792.2003922001
INFO:root:final train perplexity: 4.301603317260742
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:35<00:00, 95.83s/it]
INFO:root:eval mean loss: 22406.48718843006
INFO:root:eval perplexity: 10.165246963500977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [33:50:41<41:52, 628.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14821.513285542582
INFO:root:current train perplexity4.302849769592285
INFO:root:current mean train loss 14813.38828840805
INFO:root:current train perplexity4.305037975311279

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:04<00:00, 544.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:04<00:00, 544.94s/it]
INFO:root:final mean train loss: 14789.456535093246
INFO:root:final train perplexity: 4.30043888092041
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.36s/it]
INFO:root:eval mean loss: 22405.810593377977
INFO:root:eval perplexity: 10.164539337158203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [34:01:26<31:39, 633.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14794.33804960029
INFO:root:current train perplexity4.29245662689209
INFO:root:current mean train loss 14803.995062554633
INFO:root:current train perplexity4.296645641326904
INFO:root:current mean train loss 14805.631651073816
INFO:root:current train perplexity4.3023762702941895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:51<00:00, 531.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:51<00:00, 531.36s/it]
INFO:root:final mean train loss: 14793.608422064011
INFO:root:final train perplexity: 4.302200794219971
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.69s/it]
INFO:root:eval mean loss: 22406.201706659227
INFO:root:eval perplexity: 10.164948463439941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [34:11:58<21:05, 632.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14821.905807976973
INFO:root:current train perplexity4.310457229614258
INFO:root:current mean train loss 14809.710546875
INFO:root:current train perplexity4.303832054138184

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:47<00:00, 527.15s/it]
INFO:root:final mean train loss: 14792.52406360257
INFO:root:final train perplexity: 4.3017401695251465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.38s/it]
INFO:root:eval mean loss: 22406.42638578869
INFO:root:eval perplexity: 10.165184020996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [34:22:24<10:30, 630.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14778.685297539894
INFO:root:current train perplexity4.2912092208862305
INFO:root:current mean train loss 14787.84654017857
INFO:root:current train perplexity4.299201011657715
INFO:root:current mean train loss 14802.584241365132
INFO:root:current train perplexity4.301182746887207

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:44<00:00, 524.47s/it]
INFO:root:final mean train loss: 14790.817512758316
INFO:root:final train perplexity: 4.301015853881836
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:36<00:00, 96.75s/it]
INFO:root:eval mean loss: 22406.30008370536
INFO:root:eval perplexity: 10.165053367614746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_10/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [34:32:49<00:00, 629.04s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [34:32:49<00:00, 621.85s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.44s/it]
INFO:root:eval mean loss: 22406.30008370536
INFO:root:eval perplexity: 10.165053367614746
INFO:root:evalaution complete
INFO:root:save model final: small_topk_10/final
