INFO:root:Output: large_distilroberta_roberta_64_low
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11368.699406171087
INFO:root:current train perplexity7868.06689453125
INFO:root:current mean train loss 9728.246491245289
INFO:root:current train perplexity2174.1455078125
INFO:root:current mean train loss 8965.617461852007
INFO:root:current train perplexity1195.2989501953125
INFO:root:current mean train loss 8535.738850299576
INFO:root:current train perplexity843.8784790039062
INFO:root:current mean train loss 8184.446642308054
INFO:root:current train perplexity646.981201171875
INFO:root:current mean train loss 7925.7683957311665
INFO:root:current train perplexity523.0516967773438
INFO:root:current mean train loss 7704.258159675653
INFO:root:current train perplexity438.5334167480469
INFO:root:current mean train loss 7500.3476140830335
INFO:root:current train perplexity373.8497009277344
INFO:root:current mean train loss 7315.203233084504
INFO:root:current train perplexity323.86920166015625
INFO:root:current mean train loss 7154.500660817067
INFO:root:current train perplexity283.364013671875
INFO:root:current mean train loss 6988.858559272634
INFO:root:current train perplexity249.17918395996094
INFO:root:current mean train loss 6832.717765292692
INFO:root:current train perplexity219.89755249023438
INFO:root:current mean train loss 6684.700653522541
INFO:root:current train perplexity195.40121459960938
INFO:root:current mean train loss 6538.58377792871
INFO:root:current train perplexity174.15347290039062
INFO:root:current mean train loss 6404.499738106654
INFO:root:current train perplexity156.4046173095703
INFO:root:current mean train loss 6277.876766698669
INFO:root:current train perplexity141.47218322753906
INFO:root:current mean train loss 6159.085762621165
INFO:root:current train perplexity128.67498779296875
INFO:root:current mean train loss 6045.850818542854
INFO:root:current train perplexity117.71047973632812
INFO:root:current mean train loss 5940.757218797311
INFO:root:current train perplexity108.40130615234375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.37s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.53s/it]
INFO:root:eval mean loss: 4065.791478978979
INFO:root:eval perplexity: 28.61026954650879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/1

  0%|          | 1/200 [08:27<28:02:49, 507.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3995.7162475585938
INFO:root:current train perplexity22.535972595214844
INFO:root:current mean train loss 3886.560304839036
INFO:root:current train perplexity21.138776779174805
INFO:root:current mean train loss 3847.4266696506074
INFO:root:current train perplexity20.507976531982422
INFO:root:current mean train loss 3802.9356334058543
INFO:root:current train perplexity19.819971084594727
INFO:root:current mean train loss 3770.324272742638
INFO:root:current train perplexity19.31041717529297
INFO:root:current mean train loss 3734.0627058162245
INFO:root:current train perplexity18.828710556030273
INFO:root:current mean train loss 3705.0530272644837
INFO:root:current train perplexity18.408266067504883
INFO:root:current mean train loss 3670.4257679518373
INFO:root:current train perplexity17.99406623840332
INFO:root:current mean train loss 3637.6988495471433
INFO:root:current train perplexity17.600114822387695
INFO:root:current mean train loss 3611.013402414114
INFO:root:current train perplexity17.210657119750977
INFO:root:current mean train loss 3584.17330067552
INFO:root:current train perplexity16.855873107910156
INFO:root:current mean train loss 3558.9122410709288
INFO:root:current train perplexity16.539072036743164
INFO:root:current mean train loss 3534.1809935318797
INFO:root:current train perplexity16.226985931396484
INFO:root:current mean train loss 3512.746693898117
INFO:root:current train perplexity15.945178031921387
INFO:root:current mean train loss 3490.906966387215
INFO:root:current train perplexity15.687588691711426
INFO:root:current mean train loss 3469.6436450324145
INFO:root:current train perplexity15.436305046081543
INFO:root:current mean train loss 3448.186159492719
INFO:root:current train perplexity15.208691596984863
INFO:root:current mean train loss 3431.035148851799
INFO:root:current train perplexity15.002579689025879
INFO:root:current mean train loss 3413.979426178113
INFO:root:current train perplexity14.79308032989502
INFO:root:current mean train loss 3397.05565093803
INFO:root:current train perplexity14.595098495483398


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.83s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.18s/it]
INFO:root:eval mean loss: 3428.701216597457
INFO:root:eval perplexity: 16.91577911376953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/2

  1%|          | 2/200 [17:09<28:23:48, 516.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3076.156434955019
INFO:root:current train perplexity11.011399269104004
INFO:root:current mean train loss 3034.1537608670114
INFO:root:current train perplexity10.852206230163574
INFO:root:current mean train loss 3010.823076632913
INFO:root:current train perplexity10.71461296081543
INFO:root:current mean train loss 3010.6252610031906
INFO:root:current train perplexity10.719725608825684
INFO:root:current mean train loss 3007.5575901234124
INFO:root:current train perplexity10.695592880249023
INFO:root:current mean train loss 2995.4916369239563
INFO:root:current train perplexity10.595539093017578
INFO:root:current mean train loss 2982.803496880554
INFO:root:current train perplexity10.497142791748047
INFO:root:current mean train loss 2972.150887233011
INFO:root:current train perplexity10.41995620727539
INFO:root:current mean train loss 2963.504820385185
INFO:root:current train perplexity10.35197925567627
INFO:root:current mean train loss 2953.647209993385
INFO:root:current train perplexity10.27176284790039
INFO:root:current mean train loss 2944.0439550024957
INFO:root:current train perplexity10.194148063659668
INFO:root:current mean train loss 2934.7469020213894
INFO:root:current train perplexity10.121264457702637
INFO:root:current mean train loss 2927.5796042981424
INFO:root:current train perplexity10.060685157775879
INFO:root:current mean train loss 2920.3503906982605
INFO:root:current train perplexity10.005290031433105
INFO:root:current mean train loss 2912.5246714920077
INFO:root:current train perplexity9.953194618225098
INFO:root:current mean train loss 2906.2887304496394
INFO:root:current train perplexity9.897647857666016
INFO:root:current mean train loss 2899.009122905743
INFO:root:current train perplexity9.840545654296875
INFO:root:current mean train loss 2890.3066289321714
INFO:root:current train perplexity9.77695083618164
INFO:root:current mean train loss 2883.283256401732
INFO:root:current train perplexity9.72594165802002
INFO:root:current mean train loss 2876.3395958203328
INFO:root:current train perplexity9.675928115844727


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.67s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.34s/it]
INFO:root:eval mean loss: 3210.9655204520927
INFO:root:eval perplexity: 14.134831428527832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/3

  2%|â–         | 3/200 [25:43<28:10:29, 514.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2753.942080078125
INFO:root:current train perplexity8.591827392578125
INFO:root:current mean train loss 2727.218966471354
INFO:root:current train perplexity8.53355884552002
INFO:root:current mean train loss 2729.0528740234377
INFO:root:current train perplexity8.56557559967041
INFO:root:current mean train loss 2718.583990652902
INFO:root:current train perplexity8.501493453979492
INFO:root:current mean train loss 2718.193134765625
INFO:root:current train perplexity8.490177154541016
INFO:root:current mean train loss 2709.4589581853693
INFO:root:current train perplexity8.45340347290039
INFO:root:current mean train loss 2703.6424267578127
INFO:root:current train perplexity8.42354965209961
INFO:root:current mean train loss 2695.904558919271
INFO:root:current train perplexity8.389511108398438
INFO:root:current mean train loss 2692.3513893037684
INFO:root:current train perplexity8.364900588989258
INFO:root:current mean train loss 2690.1127829461348
INFO:root:current train perplexity8.338618278503418
INFO:root:current mean train loss 2686.2384851655506
INFO:root:current train perplexity8.314143180847168
INFO:root:current mean train loss 2680.641945482337
INFO:root:current train perplexity8.27975082397461
INFO:root:current mean train loss 2676.0365291015623
INFO:root:current train perplexity8.241250038146973
INFO:root:current mean train loss 2673.4960101996526
INFO:root:current train perplexity8.217843055725098
INFO:root:current mean train loss 2668.1650112809807
INFO:root:current train perplexity8.195072174072266
INFO:root:current mean train loss 2665.832610887097
INFO:root:current train perplexity8.1780424118042
INFO:root:current mean train loss 2661.853332741477
INFO:root:current train perplexity8.157251358032227
INFO:root:current mean train loss 2657.0682540457587
INFO:root:current train perplexity8.128996849060059
INFO:root:current mean train loss 2652.931552404455
INFO:root:current train perplexity8.107199668884277
INFO:root:current mean train loss 2649.2413294396033
INFO:root:current train perplexity8.084028244018555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.75s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.88s/it]
INFO:root:eval mean loss: 3096.818315385698
INFO:root:eval perplexity: 12.864673614501953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/4

  2%|â–         | 4/200 [34:12<27:55:19, 512.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2572.271542677239
INFO:root:current train perplexity7.593681812286377
INFO:root:current mean train loss 2563.148060324663
INFO:root:current train perplexity7.542482852935791
INFO:root:current mean train loss 2553.6312534746608
INFO:root:current train perplexity7.472645282745361
INFO:root:current mean train loss 2547.881531925877
INFO:root:current train perplexity7.468454360961914
INFO:root:current mean train loss 2554.86535513835
INFO:root:current train perplexity7.4820170402526855
INFO:root:current mean train loss 2552.4723307291665
INFO:root:current train perplexity7.454302787780762
INFO:root:current mean train loss 2544.200253144912
INFO:root:current train perplexity7.435689926147461
INFO:root:current mean train loss 2540.5925149731097
INFO:root:current train perplexity7.415164470672607
INFO:root:current mean train loss 2535.9830430678703
INFO:root:current train perplexity7.394134998321533
INFO:root:current mean train loss 2534.3118837153406
INFO:root:current train perplexity7.384798526763916
INFO:root:current mean train loss 2532.3235492509666
INFO:root:current train perplexity7.3769965171813965
INFO:root:current mean train loss 2526.7626461496493
INFO:root:current train perplexity7.363087177276611
INFO:root:current mean train loss 2522.999374907508
INFO:root:current train perplexity7.347389221191406
INFO:root:current mean train loss 2520.2830447104175
INFO:root:current train perplexity7.328724384307861
INFO:root:current mean train loss 2518.8411351823447
INFO:root:current train perplexity7.321499347686768
INFO:root:current mean train loss 2517.582666296067
INFO:root:current train perplexity7.305444717407227
INFO:root:current mean train loss 2515.399512407089
INFO:root:current train perplexity7.29067850112915
INFO:root:current mean train loss 2516.316740129921
INFO:root:current train perplexity7.285370826721191
INFO:root:current mean train loss 2514.608601844432
INFO:root:current train perplexity7.272209644317627
INFO:root:current mean train loss 2512.0895419564413
INFO:root:current train perplexity7.2590532302856445


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.80s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.40s/it]
INFO:root:eval mean loss: 3021.6007464984514
INFO:root:eval perplexity: 12.090741157531738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/5

  2%|â–Ž         | 5/200 [42:39<27:39:02, 510.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2407.473198300316
INFO:root:current train perplexity6.815610885620117
INFO:root:current mean train loss 2430.762491640837
INFO:root:current train perplexity6.838662624359131
INFO:root:current mean train loss 2432.793977979203
INFO:root:current train perplexity6.839343070983887
INFO:root:current mean train loss 2437.715692838033
INFO:root:current train perplexity6.8570427894592285
INFO:root:current mean train loss 2439.256642743576
INFO:root:current train perplexity6.8555755615234375
INFO:root:current mean train loss 2434.792403129682
INFO:root:current train perplexity6.833975791931152
INFO:root:current mean train loss 2434.0376490542762
INFO:root:current train perplexity6.8172287940979
INFO:root:current mean train loss 2432.3975677490234
INFO:root:current train perplexity6.815688610076904
INFO:root:current mean train loss 2429.7693917805254
INFO:root:current train perplexity6.809298038482666
INFO:root:current mean train loss 2427.349656267864
INFO:root:current train perplexity6.795555114746094
INFO:root:current mean train loss 2426.444027045556
INFO:root:current train perplexity6.787872314453125
INFO:root:current mean train loss 2425.3090018195076
INFO:root:current train perplexity6.776988506317139
INFO:root:current mean train loss 2423.6540086217387
INFO:root:current train perplexity6.775206565856934
INFO:root:current mean train loss 2424.543814774883
INFO:root:current train perplexity6.770750522613525
INFO:root:current mean train loss 2422.1877030118135
INFO:root:current train perplexity6.7592644691467285
INFO:root:current mean train loss 2420.3073520082417
INFO:root:current train perplexity6.750067234039307
INFO:root:current mean train loss 2419.441299837162
INFO:root:current train perplexity6.746775150299072
INFO:root:current mean train loss 2417.978720626489
INFO:root:current train perplexity6.740520000457764
INFO:root:current mean train loss 2416.768675008397
INFO:root:current train perplexity6.733146667480469


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.61s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.96s/it]
INFO:root:eval mean loss: 2967.3803820910753
INFO:root:eval perplexity: 11.561898231506348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/6

  3%|â–Ž         | 6/200 [51:03<27:24:09, 508.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2394.3544921875
INFO:root:current train perplexity6.9174723625183105
INFO:root:current mean train loss 2355.2069974087253
INFO:root:current train perplexity6.411548137664795
INFO:root:current mean train loss 2360.8372341174986
INFO:root:current train perplexity6.424051761627197
INFO:root:current mean train loss 2356.1628961404695
INFO:root:current train perplexity6.434692859649658
INFO:root:current mean train loss 2358.1398323040057
INFO:root:current train perplexity6.432718753814697
INFO:root:current mean train loss 2353.6099488035647
INFO:root:current train perplexity6.417791843414307
INFO:root:current mean train loss 2352.031996233491
INFO:root:current train perplexity6.415878772735596
INFO:root:current mean train loss 2348.923799740605
INFO:root:current train perplexity6.4037370681762695
INFO:root:current mean train loss 2344.681335677815
INFO:root:current train perplexity6.394759654998779
INFO:root:current mean train loss 2346.6045714451393
INFO:root:current train perplexity6.398919582366943
INFO:root:current mean train loss 2346.5418932873768
INFO:root:current train perplexity6.394528865814209
INFO:root:current mean train loss 2347.2165225771314
INFO:root:current train perplexity6.388685703277588
INFO:root:current mean train loss 2346.1842031867973
INFO:root:current train perplexity6.379957675933838
INFO:root:current mean train loss 2347.0687962384704
INFO:root:current train perplexity6.383026599884033
INFO:root:current mean train loss 2347.856221124838
INFO:root:current train perplexity6.381208896636963
INFO:root:current mean train loss 2349.079419953755
INFO:root:current train perplexity6.383670806884766
INFO:root:current mean train loss 2347.2510387032276
INFO:root:current train perplexity6.374547958374023
INFO:root:current mean train loss 2347.3055843328602
INFO:root:current train perplexity6.371755599975586
INFO:root:current mean train loss 2346.9817844117633
INFO:root:current train perplexity6.367018699645996
INFO:root:current mean train loss 2345.9089593059575
INFO:root:current train perplexity6.36223030090332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.81s/it]
INFO:root:eval mean loss: 2923.0774292358765
INFO:root:eval perplexity: 11.147003173828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/7

  4%|â–Ž         | 7/200 [59:20<27:03:24, 504.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2298.892374674479
INFO:root:current train perplexity6.144957065582275
INFO:root:current mean train loss 2294.3392820196636
INFO:root:current train perplexity6.139481544494629
INFO:root:current mean train loss 2296.8338555852206
INFO:root:current train perplexity6.114217758178711
INFO:root:current mean train loss 2286.7627563476562
INFO:root:current train perplexity6.092382907867432
INFO:root:current mean train loss 2289.3821828741775
INFO:root:current train perplexity6.0946784019470215
INFO:root:current mean train loss 2292.4575841012606
INFO:root:current train perplexity6.097117900848389
INFO:root:current mean train loss 2293.2922600310985
INFO:root:current train perplexity6.1065754890441895
INFO:root:current mean train loss 2292.665584468576
INFO:root:current train perplexity6.098980903625488
INFO:root:current mean train loss 2291.9105487254546
INFO:root:current train perplexity6.1017327308654785
INFO:root:current mean train loss 2292.281963406565
INFO:root:current train perplexity6.0958991050720215
INFO:root:current mean train loss 2293.6299340148807
INFO:root:current train perplexity6.099130630493164
INFO:root:current mean train loss 2292.800342102597
INFO:root:current train perplexity6.100227355957031
INFO:root:current mean train loss 2290.255679877521
INFO:root:current train perplexity6.095271110534668
INFO:root:current mean train loss 2290.0935247534144
INFO:root:current train perplexity6.089052200317383
INFO:root:current mean train loss 2289.245711613105
INFO:root:current train perplexity6.0883026123046875
INFO:root:current mean train loss 2287.9519141879478
INFO:root:current train perplexity6.0805182456970215
INFO:root:current mean train loss 2287.739779214187
INFO:root:current train perplexity6.076469421386719
INFO:root:current mean train loss 2285.908621276216
INFO:root:current train perplexity6.070007801055908
INFO:root:current mean train loss 2284.3840363589584
INFO:root:current train perplexity6.068878173828125
INFO:root:current mean train loss 2285.21489581127
INFO:root:current train perplexity6.068830490112305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.50s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.40s/it]
INFO:root:eval mean loss: 2899.6761368301895
INFO:root:eval perplexity: 10.933893203735352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/8

  4%|â–         | 8/200 [1:07:43<26:53:10, 504.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2232.8766462053572
INFO:root:current train perplexity5.769875526428223
INFO:root:current mean train loss 2240.104796911169
INFO:root:current train perplexity5.8672332763671875
INFO:root:current mean train loss 2248.2494010762966
INFO:root:current train perplexity5.890466690063477
INFO:root:current mean train loss 2247.6027026731576
INFO:root:current train perplexity5.885665416717529
INFO:root:current mean train loss 2251.1787777253953
INFO:root:current train perplexity5.889411926269531
INFO:root:current mean train loss 2249.986678135952
INFO:root:current train perplexity5.872438430786133
INFO:root:current mean train loss 2247.0893299012673
INFO:root:current train perplexity5.864596843719482
INFO:root:current mean train loss 2248.843603515625
INFO:root:current train perplexity5.864648818969727
INFO:root:current mean train loss 2247.2953870579154
INFO:root:current train perplexity5.863117694854736
INFO:root:current mean train loss 2247.8040716650653
INFO:root:current train perplexity5.86506986618042
INFO:root:current mean train loss 2246.104330724449
INFO:root:current train perplexity5.8604536056518555
INFO:root:current mean train loss 2241.917519617291
INFO:root:current train perplexity5.848513603210449
INFO:root:current mean train loss 2239.086789323444
INFO:root:current train perplexity5.848665714263916
INFO:root:current mean train loss 2238.9487647581636
INFO:root:current train perplexity5.845224380493164
INFO:root:current mean train loss 2237.934292570639
INFO:root:current train perplexity5.841403961181641
INFO:root:current mean train loss 2236.688417316648
INFO:root:current train perplexity5.837127685546875
INFO:root:current mean train loss 2237.3305822343273
INFO:root:current train perplexity5.83740234375
INFO:root:current mean train loss 2237.2500411591545
INFO:root:current train perplexity5.83772611618042
INFO:root:current mean train loss 2236.310895324208
INFO:root:current train perplexity5.83356237411499
INFO:root:current mean train loss 2235.994698426397
INFO:root:current train perplexity5.836352348327637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.15s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.78s/it]
INFO:root:eval mean loss: 2877.577740093609
INFO:root:eval perplexity: 10.7363920211792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/9

  4%|â–         | 9/200 [1:15:58<26:35:37, 501.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2206.080289400541
INFO:root:current train perplexity5.659103870391846
INFO:root:current mean train loss 2209.7542820980675
INFO:root:current train perplexity5.641197204589844
INFO:root:current mean train loss 2215.9417341928634
INFO:root:current train perplexity5.65908670425415
INFO:root:current mean train loss 2210.65423896096
INFO:root:current train perplexity5.659266948699951
INFO:root:current mean train loss 2208.7740807997443
INFO:root:current train perplexity5.667902946472168
INFO:root:current mean train loss 2208.718721251557
INFO:root:current train perplexity5.67142391204834
INFO:root:current mean train loss 2203.6945594834406
INFO:root:current train perplexity5.667465686798096
INFO:root:current mean train loss 2199.2299579052215
INFO:root:current train perplexity5.665503978729248
INFO:root:current mean train loss 2202.6382000703766
INFO:root:current train perplexity5.673655033111572
INFO:root:current mean train loss 2202.9451254195524
INFO:root:current train perplexity5.672233581542969
INFO:root:current mean train loss 2201.913848528844
INFO:root:current train perplexity5.67138671875
INFO:root:current mean train loss 2201.3746938705444
INFO:root:current train perplexity5.670253276824951
INFO:root:current mean train loss 2198.9024249677077
INFO:root:current train perplexity5.663853168487549
INFO:root:current mean train loss 2198.7075860740165
INFO:root:current train perplexity5.662720680236816
INFO:root:current mean train loss 2199.2011049549114
INFO:root:current train perplexity5.660924911499023
INFO:root:current mean train loss 2198.717967947734
INFO:root:current train perplexity5.660486221313477
INFO:root:current mean train loss 2199.039426494164
INFO:root:current train perplexity5.66013240814209
INFO:root:current mean train loss 2198.0164790741383
INFO:root:current train perplexity5.655461311340332
INFO:root:current mean train loss 2196.315268267308
INFO:root:current train perplexity5.653008937835693
INFO:root:current mean train loss 2194.78025505191
INFO:root:current train perplexity5.649449825286865


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.43s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.97s/it]
INFO:root:eval mean loss: 2857.639298722551
INFO:root:eval perplexity: 10.561259269714355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/10

  5%|â–Œ         | 10/200 [1:24:15<26:23:32, 500.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2163.057167827219
INFO:root:current train perplexity5.52174186706543
INFO:root:current mean train loss 2145.212043355908
INFO:root:current train perplexity5.468077182769775
INFO:root:current mean train loss 2153.328501194383
INFO:root:current train perplexity5.4860639572143555
INFO:root:current mean train loss 2154.153735153074
INFO:root:current train perplexity5.496519088745117
INFO:root:current mean train loss 2155.343173744836
INFO:root:current train perplexity5.490987777709961
INFO:root:current mean train loss 2159.478611092995
INFO:root:current train perplexity5.500313758850098
INFO:root:current mean train loss 2164.21452552784
INFO:root:current train perplexity5.508470058441162
INFO:root:current mean train loss 2166.1067150101085
INFO:root:current train perplexity5.506799221038818
INFO:root:current mean train loss 2163.9179683285834
INFO:root:current train perplexity5.500731945037842
INFO:root:current mean train loss 2162.983317309243
INFO:root:current train perplexity5.4972991943359375
INFO:root:current mean train loss 2162.282537504933
INFO:root:current train perplexity5.505177021026611
INFO:root:current mean train loss 2161.426564108112
INFO:root:current train perplexity5.50752592086792
INFO:root:current mean train loss 2161.3133335218736
INFO:root:current train perplexity5.505549430847168
INFO:root:current mean train loss 2161.578195620663
INFO:root:current train perplexity5.504469871520996
INFO:root:current mean train loss 2163.7816335783273
INFO:root:current train perplexity5.506767749786377
INFO:root:current mean train loss 2163.9472706820875
INFO:root:current train perplexity5.505884170532227
INFO:root:current mean train loss 2162.9684007574942
INFO:root:current train perplexity5.507940292358398
INFO:root:current mean train loss 2160.729900146622
INFO:root:current train perplexity5.503316879272461
INFO:root:current mean train loss 2160.6218941628836
INFO:root:current train perplexity5.500074863433838
INFO:root:current mean train loss 2159.651070474186
INFO:root:current train perplexity5.497126579284668


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 2845.632696661505
INFO:root:eval perplexity: 10.457178115844727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/11

  6%|â–Œ         | 11/200 [1:32:29<26:09:10, 498.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2133.409990177598
INFO:root:current train perplexity5.3248162269592285
INFO:root:current mean train loss 2122.6895049720683
INFO:root:current train perplexity5.349730491638184
INFO:root:current mean train loss 2110.568354679988
INFO:root:current train perplexity5.3174662590026855
INFO:root:current mean train loss 2113.0842851233606
INFO:root:current train perplexity5.332862377166748
INFO:root:current mean train loss 2117.0767576115613
INFO:root:current train perplexity5.335495948791504
INFO:root:current mean train loss 2118.171546701685
INFO:root:current train perplexity5.344157695770264
INFO:root:current mean train loss 2121.9274799135615
INFO:root:current train perplexity5.351714134216309
INFO:root:current mean train loss 2122.7025759942053
INFO:root:current train perplexity5.354422569274902
INFO:root:current mean train loss 2125.500294291408
INFO:root:current train perplexity5.361598014831543
INFO:root:current mean train loss 2126.3332942939433
INFO:root:current train perplexity5.360224723815918
INFO:root:current mean train loss 2127.612785999727
INFO:root:current train perplexity5.363190174102783
INFO:root:current mean train loss 2128.0289784217566
INFO:root:current train perplexity5.366029739379883
INFO:root:current mean train loss 2129.753917261008
INFO:root:current train perplexity5.369955062866211
INFO:root:current mean train loss 2129.9583577297813
INFO:root:current train perplexity5.366292476654053
INFO:root:current mean train loss 2128.768601839828
INFO:root:current train perplexity5.36299991607666
INFO:root:current mean train loss 2129.9701184035853
INFO:root:current train perplexity5.364350318908691
INFO:root:current mean train loss 2129.7964090186647
INFO:root:current train perplexity5.362170696258545
INFO:root:current mean train loss 2130.235054315138
INFO:root:current train perplexity5.363704204559326
INFO:root:current mean train loss 2129.065957515389
INFO:root:current train perplexity5.362857341766357


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.90s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it]
INFO:root:eval mean loss: 2833.6187960421357
INFO:root:eval perplexity: 10.354060173034668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/12

  6%|â–Œ         | 12/200 [1:40:46<26:00:00, 497.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2159.333251953125
INFO:root:current train perplexity5.351458549499512
INFO:root:current mean train loss 2097.8811615879094
INFO:root:current train perplexity5.2052507400512695
INFO:root:current mean train loss 2097.0086573708822
INFO:root:current train perplexity5.2134690284729
INFO:root:current mean train loss 2103.0940392623247
INFO:root:current train perplexity5.241501331329346
INFO:root:current mean train loss 2103.9067931068744
INFO:root:current train perplexity5.243751525878906
INFO:root:current mean train loss 2102.111508439597
INFO:root:current train perplexity5.243139266967773
INFO:root:current mean train loss 2103.4417945267155
INFO:root:current train perplexity5.248385429382324
INFO:root:current mean train loss 2102.1700538429054
INFO:root:current train perplexity5.2456536293029785
INFO:root:current mean train loss 2100.1478365735425
INFO:root:current train perplexity5.240879058837891
INFO:root:current mean train loss 2101.8275396573054
INFO:root:current train perplexity5.245495796203613
INFO:root:current mean train loss 2100.5585315586445
INFO:root:current train perplexity5.241287708282471
INFO:root:current mean train loss 2101.671172680686
INFO:root:current train perplexity5.2479023933410645
INFO:root:current mean train loss 2100.320639542907
INFO:root:current train perplexity5.246262550354004
INFO:root:current mean train loss 2099.410174705757
INFO:root:current train perplexity5.249547958374023
INFO:root:current mean train loss 2098.7432233140203
INFO:root:current train perplexity5.247308731079102
INFO:root:current mean train loss 2098.9797407951023
INFO:root:current train perplexity5.246134281158447
INFO:root:current mean train loss 2100.086660098375
INFO:root:current train perplexity5.248338222503662
INFO:root:current mean train loss 2099.28720499555
INFO:root:current train perplexity5.244078636169434
INFO:root:current mean train loss 2099.223474249731
INFO:root:current train perplexity5.243687152862549
INFO:root:current mean train loss 2100.2349377550454
INFO:root:current train perplexity5.247689247131348


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.15s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.96s/it]
INFO:root:eval mean loss: 2822.292579444679
INFO:root:eval perplexity: 10.257773399353027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/13

  6%|â–‹         | 13/200 [1:49:08<25:54:49, 498.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2099.5597900390626
INFO:root:current train perplexity5.200981616973877
INFO:root:current mean train loss 2070.4888844807942
INFO:root:current train perplexity5.1255693435668945
INFO:root:current mean train loss 2074.4689203435723
INFO:root:current train perplexity5.135758876800537
INFO:root:current mean train loss 2066.7963367462157
INFO:root:current train perplexity5.119272232055664
INFO:root:current mean train loss 2071.3225074404763
INFO:root:current train perplexity5.129676818847656
INFO:root:current mean train loss 2071.1996133657603
INFO:root:current train perplexity5.130720138549805
INFO:root:current mean train loss 2073.2707397460936
INFO:root:current train perplexity5.135476112365723
INFO:root:current mean train loss 2073.0377824571397
INFO:root:current train perplexity5.137514591217041
INFO:root:current mean train loss 2073.0427065965605
INFO:root:current train perplexity5.138138294219971
INFO:root:current mean train loss 2074.57776184082
INFO:root:current train perplexity5.144201278686523
INFO:root:current mean train loss 2074.2553161621095
INFO:root:current train perplexity5.146950721740723
INFO:root:current mean train loss 2073.506255231585
INFO:root:current train perplexity5.150284767150879
INFO:root:current mean train loss 2074.686544649718
INFO:root:current train perplexity5.153134346008301
INFO:root:current mean train loss 2075.960722767223
INFO:root:current train perplexity5.1516265869140625
INFO:root:current mean train loss 2075.8886588083187
INFO:root:current train perplexity5.14844274520874
INFO:root:current mean train loss 2074.9612016376695
INFO:root:current train perplexity5.145933628082275
INFO:root:current mean train loss 2075.0454985441984
INFO:root:current train perplexity5.146190643310547
INFO:root:current mean train loss 2074.756555814521
INFO:root:current train perplexity5.145442485809326
INFO:root:current mean train loss 2075.670434167883
INFO:root:current train perplexity5.143826484680176
INFO:root:current mean train loss 2075.3078330993653
INFO:root:current train perplexity5.142674922943115


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.34s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.22s/it]
INFO:root:eval mean loss: 2822.3163160719314
INFO:root:eval perplexity: 10.257974624633789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/14

  7%|â–‹         | 14/200 [1:57:21<25:41:36, 497.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2055.08136151288
INFO:root:current train perplexity5.003762722015381
INFO:root:current mean train loss 2055.3721513957003
INFO:root:current train perplexity5.065629959106445
INFO:root:current mean train loss 2055.1878651808083
INFO:root:current train perplexity5.063962459564209
INFO:root:current mean train loss 2055.64931010351
INFO:root:current train perplexity5.047236442565918
INFO:root:current mean train loss 2055.690870202106
INFO:root:current train perplexity5.055319786071777
INFO:root:current mean train loss 2053.3954944006778
INFO:root:current train perplexity5.05491304397583
INFO:root:current mean train loss 2050.523218463317
INFO:root:current train perplexity5.0540242195129395
INFO:root:current mean train loss 2048.7107826346783
INFO:root:current train perplexity5.055919647216797
INFO:root:current mean train loss 2045.140155094926
INFO:root:current train perplexity5.047741889953613
INFO:root:current mean train loss 2046.5247503095402
INFO:root:current train perplexity5.047673225402832
INFO:root:current mean train loss 2045.8721356442488
INFO:root:current train perplexity5.044137477874756
INFO:root:current mean train loss 2047.3200308901235
INFO:root:current train perplexity5.04440450668335
INFO:root:current mean train loss 2049.312188557877
INFO:root:current train perplexity5.04641580581665
INFO:root:current mean train loss 2048.4444643076267
INFO:root:current train perplexity5.043402671813965
INFO:root:current mean train loss 2049.767718629034
INFO:root:current train perplexity5.0449371337890625
INFO:root:current mean train loss 2050.4479547623464
INFO:root:current train perplexity5.04460334777832
INFO:root:current mean train loss 2050.4200858533854
INFO:root:current train perplexity5.044370651245117
INFO:root:current mean train loss 2051.118645737757
INFO:root:current train perplexity5.042913913726807
INFO:root:current mean train loss 2051.279207764336
INFO:root:current train perplexity5.042712688446045
INFO:root:current mean train loss 2052.0525411547737
INFO:root:current train perplexity5.047359943389893


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.19s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.46s/it]
INFO:root:eval mean loss: 2822.983443893112
INFO:root:eval perplexity: 10.263623237609863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/15

  8%|â–Š         | 15/200 [2:05:37<25:31:49, 496.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2021.3782280815972
INFO:root:current train perplexity4.859189033508301
INFO:root:current mean train loss 2013.7502060927354
INFO:root:current train perplexity4.923081398010254
INFO:root:current mean train loss 2024.637413685716
INFO:root:current train perplexity4.9509172439575195
INFO:root:current mean train loss 2022.6310776532707
INFO:root:current train perplexity4.932798385620117
INFO:root:current mean train loss 2027.9345367028325
INFO:root:current train perplexity4.939513683319092
INFO:root:current mean train loss 2030.3819337700247
INFO:root:current train perplexity4.950146198272705
INFO:root:current mean train loss 2028.8563909968104
INFO:root:current train perplexity4.950082778930664
INFO:root:current mean train loss 2029.937762920673
INFO:root:current train perplexity4.95259428024292
INFO:root:current mean train loss 2028.842248277865
INFO:root:current train perplexity4.94816780090332
INFO:root:current mean train loss 2030.7272665155758
INFO:root:current train perplexity4.955037593841553
INFO:root:current mean train loss 2031.6564954146036
INFO:root:current train perplexity4.952426910400391
INFO:root:current mean train loss 2032.1808671604203
INFO:root:current train perplexity4.955339431762695
INFO:root:current mean train loss 2033.3380207749265
INFO:root:current train perplexity4.958720684051514
INFO:root:current mean train loss 2032.7977488755828
INFO:root:current train perplexity4.959954261779785
INFO:root:current mean train loss 2034.1801584865566
INFO:root:current train perplexity4.963484287261963
INFO:root:current mean train loss 2034.0534880060027
INFO:root:current train perplexity4.964504241943359
INFO:root:current mean train loss 2033.4699366799
INFO:root:current train perplexity4.96480131149292
INFO:root:current mean train loss 2034.063148559431
INFO:root:current train perplexity4.967465400695801
INFO:root:current mean train loss 2032.4651295025114
INFO:root:current train perplexity4.962393283843994
INFO:root:current mean train loss 2031.0242706267593
INFO:root:current train perplexity4.964151859283447


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.89s/it]
INFO:root:eval mean loss: 2810.552435247748
INFO:root:eval perplexity: 10.158916473388672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/16

  8%|â–Š         | 16/200 [2:13:51<25:20:34, 495.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2005.8672046930017
INFO:root:current train perplexity4.852581024169922
INFO:root:current mean train loss 1995.3191888820359
INFO:root:current train perplexity4.840303421020508
INFO:root:current mean train loss 1996.203473643623
INFO:root:current train perplexity4.837924957275391
INFO:root:current mean train loss 1998.7526957468203
INFO:root:current train perplexity4.842609405517578
INFO:root:current mean train loss 1999.0850907829915
INFO:root:current train perplexity4.847462177276611
INFO:root:current mean train loss 2004.235884310694
INFO:root:current train perplexity4.857739448547363
INFO:root:current mean train loss 2007.2759176558425
INFO:root:current train perplexity4.86694860458374
INFO:root:current mean train loss 2008.2869291985855
INFO:root:current train perplexity4.862539768218994
INFO:root:current mean train loss 2008.8563427229835
INFO:root:current train perplexity4.871364116668701
INFO:root:current mean train loss 2008.3936023338938
INFO:root:current train perplexity4.8721923828125
INFO:root:current mean train loss 2009.3094303476598
INFO:root:current train perplexity4.876059055328369
INFO:root:current mean train loss 2008.0195527243675
INFO:root:current train perplexity4.877328395843506
INFO:root:current mean train loss 2007.696927170412
INFO:root:current train perplexity4.876745223999023
INFO:root:current mean train loss 2007.4841458176627
INFO:root:current train perplexity4.877925395965576
INFO:root:current mean train loss 2008.5243504963141
INFO:root:current train perplexity4.881727695465088
INFO:root:current mean train loss 2009.552527065265
INFO:root:current train perplexity4.88203239440918
INFO:root:current mean train loss 2010.8275743321271
INFO:root:current train perplexity4.884603023529053
INFO:root:current mean train loss 2011.1007514320343
INFO:root:current train perplexity4.884754657745361
INFO:root:current mean train loss 2010.6931590126644
INFO:root:current train perplexity4.884336948394775
INFO:root:current mean train loss 2010.189303432486
INFO:root:current train perplexity4.884367942810059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.30s/it]
INFO:root:eval mean loss: 2806.018116260792
INFO:root:eval perplexity: 10.120991706848145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/17
##########################best############
  8%|â–Š         | 17/200 [2:22:08<25:13:57, 496.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1983.571351484819
INFO:root:current train perplexity4.778604984283447
INFO:root:current mean train loss 1990.7356586862118
INFO:root:current train perplexity4.7950334548950195
INFO:root:current mean train loss 1991.185774061415
INFO:root:current train perplexity4.78840446472168
INFO:root:current mean train loss 1990.8305188995046
INFO:root:current train perplexity4.779558181762695
INFO:root:current mean train loss 1991.844539454726
INFO:root:current train perplexity4.790802478790283
INFO:root:current mean train loss 1984.8348014987246
INFO:root:current train perplexity4.782813549041748
INFO:root:current mean train loss 1985.4863575780114
INFO:root:current train perplexity4.7884955406188965
INFO:root:current mean train loss 1988.0670935926098
INFO:root:current train perplexity4.789103984832764
INFO:root:current mean train loss 1988.02964576515
INFO:root:current train perplexity4.7902350425720215
INFO:root:current mean train loss 1985.5519150954026
INFO:root:current train perplexity4.78922700881958
INFO:root:current mean train loss 1987.8270725923426
INFO:root:current train perplexity4.796225070953369
INFO:root:current mean train loss 1985.9781130395754
INFO:root:current train perplexity4.79506254196167
INFO:root:current mean train loss 1986.2018920708888
INFO:root:current train perplexity4.796279430389404
INFO:root:current mean train loss 1987.021751205928
INFO:root:current train perplexity4.797851085662842
INFO:root:current mean train loss 1986.4152446459698
INFO:root:current train perplexity4.797138214111328
INFO:root:current mean train loss 1987.938059309568
INFO:root:current train perplexity4.79984188079834
INFO:root:current mean train loss 1988.9449545331477
INFO:root:current train perplexity4.804917812347412
INFO:root:current mean train loss 1989.6116324815175
INFO:root:current train perplexity4.806109428405762
INFO:root:current mean train loss 1989.912049956241
INFO:root:current train perplexity4.807494640350342


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.84s/it]
INFO:root:eval mean loss: 2809.1769110419013
INFO:root:eval perplexity: 10.147396087646484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/18

  9%|â–‰         | 18/200 [2:30:27<25:08:12, 497.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1915.2902587890626
INFO:root:current train perplexity4.718996524810791
INFO:root:current mean train loss 1972.9249558221727
INFO:root:current train perplexity4.739126205444336
INFO:root:current mean train loss 1964.3094017959222
INFO:root:current train perplexity4.722031116485596
INFO:root:current mean train loss 1970.0145055551998
INFO:root:current train perplexity4.73248815536499
INFO:root:current mean train loss 1968.529803542149
INFO:root:current train perplexity4.727287292480469
INFO:root:current mean train loss 1966.706841497138
INFO:root:current train perplexity4.715427398681641
INFO:root:current mean train loss 1965.4814031427557
INFO:root:current train perplexity4.719135284423828
INFO:root:current mean train loss 1970.251571157469
INFO:root:current train perplexity4.7268452644348145
INFO:root:current mean train loss 1969.7452595775912
INFO:root:current train perplexity4.7245306968688965
INFO:root:current mean train loss 1969.5696212178436
INFO:root:current train perplexity4.726681709289551
INFO:root:current mean train loss 1971.8604763049984
INFO:root:current train perplexity4.732885360717773
INFO:root:current mean train loss 1972.120692851209
INFO:root:current train perplexity4.735551834106445
INFO:root:current mean train loss 1972.8037926891532
INFO:root:current train perplexity4.737565040588379
INFO:root:current mean train loss 1972.4849265146072
INFO:root:current train perplexity4.739865303039551
INFO:root:current mean train loss 1973.385562600784
INFO:root:current train perplexity4.740030765533447
INFO:root:current mean train loss 1971.7595523872249
INFO:root:current train perplexity4.736382484436035
INFO:root:current mean train loss 1971.624114020517
INFO:root:current train perplexity4.737227439880371
INFO:root:current mean train loss 1972.2624359936308
INFO:root:current train perplexity4.7389912605285645
INFO:root:current mean train loss 1972.1238348878983
INFO:root:current train perplexity4.740796089172363
INFO:root:current mean train loss 1972.6557240403542
INFO:root:current train perplexity4.74198055267334


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.33s/it]
INFO:root:eval mean loss: 2811.96106433582
INFO:root:eval perplexity: 10.17072582244873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/19

 10%|â–‰         | 19/200 [2:38:45<25:00:15, 497.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1977.7140280983665
INFO:root:current train perplexity4.545651912689209
INFO:root:current mean train loss 1953.2848620805585
INFO:root:current train perplexity4.6504225730896
INFO:root:current mean train loss 1963.03567697336
INFO:root:current train perplexity4.6682000160217285
INFO:root:current mean train loss 1957.75537109375
INFO:root:current train perplexity4.6602582931518555
INFO:root:current mean train loss 1954.3961771743557
INFO:root:current train perplexity4.6603875160217285
INFO:root:current mean train loss 1950.3999418645983
INFO:root:current train perplexity4.655305862426758
INFO:root:current mean train loss 1951.9551295436847
INFO:root:current train perplexity4.663325309753418
INFO:root:current mean train loss 1953.1824163294234
INFO:root:current train perplexity4.665255546569824
INFO:root:current mean train loss 1950.841203749905
INFO:root:current train perplexity4.662357330322266
INFO:root:current mean train loss 1952.8328529076566
INFO:root:current train perplexity4.66763162612915
INFO:root:current mean train loss 1952.2167948444762
INFO:root:current train perplexity4.666615009307861
INFO:root:current mean train loss 1952.6721677729154
INFO:root:current train perplexity4.6669158935546875
INFO:root:current mean train loss 1952.7136253444341
INFO:root:current train perplexity4.667884349822998
INFO:root:current mean train loss 1953.849096347272
INFO:root:current train perplexity4.670652866363525
INFO:root:current mean train loss 1953.521478451757
INFO:root:current train perplexity4.671647548675537
INFO:root:current mean train loss 1954.01920153183
INFO:root:current train perplexity4.672458171844482
INFO:root:current mean train loss 1954.6679541497283
INFO:root:current train perplexity4.67499303817749
INFO:root:current mean train loss 1953.6627050526051
INFO:root:current train perplexity4.675156593322754
INFO:root:current mean train loss 1954.1602809332335
INFO:root:current train perplexity4.675878524780273
INFO:root:current mean train loss 1956.2853135695443
INFO:root:current train perplexity4.678733825683594


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.60s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.75s/it]
INFO:root:eval mean loss: 2811.6332201341966
INFO:root:eval perplexity: 10.167978286743164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/20

 10%|â–ˆ         | 20/200 [2:47:04<24:53:47, 497.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1932.2422970502805
INFO:root:current train perplexity4.549544811248779
INFO:root:current mean train loss 1937.397418783723
INFO:root:current train perplexity4.579058647155762
INFO:root:current mean train loss 1932.2278557302564
INFO:root:current train perplexity4.576700210571289
INFO:root:current mean train loss 1938.5407858879516
INFO:root:current train perplexity4.579401016235352
INFO:root:current mean train loss 1934.2806338090832
INFO:root:current train perplexity4.56805944442749
INFO:root:current mean train loss 1936.010297163077
INFO:root:current train perplexity4.57912540435791
INFO:root:current mean train loss 1934.3488007308172
INFO:root:current train perplexity4.5878586769104
INFO:root:current mean train loss 1935.5388214978539
INFO:root:current train perplexity4.591105937957764
INFO:root:current mean train loss 1935.090821912945
INFO:root:current train perplexity4.5950398445129395
INFO:root:current mean train loss 1936.6196902664071
INFO:root:current train perplexity4.6011810302734375
INFO:root:current mean train loss 1938.2216171837404
INFO:root:current train perplexity4.605457782745361
INFO:root:current mean train loss 1937.288518595842
INFO:root:current train perplexity4.603448390960693
INFO:root:current mean train loss 1938.3784376734009
INFO:root:current train perplexity4.606129169464111
INFO:root:current mean train loss 1937.6410734420217
INFO:root:current train perplexity4.6077728271484375
INFO:root:current mean train loss 1938.5666720222648
INFO:root:current train perplexity4.613799095153809
INFO:root:current mean train loss 1938.1299081942414
INFO:root:current train perplexity4.61435604095459
INFO:root:current mean train loss 1937.7711084818534
INFO:root:current train perplexity4.6172261238098145
INFO:root:current mean train loss 1937.9098346835242
INFO:root:current train perplexity4.61782693862915
INFO:root:current mean train loss 1938.9287260054505
INFO:root:current train perplexity4.618759632110596
INFO:root:current mean train loss 1939.5176854637741
INFO:root:current train perplexity4.619734764099121


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.03s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 2816.9261896173516
INFO:root:eval perplexity: 10.212468147277832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/21

 10%|â–ˆ         | 21/200 [2:55:18<24:41:57, 496.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1926.423581804548
INFO:root:current train perplexity4.560752868652344
INFO:root:current mean train loss 1914.32323044997
INFO:root:current train perplexity4.556828498840332
INFO:root:current mean train loss 1908.2643547058105
INFO:root:current train perplexity4.550673484802246
INFO:root:current mean train loss 1917.4170367637378
INFO:root:current train perplexity4.564244747161865
INFO:root:current mean train loss 1917.2386407684862
INFO:root:current train perplexity4.5548014640808105
INFO:root:current mean train loss 1917.098924595675
INFO:root:current train perplexity4.550557613372803
INFO:root:current mean train loss 1916.1980661531775
INFO:root:current train perplexity4.5475873947143555
INFO:root:current mean train loss 1915.6930541992188
INFO:root:current train perplexity4.546473026275635
INFO:root:current mean train loss 1917.202474433685
INFO:root:current train perplexity4.549936771392822
INFO:root:current mean train loss 1919.5351626344307
INFO:root:current train perplexity4.556443691253662
INFO:root:current mean train loss 1922.7261638063374
INFO:root:current train perplexity4.562885761260986
INFO:root:current mean train loss 1922.1350431343265
INFO:root:current train perplexity4.561647891998291
INFO:root:current mean train loss 1923.5559297792472
INFO:root:current train perplexity4.563816070556641
INFO:root:current mean train loss 1923.5108414821568
INFO:root:current train perplexity4.565578460693359
INFO:root:current mean train loss 1924.3508869213063
INFO:root:current train perplexity4.564273357391357
INFO:root:current mean train loss 1925.1077227359565
INFO:root:current train perplexity4.566134929656982
INFO:root:current mean train loss 1925.486403460664
INFO:root:current train perplexity4.566376209259033
INFO:root:current mean train loss 1925.477571248465
INFO:root:current train perplexity4.567012310028076
INFO:root:current mean train loss 1925.0377435355351
INFO:root:current train perplexity4.564963340759277
INFO:root:current mean train loss 1925.153130092504
INFO:root:current train perplexity4.566511154174805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it]
INFO:root:eval mean loss: 2826.530129005959
INFO:root:eval perplexity: 10.293691635131836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/22

 11%|â–ˆ         | 22/200 [3:03:38<24:36:17, 497.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1879.656544306507
INFO:root:current train perplexity4.462139129638672
INFO:root:current mean train loss 1900.1241779658146
INFO:root:current train perplexity4.497603893280029
INFO:root:current mean train loss 1909.7872761597127
INFO:root:current train perplexity4.503147602081299
INFO:root:current mean train loss 1905.613672660439
INFO:root:current train perplexity4.497452259063721
INFO:root:current mean train loss 1905.0045191823301
INFO:root:current train perplexity4.498316764831543
INFO:root:current mean train loss 1903.1404801347078
INFO:root:current train perplexity4.4985151290893555
INFO:root:current mean train loss 1904.2342342473069
INFO:root:current train perplexity4.498970985412598
INFO:root:current mean train loss 1905.5841822457653
INFO:root:current train perplexity4.500959873199463
INFO:root:current mean train loss 1904.4676089991408
INFO:root:current train perplexity4.49662446975708
INFO:root:current mean train loss 1904.9268521566676
INFO:root:current train perplexity4.498971939086914
INFO:root:current mean train loss 1905.057196255388
INFO:root:current train perplexity4.496584415435791
INFO:root:current mean train loss 1906.038058359808
INFO:root:current train perplexity4.498735427856445
INFO:root:current mean train loss 1905.330740354048
INFO:root:current train perplexity4.501079559326172
INFO:root:current mean train loss 1907.3944088240567
INFO:root:current train perplexity4.502955436706543
INFO:root:current mean train loss 1907.4598667950186
INFO:root:current train perplexity4.505909442901611
INFO:root:current mean train loss 1907.5547801585842
INFO:root:current train perplexity4.50521993637085
INFO:root:current mean train loss 1906.9406110053283
INFO:root:current train perplexity4.507605075836182
INFO:root:current mean train loss 1908.4035081341653
INFO:root:current train perplexity4.509542942047119
INFO:root:current mean train loss 1908.1567330021815
INFO:root:current train perplexity4.507828235626221
INFO:root:current mean train loss 1908.6775079911818
INFO:root:current train perplexity4.508660316467285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.12s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.20s/it]
INFO:root:eval mean loss: 2821.858568529467
INFO:root:eval perplexity: 10.254103660583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/23

 12%|â–ˆâ–        | 23/200 [3:11:54<24:26:53, 497.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1892.4073554144966
INFO:root:current train perplexity4.50033712387085
INFO:root:current mean train loss 1885.060343852796
INFO:root:current train perplexity4.459527492523193
INFO:root:current mean train loss 1882.8031506768589
INFO:root:current train perplexity4.451195240020752
INFO:root:current mean train loss 1884.0812794220753
INFO:root:current train perplexity4.452761650085449
INFO:root:current mean train loss 1887.0739643953284
INFO:root:current train perplexity4.457072734832764
INFO:root:current mean train loss 1886.8315543481858
INFO:root:current train perplexity4.449093818664551
INFO:root:current mean train loss 1888.989377582937
INFO:root:current train perplexity4.44667911529541
INFO:root:current mean train loss 1887.4853036614913
INFO:root:current train perplexity4.446487903594971
INFO:root:current mean train loss 1887.3930306081022
INFO:root:current train perplexity4.4461774826049805
INFO:root:current mean train loss 1888.298349831321
INFO:root:current train perplexity4.446541786193848
INFO:root:current mean train loss 1889.9249498279817
INFO:root:current train perplexity4.447859287261963
INFO:root:current mean train loss 1891.1010539078914
INFO:root:current train perplexity4.44840669631958
INFO:root:current mean train loss 1891.0470758955607
INFO:root:current train perplexity4.447986125946045
INFO:root:current mean train loss 1892.1089778762928
INFO:root:current train perplexity4.449321746826172
INFO:root:current mean train loss 1892.3256425486315
INFO:root:current train perplexity4.44846248626709
INFO:root:current mean train loss 1891.9698606862962
INFO:root:current train perplexity4.450063705444336
INFO:root:current mean train loss 1892.0343596148068
INFO:root:current train perplexity4.450490474700928
INFO:root:current mean train loss 1893.3848193632157
INFO:root:current train perplexity4.45243501663208
INFO:root:current mean train loss 1894.6611251265915
INFO:root:current train perplexity4.456386089324951


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.96s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.16s/it]
INFO:root:eval mean loss: 2826.3534305790167
INFO:root:eval perplexity: 10.292192459106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/24

 12%|â–ˆâ–        | 24/200 [3:20:08<24:15:54, 496.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1903.7273821149554
INFO:root:current train perplexity4.5739426612854
INFO:root:current mean train loss 1881.7338388033002
INFO:root:current train perplexity4.379388332366943
INFO:root:current mean train loss 1874.2588026258682
INFO:root:current train perplexity4.379427433013916
INFO:root:current mean train loss 1876.044568785627
INFO:root:current train perplexity4.3721699714660645
INFO:root:current mean train loss 1877.3877088092177
INFO:root:current train perplexity4.377864360809326
INFO:root:current mean train loss 1876.6701761279585
INFO:root:current train perplexity4.393030166625977
INFO:root:current mean train loss 1877.1436097900794
INFO:root:current train perplexity4.396001815795898
INFO:root:current mean train loss 1878.1909659681091
INFO:root:current train perplexity4.398714542388916
INFO:root:current mean train loss 1876.4122143524435
INFO:root:current train perplexity4.391959190368652
INFO:root:current mean train loss 1876.298357743807
INFO:root:current train perplexity4.396576881408691
INFO:root:current mean train loss 1876.157266444459
INFO:root:current train perplexity4.3991923332214355
INFO:root:current mean train loss 1875.4675981061569
INFO:root:current train perplexity4.400832176208496
INFO:root:current mean train loss 1877.8193461521657
INFO:root:current train perplexity4.405570983886719
INFO:root:current mean train loss 1879.5266086196025
INFO:root:current train perplexity4.4087958335876465
INFO:root:current mean train loss 1879.4575698516346
INFO:root:current train perplexity4.409661769866943
INFO:root:current mean train loss 1880.5729103214944
INFO:root:current train perplexity4.410293102264404
INFO:root:current mean train loss 1880.9889522948915
INFO:root:current train perplexity4.412343978881836
INFO:root:current mean train loss 1881.914154463926
INFO:root:current train perplexity4.412229537963867
INFO:root:current mean train loss 1881.8348196142606
INFO:root:current train perplexity4.412428379058838
INFO:root:current mean train loss 1881.486751690421
INFO:root:current train perplexity4.411764144897461


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.67s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.31s/it]
INFO:root:eval mean loss: 2825.3733474685623
INFO:root:eval perplexity: 10.283875465393066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/25

 12%|â–ˆâ–Ž        | 25/200 [3:28:23<24:06:28, 495.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1873.6088053385417
INFO:root:current train perplexity4.28769063949585
INFO:root:current mean train loss 1874.8886423418598
INFO:root:current train perplexity4.341243743896484
INFO:root:current mean train loss 1869.0623299734932
INFO:root:current train perplexity4.332976818084717
INFO:root:current mean train loss 1871.1502806110148
INFO:root:current train perplexity4.353514671325684
INFO:root:current mean train loss 1865.8077769729327
INFO:root:current train perplexity4.343410968780518
INFO:root:current mean train loss 1863.3095344368737
INFO:root:current train perplexity4.339473247528076
INFO:root:current mean train loss 1863.7637143257336
INFO:root:current train perplexity4.342617511749268
INFO:root:current mean train loss 1865.310324821683
INFO:root:current train perplexity4.351561069488525
INFO:root:current mean train loss 1864.106372870288
INFO:root:current train perplexity4.348310947418213
INFO:root:current mean train loss 1866.0014553317776
INFO:root:current train perplexity4.3523077964782715
INFO:root:current mean train loss 1866.5774581432343
INFO:root:current train perplexity4.3508992195129395
INFO:root:current mean train loss 1866.3861847466846
INFO:root:current train perplexity4.3510284423828125
INFO:root:current mean train loss 1865.5334471658944
INFO:root:current train perplexity4.350589752197266
INFO:root:current mean train loss 1864.8331577266451
INFO:root:current train perplexity4.349678039550781
INFO:root:current mean train loss 1865.3337806101595
INFO:root:current train perplexity4.352861404418945
INFO:root:current mean train loss 1864.4086872411212
INFO:root:current train perplexity4.352304458618164
INFO:root:current mean train loss 1864.9436131369305
INFO:root:current train perplexity4.355036735534668
INFO:root:current mean train loss 1865.6755448980841
INFO:root:current train perplexity4.357185363769531
INFO:root:current mean train loss 1865.6275381121718
INFO:root:current train perplexity4.358560085296631
INFO:root:current mean train loss 1865.964886385785
INFO:root:current train perplexity4.360403060913086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 2827.228364595064
INFO:root:eval perplexity: 10.299620628356934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/26

 13%|â–ˆâ–Ž        | 26/200 [3:36:37<23:56:06, 495.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1824.6118819073934
INFO:root:current train perplexity4.287781715393066
INFO:root:current mean train loss 1830.3348691683289
INFO:root:current train perplexity4.266740322113037
INFO:root:current mean train loss 1837.7370174930304
INFO:root:current train perplexity4.285117149353027
INFO:root:current mean train loss 1846.7125469666423
INFO:root:current train perplexity4.290621757507324
INFO:root:current mean train loss 1844.0143337120005
INFO:root:current train perplexity4.294038772583008
INFO:root:current mean train loss 1842.3561023423058
INFO:root:current train perplexity4.290163993835449
INFO:root:current mean train loss 1844.036399424727
INFO:root:current train perplexity4.295953273773193
INFO:root:current mean train loss 1846.9096210186299
INFO:root:current train perplexity4.298436164855957
INFO:root:current mean train loss 1847.5660751651214
INFO:root:current train perplexity4.301031589508057
INFO:root:current mean train loss 1849.221868176516
INFO:root:current train perplexity4.304027080535889
INFO:root:current mean train loss 1851.116386362272
INFO:root:current train perplexity4.304880142211914
INFO:root:current mean train loss 1850.0404337315472
INFO:root:current train perplexity4.299779891967773
INFO:root:current mean train loss 1851.5849596587618
INFO:root:current train perplexity4.302504062652588
INFO:root:current mean train loss 1851.3292111617961
INFO:root:current train perplexity4.305202484130859
INFO:root:current mean train loss 1851.2307007767772
INFO:root:current train perplexity4.306885719299316
INFO:root:current mean train loss 1851.807818314381
INFO:root:current train perplexity4.312245845794678
INFO:root:current mean train loss 1851.9575097120658
INFO:root:current train perplexity4.312396049499512
INFO:root:current mean train loss 1852.7763401230884
INFO:root:current train perplexity4.313930511474609
INFO:root:current mean train loss 1852.4044192105387
INFO:root:current train perplexity4.313831806182861
INFO:root:current mean train loss 1852.9720481624927
INFO:root:current train perplexity4.315716743469238


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.10s/it]
INFO:root:eval mean loss: 2844.010741454345
INFO:root:eval perplexity: 10.443192481994629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/27

 14%|â–ˆâ–Ž        | 27/200 [3:44:47<23:43:10, 493.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1825.2375172582165
INFO:root:current train perplexity4.214712142944336
INFO:root:current mean train loss 1831.1341745883603
INFO:root:current train perplexity4.2505950927734375
INFO:root:current mean train loss 1829.387421364008
INFO:root:current train perplexity4.246608257293701
INFO:root:current mean train loss 1828.5552743240441
INFO:root:current train perplexity4.2400970458984375
INFO:root:current mean train loss 1834.6085652846957
INFO:root:current train perplexity4.2518134117126465
INFO:root:current mean train loss 1835.0948195372005
INFO:root:current train perplexity4.248157501220703
INFO:root:current mean train loss 1838.1857587356335
INFO:root:current train perplexity4.256738662719727
INFO:root:current mean train loss 1836.7788379035085
INFO:root:current train perplexity4.257458209991455
INFO:root:current mean train loss 1835.9318513314486
INFO:root:current train perplexity4.256084442138672
INFO:root:current mean train loss 1836.2828632904243
INFO:root:current train perplexity4.256640434265137
INFO:root:current mean train loss 1835.7955037281058
INFO:root:current train perplexity4.257781982421875
INFO:root:current mean train loss 1836.3973197871128
INFO:root:current train perplexity4.260300159454346
INFO:root:current mean train loss 1836.7644978388314
INFO:root:current train perplexity4.261595726013184
INFO:root:current mean train loss 1835.564255816947
INFO:root:current train perplexity4.261105060577393
INFO:root:current mean train loss 1836.3425180777929
INFO:root:current train perplexity4.263710975646973
INFO:root:current mean train loss 1836.9980387265325
INFO:root:current train perplexity4.265052795410156
INFO:root:current mean train loss 1838.5983919113767
INFO:root:current train perplexity4.2677459716796875
INFO:root:current mean train loss 1839.7093592655672
INFO:root:current train perplexity4.2696638107299805
INFO:root:current mean train loss 1840.4804807730716
INFO:root:current train perplexity4.271149635314941
INFO:root:current mean train loss 1840.068187304488
INFO:root:current train perplexity4.271326541900635


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.87s/it]
INFO:root:eval mean loss: 2845.9523999096755
INFO:root:eval perplexity: 10.459935188293457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/28

 14%|â–ˆâ–        | 28/200 [3:53:01<23:35:09, 493.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1834.880984700521
INFO:root:current train perplexity4.223860740661621
INFO:root:current mean train loss 1818.6723486328126
INFO:root:current train perplexity4.202727317810059
INFO:root:current mean train loss 1815.4075989879261
INFO:root:current train perplexity4.191615104675293
INFO:root:current mean train loss 1814.6203362630208
INFO:root:current train perplexity4.200381278991699
INFO:root:current mean train loss 1816.9212073396382
INFO:root:current train perplexity4.206944465637207
INFO:root:current mean train loss 1819.9140898862092
INFO:root:current train perplexity4.214357852935791
INFO:root:current mean train loss 1819.8662563295718
INFO:root:current train perplexity4.214517116546631
INFO:root:current mean train loss 1821.3126326234878
INFO:root:current train perplexity4.215132236480713
INFO:root:current mean train loss 1822.1454976283483
INFO:root:current train perplexity4.2123799324035645
INFO:root:current mean train loss 1822.5528638321314
INFO:root:current train perplexity4.214732646942139
INFO:root:current mean train loss 1822.9341285882995
INFO:root:current train perplexity4.214533805847168
INFO:root:current mean train loss 1824.148182450964
INFO:root:current train perplexity4.21596622467041
INFO:root:current mean train loss 1823.9870102826287
INFO:root:current train perplexity4.2191481590271
INFO:root:current mean train loss 1824.4794429154829
INFO:root:current train perplexity4.2216291427612305
INFO:root:current mean train loss 1825.436485616393
INFO:root:current train perplexity4.2246294021606445
INFO:root:current mean train loss 1825.1274036613343
INFO:root:current train perplexity4.223300457000732
INFO:root:current mean train loss 1825.2790383774486
INFO:root:current train perplexity4.226136207580566
INFO:root:current mean train loss 1825.9707190113336
INFO:root:current train perplexity4.227867603302002
INFO:root:current mean train loss 1827.1471528645834
INFO:root:current train perplexity4.229534149169922
INFO:root:current mean train loss 1828.0495629573775
INFO:root:current train perplexity4.230747699737549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it]
INFO:root:eval mean loss: 2850.611035596143
INFO:root:eval perplexity: 10.500208854675293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/29

 14%|â–ˆâ–        | 29/200 [4:01:14<23:27:03, 493.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1794.7797453507133
INFO:root:current train perplexity4.117155075073242
INFO:root:current mean train loss 1798.4939339955647
INFO:root:current train perplexity4.13865852355957
INFO:root:current mean train loss 1807.4304813750803
INFO:root:current train perplexity4.154880046844482
INFO:root:current mean train loss 1810.5218356385524
INFO:root:current train perplexity4.16416597366333
INFO:root:current mean train loss 1812.3409967189882
INFO:root:current train perplexity4.166263580322266
INFO:root:current mean train loss 1814.997576739337
INFO:root:current train perplexity4.169914245605469
INFO:root:current mean train loss 1814.7478358979859
INFO:root:current train perplexity4.170164585113525
INFO:root:current mean train loss 1811.1122353293679
INFO:root:current train perplexity4.166242599487305
INFO:root:current mean train loss 1812.1421129714213
INFO:root:current train perplexity4.170525550842285
INFO:root:current mean train loss 1812.7410488743935
INFO:root:current train perplexity4.1750569343566895
INFO:root:current mean train loss 1813.3092769860348
INFO:root:current train perplexity4.176990985870361
INFO:root:current mean train loss 1813.5459646954632
INFO:root:current train perplexity4.177628517150879
INFO:root:current mean train loss 1814.9785725974446
INFO:root:current train perplexity4.180130958557129
INFO:root:current mean train loss 1815.5934695539804
INFO:root:current train perplexity4.183232307434082
INFO:root:current mean train loss 1815.241105066867
INFO:root:current train perplexity4.184456825256348
INFO:root:current mean train loss 1815.1281109527129
INFO:root:current train perplexity4.186217784881592
INFO:root:current mean train loss 1814.3761057030788
INFO:root:current train perplexity4.186975479125977
INFO:root:current mean train loss 1814.7693249838692
INFO:root:current train perplexity4.187084197998047
INFO:root:current mean train loss 1814.7761132399578
INFO:root:current train perplexity4.1875224113464355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.67s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.19s/it]
INFO:root:eval mean loss: 2849.183227172485
INFO:root:eval perplexity: 10.487849235534668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/30

 15%|â–ˆâ–Œ        | 30/200 [4:09:31<23:21:32, 494.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1810.5431450737847
INFO:root:current train perplexity4.147531032562256
INFO:root:current mean train loss 1797.9338882866255
INFO:root:current train perplexity4.125175476074219
INFO:root:current mean train loss 1799.0466039922248
INFO:root:current train perplexity4.12131404876709
INFO:root:current mean train loss 1796.0959879557292
INFO:root:current train perplexity4.126998424530029
INFO:root:current mean train loss 1796.7543151407779
INFO:root:current train perplexity4.1345438957214355
INFO:root:current mean train loss 1793.542558411484
INFO:root:current train perplexity4.126458644866943
INFO:root:current mean train loss 1794.5100063580794
INFO:root:current train perplexity4.126871585845947
INFO:root:current mean train loss 1796.6590119914713
INFO:root:current train perplexity4.1294050216674805
INFO:root:current mean train loss 1796.9618405788879
INFO:root:current train perplexity4.133818626403809
INFO:root:current mean train loss 1796.4128491828676
INFO:root:current train perplexity4.1335906982421875
INFO:root:current mean train loss 1796.7805464926985
INFO:root:current train perplexity4.134222030639648
INFO:root:current mean train loss 1797.8371285936444
INFO:root:current train perplexity4.137746334075928
INFO:root:current mean train loss 1800.0027228040672
INFO:root:current train perplexity4.13902473449707
INFO:root:current mean train loss 1800.406987737389
INFO:root:current train perplexity4.138573169708252
INFO:root:current mean train loss 1800.1216656351191
INFO:root:current train perplexity4.1385087966918945
INFO:root:current mean train loss 1800.0348372007538
INFO:root:current train perplexity4.138994216918945
INFO:root:current mean train loss 1801.3764687129767
INFO:root:current train perplexity4.143926620483398
INFO:root:current mean train loss 1802.1727943487374
INFO:root:current train perplexity4.147678375244141
INFO:root:current mean train loss 1803.3449692860568
INFO:root:current train perplexity4.150048732757568
INFO:root:current mean train loss 1803.5067297894022
INFO:root:current train perplexity4.150939464569092


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.80s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.29s/it]
INFO:root:eval mean loss: 2856.836604671077
INFO:root:eval perplexity: 10.554266929626465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/31

 16%|â–ˆâ–Œ        | 31/200 [4:17:43<23:11:07, 493.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1788.2697096604568
INFO:root:current train perplexity4.093362331390381
INFO:root:current mean train loss 1782.2745235382565
INFO:root:current train perplexity4.074483394622803
INFO:root:current mean train loss 1775.4515175608408
INFO:root:current train perplexity4.072314739227295
INFO:root:current mean train loss 1775.0496238287242
INFO:root:current train perplexity4.074276924133301
INFO:root:current mean train loss 1781.6894909496039
INFO:root:current train perplexity4.0807318687438965
INFO:root:current mean train loss 1785.6505874227662
INFO:root:current train perplexity4.0927276611328125
INFO:root:current mean train loss 1786.3720334574057
INFO:root:current train perplexity4.089446544647217
INFO:root:current mean train loss 1784.8476147191911
INFO:root:current train perplexity4.086195945739746
INFO:root:current mean train loss 1786.567419315366
INFO:root:current train perplexity4.090897560119629
INFO:root:current mean train loss 1786.5623112260394
INFO:root:current train perplexity4.093051910400391
INFO:root:current mean train loss 1786.619971440782
INFO:root:current train perplexity4.092787265777588
INFO:root:current mean train loss 1788.2553201407777
INFO:root:current train perplexity4.095118045806885
INFO:root:current mean train loss 1788.6211632484324
INFO:root:current train perplexity4.095489025115967
INFO:root:current mean train loss 1788.2606157904117
INFO:root:current train perplexity4.095913887023926
INFO:root:current mean train loss 1789.1493038054437
INFO:root:current train perplexity4.0986504554748535
INFO:root:current mean train loss 1789.9259313980842
INFO:root:current train perplexity4.101369857788086
INFO:root:current mean train loss 1790.3438411398274
INFO:root:current train perplexity4.104290008544922
INFO:root:current mean train loss 1790.5802011854541
INFO:root:current train perplexity4.106801986694336
INFO:root:current mean train loss 1791.094123631422
INFO:root:current train perplexity4.107112884521484
INFO:root:current mean train loss 1790.9087313357916
INFO:root:current train perplexity4.108590126037598


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.99s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.27s/it]
INFO:root:eval mean loss: 2856.610137481231
INFO:root:eval perplexity: 10.552298545837402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/32

 16%|â–ˆâ–Œ        | 32/200 [4:26:00<23:04:49, 494.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1732.6684314816498
INFO:root:current train perplexity4.005354881286621
INFO:root:current mean train loss 1759.9295099431818
INFO:root:current train perplexity4.023642539978027
INFO:root:current mean train loss 1761.4966438199267
INFO:root:current train perplexity4.030913352966309
INFO:root:current mean train loss 1767.7180449816644
INFO:root:current train perplexity4.036234378814697
INFO:root:current mean train loss 1768.012184711396
INFO:root:current train perplexity4.035724639892578
INFO:root:current mean train loss 1770.4218338602814
INFO:root:current train perplexity4.041144371032715
INFO:root:current mean train loss 1774.4305125662179
INFO:root:current train perplexity4.0502166748046875
INFO:root:current mean train loss 1774.6095670594823
INFO:root:current train perplexity4.0544023513793945
INFO:root:current mean train loss 1775.6056116721809
INFO:root:current train perplexity4.05652379989624
INFO:root:current mean train loss 1775.7640441700357
INFO:root:current train perplexity4.057004451751709
INFO:root:current mean train loss 1776.2001404218227
INFO:root:current train perplexity4.0614848136901855
INFO:root:current mean train loss 1777.118973913051
INFO:root:current train perplexity4.060934066772461
INFO:root:current mean train loss 1778.2819360685464
INFO:root:current train perplexity4.06392240524292
INFO:root:current mean train loss 1778.255818291023
INFO:root:current train perplexity4.062863826751709
INFO:root:current mean train loss 1778.5054067081655
INFO:root:current train perplexity4.063112735748291
INFO:root:current mean train loss 1778.5580249324064
INFO:root:current train perplexity4.0650506019592285
INFO:root:current mean train loss 1778.352257773271
INFO:root:current train perplexity4.067312240600586
INFO:root:current mean train loss 1779.316806567789
INFO:root:current train perplexity4.071075439453125
INFO:root:current mean train loss 1779.0894943626433
INFO:root:current train perplexity4.07033634185791
INFO:root:current mean train loss 1779.5044844976358
INFO:root:current train perplexity4.071766376495361


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.98s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.59s/it]
INFO:root:eval mean loss: 2864.806142812735
INFO:root:eval perplexity: 10.623880386352539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/33

 16%|â–ˆâ–‹        | 33/200 [4:34:14<22:56:34, 494.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1752.0574829101563
INFO:root:current train perplexity4.031938552856445
INFO:root:current mean train loss 1752.8381690979004
INFO:root:current train perplexity4.013924598693848
INFO:root:current mean train loss 1759.5078641451323
INFO:root:current train perplexity4.007534027099609
INFO:root:current mean train loss 1765.088028971354
INFO:root:current train perplexity4.018688678741455
INFO:root:current mean train loss 1763.081544030231
INFO:root:current train perplexity4.013022422790527
INFO:root:current mean train loss 1764.5060222080776
INFO:root:current train perplexity4.014458656311035
INFO:root:current mean train loss 1766.4119593764797
INFO:root:current train perplexity4.019527435302734
INFO:root:current mean train loss 1765.7583557128905
INFO:root:current train perplexity4.019838333129883
INFO:root:current mean train loss 1764.8341890556867
INFO:root:current train perplexity4.0202860832214355
INFO:root:current mean train loss 1764.3832892100015
INFO:root:current train perplexity4.020322322845459
INFO:root:current mean train loss 1762.7631611374188
INFO:root:current train perplexity4.021128177642822
INFO:root:current mean train loss 1765.4484537058863
INFO:root:current train perplexity4.023687839508057
INFO:root:current mean train loss 1765.184813968719
INFO:root:current train perplexity4.02322244644165
INFO:root:current mean train loss 1765.309209127987
INFO:root:current train perplexity4.02659797668457
INFO:root:current mean train loss 1765.5658120351295
INFO:root:current train perplexity4.027653694152832
INFO:root:current mean train loss 1767.1336363963592
INFO:root:current train perplexity4.029221057891846
INFO:root:current mean train loss 1767.116573251012
INFO:root:current train perplexity4.030764579772949
INFO:root:current mean train loss 1767.321143618497
INFO:root:current train perplexity4.032715320587158
INFO:root:current mean train loss 1768.0238289781796
INFO:root:current train perplexity4.034027099609375
INFO:root:current mean train loss 1768.265177948621
INFO:root:current train perplexity4.0354413986206055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.40s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 2871.9592123862144
INFO:root:eval perplexity: 10.686750411987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/34

 17%|â–ˆâ–‹        | 34/200 [4:42:30<22:49:45, 495.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1744.9649341137379
INFO:root:current train perplexity3.951575994491577
INFO:root:current mean train loss 1744.994368903381
INFO:root:current train perplexity3.9683830738067627
INFO:root:current mean train loss 1744.878941945651
INFO:root:current train perplexity3.9660019874572754
INFO:root:current mean train loss 1744.663549610411
INFO:root:current train perplexity3.961970090866089
INFO:root:current mean train loss 1743.496826939613
INFO:root:current train perplexity3.9656755924224854
INFO:root:current mean train loss 1745.4705362038967
INFO:root:current train perplexity3.964674949645996
INFO:root:current mean train loss 1744.8439705199294
INFO:root:current train perplexity3.963949203491211
INFO:root:current mean train loss 1747.8806848317347
INFO:root:current train perplexity3.9683985710144043
INFO:root:current mean train loss 1747.1634360023072
INFO:root:current train perplexity3.968609571456909
INFO:root:current mean train loss 1748.03208687508
INFO:root:current train perplexity3.972731590270996
INFO:root:current mean train loss 1750.0048633175197
INFO:root:current train perplexity3.979553699493408
INFO:root:current mean train loss 1752.4123001033813
INFO:root:current train perplexity3.9849181175231934
INFO:root:current mean train loss 1753.83118956707
INFO:root:current train perplexity3.985748052597046
INFO:root:current mean train loss 1754.23800980534
INFO:root:current train perplexity3.989037275314331
INFO:root:current mean train loss 1754.8150560382903
INFO:root:current train perplexity3.991652250289917
INFO:root:current mean train loss 1755.6063064449409
INFO:root:current train perplexity3.9950082302093506
INFO:root:current mean train loss 1756.3376713788573
INFO:root:current train perplexity3.9958407878875732
INFO:root:current mean train loss 1756.424128938674
INFO:root:current train perplexity3.997649908065796
INFO:root:current mean train loss 1757.254655841061
INFO:root:current train perplexity3.9997339248657227
INFO:root:current mean train loss 1757.1878633706572
INFO:root:current train perplexity3.99990177154541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.42s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.48s/it]
INFO:root:eval mean loss: 2879.066602002393
INFO:root:eval perplexity: 10.749585151672363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/35

 18%|â–ˆâ–Š        | 35/200 [4:50:43<22:39:42, 494.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1750.889277032081
INFO:root:current train perplexity3.95499324798584
INFO:root:current mean train loss 1734.9408934288417
INFO:root:current train perplexity3.9505155086517334
INFO:root:current mean train loss 1741.4536876029708
INFO:root:current train perplexity3.9544260501861572
INFO:root:current mean train loss 1739.5626546017409
INFO:root:current train perplexity3.9580013751983643
INFO:root:current mean train loss 1740.1331263244876
INFO:root:current train perplexity3.9586586952209473
INFO:root:current mean train loss 1740.5621755066945
INFO:root:current train perplexity3.954367160797119
INFO:root:current mean train loss 1740.5575503478476
INFO:root:current train perplexity3.9517922401428223
INFO:root:current mean train loss 1739.2628724220717
INFO:root:current train perplexity3.9456193447113037
INFO:root:current mean train loss 1741.224743870814
INFO:root:current train perplexity3.948957920074463
INFO:root:current mean train loss 1741.3732698927943
INFO:root:current train perplexity3.9495935440063477
INFO:root:current mean train loss 1741.7355535252657
INFO:root:current train perplexity3.9500539302825928
INFO:root:current mean train loss 1741.4704073549715
INFO:root:current train perplexity3.9506473541259766
INFO:root:current mean train loss 1742.4913569690639
INFO:root:current train perplexity3.9527034759521484
INFO:root:current mean train loss 1742.6222072694359
INFO:root:current train perplexity3.951856851577759
INFO:root:current mean train loss 1743.5183826941882
INFO:root:current train perplexity3.954491138458252
INFO:root:current mean train loss 1744.3201055012205
INFO:root:current train perplexity3.957850933074951
INFO:root:current mean train loss 1745.051626230215
INFO:root:current train perplexity3.960747003555298
INFO:root:current mean train loss 1745.4926991882664
INFO:root:current train perplexity3.9620423316955566
INFO:root:current mean train loss 1745.7225834202993
INFO:root:current train perplexity3.96386981010437


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.76s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.14s/it]
INFO:root:eval mean loss: 2880.5149189717063
INFO:root:eval perplexity: 10.762435913085938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/36

 18%|â–ˆâ–Š        | 36/200 [4:59:02<22:35:08, 495.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1730.6427223899148
INFO:root:current train perplexity3.907531976699829
INFO:root:current mean train loss 1711.4561327685108
INFO:root:current train perplexity3.902770519256592
INFO:root:current mean train loss 1722.4915343370483
INFO:root:current train perplexity3.8994381427764893
INFO:root:current mean train loss 1718.839917934209
INFO:root:current train perplexity3.902681350708008
INFO:root:current mean train loss 1718.6394654805354
INFO:root:current train perplexity3.8947348594665527
INFO:root:current mean train loss 1720.3618918939578
INFO:root:current train perplexity3.8979949951171875
INFO:root:current mean train loss 1721.391677681631
INFO:root:current train perplexity3.8986446857452393
INFO:root:current mean train loss 1722.7495802223432
INFO:root:current train perplexity3.899988889694214
INFO:root:current mean train loss 1723.4725876136713
INFO:root:current train perplexity3.9015398025512695
INFO:root:current mean train loss 1724.1954660593613
INFO:root:current train perplexity3.906228542327881
INFO:root:current mean train loss 1727.2723816099083
INFO:root:current train perplexity3.9139833450317383
INFO:root:current mean train loss 1728.2446966986738
INFO:root:current train perplexity3.91469669342041
INFO:root:current mean train loss 1728.4571477762438
INFO:root:current train perplexity3.9155232906341553
INFO:root:current mean train loss 1729.6397443432375
INFO:root:current train perplexity3.916663885116577
INFO:root:current mean train loss 1730.2371595181276
INFO:root:current train perplexity3.9197816848754883
INFO:root:current mean train loss 1730.3361872957685
INFO:root:current train perplexity3.9200665950775146
INFO:root:current mean train loss 1730.9367015798368
INFO:root:current train perplexity3.9227209091186523
INFO:root:current mean train loss 1731.3085234757407
INFO:root:current train perplexity3.9247069358825684
INFO:root:current mean train loss 1732.4055066585277
INFO:root:current train perplexity3.925724744796753
INFO:root:current mean train loss 1733.668268847503
INFO:root:current train perplexity3.9285428524017334


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.15s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it]
INFO:root:eval mean loss: 2890.848994991085
INFO:root:eval perplexity: 10.854571342468262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/37

 18%|â–ˆâ–Š        | 37/200 [5:07:16<22:25:23, 495.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1702.4640459333148
INFO:root:current train perplexity3.9205269813537598
INFO:root:current mean train loss 1708.524172782898
INFO:root:current train perplexity3.865818977355957
INFO:root:current mean train loss 1718.2492001182154
INFO:root:current train perplexity3.879932165145874
INFO:root:current mean train loss 1714.255575784823
INFO:root:current train perplexity3.871121406555176
INFO:root:current mean train loss 1716.2551449214186
INFO:root:current train perplexity3.875032901763916
INFO:root:current mean train loss 1714.8015807180693
INFO:root:current train perplexity3.880366563796997
INFO:root:current mean train loss 1714.8112071820885
INFO:root:current train perplexity3.877257823944092
INFO:root:current mean train loss 1716.5240912804236
INFO:root:current train perplexity3.8815855979919434
INFO:root:current mean train loss 1718.0281190733979
INFO:root:current train perplexity3.8847501277923584
INFO:root:current mean train loss 1718.3845184589254
INFO:root:current train perplexity3.883683204650879
INFO:root:current mean train loss 1719.0661058240364
INFO:root:current train perplexity3.8857269287109375
INFO:root:current mean train loss 1720.153886619189
INFO:root:current train perplexity3.8856170177459717
INFO:root:current mean train loss 1721.0995798514798
INFO:root:current train perplexity3.888056993484497
INFO:root:current mean train loss 1721.6082999907344
INFO:root:current train perplexity3.8893489837646484
INFO:root:current mean train loss 1722.1192240568103
INFO:root:current train perplexity3.8902621269226074
INFO:root:current mean train loss 1723.3860460051692
INFO:root:current train perplexity3.8928990364074707
INFO:root:current mean train loss 1723.7099293701772
INFO:root:current train perplexity3.894366502761841
INFO:root:current mean train loss 1725.2732057218197
INFO:root:current train perplexity3.896291732788086
INFO:root:current mean train loss 1725.014423195069
INFO:root:current train perplexity3.8976938724517822
INFO:root:current mean train loss 1724.43258932913
INFO:root:current train perplexity3.8975610733032227


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.05s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 2901.9702925581832
INFO:root:eval perplexity: 10.954605102539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/38

 19%|â–ˆâ–‰        | 38/200 [5:15:31<22:16:53, 495.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1749.204484049479
INFO:root:current train perplexity3.8161661624908447
INFO:root:current mean train loss 1715.6114939722522
INFO:root:current train perplexity3.803316116333008
INFO:root:current mean train loss 1710.6317367865115
INFO:root:current train perplexity3.8159985542297363
INFO:root:current mean train loss 1709.862982266191
INFO:root:current train perplexity3.8284993171691895
INFO:root:current mean train loss 1710.7595052997717
INFO:root:current train perplexity3.8363852500915527
INFO:root:current mean train loss 1711.4951026286553
INFO:root:current train perplexity3.843125820159912
INFO:root:current mean train loss 1708.0831667877908
INFO:root:current train perplexity3.8393595218658447
INFO:root:current mean train loss 1709.0347254810717
INFO:root:current train perplexity3.845703363418579
INFO:root:current mean train loss 1707.3526546320265
INFO:root:current train perplexity3.8504161834716797
INFO:root:current mean train loss 1707.528355706432
INFO:root:current train perplexity3.851624011993408
INFO:root:current mean train loss 1708.546896960975
INFO:root:current train perplexity3.851055145263672
INFO:root:current mean train loss 1708.639440544828
INFO:root:current train perplexity3.851630926132202
INFO:root:current mean train loss 1709.7313722664094
INFO:root:current train perplexity3.8517723083496094
INFO:root:current mean train loss 1709.6278902075105
INFO:root:current train perplexity3.853318691253662
INFO:root:current mean train loss 1710.513036518031
INFO:root:current train perplexity3.8550145626068115
INFO:root:current mean train loss 1712.2200930104673
INFO:root:current train perplexity3.8575875759124756
INFO:root:current mean train loss 1712.251148870868
INFO:root:current train perplexity3.85967755317688
INFO:root:current mean train loss 1713.3634789409473
INFO:root:current train perplexity3.8608498573303223
INFO:root:current mean train loss 1713.797671864414
INFO:root:current train perplexity3.8627712726593018
INFO:root:current mean train loss 1713.600627485339
INFO:root:current train perplexity3.8650951385498047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.15s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.81s/it]
INFO:root:eval mean loss: 2908.141036299972
INFO:root:eval perplexity: 11.010506629943848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/39

 20%|â–ˆâ–‰        | 39/200 [5:23:43<22:06:05, 494.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.9592639553932
INFO:root:current train perplexity3.804438591003418
INFO:root:current mean train loss 1703.8838885271991
INFO:root:current train perplexity3.8042080402374268
INFO:root:current mean train loss 1702.9394862051229
INFO:root:current train perplexity3.8063976764678955
INFO:root:current mean train loss 1702.7208164278316
INFO:root:current train perplexity3.813643455505371
INFO:root:current mean train loss 1699.7782534644716
INFO:root:current train perplexity3.8094069957733154
INFO:root:current mean train loss 1698.4722674495385
INFO:root:current train perplexity3.8030519485473633
INFO:root:current mean train loss 1697.8730070454117
INFO:root:current train perplexity3.804875612258911
INFO:root:current mean train loss 1697.5231113383777
INFO:root:current train perplexity3.8053057193756104
INFO:root:current mean train loss 1699.005207955699
INFO:root:current train perplexity3.8100428581237793
INFO:root:current mean train loss 1699.8464217156234
INFO:root:current train perplexity3.814401865005493
INFO:root:current mean train loss 1700.4104984376838
INFO:root:current train perplexity3.8128883838653564
INFO:root:current mean train loss 1700.0161722153682
INFO:root:current train perplexity3.812323808670044
INFO:root:current mean train loss 1700.21465406705
INFO:root:current train perplexity3.81520676612854
INFO:root:current mean train loss 1700.6366522476712
INFO:root:current train perplexity3.8206920623779297
INFO:root:current mean train loss 1701.2083063921425
INFO:root:current train perplexity3.822995185852051
INFO:root:current mean train loss 1701.2696436297115
INFO:root:current train perplexity3.825578212738037
INFO:root:current mean train loss 1701.3803206350829
INFO:root:current train perplexity3.82857608795166
INFO:root:current mean train loss 1702.06811932186
INFO:root:current train perplexity3.8304097652435303
INFO:root:current mean train loss 1703.1784810886732
INFO:root:current train perplexity3.832261323928833
INFO:root:current mean train loss 1702.9286201624816
INFO:root:current train perplexity3.832580327987671


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.08s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.46s/it]
INFO:root:eval mean loss: 2912.4759393182244
INFO:root:eval perplexity: 11.049948692321777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/40

 20%|â–ˆâ–ˆ        | 40/200 [5:31:58<21:58:06, 494.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1691.9430333391022
INFO:root:current train perplexity3.768826723098755
INFO:root:current mean train loss 1683.5810771920828
INFO:root:current train perplexity3.780205249786377
INFO:root:current mean train loss 1683.1467193275369
INFO:root:current train perplexity3.7728500366210938
INFO:root:current mean train loss 1683.1998361874382
INFO:root:current train perplexity3.779541492462158
INFO:root:current mean train loss 1685.2521231059989
INFO:root:current train perplexity3.7750661373138428
INFO:root:current mean train loss 1682.3338190846287
INFO:root:current train perplexity3.778055429458618
INFO:root:current mean train loss 1682.2850539554377
INFO:root:current train perplexity3.7776150703430176
INFO:root:current mean train loss 1681.9578586328626
INFO:root:current train perplexity3.7768924236297607
INFO:root:current mean train loss 1684.9768846878555
INFO:root:current train perplexity3.780411720275879
INFO:root:current mean train loss 1684.428846100134
INFO:root:current train perplexity3.7837042808532715
INFO:root:current mean train loss 1686.6131746788838
INFO:root:current train perplexity3.787656545639038
INFO:root:current mean train loss 1686.9897126512471
INFO:root:current train perplexity3.791234016418457
INFO:root:current mean train loss 1688.7581768020975
INFO:root:current train perplexity3.7936034202575684
INFO:root:current mean train loss 1689.0404886601195
INFO:root:current train perplexity3.792750597000122
INFO:root:current mean train loss 1689.8403089212518
INFO:root:current train perplexity3.794511079788208
INFO:root:current mean train loss 1690.406215520355
INFO:root:current train perplexity3.796210527420044
INFO:root:current mean train loss 1690.3828369286034
INFO:root:current train perplexity3.797111988067627
INFO:root:current mean train loss 1691.5256485577177
INFO:root:current train perplexity3.7989938259124756
INFO:root:current mean train loss 1692.8581136933915
INFO:root:current train perplexity3.8019540309906006
INFO:root:current mean train loss 1693.4564743774476
INFO:root:current train perplexity3.8040807247161865


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.13s/it]
INFO:root:eval mean loss: 2917.245801221143
INFO:root:eval perplexity: 11.09350872039795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/41

 20%|â–ˆâ–ˆ        | 41/200 [5:40:14<21:51:32, 494.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1664.0928853352864
INFO:root:current train perplexity3.7124290466308594
INFO:root:current mean train loss 1667.590406768176
INFO:root:current train perplexity3.7176029682159424
INFO:root:current mean train loss 1666.852477614944
INFO:root:current train perplexity3.7244017124176025
INFO:root:current mean train loss 1672.2033719149504
INFO:root:current train perplexity3.7366695404052734
INFO:root:current mean train loss 1676.904174066359
INFO:root:current train perplexity3.7397360801696777
INFO:root:current mean train loss 1677.4198751769609
INFO:root:current train perplexity3.7425899505615234
INFO:root:current mean train loss 1679.3245225226742
INFO:root:current train perplexity3.7475736141204834
INFO:root:current mean train loss 1679.428004892627
INFO:root:current train perplexity3.748004674911499
INFO:root:current mean train loss 1681.8747487749372
INFO:root:current train perplexity3.751155138015747
INFO:root:current mean train loss 1681.3516264766095
INFO:root:current train perplexity3.7524466514587402
INFO:root:current mean train loss 1681.5638487878507
INFO:root:current train perplexity3.758558988571167
INFO:root:current mean train loss 1681.41771501522
INFO:root:current train perplexity3.7606403827667236
INFO:root:current mean train loss 1681.8787840854975
INFO:root:current train perplexity3.762357711791992
INFO:root:current mean train loss 1682.1463488384782
INFO:root:current train perplexity3.764031410217285
INFO:root:current mean train loss 1681.1331683480166
INFO:root:current train perplexity3.7636475563049316
INFO:root:current mean train loss 1681.5586707705543
INFO:root:current train perplexity3.7666313648223877
INFO:root:current mean train loss 1681.1498972334953
INFO:root:current train perplexity3.7679295539855957
INFO:root:current mean train loss 1682.4291739346986
INFO:root:current train perplexity3.770082712173462
INFO:root:current mean train loss 1682.3053748537216
INFO:root:current train perplexity3.7712302207946777


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.35s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it]
INFO:root:eval mean loss: 2928.8250648109047
INFO:root:eval perplexity: 11.199976921081543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/42

 21%|â–ˆâ–ˆ        | 42/200 [5:48:32<21:45:27, 495.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1664.0933556189905
INFO:root:current train perplexity3.7070281505584717
INFO:root:current mean train loss 1651.7626888308905
INFO:root:current train perplexity3.7089805603027344
INFO:root:current mean train loss 1659.6129333782644
INFO:root:current train perplexity3.7089734077453613
INFO:root:current mean train loss 1666.6372475913538
INFO:root:current train perplexity3.719028949737549
INFO:root:current mean train loss 1665.1615675956227
INFO:root:current train perplexity3.7181880474090576
INFO:root:current mean train loss 1665.1590754637243
INFO:root:current train perplexity3.718355655670166
INFO:root:current mean train loss 1664.1790468797792
INFO:root:current train perplexity3.7194390296936035
INFO:root:current mean train loss 1666.1984936900092
INFO:root:current train perplexity3.7240607738494873
INFO:root:current mean train loss 1668.2535506995696
INFO:root:current train perplexity3.725308418273926
INFO:root:current mean train loss 1668.3682833250616
INFO:root:current train perplexity3.725484848022461
INFO:root:current mean train loss 1669.710112290227
INFO:root:current train perplexity3.7320046424865723
INFO:root:current mean train loss 1671.3400441295696
INFO:root:current train perplexity3.7332444190979004
INFO:root:current mean train loss 1671.2239418627305
INFO:root:current train perplexity3.7322967052459717
INFO:root:current mean train loss 1671.230166037938
INFO:root:current train perplexity3.732701539993286
INFO:root:current mean train loss 1671.2695443814137
INFO:root:current train perplexity3.7355103492736816
INFO:root:current mean train loss 1671.4857813500444
INFO:root:current train perplexity3.737119197845459
INFO:root:current mean train loss 1672.5418503649546
INFO:root:current train perplexity3.737973213195801
INFO:root:current mean train loss 1673.3711590964544
INFO:root:current train perplexity3.7398862838745117
INFO:root:current mean train loss 1673.4816776702762
INFO:root:current train perplexity3.740502119064331
INFO:root:current mean train loss 1673.1964852811152
INFO:root:current train perplexity3.7426950931549072


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 42.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 42.00s/it]
INFO:root:eval mean loss: 2931.4200736380913
INFO:root:eval perplexity: 11.223976135253906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/43

 22%|â–ˆâ–ˆâ–       | 43/200 [5:56:50<21:39:27, 496.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1646.6275390625
INFO:root:current train perplexity3.7188494205474854
INFO:root:current mean train loss 1654.204696890024
INFO:root:current train perplexity3.6887309551239014
INFO:root:current mean train loss 1656.2400263247282
INFO:root:current train perplexity3.697007179260254
INFO:root:current mean train loss 1654.7147583007813
INFO:root:current train perplexity3.6993935108184814
INFO:root:current mean train loss 1656.8377336369003
INFO:root:current train perplexity3.701864004135132
INFO:root:current mean train loss 1655.1673689932193
INFO:root:current train perplexity3.6986091136932373
INFO:root:current mean train loss 1656.3231960720486
INFO:root:current train perplexity3.70330810546875
INFO:root:current mean train loss 1655.1050303001925
INFO:root:current train perplexity3.6989715099334717
INFO:root:current mean train loss 1657.2662235857492
INFO:root:current train perplexity3.69850754737854
INFO:root:current mean train loss 1657.0435390677503
INFO:root:current train perplexity3.697012424468994
INFO:root:current mean train loss 1658.8143433802336
INFO:root:current train perplexity3.701808452606201
INFO:root:current mean train loss 1658.3404503206236
INFO:root:current train perplexity3.699404001235962
INFO:root:current mean train loss 1660.4056207920478
INFO:root:current train perplexity3.7051548957824707
INFO:root:current mean train loss 1661.7406454674283
INFO:root:current train perplexity3.707200050354004
INFO:root:current mean train loss 1661.8735113397345
INFO:root:current train perplexity3.707584857940674
INFO:root:current mean train loss 1662.220357658037
INFO:root:current train perplexity3.7101378440856934
INFO:root:current mean train loss 1661.8051804244153
INFO:root:current train perplexity3.7120962142944336
INFO:root:current mean train loss 1662.8280411736814
INFO:root:current train perplexity3.712500810623169
INFO:root:current mean train loss 1663.2981561379354
INFO:root:current train perplexity3.7140398025512695
INFO:root:current mean train loss 1663.0378895497693
INFO:root:current train perplexity3.715954303741455


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.26s/it]
INFO:root:eval mean loss: 2934.749244117164
INFO:root:eval perplexity: 11.254840850830078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:05:09<21:32:38, 497.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.4236904712434
INFO:root:current train perplexity3.636335849761963
INFO:root:current mean train loss 1632.6834492586097
INFO:root:current train perplexity3.6421873569488525
INFO:root:current mean train loss 1638.9245333652266
INFO:root:current train perplexity3.6625516414642334
INFO:root:current mean train loss 1642.2465419274586
INFO:root:current train perplexity3.6623215675354004
INFO:root:current mean train loss 1644.2360495752937
INFO:root:current train perplexity3.6678144931793213
INFO:root:current mean train loss 1643.524156978405
INFO:root:current train perplexity3.6694512367248535
INFO:root:current mean train loss 1642.6055923296826
INFO:root:current train perplexity3.6710140705108643
INFO:root:current mean train loss 1644.2680500648428
INFO:root:current train perplexity3.672727346420288
INFO:root:current mean train loss 1644.212333742252
INFO:root:current train perplexity3.6686933040618896
INFO:root:current mean train loss 1645.0018474252574
INFO:root:current train perplexity3.66809344291687
INFO:root:current mean train loss 1647.2506330867209
INFO:root:current train perplexity3.6697041988372803
INFO:root:current mean train loss 1647.7430859460142
INFO:root:current train perplexity3.6710119247436523
INFO:root:current mean train loss 1648.511218526025
INFO:root:current train perplexity3.673213481903076
INFO:root:current mean train loss 1650.0936490450422
INFO:root:current train perplexity3.6740472316741943
INFO:root:current mean train loss 1651.4232330427717
INFO:root:current train perplexity3.6769518852233887
INFO:root:current mean train loss 1651.5560430564954
INFO:root:current train perplexity3.6776225566864014
INFO:root:current mean train loss 1652.6123123956436
INFO:root:current train perplexity3.680518865585327
INFO:root:current mean train loss 1652.8071690140687
INFO:root:current train perplexity3.68220853805542
INFO:root:current mean train loss 1652.5109351075012
INFO:root:current train perplexity3.6824846267700195
INFO:root:current mean train loss 1653.719681420936
INFO:root:current train perplexity3.6865618228912354


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.62s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.51s/it]
INFO:root:eval mean loss: 2948.550706468187
INFO:root:eval perplexity: 11.383706092834473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:13:23<21:22:00, 496.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.8240299224854
INFO:root:current train perplexity3.626742362976074
INFO:root:current mean train loss 1633.8996902093654
INFO:root:current train perplexity3.6299376487731934
INFO:root:current mean train loss 1630.1667466597123
INFO:root:current train perplexity3.629286050796509
INFO:root:current mean train loss 1638.3894368266012
INFO:root:current train perplexity3.6340463161468506
INFO:root:current mean train loss 1636.5322789159313
INFO:root:current train perplexity3.635648727416992
INFO:root:current mean train loss 1635.3448365123559
INFO:root:current train perplexity3.6360344886779785
INFO:root:current mean train loss 1637.3744225559465
INFO:root:current train perplexity3.6376914978027344
INFO:root:current mean train loss 1636.3043425395226
INFO:root:current train perplexity3.6369709968566895
INFO:root:current mean train loss 1636.934055045799
INFO:root:current train perplexity3.6424152851104736
INFO:root:current mean train loss 1636.9334695269952
INFO:root:current train perplexity3.643028497695923
INFO:root:current mean train loss 1637.7628692397498
INFO:root:current train perplexity3.6426806449890137
INFO:root:current mean train loss 1639.1964934568634
INFO:root:current train perplexity3.6469743251800537
INFO:root:current mean train loss 1639.6199647927585
INFO:root:current train perplexity3.647515296936035
INFO:root:current mean train loss 1640.653837858407
INFO:root:current train perplexity3.6499452590942383
INFO:root:current mean train loss 1641.8610254506596
INFO:root:current train perplexity3.6538493633270264
INFO:root:current mean train loss 1641.8297354071342
INFO:root:current train perplexity3.653630256652832
INFO:root:current mean train loss 1643.0151499234712
INFO:root:current train perplexity3.656048536300659
INFO:root:current mean train loss 1643.4927622823218
INFO:root:current train perplexity3.6567981243133545
INFO:root:current mean train loss 1643.49331226267
INFO:root:current train perplexity3.658611297607422
INFO:root:current mean train loss 1644.4438534987191
INFO:root:current train perplexity3.6602513790130615


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.70s/it]
INFO:root:eval mean loss: 2953.382300024634
INFO:root:eval perplexity: 11.42916488647461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:21:43<21:16:45, 497.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.1901599271798
INFO:root:current train perplexity3.6224703788757324
INFO:root:current mean train loss 1618.0761927820702
INFO:root:current train perplexity3.6163973808288574
INFO:root:current mean train loss 1622.3742154484542
INFO:root:current train perplexity3.61051082611084
INFO:root:current mean train loss 1621.9965262826033
INFO:root:current train perplexity3.6011898517608643
INFO:root:current mean train loss 1621.70998783865
INFO:root:current train perplexity3.6039199829101562
INFO:root:current mean train loss 1620.1796234183385
INFO:root:current train perplexity3.6003048419952393
INFO:root:current mean train loss 1621.7607624429263
INFO:root:current train perplexity3.605226516723633
INFO:root:current mean train loss 1624.1707466701844
INFO:root:current train perplexity3.6098246574401855
INFO:root:current mean train loss 1627.962962398464
INFO:root:current train perplexity3.614536762237549
INFO:root:current mean train loss 1629.0301419101602
INFO:root:current train perplexity3.614858388900757
INFO:root:current mean train loss 1628.2851958861513
INFO:root:current train perplexity3.617234230041504
INFO:root:current mean train loss 1629.1246576656436
INFO:root:current train perplexity3.6190781593322754
INFO:root:current mean train loss 1629.0626534217042
INFO:root:current train perplexity3.619795799255371
INFO:root:current mean train loss 1630.835928483945
INFO:root:current train perplexity3.622980833053589
INFO:root:current mean train loss 1630.943778419881
INFO:root:current train perplexity3.6237242221832275
INFO:root:current mean train loss 1631.860836292052
INFO:root:current train perplexity3.625532627105713
INFO:root:current mean train loss 1632.7243366956286
INFO:root:current train perplexity3.6280837059020996
INFO:root:current mean train loss 1632.760395716175
INFO:root:current train perplexity3.6274330615997314
INFO:root:current mean train loss 1633.4797334077825
INFO:root:current train perplexity3.629772663116455
INFO:root:current mean train loss 1634.4889206864627
INFO:root:current train perplexity3.6311957836151123


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.91s/it]
INFO:root:eval mean loss: 2965.3390233495215
INFO:root:eval perplexity: 11.542445182800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [6:30:03<21:09:58, 498.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.1814176598373
INFO:root:current train perplexity3.566265821456909
INFO:root:current mean train loss 1606.255325471512
INFO:root:current train perplexity3.568331718444824
INFO:root:current mean train loss 1613.0210960439388
INFO:root:current train perplexity3.564074993133545
INFO:root:current mean train loss 1615.4446171899538
INFO:root:current train perplexity3.5659937858581543
INFO:root:current mean train loss 1617.9827380812312
INFO:root:current train perplexity3.571040630340576
INFO:root:current mean train loss 1616.6607702759197
INFO:root:current train perplexity3.5743789672851562
INFO:root:current mean train loss 1617.1828162075797
INFO:root:current train perplexity3.578517436981201
INFO:root:current mean train loss 1618.4909651142016
INFO:root:current train perplexity3.5828328132629395
INFO:root:current mean train loss 1620.431031632795
INFO:root:current train perplexity3.587350606918335
INFO:root:current mean train loss 1621.2200999900192
INFO:root:current train perplexity3.5897369384765625
INFO:root:current mean train loss 1622.4679328626623
INFO:root:current train perplexity3.5928351879119873
INFO:root:current mean train loss 1622.3280171949996
INFO:root:current train perplexity3.5934035778045654
INFO:root:current mean train loss 1621.9308925916675
INFO:root:current train perplexity3.594959259033203
INFO:root:current mean train loss 1622.8845983240567
INFO:root:current train perplexity3.595282793045044
INFO:root:current mean train loss 1623.2106896108874
INFO:root:current train perplexity3.597630500793457
INFO:root:current mean train loss 1623.395149469674
INFO:root:current train perplexity3.5990827083587646
INFO:root:current mean train loss 1624.17516557853
INFO:root:current train perplexity3.602782964706421
INFO:root:current mean train loss 1625.4377410854727
INFO:root:current train perplexity3.6043105125427246
INFO:root:current mean train loss 1625.5498268119402
INFO:root:current train perplexity3.6050894260406494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it]
INFO:root:eval mean loss: 2972.9599462743995
INFO:root:eval perplexity: 11.615233421325684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/48

 24%|â–ˆâ–ˆâ–       | 48/200 [6:38:23<21:03:38, 498.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1591.576798502604
INFO:root:current train perplexity3.484398603439331
INFO:root:current mean train loss 1614.88744586447
INFO:root:current train perplexity3.5238406658172607
INFO:root:current mean train loss 1607.996311205487
INFO:root:current train perplexity3.526414155960083
INFO:root:current mean train loss 1605.8849547371033
INFO:root:current train perplexity3.5327794551849365
INFO:root:current mean train loss 1607.442265742658
INFO:root:current train perplexity3.5400595664978027
INFO:root:current mean train loss 1608.454373198574
INFO:root:current train perplexity3.5474965572357178
INFO:root:current mean train loss 1606.12794616203
INFO:root:current train perplexity3.5475244522094727
INFO:root:current mean train loss 1606.6157187295128
INFO:root:current train perplexity3.5479736328125
INFO:root:current mean train loss 1607.3900672210507
INFO:root:current train perplexity3.550823211669922
INFO:root:current mean train loss 1609.3349640059341
INFO:root:current train perplexity3.553894281387329
INFO:root:current mean train loss 1610.8434798818503
INFO:root:current train perplexity3.5540108680725098
INFO:root:current mean train loss 1611.1349823955998
INFO:root:current train perplexity3.5579540729522705
INFO:root:current mean train loss 1612.4153625739455
INFO:root:current train perplexity3.560636520385742
INFO:root:current mean train loss 1614.0006409851771
INFO:root:current train perplexity3.561812400817871
INFO:root:current mean train loss 1614.0928877436231
INFO:root:current train perplexity3.565802574157715
INFO:root:current mean train loss 1614.0444340771967
INFO:root:current train perplexity3.569121837615967
INFO:root:current mean train loss 1613.7579030512288
INFO:root:current train perplexity3.5697617530822754
INFO:root:current mean train loss 1615.3121493058718
INFO:root:current train perplexity3.573260545730591
INFO:root:current mean train loss 1615.5552501667958
INFO:root:current train perplexity3.5761516094207764
INFO:root:current mean train loss 1615.6746719718913
INFO:root:current train perplexity3.577840566635132


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 42.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 42.00s/it]
INFO:root:eval mean loss: 2979.6454726210586
INFO:root:eval perplexity: 11.67946720123291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/49

 24%|â–ˆâ–ˆâ–       | 49/200 [6:46:35<20:50:23, 496.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1609.5036811828613
INFO:root:current train perplexity3.523172616958618
INFO:root:current mean train loss 1596.6581892533736
INFO:root:current train perplexity3.518057346343994
INFO:root:current mean train loss 1597.6141331113618
INFO:root:current train perplexity3.5166592597961426
INFO:root:current mean train loss 1597.99267798734
INFO:root:current train perplexity3.519541025161743
INFO:root:current mean train loss 1595.2774638423214
INFO:root:current train perplexity3.515519380569458
INFO:root:current mean train loss 1598.748684531764
INFO:root:current train perplexity3.5197033882141113
INFO:root:current mean train loss 1601.2325760080844
INFO:root:current train perplexity3.528876543045044
INFO:root:current mean train loss 1601.3069316259498
INFO:root:current train perplexity3.5298125743865967
INFO:root:current mean train loss 1602.4317238147441
INFO:root:current train perplexity3.5330398082733154
INFO:root:current mean train loss 1602.1694591342123
INFO:root:current train perplexity3.532975673675537
INFO:root:current mean train loss 1602.307949450589
INFO:root:current train perplexity3.5374748706817627
INFO:root:current mean train loss 1602.9974915197797
INFO:root:current train perplexity3.538846731185913
INFO:root:current mean train loss 1603.4047804993468
INFO:root:current train perplexity3.540259838104248
INFO:root:current mean train loss 1605.1768022600238
INFO:root:current train perplexity3.543095588684082
INFO:root:current mean train loss 1605.7377973162263
INFO:root:current train perplexity3.5474956035614014
INFO:root:current mean train loss 1606.3666929240014
INFO:root:current train perplexity3.5490176677703857
INFO:root:current mean train loss 1606.5603420781154
INFO:root:current train perplexity3.549182891845703
INFO:root:current mean train loss 1606.2849121798545
INFO:root:current train perplexity3.549738645553589
INFO:root:current mean train loss 1606.201397758384
INFO:root:current train perplexity3.551614999771118
INFO:root:current mean train loss 1607.3761819085225
INFO:root:current train perplexity3.553060293197632


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.72s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.66s/it]
INFO:root:eval mean loss: 2989.753630583709
INFO:root:eval perplexity: 11.777254104614258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [6:54:48<20:38:46, 495.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1577.2450499242666
INFO:root:current train perplexity3.4847803115844727
INFO:root:current mean train loss 1588.9096999200399
INFO:root:current train perplexity3.4793317317962646
INFO:root:current mean train loss 1584.4527690841492
INFO:root:current train perplexity3.4867398738861084
INFO:root:current mean train loss 1585.0948996994762
INFO:root:current train perplexity3.4931108951568604
INFO:root:current mean train loss 1586.7563800089608
INFO:root:current train perplexity3.496656894683838
INFO:root:current mean train loss 1589.8078786714482
INFO:root:current train perplexity3.5044736862182617
INFO:root:current mean train loss 1587.9718081528674
INFO:root:current train perplexity3.50579833984375
INFO:root:current mean train loss 1588.8610293868387
INFO:root:current train perplexity3.5056700706481934
INFO:root:current mean train loss 1590.3284266531398
INFO:root:current train perplexity3.5111308097839355
INFO:root:current mean train loss 1591.5697153973756
INFO:root:current train perplexity3.5128066539764404
INFO:root:current mean train loss 1591.920620782814
INFO:root:current train perplexity3.5159895420074463
INFO:root:current mean train loss 1592.6730534194137
INFO:root:current train perplexity3.5194826126098633
INFO:root:current mean train loss 1593.3947968922012
INFO:root:current train perplexity3.521291732788086
INFO:root:current mean train loss 1594.4551861694426
INFO:root:current train perplexity3.52386212348938
INFO:root:current mean train loss 1595.3902002391196
INFO:root:current train perplexity3.5256941318511963
INFO:root:current mean train loss 1595.5113615229332
INFO:root:current train perplexity3.5267443656921387
INFO:root:current mean train loss 1595.6267314145173
INFO:root:current train perplexity3.5269384384155273
INFO:root:current mean train loss 1595.6223448834603
INFO:root:current train perplexity3.527273654937744
INFO:root:current mean train loss 1596.9009104636375
INFO:root:current train perplexity3.5280489921569824
INFO:root:current mean train loss 1597.717490527494
INFO:root:current train perplexity3.5292932987213135


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.31s/it]
INFO:root:eval mean loss: 2994.301143428585
INFO:root:eval perplexity: 11.821514129638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:03:00<20:27:42, 494.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1581.9746019767992
INFO:root:current train perplexity3.4602460861206055
INFO:root:current mean train loss 1581.6114810805723
INFO:root:current train perplexity3.482225179672241
INFO:root:current mean train loss 1579.3317540677867
INFO:root:current train perplexity3.4852871894836426
INFO:root:current mean train loss 1577.506684850474
INFO:root:current train perplexity3.4842443466186523
INFO:root:current mean train loss 1577.0454612371748
INFO:root:current train perplexity3.4825994968414307
INFO:root:current mean train loss 1579.2985980030505
INFO:root:current train perplexity3.482048988342285
INFO:root:current mean train loss 1579.5069202503284
INFO:root:current train perplexity3.485614538192749
INFO:root:current mean train loss 1582.290763516339
INFO:root:current train perplexity3.4917690753936768
INFO:root:current mean train loss 1583.8064547003555
INFO:root:current train perplexity3.4949121475219727
INFO:root:current mean train loss 1584.1713792631099
INFO:root:current train perplexity3.4971771240234375
INFO:root:current mean train loss 1584.8172778045482
INFO:root:current train perplexity3.4942355155944824
INFO:root:current mean train loss 1585.6994850852273
INFO:root:current train perplexity3.4970037937164307
INFO:root:current mean train loss 1585.7728084425603
INFO:root:current train perplexity3.496877908706665
INFO:root:current mean train loss 1585.093655453594
INFO:root:current train perplexity3.497384786605835
INFO:root:current mean train loss 1585.9606260791481
INFO:root:current train perplexity3.4981493949890137
INFO:root:current mean train loss 1587.3520074408325
INFO:root:current train perplexity3.4993255138397217
INFO:root:current mean train loss 1587.2577360045582
INFO:root:current train perplexity3.5008113384246826
INFO:root:current mean train loss 1588.5526833626045
INFO:root:current train perplexity3.5024778842926025
INFO:root:current mean train loss 1589.1072332089864
INFO:root:current train perplexity3.504251003265381
INFO:root:current mean train loss 1589.9005824852572
INFO:root:current train perplexity3.5058107376098633


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.40s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.43s/it]
INFO:root:eval mean loss: 3004.5013006170234
INFO:root:eval perplexity: 11.92140007019043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:11:41<20:39:07, 502.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.8094188276543
INFO:root:current train perplexity3.4657137393951416
INFO:root:current mean train loss 1562.8982994204662
INFO:root:current train perplexity3.4613583087921143
INFO:root:current mean train loss 1560.5268800553501
INFO:root:current train perplexity3.461649179458618
INFO:root:current mean train loss 1561.9276123046875
INFO:root:current train perplexity3.4621760845184326
INFO:root:current mean train loss 1564.0014413395284
INFO:root:current train perplexity3.458139657974243
INFO:root:current mean train loss 1566.3714090808587
INFO:root:current train perplexity3.459157705307007
INFO:root:current mean train loss 1567.3010121648517
INFO:root:current train perplexity3.458284378051758
INFO:root:current mean train loss 1567.7822778538573
INFO:root:current train perplexity3.4619596004486084
INFO:root:current mean train loss 1569.2676034238302
INFO:root:current train perplexity3.4621071815490723
INFO:root:current mean train loss 1571.8487355105146
INFO:root:current train perplexity3.4646658897399902
INFO:root:current mean train loss 1573.431681540534
INFO:root:current train perplexity3.4651763439178467
INFO:root:current mean train loss 1573.8859859153768
INFO:root:current train perplexity3.4696130752563477
INFO:root:current mean train loss 1575.0827074415129
INFO:root:current train perplexity3.469273328781128
INFO:root:current mean train loss 1574.5028034687386
INFO:root:current train perplexity3.469256639480591
INFO:root:current mean train loss 1575.990051557627
INFO:root:current train perplexity3.4708471298217773
INFO:root:current mean train loss 1577.4583522003811
INFO:root:current train perplexity3.473489284515381
INFO:root:current mean train loss 1578.6439473723913
INFO:root:current train perplexity3.4747259616851807
INFO:root:current mean train loss 1578.6192919292012
INFO:root:current train perplexity3.4752519130706787
INFO:root:current mean train loss 1579.0638655927419
INFO:root:current train perplexity3.4775350093841553
INFO:root:current mean train loss 1580.190302015373
INFO:root:current train perplexity3.4802000522613525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.87s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 3007.510761982686
INFO:root:eval perplexity: 11.951030731201172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [7:20:00<20:28:17, 501.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1553.9926232910157
INFO:root:current train perplexity3.404644250869751
INFO:root:current mean train loss 1562.9903442382813
INFO:root:current train perplexity3.4205143451690674
INFO:root:current mean train loss 1567.554755859375
INFO:root:current train perplexity3.427136182785034
INFO:root:current mean train loss 1564.7587063598633
INFO:root:current train perplexity3.4247870445251465
INFO:root:current mean train loss 1567.3621069335939
INFO:root:current train perplexity3.4344799518585205
INFO:root:current mean train loss 1568.436127319336
INFO:root:current train perplexity3.432624578475952
INFO:root:current mean train loss 1568.6612020438058
INFO:root:current train perplexity3.43609619140625
INFO:root:current mean train loss 1569.7471884155273
INFO:root:current train perplexity3.436856269836426
INFO:root:current mean train loss 1569.039673936632
INFO:root:current train perplexity3.438596725463867
INFO:root:current mean train loss 1567.624732055664
INFO:root:current train perplexity3.439093589782715
INFO:root:current mean train loss 1566.37740234375
INFO:root:current train perplexity3.4399287700653076
INFO:root:current mean train loss 1566.308037109375
INFO:root:current train perplexity3.4420998096466064
INFO:root:current mean train loss 1568.0448358623798
INFO:root:current train perplexity3.4451143741607666
INFO:root:current mean train loss 1568.7337611607143
INFO:root:current train perplexity3.446798086166382
INFO:root:current mean train loss 1569.235717936198
INFO:root:current train perplexity3.447402238845825
INFO:root:current mean train loss 1570.2296244049073
INFO:root:current train perplexity3.449363946914673
INFO:root:current mean train loss 1570.365598862592
INFO:root:current train perplexity3.4524972438812256
INFO:root:current mean train loss 1570.7259462483723
INFO:root:current train perplexity3.4536471366882324
INFO:root:current mean train loss 1571.5050551886306
INFO:root:current train perplexity3.4554660320281982


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.28s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.74s/it]
INFO:root:eval mean loss: 3013.192521378801
INFO:root:eval perplexity: 12.00717544555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [7:28:14<20:14:35, 499.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1566.7562614889705
INFO:root:current train perplexity3.4353668689727783
INFO:root:current mean train loss 1555.8895806206597
INFO:root:current train perplexity3.4088141918182373
INFO:root:current mean train loss 1552.7105082850303
INFO:root:current train perplexity3.4154582023620605
INFO:root:current mean train loss 1552.8929501121352
INFO:root:current train perplexity3.4135043621063232
INFO:root:current mean train loss 1556.505422615033
INFO:root:current train perplexity3.4134719371795654
INFO:root:current mean train loss 1557.4633352253838
INFO:root:current train perplexity3.4142613410949707
INFO:root:current mean train loss 1556.725169473954
INFO:root:current train perplexity3.416710138320923
INFO:root:current mean train loss 1557.4677979537134
INFO:root:current train perplexity3.4161674976348877
INFO:root:current mean train loss 1559.058901839332
INFO:root:current train perplexity3.4168643951416016
INFO:root:current mean train loss 1559.8122756941368
INFO:root:current train perplexity3.4194843769073486
INFO:root:current mean train loss 1558.9048737382466
INFO:root:current train perplexity3.4191946983337402
INFO:root:current mean train loss 1560.0966451537322
INFO:root:current train perplexity3.4216742515563965
INFO:root:current mean train loss 1561.2304440751464
INFO:root:current train perplexity3.4219017028808594
INFO:root:current mean train loss 1561.2647162852363
INFO:root:current train perplexity3.4231934547424316
INFO:root:current mean train loss 1561.5368593763783
INFO:root:current train perplexity3.425030469894409
INFO:root:current mean train loss 1561.753240294887
INFO:root:current train perplexity3.4284400939941406
INFO:root:current mean train loss 1562.666821349456
INFO:root:current train perplexity3.4295482635498047
INFO:root:current mean train loss 1562.7138896535564
INFO:root:current train perplexity3.429111957550049
INFO:root:current mean train loss 1562.8059409209247
INFO:root:current train perplexity3.430797815322876
INFO:root:current mean train loss 1563.6591660604543
INFO:root:current train perplexity3.4330227375030518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.09s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.07s/it]
INFO:root:eval mean loss: 3023.805273290869
INFO:root:eval perplexity: 12.112747192382812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [7:36:28<20:03:03, 497.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1532.4571317784926
INFO:root:current train perplexity3.353652238845825
INFO:root:current mean train loss 1542.4321416598648
INFO:root:current train perplexity3.389963388442993
INFO:root:current mean train loss 1541.471466325287
INFO:root:current train perplexity3.3941149711608887
INFO:root:current mean train loss 1543.9291615743123
INFO:root:current train perplexity3.3908324241638184
INFO:root:current mean train loss 1547.0948874477967
INFO:root:current train perplexity3.3931407928466797
INFO:root:current mean train loss 1546.9330679789912
INFO:root:current train perplexity3.3914804458618164
INFO:root:current mean train loss 1548.9397556052222
INFO:root:current train perplexity3.391815423965454
INFO:root:current mean train loss 1548.8622162114698
INFO:root:current train perplexity3.3911516666412354
INFO:root:current mean train loss 1547.0056926626667
INFO:root:current train perplexity3.389742612838745
INFO:root:current mean train loss 1546.3539372511543
INFO:root:current train perplexity3.3931639194488525
INFO:root:current mean train loss 1546.8464133522727
INFO:root:current train perplexity3.3963794708251953
INFO:root:current mean train loss 1547.802059220679
INFO:root:current train perplexity3.398015260696411
INFO:root:current mean train loss 1550.0140902180726
INFO:root:current train perplexity3.4007387161254883
INFO:root:current mean train loss 1551.0858200965436
INFO:root:current train perplexity3.40236496925354
INFO:root:current mean train loss 1551.1253044947753
INFO:root:current train perplexity3.4043939113616943
INFO:root:current mean train loss 1552.0078584952025
INFO:root:current train perplexity3.40535306930542
INFO:root:current mean train loss 1552.8535712065866
INFO:root:current train perplexity3.406583309173584
INFO:root:current mean train loss 1553.5696228801723
INFO:root:current train perplexity3.407613515853882
INFO:root:current mean train loss 1554.1348374428119
INFO:root:current train perplexity3.4092278480529785
INFO:root:current mean train loss 1554.945535937904
INFO:root:current train perplexity3.410628318786621


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.80s/it]
INFO:root:eval mean loss: 3032.206809837181
INFO:root:eval perplexity: 12.19698429107666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [7:44:47<19:55:43, 498.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1530.995942957261
INFO:root:current train perplexity3.3440616130828857
INFO:root:current mean train loss 1531.6394196567157
INFO:root:current train perplexity3.3411993980407715
INFO:root:current mean train loss 1534.5911232997696
INFO:root:current train perplexity3.3510847091674805
INFO:root:current mean train loss 1535.7808737034811
INFO:root:current train perplexity3.35060453414917
INFO:root:current mean train loss 1538.2113713774079
INFO:root:current train perplexity3.3556761741638184
INFO:root:current mean train loss 1537.3323560323558
INFO:root:current train perplexity3.364171028137207
INFO:root:current mean train loss 1536.6082808674755
INFO:root:current train perplexity3.3659818172454834
INFO:root:current mean train loss 1539.095519125541
INFO:root:current train perplexity3.369819402694702
INFO:root:current mean train loss 1539.1575663798565
INFO:root:current train perplexity3.372037410736084
INFO:root:current mean train loss 1539.96648842604
INFO:root:current train perplexity3.372706890106201
INFO:root:current mean train loss 1540.2874213453704
INFO:root:current train perplexity3.3742942810058594
INFO:root:current mean train loss 1540.5685390336528
INFO:root:current train perplexity3.3750033378601074
INFO:root:current mean train loss 1542.1926805235498
INFO:root:current train perplexity3.376234769821167
INFO:root:current mean train loss 1543.3308886140474
INFO:root:current train perplexity3.378688335418701
INFO:root:current mean train loss 1544.160604233745
INFO:root:current train perplexity3.3805999755859375
INFO:root:current mean train loss 1544.4410137518385
INFO:root:current train perplexity3.3818857669830322
INFO:root:current mean train loss 1544.9531229297586
INFO:root:current train perplexity3.3851380348205566
INFO:root:current mean train loss 1545.9464655102217
INFO:root:current train perplexity3.386385917663574
INFO:root:current mean train loss 1547.2013864707844
INFO:root:current train perplexity3.388141393661499
INFO:root:current mean train loss 1546.663559765525
INFO:root:current train perplexity3.3880536556243896


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.82s/it]
INFO:root:eval mean loss: 3035.191136448949
INFO:root:eval perplexity: 12.227046966552734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [7:53:01<19:44:07, 496.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1519.8893971162684
INFO:root:current train perplexity3.3061821460723877
INFO:root:current mean train loss 1523.2422579810732
INFO:root:current train perplexity3.3235950469970703
INFO:root:current mean train loss 1528.0939499584597
INFO:root:current train perplexity3.340482473373413
INFO:root:current mean train loss 1530.6438333262568
INFO:root:current train perplexity3.3385815620422363
INFO:root:current mean train loss 1530.2762847639556
INFO:root:current train perplexity3.3411126136779785
INFO:root:current mean train loss 1529.9849372111576
INFO:root:current train perplexity3.344268560409546
INFO:root:current mean train loss 1528.378694819833
INFO:root:current train perplexity3.345416784286499
INFO:root:current mean train loss 1529.241448243459
INFO:root:current train perplexity3.346269130706787
INFO:root:current mean train loss 1529.6031712123327
INFO:root:current train perplexity3.346930742263794
INFO:root:current mean train loss 1528.673969363378
INFO:root:current train perplexity3.3466389179229736
INFO:root:current mean train loss 1530.5609226869733
INFO:root:current train perplexity3.3486475944519043
INFO:root:current mean train loss 1531.2893036097696
INFO:root:current train perplexity3.3503353595733643
INFO:root:current mean train loss 1533.1326289131814
INFO:root:current train perplexity3.352733612060547
INFO:root:current mean train loss 1534.4649215609008
INFO:root:current train perplexity3.3535807132720947
INFO:root:current mean train loss 1534.6964904618849
INFO:root:current train perplexity3.3560750484466553
INFO:root:current mean train loss 1535.0086975876166
INFO:root:current train perplexity3.3575963973999023
INFO:root:current mean train loss 1536.037412135721
INFO:root:current train perplexity3.3598198890686035
INFO:root:current mean train loss 1536.749959678132
INFO:root:current train perplexity3.3622307777404785
INFO:root:current mean train loss 1538.379366562249
INFO:root:current train perplexity3.3656468391418457
INFO:root:current mean train loss 1539.115791444856
INFO:root:current train perplexity3.3677780628204346


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.39s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.87s/it]
INFO:root:eval mean loss: 3045.7802140519425
INFO:root:eval perplexity: 12.334314346313477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:01:14<19:33:18, 495.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1507.107506606158
INFO:root:current train perplexity3.306142807006836
INFO:root:current mean train loss 1514.407666015625
INFO:root:current train perplexity3.308856964111328
INFO:root:current mean train loss 1518.602140727796
INFO:root:current train perplexity3.316410541534424
INFO:root:current mean train loss 1522.1521677785106
INFO:root:current train perplexity3.3274004459381104
INFO:root:current mean train loss 1524.6161346750162
INFO:root:current train perplexity3.326138496398926
INFO:root:current mean train loss 1527.751024347289
INFO:root:current train perplexity3.3269379138946533
INFO:root:current mean train loss 1527.3387224851733
INFO:root:current train perplexity3.3273849487304688
INFO:root:current mean train loss 1529.0255030540904
INFO:root:current train perplexity3.3291091918945312
INFO:root:current mean train loss 1528.0992448192533
INFO:root:current train perplexity3.3305227756500244
INFO:root:current mean train loss 1527.57425183911
INFO:root:current train perplexity3.3316915035247803
INFO:root:current mean train loss 1527.4254135764688
INFO:root:current train perplexity3.330895185470581
INFO:root:current mean train loss 1527.94721556072
INFO:root:current train perplexity3.332869291305542
INFO:root:current mean train loss 1528.748830024927
INFO:root:current train perplexity3.336345911026001
INFO:root:current mean train loss 1528.3860759638708
INFO:root:current train perplexity3.3387720584869385
INFO:root:current mean train loss 1529.4062901146885
INFO:root:current train perplexity3.3409600257873535
INFO:root:current mean train loss 1529.615231833473
INFO:root:current train perplexity3.344524383544922
INFO:root:current mean train loss 1529.2266064018454
INFO:root:current train perplexity3.3449654579162598
INFO:root:current mean train loss 1529.7151274865414
INFO:root:current train perplexity3.346750259399414
INFO:root:current mean train loss 1530.5265672273915
INFO:root:current train perplexity3.346734046936035


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.07s/it]
INFO:root:eval mean loss: 3053.1229039097693
INFO:root:eval perplexity: 12.409247398376465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:09:24<19:20:51, 493.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1523.9130859375
INFO:root:current train perplexity3.5456042289733887
INFO:root:current mean train loss 1511.3294653799019
INFO:root:current train perplexity3.291210889816284
INFO:root:current mean train loss 1504.8196628683866
INFO:root:current train perplexity3.286682605743408
INFO:root:current mean train loss 1507.0727037846648
INFO:root:current train perplexity3.293358087539673
INFO:root:current mean train loss 1509.7967404797303
INFO:root:current train perplexity3.3053321838378906
INFO:root:current mean train loss 1512.0053064110743
INFO:root:current train perplexity3.3019909858703613
INFO:root:current mean train loss 1514.550420919526
INFO:root:current train perplexity3.303891897201538
INFO:root:current mean train loss 1513.963292483251
INFO:root:current train perplexity3.3059937953948975
INFO:root:current mean train loss 1514.4300430564215
INFO:root:current train perplexity3.307595729827881
INFO:root:current mean train loss 1515.8883030927366
INFO:root:current train perplexity3.307713031768799
INFO:root:current mean train loss 1517.1736698721697
INFO:root:current train perplexity3.30976939201355
INFO:root:current mean train loss 1518.2079181982688
INFO:root:current train perplexity3.3114120960235596
INFO:root:current mean train loss 1517.4476813952656
INFO:root:current train perplexity3.31270432472229
INFO:root:current mean train loss 1517.3461915937621
INFO:root:current train perplexity3.314138650894165
INFO:root:current mean train loss 1518.2599354089582
INFO:root:current train perplexity3.317051887512207
INFO:root:current mean train loss 1518.35032131637
INFO:root:current train perplexity3.316490411758423
INFO:root:current mean train loss 1519.6976659729537
INFO:root:current train perplexity3.3181769847869873
INFO:root:current mean train loss 1520.494052407324
INFO:root:current train perplexity3.3190948963165283
INFO:root:current mean train loss 1521.4652686928803
INFO:root:current train perplexity3.322084426879883
INFO:root:current mean train loss 1521.3080214571628
INFO:root:current train perplexity3.3221559524536133


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 3063.0717854084555
INFO:root:eval perplexity: 12.511500358581543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [8:17:39<19:12:56, 494.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1497.979093852796
INFO:root:current train perplexity3.260439395904541
INFO:root:current mean train loss 1506.9149375082063
INFO:root:current train perplexity3.278377056121826
INFO:root:current mean train loss 1509.4989872065853
INFO:root:current train perplexity3.284290313720703
INFO:root:current mean train loss 1510.2618113550648
INFO:root:current train perplexity3.283026695251465
INFO:root:current mean train loss 1509.1364294520995
INFO:root:current train perplexity3.282989501953125
INFO:root:current mean train loss 1507.3375975621689
INFO:root:current train perplexity3.2823116779327393
INFO:root:current mean train loss 1505.6198272951585
INFO:root:current train perplexity3.2835867404937744
INFO:root:current mean train loss 1506.3972543177917
INFO:root:current train perplexity3.284885883331299
INFO:root:current mean train loss 1506.756567651099
INFO:root:current train perplexity3.288187026977539
INFO:root:current mean train loss 1507.2945646964686
INFO:root:current train perplexity3.286062717437744
INFO:root:current mean train loss 1507.132152913012
INFO:root:current train perplexity3.2879819869995117
INFO:root:current mean train loss 1508.7494175751578
INFO:root:current train perplexity3.289536476135254
INFO:root:current mean train loss 1509.5372514732555
INFO:root:current train perplexity3.2927258014678955
INFO:root:current mean train loss 1510.8128910137
INFO:root:current train perplexity3.296926975250244
INFO:root:current mean train loss 1512.4647269272484
INFO:root:current train perplexity3.2990896701812744
INFO:root:current mean train loss 1511.9342083607637
INFO:root:current train perplexity3.3005387783050537
INFO:root:current mean train loss 1512.6001061762904
INFO:root:current train perplexity3.300734043121338
INFO:root:current mean train loss 1512.8404783167903
INFO:root:current train perplexity3.3004515171051025
INFO:root:current mean train loss 1513.6819848243583
INFO:root:current train perplexity3.302342176437378
INFO:root:current mean train loss 1514.5676577410516
INFO:root:current train perplexity3.3041250705718994


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.79s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.01s/it]
INFO:root:eval mean loss: 3070.641916819163
INFO:root:eval perplexity: 12.589872360229492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [8:25:51<19:03:48, 493.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1483.33347913954
INFO:root:current train perplexity3.2600936889648438
INFO:root:current mean train loss 1489.8309128705193
INFO:root:current train perplexity3.244004249572754
INFO:root:current mean train loss 1495.0902208231264
INFO:root:current train perplexity3.2488391399383545
INFO:root:current mean train loss 1500.2620304652623
INFO:root:current train perplexity3.2626750469207764
INFO:root:current mean train loss 1498.1202535366792
INFO:root:current train perplexity3.261852979660034
INFO:root:current mean train loss 1497.9842435922196
INFO:root:current train perplexity3.2649757862091064
INFO:root:current mean train loss 1496.8472689262726
INFO:root:current train perplexity3.263448715209961
INFO:root:current mean train loss 1496.7136557205863
INFO:root:current train perplexity3.2641561031341553
INFO:root:current mean train loss 1498.3289889833004
INFO:root:current train perplexity3.2668864727020264
INFO:root:current mean train loss 1499.3050527980186
INFO:root:current train perplexity3.266026020050049
INFO:root:current mean train loss 1499.5975381858561
INFO:root:current train perplexity3.2683403491973877
INFO:root:current mean train loss 1500.3497914059062
INFO:root:current train perplexity3.270158290863037
INFO:root:current mean train loss 1501.1824201565344
INFO:root:current train perplexity3.272594928741455
INFO:root:current mean train loss 1501.359145021724
INFO:root:current train perplexity3.2750720977783203
INFO:root:current mean train loss 1502.5009457048932
INFO:root:current train perplexity3.2782506942749023
INFO:root:current mean train loss 1503.5562790234883
INFO:root:current train perplexity3.2799184322357178
INFO:root:current mean train loss 1504.5679139228205
INFO:root:current train perplexity3.2810747623443604
INFO:root:current mean train loss 1505.7156999297956
INFO:root:current train perplexity3.282618522644043
INFO:root:current mean train loss 1506.641358751617
INFO:root:current train perplexity3.2846152782440186
INFO:root:current mean train loss 1507.156490861877
INFO:root:current train perplexity3.2847511768341064


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.43s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.19s/it]
INFO:root:eval mean loss: 3075.691376923799
INFO:root:eval perplexity: 12.642420768737793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [8:34:08<18:57:34, 494.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1488.2266730542453
INFO:root:current train perplexity3.2285757064819336
INFO:root:current mean train loss 1482.8245259203943
INFO:root:current train perplexity3.2252209186553955
INFO:root:current mean train loss 1485.714189009233
INFO:root:current train perplexity3.225445032119751
INFO:root:current mean train loss 1485.6904725677232
INFO:root:current train perplexity3.236638069152832
INFO:root:current mean train loss 1488.003976581902
INFO:root:current train perplexity3.2392311096191406
INFO:root:current mean train loss 1491.88660926198
INFO:root:current train perplexity3.2419257164001465
INFO:root:current mean train loss 1491.1099485248374
INFO:root:current train perplexity3.2422337532043457
INFO:root:current mean train loss 1492.465586384929
INFO:root:current train perplexity3.244837522506714
INFO:root:current mean train loss 1493.06840688654
INFO:root:current train perplexity3.2461633682250977
INFO:root:current mean train loss 1492.6408437786922
INFO:root:current train perplexity3.2477290630340576
INFO:root:current mean train loss 1493.5621604520936
INFO:root:current train perplexity3.249312162399292
INFO:root:current mean train loss 1495.0375405912903
INFO:root:current train perplexity3.2499396800994873
INFO:root:current mean train loss 1495.7670186279493
INFO:root:current train perplexity3.252800464630127
INFO:root:current mean train loss 1496.0415893464465
INFO:root:current train perplexity3.2538046836853027
INFO:root:current mean train loss 1498.0591145441274
INFO:root:current train perplexity3.256239175796509
INFO:root:current mean train loss 1498.0012274629748
INFO:root:current train perplexity3.2575199604034424
INFO:root:current mean train loss 1498.6642552425988
INFO:root:current train perplexity3.2599384784698486
INFO:root:current mean train loss 1498.9044715198054
INFO:root:current train perplexity3.2614541053771973
INFO:root:current mean train loss 1498.9074837467958
INFO:root:current train perplexity3.2628018856048584
INFO:root:current mean train loss 1499.4513887013768
INFO:root:current train perplexity3.2646164894104004


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.59s/it]
INFO:root:eval mean loss: 3076.5913760440126
INFO:root:eval perplexity: 12.651810646057129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [8:42:22<18:48:48, 494.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1467.5143798828126
INFO:root:current train perplexity3.1949539184570312
INFO:root:current mean train loss 1478.3498010971966
INFO:root:current train perplexity3.221921920776367
INFO:root:current mean train loss 1481.2268568250868
INFO:root:current train perplexity3.2276697158813477
INFO:root:current mean train loss 1484.9965615762246
INFO:root:current train perplexity3.2282488346099854
INFO:root:current mean train loss 1486.9965056723736
INFO:root:current train perplexity3.232499122619629
INFO:root:current mean train loss 1485.3391158254524
INFO:root:current train perplexity3.2283852100372314
INFO:root:current mean train loss 1486.2598339479362
INFO:root:current train perplexity3.2303175926208496
INFO:root:current mean train loss 1487.4398973341113
INFO:root:current train perplexity3.23045015335083
INFO:root:current mean train loss 1486.3936972431752
INFO:root:current train perplexity3.2308013439178467
INFO:root:current mean train loss 1487.2660567765383
INFO:root:current train perplexity3.2320494651794434
INFO:root:current mean train loss 1487.9268380138362
INFO:root:current train perplexity3.233807325363159
INFO:root:current mean train loss 1489.205836943276
INFO:root:current train perplexity3.2372264862060547
INFO:root:current mean train loss 1488.970918622355
INFO:root:current train perplexity3.237170934677124
INFO:root:current mean train loss 1489.6755051216069
INFO:root:current train perplexity3.2373619079589844
INFO:root:current mean train loss 1490.5858785408695
INFO:root:current train perplexity3.2388744354248047
INFO:root:current mean train loss 1490.5225605375447
INFO:root:current train perplexity3.2400221824645996
INFO:root:current mean train loss 1491.114793752339
INFO:root:current train perplexity3.2401905059814453
INFO:root:current mean train loss 1491.31371449616
INFO:root:current train perplexity3.2418453693389893
INFO:root:current mean train loss 1491.7184269379804
INFO:root:current train perplexity3.242840051651001
INFO:root:current mean train loss 1492.1722037843035
INFO:root:current train perplexity3.245166540145874


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.53s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.32s/it]
INFO:root:eval mean loss: 3094.0847989102385
INFO:root:eval perplexity: 12.835694313049316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [8:50:40<18:42:57, 495.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1469.0679145900683
INFO:root:current train perplexity3.197517156600952
INFO:root:current mean train loss 1470.6780862247242
INFO:root:current train perplexity3.206317186355591
INFO:root:current mean train loss 1475.8955282284408
INFO:root:current train perplexity3.2089850902557373
INFO:root:current mean train loss 1475.9612939326955
INFO:root:current train perplexity3.2094616889953613
INFO:root:current mean train loss 1475.4095787345996
INFO:root:current train perplexity3.2077603340148926
INFO:root:current mean train loss 1477.0425029280239
INFO:root:current train perplexity3.209484338760376
INFO:root:current mean train loss 1476.8058783518695
INFO:root:current train perplexity3.211644411087036
INFO:root:current mean train loss 1478.775458097155
INFO:root:current train perplexity3.2138783931732178
INFO:root:current mean train loss 1479.5288315765483
INFO:root:current train perplexity3.214611530303955
INFO:root:current mean train loss 1481.2183247945954
INFO:root:current train perplexity3.217681646347046
INFO:root:current mean train loss 1480.5603519218607
INFO:root:current train perplexity3.219698667526245
INFO:root:current mean train loss 1481.3767996886847
INFO:root:current train perplexity3.2202987670898438
INFO:root:current mean train loss 1481.0908028603342
INFO:root:current train perplexity3.2208683490753174
INFO:root:current mean train loss 1482.5739159064922
INFO:root:current train perplexity3.222125768661499
INFO:root:current mean train loss 1483.1941491789519
INFO:root:current train perplexity3.224114418029785
INFO:root:current mean train loss 1483.9157248715146
INFO:root:current train perplexity3.2247986793518066
INFO:root:current mean train loss 1484.1379172387883
INFO:root:current train perplexity3.224818229675293
INFO:root:current mean train loss 1483.9040069665466
INFO:root:current train perplexity3.22407603263855
INFO:root:current mean train loss 1484.1405686548796
INFO:root:current train perplexity3.2246246337890625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.73s/it]
INFO:root:eval mean loss: 3096.5339472773553
INFO:root:eval perplexity: 12.861656188964844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [8:58:53<18:33:29, 494.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1501.0700073242188
INFO:root:current train perplexity3.2329607009887695
INFO:root:current mean train loss 1463.8130023662861
INFO:root:current train perplexity3.156400442123413
INFO:root:current mean train loss 1466.4578444536994
INFO:root:current train perplexity3.162540912628174
INFO:root:current mean train loss 1468.058374103747
INFO:root:current train perplexity3.1706888675689697
INFO:root:current mean train loss 1466.8808361091237
INFO:root:current train perplexity3.1732935905456543
INFO:root:current mean train loss 1467.6310858348058
INFO:root:current train perplexity3.173999071121216
INFO:root:current mean train loss 1467.6554805932456
INFO:root:current train perplexity3.175100088119507
INFO:root:current mean train loss 1470.3248877091842
INFO:root:current train perplexity3.1814799308776855
INFO:root:current mean train loss 1471.8349244986007
INFO:root:current train perplexity3.1832382678985596
INFO:root:current mean train loss 1471.6984787662473
INFO:root:current train perplexity3.185826301574707
INFO:root:current mean train loss 1472.764924311543
INFO:root:current train perplexity3.1892526149749756
INFO:root:current mean train loss 1473.5609005914218
INFO:root:current train perplexity3.194537401199341
INFO:root:current mean train loss 1474.6313445132437
INFO:root:current train perplexity3.1997721195220947
INFO:root:current mean train loss 1475.3068821444833
INFO:root:current train perplexity3.200366973876953
INFO:root:current mean train loss 1475.106876905827
INFO:root:current train perplexity3.202361822128296
INFO:root:current mean train loss 1475.6863945494306
INFO:root:current train perplexity3.203136444091797
INFO:root:current mean train loss 1476.1286550317323
INFO:root:current train perplexity3.2056334018707275
INFO:root:current mean train loss 1476.4783681233723
INFO:root:current train perplexity3.205516815185547
INFO:root:current mean train loss 1477.3221450433498
INFO:root:current train perplexity3.2072834968566895
INFO:root:current mean train loss 1477.198766820571
INFO:root:current train perplexity3.2077829837799072


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.35s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 3106.784211213166
INFO:root:eval perplexity: 12.970866203308105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:07:07<18:24:18, 494.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1464.6534830729167
INFO:root:current train perplexity3.1737570762634277
INFO:root:current mean train loss 1446.8180910220817
INFO:root:current train perplexity3.171762228012085
INFO:root:current mean train loss 1454.6719445966487
INFO:root:current train perplexity3.1689352989196777
INFO:root:current mean train loss 1457.5118381583432
INFO:root:current train perplexity3.1729989051818848
INFO:root:current mean train loss 1458.342652237047
INFO:root:current train perplexity3.1702985763549805
INFO:root:current mean train loss 1460.6338401399053
INFO:root:current train perplexity3.1761622428894043
INFO:root:current mean train loss 1461.0705713834163
INFO:root:current train perplexity3.1752073764801025
INFO:root:current mean train loss 1462.272485825622
INFO:root:current train perplexity3.1764938831329346
INFO:root:current mean train loss 1461.2609549556087
INFO:root:current train perplexity3.1755001544952393
INFO:root:current mean train loss 1463.0143979349043
INFO:root:current train perplexity3.1760754585266113
INFO:root:current mean train loss 1463.0334362661454
INFO:root:current train perplexity3.176424741744995
INFO:root:current mean train loss 1464.2685298596398
INFO:root:current train perplexity3.177121877670288
INFO:root:current mean train loss 1465.216017664504
INFO:root:current train perplexity3.178036689758301
INFO:root:current mean train loss 1466.2095494468856
INFO:root:current train perplexity3.1807312965393066
INFO:root:current mean train loss 1466.864725304858
INFO:root:current train perplexity3.182539701461792
INFO:root:current mean train loss 1467.2891024677947
INFO:root:current train perplexity3.1836907863616943
INFO:root:current mean train loss 1467.873338382822
INFO:root:current train perplexity3.1831889152526855
INFO:root:current mean train loss 1468.9345553462967
INFO:root:current train perplexity3.186047315597534
INFO:root:current mean train loss 1469.6950717111133
INFO:root:current train perplexity3.188537120819092
INFO:root:current mean train loss 1469.6907728950782
INFO:root:current train perplexity3.188936233520508


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it]
INFO:root:eval mean loss: 3116.1578812699418
INFO:root:eval perplexity: 13.071547508239746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [9:15:22<18:16:44, 494.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.6053370425575
INFO:root:current train perplexity3.1441731452941895
INFO:root:current mean train loss 1444.9872445369112
INFO:root:current train perplexity3.1532909870147705
INFO:root:current mean train loss 1445.5749265526524
INFO:root:current train perplexity3.1536266803741455
INFO:root:current mean train loss 1452.9722853440505
INFO:root:current train perplexity3.1576616764068604
INFO:root:current mean train loss 1453.18351738638
INFO:root:current train perplexity3.159777879714966
INFO:root:current mean train loss 1452.5873295099761
INFO:root:current train perplexity3.1562767028808594
INFO:root:current mean train loss 1455.2512666229918
INFO:root:current train perplexity3.158602476119995
INFO:root:current mean train loss 1456.2674380253325
INFO:root:current train perplexity3.1580562591552734
INFO:root:current mean train loss 1458.0080713531568
INFO:root:current train perplexity3.1597766876220703
INFO:root:current mean train loss 1458.2643552345
INFO:root:current train perplexity3.157522678375244
INFO:root:current mean train loss 1458.7905882613047
INFO:root:current train perplexity3.1596336364746094
INFO:root:current mean train loss 1459.2255607296586
INFO:root:current train perplexity3.159297227859497
INFO:root:current mean train loss 1459.8564112945212
INFO:root:current train perplexity3.1606547832489014
INFO:root:current mean train loss 1460.8612212906683
INFO:root:current train perplexity3.164862632751465
INFO:root:current mean train loss 1461.1397752615937
INFO:root:current train perplexity3.1664278507232666
INFO:root:current mean train loss 1461.5972545608897
INFO:root:current train perplexity3.166536569595337
INFO:root:current mean train loss 1462.533230922452
INFO:root:current train perplexity3.1682863235473633
INFO:root:current mean train loss 1463.1872672376205
INFO:root:current train perplexity3.170374870300293
INFO:root:current mean train loss 1462.5588739538348
INFO:root:current train perplexity3.1700034141540527
INFO:root:current mean train loss 1463.1430170238325
INFO:root:current train perplexity3.1719179153442383


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.23s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 3115.7388795045044
INFO:root:eval perplexity: 13.067026138305664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [9:23:35<18:07:23, 494.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.339737215909
INFO:root:current train perplexity3.1260595321655273
INFO:root:current mean train loss 1457.9724428238408
INFO:root:current train perplexity3.1341300010681152
INFO:root:current mean train loss 1454.5326411228555
INFO:root:current train perplexity3.138972520828247
INFO:root:current mean train loss 1451.7207258197623
INFO:root:current train perplexity3.135434865951538
INFO:root:current mean train loss 1450.5472184065934
INFO:root:current train perplexity3.1377716064453125
INFO:root:current mean train loss 1450.857310802013
INFO:root:current train perplexity3.138503313064575
INFO:root:current mean train loss 1450.7472395336356
INFO:root:current train perplexity3.139831781387329
INFO:root:current mean train loss 1451.262631932947
INFO:root:current train perplexity3.140784740447998
INFO:root:current mean train loss 1451.1884324458608
INFO:root:current train perplexity3.1404991149902344
INFO:root:current mean train loss 1452.1519138835488
INFO:root:current train perplexity3.1419708728790283
INFO:root:current mean train loss 1453.1894282481117
INFO:root:current train perplexity3.1452529430389404
INFO:root:current mean train loss 1453.1964613348891
INFO:root:current train perplexity3.1466243267059326
INFO:root:current mean train loss 1454.81908265376
INFO:root:current train perplexity3.1498377323150635
INFO:root:current mean train loss 1455.268358924556
INFO:root:current train perplexity3.1498496532440186
INFO:root:current mean train loss 1454.6638057748066
INFO:root:current train perplexity3.1497130393981934
INFO:root:current mean train loss 1454.4911126532356
INFO:root:current train perplexity3.1506097316741943
INFO:root:current mean train loss 1454.4093085435943
INFO:root:current train perplexity3.151139736175537
INFO:root:current mean train loss 1454.9888216284944
INFO:root:current train perplexity3.1522390842437744
INFO:root:current mean train loss 1455.6656063768742
INFO:root:current train perplexity3.152559280395508
INFO:root:current mean train loss 1455.6351277148938
INFO:root:current train perplexity3.15354323387146


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.48s/it]
INFO:root:eval mean loss: 3128.5905079884574
INFO:root:eval perplexity: 13.206287384033203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [9:31:52<18:00:47, 495.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1439.6031646728516
INFO:root:current train perplexity3.0860087871551514
INFO:root:current mean train loss 1441.464522250863
INFO:root:current train perplexity3.101775884628296
INFO:root:current mean train loss 1441.5020173016717
INFO:root:current train perplexity3.1043500900268555
INFO:root:current mean train loss 1438.8016813544816
INFO:root:current train perplexity3.1022605895996094
INFO:root:current mean train loss 1441.3030467922404
INFO:root:current train perplexity3.1106483936309814
INFO:root:current mean train loss 1442.5194674405184
INFO:root:current train perplexity3.1137311458587646
INFO:root:current mean train loss 1443.3488665989466
INFO:root:current train perplexity3.1177427768707275
INFO:root:current mean train loss 1443.9520817099458
INFO:root:current train perplexity3.117370367050171
INFO:root:current mean train loss 1444.1607317443288
INFO:root:current train perplexity3.1208696365356445
INFO:root:current mean train loss 1441.9386005166136
INFO:root:current train perplexity3.119173288345337
INFO:root:current mean train loss 1443.2961546485103
INFO:root:current train perplexity3.12326717376709
INFO:root:current mean train loss 1443.7574418103736
INFO:root:current train perplexity3.1245176792144775
INFO:root:current mean train loss 1444.638071504029
INFO:root:current train perplexity3.1274032592773438
INFO:root:current mean train loss 1445.1704220785691
INFO:root:current train perplexity3.1279826164245605
INFO:root:current mean train loss 1446.009019188259
INFO:root:current train perplexity3.1304714679718018
INFO:root:current mean train loss 1447.199032771375
INFO:root:current train perplexity3.131300449371338
INFO:root:current mean train loss 1447.3442915044902
INFO:root:current train perplexity3.132840633392334
INFO:root:current mean train loss 1448.465410495181
INFO:root:current train perplexity3.13474440574646
INFO:root:current mean train loss 1449.398306365706
INFO:root:current train perplexity3.1355268955230713
INFO:root:current mean train loss 1448.8972224918389
INFO:root:current train perplexity3.1361770629882812


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.35s/it]
INFO:root:eval mean loss: 3136.4237386800864
INFO:root:eval perplexity: 13.291897773742676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [9:40:07<17:52:40, 495.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1427.4941186797753
INFO:root:current train perplexity3.0868844985961914
INFO:root:current mean train loss 1431.0004966776207
INFO:root:current train perplexity3.08615779876709
INFO:root:current mean train loss 1436.2928226035358
INFO:root:current train perplexity3.09653377532959
INFO:root:current mean train loss 1437.5444445769401
INFO:root:current train perplexity3.0927727222442627
INFO:root:current mean train loss 1437.2730238588797
INFO:root:current train perplexity3.096125602722168
INFO:root:current mean train loss 1438.6189538926544
INFO:root:current train perplexity3.0972023010253906
INFO:root:current mean train loss 1436.8334564076106
INFO:root:current train perplexity3.096958637237549
INFO:root:current mean train loss 1436.106864126614
INFO:root:current train perplexity3.0994553565979004
INFO:root:current mean train loss 1437.358682261275
INFO:root:current train perplexity3.101696014404297
INFO:root:current mean train loss 1437.7715722557507
INFO:root:current train perplexity3.1043803691864014
INFO:root:current mean train loss 1438.3963697028883
INFO:root:current train perplexity3.106475830078125
INFO:root:current mean train loss 1439.1121070547401
INFO:root:current train perplexity3.1092917919158936
INFO:root:current mean train loss 1439.8827012256654
INFO:root:current train perplexity3.1105523109436035
INFO:root:current mean train loss 1439.9465890092074
INFO:root:current train perplexity3.1128320693969727
INFO:root:current mean train loss 1440.0950047254082
INFO:root:current train perplexity3.114762783050537
INFO:root:current mean train loss 1440.457871145989
INFO:root:current train perplexity3.1158461570739746
INFO:root:current mean train loss 1440.1540396528317
INFO:root:current train perplexity3.1157805919647217
INFO:root:current mean train loss 1440.6580100915098
INFO:root:current train perplexity3.116811990737915
INFO:root:current mean train loss 1441.9058588451023
INFO:root:current train perplexity3.1198883056640625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.10s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 3145.8649975659255
INFO:root:eval perplexity: 13.395814895629883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [9:48:21<17:43:24, 494.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.7323201497395
INFO:root:current train perplexity3.1033785343170166
INFO:root:current mean train loss 1413.1703836692955
INFO:root:current train perplexity3.073394775390625
INFO:root:current mean train loss 1422.4983538285042
INFO:root:current train perplexity3.0833113193511963
INFO:root:current mean train loss 1426.0300081539776
INFO:root:current train perplexity3.0926430225372314
INFO:root:current mean train loss 1426.8396519252233
INFO:root:current train perplexity3.0913259983062744
INFO:root:current mean train loss 1426.1727890798697
INFO:root:current train perplexity3.0887949466705322
INFO:root:current mean train loss 1425.8760138281896
INFO:root:current train perplexity3.085639238357544
INFO:root:current mean train loss 1425.8169797729838
INFO:root:current train perplexity3.0850462913513184
INFO:root:current mean train loss 1427.376480291854
INFO:root:current train perplexity3.085076332092285
INFO:root:current mean train loss 1427.3865395518592
INFO:root:current train perplexity3.085132360458374
INFO:root:current mean train loss 1427.5987124130218
INFO:root:current train perplexity3.086217164993286
INFO:root:current mean train loss 1429.0094121067261
INFO:root:current train perplexity3.089000701904297
INFO:root:current mean train loss 1429.8938977192488
INFO:root:current train perplexity3.089153528213501
INFO:root:current mean train loss 1430.7015358426852
INFO:root:current train perplexity3.0888869762420654
INFO:root:current mean train loss 1432.146727386952
INFO:root:current train perplexity3.0906174182891846
INFO:root:current mean train loss 1432.710094923042
INFO:root:current train perplexity3.092589855194092
INFO:root:current mean train loss 1432.7730442298898
INFO:root:current train perplexity3.093487024307251
INFO:root:current mean train loss 1433.226207666073
INFO:root:current train perplexity3.095853805541992
INFO:root:current mean train loss 1433.5960907624542
INFO:root:current train perplexity3.097743034362793
INFO:root:current mean train loss 1434.342437263801
INFO:root:current train perplexity3.1004369258880615


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.51s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.27s/it]
INFO:root:eval mean loss: 3153.0301172754785
INFO:root:eval perplexity: 13.475217819213867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [9:56:40<17:37:57, 495.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1391.0702912703805
INFO:root:current train perplexity3.0692636966705322
INFO:root:current mean train loss 1414.8718956427845
INFO:root:current train perplexity3.056781768798828
INFO:root:current mean train loss 1419.9375979846902
INFO:root:current train perplexity3.060488700866699
INFO:root:current mean train loss 1418.9985022766302
INFO:root:current train perplexity3.060944080352783
INFO:root:current mean train loss 1418.9623939748633
INFO:root:current train perplexity3.065916061401367
INFO:root:current mean train loss 1419.0028148527126
INFO:root:current train perplexity3.067082166671753
INFO:root:current mean train loss 1418.114075980638
INFO:root:current train perplexity3.06723690032959
INFO:root:current mean train loss 1420.107326650026
INFO:root:current train perplexity3.071528196334839
INFO:root:current mean train loss 1421.2034192739975
INFO:root:current train perplexity3.0741353034973145
INFO:root:current mean train loss 1421.5792654250322
INFO:root:current train perplexity3.0737342834472656
INFO:root:current mean train loss 1421.7968129505743
INFO:root:current train perplexity3.0723865032196045
INFO:root:current mean train loss 1423.212762467477
INFO:root:current train perplexity3.074280261993408
INFO:root:current mean train loss 1422.4274875394458
INFO:root:current train perplexity3.074124336242676
INFO:root:current mean train loss 1423.1002424244436
INFO:root:current train perplexity3.0764360427856445
INFO:root:current mean train loss 1423.2180476024464
INFO:root:current train perplexity3.0769007205963135
INFO:root:current mean train loss 1424.7694555070943
INFO:root:current train perplexity3.0793769359588623
INFO:root:current mean train loss 1424.9404193081389
INFO:root:current train perplexity3.0797231197357178
INFO:root:current mean train loss 1425.5906846394596
INFO:root:current train perplexity3.0801618099212646
INFO:root:current mean train loss 1426.0206099953289
INFO:root:current train perplexity3.081674098968506
INFO:root:current mean train loss 1427.1367624871011
INFO:root:current train perplexity3.083287000656128


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.15s/it]
INFO:root:eval mean loss: 3157.8552290083053
INFO:root:eval perplexity: 13.528961181640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [10:04:53<17:27:45, 495.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1398.326416015625
INFO:root:current train perplexity3.0321643352508545
INFO:root:current mean train loss 1411.2699384416853
INFO:root:current train perplexity3.0637404918670654
INFO:root:current mean train loss 1416.987215169271
INFO:root:current train perplexity3.0569818019866943
INFO:root:current mean train loss 1414.994818833295
INFO:root:current train perplexity3.050429582595825
INFO:root:current mean train loss 1414.8695215398616
INFO:root:current train perplexity3.0474157333374023
INFO:root:current mean train loss 1413.819137008102
INFO:root:current train perplexity3.048191547393799
INFO:root:current mean train loss 1415.8982381820679
INFO:root:current train perplexity3.051582098007202
INFO:root:current mean train loss 1416.7204000936972
INFO:root:current train perplexity3.053528308868408
INFO:root:current mean train loss 1416.6440409342447
INFO:root:current train perplexity3.054436206817627
INFO:root:current mean train loss 1417.4504777624252
INFO:root:current train perplexity3.055018424987793
INFO:root:current mean train loss 1417.3645364614633
INFO:root:current train perplexity3.0574774742126465
INFO:root:current mean train loss 1417.837343343099
INFO:root:current train perplexity3.0600359439849854
INFO:root:current mean train loss 1418.4960630355342
INFO:root:current train perplexity3.0618371963500977
INFO:root:current mean train loss 1419.1318457760028
INFO:root:current train perplexity3.0636374950408936
INFO:root:current mean train loss 1419.865294816759
INFO:root:current train perplexity3.0654468536376953
INFO:root:current mean train loss 1419.7367545784293
INFO:root:current train perplexity3.065915107727051
INFO:root:current mean train loss 1420.257669290682
INFO:root:current train perplexity3.066798448562622
INFO:root:current mean train loss 1420.607712458468
INFO:root:current train perplexity3.067594528198242
INFO:root:current mean train loss 1421.3906311035157
INFO:root:current train perplexity3.06929612159729
INFO:root:current mean train loss 1421.3462871748147
INFO:root:current train perplexity3.0702457427978516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.75s/it]
INFO:root:eval mean loss: 3157.3216864325263
INFO:root:eval perplexity: 13.523011207580566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [10:13:06<17:18:31, 494.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1411.4525960286458
INFO:root:current train perplexity3.0112788677215576
INFO:root:current mean train loss 1398.597828858977
INFO:root:current train perplexity3.0130105018615723
INFO:root:current mean train loss 1400.3623251117158
INFO:root:current train perplexity3.0249977111816406
INFO:root:current mean train loss 1404.8999679950105
INFO:root:current train perplexity3.0267980098724365
INFO:root:current mean train loss 1406.231178200219
INFO:root:current train perplexity3.025874137878418
INFO:root:current mean train loss 1407.456574308166
INFO:root:current train perplexity3.0319442749023438
INFO:root:current mean train loss 1408.70058586318
INFO:root:current train perplexity3.0324313640594482
INFO:root:current mean train loss 1409.9518552752436
INFO:root:current train perplexity3.033825635910034
INFO:root:current mean train loss 1410.3033648104763
INFO:root:current train perplexity3.0354630947113037
INFO:root:current mean train loss 1410.7370425615939
INFO:root:current train perplexity3.036013126373291
INFO:root:current mean train loss 1410.4738523542824
INFO:root:current train perplexity3.039151191711426
INFO:root:current mean train loss 1412.0776091817131
INFO:root:current train perplexity3.0436112880706787
INFO:root:current mean train loss 1413.3031404020298
INFO:root:current train perplexity3.0462615489959717
INFO:root:current mean train loss 1412.9932867625046
INFO:root:current train perplexity3.0476930141448975
INFO:root:current mean train loss 1412.7034480632292
INFO:root:current train perplexity3.04935622215271
INFO:root:current mean train loss 1412.8166247535073
INFO:root:current train perplexity3.0505783557891846
INFO:root:current mean train loss 1413.8473481642748
INFO:root:current train perplexity3.051133632659912
INFO:root:current mean train loss 1414.5471923689172
INFO:root:current train perplexity3.05342960357666
INFO:root:current mean train loss 1415.5090443781132
INFO:root:current train perplexity3.0550947189331055
INFO:root:current mean train loss 1415.7080202877492
INFO:root:current train perplexity3.055872678756714


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.79s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.16s/it]
INFO:root:eval mean loss: 3175.5232952679244
INFO:root:eval perplexity: 13.72757625579834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [10:21:17<17:08:03, 493.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1401.7116451778927
INFO:root:current train perplexity3.039323568344116
INFO:root:current mean train loss 1396.5871224238954
INFO:root:current train perplexity3.012104034423828
INFO:root:current mean train loss 1396.4864738074532
INFO:root:current train perplexity3.007563829421997
INFO:root:current mean train loss 1398.0489782649565
INFO:root:current train perplexity3.0115370750427246
INFO:root:current mean train loss 1399.379028835377
INFO:root:current train perplexity3.0138614177703857
INFO:root:current mean train loss 1400.8367186224004
INFO:root:current train perplexity3.0155649185180664
INFO:root:current mean train loss 1402.1269964110604
INFO:root:current train perplexity3.017606735229492
INFO:root:current mean train loss 1402.3251456327216
INFO:root:current train perplexity3.0186376571655273
INFO:root:current mean train loss 1401.1837614919282
INFO:root:current train perplexity3.0194013118743896
INFO:root:current mean train loss 1401.426962599862
INFO:root:current train perplexity3.019634962081909
INFO:root:current mean train loss 1402.3150942782895
INFO:root:current train perplexity3.0234529972076416
INFO:root:current mean train loss 1404.6069884941971
INFO:root:current train perplexity3.0259487628936768
INFO:root:current mean train loss 1404.88406185228
INFO:root:current train perplexity3.0280778408050537
INFO:root:current mean train loss 1405.3805595298002
INFO:root:current train perplexity3.02801513671875
INFO:root:current mean train loss 1405.725094260807
INFO:root:current train perplexity3.0314834117889404
INFO:root:current mean train loss 1405.758529100818
INFO:root:current train perplexity3.0316412448883057
INFO:root:current mean train loss 1406.3530051756645
INFO:root:current train perplexity3.033068895339966
INFO:root:current mean train loss 1407.0906524142122
INFO:root:current train perplexity3.033738136291504
INFO:root:current mean train loss 1406.9930048630104
INFO:root:current train perplexity3.035325765609741
INFO:root:current mean train loss 1407.7667807721077
INFO:root:current train perplexity3.0367465019226074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.80s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.62s/it]
INFO:root:eval mean loss: 3181.1543093386354
INFO:root:eval perplexity: 13.791487693786621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [10:29:37<17:03:31, 495.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1384.5748841002746
INFO:root:current train perplexity3.014460325241089
INFO:root:current mean train loss 1386.731171772742
INFO:root:current train perplexity3.010042190551758
INFO:root:current mean train loss 1386.3311469743342
INFO:root:current train perplexity2.998814821243286
INFO:root:current mean train loss 1387.3304930391823
INFO:root:current train perplexity3.000406265258789
INFO:root:current mean train loss 1391.0890921847156
INFO:root:current train perplexity3.0044565200805664
INFO:root:current mean train loss 1391.7001087685728
INFO:root:current train perplexity3.003704071044922
INFO:root:current mean train loss 1392.7208327915837
INFO:root:current train perplexity3.0052242279052734
INFO:root:current mean train loss 1393.4553960325143
INFO:root:current train perplexity3.007866621017456
INFO:root:current mean train loss 1395.0564680003156
INFO:root:current train perplexity3.0080299377441406
INFO:root:current mean train loss 1395.3563922223843
INFO:root:current train perplexity3.0099050998687744
INFO:root:current mean train loss 1397.6197253541045
INFO:root:current train perplexity3.0129480361938477
INFO:root:current mean train loss 1398.3131757427122
INFO:root:current train perplexity3.014934539794922
INFO:root:current mean train loss 1399.596530669055
INFO:root:current train perplexity3.016322374343872
INFO:root:current mean train loss 1399.470759026358
INFO:root:current train perplexity3.0179877281188965
INFO:root:current mean train loss 1399.9995706661846
INFO:root:current train perplexity3.018751621246338
INFO:root:current mean train loss 1400.5010997069699
INFO:root:current train perplexity3.0198702812194824
INFO:root:current mean train loss 1400.733644382831
INFO:root:current train perplexity3.0200419425964355
INFO:root:current mean train loss 1401.4676626813539
INFO:root:current train perplexity3.0208842754364014
INFO:root:current mean train loss 1402.0671545519897
INFO:root:current train perplexity3.022318124771118


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.20s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.78s/it]
INFO:root:eval mean loss: 3190.4879894542983
INFO:root:eval perplexity: 13.89808464050293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [10:37:51<16:54:30, 494.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1384.2737426757812
INFO:root:current train perplexity3.0052971839904785
INFO:root:current mean train loss 1392.900507043909
INFO:root:current train perplexity2.994436264038086
INFO:root:current mean train loss 1387.5381117600662
INFO:root:current train perplexity2.9915683269500732
INFO:root:current mean train loss 1386.535962389661
INFO:root:current train perplexity2.991281747817993
INFO:root:current mean train loss 1388.2685089111328
INFO:root:current train perplexity2.990504026412964
INFO:root:current mean train loss 1388.8520978792446
INFO:root:current train perplexity2.989838123321533
INFO:root:current mean train loss 1388.2450808474891
INFO:root:current train perplexity2.99214506149292
INFO:root:current mean train loss 1388.85917792886
INFO:root:current train perplexity2.9945180416107178
INFO:root:current mean train loss 1388.469668246732
INFO:root:current train perplexity2.9941816329956055
INFO:root:current mean train loss 1389.3979611837917
INFO:root:current train perplexity2.999476909637451
INFO:root:current mean train loss 1391.948846362886
INFO:root:current train perplexity3.0012459754943848
INFO:root:current mean train loss 1392.57015495438
INFO:root:current train perplexity3.0023040771484375
INFO:root:current mean train loss 1391.4240043589612
INFO:root:current train perplexity3.001286268234253
INFO:root:current mean train loss 1392.1977319746572
INFO:root:current train perplexity3.002389907836914
INFO:root:current mean train loss 1392.9100701592185
INFO:root:current train perplexity3.0035712718963623
INFO:root:current mean train loss 1393.5925833704616
INFO:root:current train perplexity3.005241632461548
INFO:root:current mean train loss 1394.4902005171894
INFO:root:current train perplexity3.007068634033203
INFO:root:current mean train loss 1394.7449072808795
INFO:root:current train perplexity3.0072929859161377
INFO:root:current mean train loss 1394.965656314276
INFO:root:current train perplexity3.0074996948242188
INFO:root:current mean train loss 1395.4453802528622
INFO:root:current train perplexity3.0081167221069336


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.49s/it]
INFO:root:eval mean loss: 3201.546705641188
INFO:root:eval perplexity: 14.02543830871582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [10:46:06<16:46:41, 495.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1385.3035400390625
INFO:root:current train perplexity2.9883482456207275
INFO:root:current mean train loss 1378.10898046875
INFO:root:current train perplexity2.9721434116363525
INFO:root:current mean train loss 1379.9438492838542
INFO:root:current train perplexity2.9693777561187744
INFO:root:current mean train loss 1380.422928560697
INFO:root:current train perplexity2.9716150760650635
INFO:root:current mean train loss 1379.9934897748162
INFO:root:current train perplexity2.9719502925872803
INFO:root:current mean train loss 1380.0541069103422
INFO:root:current train perplexity2.9709432125091553
INFO:root:current mean train loss 1383.4820796875
INFO:root:current train perplexity2.9751856327056885
INFO:root:current mean train loss 1384.6601092739763
INFO:root:current train perplexity2.977088689804077
INFO:root:current mean train loss 1383.4492610677082
INFO:root:current train perplexity2.9770889282226562
INFO:root:current mean train loss 1383.543572899071
INFO:root:current train perplexity2.9805333614349365
INFO:root:current mean train loss 1384.2584152296113
INFO:root:current train perplexity2.980140209197998
INFO:root:current mean train loss 1385.646855251736
INFO:root:current train perplexity2.9819600582122803
INFO:root:current mean train loss 1386.2592256257972
INFO:root:current train perplexity2.9853947162628174
INFO:root:current mean train loss 1386.5035230874116
INFO:root:current train perplexity2.987624168395996
INFO:root:current mean train loss 1387.2230301706416
INFO:root:current train perplexity2.9885356426239014
INFO:root:current mean train loss 1387.5070374135503
INFO:root:current train perplexity2.988757371902466
INFO:root:current mean train loss 1386.6334649939904
INFO:root:current train perplexity2.987689256668091
INFO:root:current mean train loss 1387.6575505264946
INFO:root:current train perplexity2.989410161972046
INFO:root:current mean train loss 1388.0699419413527
INFO:root:current train perplexity2.9905457496643066
INFO:root:current mean train loss 1388.6067389787947
INFO:root:current train perplexity2.9911928176879883


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 450.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 450.00s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.90s/it]
INFO:root:eval mean loss: 3205.005224462744
INFO:root:eval perplexity: 14.065508842468262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [10:54:21<16:38:19, 495.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1379.4571416945685
INFO:root:current train perplexity2.960543394088745
INFO:root:current mean train loss 1372.61738264057
INFO:root:current train perplexity2.9571380615234375
INFO:root:current mean train loss 1374.1913714448283
INFO:root:current train perplexity2.9579918384552
INFO:root:current mean train loss 1376.469730845669
INFO:root:current train perplexity2.9535388946533203
INFO:root:current mean train loss 1375.8609141354107
INFO:root:current train perplexity2.9532365798950195
INFO:root:current mean train loss 1375.0037344056302
INFO:root:current train perplexity2.953701972961426
INFO:root:current mean train loss 1375.5123979324865
INFO:root:current train perplexity2.954263687133789
INFO:root:current mean train loss 1377.0625167805551
INFO:root:current train perplexity2.9587485790252686
INFO:root:current mean train loss 1376.302726836216
INFO:root:current train perplexity2.959582805633545
INFO:root:current mean train loss 1377.115838376833
INFO:root:current train perplexity2.9624974727630615
INFO:root:current mean train loss 1377.570155636134
INFO:root:current train perplexity2.9633941650390625
INFO:root:current mean train loss 1378.6351617656114
INFO:root:current train perplexity2.966574192047119
INFO:root:current mean train loss 1379.6266841182003
INFO:root:current train perplexity2.9681544303894043
INFO:root:current mean train loss 1379.8434119871229
INFO:root:current train perplexity2.968975067138672
INFO:root:current mean train loss 1380.2590256689657
INFO:root:current train perplexity2.9695796966552734
INFO:root:current mean train loss 1379.9502440772942
INFO:root:current train perplexity2.9720304012298584
INFO:root:current mean train loss 1380.405368521501
INFO:root:current train perplexity2.9730310440063477
INFO:root:current mean train loss 1381.303450189146
INFO:root:current train perplexity2.9747121334075928
INFO:root:current mean train loss 1381.614728134437
INFO:root:current train perplexity2.9757816791534424
INFO:root:current mean train loss 1382.2340867330313
INFO:root:current train perplexity2.976426362991333


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.43s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.68s/it]
INFO:root:eval mean loss: 3212.1937611439566
INFO:root:eval perplexity: 14.149164199829102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [11:02:43<16:34:23, 497.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.7244335109904
INFO:root:current train perplexity2.921783447265625
INFO:root:current mean train loss 1361.7336402749115
INFO:root:current train perplexity2.9192728996276855
INFO:root:current mean train loss 1362.2329718983772
INFO:root:current train perplexity2.9296486377716064
INFO:root:current mean train loss 1366.2383237535907
INFO:root:current train perplexity2.930877447128296
INFO:root:current mean train loss 1369.1406952103757
INFO:root:current train perplexity2.9366540908813477
INFO:root:current mean train loss 1370.4842374252291
INFO:root:current train perplexity2.9409985542297363
INFO:root:current mean train loss 1371.287768999253
INFO:root:current train perplexity2.939676523208618
INFO:root:current mean train loss 1371.7312466868927
INFO:root:current train perplexity2.9412784576416016
INFO:root:current mean train loss 1372.738364382867
INFO:root:current train perplexity2.943297863006592
INFO:root:current mean train loss 1372.7950481458552
INFO:root:current train perplexity2.947051763534546
INFO:root:current mean train loss 1372.3501050334926
INFO:root:current train perplexity2.9494755268096924
INFO:root:current mean train loss 1372.462866611168
INFO:root:current train perplexity2.9504282474517822
INFO:root:current mean train loss 1373.5613925439957
INFO:root:current train perplexity2.952289342880249
INFO:root:current mean train loss 1374.3176876738985
INFO:root:current train perplexity2.9537911415100098
INFO:root:current mean train loss 1374.5097321581563
INFO:root:current train perplexity2.9551193714141846
INFO:root:current mean train loss 1375.2422908565186
INFO:root:current train perplexity2.9569506645202637
INFO:root:current mean train loss 1375.8291231216328
INFO:root:current train perplexity2.9593498706817627
INFO:root:current mean train loss 1376.4323234276178
INFO:root:current train perplexity2.9615867137908936
INFO:root:current mean train loss 1377.0515076307406
INFO:root:current train perplexity2.963329553604126
INFO:root:current mean train loss 1377.4113187531905
INFO:root:current train perplexity2.9646878242492676


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.03s/it]
INFO:root:eval mean loss: 3218.4189929675767
INFO:root:eval perplexity: 14.222001075744629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [11:11:11<16:32:11, 500.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1364.1683783280223
INFO:root:current train perplexity2.948564052581787
INFO:root:current mean train loss 1364.4130429354582
INFO:root:current train perplexity2.9354496002197266
INFO:root:current mean train loss 1361.7644989455955
INFO:root:current train perplexity2.9267473220825195
INFO:root:current mean train loss 1360.0635509085148
INFO:root:current train perplexity2.9300453662872314
INFO:root:current mean train loss 1359.7291567505908
INFO:root:current train perplexity2.930471181869507
INFO:root:current mean train loss 1361.5421687232124
INFO:root:current train perplexity2.930253028869629
INFO:root:current mean train loss 1361.1226580918894
INFO:root:current train perplexity2.9303998947143555
INFO:root:current mean train loss 1361.4926216676063
INFO:root:current train perplexity2.93290638923645
INFO:root:current mean train loss 1363.0010449831889
INFO:root:current train perplexity2.9363317489624023
INFO:root:current mean train loss 1363.097261647709
INFO:root:current train perplexity2.9366817474365234
INFO:root:current mean train loss 1363.4262352698797
INFO:root:current train perplexity2.9371068477630615
INFO:root:current mean train loss 1364.8577480186411
INFO:root:current train perplexity2.9389445781707764
INFO:root:current mean train loss 1366.2614114695582
INFO:root:current train perplexity2.9402730464935303
INFO:root:current mean train loss 1367.8054418342058
INFO:root:current train perplexity2.9417917728424072
INFO:root:current mean train loss 1368.8350677903752
INFO:root:current train perplexity2.942991256713867
INFO:root:current mean train loss 1369.4434053062787
INFO:root:current train perplexity2.944838285446167
INFO:root:current mean train loss 1369.5918584928309
INFO:root:current train perplexity2.946359395980835
INFO:root:current mean train loss 1370.8887034235775
INFO:root:current train perplexity2.9482173919677734
INFO:root:current mean train loss 1371.3607702194247
INFO:root:current train perplexity2.948728084564209
INFO:root:current mean train loss 1371.2352001483623
INFO:root:current train perplexity2.9500317573547363


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.66s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.81s/it]
INFO:root:eval mean loss: 3227.9381180496903
INFO:root:eval perplexity: 14.334113121032715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [11:19:37<16:27:14, 501.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1347.821063298051
INFO:root:current train perplexity2.893725872039795
INFO:root:current mean train loss 1351.3984836716727
INFO:root:current train perplexity2.9032299518585205
INFO:root:current mean train loss 1353.1654311040156
INFO:root:current train perplexity2.909015417098999
INFO:root:current mean train loss 1355.5598657040196
INFO:root:current train perplexity2.909869432449341
INFO:root:current mean train loss 1354.8245453437976
INFO:root:current train perplexity2.909684896469116
INFO:root:current mean train loss 1356.158038855001
INFO:root:current train perplexity2.9159579277038574
INFO:root:current mean train loss 1357.342546383196
INFO:root:current train perplexity2.9185969829559326
INFO:root:current mean train loss 1357.46758502128
INFO:root:current train perplexity2.9214606285095215
INFO:root:current mean train loss 1358.7150761346934
INFO:root:current train perplexity2.92326283454895
INFO:root:current mean train loss 1360.031508400601
INFO:root:current train perplexity2.9252116680145264
INFO:root:current mean train loss 1361.4986855942288
INFO:root:current train perplexity2.924783945083618
INFO:root:current mean train loss 1361.6472476981612
INFO:root:current train perplexity2.9259047508239746
INFO:root:current mean train loss 1362.314341439633
INFO:root:current train perplexity2.9268205165863037
INFO:root:current mean train loss 1362.551331048378
INFO:root:current train perplexity2.9286129474639893
INFO:root:current mean train loss 1363.0485292857554
INFO:root:current train perplexity2.9300668239593506
INFO:root:current mean train loss 1363.043934507783
INFO:root:current train perplexity2.9311022758483887
INFO:root:current mean train loss 1363.6959315760207
INFO:root:current train perplexity2.932706594467163
INFO:root:current mean train loss 1364.378076811842
INFO:root:current train perplexity2.93380069732666
INFO:root:current mean train loss 1364.9090546508723
INFO:root:current train perplexity2.9346251487731934


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.25s/it]
INFO:root:eval mean loss: 3227.5314545502533
INFO:root:eval perplexity: 14.329309463500977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [11:28:25<16:34:13, 509.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1379.1703979492188
INFO:root:current train perplexity2.923506021499634
INFO:root:current mean train loss 1358.824102228338
INFO:root:current train perplexity2.9187614917755127
INFO:root:current mean train loss 1354.974049595424
INFO:root:current train perplexity2.904998779296875
INFO:root:current mean train loss 1354.336218655494
INFO:root:current train perplexity2.9100255966186523
INFO:root:current mean train loss 1355.7625759217797
INFO:root:current train perplexity2.9085497856140137
INFO:root:current mean train loss 1355.651584520527
INFO:root:current train perplexity2.911787509918213
INFO:root:current mean train loss 1356.0987180616034
INFO:root:current train perplexity2.910733938217163
INFO:root:current mean train loss 1356.8940594740318
INFO:root:current train perplexity2.912093162536621
INFO:root:current mean train loss 1356.9430240584009
INFO:root:current train perplexity2.9120070934295654
INFO:root:current mean train loss 1356.399176494892
INFO:root:current train perplexity2.91337251663208
INFO:root:current mean train loss 1356.153838325727
INFO:root:current train perplexity2.913159132003784
INFO:root:current mean train loss 1357.1444787927576
INFO:root:current train perplexity2.914396047592163
INFO:root:current mean train loss 1357.7023391093105
INFO:root:current train perplexity2.915520668029785
INFO:root:current mean train loss 1357.8946208924738
INFO:root:current train perplexity2.9179537296295166
INFO:root:current mean train loss 1358.4959565291167
INFO:root:current train perplexity2.919508218765259
INFO:root:current mean train loss 1358.5839769376034
INFO:root:current train perplexity2.9200661182403564
INFO:root:current mean train loss 1358.9217327615488
INFO:root:current train perplexity2.9211697578430176
INFO:root:current mean train loss 1359.1139384308754
INFO:root:current train perplexity2.9222183227539062
INFO:root:current mean train loss 1359.6652720077261
INFO:root:current train perplexity2.9236299991607666
INFO:root:current mean train loss 1360.226047631708
INFO:root:current train perplexity2.923537015914917


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.97s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.86s/it]
INFO:root:eval mean loss: 3240.1181801919106
INFO:root:eval perplexity: 14.478857040405273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [11:36:41<16:17:35, 505.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1334.0297987196182
INFO:root:current train perplexity2.8750014305114746
INFO:root:current mean train loss 1342.6614461583415
INFO:root:current train perplexity2.8841679096221924
INFO:root:current mean train loss 1342.7080804093819
INFO:root:current train perplexity2.8812997341156006
INFO:root:current mean train loss 1342.136465650086
INFO:root:current train perplexity2.8806707859039307
INFO:root:current mean train loss 1342.1427942494877
INFO:root:current train perplexity2.8826863765716553
INFO:root:current mean train loss 1344.8933457550106
INFO:root:current train perplexity2.8866381645202637
INFO:root:current mean train loss 1345.8128669896955
INFO:root:current train perplexity2.8905534744262695
INFO:root:current mean train loss 1347.8944679480635
INFO:root:current train perplexity2.8934760093688965
INFO:root:current mean train loss 1348.7699045460342
INFO:root:current train perplexity2.895724296569824
INFO:root:current mean train loss 1350.1105866169853
INFO:root:current train perplexity2.89919114112854
INFO:root:current mean train loss 1349.7274413587056
INFO:root:current train perplexity2.9004526138305664
INFO:root:current mean train loss 1350.8123636321886
INFO:root:current train perplexity2.9017369747161865
INFO:root:current mean train loss 1351.7489741915304
INFO:root:current train perplexity2.90490984916687
INFO:root:current mean train loss 1352.5709534841278
INFO:root:current train perplexity2.9045157432556152
INFO:root:current mean train loss 1352.2484537190128
INFO:root:current train perplexity2.9052529335021973
INFO:root:current mean train loss 1352.8792386457822
INFO:root:current train perplexity2.9065444469451904
INFO:root:current mean train loss 1352.7361525598303
INFO:root:current train perplexity2.9075400829315186
INFO:root:current mean train loss 1352.8233411443255
INFO:root:current train perplexity2.9079930782318115
INFO:root:current mean train loss 1353.1118244240045
INFO:root:current train perplexity2.909151554107666
INFO:root:current mean train loss 1353.7035059708662
INFO:root:current train perplexity2.909545660018921


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.32s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.87s/it]
INFO:root:eval mean loss: 3249.2949621985267
INFO:root:eval perplexity: 14.588874816894531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [11:44:54<16:02:00, 501.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1339.3373274369674
INFO:root:current train perplexity2.8728768825531006
INFO:root:current mean train loss 1338.0013003879124
INFO:root:current train perplexity2.878661632537842
INFO:root:current mean train loss 1335.2298473920978
INFO:root:current train perplexity2.874160051345825
INFO:root:current mean train loss 1339.3698975319087
INFO:root:current train perplexity2.881640911102295
INFO:root:current mean train loss 1340.2542323206997
INFO:root:current train perplexity2.8768060207366943
INFO:root:current mean train loss 1340.6023660547594
INFO:root:current train perplexity2.876922369003296
INFO:root:current mean train loss 1342.3274763593022
INFO:root:current train perplexity2.877802848815918
INFO:root:current mean train loss 1342.527847125966
INFO:root:current train perplexity2.879197120666504
INFO:root:current mean train loss 1341.8249686724766
INFO:root:current train perplexity2.880026340484619
INFO:root:current mean train loss 1342.271129802122
INFO:root:current train perplexity2.88407301902771
INFO:root:current mean train loss 1343.5869206103328
INFO:root:current train perplexity2.8859903812408447
INFO:root:current mean train loss 1343.8486413488854
INFO:root:current train perplexity2.8867387771606445
INFO:root:current mean train loss 1344.2785841767045
INFO:root:current train perplexity2.886573076248169
INFO:root:current mean train loss 1345.8634697142102
INFO:root:current train perplexity2.88889741897583
INFO:root:current mean train loss 1347.2650340072337
INFO:root:current train perplexity2.891188144683838
INFO:root:current mean train loss 1347.9187331125527
INFO:root:current train perplexity2.8939692974090576
INFO:root:current mean train loss 1347.9391779470327
INFO:root:current train perplexity2.895256280899048
INFO:root:current mean train loss 1348.0179946619437
INFO:root:current train perplexity2.895585536956787
INFO:root:current mean train loss 1347.946053461501
INFO:root:current train perplexity2.8962514400482178
INFO:root:current mean train loss 1348.6326529420453
INFO:root:current train perplexity2.898425340652466


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.44s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 3256.1210556259384
INFO:root:eval perplexity: 14.671245574951172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [11:53:11<15:50:36, 500.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1320.8463935226691
INFO:root:current train perplexity2.840590476989746
INFO:root:current mean train loss 1326.871350780037
INFO:root:current train perplexity2.8465588092803955
INFO:root:current mean train loss 1329.809109157986
INFO:root:current train perplexity2.8549909591674805
INFO:root:current mean train loss 1330.6034794434947
INFO:root:current train perplexity2.858813524246216
INFO:root:current mean train loss 1330.9387418866934
INFO:root:current train perplexity2.8591957092285156
INFO:root:current mean train loss 1331.5856907482453
INFO:root:current train perplexity2.8619251251220703
INFO:root:current mean train loss 1333.7155776492766
INFO:root:current train perplexity2.8651583194732666
INFO:root:current mean train loss 1333.647563117249
INFO:root:current train perplexity2.8670928478240967
INFO:root:current mean train loss 1335.4176730024137
INFO:root:current train perplexity2.8715410232543945
INFO:root:current mean train loss 1337.0155755875635
INFO:root:current train perplexity2.8725152015686035
INFO:root:current mean train loss 1337.9217043776875
INFO:root:current train perplexity2.874068260192871
INFO:root:current mean train loss 1338.555735559324
INFO:root:current train perplexity2.874992609024048
INFO:root:current mean train loss 1339.253453883172
INFO:root:current train perplexity2.8752989768981934
INFO:root:current mean train loss 1340.1427314976925
INFO:root:current train perplexity2.8771584033966064
INFO:root:current mean train loss 1340.403400188762
INFO:root:current train perplexity2.8791592121124268
INFO:root:current mean train loss 1340.9260115492123
INFO:root:current train perplexity2.8810882568359375
INFO:root:current mean train loss 1341.376820618815
INFO:root:current train perplexity2.881972074508667
INFO:root:current mean train loss 1341.2978609898496
INFO:root:current train perplexity2.8832616806030273
INFO:root:current mean train loss 1342.5938192671952
INFO:root:current train perplexity2.8842713832855225
INFO:root:current mean train loss 1342.854232422373
INFO:root:current train perplexity2.885009288787842


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.51s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.92s/it]
INFO:root:eval mean loss: 3261.305443382836
INFO:root:eval perplexity: 14.734125137329102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [12:01:30<15:41:46, 500.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1324.1939947666267
INFO:root:current train perplexity2.8335089683532715
INFO:root:current mean train loss 1327.8445496291256
INFO:root:current train perplexity2.8307552337646484
INFO:root:current mean train loss 1328.308837890625
INFO:root:current train perplexity2.833678722381592
INFO:root:current mean train loss 1327.7916298518105
INFO:root:current train perplexity2.841421365737915
INFO:root:current mean train loss 1328.7169863648994
INFO:root:current train perplexity2.8446741104125977
INFO:root:current mean train loss 1332.049273744999
INFO:root:current train perplexity2.8522579669952393
INFO:root:current mean train loss 1331.7476621194576
INFO:root:current train perplexity2.853332281112671
INFO:root:current mean train loss 1331.6059567174445
INFO:root:current train perplexity2.8565831184387207
INFO:root:current mean train loss 1332.0634294305685
INFO:root:current train perplexity2.857609987258911
INFO:root:current mean train loss 1331.3058792457502
INFO:root:current train perplexity2.859250545501709
INFO:root:current mean train loss 1332.2349718762682
INFO:root:current train perplexity2.8604393005371094
INFO:root:current mean train loss 1332.7076079234203
INFO:root:current train perplexity2.861621141433716
INFO:root:current mean train loss 1332.8505255709604
INFO:root:current train perplexity2.862734794616699
INFO:root:current mean train loss 1333.5262504322955
INFO:root:current train perplexity2.8649086952209473
INFO:root:current mean train loss 1334.2462978337228
INFO:root:current train perplexity2.865959405899048
INFO:root:current mean train loss 1334.9311921829205
INFO:root:current train perplexity2.866715669631958
INFO:root:current mean train loss 1335.655327634391
INFO:root:current train perplexity2.867609739303589
INFO:root:current mean train loss 1336.3486564301534
INFO:root:current train perplexity2.8696212768554688
INFO:root:current mean train loss 1337.386734025039
INFO:root:current train perplexity2.8716647624969482
INFO:root:current mean train loss 1337.6808822215266
INFO:root:current train perplexity2.872840642929077


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.87s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.32s/it]
INFO:root:eval mean loss: 3274.8919586089996
INFO:root:eval perplexity: 14.90018081665039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [12:09:43<15:29:37, 498.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1312.4629651521382
INFO:root:current train perplexity2.8343710899353027
INFO:root:current mean train loss 1317.514703525641
INFO:root:current train perplexity2.8253774642944336
INFO:root:current mean train loss 1317.3786724543168
INFO:root:current train perplexity2.832186460494995
INFO:root:current mean train loss 1318.6652220134495
INFO:root:current train perplexity2.833569288253784
INFO:root:current mean train loss 1320.893040759154
INFO:root:current train perplexity2.837636947631836
INFO:root:current mean train loss 1320.539623613117
INFO:root:current train perplexity2.8384525775909424
INFO:root:current mean train loss 1321.6910424980329
INFO:root:current train perplexity2.839022159576416
INFO:root:current mean train loss 1322.131302359719
INFO:root:current train perplexity2.840024948120117
INFO:root:current mean train loss 1322.5742817628318
INFO:root:current train perplexity2.839897871017456
INFO:root:current mean train loss 1323.595737844496
INFO:root:current train perplexity2.842167854309082
INFO:root:current mean train loss 1324.1924364342538
INFO:root:current train perplexity2.8441245555877686
INFO:root:current mean train loss 1324.9222064796352
INFO:root:current train perplexity2.8453545570373535
INFO:root:current mean train loss 1326.3761990226833
INFO:root:current train perplexity2.8481552600860596
INFO:root:current mean train loss 1327.5866565335182
INFO:root:current train perplexity2.8511903285980225
INFO:root:current mean train loss 1328.399638606553
INFO:root:current train perplexity2.8517136573791504
INFO:root:current mean train loss 1329.2916153384601
INFO:root:current train perplexity2.854210138320923
INFO:root:current mean train loss 1329.3791677469349
INFO:root:current train perplexity2.8555350303649902
INFO:root:current mean train loss 1330.3990505717923
INFO:root:current train perplexity2.856083631515503
INFO:root:current mean train loss 1331.2404991934984
INFO:root:current train perplexity2.857840061187744


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.88s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.33s/it]
INFO:root:eval mean loss: 3281.920539191535
INFO:root:eval perplexity: 14.986817359924316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [12:18:01<15:20:54, 497.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1346.9654235839844
INFO:root:current train perplexity2.8461968898773193
INFO:root:current mean train loss 1323.0401317051478
INFO:root:current train perplexity2.836726665496826
INFO:root:current mean train loss 1320.1792061283904
INFO:root:current train perplexity2.8216962814331055
INFO:root:current mean train loss 1316.3091954940405
INFO:root:current train perplexity2.8191938400268555
INFO:root:current mean train loss 1318.3895242931774
INFO:root:current train perplexity2.821300745010376
INFO:root:current mean train loss 1321.832938194275
INFO:root:current train perplexity2.825735092163086
INFO:root:current mean train loss 1320.5820645600363
INFO:root:current train perplexity2.8276877403259277
INFO:root:current mean train loss 1320.2285109959291
INFO:root:current train perplexity2.8299946784973145
INFO:root:current mean train loss 1319.7802848628003
INFO:root:current train perplexity2.829373836517334
INFO:root:current mean train loss 1320.3349084686815
INFO:root:current train perplexity2.830723285675049
INFO:root:current mean train loss 1320.3834762874799
INFO:root:current train perplexity2.8335657119750977
INFO:root:current mean train loss 1321.1723743685716
INFO:root:current train perplexity2.8349435329437256
INFO:root:current mean train loss 1322.5536123345
INFO:root:current train perplexity2.8354504108428955
INFO:root:current mean train loss 1323.8046067400676
INFO:root:current train perplexity2.837857723236084
INFO:root:current mean train loss 1324.5386479623594
INFO:root:current train perplexity2.8406107425689697
INFO:root:current mean train loss 1324.4326035433976
INFO:root:current train perplexity2.841425657272339
INFO:root:current mean train loss 1325.2277472332748
INFO:root:current train perplexity2.842838764190674
INFO:root:current mean train loss 1325.8560796719846
INFO:root:current train perplexity2.8440940380096436
INFO:root:current mean train loss 1326.6317977400015
INFO:root:current train perplexity2.846841335296631
INFO:root:current mean train loss 1326.7596353826164
INFO:root:current train perplexity2.8477518558502197


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.83s/it]
INFO:root:eval mean loss: 3280.4200875680367
INFO:root:eval perplexity: 14.968283653259277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [12:26:15<15:10:33, 496.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1308.3276914399246
INFO:root:current train perplexity2.771868944168091
INFO:root:current mean train loss 1316.0302980408187
INFO:root:current train perplexity2.798936605453491
INFO:root:current mean train loss 1311.8905866198145
INFO:root:current train perplexity2.8046412467956543
INFO:root:current mean train loss 1314.9615868101728
INFO:root:current train perplexity2.8095569610595703
INFO:root:current mean train loss 1315.3830850952434
INFO:root:current train perplexity2.8143250942230225
INFO:root:current mean train loss 1315.1055518224243
INFO:root:current train perplexity2.81611704826355
INFO:root:current mean train loss 1314.7356519835553
INFO:root:current train perplexity2.819549560546875
INFO:root:current mean train loss 1314.7861641254608
INFO:root:current train perplexity2.821716547012329
INFO:root:current mean train loss 1316.4396589806054
INFO:root:current train perplexity2.8249289989471436
INFO:root:current mean train loss 1316.8096562478977
INFO:root:current train perplexity2.8266329765319824
INFO:root:current mean train loss 1317.1744448825848
INFO:root:current train perplexity2.82828688621521
INFO:root:current mean train loss 1317.4441561297679
INFO:root:current train perplexity2.829319715499878
INFO:root:current mean train loss 1318.0724690424124
INFO:root:current train perplexity2.8303885459899902
INFO:root:current mean train loss 1319.1509730537823
INFO:root:current train perplexity2.831317186355591
INFO:root:current mean train loss 1320.0290847682218
INFO:root:current train perplexity2.8323755264282227
INFO:root:current mean train loss 1320.5246732124244
INFO:root:current train perplexity2.8327081203460693
INFO:root:current mean train loss 1320.3699326207843
INFO:root:current train perplexity2.8345000743865967
INFO:root:current mean train loss 1320.7046670113912
INFO:root:current train perplexity2.83453631401062
INFO:root:current mean train loss 1320.4703425203536
INFO:root:current train perplexity2.8357887268066406
INFO:root:current mean train loss 1320.5601201035186
INFO:root:current train perplexity2.835672616958618


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.03s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.43s/it]
INFO:root:eval mean loss: 3291.9896119263794
INFO:root:eval perplexity: 15.11181640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [12:34:25<14:58:55, 494.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1304.2804034689198
INFO:root:current train perplexity2.7670350074768066
INFO:root:current mean train loss 1293.2779348713077
INFO:root:current train perplexity2.7811167240142822
INFO:root:current mean train loss 1298.888217832984
INFO:root:current train perplexity2.797046422958374
INFO:root:current mean train loss 1303.7304369976066
INFO:root:current train perplexity2.798757791519165
INFO:root:current mean train loss 1304.2467626734165
INFO:root:current train perplexity2.8062422275543213
INFO:root:current mean train loss 1305.2498146588111
INFO:root:current train perplexity2.806964159011841
INFO:root:current mean train loss 1307.3185194269422
INFO:root:current train perplexity2.8124139308929443
INFO:root:current mean train loss 1308.5029779592724
INFO:root:current train perplexity2.8110666275024414
INFO:root:current mean train loss 1309.906365865793
INFO:root:current train perplexity2.813521385192871
INFO:root:current mean train loss 1310.0786301852784
INFO:root:current train perplexity2.814390182495117
INFO:root:current mean train loss 1311.5534749660164
INFO:root:current train perplexity2.8165266513824463
INFO:root:current mean train loss 1312.0025748740525
INFO:root:current train perplexity2.815842866897583
INFO:root:current mean train loss 1312.4542954446415
INFO:root:current train perplexity2.8182249069213867
INFO:root:current mean train loss 1312.8675543457757
INFO:root:current train perplexity2.818612813949585
INFO:root:current mean train loss 1313.417668977054
INFO:root:current train perplexity2.8201539516448975
INFO:root:current mean train loss 1313.9607566053778
INFO:root:current train perplexity2.8207895755767822
INFO:root:current mean train loss 1314.0255205564627
INFO:root:current train perplexity2.8209800720214844
INFO:root:current mean train loss 1314.1735966388576
INFO:root:current train perplexity2.8222343921661377
INFO:root:current mean train loss 1314.8924539386257
INFO:root:current train perplexity2.823573350906372
INFO:root:current mean train loss 1315.909353948203
INFO:root:current train perplexity2.823812961578369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.87s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.92s/it]
INFO:root:eval mean loss: 3300.7381390179244
INFO:root:eval perplexity: 15.221264839172363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [12:42:46<14:53:56, 496.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1289.4658668154761
INFO:root:current train perplexity2.7795190811157227
INFO:root:current mean train loss 1292.109494074722
INFO:root:current train perplexity2.778623580932617
INFO:root:current mean train loss 1295.749097700808
INFO:root:current train perplexity2.7814433574676514
INFO:root:current mean train loss 1300.4516050060263
INFO:root:current train perplexity2.7869911193847656
INFO:root:current mean train loss 1301.7776142557034
INFO:root:current train perplexity2.7909581661224365
INFO:root:current mean train loss 1302.518682828805
INFO:root:current train perplexity2.7934305667877197
INFO:root:current mean train loss 1303.428675034467
INFO:root:current train perplexity2.7947020530700684
INFO:root:current mean train loss 1303.6416233207733
INFO:root:current train perplexity2.7948315143585205
INFO:root:current mean train loss 1305.123073467374
INFO:root:current train perplexity2.7947065830230713
INFO:root:current mean train loss 1304.9000867802035
INFO:root:current train perplexity2.797478437423706
INFO:root:current mean train loss 1306.6077643149547
INFO:root:current train perplexity2.8005220890045166
INFO:root:current mean train loss 1306.9161311876949
INFO:root:current train perplexity2.8014180660247803
INFO:root:current mean train loss 1308.1633344274235
INFO:root:current train perplexity2.803826332092285
INFO:root:current mean train loss 1308.1937234364968
INFO:root:current train perplexity2.8060686588287354
INFO:root:current mean train loss 1308.7634744598536
INFO:root:current train perplexity2.806258201599121
INFO:root:current mean train loss 1309.1186170425465
INFO:root:current train perplexity2.8077311515808105
INFO:root:current mean train loss 1309.6655324086037
INFO:root:current train perplexity2.808640480041504
INFO:root:current mean train loss 1310.0458494155027
INFO:root:current train perplexity2.8090760707855225
INFO:root:current mean train loss 1310.2209442515432
INFO:root:current train perplexity2.809753894805908
INFO:root:current mean train loss 1310.4817718490312
INFO:root:current train perplexity2.8120076656341553


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.76s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.54s/it]
INFO:root:eval mean loss: 3312.79749158338
INFO:root:eval perplexity: 15.373431205749512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [12:51:06<14:47:39, 497.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1297.0740463256836
INFO:root:current train perplexity2.772507905960083
INFO:root:current mean train loss 1298.0370659722223
INFO:root:current train perplexity2.7749791145324707
INFO:root:current mean train loss 1296.0636897495815
INFO:root:current train perplexity2.7795305252075195
INFO:root:current mean train loss 1298.4239791067023
INFO:root:current train perplexity2.777505874633789
INFO:root:current mean train loss 1301.8466451009115
INFO:root:current train perplexity2.7821755409240723
INFO:root:current mean train loss 1301.0672725282866
INFO:root:current train perplexity2.782456874847412
INFO:root:current mean train loss 1301.8236994126264
INFO:root:current train perplexity2.784726142883301
INFO:root:current mean train loss 1302.1432975573418
INFO:root:current train perplexity2.7882559299468994
INFO:root:current mean train loss 1300.9059930974786
INFO:root:current train perplexity2.7876696586608887
INFO:root:current mean train loss 1301.8368165308116
INFO:root:current train perplexity2.7903809547424316
INFO:root:current mean train loss 1302.2223368326822
INFO:root:current train perplexity2.7908456325531006
INFO:root:current mean train loss 1301.8187557931674
INFO:root:current train perplexity2.791980743408203
INFO:root:current mean train loss 1302.8240827560426
INFO:root:current train perplexity2.792917013168335
INFO:root:current mean train loss 1303.3623944710994
INFO:root:current train perplexity2.7945876121520996
INFO:root:current mean train loss 1304.2055317646748
INFO:root:current train perplexity2.796337127685547
INFO:root:current mean train loss 1304.6978449181665
INFO:root:current train perplexity2.7987523078918457
INFO:root:current mean train loss 1304.3214272635323
INFO:root:current train perplexity2.7996859550476074
INFO:root:current mean train loss 1304.7060784157743
INFO:root:current train perplexity2.8007946014404297
INFO:root:current mean train loss 1304.9329277525558
INFO:root:current train perplexity2.800035238265991
INFO:root:current mean train loss 1305.7525642163826
INFO:root:current train perplexity2.8016505241394043


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 3310.0974069772897
INFO:root:eval perplexity: 15.339231491088867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [12:59:31<14:43:14, 499.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1288.949728425016
INFO:root:current train perplexity2.771110773086548
INFO:root:current mean train loss 1284.3784607243417
INFO:root:current train perplexity2.768772840499878
INFO:root:current mean train loss 1291.5832297585227
INFO:root:current train perplexity2.769798755645752
INFO:root:current mean train loss 1294.2731127991183
INFO:root:current train perplexity2.7710602283477783
INFO:root:current mean train loss 1293.6800045880752
INFO:root:current train perplexity2.773261785507202
INFO:root:current mean train loss 1294.062672779588
INFO:root:current train perplexity2.7760009765625
INFO:root:current mean train loss 1293.9061494715074
INFO:root:current train perplexity2.774911642074585
INFO:root:current mean train loss 1295.4591231399977
INFO:root:current train perplexity2.7761454582214355
INFO:root:current mean train loss 1295.9347575686315
INFO:root:current train perplexity2.7780206203460693
INFO:root:current mean train loss 1296.8220936001364
INFO:root:current train perplexity2.7823729515075684
INFO:root:current mean train loss 1297.0170616907974
INFO:root:current train perplexity2.783799409866333
INFO:root:current mean train loss 1297.4082314754987
INFO:root:current train perplexity2.7832820415496826
INFO:root:current mean train loss 1297.4822475695114
INFO:root:current train perplexity2.784320116043091
INFO:root:current mean train loss 1298.2917172016207
INFO:root:current train perplexity2.7861814498901367
INFO:root:current mean train loss 1298.4101899273808
INFO:root:current train perplexity2.78786301612854
INFO:root:current mean train loss 1298.7721512137014
INFO:root:current train perplexity2.7881524562835693
INFO:root:current mean train loss 1299.000221697527
INFO:root:current train perplexity2.788475513458252
INFO:root:current mean train loss 1299.4573591483854
INFO:root:current train perplexity2.7883975505828857
INFO:root:current mean train loss 1300.1124910426001
INFO:root:current train perplexity2.7906172275543213


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.59s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.00s/it]
INFO:root:eval mean loss: 3322.1114102383635
INFO:root:eval perplexity: 15.492000579833984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [13:07:53<14:35:46, 500.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1300.370326450893
INFO:root:current train perplexity2.7536256313323975
INFO:root:current mean train loss 1296.5904134114583
INFO:root:current train perplexity2.767648935317993
INFO:root:current mean train loss 1287.393770307024
INFO:root:current train perplexity2.77016544342041
INFO:root:current mean train loss 1287.0787287426601
INFO:root:current train perplexity2.767091751098633
INFO:root:current mean train loss 1286.9952259893003
INFO:root:current train perplexity2.765744686126709
INFO:root:current mean train loss 1287.4074493289459
INFO:root:current train perplexity2.765631914138794
INFO:root:current mean train loss 1287.5459525142508
INFO:root:current train perplexity2.76800537109375
INFO:root:current mean train loss 1288.3686518308496
INFO:root:current train perplexity2.767585039138794
INFO:root:current mean train loss 1289.9179317090084
INFO:root:current train perplexity2.7679831981658936
INFO:root:current mean train loss 1290.6523361373
INFO:root:current train perplexity2.769037961959839
INFO:root:current mean train loss 1291.7607996111085
INFO:root:current train perplexity2.7688002586364746
INFO:root:current mean train loss 1291.5792674641634
INFO:root:current train perplexity2.7696361541748047
INFO:root:current mean train loss 1291.7660704460332
INFO:root:current train perplexity2.772663116455078
INFO:root:current mean train loss 1292.0649350890649
INFO:root:current train perplexity2.7737107276916504
INFO:root:current mean train loss 1292.2678457473257
INFO:root:current train perplexity2.7753477096557617
INFO:root:current mean train loss 1293.1308684859282
INFO:root:current train perplexity2.7763400077819824
INFO:root:current mean train loss 1294.183754770877
INFO:root:current train perplexity2.777237892150879
INFO:root:current mean train loss 1294.4846209923332
INFO:root:current train perplexity2.7781636714935303
INFO:root:current mean train loss 1295.311973765246
INFO:root:current train perplexity2.779224395751953
INFO:root:current mean train loss 1295.940846537846
INFO:root:current train perplexity2.7796878814697266


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.17s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.28s/it]
INFO:root:eval mean loss: 3327.33790088917
INFO:root:eval perplexity: 15.558928489685059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [13:16:13<14:26:55, 500.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1285.0948919480848
INFO:root:current train perplexity2.7577381134033203
INFO:root:current mean train loss 1274.0672141504651
INFO:root:current train perplexity2.758397340774536
INFO:root:current mean train loss 1275.5510222199675
INFO:root:current train perplexity2.758291482925415
INFO:root:current mean train loss 1277.7640303412952
INFO:root:current train perplexity2.7556040287017822
INFO:root:current mean train loss 1280.2880012529909
INFO:root:current train perplexity2.758417844772339
INFO:root:current mean train loss 1280.7386957373324
INFO:root:current train perplexity2.760178565979004
INFO:root:current mean train loss 1281.006967680579
INFO:root:current train perplexity2.759232759475708
INFO:root:current mean train loss 1281.295551931483
INFO:root:current train perplexity2.7580387592315674
INFO:root:current mean train loss 1282.6278252446693
INFO:root:current train perplexity2.7584385871887207
INFO:root:current mean train loss 1284.494316977922
INFO:root:current train perplexity2.7596354484558105
INFO:root:current mean train loss 1285.3727005552482
INFO:root:current train perplexity2.7617297172546387
INFO:root:current mean train loss 1286.3457287047206
INFO:root:current train perplexity2.761301279067993
INFO:root:current mean train loss 1286.553860674439
INFO:root:current train perplexity2.762212038040161
INFO:root:current mean train loss 1287.9653192831106
INFO:root:current train perplexity2.763824462890625
INFO:root:current mean train loss 1288.3496814570503
INFO:root:current train perplexity2.764899730682373
INFO:root:current mean train loss 1288.8814896596639
INFO:root:current train perplexity2.767096519470215
INFO:root:current mean train loss 1288.9273155488388
INFO:root:current train perplexity2.766954183578491
INFO:root:current mean train loss 1289.1541963838415
INFO:root:current train perplexity2.767324686050415
INFO:root:current mean train loss 1290.1398977116116
INFO:root:current train perplexity2.76800537109375
INFO:root:current mean train loss 1290.5908199332034
INFO:root:current train perplexity2.769096851348877


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.96s/it]
INFO:root:eval mean loss: 3330.1982055297485
INFO:root:eval perplexity: 15.595681190490723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [13:24:31<14:17:50, 499.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1270.1005427042644
INFO:root:current train perplexity2.727595806121826
INFO:root:current mean train loss 1267.5876258643898
INFO:root:current train perplexity2.7227261066436768
INFO:root:current mean train loss 1272.8562636836882
INFO:root:current train perplexity2.7354531288146973
INFO:root:current mean train loss 1272.9075987366425
INFO:root:current train perplexity2.7364282608032227
INFO:root:current mean train loss 1275.630992071969
INFO:root:current train perplexity2.7389912605285645
INFO:root:current mean train loss 1276.7956273433936
INFO:root:current train perplexity2.7423501014709473
INFO:root:current mean train loss 1277.507893315068
INFO:root:current train perplexity2.7428641319274902
INFO:root:current mean train loss 1277.2494763053037
INFO:root:current train perplexity2.7440614700317383
INFO:root:current mean train loss 1278.441639594312
INFO:root:current train perplexity2.7448513507843018
INFO:root:current mean train loss 1278.6618975546792
INFO:root:current train perplexity2.7449421882629395
INFO:root:current mean train loss 1279.0854973247033
INFO:root:current train perplexity2.7473230361938477
INFO:root:current mean train loss 1280.6360442796235
INFO:root:current train perplexity2.7482569217681885
INFO:root:current mean train loss 1281.375160315098
INFO:root:current train perplexity2.7505075931549072
INFO:root:current mean train loss 1281.161230251414
INFO:root:current train perplexity2.7510080337524414
INFO:root:current mean train loss 1282.1564909371223
INFO:root:current train perplexity2.7514543533325195
INFO:root:current mean train loss 1283.9709679261043
INFO:root:current train perplexity2.7548165321350098
INFO:root:current mean train loss 1284.7044682178682
INFO:root:current train perplexity2.7569081783294678
INFO:root:current mean train loss 1285.4495929918792
INFO:root:current train perplexity2.757723093032837
INFO:root:current mean train loss 1285.8741684290237
INFO:root:current train perplexity2.758547782897949
INFO:root:current mean train loss 1286.4487680047444
INFO:root:current train perplexity2.760234832763672


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.93s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.46s/it]
INFO:root:eval mean loss: 3333.213083444773
INFO:root:eval perplexity: 15.63451099395752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [13:32:49<14:08:21, 499.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1276.6898080679086
INFO:root:current train perplexity2.7236642837524414
INFO:root:current mean train loss 1273.521738873106
INFO:root:current train perplexity2.7386510372161865
INFO:root:current mean train loss 1268.9779421248527
INFO:root:current train perplexity2.732581377029419
INFO:root:current mean train loss 1269.2218321917808
INFO:root:current train perplexity2.733333110809326
INFO:root:current mean train loss 1271.0192674206148
INFO:root:current train perplexity2.732339382171631
INFO:root:current mean train loss 1271.8312046287333
INFO:root:current train perplexity2.7363553047180176
INFO:root:current mean train loss 1272.5305759515977
INFO:root:current train perplexity2.7397031784057617
INFO:root:current mean train loss 1274.150572214563
INFO:root:current train perplexity2.740358829498291
INFO:root:current mean train loss 1275.1590010273662
INFO:root:current train perplexity2.7395918369293213
INFO:root:current mean train loss 1275.8538738665802
INFO:root:current train perplexity2.7395107746124268
INFO:root:current mean train loss 1277.36943393761
INFO:root:current train perplexity2.7414493560791016
INFO:root:current mean train loss 1278.5574266949436
INFO:root:current train perplexity2.741089344024658
INFO:root:current mean train loss 1279.155704688272
INFO:root:current train perplexity2.7430341243743896
INFO:root:current mean train loss 1279.2203067765568
INFO:root:current train perplexity2.7445132732391357
INFO:root:current mean train loss 1279.662628736268
INFO:root:current train perplexity2.7435717582702637
INFO:root:current mean train loss 1279.8127269805811
INFO:root:current train perplexity2.745055675506592
INFO:root:current mean train loss 1280.5321063983906
INFO:root:current train perplexity2.7459404468536377
INFO:root:current mean train loss 1280.5629792902355
INFO:root:current train perplexity2.7463245391845703
INFO:root:current mean train loss 1280.6227776003268
INFO:root:current train perplexity2.747227191925049
INFO:root:current mean train loss 1281.1174563031766
INFO:root:current train perplexity2.7477643489837646


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.36s/it]
INFO:root:eval mean loss: 3340.8168153505067
INFO:root:eval perplexity: 15.732880592346191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [13:41:07<13:59:39, 498.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1265.4510498046875
INFO:root:current train perplexity2.707681894302368
INFO:root:current mean train loss 1265.9603157462655
INFO:root:current train perplexity2.703442096710205
INFO:root:current mean train loss 1268.2039015749667
INFO:root:current train perplexity2.7096920013427734
INFO:root:current mean train loss 1267.246134972697
INFO:root:current train perplexity2.71348237991333
INFO:root:current mean train loss 1268.7309547519287
INFO:root:current train perplexity2.718172073364258
INFO:root:current mean train loss 1269.6867820503785
INFO:root:current train perplexity2.7211432456970215
INFO:root:current mean train loss 1270.3115705115354
INFO:root:current train perplexity2.7203283309936523
INFO:root:current mean train loss 1269.5542975618407
INFO:root:current train perplexity2.720698356628418
INFO:root:current mean train loss 1270.3909019418313
INFO:root:current train perplexity2.7225685119628906
INFO:root:current mean train loss 1269.7638306161293
INFO:root:current train perplexity2.723426103591919
INFO:root:current mean train loss 1270.7255751068624
INFO:root:current train perplexity2.725010871887207
INFO:root:current mean train loss 1271.5998777851034
INFO:root:current train perplexity2.725764036178589
INFO:root:current mean train loss 1271.6366249596274
INFO:root:current train perplexity2.7275702953338623
INFO:root:current mean train loss 1272.7187722588415
INFO:root:current train perplexity2.729041814804077
INFO:root:current mean train loss 1273.3385476795768
INFO:root:current train perplexity2.730905055999756
INFO:root:current mean train loss 1273.4592593032703
INFO:root:current train perplexity2.732447862625122
INFO:root:current mean train loss 1273.7764133448834
INFO:root:current train perplexity2.733194589614868
INFO:root:current mean train loss 1274.9198795408513
INFO:root:current train perplexity2.735950231552124
INFO:root:current mean train loss 1275.3546064743707
INFO:root:current train perplexity2.735912322998047
INFO:root:current mean train loss 1276.0045472731142
INFO:root:current train perplexity2.736715793609619


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.40s/it]
INFO:root:eval mean loss: 3348.402419264968
INFO:root:eval perplexity: 15.831634521484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [13:49:22<13:49:42, 497.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1257.993775647096
INFO:root:current train perplexity2.6926627159118652
INFO:root:current mean train loss 1262.199803337979
INFO:root:current train perplexity2.697175979614258
INFO:root:current mean train loss 1261.2292745838995
INFO:root:current train perplexity2.700089454650879
INFO:root:current mean train loss 1262.4419964094807
INFO:root:current train perplexity2.7044670581817627
INFO:root:current mean train loss 1263.5878324030875
INFO:root:current train perplexity2.7055819034576416
INFO:root:current mean train loss 1263.3648125293457
INFO:root:current train perplexity2.708512544631958
INFO:root:current mean train loss 1265.6721200138031
INFO:root:current train perplexity2.7104556560516357
INFO:root:current mean train loss 1266.4610684314866
INFO:root:current train perplexity2.711186647415161
INFO:root:current mean train loss 1267.0469720859548
INFO:root:current train perplexity2.7136433124542236
INFO:root:current mean train loss 1266.8139476146068
INFO:root:current train perplexity2.7163939476013184
INFO:root:current mean train loss 1267.2041783146256
INFO:root:current train perplexity2.7187278270721436
INFO:root:current mean train loss 1267.6387269542652
INFO:root:current train perplexity2.720611333847046
INFO:root:current mean train loss 1268.3689371744792
INFO:root:current train perplexity2.721667528152466
INFO:root:current mean train loss 1268.5173772630562
INFO:root:current train perplexity2.7222530841827393
INFO:root:current mean train loss 1269.1048052217102
INFO:root:current train perplexity2.7215371131896973
INFO:root:current mean train loss 1269.4147937004084
INFO:root:current train perplexity2.7231454849243164
INFO:root:current mean train loss 1269.8879986561487
INFO:root:current train perplexity2.7241439819335938
INFO:root:current mean train loss 1271.0627889924742
INFO:root:current train perplexity2.726255178451538
INFO:root:current mean train loss 1271.578704303663
INFO:root:current train perplexity2.7271769046783447


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.99s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:42<00:00, 42.21s/it]
INFO:root:eval mean loss: 3356.7364358987893
INFO:root:eval perplexity: 15.940848350524902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [13:57:37<13:39:37, 496.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1251.1436004638672
INFO:root:current train perplexity2.6907737255096436
INFO:root:current mean train loss 1257.2754800730738
INFO:root:current train perplexity2.6947340965270996
INFO:root:current mean train loss 1256.516663727937
INFO:root:current train perplexity2.6956684589385986
INFO:root:current mean train loss 1256.7518248739123
INFO:root:current train perplexity2.696570873260498
INFO:root:current mean train loss 1260.3372594393218
INFO:root:current train perplexity2.6961848735809326
INFO:root:current mean train loss 1259.4861365029979
INFO:root:current train perplexity2.700300455093384
INFO:root:current mean train loss 1260.8573366635806
INFO:root:current train perplexity2.7014589309692383
INFO:root:current mean train loss 1261.7271489830657
INFO:root:current train perplexity2.7017335891723633
INFO:root:current mean train loss 1261.2905572629443
INFO:root:current train perplexity2.7028558254241943
INFO:root:current mean train loss 1261.62179192289
INFO:root:current train perplexity2.707576036453247
INFO:root:current mean train loss 1262.8911764790694
INFO:root:current train perplexity2.71040678024292
INFO:root:current mean train loss 1262.9339333811115
INFO:root:current train perplexity2.7089955806732178
INFO:root:current mean train loss 1263.7716661754407
INFO:root:current train perplexity2.7104451656341553
INFO:root:current mean train loss 1264.1065037504156
INFO:root:current train perplexity2.7123491764068604
INFO:root:current mean train loss 1264.853977354233
INFO:root:current train perplexity2.7146482467651367
INFO:root:current mean train loss 1265.4397390561871
INFO:root:current train perplexity2.7151131629943848
INFO:root:current mean train loss 1266.0681894887791
INFO:root:current train perplexity2.7162210941314697
INFO:root:current mean train loss 1266.496639651852
INFO:root:current train perplexity2.717153310775757
INFO:root:current mean train loss 1266.5013410929541
INFO:root:current train perplexity2.717984437942505
INFO:root:current mean train loss 1267.2949968628693
INFO:root:current train perplexity2.718626022338867


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.18s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.29s/it]
INFO:root:eval mean loss: 3361.0088242539414
INFO:root:eval perplexity: 15.997123718261719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_64_low/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [14:05:57<13:33:10, 497.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1263.2331284031723
INFO:root:current train perplexity2.7012736797332764
INFO:root:current mean train loss 1252.1362736063793
INFO:root:current train perplexity2.6870357990264893
INFO:root:current mean train loss 1252.9430135964324
INFO:root:current train perplexity2.693664073944092
INFO:root:current mean train loss 1254.5786338095909
INFO:root:current train perplexity2.702023983001709
INFO:root:current mean train loss 1255.785555727212
INFO:root:current train perplexity2.701792001724243
INFO:root:current mean train loss 1255.8561945301508
INFO:root:current train perplexity2.7048282623291016
INFO:root:current mean train loss 1256.2909568075509
INFO:root:current train perplexity2.699859619140625
INFO:root:current mean train loss 1257.8324284364874
INFO:root:current train perplexity2.700998067855835
INFO:root:current mean train loss 1257.5603915394283
INFO:root:current train perplexity2.699826955795288
INFO:root:current mean train loss 1258.0450222264787
INFO:root:current train perplexity2.699477195739746
INFO:root:current mean train loss 1257.4114056686003
INFO:root:current train perplexity2.699802875518799
INFO:root:current mean train loss 1258.014049614202
INFO:root:current train perplexity2.700263261795044
INFO:root:current mean train loss 1259.4364699760492
INFO:root:current train perplexity2.702378273010254
INFO:root:current mean train loss 1259.8775738246086
INFO:root:current train perplexity2.7043817043304443
INFO:root:current mean train loss 1260.7289559470134
INFO:root:current train perplexity2.70522141456604
INFO:root:current mean train loss 1260.9783335945144
INFO:root:current train perplexity2.705878734588623
INFO:root:current mean train loss 1261.6232603672306
INFO:root:current train perplexity2.707122564315796
INFO:root:current mean train loss 1261.9455669951178
INFO:root:current train perplexity2.708099126815796
INFO:root:current mean train loss 1262.2556991452282
INFO:root:current train perplexity2.7089335918426514
INFO:root:current mean train loss 1263.2828755622938
INFO:root:current train perplexity2.709430694580078


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 25937075 ON ga003 CANCELLED AT 2022-10-16T04:12:17 ***
