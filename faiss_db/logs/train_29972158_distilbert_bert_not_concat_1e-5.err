INFO:root:Output: distilbert_bert_not_concat_1e-5
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models.py:436: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of the model checkpoint at bert-base-uncased were not used when initializing RetrievalGenerationModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing RetrievalGenerationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RetrievalGenerationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:450: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10366.397007181187
INFO:root:current train perplexity3425.111572265625
INFO:root:current mean train loss 8910.189806454146
INFO:root:current train perplexity1104.151123046875
INFO:root:current mean train loss 7876.727468841451
INFO:root:current train perplexity495.7410583496094
INFO:root:current mean train loss 7127.967655344416
INFO:root:current train perplexity274.03021240234375
INFO:root:current mean train loss 6559.374980429609
INFO:root:current train perplexity175.1759796142578
INFO:root:current mean train loss 6109.985736725924
INFO:root:current train perplexity124.17584228515625
INFO:root:current mean train loss 5752.3902764272625
INFO:root:current train perplexity94.3756103515625
INFO:root:current mean train loss 5473.4006054320835
INFO:root:current train perplexity75.55108642578125
INFO:root:current mean train loss 5240.137391155103
INFO:root:current train perplexity62.654273986816406
INFO:root:current mean train loss 5040.595018358202
INFO:root:current train perplexity53.590797424316406
INFO:root:current mean train loss 4872.727835852195
INFO:root:current train perplexity46.849308013916016
INFO:root:current mean train loss 4727.027032618328
INFO:root:current train perplexity41.63003921508789
INFO:root:current mean train loss 4597.093615431341
INFO:root:current train perplexity37.597816467285156
INFO:root:current mean train loss 4484.10681108716
INFO:root:current train perplexity34.33668518066406
INFO:root:current mean train loss 4382.265733633621
INFO:root:current train perplexity31.676376342773438
INFO:root:current mean train loss 4289.471687016299
INFO:root:current train perplexity29.417356491088867
INFO:root:current mean train loss 4205.968195187196
INFO:root:current train perplexity27.508901596069336
INFO:root:current mean train loss 4128.5075093259275
INFO:root:current train perplexity25.910905838012695
INFO:root:current mean train loss 4058.1724846444677
INFO:root:current train perplexity24.519227981567383

100%|██████████| 1/1 [18:24<00:00, 1104.67s/it][A100%|██████████| 1/1 [18:24<00:00, 1104.94s/it]
INFO:root:final mean train loss: 4003.849379639041
INFO:root:final train perplexity: 23.515892028808594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.43s/it][A100%|██████████| 1/1 [01:17<00:00, 77.43s/it]
INFO:root:eval mean loss: 2545.9005118295654
INFO:root:eval perplexity: 7.837929725646973
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.38s/it][A100%|██████████| 1/1 [01:16<00:00, 76.38s/it]
INFO:root:eval mean loss: 2864.070975662123
INFO:root:eval perplexity: 10.405308723449707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/1
  0%|          | 1/200 [21:01<69:44:18, 1261.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2691.586181640625
INFO:root:current train perplexity8.596308708190918
INFO:root:current mean train loss 2705.8309620824352
INFO:root:current train perplexity8.57009220123291
INFO:root:current mean train loss 2691.5478300871673
INFO:root:current train perplexity8.510246276855469
INFO:root:current mean train loss 2685.0372669847707
INFO:root:current train perplexity8.460668563842773
INFO:root:current mean train loss 2687.363335242638
INFO:root:current train perplexity8.426319122314453
INFO:root:current mean train loss 2675.900232595991
INFO:root:current train perplexity8.328346252441406
INFO:root:current mean train loss 2671.7088456587358
INFO:root:current train perplexity8.284723281860352
INFO:root:current mean train loss 2662.742801943305
INFO:root:current train perplexity8.228941917419434
INFO:root:current mean train loss 2653.404400694604
INFO:root:current train perplexity8.166305541992188
INFO:root:current mean train loss 2646.111621306973
INFO:root:current train perplexity8.107783317565918
INFO:root:current mean train loss 2637.8606252595196
INFO:root:current train perplexity8.051482200622559
INFO:root:current mean train loss 2629.661111592392
INFO:root:current train perplexity7.996172904968262
INFO:root:current mean train loss 2622.227268419768
INFO:root:current train perplexity7.940209865570068
INFO:root:current mean train loss 2614.448493192261
INFO:root:current train perplexity7.885982513427734
INFO:root:current mean train loss 2607.995469605182
INFO:root:current train perplexity7.846189975738525
INFO:root:current mean train loss 2600.2887964575775
INFO:root:current train perplexity7.79447603225708
INFO:root:current mean train loss 2594.0936788426766
INFO:root:current train perplexity7.747280120849609
INFO:root:current mean train loss 2587.434495334581
INFO:root:current train perplexity7.704150676727295
INFO:root:current mean train loss 2582.7411470791317
INFO:root:current train perplexity7.667267799377441
INFO:root:current mean train loss 2576.2494165344874
INFO:root:current train perplexity7.624209880828857

100%|██████████| 1/1 [18:23<00:00, 1103.88s/it][A100%|██████████| 1/1 [18:23<00:00, 1103.88s/it]
INFO:root:final mean train loss: 2571.008689091654
INFO:root:final train perplexity: 7.596231460571289
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:21<00:00, 81.86s/it][A100%|██████████| 1/1 [01:21<00:00, 81.86s/it]
INFO:root:eval mean loss: 2278.517589812583
INFO:root:eval perplexity: 6.313759803771973
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.84s/it][A100%|██████████| 1/1 [01:18<00:00, 78.84s/it]
INFO:root:eval mean loss: 2629.159575333832
INFO:root:eval perplexity: 8.586565971374512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/2
  1%|          | 2/200 [42:08<69:33:43, 1264.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2427.7106711647725
INFO:root:current train perplexity6.829216480255127
INFO:root:current mean train loss 2421.35733468192
INFO:root:current train perplexity6.759415626525879
INFO:root:current mean train loss 2406.678125209563
INFO:root:current train perplexity6.704919815063477
INFO:root:current mean train loss 2403.3934590107688
INFO:root:current train perplexity6.671160697937012
INFO:root:current mean train loss 2401.509112140048
INFO:root:current train perplexity6.649088382720947
INFO:root:current mean train loss 2399.689765057018
INFO:root:current train perplexity6.635676383972168
INFO:root:current mean train loss 2393.4979271188167
INFO:root:current train perplexity6.602725505828857
INFO:root:current mean train loss 2387.8672989120587
INFO:root:current train perplexity6.578244686126709
INFO:root:current mean train loss 2387.058103270605
INFO:root:current train perplexity6.564425468444824
INFO:root:current mean train loss 2387.5540023100466
INFO:root:current train perplexity6.557699203491211
INFO:root:current mean train loss 2383.9685495825265
INFO:root:current train perplexity6.533492565155029
INFO:root:current mean train loss 2377.604350616105
INFO:root:current train perplexity6.510879993438721
INFO:root:current mean train loss 2370.473099980041
INFO:root:current train perplexity6.490503311157227
INFO:root:current mean train loss 2368.3122543025893
INFO:root:current train perplexity6.474514007568359
INFO:root:current mean train loss 2363.9742716159008
INFO:root:current train perplexity6.453365802764893
INFO:root:current mean train loss 2360.5867052768776
INFO:root:current train perplexity6.435108661651611
INFO:root:current mean train loss 2356.834368974974
INFO:root:current train perplexity6.416522979736328
INFO:root:current mean train loss 2353.376290719219
INFO:root:current train perplexity6.399068355560303
INFO:root:current mean train loss 2351.1880687957114
INFO:root:current train perplexity6.386992931365967
INFO:root:current mean train loss 2347.830806315778
INFO:root:current train perplexity6.370956897735596

100%|██████████| 1/1 [18:02<00:00, 1082.09s/it][A100%|██████████| 1/1 [18:02<00:00, 1082.09s/it]
INFO:root:final mean train loss: 2346.243176435739
INFO:root:final train perplexity: 6.362290382385254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.72s/it][A100%|██████████| 1/1 [01:18<00:00, 78.72s/it]
INFO:root:eval mean loss: 2142.706802692819
INFO:root:eval perplexity: 5.657010078430176
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.10s/it][A100%|██████████| 1/1 [01:16<00:00, 76.10s/it]
INFO:root:eval mean loss: 2505.471139461436
INFO:root:eval perplexity: 7.760472297668457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/3
  2%|▏         | 3/200 [1:02:48<68:35:04, 1253.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2252.103981933594
INFO:root:current train perplexity5.95943546295166
INFO:root:current mean train loss 2240.3740096028646
INFO:root:current train perplexity5.867691516876221
INFO:root:current mean train loss 2247.39500390625
INFO:root:current train perplexity5.887259006500244
INFO:root:current mean train loss 2237.3842714146203
INFO:root:current train perplexity5.877358436584473
INFO:root:current mean train loss 2240.8294067382812
INFO:root:current train perplexity5.876136779785156
INFO:root:current mean train loss 2239.4572201260653
INFO:root:current train perplexity5.861023426055908
INFO:root:current mean train loss 2243.2795166015626
INFO:root:current train perplexity5.86891508102417
INFO:root:current mean train loss 2244.9171248372395
INFO:root:current train perplexity5.8662800788879395
INFO:root:current mean train loss 2239.7453926355697
INFO:root:current train perplexity5.846593856811523
INFO:root:current mean train loss 2238.1040226665295
INFO:root:current train perplexity5.843174934387207
INFO:root:current mean train loss 2233.840321800595
INFO:root:current train perplexity5.829307556152344
INFO:root:current mean train loss 2232.7257689368207
INFO:root:current train perplexity5.822147369384766
INFO:root:current mean train loss 2230.570116113281
INFO:root:current train perplexity5.813274383544922
INFO:root:current mean train loss 2230.1153965024596
INFO:root:current train perplexity5.806212902069092
INFO:root:current mean train loss 2227.6381345972522
INFO:root:current train perplexity5.791111946105957
INFO:root:current mean train loss 2225.276867203251
INFO:root:current train perplexity5.784058570861816
INFO:root:current mean train loss 2223.763240116004
INFO:root:current train perplexity5.776744842529297
INFO:root:current mean train loss 2221.7555875418525
INFO:root:current train perplexity5.7676825523376465
INFO:root:current mean train loss 2220.082461135452
INFO:root:current train perplexity5.757501602172852
INFO:root:current mean train loss 2219.5015777744393
INFO:root:current train perplexity5.751733303070068

100%|██████████| 1/1 [18:00<00:00, 1080.56s/it][A100%|██████████| 1/1 [18:00<00:00, 1080.56s/it]
INFO:root:final mean train loss: 2217.522371800933
INFO:root:final train perplexity: 5.748112678527832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.59s/it][A100%|██████████| 1/1 [01:16<00:00, 76.61s/it]
INFO:root:eval mean loss: 2059.4308004176364
INFO:root:eval perplexity: 5.288564205169678
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.48s/it][A100%|██████████| 1/1 [01:13<00:00, 73.51s/it]
INFO:root:eval mean loss: 2434.6535930227724
INFO:root:eval perplexity: 7.323780536651611
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/4
  2%|▏         | 4/200 [1:23:21<67:48:36, 1245.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2127.0709847976914
INFO:root:current train perplexity5.3973870277404785
INFO:root:current mean train loss 2137.4708113070733
INFO:root:current train perplexity5.399937629699707
INFO:root:current mean train loss 2147.819116942445
INFO:root:current train perplexity5.412526607513428
INFO:root:current mean train loss 2146.0859647745656
INFO:root:current train perplexity5.416544437408447
INFO:root:current mean train loss 2140.9215474751572
INFO:root:current train perplexity5.409990310668945
INFO:root:current mean train loss 2143.841240561618
INFO:root:current train perplexity5.415627479553223
INFO:root:current mean train loss 2139.8974290930705
INFO:root:current train perplexity5.405832767486572
INFO:root:current mean train loss 2139.408212196718
INFO:root:current train perplexity5.398797035217285
INFO:root:current mean train loss 2137.2778272441788
INFO:root:current train perplexity5.3971076011657715
INFO:root:current mean train loss 2135.431084050018
INFO:root:current train perplexity5.388506889343262
INFO:root:current mean train loss 2135.713237043844
INFO:root:current train perplexity5.393090724945068
INFO:root:current mean train loss 2133.8491367840215
INFO:root:current train perplexity5.392043113708496
INFO:root:current mean train loss 2135.457495155726
INFO:root:current train perplexity5.391341209411621
INFO:root:current mean train loss 2136.0985018123915
INFO:root:current train perplexity5.393181800842285
INFO:root:current mean train loss 2136.0730129416165
INFO:root:current train perplexity5.388153553009033
INFO:root:current mean train loss 2135.583397004128
INFO:root:current train perplexity5.384093284606934
INFO:root:current mean train loss 2135.120974828472
INFO:root:current train perplexity5.3840789794921875
INFO:root:current mean train loss 2134.7629777253114
INFO:root:current train perplexity5.381470203399658
INFO:root:current mean train loss 2133.4797204400234
INFO:root:current train perplexity5.378502368927002
INFO:root:current mean train loss 2133.0035039206477
INFO:root:current train perplexity5.373445510864258

100%|██████████| 1/1 [18:00<00:00, 1080.86s/it][A100%|██████████| 1/1 [18:00<00:00, 1080.86s/it]
INFO:root:final mean train loss: 2131.5275725318033
INFO:root:final train perplexity: 5.3711981773376465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.80s/it][A100%|██████████| 1/1 [01:14<00:00, 74.80s/it]
INFO:root:eval mean loss: 2010.4826976153868
INFO:root:eval perplexity: 5.083298206329346
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.14s/it][A100%|██████████| 1/1 [01:15<00:00, 75.14s/it]
INFO:root:eval mean loss: 2393.685432163536
INFO:root:eval perplexity: 7.08246374130249
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/5
  2%|▎         | 5/200 [1:43:55<67:13:59, 1241.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2088.165309361049
INFO:root:current train perplexity5.177873134613037
INFO:root:current mean train loss 2070.611715565557
INFO:root:current train perplexity5.126614570617676
INFO:root:current mean train loss 2076.0608421648053
INFO:root:current train perplexity5.122663497924805
INFO:root:current mean train loss 2080.277004559835
INFO:root:current train perplexity5.133372783660889
INFO:root:current mean train loss 2083.1547647271273
INFO:root:current train perplexity5.1488213539123535
INFO:root:current mean train loss 2080.2001190185547
INFO:root:current train perplexity5.1384429931640625
INFO:root:current mean train loss 2080.2548008968956
INFO:root:current train perplexity5.1380085945129395
INFO:root:current mean train loss 2081.198149544852
INFO:root:current train perplexity5.147341728210449
INFO:root:current mean train loss 2085.341104636904
INFO:root:current train perplexity5.154812812805176
INFO:root:current mean train loss 2084.7428170801177
INFO:root:current train perplexity5.15418815612793
INFO:root:current mean train loss 2082.999512281805
INFO:root:current train perplexity5.146630764007568
INFO:root:current mean train loss 2081.707290546314
INFO:root:current train perplexity5.149013042449951
INFO:root:current mean train loss 2081.282081770377
INFO:root:current train perplexity5.150062084197998
INFO:root:current mean train loss 2078.556785980401
INFO:root:current train perplexity5.145175457000732
INFO:root:current mean train loss 2078.9678263291516
INFO:root:current train perplexity5.1465582847595215
INFO:root:current mean train loss 2076.7702462552775
INFO:root:current train perplexity5.144174098968506
INFO:root:current mean train loss 2076.392720999457
INFO:root:current train perplexity5.142218112945557
INFO:root:current mean train loss 2074.476387947664
INFO:root:current train perplexity5.135466575622559
INFO:root:current mean train loss 2074.04219090812
INFO:root:current train perplexity5.130537509918213

100%|██████████| 1/1 [17:50<00:00, 1070.28s/it][A100%|██████████| 1/1 [17:50<00:00, 1070.28s/it]
INFO:root:final mean train loss: 2072.347723071646
INFO:root:final train perplexity: 5.1262688636779785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.89s/it][A100%|██████████| 1/1 [01:14<00:00, 74.89s/it]
INFO:root:eval mean loss: 1974.4576454974235
INFO:root:eval perplexity: 4.937334060668945
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.88s/it][A100%|██████████| 1/1 [01:12<00:00, 72.88s/it]
INFO:root:eval mean loss: 2359.906759492049
INFO:root:eval perplexity: 6.889485836029053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/6
  3%|▎         | 6/200 [2:04:16<66:30:40, 1234.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2059.84716796875
INFO:root:current train perplexity4.783791542053223
INFO:root:current mean train loss 2030.491673837794
INFO:root:current train perplexity4.942225456237793
INFO:root:current mean train loss 2028.7719629392102
INFO:root:current train perplexity4.927911758422852
INFO:root:current mean train loss 2033.4207418955045
INFO:root:current train perplexity4.960903644561768
INFO:root:current mean train loss 2032.1807173350803
INFO:root:current train perplexity4.953301906585693
INFO:root:current mean train loss 2028.468575544224
INFO:root:current train perplexity4.946917533874512
INFO:root:current mean train loss 2028.2200996792455
INFO:root:current train perplexity4.954428672790527
INFO:root:current mean train loss 2028.2273751992132
INFO:root:current train perplexity4.949370384216309
INFO:root:current mean train loss 2025.616585835089
INFO:root:current train perplexity4.9407453536987305
INFO:root:current mean train loss 2026.2127405096767
INFO:root:current train perplexity4.937074661254883
INFO:root:current mean train loss 2024.4286481340926
INFO:root:current train perplexity4.935161113739014
INFO:root:current mean train loss 2025.6094782220355
INFO:root:current train perplexity4.939770698547363
INFO:root:current mean train loss 2023.3619673424814
INFO:root:current train perplexity4.931329727172852
INFO:root:current mean train loss 2022.104492093672
INFO:root:current train perplexity4.930403709411621
INFO:root:current mean train loss 2023.295873343817
INFO:root:current train perplexity4.933892250061035
INFO:root:current mean train loss 2021.9361964256902
INFO:root:current train perplexity4.933467388153076
INFO:root:current mean train loss 2022.232381311973
INFO:root:current train perplexity4.930056095123291
INFO:root:current mean train loss 2021.372408176716
INFO:root:current train perplexity4.927560329437256
INFO:root:current mean train loss 2020.9551808104654
INFO:root:current train perplexity4.92576789855957
INFO:root:current mean train loss 2021.5800164798134
INFO:root:current train perplexity4.92692756652832

100%|██████████| 1/1 [17:43<00:00, 1063.37s/it][A100%|██████████| 1/1 [17:43<00:00, 1063.37s/it]
INFO:root:final mean train loss: 2021.4791768853615
INFO:root:final train perplexity: 4.924683570861816
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.61s/it][A100%|██████████| 1/1 [01:15<00:00, 75.62s/it]
INFO:root:eval mean loss: 1940.74402764503
INFO:root:eval perplexity: 4.804533958435059
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.00s/it][A100%|██████████| 1/1 [01:13<00:00, 73.00s/it]
INFO:root:eval mean loss: 2335.980091284353
INFO:root:eval perplexity: 6.755984306335449
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/7
  4%|▎         | 7/200 [2:24:30<65:49:32, 1227.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2011.1681586371528
INFO:root:current train perplexity4.852962017059326
INFO:root:current mean train loss 1980.01049183991
INFO:root:current train perplexity4.789581775665283
INFO:root:current mean train loss 1985.7116592827192
INFO:root:current train perplexity4.798426628112793
INFO:root:current mean train loss 1990.2653217435634
INFO:root:current train perplexity4.791702747344971
INFO:root:current mean train loss 1986.490295118122
INFO:root:current train perplexity4.774816513061523
INFO:root:current mean train loss 1985.6866643603705
INFO:root:current train perplexity4.778241157531738
INFO:root:current mean train loss 1986.2616634430624
INFO:root:current train perplexity4.777520656585693
INFO:root:current mean train loss 1987.0085546126936
INFO:root:current train perplexity4.780611038208008
INFO:root:current mean train loss 1986.949781347895
INFO:root:current train perplexity4.781365871429443
INFO:root:current mean train loss 1984.2370400688487
INFO:root:current train perplexity4.779080867767334
INFO:root:current mean train loss 1981.1881965682173
INFO:root:current train perplexity4.770633220672607
INFO:root:current mean train loss 1979.4817036473473
INFO:root:current train perplexity4.765827178955078
INFO:root:current mean train loss 1979.8267466477769
INFO:root:current train perplexity4.768317222595215
INFO:root:current mean train loss 1979.651105079014
INFO:root:current train perplexity4.770506381988525
INFO:root:current mean train loss 1979.5844159254066
INFO:root:current train perplexity4.767807960510254
INFO:root:current mean train loss 1979.3634042852952
INFO:root:current train perplexity4.765376091003418
INFO:root:current mean train loss 1978.6616463678877
INFO:root:current train perplexity4.760993003845215
INFO:root:current mean train loss 1979.4009935869742
INFO:root:current train perplexity4.7617268562316895
INFO:root:current mean train loss 1979.8168335632392
INFO:root:current train perplexity4.762718200683594
INFO:root:current mean train loss 1980.1906120928784
INFO:root:current train perplexity4.762248516082764

100%|██████████| 1/1 [17:46<00:00, 1066.52s/it][A100%|██████████| 1/1 [17:46<00:00, 1066.52s/it]
INFO:root:final mean train loss: 1978.524589873298
INFO:root:final train perplexity: 4.760646820068359
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.19s/it][A100%|██████████| 1/1 [01:16<00:00, 76.19s/it]
INFO:root:eval mean loss: 1916.2413027066711
INFO:root:eval perplexity: 4.71026086807251
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.68s/it][A100%|██████████| 1/1 [01:13<00:00, 73.68s/it]
INFO:root:eval mean loss: 2313.1112701199577
INFO:root:eval perplexity: 6.630803108215332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/8
  4%|▍         | 8/200 [2:44:49<65:20:06, 1225.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1931.3598388671876
INFO:root:current train perplexity4.5974040031433105
INFO:root:current mean train loss 1931.1911313657408
INFO:root:current train perplexity4.600516319274902
INFO:root:current mean train loss 1944.368092378657
INFO:root:current train perplexity4.633704662322998
INFO:root:current mean train loss 1946.164595236707
INFO:root:current train perplexity4.6249213218688965
INFO:root:current mean train loss 1944.476235014817
INFO:root:current train perplexity4.634761333465576
INFO:root:current mean train loss 1945.1149315949913
INFO:root:current train perplexity4.636173248291016
INFO:root:current mean train loss 1945.4763081708293
INFO:root:current train perplexity4.641513824462891
INFO:root:current mean train loss 1946.2276182836415
INFO:root:current train perplexity4.644416332244873
INFO:root:current mean train loss 1945.365021811845
INFO:root:current train perplexity4.6438446044921875
INFO:root:current mean train loss 1945.2903530508438
INFO:root:current train perplexity4.641116619110107
INFO:root:current mean train loss 1944.0399568566954
INFO:root:current train perplexity4.639083385467529
INFO:root:current mean train loss 1946.298356729247
INFO:root:current train perplexity4.644802570343018
INFO:root:current mean train loss 1947.3716996536564
INFO:root:current train perplexity4.6454668045043945
INFO:root:current mean train loss 1946.530038074965
INFO:root:current train perplexity4.644630432128906
INFO:root:current mean train loss 1946.9435031372495
INFO:root:current train perplexity4.642735481262207
INFO:root:current mean train loss 1946.9670267807155
INFO:root:current train perplexity4.642504692077637
INFO:root:current mean train loss 1947.1540142840931
INFO:root:current train perplexity4.6397786140441895
INFO:root:current mean train loss 1946.0521114997973
INFO:root:current train perplexity4.638976097106934
INFO:root:current mean train loss 1945.820929570419
INFO:root:current train perplexity4.637258052825928
INFO:root:current mean train loss 1944.7030558583656
INFO:root:current train perplexity4.631934642791748

100%|██████████| 1/1 [17:48<00:00, 1068.69s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.69s/it]
INFO:root:final mean train loss: 1943.8606748056725
INFO:root:final train perplexity: 4.63226318359375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.26s/it][A100%|██████████| 1/1 [01:16<00:00, 76.29s/it]
INFO:root:eval mean loss: 1897.0572323630042
INFO:root:eval perplexity: 4.6377458572387695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.56s/it][A100%|██████████| 1/1 [01:13<00:00, 73.56s/it]
INFO:root:eval mean loss: 2304.19712883699
INFO:root:eval perplexity: 6.582639217376709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/9
  4%|▍         | 9/200 [3:05:10<64:55:47, 1223.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1902.3827984149639
INFO:root:current train perplexity4.475509166717529
INFO:root:current mean train loss 1915.403648778012
INFO:root:current train perplexity4.4933695793151855
INFO:root:current mean train loss 1910.8744618249318
INFO:root:current train perplexity4.512526988983154
INFO:root:current mean train loss 1908.98756720803
INFO:root:current train perplexity4.500429153442383
INFO:root:current mean train loss 1908.8197569720512
INFO:root:current train perplexity4.509779453277588
INFO:root:current mean train loss 1908.0524146038554
INFO:root:current train perplexity4.510125637054443
INFO:root:current mean train loss 1913.0697257387126
INFO:root:current train perplexity4.5145111083984375
INFO:root:current mean train loss 1912.2989839594416
INFO:root:current train perplexity4.514711856842041
INFO:root:current mean train loss 1913.4326328044765
INFO:root:current train perplexity4.517270565032959
INFO:root:current mean train loss 1912.567582459009
INFO:root:current train perplexity4.523107528686523
INFO:root:current mean train loss 1913.7116019245336
INFO:root:current train perplexity4.525256633758545
INFO:root:current mean train loss 1914.1869133843315
INFO:root:current train perplexity4.522181987762451
INFO:root:current mean train loss 1913.949892574225
INFO:root:current train perplexity4.521266460418701
INFO:root:current mean train loss 1913.4014114289594
INFO:root:current train perplexity4.520465850830078
INFO:root:current mean train loss 1912.2272317008867
INFO:root:current train perplexity4.516943454742432
INFO:root:current mean train loss 1911.877283233957
INFO:root:current train perplexity4.516881942749023
INFO:root:current mean train loss 1911.7129020044358
INFO:root:current train perplexity4.517497539520264
INFO:root:current mean train loss 1911.6632132334253
INFO:root:current train perplexity4.515045642852783
INFO:root:current mean train loss 1911.0751951806747
INFO:root:current train perplexity4.514285564422607
INFO:root:current mean train loss 1911.8437550654178
INFO:root:current train perplexity4.514054298400879

100%|██████████| 1/1 [17:49<00:00, 1069.08s/it][A100%|██████████| 1/1 [17:49<00:00, 1069.08s/it]
INFO:root:final mean train loss: 1910.9117729583295
INFO:root:final train perplexity: 4.513442039489746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.44s/it][A100%|██████████| 1/1 [01:15<00:00, 75.44s/it]
INFO:root:eval mean loss: 1870.8645707800033
INFO:root:eval perplexity: 4.5405378341674805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.30s/it][A100%|██████████| 1/1 [01:14<00:00, 74.34s/it]
INFO:root:eval mean loss: 2281.114614067348
INFO:root:eval perplexity: 6.459540367126465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/10
  5%|▌         | 10/200 [3:25:32<64:33:08, 1223.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1882.3227715975997
INFO:root:current train perplexity4.375171661376953
INFO:root:current mean train loss 1875.3883612818972
INFO:root:current train perplexity4.391430377960205
INFO:root:current mean train loss 1875.056504940898
INFO:root:current train perplexity4.412432670593262
INFO:root:current mean train loss 1877.5277830707994
INFO:root:current train perplexity4.405702114105225
INFO:root:current mean train loss 1878.7595873346716
INFO:root:current train perplexity4.40822696685791
INFO:root:current mean train loss 1880.1696824541411
INFO:root:current train perplexity4.404404163360596
INFO:root:current mean train loss 1879.7512756256424
INFO:root:current train perplexity4.399317264556885
INFO:root:current mean train loss 1879.2856459599013
INFO:root:current train perplexity4.401030540466309
INFO:root:current mean train loss 1881.546948466943
INFO:root:current train perplexity4.406253337860107
INFO:root:current mean train loss 1881.7354289114905
INFO:root:current train perplexity4.407732963562012
INFO:root:current mean train loss 1881.393395733454
INFO:root:current train perplexity4.406030654907227
INFO:root:current mean train loss 1880.7519362084981
INFO:root:current train perplexity4.405922889709473
INFO:root:current mean train loss 1880.61288752755
INFO:root:current train perplexity4.402151107788086
INFO:root:current mean train loss 1881.0155670411227
INFO:root:current train perplexity4.400048732757568
INFO:root:current mean train loss 1880.7377870688233
INFO:root:current train perplexity4.398345947265625
INFO:root:current mean train loss 1880.3073275330873
INFO:root:current train perplexity4.400808811187744
INFO:root:current mean train loss 1880.843108564026
INFO:root:current train perplexity4.4020280838012695
INFO:root:current mean train loss 1879.8789050769105
INFO:root:current train perplexity4.400932312011719
INFO:root:current mean train loss 1880.1460071969886
INFO:root:current train perplexity4.403015613555908
INFO:root:current mean train loss 1880.6789482585546
INFO:root:current train perplexity4.404703140258789

100%|██████████| 1/1 [17:52<00:00, 1072.88s/it][A100%|██████████| 1/1 [17:52<00:00, 1072.88s/it]
INFO:root:final mean train loss: 1879.7167087541466
INFO:root:final train perplexity: 4.403755187988281
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.31s/it][A100%|██████████| 1/1 [01:16<00:00, 76.35s/it]
INFO:root:eval mean loss: 1856.5431111653645
INFO:root:eval perplexity: 4.488250732421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.44s/it][A100%|██████████| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 2266.0155561731217
INFO:root:eval perplexity: 6.380265712738037
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/11
  6%|▌         | 11/200 [3:45:58<64:15:55, 1224.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1835.7757880632269
INFO:root:current train perplexity4.282320022583008
INFO:root:current mean train loss 1843.7710886309223
INFO:root:current train perplexity4.295721054077148
INFO:root:current mean train loss 1847.225981172148
INFO:root:current train perplexity4.2915425300598145
INFO:root:current mean train loss 1848.6852755373623
INFO:root:current train perplexity4.295084476470947
INFO:root:current mean train loss 1846.921027038323
INFO:root:current train perplexity4.294604301452637
INFO:root:current mean train loss 1845.405996485375
INFO:root:current train perplexity4.294599533081055
INFO:root:current mean train loss 1847.4936792134542
INFO:root:current train perplexity4.303411483764648
INFO:root:current mean train loss 1849.0055580818623
INFO:root:current train perplexity4.30470609664917
INFO:root:current mean train loss 1849.0177905769435
INFO:root:current train perplexity4.301612854003906
INFO:root:current mean train loss 1850.7969243976213
INFO:root:current train perplexity4.305229187011719
INFO:root:current mean train loss 1851.553612247137
INFO:root:current train perplexity4.307212829589844
INFO:root:current mean train loss 1850.4277577392165
INFO:root:current train perplexity4.307803153991699
INFO:root:current mean train loss 1851.6952618113944
INFO:root:current train perplexity4.310694217681885
INFO:root:current mean train loss 1851.1081558822038
INFO:root:current train perplexity4.308332920074463
INFO:root:current mean train loss 1851.5530277216258
INFO:root:current train perplexity4.310046195983887
INFO:root:current mean train loss 1852.3863736281328
INFO:root:current train perplexity4.311974048614502
INFO:root:current mean train loss 1851.8413472565892
INFO:root:current train perplexity4.310080528259277
INFO:root:current mean train loss 1851.7238969108694
INFO:root:current train perplexity4.309454917907715
INFO:root:current mean train loss 1852.36029628782
INFO:root:current train perplexity4.307768821716309

100%|██████████| 1/1 [17:52<00:00, 1072.20s/it][A100%|██████████| 1/1 [17:52<00:00, 1072.21s/it]
INFO:root:final mean train loss: 1851.7267701980506
INFO:root:final train perplexity: 4.307608604431152
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.27s/it][A100%|██████████| 1/1 [01:15<00:00, 75.27s/it]
INFO:root:eval mean loss: 1840.0839774490248
INFO:root:eval perplexity: 4.428902626037598
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.45s/it][A100%|██████████| 1/1 [01:12<00:00, 72.45s/it]
INFO:root:eval mean loss: 2256.240327875665
INFO:root:eval perplexity: 6.32946252822876
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/12
  6%|▌         | 12/200 [4:06:21<63:54:08, 1223.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1926.9418538411458
INFO:root:current train perplexity4.348477840423584
INFO:root:current mean train loss 1830.8587717593296
INFO:root:current train perplexity4.235527992248535
INFO:root:current mean train loss 1830.1008950219366
INFO:root:current train perplexity4.220846176147461
INFO:root:current mean train loss 1834.2830367387326
INFO:root:current train perplexity4.219054698944092
INFO:root:current mean train loss 1832.112544587469
INFO:root:current train perplexity4.221977710723877
INFO:root:current mean train loss 1827.296881067113
INFO:root:current train perplexity4.2146501541137695
INFO:root:current mean train loss 1825.3459790484426
INFO:root:current train perplexity4.208845615386963
INFO:root:current mean train loss 1823.8557050767358
INFO:root:current train perplexity4.207971572875977
INFO:root:current mean train loss 1822.8037317639419
INFO:root:current train perplexity4.209901809692383
INFO:root:current mean train loss 1823.8697524635763
INFO:root:current train perplexity4.212337017059326
INFO:root:current mean train loss 1825.9678431745779
INFO:root:current train perplexity4.214738845825195
INFO:root:current mean train loss 1826.2433472233045
INFO:root:current train perplexity4.21747350692749
INFO:root:current mean train loss 1826.357622179902
INFO:root:current train perplexity4.217347621917725
INFO:root:current mean train loss 1827.3311686073005
INFO:root:current train perplexity4.2200703620910645
INFO:root:current mean train loss 1828.0874807367304
INFO:root:current train perplexity4.2210774421691895
INFO:root:current mean train loss 1828.637181691305
INFO:root:current train perplexity4.223200798034668
INFO:root:current mean train loss 1827.1005914203836
INFO:root:current train perplexity4.222263336181641
INFO:root:current mean train loss 1827.9850540922728
INFO:root:current train perplexity4.224298477172852
INFO:root:current mean train loss 1826.7573800068462
INFO:root:current train perplexity4.222670555114746
INFO:root:current mean train loss 1826.399751600064
INFO:root:current train perplexity4.222414016723633

100%|██████████| 1/1 [17:43<00:00, 1063.96s/it][A100%|██████████| 1/1 [17:43<00:00, 1063.96s/it]
INFO:root:final mean train loss: 1825.887803409055
INFO:root:final train perplexity: 4.22071647644043
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.24s/it][A100%|██████████| 1/1 [01:15<00:00, 75.26s/it]
INFO:root:eval mean loss: 1829.254440848709
INFO:root:eval perplexity: 4.390282154083252
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.86s/it][A100%|██████████| 1/1 [01:12<00:00, 72.86s/it]
INFO:root:eval mean loss: 2246.4148377763463
INFO:root:eval perplexity: 6.278805732727051
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/13
  6%|▋         | 13/200 [4:26:36<63:25:16, 1220.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1776.9680786132812
INFO:root:current train perplexity4.111659526824951
INFO:root:current mean train loss 1781.1280822753906
INFO:root:current train perplexity4.093837261199951
INFO:root:current mean train loss 1788.9127086292613
INFO:root:current train perplexity4.110358238220215
INFO:root:current mean train loss 1794.686157989502
INFO:root:current train perplexity4.1186323165893555
INFO:root:current mean train loss 1794.5572687058223
INFO:root:current train perplexity4.110538005828857
INFO:root:current mean train loss 1797.878842632587
INFO:root:current train perplexity4.118722915649414
INFO:root:current mean train loss 1800.7386078865297
INFO:root:current train perplexity4.123330593109131
INFO:root:current mean train loss 1803.3697146945528
INFO:root:current train perplexity4.129783630371094
INFO:root:current mean train loss 1802.1724828208364
INFO:root:current train perplexity4.1311140060424805
INFO:root:current mean train loss 1801.083117543096
INFO:root:current train perplexity4.132087707519531
INFO:root:current mean train loss 1802.4513540230546
INFO:root:current train perplexity4.134398460388184
INFO:root:current mean train loss 1803.1939279828753
INFO:root:current train perplexity4.138899326324463
INFO:root:current mean train loss 1803.5960909483863
INFO:root:current train perplexity4.138848781585693
INFO:root:current mean train loss 1803.234219267874
INFO:root:current train perplexity4.140224456787109
INFO:root:current mean train loss 1803.685331790548
INFO:root:current train perplexity4.139673233032227
INFO:root:current mean train loss 1804.7673527767784
INFO:root:current train perplexity4.141695976257324
INFO:root:current mean train loss 1803.0052898642457
INFO:root:current train perplexity4.139272212982178
INFO:root:current mean train loss 1801.8337222077125
INFO:root:current train perplexity4.1398186683654785
INFO:root:current mean train loss 1802.2356156904618
INFO:root:current train perplexity4.1388349533081055
INFO:root:current mean train loss 1802.2763692855835
INFO:root:current train perplexity4.140227794647217

100%|██████████| 1/1 [17:39<00:00, 1059.69s/it][A100%|██████████| 1/1 [17:39<00:00, 1059.70s/it]
INFO:root:final mean train loss: 1801.7036856431523
INFO:root:final train perplexity: 4.14097785949707
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.93s/it][A100%|██████████| 1/1 [01:14<00:00, 74.93s/it]
INFO:root:eval mean loss: 1833.6680289194094
INFO:root:eval perplexity: 4.405981540679932
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.38s/it][A100%|██████████| 1/1 [01:12<00:00, 72.38s/it]
INFO:root:eval mean loss: 2250.1162109375
INFO:root:eval perplexity: 6.297841548919678
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/14
  7%|▋         | 14/200 [4:46:45<62:54:21, 1217.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1781.1537261138092
INFO:root:current train perplexity4.1234211921691895
INFO:root:current mean train loss 1768.6550034571737
INFO:root:current train perplexity4.0625152587890625
INFO:root:current mean train loss 1766.2322647802941
INFO:root:current train perplexity4.05888557434082
INFO:root:current mean train loss 1764.8328263370502
INFO:root:current train perplexity4.058295726776123
INFO:root:current mean train loss 1769.5239782966248
INFO:root:current train perplexity4.063157081604004
INFO:root:current mean train loss 1766.692602857309
INFO:root:current train perplexity4.057008743286133
INFO:root:current mean train loss 1769.6319183397518
INFO:root:current train perplexity4.056254863739014
INFO:root:current mean train loss 1771.2586476133183
INFO:root:current train perplexity4.058509826660156
INFO:root:current mean train loss 1774.7396909652218
INFO:root:current train perplexity4.065169334411621
INFO:root:current mean train loss 1774.2401651974803
INFO:root:current train perplexity4.06012487411499
INFO:root:current mean train loss 1774.7271548411886
INFO:root:current train perplexity4.0636420249938965
INFO:root:current mean train loss 1773.1012288196735
INFO:root:current train perplexity4.059982776641846
INFO:root:current mean train loss 1773.1808184020058
INFO:root:current train perplexity4.060604095458984
INFO:root:current mean train loss 1775.1121317621657
INFO:root:current train perplexity4.063011646270752
INFO:root:current mean train loss 1775.895106178236
INFO:root:current train perplexity4.0659499168396
INFO:root:current mean train loss 1778.8228610453857
INFO:root:current train perplexity4.069005012512207
INFO:root:current mean train loss 1778.9782872931144
INFO:root:current train perplexity4.068594932556152
INFO:root:current mean train loss 1778.6893727989395
INFO:root:current train perplexity4.067203521728516
INFO:root:current mean train loss 1780.1504861149506
INFO:root:current train perplexity4.0676093101501465
INFO:root:current mean train loss 1779.6962569221494
INFO:root:current train perplexity4.067856788635254

100%|██████████| 1/1 [17:51<00:00, 1071.33s/it][A100%|██████████| 1/1 [17:51<00:00, 1071.33s/it]
INFO:root:final mean train loss: 1779.1324636177528
INFO:root:final train perplexity: 4.067915916442871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.79s/it][A100%|██████████| 1/1 [01:15<00:00, 75.79s/it]
INFO:root:eval mean loss: 1815.8442136074634
INFO:root:eval perplexity: 4.342925071716309
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.65s/it][A100%|██████████| 1/1 [01:13<00:00, 73.65s/it]
INFO:root:eval mean loss: 2237.97334408591
INFO:root:eval perplexity: 6.235608100891113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/15
  8%|▊         | 15/200 [5:07:09<62:39:29, 1219.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1760.7778930664062
INFO:root:current train perplexity4.014794826507568
INFO:root:current mean train loss 1749.7050020292209
INFO:root:current train perplexity4.020940780639648
INFO:root:current mean train loss 1760.676284910187
INFO:root:current train perplexity4.022536277770996
INFO:root:current mean train loss 1759.0199188404838
INFO:root:current train perplexity4.017437934875488
INFO:root:current mean train loss 1759.0054052411722
INFO:root:current train perplexity4.0130743980407715
INFO:root:current mean train loss 1761.1285929215082
INFO:root:current train perplexity4.022791385650635
INFO:root:current mean train loss 1762.7573223522315
INFO:root:current train perplexity4.0169548988342285
INFO:root:current mean train loss 1760.6821922079637
INFO:root:current train perplexity4.013495922088623
INFO:root:current mean train loss 1765.3413576219903
INFO:root:current train perplexity4.025148868560791
INFO:root:current mean train loss 1762.7958084842194
INFO:root:current train perplexity4.021146297454834
INFO:root:current mean train loss 1762.7203299650885
INFO:root:current train perplexity4.017670631408691
INFO:root:current mean train loss 1764.053991254942
INFO:root:current train perplexity4.017480373382568
INFO:root:current mean train loss 1763.698357930404
INFO:root:current train perplexity4.016397476196289
INFO:root:current mean train loss 1763.3352306822378
INFO:root:current train perplexity4.015939235687256
INFO:root:current mean train loss 1762.5663058400319
INFO:root:current train perplexity4.015066146850586
INFO:root:current mean train loss 1761.1107275139257
INFO:root:current train perplexity4.0145392417907715
INFO:root:current mean train loss 1759.74949548207
INFO:root:current train perplexity4.012826442718506
INFO:root:current mean train loss 1760.1386814095683
INFO:root:current train perplexity4.011780261993408
INFO:root:current mean train loss 1760.460279413283
INFO:root:current train perplexity4.009889602661133
INFO:root:current mean train loss 1760.8638038908568
INFO:root:current train perplexity4.008636951446533

100%|██████████| 1/1 [17:50<00:00, 1070.07s/it][A100%|██████████| 1/1 [17:50<00:00, 1070.08s/it]
INFO:root:final mean train loss: 1760.5480971189684
INFO:root:final train perplexity: 4.00872802734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.06s/it][A100%|██████████| 1/1 [01:16<00:00, 76.06s/it]
INFO:root:eval mean loss: 1797.2393422228224
INFO:root:eval perplexity: 4.278069019317627
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.53s/it][A100%|██████████| 1/1 [01:13<00:00, 73.53s/it]
INFO:root:eval mean loss: 2222.6435542546265
INFO:root:eval perplexity: 6.157919883728027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/16
  8%|▊         | 16/200 [5:27:31<62:21:55, 1220.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1728.8976534991198
INFO:root:current train perplexity3.918102979660034
INFO:root:current mean train loss 1732.7591652674982
INFO:root:current train perplexity3.9417126178741455
INFO:root:current mean train loss 1729.7277814013491
INFO:root:current train perplexity3.9317123889923096
INFO:root:current mean train loss 1733.6028756606931
INFO:root:current train perplexity3.93101167678833
INFO:root:current mean train loss 1735.9535098713675
INFO:root:current train perplexity3.9319238662719727
INFO:root:current mean train loss 1735.7646159424255
INFO:root:current train perplexity3.9313342571258545
INFO:root:current mean train loss 1735.0513070073816
INFO:root:current train perplexity3.929140567779541
INFO:root:current mean train loss 1733.9931928780602
INFO:root:current train perplexity3.9287662506103516
INFO:root:current mean train loss 1736.3058890026282
INFO:root:current train perplexity3.930708169937134
INFO:root:current mean train loss 1738.787551392733
INFO:root:current train perplexity3.9360666275024414
INFO:root:current mean train loss 1736.4443434600403
INFO:root:current train perplexity3.9304161071777344
INFO:root:current mean train loss 1737.6829110527526
INFO:root:current train perplexity3.9341208934783936
INFO:root:current mean train loss 1738.5878192652501
INFO:root:current train perplexity3.9343066215515137
INFO:root:current mean train loss 1737.4116950838518
INFO:root:current train perplexity3.9321844577789307
INFO:root:current mean train loss 1737.9075357630331
INFO:root:current train perplexity3.9342896938323975
INFO:root:current mean train loss 1739.3900214085375
INFO:root:current train perplexity3.935652017593384
INFO:root:current mean train loss 1740.243429972696
INFO:root:current train perplexity3.9370994567871094
INFO:root:current mean train loss 1739.315191681408
INFO:root:current train perplexity3.9355716705322266
INFO:root:current mean train loss 1738.4633794934402
INFO:root:current train perplexity3.9352803230285645
INFO:root:current mean train loss 1737.4936351263239
INFO:root:current train perplexity3.934143304824829

100%|██████████| 1/1 [17:48<00:00, 1068.52s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.52s/it]
INFO:root:final mean train loss: 1737.005868393306
INFO:root:final train perplexity: 3.934985637664795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.03s/it][A100%|██████████| 1/1 [01:16<00:00, 76.03s/it]
INFO:root:eval mean loss: 1790.2099613703735
INFO:root:eval perplexity: 4.253817081451416
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.14s/it][A100%|██████████| 1/1 [01:13<00:00, 73.14s/it]
INFO:root:eval mean loss: 2219.6794727947695
INFO:root:eval perplexity: 6.143010139465332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/17
  8%|▊         | 17/200 [5:47:51<62:01:50, 1220.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1723.6281752152877
INFO:root:current train perplexity3.861463785171509
INFO:root:current mean train loss 1717.2970873244265
INFO:root:current train perplexity3.8491408824920654
INFO:root:current mean train loss 1710.22173860338
INFO:root:current train perplexity3.852015733718872
INFO:root:current mean train loss 1714.0795137071118
INFO:root:current train perplexity3.8712408542633057
INFO:root:current mean train loss 1716.9394131019467
INFO:root:current train perplexity3.8688511848449707
INFO:root:current mean train loss 1717.117054426751
INFO:root:current train perplexity3.8721086978912354
INFO:root:current mean train loss 1719.3708432219748
INFO:root:current train perplexity3.873929500579834
INFO:root:current mean train loss 1717.388233010539
INFO:root:current train perplexity3.8683407306671143
INFO:root:current mean train loss 1721.3426373455975
INFO:root:current train perplexity3.8739545345306396
INFO:root:current mean train loss 1721.2378987300733
INFO:root:current train perplexity3.8760111331939697
INFO:root:current mean train loss 1723.6309182784137
INFO:root:current train perplexity3.879941463470459
INFO:root:current mean train loss 1721.6855122473103
INFO:root:current train perplexity3.876943588256836
INFO:root:current mean train loss 1720.0756168720884
INFO:root:current train perplexity3.8751137256622314
INFO:root:current mean train loss 1721.2353424160212
INFO:root:current train perplexity3.8774311542510986
INFO:root:current mean train loss 1721.6471547772808
INFO:root:current train perplexity3.880283832550049
INFO:root:current mean train loss 1721.1382640771362
INFO:root:current train perplexity3.8815255165100098
INFO:root:current mean train loss 1720.795832846402
INFO:root:current train perplexity3.8812849521636963
INFO:root:current mean train loss 1720.9068361832792
INFO:root:current train perplexity3.881960391998291
INFO:root:current mean train loss 1719.7612708140227
INFO:root:current train perplexity3.8810834884643555

100%|██████████| 1/1 [17:48<00:00, 1068.86s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.86s/it]
INFO:root:final mean train loss: 1719.742508403953
INFO:root:final train perplexity: 3.881774425506592
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.89s/it][A100%|██████████| 1/1 [01:15<00:00, 75.90s/it]
INFO:root:eval mean loss: 1776.241413522274
INFO:root:eval perplexity: 4.206032752990723
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.67s/it][A100%|██████████| 1/1 [01:12<00:00, 72.67s/it]
INFO:root:eval mean loss: 2209.9538682437114
INFO:root:eval perplexity: 6.094343185424805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/18
  9%|▉         | 18/200 [6:08:12<61:41:21, 1220.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1732.5361083984376
INFO:root:current train perplexity3.850785255432129
INFO:root:current mean train loss 1699.5760393415178
INFO:root:current train perplexity3.8076248168945312
INFO:root:current mean train loss 1702.2212408298399
INFO:root:current train perplexity3.8397865295410156
INFO:root:current mean train loss 1698.4446877401383
INFO:root:current train perplexity3.8274762630462646
INFO:root:current mean train loss 1691.1500141661845
INFO:root:current train perplexity3.8163540363311768
INFO:root:current mean train loss 1695.9528470181003
INFO:root:current train perplexity3.824083089828491
INFO:root:current mean train loss 1697.6659653844913
INFO:root:current train perplexity3.8285772800445557
INFO:root:current mean train loss 1698.516659048094
INFO:root:current train perplexity3.8260953426361084
INFO:root:current mean train loss 1700.964584445361
INFO:root:current train perplexity3.825739622116089
INFO:root:current mean train loss 1701.1191518203989
INFO:root:current train perplexity3.8242509365081787
INFO:root:current mean train loss 1701.62138671875
INFO:root:current train perplexity3.8257105350494385
INFO:root:current mean train loss 1702.4281187031604
INFO:root:current train perplexity3.8274178504943848
INFO:root:current mean train loss 1703.4336977883493
INFO:root:current train perplexity3.8304455280303955
INFO:root:current mean train loss 1703.9194707293163
INFO:root:current train perplexity3.831096649169922
INFO:root:current mean train loss 1704.256351305327
INFO:root:current train perplexity3.832963705062866
INFO:root:current mean train loss 1703.4039352873235
INFO:root:current train perplexity3.830425262451172
INFO:root:current mean train loss 1702.9018671814156
INFO:root:current train perplexity3.830190658569336
INFO:root:current mean train loss 1703.0343327586602
INFO:root:current train perplexity3.8307249546051025
INFO:root:current mean train loss 1703.0904945436937
INFO:root:current train perplexity3.8314433097839355
INFO:root:current mean train loss 1703.4099196706857
INFO:root:current train perplexity3.8312697410583496

100%|██████████| 1/1 [17:57<00:00, 1077.76s/it][A100%|██████████| 1/1 [17:57<00:00, 1077.76s/it]
INFO:root:final mean train loss: 1703.6091819528492
INFO:root:final train perplexity: 3.8326969146728516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.13s/it][A100%|██████████| 1/1 [01:16<00:00, 76.13s/it]
INFO:root:eval mean loss: 1788.4959071815438
INFO:root:eval perplexity: 4.247924327850342
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.39s/it][A100%|██████████| 1/1 [01:13<00:00, 73.39s/it]
INFO:root:eval mean loss: 2220.7371055657136
INFO:root:eval perplexity: 6.148326873779297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/19
 10%|▉         | 19/200 [6:28:41<61:29:50, 1223.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1699.4274291992188
INFO:root:current train perplexity3.732088804244995
INFO:root:current mean train loss 1683.9489565990011
INFO:root:current train perplexity3.7526803016662598
INFO:root:current mean train loss 1678.7570366386894
INFO:root:current train perplexity3.7549893856048584
INFO:root:current mean train loss 1677.6331089564733
INFO:root:current train perplexity3.7459566593170166
INFO:root:current mean train loss 1679.7145466736708
INFO:root:current train perplexity3.7565701007843018
INFO:root:current mean train loss 1679.5176985583544
INFO:root:current train perplexity3.7558224201202393
INFO:root:current mean train loss 1682.3361449410295
INFO:root:current train perplexity3.7662353515625
INFO:root:current mean train loss 1684.635565337712
INFO:root:current train perplexity3.7722513675689697
INFO:root:current mean train loss 1685.739790050943
INFO:root:current train perplexity3.7703001499176025
INFO:root:current mean train loss 1684.060279962039
INFO:root:current train perplexity3.769369602203369
INFO:root:current mean train loss 1684.9837065993456
INFO:root:current train perplexity3.773697853088379
INFO:root:current mean train loss 1685.931772704643
INFO:root:current train perplexity3.774446964263916
INFO:root:current mean train loss 1686.1528708899666
INFO:root:current train perplexity3.7790098190307617
INFO:root:current mean train loss 1686.5067497865155
INFO:root:current train perplexity3.778895139694214
INFO:root:current mean train loss 1687.0909357728167
INFO:root:current train perplexity3.780363082885742
INFO:root:current mean train loss 1687.9689853181976
INFO:root:current train perplexity3.783402442932129
INFO:root:current mean train loss 1688.3137301857757
INFO:root:current train perplexity3.7850468158721924
INFO:root:current mean train loss 1687.8267182849702
INFO:root:current train perplexity3.7854042053222656
INFO:root:current mean train loss 1686.6388778267785
INFO:root:current train perplexity3.7824573516845703
INFO:root:current mean train loss 1688.240806746309
INFO:root:current train perplexity3.785139322280884

100%|██████████| 1/1 [18:07<00:00, 1087.64s/it][A100%|██████████| 1/1 [18:07<00:00, 1087.64s/it]
INFO:root:final mean train loss: 1688.6675551390924
INFO:root:final train perplexity: 3.7877984046936035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.08s/it][A100%|██████████| 1/1 [01:16<00:00, 76.08s/it]
INFO:root:eval mean loss: 1771.182352701823
INFO:root:eval perplexity: 4.188858509063721
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.24s/it][A100%|██████████| 1/1 [01:13<00:00, 73.24s/it]
INFO:root:eval mean loss: 2206.247769835993
INFO:root:eval perplexity: 6.075900077819824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/20
 10%|█         | 20/200 [6:49:21<61:24:15, 1228.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1687.9492657001201
INFO:root:current train perplexity3.7710444927215576
INFO:root:current mean train loss 1675.673403952619
INFO:root:current train perplexity3.7317752838134766
INFO:root:current mean train loss 1672.5060100475614
INFO:root:current train perplexity3.7404048442840576
INFO:root:current mean train loss 1668.7285296684872
INFO:root:current train perplexity3.7383320331573486
INFO:root:current mean train loss 1663.9584621698818
INFO:root:current train perplexity3.731666088104248
INFO:root:current mean train loss 1667.779572042773
INFO:root:current train perplexity3.733513593673706
INFO:root:current mean train loss 1672.903101579311
INFO:root:current train perplexity3.7379720211029053
INFO:root:current mean train loss 1674.4680109708008
INFO:root:current train perplexity3.744314193725586
INFO:root:current mean train loss 1672.897965659686
INFO:root:current train perplexity3.7394025325775146
INFO:root:current mean train loss 1673.5617430080622
INFO:root:current train perplexity3.742662191390991
INFO:root:current mean train loss 1673.2677958307643
INFO:root:current train perplexity3.7411410808563232
INFO:root:current mean train loss 1673.330282290009
INFO:root:current train perplexity3.742023229598999
INFO:root:current mean train loss 1672.3953746090597
INFO:root:current train perplexity3.741062879562378
INFO:root:current mean train loss 1670.1655096576853
INFO:root:current train perplexity3.738205909729004
INFO:root:current mean train loss 1673.1598752931723
INFO:root:current train perplexity3.7412173748016357
INFO:root:current mean train loss 1673.550643633371
INFO:root:current train perplexity3.741783857345581
INFO:root:current mean train loss 1673.1696652964604
INFO:root:current train perplexity3.7404747009277344
INFO:root:current mean train loss 1673.6190669897167
INFO:root:current train perplexity3.7423484325408936
INFO:root:current mean train loss 1672.8151175087726
INFO:root:current train perplexity3.7412021160125732
INFO:root:current mean train loss 1672.81124057937
INFO:root:current train perplexity3.740304470062256

100%|██████████| 1/1 [17:59<00:00, 1079.74s/it][A100%|██████████| 1/1 [17:59<00:00, 1079.74s/it]
INFO:root:final mean train loss: 1672.737297515946
INFO:root:final train perplexity: 3.7405073642730713
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.30s/it][A100%|██████████| 1/1 [01:17<00:00, 77.30s/it]
INFO:root:eval mean loss: 1765.7000039824356
INFO:root:eval perplexity: 4.170327186584473
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.09s/it][A100%|██████████| 1/1 [01:13<00:00, 73.09s/it]
INFO:root:eval mean loss: 2202.8485224297706
INFO:root:eval perplexity: 6.059032440185547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/21
 10%|█         | 21/200 [7:09:54<61:08:00, 1229.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.869134085519
INFO:root:current train perplexity3.669320583343506
INFO:root:current mean train loss 1641.4067844488682
INFO:root:current train perplexity3.6609373092651367
INFO:root:current mean train loss 1648.3322286605835
INFO:root:current train perplexity3.671144485473633
INFO:root:current mean train loss 1648.911794255289
INFO:root:current train perplexity3.675374984741211
INFO:root:current mean train loss 1652.8147845351905
INFO:root:current train perplexity3.679962158203125
INFO:root:current mean train loss 1655.9633049175893
INFO:root:current train perplexity3.6899843215942383
INFO:root:current mean train loss 1655.6476836786037
INFO:root:current train perplexity3.6898083686828613
INFO:root:current mean train loss 1653.4466339595733
INFO:root:current train perplexity3.688044548034668
INFO:root:current mean train loss 1657.6711350200333
INFO:root:current train perplexity3.694911241531372
INFO:root:current mean train loss 1658.1460695147016
INFO:root:current train perplexity3.6945958137512207
INFO:root:current mean train loss 1656.9894135215065
INFO:root:current train perplexity3.6925435066223145
INFO:root:current mean train loss 1655.8612894764408
INFO:root:current train perplexity3.6889142990112305
INFO:root:current mean train loss 1656.5685407310534
INFO:root:current train perplexity3.6908133029937744
INFO:root:current mean train loss 1657.270669402626
INFO:root:current train perplexity3.6916897296905518
INFO:root:current mean train loss 1658.0849232097248
INFO:root:current train perplexity3.692199468612671
INFO:root:current mean train loss 1659.2203474265443
INFO:root:current train perplexity3.6934192180633545
INFO:root:current mean train loss 1659.4392080998075
INFO:root:current train perplexity3.695545196533203
INFO:root:current mean train loss 1658.4471936758123
INFO:root:current train perplexity3.694455862045288
INFO:root:current mean train loss 1658.3108216647445
INFO:root:current train perplexity3.6943342685699463
INFO:root:current mean train loss 1657.6253112917784
INFO:root:current train perplexity3.6940577030181885

100%|██████████| 1/1 [17:48<00:00, 1068.08s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.08s/it]
INFO:root:final mean train loss: 1657.0151992620872
INFO:root:final train perplexity: 3.694413900375366
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.47s/it][A100%|██████████| 1/1 [01:15<00:00, 75.47s/it]
INFO:root:eval mean loss: 1759.451635915337
INFO:root:eval perplexity: 4.149306297302246
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.30s/it][A100%|██████████| 1/1 [01:13<00:00, 73.30s/it]
INFO:root:eval mean loss: 2200.674323332225
INFO:root:eval perplexity: 6.0482683181762695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/22
 11%|█         | 22/200 [7:30:13<60:38:42, 1226.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1626.0837485953552
INFO:root:current train perplexity3.620600700378418
INFO:root:current mean train loss 1627.658735153992
INFO:root:current train perplexity3.596938133239746
INFO:root:current mean train loss 1633.970191592262
INFO:root:current train perplexity3.6116631031036377
INFO:root:current mean train loss 1633.8460385729097
INFO:root:current train perplexity3.622253179550171
INFO:root:current mean train loss 1636.0861078306687
INFO:root:current train perplexity3.6309926509857178
INFO:root:current mean train loss 1637.526727220359
INFO:root:current train perplexity3.6338796615600586
INFO:root:current mean train loss 1639.9065461973555
INFO:root:current train perplexity3.640956163406372
INFO:root:current mean train loss 1639.9808619648488
INFO:root:current train perplexity3.6432981491088867
INFO:root:current mean train loss 1639.0737699003973
INFO:root:current train perplexity3.6433606147766113
INFO:root:current mean train loss 1642.4689814694004
INFO:root:current train perplexity3.6507134437561035
INFO:root:current mean train loss 1643.1558411270241
INFO:root:current train perplexity3.653042793273926
INFO:root:current mean train loss 1642.072355018349
INFO:root:current train perplexity3.65207839012146
INFO:root:current mean train loss 1642.1060592519516
INFO:root:current train perplexity3.651951789855957
INFO:root:current mean train loss 1643.9680147330776
INFO:root:current train perplexity3.6547467708587646
INFO:root:current mean train loss 1644.0848642259896
INFO:root:current train perplexity3.6563408374786377
INFO:root:current mean train loss 1644.1625552847365
INFO:root:current train perplexity3.6559665203094482
INFO:root:current mean train loss 1644.5715714367434
INFO:root:current train perplexity3.6573004722595215
INFO:root:current mean train loss 1644.074583859344
INFO:root:current train perplexity3.6558358669281006
INFO:root:current mean train loss 1644.7607722325688
INFO:root:current train perplexity3.6587467193603516
INFO:root:current mean train loss 1644.6083214088435
INFO:root:current train perplexity3.657639265060425

100%|██████████| 1/1 [17:53<00:00, 1073.55s/it][A100%|██████████| 1/1 [17:53<00:00, 1073.55s/it]
INFO:root:final mean train loss: 1644.4249182011945
INFO:root:final train perplexity: 3.657911777496338
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.65s/it][A100%|██████████| 1/1 [01:15<00:00, 75.65s/it]
INFO:root:eval mean loss: 1772.7782333049367
INFO:root:eval perplexity: 4.194268226623535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.66s/it][A100%|██████████| 1/1 [01:12<00:00, 72.66s/it]
INFO:root:eval mean loss: 2219.568338164201
INFO:root:eval perplexity: 6.142451286315918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/23
 12%|█▏        | 23/200 [7:50:38<60:16:30, 1225.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.5419189453125
INFO:root:current train perplexity3.5969152450561523
INFO:root:current mean train loss 1619.9127177991365
INFO:root:current train perplexity3.6046807765960693
INFO:root:current mean train loss 1617.7347420528017
INFO:root:current train perplexity3.595949411392212
INFO:root:current mean train loss 1623.1576397235576
INFO:root:current train perplexity3.602505922317505
INFO:root:current mean train loss 1624.1529010383451
INFO:root:current train perplexity3.6016392707824707
INFO:root:current mean train loss 1627.4421740515756
INFO:root:current train perplexity3.6080715656280518
INFO:root:current mean train loss 1622.8068161231884
INFO:root:current train perplexity3.6026432514190674
INFO:root:current mean train loss 1623.6235560163666
INFO:root:current train perplexity3.6042141914367676
INFO:root:current mean train loss 1623.9600629827949
INFO:root:current train perplexity3.600431442260742
INFO:root:current mean train loss 1622.997750330453
INFO:root:current train perplexity3.5977377891540527
INFO:root:current mean train loss 1624.699628301498
INFO:root:current train perplexity3.6031479835510254
INFO:root:current mean train loss 1626.0760033359047
INFO:root:current train perplexity3.6038589477539062
INFO:root:current mean train loss 1625.1022065391837
INFO:root:current train perplexity3.601940393447876
INFO:root:current mean train loss 1626.615276792238
INFO:root:current train perplexity3.6066927909851074
INFO:root:current mean train loss 1626.902392578125
INFO:root:current train perplexity3.6079180240631104
INFO:root:current mean train loss 1627.5845219450177
INFO:root:current train perplexity3.6098432540893555
INFO:root:current mean train loss 1627.837333074265
INFO:root:current train perplexity3.609572172164917
INFO:root:current mean train loss 1628.6062354743149
INFO:root:current train perplexity3.610281229019165
INFO:root:current mean train loss 1628.6605202649637
INFO:root:current train perplexity3.61118745803833

100%|██████████| 1/1 [17:48<00:00, 1068.16s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.16s/it]
INFO:root:final mean train loss: 1628.5513631615804
INFO:root:final train perplexity: 3.6124045848846436
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.46s/it][A100%|██████████| 1/1 [01:15<00:00, 75.46s/it]
INFO:root:eval mean loss: 1752.6737372215757
INFO:root:eval perplexity: 4.12662410736084
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.10s/it][A100%|██████████| 1/1 [01:14<00:00, 74.12s/it]
INFO:root:eval mean loss: 2197.475762549867
INFO:root:eval perplexity: 6.032466411590576
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/24
 12%|█▏        | 24/200 [8:10:58<59:51:15, 1224.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.0750558035713
INFO:root:current train perplexity3.474456548690796
INFO:root:current mean train loss 1605.010660046729
INFO:root:current train perplexity3.568240165710449
INFO:root:current mean train loss 1605.2909206814236
INFO:root:current train perplexity3.5557596683502197
INFO:root:current mean train loss 1610.0442817017
INFO:root:current train perplexity3.5642342567443848
INFO:root:current mean train loss 1612.6434362163122
INFO:root:current train perplexity3.574091672897339
INFO:root:current mean train loss 1613.6618830513437
INFO:root:current train perplexity3.5709781646728516
INFO:root:current mean train loss 1617.9050085831318
INFO:root:current train perplexity3.573608636856079
INFO:root:current mean train loss 1615.944349059627
INFO:root:current train perplexity3.569960117340088
INFO:root:current mean train loss 1616.3915510402146
INFO:root:current train perplexity3.5727992057800293
INFO:root:current mean train loss 1616.7735697989165
INFO:root:current train perplexity3.576110601425171
INFO:root:current mean train loss 1616.310519600104
INFO:root:current train perplexity3.574537515640259
INFO:root:current mean train loss 1617.2029469118747
INFO:root:current train perplexity3.5754191875457764
INFO:root:current mean train loss 1616.954534421603
INFO:root:current train perplexity3.57633900642395
INFO:root:current mean train loss 1616.2662149909443
INFO:root:current train perplexity3.574256181716919
INFO:root:current mean train loss 1615.6042271378876
INFO:root:current train perplexity3.573803663253784
INFO:root:current mean train loss 1616.8285091934254
INFO:root:current train perplexity3.576155662536621
INFO:root:current mean train loss 1616.1947493965608
INFO:root:current train perplexity3.5740034580230713
INFO:root:current mean train loss 1616.2287252970307
INFO:root:current train perplexity3.5751190185546875
INFO:root:current mean train loss 1616.123215490108
INFO:root:current train perplexity3.5753159523010254
INFO:root:current mean train loss 1616.309587403624
INFO:root:current train perplexity3.576594352722168

100%|██████████| 1/1 [17:44<00:00, 1064.81s/it][A100%|██████████| 1/1 [17:44<00:00, 1064.81s/it]
INFO:root:final mean train loss: 1615.6329964672864
INFO:root:final train perplexity: 3.57578706741333
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.50s/it][A100%|██████████| 1/1 [01:16<00:00, 76.50s/it]
INFO:root:eval mean loss: 1758.478813874806
INFO:root:eval perplexity: 4.146042823791504
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.58s/it][A100%|██████████| 1/1 [01:13<00:00, 73.59s/it]
INFO:root:eval mean loss: 2206.640016812805
INFO:root:eval perplexity: 6.077849388122559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/25
 12%|█▎        | 25/200 [8:31:16<59:24:58, 1222.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1605.7706502278645
INFO:root:current train perplexity3.5059709548950195
INFO:root:current mean train loss 1597.2161471459174
INFO:root:current train perplexity3.4855284690856934
INFO:root:current mean train loss 1607.7830499921527
INFO:root:current train perplexity3.5151429176330566
INFO:root:current mean train loss 1608.8713921440972
INFO:root:current train perplexity3.533950090408325
INFO:root:current mean train loss 1614.6155409902897
INFO:root:current train perplexity3.543063163757324
INFO:root:current mean train loss 1612.857506438976
INFO:root:current train perplexity3.539828062057495
INFO:root:current mean train loss 1610.6677989470652
INFO:root:current train perplexity3.540220022201538
INFO:root:current mean train loss 1608.4824724566213
INFO:root:current train perplexity3.541234254837036
INFO:root:current mean train loss 1605.8770310485247
INFO:root:current train perplexity3.5386803150177
INFO:root:current mean train loss 1604.6262713015337
INFO:root:current train perplexity3.5373716354370117
INFO:root:current mean train loss 1605.5663930177689
INFO:root:current train perplexity3.5394794940948486
INFO:root:current mean train loss 1605.865580820104
INFO:root:current train perplexity3.5403378009796143
INFO:root:current mean train loss 1605.8433832904093
INFO:root:current train perplexity3.540381669998169
INFO:root:current mean train loss 1606.2171951708838
INFO:root:current train perplexity3.5416903495788574
INFO:root:current mean train loss 1606.5386427975773
INFO:root:current train perplexity3.5440330505371094
INFO:root:current mean train loss 1606.2719773019705
INFO:root:current train perplexity3.543630361557007
INFO:root:current mean train loss 1605.5324829552562
INFO:root:current train perplexity3.543823480606079
INFO:root:current mean train loss 1604.493480779732
INFO:root:current train perplexity3.5421876907348633
INFO:root:current mean train loss 1603.9381025213945
INFO:root:current train perplexity3.5420191287994385
INFO:root:current mean train loss 1603.8268928131295
INFO:root:current train perplexity3.5404446125030518

100%|██████████| 1/1 [17:49<00:00, 1069.47s/it][A100%|██████████| 1/1 [17:49<00:00, 1069.47s/it]
INFO:root:final mean train loss: 1603.4886844575376
INFO:root:final train perplexity: 3.54170298576355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.98s/it][A100%|██████████| 1/1 [01:17<00:00, 77.98s/it]
INFO:root:eval mean loss: 1747.600174707724
INFO:root:eval perplexity: 4.109726905822754
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.88s/it][A100%|██████████| 1/1 [01:13<00:00, 73.88s/it]
INFO:root:eval mean loss: 2197.9841728480997
INFO:root:eval perplexity: 6.034976005554199
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/26
 13%|█▎        | 26/200 [8:51:40<59:06:08, 1222.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1619.812997213224
INFO:root:current train perplexity3.5205585956573486
INFO:root:current mean train loss 1596.7049820270945
INFO:root:current train perplexity3.492872476577759
INFO:root:current mean train loss 1593.7215287457857
INFO:root:current train perplexity3.4874565601348877
INFO:root:current mean train loss 1596.286991600417
INFO:root:current train perplexity3.494638204574585
INFO:root:current mean train loss 1599.614002599738
INFO:root:current train perplexity3.5023157596588135
INFO:root:current mean train loss 1595.0758699709738
INFO:root:current train perplexity3.5006511211395264
INFO:root:current mean train loss 1594.3566709807064
INFO:root:current train perplexity3.500504732131958
INFO:root:current mean train loss 1591.6231022267207
INFO:root:current train perplexity3.5000574588775635
INFO:root:current mean train loss 1592.8016508376838
INFO:root:current train perplexity3.5040059089660645
INFO:root:current mean train loss 1589.9826566754948
INFO:root:current train perplexity3.502675771713257
INFO:root:current mean train loss 1591.0115704128768
INFO:root:current train perplexity3.5027029514312744
INFO:root:current mean train loss 1592.4307076055475
INFO:root:current train perplexity3.5046513080596924
INFO:root:current mean train loss 1591.0658936727248
INFO:root:current train perplexity3.504093885421753
INFO:root:current mean train loss 1591.0654330555847
INFO:root:current train perplexity3.504183292388916
INFO:root:current mean train loss 1590.929353818556
INFO:root:current train perplexity3.503129720687866
INFO:root:current mean train loss 1591.4282080014752
INFO:root:current train perplexity3.505446434020996
INFO:root:current mean train loss 1591.5693120590295
INFO:root:current train perplexity3.5065486431121826
INFO:root:current mean train loss 1592.1635367071951
INFO:root:current train perplexity3.508090019226074
INFO:root:current mean train loss 1592.0801044486905
INFO:root:current train perplexity3.50907301902771
INFO:root:current mean train loss 1591.4392253987755
INFO:root:current train perplexity3.5066275596618652

100%|██████████| 1/1 [17:52<00:00, 1072.66s/it][A100%|██████████| 1/1 [17:52<00:00, 1072.66s/it]
INFO:root:final mean train loss: 1591.0292350088052
INFO:root:final train perplexity: 3.5070714950561523
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.62s/it][A100%|██████████| 1/1 [01:15<00:00, 75.62s/it]
INFO:root:eval mean loss: 1748.6351521809895
INFO:root:eval perplexity: 4.113167762756348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.70s/it][A100%|██████████| 1/1 [01:16<00:00, 76.70s/it]
INFO:root:eval mean loss: 2201.1527138567985
INFO:root:eval perplexity: 6.050634384155273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/27
 14%|█▎        | 27/200 [9:12:08<58:49:55, 1224.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.7515342975485
INFO:root:current train perplexity3.476938486099243
INFO:root:current mean train loss 1568.516138776948
INFO:root:current train perplexity3.444667100906372
INFO:root:current mean train loss 1570.00013957652
INFO:root:current train perplexity3.4517669677734375
INFO:root:current mean train loss 1567.6857811272478
INFO:root:current train perplexity3.447176933288574
INFO:root:current mean train loss 1571.3985862232191
INFO:root:current train perplexity3.455862283706665
INFO:root:current mean train loss 1569.7583681605622
INFO:root:current train perplexity3.4557242393493652
INFO:root:current mean train loss 1573.6604831312927
INFO:root:current train perplexity3.461441993713379
INFO:root:current mean train loss 1573.5494681084062
INFO:root:current train perplexity3.4610307216644287
INFO:root:current mean train loss 1573.3576449592074
INFO:root:current train perplexity3.462463855743408
INFO:root:current mean train loss 1572.8347791062517
INFO:root:current train perplexity3.4573163986206055
INFO:root:current mean train loss 1572.4797312514768
INFO:root:current train perplexity3.457413673400879
INFO:root:current mean train loss 1574.4314007009662
INFO:root:current train perplexity3.462707281112671
INFO:root:current mean train loss 1575.3384602217682
INFO:root:current train perplexity3.464038372039795
INFO:root:current mean train loss 1575.3407916376623
INFO:root:current train perplexity3.462401866912842
INFO:root:current mean train loss 1576.1641067902574
INFO:root:current train perplexity3.4645938873291016
INFO:root:current mean train loss 1577.1852220332057
INFO:root:current train perplexity3.4667508602142334
INFO:root:current mean train loss 1577.1535112958477
INFO:root:current train perplexity3.46662974357605
INFO:root:current mean train loss 1577.6087552050003
INFO:root:current train perplexity3.468125343322754
INFO:root:current mean train loss 1579.48132212529
INFO:root:current train perplexity3.473073959350586
INFO:root:current mean train loss 1579.6488728508643
INFO:root:current train perplexity3.4743597507476807

100%|██████████| 1/1 [17:51<00:00, 1071.95s/it][A100%|██████████| 1/1 [17:51<00:00, 1071.95s/it]
INFO:root:final mean train loss: 1579.0935662173888
INFO:root:final train perplexity: 3.4742136001586914
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.13s/it][A100%|██████████| 1/1 [01:16<00:00, 76.13s/it]
INFO:root:eval mean loss: 1745.7652250249334
INFO:root:eval perplexity: 4.10363245010376
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.72s/it][A100%|██████████| 1/1 [01:14<00:00, 74.74s/it]
INFO:root:eval mean loss: 2196.4940003740026
INFO:root:eval perplexity: 6.027626037597656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/28
 14%|█▍        | 28/200 [9:32:33<58:30:34, 1224.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.0677376302083
INFO:root:current train perplexity3.3946025371551514
INFO:root:current mean train loss 1540.7198577008928
INFO:root:current train perplexity3.3859519958496094
INFO:root:current mean train loss 1544.805858043324
INFO:root:current train perplexity3.4029664993286133
INFO:root:current mean train loss 1549.3920201822916
INFO:root:current train perplexity3.4066364765167236
INFO:root:current mean train loss 1554.3127176706414
INFO:root:current train perplexity3.411491632461548
INFO:root:current mean train loss 1554.3667569633153
INFO:root:current train perplexity3.4113407135009766
INFO:root:current mean train loss 1554.2003045428241
INFO:root:current train perplexity3.415940761566162
INFO:root:current mean train loss 1552.4117023689516
INFO:root:current train perplexity3.414640426635742
INFO:root:current mean train loss 1554.4560969587053
INFO:root:current train perplexity3.419442892074585
INFO:root:current mean train loss 1557.5879796424279
INFO:root:current train perplexity3.422274589538574
INFO:root:current mean train loss 1559.4732219749274
INFO:root:current train perplexity3.4275283813476562
INFO:root:current mean train loss 1560.510974588597
INFO:root:current train perplexity3.4278745651245117
INFO:root:current mean train loss 1561.413368661918
INFO:root:current train perplexity3.4287731647491455
INFO:root:current mean train loss 1562.1731235795455
INFO:root:current train perplexity3.427945137023926
INFO:root:current mean train loss 1564.392941604873
INFO:root:current train perplexity3.4342293739318848
INFO:root:current mean train loss 1565.9669440569196
INFO:root:current train perplexity3.4370081424713135
INFO:root:current mean train loss 1566.7781977320428
INFO:root:current train perplexity3.4378085136413574
INFO:root:current mean train loss 1568.2307251320422
INFO:root:current train perplexity3.4409067630767822
INFO:root:current mean train loss 1568.6475545572916
INFO:root:current train perplexity3.443553924560547
INFO:root:current mean train loss 1567.8250467266614
INFO:root:current train perplexity3.442377805709839

100%|██████████| 1/1 [17:49<00:00, 1070.00s/it][A100%|██████████| 1/1 [17:49<00:00, 1070.00s/it]
INFO:root:final mean train loss: 1567.3136376300606
INFO:root:final train perplexity: 3.4420864582061768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.75s/it][A100%|██████████| 1/1 [01:15<00:00, 75.75s/it]
INFO:root:eval mean loss: 1755.8154894240358
INFO:root:eval perplexity: 4.13712215423584
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.01s/it][A100%|██████████| 1/1 [01:13<00:00, 73.01s/it]
INFO:root:eval mean loss: 2212.9832711727063
INFO:root:eval perplexity: 6.109460353851318
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/29
 14%|█▍        | 29/200 [9:52:55<58:07:26, 1223.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1528.2768766983695
INFO:root:current train perplexity3.373744249343872
INFO:root:current mean train loss 1541.257942199707
INFO:root:current train perplexity3.3797404766082764
INFO:root:current mean train loss 1550.8532267531305
INFO:root:current train perplexity3.394991397857666
INFO:root:current mean train loss 1555.713627406529
INFO:root:current train perplexity3.4041190147399902
INFO:root:current mean train loss 1553.8078265926702
INFO:root:current train perplexity3.3982789516448975
INFO:root:current mean train loss 1552.018549120104
INFO:root:current train perplexity3.396940231323242
INFO:root:current mean train loss 1553.1639640675803
INFO:root:current train perplexity3.397416114807129
INFO:root:current mean train loss 1554.122668487857
INFO:root:current train perplexity3.3977482318878174
INFO:root:current mean train loss 1553.3572980256358
INFO:root:current train perplexity3.400818347930908
INFO:root:current mean train loss 1552.2326558020807
INFO:root:current train perplexity3.400848388671875
INFO:root:current mean train loss 1551.578601879078
INFO:root:current train perplexity3.398235559463501
INFO:root:current mean train loss 1550.7660819444081
INFO:root:current train perplexity3.3974802494049072
INFO:root:current mean train loss 1551.7157478450622
INFO:root:current train perplexity3.3988218307495117
INFO:root:current mean train loss 1552.2056932120488
INFO:root:current train perplexity3.4017367362976074
INFO:root:current mean train loss 1552.6346572180537
INFO:root:current train perplexity3.4028210639953613
INFO:root:current mean train loss 1552.8629316780437
INFO:root:current train perplexity3.403418779373169
INFO:root:current mean train loss 1553.262121683035
INFO:root:current train perplexity3.403780698776245
INFO:root:current mean train loss 1554.8007886750358
INFO:root:current train perplexity3.4073646068573
INFO:root:current mean train loss 1555.4363168470452
INFO:root:current train perplexity3.409665584564209

100%|██████████| 1/1 [17:46<00:00, 1066.42s/it][A100%|██████████| 1/1 [17:46<00:00, 1066.42s/it]
INFO:root:final mean train loss: 1555.8530990285099
INFO:root:final train perplexity: 3.411115884780884
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.98s/it][A100%|██████████| 1/1 [01:15<00:00, 75.99s/it]
INFO:root:eval mean loss: 1749.8706682354
INFO:root:eval perplexity: 4.117280006408691
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.19s/it][A100%|██████████| 1/1 [01:13<00:00, 73.19s/it]
INFO:root:eval mean loss: 2205.4589185782356
INFO:root:eval perplexity: 6.071981430053711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/30
 15%|█▌        | 30/200 [10:13:13<57:42:27, 1222.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1474.9916856553818
INFO:root:current train perplexity3.2858190536499023
INFO:root:current mean train loss 1528.7088533453984
INFO:root:current train perplexity3.3569700717926025
INFO:root:current mean train loss 1536.0315361935557
INFO:root:current train perplexity3.36515474319458
INFO:root:current mean train loss 1538.5264217043386
INFO:root:current train perplexity3.366089105606079
INFO:root:current mean train loss 1537.6025360778958
INFO:root:current train perplexity3.367056369781494
INFO:root:current mean train loss 1538.2386659273698
INFO:root:current train perplexity3.3664638996124268
INFO:root:current mean train loss 1538.0403515544822
INFO:root:current train perplexity3.36126446723938
INFO:root:current mean train loss 1541.1736910756788
INFO:root:current train perplexity3.3700218200683594
INFO:root:current mean train loss 1541.3017089240188
INFO:root:current train perplexity3.3719065189361572
INFO:root:current mean train loss 1541.8348951350213
INFO:root:current train perplexity3.375483512878418
INFO:root:current mean train loss 1542.9042810264261
INFO:root:current train perplexity3.3772644996643066
INFO:root:current mean train loss 1543.0533295365688
INFO:root:current train perplexity3.3786568641662598
INFO:root:current mean train loss 1544.175530243551
INFO:root:current train perplexity3.3808341026306152
INFO:root:current mean train loss 1544.2369567544702
INFO:root:current train perplexity3.380737543106079
INFO:root:current mean train loss 1545.782490802708
INFO:root:current train perplexity3.3822643756866455
INFO:root:current mean train loss 1545.4880820060107
INFO:root:current train perplexity3.3822758197784424
INFO:root:current mean train loss 1546.8645260788921
INFO:root:current train perplexity3.3843491077423096
INFO:root:current mean train loss 1545.8840031319714
INFO:root:current train perplexity3.382456064224243
INFO:root:current mean train loss 1546.6928195394555
INFO:root:current train perplexity3.3833656311035156
INFO:root:current mean train loss 1546.350331949401
INFO:root:current train perplexity3.3840866088867188

100%|██████████| 1/1 [17:45<00:00, 1065.39s/it][A100%|██████████| 1/1 [17:45<00:00, 1065.39s/it]
INFO:root:final mean train loss: 1546.4458547064107
INFO:root:final train perplexity: 3.385901689529419
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.46s/it][A100%|██████████| 1/1 [01:17<00:00, 77.48s/it]
INFO:root:eval mean loss: 1747.1218642647384
INFO:root:eval perplexity: 4.1081366539001465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.75s/it][A100%|██████████| 1/1 [01:12<00:00, 72.79s/it]
INFO:root:eval mean loss: 2204.2507579614085
INFO:root:eval perplexity: 6.065985202789307
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/31
 16%|█▌        | 31/200 [10:33:31<57:18:56, 1220.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1522.3681640625
INFO:root:current train perplexity3.29189133644104
INFO:root:current mean train loss 1506.1221110026042
INFO:root:current train perplexity3.3034212589263916
INFO:root:current mean train loss 1523.7591115225732
INFO:root:current train perplexity3.3289029598236084
INFO:root:current mean train loss 1528.4858671785132
INFO:root:current train perplexity3.335891008377075
INFO:root:current mean train loss 1526.0870662205655
INFO:root:current train perplexity3.3347558975219727
INFO:root:current mean train loss 1527.2167776129545
INFO:root:current train perplexity3.3399736881256104
INFO:root:current mean train loss 1527.7400871496232
INFO:root:current train perplexity3.3390417098999023
INFO:root:current mean train loss 1531.0597428250903
INFO:root:current train perplexity3.3475215435028076
INFO:root:current mean train loss 1530.8133459206642
INFO:root:current train perplexity3.346717596054077
INFO:root:current mean train loss 1531.4106901428354
INFO:root:current train perplexity3.3443470001220703
INFO:root:current mean train loss 1531.5066321300485
INFO:root:current train perplexity3.3464407920837402
INFO:root:current mean train loss 1531.4564334740646
INFO:root:current train perplexity3.347464084625244
INFO:root:current mean train loss 1533.1816602398872
INFO:root:current train perplexity3.3509409427642822
INFO:root:current mean train loss 1534.263600253052
INFO:root:current train perplexity3.3534343242645264
INFO:root:current mean train loss 1533.3355751412112
INFO:root:current train perplexity3.3517186641693115
INFO:root:current mean train loss 1534.2845392589645
INFO:root:current train perplexity3.352536201477051
INFO:root:current mean train loss 1534.6504501586764
INFO:root:current train perplexity3.3520562648773193
INFO:root:current mean train loss 1535.490746419648
INFO:root:current train perplexity3.353520393371582
INFO:root:current mean train loss 1535.1122381705443
INFO:root:current train perplexity3.3535735607147217
INFO:root:current mean train loss 1535.1513309340116
INFO:root:current train perplexity3.355543851852417

100%|██████████| 1/1 [17:47<00:00, 1067.16s/it][A100%|██████████| 1/1 [17:47<00:00, 1067.16s/it]
INFO:root:final mean train loss: 1535.133392349374
INFO:root:final train perplexity: 3.355827808380127
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.74s/it][A100%|██████████| 1/1 [01:14<00:00, 74.74s/it]
INFO:root:eval mean loss: 1736.851125297817
INFO:root:eval perplexity: 4.074154853820801
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.10s/it][A100%|██████████| 1/1 [01:12<00:00, 72.10s/it]
INFO:root:eval mean loss: 2195.607026228668
INFO:root:eval perplexity: 6.023255348205566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/32
 16%|█▌        | 32/200 [10:53:48<56:55:03, 1219.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1508.7121780750363
INFO:root:current train perplexity3.3212366104125977
INFO:root:current mean train loss 1521.873001632157
INFO:root:current train perplexity3.3291239738464355
INFO:root:current mean train loss 1520.4531079202031
INFO:root:current train perplexity3.321810007095337
INFO:root:current mean train loss 1514.7816674591154
INFO:root:current train perplexity3.3067920207977295
INFO:root:current mean train loss 1516.5819830280932
INFO:root:current train perplexity3.3127553462982178
INFO:root:current mean train loss 1515.1540531839894
INFO:root:current train perplexity3.3122971057891846
INFO:root:current mean train loss 1514.8560430310192
INFO:root:current train perplexity3.3113832473754883
INFO:root:current mean train loss 1517.0988720243101
INFO:root:current train perplexity3.314509868621826
INFO:root:current mean train loss 1519.0113179307532
INFO:root:current train perplexity3.318044900894165
INFO:root:current mean train loss 1518.2261821791324
INFO:root:current train perplexity3.315412998199463
INFO:root:current mean train loss 1520.0098250801475
INFO:root:current train perplexity3.3184683322906494
INFO:root:current mean train loss 1520.1902190815015
INFO:root:current train perplexity3.317836046218872
INFO:root:current mean train loss 1521.7561940617459
INFO:root:current train perplexity3.3214237689971924
INFO:root:current mean train loss 1521.7960395046073
INFO:root:current train perplexity3.322086811065674
INFO:root:current mean train loss 1523.0819255910808
INFO:root:current train perplexity3.3239738941192627
INFO:root:current mean train loss 1523.8376653922198
INFO:root:current train perplexity3.3246214389801025
INFO:root:current mean train loss 1524.520939627872
INFO:root:current train perplexity3.3277220726013184
INFO:root:current mean train loss 1524.6169094626273
INFO:root:current train perplexity3.3275392055511475
INFO:root:current mean train loss 1525.7819787127391
INFO:root:current train perplexity3.3298230171203613
INFO:root:current mean train loss 1525.8604595598583
INFO:root:current train perplexity3.32970929145813

100%|██████████| 1/1 [17:55<00:00, 1075.76s/it][A100%|██████████| 1/1 [17:55<00:00, 1075.76s/it]
INFO:root:final mean train loss: 1525.3861516432153
INFO:root:final train perplexity: 3.330130100250244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.34s/it][A100%|██████████| 1/1 [01:15<00:00, 75.34s/it]
INFO:root:eval mean loss: 1740.1315814079123
INFO:root:eval perplexity: 4.084978103637695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.72s/it][A100%|██████████| 1/1 [01:12<00:00, 72.75s/it]
INFO:root:eval mean loss: 2201.3568050303356
INFO:root:eval perplexity: 6.051645278930664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/33
 16%|█▋        | 33/200 [11:14:14<56:40:22, 1221.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1539.1414611816406
INFO:root:current train perplexity3.3565752506256104
INFO:root:current mean train loss 1513.5572654724122
INFO:root:current train perplexity3.3151252269744873
INFO:root:current mean train loss 1508.2627399151143
INFO:root:current train perplexity3.2962124347686768
INFO:root:current mean train loss 1504.691385904948
INFO:root:current train perplexity3.289668321609497
INFO:root:current mean train loss 1508.7704876443613
INFO:root:current train perplexity3.291698694229126
INFO:root:current mean train loss 1508.5086667742048
INFO:root:current train perplexity3.2905263900756836
INFO:root:current mean train loss 1509.304989531546
INFO:root:current train perplexity3.2894740104675293
INFO:root:current mean train loss 1509.9115822239926
INFO:root:current train perplexity3.287824869155884
INFO:root:current mean train loss 1510.7680580316587
INFO:root:current train perplexity3.290297269821167
INFO:root:current mean train loss 1510.3492215474446
INFO:root:current train perplexity3.291593551635742
INFO:root:current mean train loss 1510.6183167655513
INFO:root:current train perplexity3.295095443725586
INFO:root:current mean train loss 1510.4200733053274
INFO:root:current train perplexity3.296215057373047
INFO:root:current mean train loss 1510.8108343215215
INFO:root:current train perplexity3.2963027954101562
INFO:root:current mean train loss 1511.0071077234604
INFO:root:current train perplexity3.2960548400878906
INFO:root:current mean train loss 1511.7867899019425
INFO:root:current train perplexity3.2974658012390137
INFO:root:current mean train loss 1513.029157041892
INFO:root:current train perplexity3.300469160079956
INFO:root:current mean train loss 1512.5436393278192
INFO:root:current train perplexity3.2992637157440186
INFO:root:current mean train loss 1512.6980244029653
INFO:root:current train perplexity3.3001933097839355
INFO:root:current mean train loss 1513.52244781166
INFO:root:current train perplexity3.300474166870117
INFO:root:current mean train loss 1514.215949046855
INFO:root:current train perplexity3.300230026245117

100%|██████████| 1/1 [17:44<00:00, 1064.27s/it][A100%|██████████| 1/1 [17:44<00:00, 1064.27s/it]
INFO:root:final mean train loss: 1514.1232306576112
INFO:root:final train perplexity: 3.3006808757781982
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.59s/it][A100%|██████████| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 1741.9370467814992
INFO:root:eval perplexity: 4.090946674346924
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.22s/it][A100%|██████████| 1/1 [01:13<00:00, 73.22s/it]
INFO:root:eval mean loss: 2203.258573924396
INFO:root:eval perplexity: 6.061063289642334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/34
 17%|█▋        | 34/200 [11:34:29<56:14:11, 1219.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1499.4486543729709
INFO:root:current train perplexity3.2781691551208496
INFO:root:current mean train loss 1508.3188366216455
INFO:root:current train perplexity3.278006076812744
INFO:root:current mean train loss 1509.561956632869
INFO:root:current train perplexity3.2742183208465576
INFO:root:current mean train loss 1508.3636273857137
INFO:root:current train perplexity3.2683629989624023
INFO:root:current mean train loss 1507.9498828432095
INFO:root:current train perplexity3.2739317417144775
INFO:root:current mean train loss 1508.2861317546983
INFO:root:current train perplexity3.2750403881073
INFO:root:current mean train loss 1508.5456268896558
INFO:root:current train perplexity3.2748634815216064
INFO:root:current mean train loss 1505.9067054463783
INFO:root:current train perplexity3.2704687118530273
INFO:root:current mean train loss 1505.1421680689673
INFO:root:current train perplexity3.2708165645599365
INFO:root:current mean train loss 1503.5698758206324
INFO:root:current train perplexity3.2683699131011963
INFO:root:current mean train loss 1503.4924052317274
INFO:root:current train perplexity3.2689075469970703
INFO:root:current mean train loss 1504.5890320913206
INFO:root:current train perplexity3.273289918899536
INFO:root:current mean train loss 1505.1637310843591
INFO:root:current train perplexity3.2747421264648438
INFO:root:current mean train loss 1506.1599810786583
INFO:root:current train perplexity3.2767317295074463
INFO:root:current mean train loss 1505.5598104860464
INFO:root:current train perplexity3.27546763420105
INFO:root:current mean train loss 1505.5753486551057
INFO:root:current train perplexity3.2774648666381836
INFO:root:current mean train loss 1505.82902498649
INFO:root:current train perplexity3.2767891883850098
INFO:root:current mean train loss 1505.613699874921
INFO:root:current train perplexity3.2776334285736084
INFO:root:current mean train loss 1505.7200934237856
INFO:root:current train perplexity3.278191328048706
INFO:root:current mean train loss 1506.3013889465178
INFO:root:current train perplexity3.279294729232788

100%|██████████| 1/1 [17:40<00:00, 1060.56s/it][A100%|██████████| 1/1 [17:40<00:00, 1060.56s/it]
INFO:root:final mean train loss: 1506.007728411222
INFO:root:final train perplexity: 3.279622793197632
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.94s/it][A100%|██████████| 1/1 [01:14<00:00, 74.94s/it]
INFO:root:eval mean loss: 1734.867890053607
INFO:root:eval perplexity: 4.067625522613525
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.20s/it][A100%|██████████| 1/1 [01:12<00:00, 72.21s/it]
INFO:root:eval mean loss: 2196.7804751565272
INFO:root:eval perplexity: 6.0290374755859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/35
 18%|█▊        | 35/200 [11:54:40<55:46:20, 1216.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1477.8028213825633
INFO:root:current train perplexity3.2234365940093994
INFO:root:current mean train loss 1474.9213042898277
INFO:root:current train perplexity3.2114131450653076
INFO:root:current mean train loss 1475.332930584343
INFO:root:current train perplexity3.210352897644043
INFO:root:current mean train loss 1484.580728133923
INFO:root:current train perplexity3.2206339836120605
INFO:root:current mean train loss 1487.9673649714543
INFO:root:current train perplexity3.2268199920654297
INFO:root:current mean train loss 1489.5524265276463
INFO:root:current train perplexity3.2302424907684326
INFO:root:current mean train loss 1490.4100727004345
INFO:root:current train perplexity3.232499837875366
INFO:root:current mean train loss 1493.0849507905975
INFO:root:current train perplexity3.2379250526428223
INFO:root:current mean train loss 1494.032979329427
INFO:root:current train perplexity3.2421412467956543
INFO:root:current mean train loss 1494.1708657707966
INFO:root:current train perplexity3.2438395023345947
INFO:root:current mean train loss 1494.8782981300703
INFO:root:current train perplexity3.2455992698669434
INFO:root:current mean train loss 1495.693605867063
INFO:root:current train perplexity3.24718976020813
INFO:root:current mean train loss 1495.2260971423095
INFO:root:current train perplexity3.248295783996582
INFO:root:current mean train loss 1494.1140473856988
INFO:root:current train perplexity3.2463738918304443
INFO:root:current mean train loss 1494.5457716281794
INFO:root:current train perplexity3.248044490814209
INFO:root:current mean train loss 1494.6907213850034
INFO:root:current train perplexity3.2484054565429688
INFO:root:current mean train loss 1495.0349789814234
INFO:root:current train perplexity3.249114513397217
INFO:root:current mean train loss 1495.861557568196
INFO:root:current train perplexity3.2514307498931885
INFO:root:current mean train loss 1496.0726349940396
INFO:root:current train perplexity3.2528884410858154

100%|██████████| 1/1 [17:47<00:00, 1067.30s/it][A100%|██████████| 1/1 [17:47<00:00, 1067.31s/it]
INFO:root:final mean train loss: 1495.7112984784733
INFO:root:final train perplexity: 3.253098249435425
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.52s/it][A100%|██████████| 1/1 [01:15<00:00, 75.52s/it]
INFO:root:eval mean loss: 1740.777351541722
INFO:root:eval perplexity: 4.087111949920654
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.83s/it][A100%|██████████| 1/1 [01:12<00:00, 72.83s/it]
INFO:root:eval mean loss: 2202.5151072833555
INFO:root:eval perplexity: 6.057380199432373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/36
 18%|█▊        | 36/200 [12:14:58<55:27:19, 1217.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1428.8389781605113
INFO:root:current train perplexity3.1566410064697266
INFO:root:current mean train loss 1479.2412307326858
INFO:root:current train perplexity3.2315196990966797
INFO:root:current mean train loss 1478.339859948904
INFO:root:current train perplexity3.217893362045288
INFO:root:current mean train loss 1477.6873084555868
INFO:root:current train perplexity3.2200264930725098
INFO:root:current mean train loss 1477.7767325074133
INFO:root:current train perplexity3.2157742977142334
INFO:root:current mean train loss 1478.822509765625
INFO:root:current train perplexity3.211751937866211
INFO:root:current mean train loss 1478.765583244361
INFO:root:current train perplexity3.21085786819458
INFO:root:current mean train loss 1478.9008461138033
INFO:root:current train perplexity3.2129533290863037
INFO:root:current mean train loss 1479.951072833982
INFO:root:current train perplexity3.2135210037231445
INFO:root:current mean train loss 1480.2703971318435
INFO:root:current train perplexity3.2162258625030518
INFO:root:current mean train loss 1482.7477118155368
INFO:root:current train perplexity3.2203965187072754
INFO:root:current mean train loss 1485.7682739587435
INFO:root:current train perplexity3.2256641387939453
INFO:root:current mean train loss 1486.043893097453
INFO:root:current train perplexity3.2249224185943604
INFO:root:current mean train loss 1486.2384114210884
INFO:root:current train perplexity3.2257931232452393
INFO:root:current mean train loss 1486.3789513234465
INFO:root:current train perplexity3.2264902591705322
INFO:root:current mean train loss 1486.9469456246638
INFO:root:current train perplexity3.2289624214172363
INFO:root:current mean train loss 1487.159553096873
INFO:root:current train perplexity3.230881452560425
INFO:root:current mean train loss 1486.499384226261
INFO:root:current train perplexity3.229426383972168
INFO:root:current mean train loss 1487.1626477381064
INFO:root:current train perplexity3.230522871017456
INFO:root:current mean train loss 1486.7081019682512
INFO:root:current train perplexity3.229954957962036

100%|██████████| 1/1 [17:48<00:00, 1068.94s/it][A100%|██████████| 1/1 [17:48<00:00, 1068.94s/it]
INFO:root:final mean train loss: 1486.6618330701097
INFO:root:final train perplexity: 3.229964256286621
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.57s/it][A100%|██████████| 1/1 [01:15<00:00, 75.57s/it]
INFO:root:eval mean loss: 1791.2132551044438
INFO:root:eval perplexity: 4.257269382476807
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:12<00:00, 72.72s/it][A100%|██████████| 1/1 [01:12<00:00, 72.72s/it]
INFO:root:eval mean loss: 2261.0937023839206
INFO:root:eval perplexity: 6.354636192321777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/37
 18%|█▊        | 37/200 [12:35:18<55:09:05, 1218.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1455.2860586983818
INFO:root:current train perplexity3.2327961921691895
INFO:root:current mean train loss 1464.9177570343018
INFO:root:current train perplexity3.1674227714538574
INFO:root:current mean train loss 1467.1360029254042
INFO:root:current train perplexity3.1750926971435547
INFO:root:current mean train loss 1470.032704050948
INFO:root:current train perplexity3.1914424896240234
INFO:root:current mean train loss 1465.6386085581557
INFO:root:current train perplexity3.1827642917633057
INFO:root:current mean train loss 1469.4044383655894
INFO:root:current train perplexity3.192721366882324
INFO:root:current mean train loss 1473.619132461062
INFO:root:current train perplexity3.19907808303833
INFO:root:current mean train loss 1471.8719081669062
INFO:root:current train perplexity3.1945271492004395
INFO:root:current mean train loss 1470.2385908486187
INFO:root:current train perplexity3.1942007541656494
INFO:root:current mean train loss 1471.1314172415898
INFO:root:current train perplexity3.195404291152954
INFO:root:current mean train loss 1473.0569604064704
INFO:root:current train perplexity3.198528528213501
INFO:root:current mean train loss 1472.9019451817721
INFO:root:current train perplexity3.19818115234375
INFO:root:current mean train loss 1473.4344334307245
INFO:root:current train perplexity3.1985909938812256
INFO:root:current mean train loss 1473.916356741664
INFO:root:current train perplexity3.1986300945281982
INFO:root:current mean train loss 1475.5643138725216
INFO:root:current train perplexity3.200213670730591
INFO:root:current mean train loss 1475.8006985649388
INFO:root:current train perplexity3.200979232788086
INFO:root:current mean train loss 1475.4698202897055
INFO:root:current train perplexity3.20200252532959
INFO:root:current mean train loss 1476.1411938137478
INFO:root:current train perplexity3.2032439708709717
INFO:root:current mean train loss 1477.1085661840125
INFO:root:current train perplexity3.2056140899658203
INFO:root:current mean train loss 1478.5998475007495
INFO:root:current train perplexity3.2068872451782227

100%|██████████| 1/1 [17:54<00:00, 1074.69s/it][A100%|██████████| 1/1 [17:54<00:00, 1074.69s/it]
INFO:root:final mean train loss: 1477.8207608914051
INFO:root:final train perplexity: 3.2075209617614746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.31s/it][A100%|██████████| 1/1 [01:15<00:00, 75.31s/it]
INFO:root:eval mean loss: 1762.0445855323305
INFO:root:eval perplexity: 4.158016681671143
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.03s/it][A100%|██████████| 1/1 [01:13<00:00, 73.03s/it]
INFO:root:eval mean loss: 2229.578157898382
INFO:root:eval perplexity: 6.1929426193237305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/38
 19%|█▉        | 38/200 [12:55:44<54:55:08, 1220.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1445.6672417534721
INFO:root:current train perplexity3.1733529567718506
INFO:root:current mean train loss 1463.552099609375
INFO:root:current train perplexity3.174062967300415
INFO:root:current mean train loss 1457.404612763074
INFO:root:current train perplexity3.165816068649292
INFO:root:current mean train loss 1456.8965017125226
INFO:root:current train perplexity3.1626968383789062
INFO:root:current mean train loss 1457.4834947221734
INFO:root:current train perplexity3.163205623626709
INFO:root:current mean train loss 1461.64442956207
INFO:root:current train perplexity3.169459342956543
INFO:root:current mean train loss 1462.1548228182535
INFO:root:current train perplexity3.1725518703460693
INFO:root:current mean train loss 1463.8817350041945
INFO:root:current train perplexity3.1740286350250244
INFO:root:current mean train loss 1464.266748191337
INFO:root:current train perplexity3.174358606338501
INFO:root:current mean train loss 1463.5565477482226
INFO:root:current train perplexity3.1731066703796387
INFO:root:current mean train loss 1465.3430322966508
INFO:root:current train perplexity3.1766438484191895
INFO:root:current mean train loss 1465.297751667406
INFO:root:current train perplexity3.174915313720703
INFO:root:current mean train loss 1465.8557218130334
INFO:root:current train perplexity3.176748514175415
INFO:root:current mean train loss 1465.6594814598338
INFO:root:current train perplexity3.1768248081207275
INFO:root:current mean train loss 1465.4787912758163
INFO:root:current train perplexity3.1758055686950684
INFO:root:current mean train loss 1466.0460705210862
INFO:root:current train perplexity3.176466703414917
INFO:root:current mean train loss 1466.194612061289
INFO:root:current train perplexity3.1776585578918457
INFO:root:current mean train loss 1467.4819831913726
INFO:root:current train perplexity3.1799564361572266
INFO:root:current mean train loss 1467.8474426765752
INFO:root:current train perplexity3.1819040775299072
INFO:root:current mean train loss 1468.0800153011528
INFO:root:current train perplexity3.1816039085388184

100%|██████████| 1/1 [17:49<00:00, 1069.38s/it][A100%|██████████| 1/1 [17:49<00:00, 1069.38s/it]
INFO:root:final mean train loss: 1467.6217577189311
INFO:root:final train perplexity: 3.1818246841430664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.13s/it][A100%|██████████| 1/1 [01:15<00:00, 75.13s/it]
INFO:root:eval mean loss: 1740.1019226507092
INFO:root:eval perplexity: 4.084879398345947
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.81s/it][A100%|██████████| 1/1 [01:15<00:00, 75.81s/it]
INFO:root:eval mean loss: 2207.9940761268563
INFO:root:eval perplexity: 6.084583282470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/39
 20%|█▉        | 39/200 [13:16:07<54:36:55, 1221.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1447.9576041929183
INFO:root:current train perplexity3.1279265880584717
INFO:root:current mean train loss 1458.5249505690585
INFO:root:current train perplexity3.145339012145996
INFO:root:current mean train loss 1456.1791456382693
INFO:root:current train perplexity3.1505398750305176
INFO:root:current mean train loss 1454.8060312850698
INFO:root:current train perplexity3.14106822013855
INFO:root:current mean train loss 1457.17739854953
INFO:root:current train perplexity3.1494297981262207
INFO:root:current mean train loss 1452.779216942838
INFO:root:current train perplexity3.14294171333313
INFO:root:current mean train loss 1456.683995365016
INFO:root:current train perplexity3.1502866744995117
INFO:root:current mean train loss 1454.226071175002
INFO:root:current train perplexity3.145552158355713
INFO:root:current mean train loss 1455.348739730233
INFO:root:current train perplexity3.1501832008361816
INFO:root:current mean train loss 1456.6257269655098
INFO:root:current train perplexity3.1512606143951416
INFO:root:current mean train loss 1458.7796115911165
INFO:root:current train perplexity3.1561193466186523
INFO:root:current mean train loss 1459.031142636954
INFO:root:current train perplexity3.157411813735962
INFO:root:current mean train loss 1460.4868407816214
INFO:root:current train perplexity3.1606462001800537
INFO:root:current mean train loss 1462.7660999270088
INFO:root:current train perplexity3.1631407737731934
INFO:root:current mean train loss 1462.2041151722544
INFO:root:current train perplexity3.1621882915496826
INFO:root:current mean train loss 1461.827372493573
INFO:root:current train perplexity3.160778522491455
INFO:root:current mean train loss 1461.5319581840872
INFO:root:current train perplexity3.160593032836914
INFO:root:current mean train loss 1460.1963406063776
INFO:root:current train perplexity3.159233570098877
INFO:root:current mean train loss 1460.452155386723
INFO:root:current train perplexity3.161186933517456
INFO:root:current mean train loss 1459.7381983765767
INFO:root:current train perplexity3.1613447666168213

100%|██████████| 1/1 [17:45<00:00, 1065.59s/it][A100%|██████████| 1/1 [17:45<00:00, 1065.59s/it]
INFO:root:final mean train loss: 1459.6203044481608
INFO:root:final train perplexity: 3.1618094444274902
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.24s/it][A100%|██████████| 1/1 [01:16<00:00, 76.24s/it]
INFO:root:eval mean loss: 1811.222874418218
INFO:root:eval perplexity: 4.326724052429199
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:13<00:00, 73.40s/it][A100%|██████████| 1/1 [01:13<00:00, 73.40s/it]
INFO:root:eval mean loss: 2282.970495345745
INFO:root:eval perplexity: 6.469352722167969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/40
 20%|██        | 40/200 [13:36:25<54:13:58, 1220.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1439.5187741050238
INFO:root:current train perplexity3.1016483306884766
INFO:root:current mean train loss 1445.7451908388618
INFO:root:current train perplexity3.110358715057373
INFO:root:current mean train loss 1449.3648209110384
INFO:root:current train perplexity3.125532627105713
INFO:root:current mean train loss 1446.1719416716483
INFO:root:current train perplexity3.1247591972351074
INFO:root:current mean train loss 1443.8542562018854
INFO:root:current train perplexity3.12666916847229
INFO:root:current mean train loss 1447.654283171079
INFO:root:current train perplexity3.1283648014068604
INFO:root:current mean train loss 1447.5091885326538
INFO:root:current train perplexity3.1304030418395996
INFO:root:current mean train loss 1448.741722253841
INFO:root:current train perplexity3.1314642429351807
INFO:root:current mean train loss 1449.7203109446104
INFO:root:current train perplexity3.1336629390716553
INFO:root:current mean train loss 1448.770006189551
INFO:root:current train perplexity3.1314048767089844
INFO:root:current mean train loss 1448.1095121169776
INFO:root:current train perplexity3.132650852203369
INFO:root:current mean train loss 1447.6706037707406
INFO:root:current train perplexity3.1323509216308594
INFO:root:current mean train loss 1447.7088349128348
INFO:root:current train perplexity3.132805824279785
INFO:root:current mean train loss 1449.184566948706
INFO:root:current train perplexity3.1344454288482666
INFO:root:current mean train loss 1449.7375130076275
INFO:root:current train perplexity3.1363840103149414
INFO:root:current mean train loss 1449.7884884834893
INFO:root:current train perplexity3.1362192630767822
INFO:root:current mean train loss 1450.116912896325
INFO:root:current train perplexity3.137269973754883
INFO:root:current mean train loss 1451.6193196477348
INFO:root:current train perplexity3.139869213104248
INFO:root:current mean train loss 1451.5467435096793
INFO:root:current train perplexity3.139115810394287
INFO:root:current mean train loss 1450.5173890054557
INFO:root:current train perplexity3.1384332180023193

100%|██████████| 1/1 [17:58<00:00, 1078.55s/it][A100%|██████████| 1/1 [17:58<00:00, 1078.55s/it]
INFO:root:final mean train loss: 1450.1147710249031
INFO:root:final train perplexity: 3.138195037841797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.95s/it][A100%|██████████| 1/1 [01:16<00:00, 76.98s/it]
INFO:root:eval mean loss: 1738.721901751579
INFO:root:eval perplexity: 4.080323219299316
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.57s/it][A100%|██████████| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 2206.153926335328
INFO:root:eval perplexity: 6.075433254241943
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/41
 20%|██        | 41/200 [13:56:57<54:03:36, 1224.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1433.827330271403
INFO:root:current train perplexity3.1160545349121094
INFO:root:current mean train loss 1434.6211006008848
INFO:root:current train perplexity3.1122536659240723
INFO:root:current mean train loss 1433.5419080579602
INFO:root:current train perplexity3.1005005836486816
INFO:root:current mean train loss 1434.0221705581203
INFO:root:current train perplexity3.099172353744507
INFO:root:current mean train loss 1437.3549674249464
INFO:root:current train perplexity3.10579776763916
INFO:root:current mean train loss 1438.995413761011
INFO:root:current train perplexity3.106165885925293
INFO:root:current mean train loss 1441.3448791503906
INFO:root:current train perplexity3.1090047359466553
INFO:root:current mean train loss 1440.4024215008146
INFO:root:current train perplexity3.108489513397217
INFO:root:current mean train loss 1440.4242267608643
INFO:root:current train perplexity3.1114509105682373
INFO:root:current mean train loss 1439.237790272418
INFO:root:current train perplexity3.111013412475586
INFO:root:current mean train loss 1437.6740850740975
INFO:root:current train perplexity3.1091501712799072
INFO:root:current mean train loss 1438.8394573300977
INFO:root:current train perplexity3.1113228797912598
INFO:root:current mean train loss 1440.085728680646
INFO:root:current train perplexity3.1141586303710938
INFO:root:current mean train loss 1440.5428544621072
INFO:root:current train perplexity3.1165943145751953
INFO:root:current mean train loss 1442.059891644646
INFO:root:current train perplexity3.1175785064697266
INFO:root:current mean train loss 1442.8235804813548
INFO:root:current train perplexity3.119244337081909
INFO:root:current mean train loss 1443.5299111132351
INFO:root:current train perplexity3.1227543354034424
INFO:root:current mean train loss 1443.9420142226868
INFO:root:current train perplexity3.1238276958465576
INFO:root:current mean train loss 1443.9754442947324
INFO:root:current train perplexity3.1221442222595215

100%|██████████| 1/1 [17:57<00:00, 1077.25s/it][A100%|██████████| 1/1 [17:57<00:00, 1077.25s/it]
INFO:root:final mean train loss: 1443.808430158545
INFO:root:final train perplexity: 3.1226253509521484
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.69s/it][A100%|██████████| 1/1 [01:16<00:00, 76.69s/it]
INFO:root:eval mean loss: 1744.4796726957281
INFO:root:eval perplexity: 4.099367141723633
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.43s/it][A100%|██████████| 1/1 [01:14<00:00, 74.43s/it]
INFO:root:eval mean loss: 2218.5344034830728
INFO:root:eval perplexity: 6.137260913848877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/42
 21%|██        | 42/200 [14:17:29<53:48:49, 1226.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1493.2747614933894
INFO:root:current train perplexity3.1572251319885254
INFO:root:current mean train loss 1430.398856644082
INFO:root:current train perplexity3.069359540939331
INFO:root:current mean train loss 1431.2988155167986
INFO:root:current train perplexity3.0793418884277344
INFO:root:current mean train loss 1429.5516341821835
INFO:root:current train perplexity3.0729401111602783
INFO:root:current mean train loss 1430.9474657257301
INFO:root:current train perplexity3.07422137260437
INFO:root:current mean train loss 1431.666862502665
INFO:root:current train perplexity3.0801007747650146
INFO:root:current mean train loss 1428.8477247527528
INFO:root:current train perplexity3.0783145427703857
INFO:root:current mean train loss 1427.60306723489
INFO:root:current train perplexity3.0765540599823
INFO:root:current mean train loss 1430.0335148322185
INFO:root:current train perplexity3.08310604095459
INFO:root:current mean train loss 1429.8648202985949
INFO:root:current train perplexity3.087120771408081
INFO:root:current mean train loss 1428.8918590790427
INFO:root:current train perplexity3.0844011306762695
INFO:root:current mean train loss 1429.9844544060254
INFO:root:current train perplexity3.0869123935699463
INFO:root:current mean train loss 1430.3892421738137
INFO:root:current train perplexity3.0902597904205322
INFO:root:current mean train loss 1431.0879341352104
INFO:root:current train perplexity3.0902132987976074
INFO:root:current mean train loss 1432.6899469352663
INFO:root:current train perplexity3.0926640033721924
INFO:root:current mean train loss 1433.4775948937336
INFO:root:current train perplexity3.094637393951416
INFO:root:current mean train loss 1432.4862564569369
INFO:root:current train perplexity3.0943832397460938
INFO:root:current mean train loss 1433.171732691527
INFO:root:current train perplexity3.095790147781372
INFO:root:current mean train loss 1433.8145053061871
INFO:root:current train perplexity3.096151828765869
INFO:root:current mean train loss 1434.1201644075895
INFO:root:current train perplexity3.0977799892425537

100%|██████████| 1/1 [18:02<00:00, 1082.59s/it][A100%|██████████| 1/1 [18:02<00:00, 1082.59s/it]
INFO:root:final mean train loss: 1433.6428223271835
INFO:root:final train perplexity: 3.097691297531128
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.88s/it][A100%|██████████| 1/1 [01:16<00:00, 76.88s/it]
INFO:root:eval mean loss: 1729.5592192521333
INFO:root:eval perplexity: 4.050198554992676
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.09s/it][A100%|██████████| 1/1 [01:14<00:00, 74.09s/it]
INFO:root:eval mean loss: 2203.0057355731938
INFO:root:eval perplexity: 6.059811115264893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/43
 22%|██▏       | 43/200 [14:38:05<53:36:19, 1229.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1385.502567545573
INFO:root:current train perplexity2.95174241065979
INFO:root:current mean train loss 1433.155868765024
INFO:root:current train perplexity3.068459987640381
INFO:root:current mean train loss 1427.0792836064877
INFO:root:current train perplexity3.0541183948516846
INFO:root:current mean train loss 1428.6572502367424
INFO:root:current train perplexity3.0650064945220947
INFO:root:current mean train loss 1423.9547706781432
INFO:root:current train perplexity3.060617446899414
INFO:root:current mean train loss 1425.3413470574144
INFO:root:current train perplexity3.0614187717437744
INFO:root:current mean train loss 1428.19345218719
INFO:root:current train perplexity3.06852126121521
INFO:root:current mean train loss 1428.7714141427655
INFO:root:current train perplexity3.06988525390625
INFO:root:current mean train loss 1423.8361522260918
INFO:root:current train perplexity3.0630476474761963
INFO:root:current mean train loss 1424.2768624254452
INFO:root:current train perplexity3.0669503211975098
INFO:root:current mean train loss 1423.8639099713669
INFO:root:current train perplexity3.069768190383911
INFO:root:current mean train loss 1424.2097169049018
INFO:root:current train perplexity3.071173667907715
INFO:root:current mean train loss 1423.0989581348451
INFO:root:current train perplexity3.070223569869995
INFO:root:current mean train loss 1422.3616783658365
INFO:root:current train perplexity3.069423198699951
INFO:root:current mean train loss 1423.223432804988
INFO:root:current train perplexity3.0698113441467285
INFO:root:current mean train loss 1424.1949787613614
INFO:root:current train perplexity3.072979211807251
INFO:root:current mean train loss 1423.8923959930983
INFO:root:current train perplexity3.072650194168091
INFO:root:current mean train loss 1424.4780353171288
INFO:root:current train perplexity3.074509382247925
INFO:root:current mean train loss 1425.3201238580089
INFO:root:current train perplexity3.0759754180908203
INFO:root:current mean train loss 1426.019645540702
INFO:root:current train perplexity3.078334093093872

100%|██████████| 1/1 [17:51<00:00, 1071.18s/it][A100%|██████████| 1/1 [17:51<00:00, 1071.18s/it]
INFO:root:final mean train loss: 1426.4063831200458
INFO:root:final train perplexity: 3.0800621509552
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.71s/it][A100%|██████████| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 1761.8377330590647
INFO:root:eval perplexity: 4.157321453094482
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.58s/it][A100%|██████████| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 2240.2065087717474
INFO:root:eval perplexity: 6.247007369995117
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/44
 22%|██▏       | 44/200 [14:58:29<53:12:03, 1227.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.2568800905917
INFO:root:current train perplexity3.05711030960083
INFO:root:current mean train loss 1424.3723252484588
INFO:root:current train perplexity3.057039260864258
INFO:root:current mean train loss 1415.5382984485702
INFO:root:current train perplexity3.0577712059020996
INFO:root:current mean train loss 1416.5783026527602
INFO:root:current train perplexity3.058640956878662
INFO:root:current mean train loss 1416.9910380728293
INFO:root:current train perplexity3.0579943656921387
INFO:root:current mean train loss 1416.7323988891824
INFO:root:current train perplexity3.0541164875030518
INFO:root:current mean train loss 1416.4532138641687
INFO:root:current train perplexity3.055570602416992
INFO:root:current mean train loss 1415.6625940611405
INFO:root:current train perplexity3.054081916809082
INFO:root:current mean train loss 1414.8684750751734
INFO:root:current train perplexity3.054039239883423
INFO:root:current mean train loss 1414.7882737478963
INFO:root:current train perplexity3.053030252456665
INFO:root:current mean train loss 1416.0867814990374
INFO:root:current train perplexity3.0551741123199463
INFO:root:current mean train loss 1416.303523947492
INFO:root:current train perplexity3.056093692779541
INFO:root:current mean train loss 1415.6650907490477
INFO:root:current train perplexity3.0553252696990967
INFO:root:current mean train loss 1416.3928605995093
INFO:root:current train perplexity3.0580530166625977
INFO:root:current mean train loss 1417.5886461617786
INFO:root:current train perplexity3.0601933002471924
INFO:root:current mean train loss 1418.0516717241283
INFO:root:current train perplexity3.059865713119507
INFO:root:current mean train loss 1418.8739794121414
INFO:root:current train perplexity3.059399127960205
INFO:root:current mean train loss 1419.307261178203
INFO:root:current train perplexity3.059434175491333
INFO:root:current mean train loss 1419.296294918174
INFO:root:current train perplexity3.0590367317199707
INFO:root:current mean train loss 1419.0260165253235
INFO:root:current train perplexity3.0611016750335693

100%|██████████| 1/1 [18:06<00:00, 1086.48s/it][A100%|██████████| 1/1 [18:06<00:00, 1086.48s/it]
INFO:root:final mean train loss: 1418.6219651707481
INFO:root:final train perplexity: 3.061210870742798
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.25s/it][A100%|██████████| 1/1 [01:17<00:00, 77.30s/it]
INFO:root:eval mean loss: 1749.6583273596798
INFO:root:eval perplexity: 4.116572856903076
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.37s/it][A100%|██████████| 1/1 [01:15<00:00, 75.39s/it]
INFO:root:eval mean loss: 2229.3512166341147
INFO:root:eval perplexity: 6.191793441772461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/45
 22%|██▎       | 45/200 [15:19:11<53:02:33, 1231.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1411.4265632629395
INFO:root:current train perplexity3.0627260208129883
INFO:root:current mean train loss 1402.8267010944646
INFO:root:current train perplexity3.025803804397583
INFO:root:current mean train loss 1409.1195535370798
INFO:root:current train perplexity3.0370545387268066
INFO:root:current mean train loss 1409.643070095188
INFO:root:current train perplexity3.039539098739624
INFO:root:current mean train loss 1410.1512866842336
INFO:root:current train perplexity3.037687063217163
INFO:root:current mean train loss 1408.8247641705452
INFO:root:current train perplexity3.033367395401001
INFO:root:current mean train loss 1409.8504304081562
INFO:root:current train perplexity3.0343503952026367
INFO:root:current mean train loss 1409.2441685861318
INFO:root:current train perplexity3.0334079265594482
INFO:root:current mean train loss 1407.9409411395038
INFO:root:current train perplexity3.0331480503082275
INFO:root:current mean train loss 1409.18771324316
INFO:root:current train perplexity3.0370287895202637
INFO:root:current mean train loss 1410.2510833740234
INFO:root:current train perplexity3.039849042892456
INFO:root:current mean train loss 1410.3905599797304
INFO:root:current train perplexity3.0398380756378174
INFO:root:current mean train loss 1412.0278802219825
INFO:root:current train perplexity3.0420353412628174
INFO:root:current mean train loss 1412.999175667413
INFO:root:current train perplexity3.043215036392212
INFO:root:current mean train loss 1412.2431154511664
INFO:root:current train perplexity3.041907548904419
INFO:root:current mean train loss 1412.31294496346
INFO:root:current train perplexity3.043661117553711
INFO:root:current mean train loss 1411.892560812143
INFO:root:current train perplexity3.0449554920196533
INFO:root:current mean train loss 1411.260790212895
INFO:root:current train perplexity3.044231414794922
INFO:root:current mean train loss 1411.8064035571185
INFO:root:current train perplexity3.044121503829956
INFO:root:current mean train loss 1411.9501475161303
INFO:root:current train perplexity3.0440876483917236

100%|██████████| 1/1 [18:09<00:00, 1089.33s/it][A100%|██████████| 1/1 [18:09<00:00, 1089.33s/it]
INFO:root:final mean train loss: 1411.2724440397185
INFO:root:final train perplexity: 3.0435187816619873
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.81s/it][A100%|██████████| 1/1 [01:17<00:00, 77.87s/it]
INFO:root:eval mean loss: 1756.8577824585827
INFO:root:eval perplexity: 4.14061164855957
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.08s/it][A100%|██████████| 1/1 [01:15<00:00, 75.14s/it]
INFO:root:eval mean loss: 2234.803023967337
INFO:root:eval perplexity: 6.2194600105285645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/46
 23%|██▎       | 46/200 [15:39:56<52:52:15, 1235.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.7771915388696
INFO:root:current train perplexity3.0408811569213867
INFO:root:current mean train loss 1408.4264010434651
INFO:root:current train perplexity3.036168336868286
INFO:root:current mean train loss 1400.399629097392
INFO:root:current train perplexity3.0247349739074707
INFO:root:current mean train loss 1399.727919691191
INFO:root:current train perplexity3.020864725112915
INFO:root:current mean train loss 1400.0951823593425
INFO:root:current train perplexity3.0146098136901855
INFO:root:current mean train loss 1399.2376391727626
INFO:root:current train perplexity3.0141384601593018
INFO:root:current mean train loss 1400.9773524616257
INFO:root:current train perplexity3.0171194076538086
INFO:root:current mean train loss 1401.413126106604
INFO:root:current train perplexity3.017958879470825
INFO:root:current mean train loss 1400.7050319849159
INFO:root:current train perplexity3.0177621841430664
INFO:root:current mean train loss 1401.4303343108913
INFO:root:current train perplexity3.021749973297119
INFO:root:current mean train loss 1401.4618219169172
INFO:root:current train perplexity3.022519111633301
INFO:root:current mean train loss 1402.0735793537654
INFO:root:current train perplexity3.0202486515045166
INFO:root:current mean train loss 1402.3493334065183
INFO:root:current train perplexity3.019554615020752
INFO:root:current mean train loss 1402.6598405112916
INFO:root:current train perplexity3.020251512527466
INFO:root:current mean train loss 1403.1967963013271
INFO:root:current train perplexity3.021420478820801
INFO:root:current mean train loss 1403.6870997390893
INFO:root:current train perplexity3.022437334060669
INFO:root:current mean train loss 1404.4674504340794
INFO:root:current train perplexity3.0251920223236084
INFO:root:current mean train loss 1404.9098812662303
INFO:root:current train perplexity3.0263500213623047
INFO:root:current mean train loss 1405.9875527868114
INFO:root:current train perplexity3.0279550552368164
INFO:root:current mean train loss 1404.9207886296772
INFO:root:current train perplexity3.027196168899536

100%|██████████| 1/1 [18:09<00:00, 1089.32s/it][A100%|██████████| 1/1 [18:09<00:00, 1089.32s/it]
INFO:root:final mean train loss: 1404.4043019843475
INFO:root:final train perplexity: 3.0270774364471436
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.87s/it][A100%|██████████| 1/1 [01:17<00:00, 77.92s/it]
INFO:root:eval mean loss: 1731.6144885340482
INFO:root:eval perplexity: 4.056936740875244
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.34s/it][A100%|██████████| 1/1 [01:15<00:00, 75.37s/it]
INFO:root:eval mean loss: 2207.368029871731
INFO:root:eval perplexity: 6.081468105316162
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/47
 24%|██▎       | 47/200 [16:00:42<52:38:52, 1238.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1384.6873991051498
INFO:root:current train perplexity2.9714088439941406
INFO:root:current mean train loss 1389.3951933889678
INFO:root:current train perplexity2.9860877990722656
INFO:root:current mean train loss 1391.179100907089
INFO:root:current train perplexity3.0025110244750977
INFO:root:current mean train loss 1389.2078648859533
INFO:root:current train perplexity2.9981977939605713
INFO:root:current mean train loss 1393.0783414419395
INFO:root:current train perplexity3.009242534637451
INFO:root:current mean train loss 1393.7982216519256
INFO:root:current train perplexity3.00874400138855
INFO:root:current mean train loss 1395.9866243816036
INFO:root:current train perplexity3.0087831020355225
INFO:root:current mean train loss 1396.0193386603717
INFO:root:current train perplexity3.0099332332611084
INFO:root:current mean train loss 1394.610514685412
INFO:root:current train perplexity3.008683919906616
INFO:root:current mean train loss 1395.5850887566148
INFO:root:current train perplexity3.009018659591675
INFO:root:current mean train loss 1395.245478395556
INFO:root:current train perplexity3.0081231594085693
INFO:root:current mean train loss 1396.3498229470993
INFO:root:current train perplexity3.0086710453033447
INFO:root:current mean train loss 1396.6081598455255
INFO:root:current train perplexity3.0079476833343506
INFO:root:current mean train loss 1396.2758657212592
INFO:root:current train perplexity3.0076143741607666
INFO:root:current mean train loss 1396.759425979431
INFO:root:current train perplexity3.0075862407684326
INFO:root:current mean train loss 1396.606674404407
INFO:root:current train perplexity3.0075669288635254
INFO:root:current mean train loss 1397.1912825262027
INFO:root:current train perplexity3.008286237716675
INFO:root:current mean train loss 1396.8414269978796
INFO:root:current train perplexity3.0078907012939453
INFO:root:current mean train loss 1397.200156466099
INFO:root:current train perplexity3.007831573486328

100%|██████████| 1/1 [18:08<00:00, 1088.58s/it][A100%|██████████| 1/1 [18:08<00:00, 1088.59s/it]
INFO:root:final mean train loss: 1396.9563839633959
INFO:root:final train perplexity: 3.0093493461608887
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.29s/it][A100%|██████████| 1/1 [01:17<00:00, 77.30s/it]
INFO:root:eval mean loss: 1744.0133710279533
INFO:root:eval perplexity: 4.0978217124938965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.78s/it][A100%|██████████| 1/1 [01:14<00:00, 74.79s/it]
INFO:root:eval mean loss: 2224.867010021886
INFO:root:eval perplexity: 6.169127464294434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/48
 24%|██▍       | 48/200 [16:21:25<52:21:41, 1240.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1368.4594482421876
INFO:root:current train perplexity2.9932100772857666
INFO:root:current mean train loss 1393.4376730213994
INFO:root:current train perplexity3.0158801078796387
INFO:root:current mean train loss 1385.8906318132267
INFO:root:current train perplexity3.0022964477539062
INFO:root:current mean train loss 1384.4707360646082
INFO:root:current train perplexity2.9893665313720703
INFO:root:current mean train loss 1382.3516054452184
INFO:root:current train perplexity2.9834063053131104
INFO:root:current mean train loss 1386.0103491922027
INFO:root:current train perplexity2.9848179817199707
INFO:root:current mean train loss 1387.6396823790014
INFO:root:current train perplexity2.986750602722168
INFO:root:current mean train loss 1389.07440586757
INFO:root:current train perplexity2.991187810897827
INFO:root:current mean train loss 1391.556566334356
INFO:root:current train perplexity2.9938790798187256
INFO:root:current mean train loss 1390.9739571326418
INFO:root:current train perplexity2.9908108711242676
INFO:root:current mean train loss 1389.8399624528556
INFO:root:current train perplexity2.989302158355713
INFO:root:current mean train loss 1389.1221108201373
INFO:root:current train perplexity2.9882442951202393
INFO:root:current mean train loss 1389.0376290027007
INFO:root:current train perplexity2.9891927242279053
INFO:root:current mean train loss 1389.4731942876663
INFO:root:current train perplexity2.9894015789031982
INFO:root:current mean train loss 1389.750045463643
INFO:root:current train perplexity2.9905121326446533
INFO:root:current mean train loss 1390.064606216481
INFO:root:current train perplexity2.991084098815918
INFO:root:current mean train loss 1390.77744367381
INFO:root:current train perplexity2.9934632778167725
INFO:root:current mean train loss 1391.2471464729865
INFO:root:current train perplexity2.9933130741119385
INFO:root:current mean train loss 1391.8746797251636
INFO:root:current train perplexity2.9953181743621826
INFO:root:current mean train loss 1391.429795992779
INFO:root:current train perplexity2.9945600032806396

100%|██████████| 1/1 [18:02<00:00, 1082.53s/it][A100%|██████████| 1/1 [18:02<00:00, 1082.62s/it]
INFO:root:final mean train loss: 1390.9512541170259
INFO:root:final train perplexity: 2.9951305389404297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.62s/it][A100%|██████████| 1/1 [01:17<00:00, 77.63s/it]
INFO:root:eval mean loss: 1733.8268739957336
INFO:root:eval perplexity: 4.064201831817627
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.89s/it][A100%|██████████| 1/1 [01:15<00:00, 75.90s/it]
INFO:root:eval mean loss: 2214.738020660184
INFO:root:eval perplexity: 6.118234634399414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/49
 24%|██▍       | 49/200 [16:42:04<52:00:01, 1239.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1379.7458114624023
INFO:root:current train perplexity2.9492294788360596
INFO:root:current mean train loss 1370.6063509854403
INFO:root:current train perplexity2.9309613704681396
INFO:root:current mean train loss 1367.374474361025
INFO:root:current train perplexity2.944899320602417
INFO:root:current mean train loss 1371.550993770002
INFO:root:current train perplexity2.950989246368408
INFO:root:current mean train loss 1371.16236199273
INFO:root:current train perplexity2.949450969696045
INFO:root:current mean train loss 1374.4956708635602
INFO:root:current train perplexity2.9562675952911377
INFO:root:current mean train loss 1379.1735345381724
INFO:root:current train perplexity2.966010332107544
INFO:root:current mean train loss 1381.7912219104871
INFO:root:current train perplexity2.9721946716308594
INFO:root:current mean train loss 1379.6781938993013
INFO:root:current train perplexity2.968695878982544
INFO:root:current mean train loss 1379.5036177082634
INFO:root:current train perplexity2.970947504043579
INFO:root:current mean train loss 1381.1681410915169
INFO:root:current train perplexity2.974018096923828
INFO:root:current mean train loss 1381.546760801713
INFO:root:current train perplexity2.97314190864563
INFO:root:current mean train loss 1381.4309846952365
INFO:root:current train perplexity2.9727578163146973
INFO:root:current mean train loss 1382.915430567286
INFO:root:current train perplexity2.9742953777313232
INFO:root:current mean train loss 1382.1833915497339
INFO:root:current train perplexity2.973262310028076
INFO:root:current mean train loss 1382.5455457722237
INFO:root:current train perplexity2.9747378826141357
INFO:root:current mean train loss 1382.5173355551326
INFO:root:current train perplexity2.975344181060791
INFO:root:current mean train loss 1382.6602828309938
INFO:root:current train perplexity2.975489854812622
INFO:root:current mean train loss 1383.8805893144233
INFO:root:current train perplexity2.9781715869903564
INFO:root:current mean train loss 1383.7972428537057
INFO:root:current train perplexity2.9779341220855713

100%|██████████| 1/1 [18:07<00:00, 1087.44s/it][A100%|██████████| 1/1 [18:07<00:00, 1087.44s/it]
INFO:root:final mean train loss: 1384.2311455538581
INFO:root:final train perplexity: 2.9792990684509277
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.37s/it][A100%|██████████| 1/1 [01:17<00:00, 77.38s/it]
INFO:root:eval mean loss: 1743.2478399614915
INFO:root:eval perplexity: 4.095285415649414
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.90s/it][A100%|██████████| 1/1 [01:14<00:00, 74.91s/it]
INFO:root:eval mean loss: 2225.1055999106547
INFO:root:eval perplexity: 6.170331001281738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/50
 25%|██▌       | 50/200 [17:02:46<51:41:16, 1240.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1346.1251818598532
INFO:root:current train perplexity2.910482168197632
INFO:root:current mean train loss 1365.8969349701133
INFO:root:current train perplexity2.9384067058563232
INFO:root:current mean train loss 1367.2683536881902
INFO:root:current train perplexity2.9419424533843994
INFO:root:current mean train loss 1371.300226512133
INFO:root:current train perplexity2.9504306316375732
INFO:root:current mean train loss 1372.0863915254386
INFO:root:current train perplexity2.9463939666748047
INFO:root:current mean train loss 1369.9994539076731
INFO:root:current train perplexity2.944051504135132
INFO:root:current mean train loss 1369.741734203462
INFO:root:current train perplexity2.9455463886260986
INFO:root:current mean train loss 1369.701784345273
INFO:root:current train perplexity2.9467623233795166
INFO:root:current mean train loss 1371.1412623824444
INFO:root:current train perplexity2.9498515129089355
INFO:root:current mean train loss 1371.6191174715161
INFO:root:current train perplexity2.9483373165130615
INFO:root:current mean train loss 1372.5063362521598
INFO:root:current train perplexity2.950369119644165
INFO:root:current mean train loss 1374.3030634017691
INFO:root:current train perplexity2.953396797180176
INFO:root:current mean train loss 1374.898661996003
INFO:root:current train perplexity2.9546658992767334
INFO:root:current mean train loss 1375.5468608836406
INFO:root:current train perplexity2.9562196731567383
INFO:root:current mean train loss 1375.8259270604187
INFO:root:current train perplexity2.9575459957122803
INFO:root:current mean train loss 1375.7202292652266
INFO:root:current train perplexity2.9590582847595215
INFO:root:current mean train loss 1376.067324849459
INFO:root:current train perplexity2.9599814414978027
INFO:root:current mean train loss 1376.0536966296588
INFO:root:current train perplexity2.959817886352539
INFO:root:current mean train loss 1376.936282927955
INFO:root:current train perplexity2.9614417552948
INFO:root:current mean train loss 1377.6645569818456
INFO:root:current train perplexity2.962373733520508

100%|██████████| 1/1 [18:07<00:00, 1087.82s/it][A100%|██████████| 1/1 [18:07<00:00, 1087.82s/it]
INFO:root:final mean train loss: 1377.3968129737534
INFO:root:final train perplexity: 2.9632835388183594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.42s/it][A100%|██████████| 1/1 [01:18<00:00, 78.43s/it]
INFO:root:eval mean loss: 1762.382215134641
INFO:root:eval perplexity: 4.159152030944824
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:16<00:00, 76.60s/it][A100%|██████████| 1/1 [01:16<00:00, 76.64s/it]
INFO:root:eval mean loss: 2245.718452615941
INFO:root:eval perplexity: 6.275230407714844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/51
 26%|██▌       | 51/200 [17:23:32<51:24:21, 1242.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1361.318409312855
INFO:root:current train perplexity2.94596791267395
INFO:root:current mean train loss 1349.7569602139024
INFO:root:current train perplexity2.9108688831329346
INFO:root:current mean train loss 1359.118353592722
INFO:root:current train perplexity2.936398506164551
INFO:root:current mean train loss 1362.0842888837303
INFO:root:current train perplexity2.944281578063965
INFO:root:current mean train loss 1362.4718696037587
INFO:root:current train perplexity2.93890380859375
INFO:root:current mean train loss 1365.1239516187472
INFO:root:current train perplexity2.9408247470855713
INFO:root:current mean train loss 1365.6063313068928
INFO:root:current train perplexity2.9388909339904785
INFO:root:current mean train loss 1365.6333589479132
INFO:root:current train perplexity2.9376838207244873
INFO:root:current mean train loss 1365.4815507496753
INFO:root:current train perplexity2.9362614154815674
INFO:root:current mean train loss 1365.289501877305
INFO:root:current train perplexity2.934971809387207
INFO:root:current mean train loss 1367.9891870437823
INFO:root:current train perplexity2.941147565841675
INFO:root:current mean train loss 1370.1282638628322
INFO:root:current train perplexity2.9453375339508057
INFO:root:current mean train loss 1369.5038249662136
INFO:root:current train perplexity2.9454643726348877
INFO:root:current mean train loss 1369.9927103648597
INFO:root:current train perplexity2.9455790519714355
INFO:root:current mean train loss 1371.225327391579
INFO:root:current train perplexity2.94730281829834
INFO:root:current mean train loss 1371.9714806022012
INFO:root:current train perplexity2.9484071731567383
INFO:root:current mean train loss 1371.7145246526345
INFO:root:current train perplexity2.947334051132202
INFO:root:current mean train loss 1371.4491608253556
INFO:root:current train perplexity2.947211742401123
INFO:root:current mean train loss 1371.4400889896503
INFO:root:current train perplexity2.9481945037841797
INFO:root:current mean train loss 1371.0253183514274
INFO:root:current train perplexity2.9475977420806885

100%|██████████| 1/1 [18:08<00:00, 1088.47s/it][A100%|██████████| 1/1 [18:08<00:00, 1088.47s/it]
INFO:root:final mean train loss: 1370.7303342756693
INFO:root:final train perplexity: 2.947744369506836
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:18<00:00, 78.57s/it][A100%|██████████| 1/1 [01:18<00:00, 78.59s/it]
INFO:root:eval mean loss: 1734.2302319682237
INFO:root:eval perplexity: 4.06552791595459
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.18s/it][A100%|██████████| 1/1 [01:15<00:00, 75.20s/it]
INFO:root:eval mean loss: 2214.6857135312775
INFO:root:eval perplexity: 6.117973327636719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/52
 26%|██▌       | 52/200 [17:44:17<51:05:48, 1242.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1346.674988528332
INFO:root:current train perplexity2.8929555416107178
INFO:root:current mean train loss 1351.9039673518614
INFO:root:current train perplexity2.894594430923462
INFO:root:current mean train loss 1349.2999453055984
INFO:root:current train perplexity2.9005320072174072
INFO:root:current mean train loss 1346.7964463196597
INFO:root:current train perplexity2.8986527919769287
INFO:root:current mean train loss 1351.3420010837215
INFO:root:current train perplexity2.9109771251678467
INFO:root:current mean train loss 1352.5903152806068
INFO:root:current train perplexity2.91497802734375
INFO:root:current mean train loss 1354.6784773417482
INFO:root:current train perplexity2.9178874492645264
INFO:root:current mean train loss 1356.4960698971804
INFO:root:current train perplexity2.916064500808716
INFO:root:current mean train loss 1356.8627347676156
INFO:root:current train perplexity2.9168758392333984
INFO:root:current mean train loss 1359.3096253248586
INFO:root:current train perplexity2.921116828918457
INFO:root:current mean train loss 1361.017454251248
INFO:root:current train perplexity2.9225494861602783
INFO:root:current mean train loss 1360.984516469483
INFO:root:current train perplexity2.92221999168396
INFO:root:current mean train loss 1361.9472782792102
INFO:root:current train perplexity2.92511248588562
INFO:root:current mean train loss 1361.7354400921627
INFO:root:current train perplexity2.9240689277648926
INFO:root:current mean train loss 1362.286903674588
INFO:root:current train perplexity2.925513505935669
INFO:root:current mean train loss 1362.1215527282059
INFO:root:current train perplexity2.9250478744506836
INFO:root:current mean train loss 1361.849915167298
INFO:root:current train perplexity2.9253690242767334
INFO:root:current mean train loss 1361.554876938337
INFO:root:current train perplexity2.9247539043426514
INFO:root:current mean train loss 1362.0385171704893
INFO:root:current train perplexity2.925642490386963
INFO:root:current mean train loss 1362.0409651778891
INFO:root:current train perplexity2.9276130199432373

100%|██████████| 1/1 [18:14<00:00, 1094.52s/it][A100%|██████████| 1/1 [18:14<00:00, 1094.52s/it]
INFO:root:final mean train loss: 1362.0409651778891
INFO:root:final train perplexity: 2.9276130199432373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.75s/it][A100%|██████████| 1/1 [01:17<00:00, 77.88s/it]
INFO:root:eval mean loss: 1742.888284886137
INFO:root:eval perplexity: 4.094095230102539
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.86s/it][A100%|██████████| 1/1 [01:14<00:00, 74.87s/it]
INFO:root:eval mean loss: 2227.6725883754434
INFO:root:eval perplexity: 6.183297634124756
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/53
 26%|██▋       | 53/200 [18:05:07<50:50:39, 1245.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1352.5215832519532
INFO:root:current train perplexity2.8931169509887695
INFO:root:current mean train loss 1348.539990234375
INFO:root:current train perplexity2.8906068801879883
INFO:root:current mean train loss 1346.4753828938801
INFO:root:current train perplexity2.892490863800049
INFO:root:current mean train loss 1349.883343811035
INFO:root:current train perplexity2.8962881565093994
INFO:root:current mean train loss 1349.5186796875
INFO:root:current train perplexity2.9002292156219482
INFO:root:current mean train loss 1350.7219380696615
INFO:root:current train perplexity2.9046061038970947
INFO:root:current mean train loss 1350.457666015625
INFO:root:current train perplexity2.9066991806030273
INFO:root:current mean train loss 1353.8085189819335
INFO:root:current train perplexity2.9113142490386963
INFO:root:current mean train loss 1352.7259787326389
INFO:root:current train perplexity2.9114279747009277
INFO:root:current mean train loss 1353.134386352539
INFO:root:current train perplexity2.912184000015259
INFO:root:current mean train loss 1354.2942717950993
INFO:root:current train perplexity2.913331985473633
INFO:root:current mean train loss 1353.2853240966797
INFO:root:current train perplexity2.9112162590026855
INFO:root:current mean train loss 1354.2138531963642
INFO:root:current train perplexity2.9122872352600098
INFO:root:current mean train loss 1354.40806867327
INFO:root:current train perplexity2.9140331745147705
INFO:root:current mean train loss 1354.7268631998697
INFO:root:current train perplexity2.9147191047668457
INFO:root:current mean train loss 1356.298268814087
INFO:root:current train perplexity2.918267011642456
INFO:root:current mean train loss 1357.3764953613281
INFO:root:current train perplexity2.918602466583252
INFO:root:current mean train loss 1358.470767211914
INFO:root:current train perplexity2.9203574657440186
INFO:root:current mean train loss 1358.6299737870065
INFO:root:current train perplexity2.9198720455169678

100%|██████████| 1/1 [18:05<00:00, 1085.58s/it][A100%|██████████| 1/1 [18:05<00:00, 1085.59s/it]
INFO:root:final mean train loss: 1359.318592804463
INFO:root:final train perplexity: 2.9213342666625977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.90s/it][A100%|██████████| 1/1 [01:17<00:00, 77.92s/it]
INFO:root:eval mean loss: 1740.3320589539007
INFO:root:eval perplexity: 4.085639476776123
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.53s/it][A100%|██████████| 1/1 [01:15<00:00, 75.57s/it]
INFO:root:eval mean loss: 2226.3165274545654
INFO:root:eval perplexity: 6.176445007324219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/54
 27%|██▋       | 54/200 [18:25:49<50:27:20, 1244.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1354.932653090533
INFO:root:current train perplexity2.889904737472534
INFO:root:current mean train loss 1349.655734592014
INFO:root:current train perplexity2.885866403579712
INFO:root:current mean train loss 1347.3510376539098
INFO:root:current train perplexity2.8850700855255127
INFO:root:current mean train loss 1345.6351079609867
INFO:root:current train perplexity2.8853988647460938
INFO:root:current mean train loss 1342.733008515063
INFO:root:current train perplexity2.886441469192505
INFO:root:current mean train loss 1346.385653409091
INFO:root:current train perplexity2.8934621810913086
INFO:root:current mean train loss 1345.566399325428
INFO:root:current train perplexity2.8934199810028076
INFO:root:current mean train loss 1344.1105396903874
INFO:root:current train perplexity2.8890151977539062
INFO:root:current mean train loss 1344.2631764219325
INFO:root:current train perplexity2.890828847885132
INFO:root:current mean train loss 1345.5679027761212
INFO:root:current train perplexity2.8919248580932617
INFO:root:current mean train loss 1346.9170305970379
INFO:root:current train perplexity2.891857147216797
INFO:root:current mean train loss 1347.6713313117236
INFO:root:current train perplexity2.894041061401367
INFO:root:current mean train loss 1348.7667075841273
INFO:root:current train perplexity2.897906541824341
INFO:root:current mean train loss 1348.553555314072
INFO:root:current train perplexity2.897179365158081
INFO:root:current mean train loss 1349.0451619667156
INFO:root:current train perplexity2.897885799407959
INFO:root:current mean train loss 1349.6932774583368
INFO:root:current train perplexity2.898508071899414
INFO:root:current mean train loss 1349.5084038276177
INFO:root:current train perplexity2.89951753616333
INFO:root:current mean train loss 1350.0155459422322
INFO:root:current train perplexity2.89992356300354
INFO:root:current mean train loss 1351.4088912065304
INFO:root:current train perplexity2.9029746055603027
INFO:root:current mean train loss 1351.4259010279122
INFO:root:current train perplexity2.903249502182007

100%|██████████| 1/1 [18:10<00:00, 1090.61s/it][A100%|██████████| 1/1 [18:10<00:00, 1090.62s/it]
INFO:root:final mean train loss: 1351.4364817624614
INFO:root:final train perplexity: 2.9032304286956787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.81s/it][A100%|██████████| 1/1 [01:17<00:00, 77.85s/it]
INFO:root:eval mean loss: 1747.2237470910904
INFO:root:eval perplexity: 4.108475208282471
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.72s/it][A100%|██████████| 1/1 [01:15<00:00, 75.73s/it]
INFO:root:eval mean loss: 2231.120801560422
INFO:root:eval perplexity: 6.2007598876953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/55
 28%|██▊       | 55/200 [18:46:36<50:08:42, 1244.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1367.110200769761
INFO:root:current train perplexity2.9605140686035156
INFO:root:current mean train loss 1344.7280100352727
INFO:root:current train perplexity2.895118474960327
INFO:root:current mean train loss 1339.9122215336204
INFO:root:current train perplexity2.8912954330444336
INFO:root:current mean train loss 1335.5834716065915
INFO:root:current train perplexity2.8832194805145264
INFO:root:current mean train loss 1336.1585189889654
INFO:root:current train perplexity2.877488613128662
INFO:root:current mean train loss 1338.9339707049537
INFO:root:current train perplexity2.880422353744507
INFO:root:current mean train loss 1340.642169555279
INFO:root:current train perplexity2.8806099891662598
INFO:root:current mean train loss 1342.0229270997424
INFO:root:current train perplexity2.8822035789489746
INFO:root:current mean train loss 1345.9182645582753
INFO:root:current train perplexity2.889441967010498
INFO:root:current mean train loss 1344.405046483434
INFO:root:current train perplexity2.8865630626678467
INFO:root:current mean train loss 1344.6500358065045
INFO:root:current train perplexity2.8868353366851807
INFO:root:current mean train loss 1344.4414443027827
INFO:root:current train perplexity2.8865256309509277
INFO:root:current mean train loss 1345.2692531295108
INFO:root:current train perplexity2.8884780406951904
INFO:root:current mean train loss 1345.906889039001
INFO:root:current train perplexity2.889059066772461
INFO:root:current mean train loss 1345.1871903551173
INFO:root:current train perplexity2.8886919021606445
INFO:root:current mean train loss 1345.3011328586545
INFO:root:current train perplexity2.890373945236206
INFO:root:current mean train loss 1345.9418516871103
INFO:root:current train perplexity2.8912878036499023
INFO:root:current mean train loss 1345.4726794461753
INFO:root:current train perplexity2.8904359340667725
INFO:root:current mean train loss 1346.6926335758055
INFO:root:current train perplexity2.8928096294403076
INFO:root:current mean train loss 1346.6215652734072
INFO:root:current train perplexity2.892583131790161

100%|██████████| 1/1 [18:01<00:00, 1081.09s/it][A100%|██████████| 1/1 [18:01<00:00, 1081.11s/it]
INFO:root:final mean train loss: 1346.6790447687176
INFO:root:final train perplexity: 2.89235782623291
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 76.91s/it][A100%|██████████| 1/1 [01:17<00:00, 77.01s/it]
INFO:root:eval mean loss: 1743.2473014669215
INFO:root:eval perplexity: 4.0952839851379395
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:14<00:00, 74.66s/it][A100%|██████████| 1/1 [01:14<00:00, 74.69s/it]
INFO:root:eval mean loss: 2229.9441329198526
INFO:root:eval perplexity: 6.194796085357666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/56
 28%|██▊       | 56/200 [19:07:12<49:41:38, 1242.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1336.9069489123774
INFO:root:current train perplexity2.858274459838867
INFO:root:current mean train loss 1335.8157950900247
INFO:root:current train perplexity2.8546571731567383
INFO:root:current mean train loss 1335.3718991222609
INFO:root:current train perplexity2.85420298576355
INFO:root:current mean train loss 1336.1446860115072
INFO:root:current train perplexity2.860712766647339
INFO:root:current mean train loss 1337.2500741624515
INFO:root:current train perplexity2.861029863357544
INFO:root:current mean train loss 1337.103475968764
INFO:root:current train perplexity2.8627870082855225
INFO:root:current mean train loss 1336.5899315056163
INFO:root:current train perplexity2.8634204864501953
INFO:root:current mean train loss 1337.464951841555
INFO:root:current train perplexity2.8637702465057373
INFO:root:current mean train loss 1338.6941299528128
INFO:root:current train perplexity2.867201566696167
INFO:root:current mean train loss 1339.9217377832133
INFO:root:current train perplexity2.8716583251953125
INFO:root:current mean train loss 1340.2207636374956
INFO:root:current train perplexity2.874593496322632
INFO:root:current mean train loss 1340.6707668221586
INFO:root:current train perplexity2.87642502784729
INFO:root:current mean train loss 1340.2421436873938
INFO:root:current train perplexity2.875847339630127
INFO:root:current mean train loss 1340.5823183094988
INFO:root:current train perplexity2.876065492630005
INFO:root:current mean train loss 1340.5966372026567
INFO:root:current train perplexity2.8755440711975098
INFO:root:current mean train loss 1341.8328214408043
INFO:root:current train perplexity2.878523111343384
INFO:root:current mean train loss 1341.465602789266
INFO:root:current train perplexity2.8794398307800293
INFO:root:current mean train loss 1341.4817636294886
INFO:root:current train perplexity2.8802199363708496
INFO:root:current mean train loss 1340.729358971924
INFO:root:current train perplexity2.879610300064087
INFO:root:current mean train loss 1341.7493076216924
INFO:root:current train perplexity2.881056785583496

100%|██████████| 1/1 [18:01<00:00, 1081.05s/it][A100%|██████████| 1/1 [18:01<00:00, 1081.05s/it]
INFO:root:final mean train loss: 1341.8026862978875
INFO:root:final train perplexity: 2.881256103515625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:17<00:00, 77.14s/it][A100%|██████████| 1/1 [01:17<00:00, 77.15s/it]
INFO:root:eval mean loss: 1737.2891607622728
INFO:root:eval perplexity: 4.075598239898682
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [01:15<00:00, 75.29s/it][A100%|██████████| 1/1 [01:15<00:00, 75.30s/it]
INFO:root:eval mean loss: 2223.37220710051
INFO:root:eval perplexity: 6.161590576171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_bert_not_concat_1e-5/57
 28%|██▊       | 57/200 [19:27:48<49:16:29, 1240.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 29972158 ON gr001 CANCELLED AT 2023-02-09T06:11:31 ***
