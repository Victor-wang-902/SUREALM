INFO:root:Output: large_distilbert_bert_final
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of the model checkpoint at bert-base-uncased were not used when initializing RetrievalGenerationModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing RetrievalGenerationModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RetrievalGenerationModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9842.903019452337
INFO:root:current train perplexity2270.79052734375
INFO:root:current mean train loss 8524.546754769943
INFO:root:current train perplexity815.31005859375
INFO:root:current mean train loss 7559.736659633675
INFO:root:current train perplexity386.17779541015625
INFO:root:current mean train loss 6865.406708299068
INFO:root:current train perplexity222.8441619873047
INFO:root:current mean train loss 6336.236696537607
INFO:root:current train perplexity146.9453582763672
INFO:root:current mean train loss 5916.153085138642
INFO:root:current train perplexity106.56291198730469
INFO:root:current mean train loss 5581.235903760395
INFO:root:current train perplexity82.4328842163086
INFO:root:current mean train loss 5319.659744969297
INFO:root:current train perplexity66.9088363647461
INFO:root:current mean train loss 5100.488389334504
INFO:root:current train perplexity56.112789154052734
INFO:root:current mean train loss 4912.406908128832
INFO:root:current train perplexity48.43035125732422
INFO:root:current mean train loss 4754.355696451675
INFO:root:current train perplexity42.66947555541992
INFO:root:current mean train loss 4616.898490441253
INFO:root:current train perplexity38.166175842285156
INFO:root:current mean train loss 4494.527557819416
INFO:root:current train perplexity34.675193786621094
INFO:root:current mean train loss 4387.686428154597
INFO:root:current train perplexity31.82257652282715
INFO:root:current mean train loss 4291.577509518064
INFO:root:current train perplexity29.490262985229492
INFO:root:current mean train loss 4203.921992566155
INFO:root:current train perplexity27.498796463012695
INFO:root:current mean train loss 4124.912505546691
INFO:root:current train perplexity25.806682586669922
INFO:root:current mean train loss 4051.720287990941
INFO:root:current train perplexity24.388927459716797
INFO:root:current mean train loss 3985.2745219266308
INFO:root:current train perplexity23.14977264404297

100%|██████████| 1/1 [05:02<00:00, 302.01s/it][A100%|██████████| 1/1 [05:02<00:00, 302.01s/it]
INFO:root:final mean train loss: 3933.9666536901555
INFO:root:final train perplexity: 22.254915237426758
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.05s/it][A100%|██████████| 1/1 [00:20<00:00, 20.05s/it]
INFO:root:eval mean loss: 2544.6465973549703
INFO:root:eval perplexity: 7.829984188079834
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:18<00:00, 18.97s/it][A100%|██████████| 1/1 [00:18<00:00, 18.97s/it]
INFO:root:eval mean loss: 2861.4345902246787
INFO:root:eval perplexity: 10.382896423339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/1
  0%|          | 1/200 [05:42<18:55:54, 342.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2693.420852661133
INFO:root:current train perplexity8.608922004699707
INFO:root:current mean train loss 2703.9392784381735
INFO:root:current train perplexity8.557230949401855
INFO:root:current mean train loss 2690.869150797526
INFO:root:current train perplexity8.505651473999023
INFO:root:current mean train loss 2683.4890917041635
INFO:root:current train perplexity8.45025634765625
INFO:root:current mean train loss 2686.313040513259
INFO:root:current train perplexity8.419304847717285
INFO:root:current mean train loss 2675.959933495337
INFO:root:current train perplexity8.328739166259766
INFO:root:current mean train loss 2672.1241522454598
INFO:root:current train perplexity8.287446022033691
INFO:root:current mean train loss 2664.18720607651
INFO:root:current train perplexity8.238356590270996
INFO:root:current mean train loss 2655.1397142597275
INFO:root:current train perplexity8.17752742767334
INFO:root:current mean train loss 2648.677078180438
INFO:root:current train perplexity8.12425422668457
INFO:root:current mean train loss 2641.418767493541
INFO:root:current train perplexity8.074165344238281
INFO:root:current mean train loss 2634.0862356753332
INFO:root:current train perplexity8.024198532104492
INFO:root:current mean train loss 2627.644035138582
INFO:root:current train perplexity7.974267482757568
INFO:root:current mean train loss 2620.6233294176836
INFO:root:current train perplexity7.924539089202881
INFO:root:current mean train loss 2615.2881212827174
INFO:root:current train perplexity7.8915181159973145
INFO:root:current mean train loss 2609.088370693076
INFO:root:current train perplexity7.848828315734863
INFO:root:current mean train loss 2604.0139352024194
INFO:root:current train perplexity7.808173179626465
INFO:root:current mean train loss 2598.539072032297
INFO:root:current train perplexity7.77195405960083
INFO:root:current mean train loss 2595.250752453237
INFO:root:current train perplexity7.743285655975342
INFO:root:current mean train loss 2590.0465384783975
INFO:root:current train perplexity7.707604885101318

100%|██████████| 1/1 [05:17<00:00, 317.79s/it][A100%|██████████| 1/1 [05:17<00:00, 317.79s/it]
INFO:root:final mean train loss: 2585.700444931823
INFO:root:final train perplexity: 7.684758186340332
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.76s/it][A100%|██████████| 1/1 [00:21<00:00, 21.76s/it]
INFO:root:eval mean loss: 2327.372105808123
INFO:root:eval perplexity: 6.5682148933410645
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.66s/it][A100%|██████████| 1/1 [00:19<00:00, 19.66s/it]
INFO:root:eval mean loss: 2677.8434755582334
INFO:root:eval perplexity: 8.935338973999023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/2
  1%|          | 2/200 [11:43<19:25:57, 353.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2467.946503610322
INFO:root:current train perplexity7.050166130065918
INFO:root:current mean train loss 2459.949688674812
INFO:root:current train perplexity6.968453884124756
INFO:root:current mean train loss 2446.980891018978
INFO:root:current train perplexity6.922016143798828
INFO:root:current mean train loss 2442.958134281743
INFO:root:current train perplexity6.882867336273193
INFO:root:current mean train loss 2440.9806215493286
INFO:root:current train perplexity6.859384536743164
INFO:root:current mean train loss 2439.135359715789
INFO:root:current train perplexity6.845342636108398
INFO:root:current mean train loss 2433.5957872050258
INFO:root:current train perplexity6.814844608306885
INFO:root:current mean train loss 2428.631772987189
INFO:root:current train perplexity6.793231010437012
INFO:root:current mean train loss 2428.5517159012043
INFO:root:current train perplexity6.782687664031982
INFO:root:current mean train loss 2429.811298791491
INFO:root:current train perplexity6.7796478271484375
INFO:root:current mean train loss 2427.3661328266803
INFO:root:current train perplexity6.760583400726318
INFO:root:current mean train loss 2422.256664414166
INFO:root:current train perplexity6.744038105010986
INFO:root:current mean train loss 2416.468649809281
INFO:root:current train perplexity6.730377674102783
INFO:root:current mean train loss 2414.9958117886404
INFO:root:current train perplexity6.71734094619751
INFO:root:current mean train loss 2411.7761072876233
INFO:root:current train perplexity6.701329708099365
INFO:root:current mean train loss 2409.0853045339763
INFO:root:current train perplexity6.68602180480957
INFO:root:current mean train loss 2405.964345078197
INFO:root:current train perplexity6.670041084289551
INFO:root:current mean train loss 2403.1838585291716
INFO:root:current train perplexity6.655452251434326
INFO:root:current mean train loss 2401.713354398953
INFO:root:current train perplexity6.646632194519043
INFO:root:current mean train loss 2399.234560284168
INFO:root:current train perplexity6.63455867767334

100%|██████████| 1/1 [05:09<00:00, 309.42s/it][A100%|██████████| 1/1 [05:09<00:00, 309.42s/it]
INFO:root:final mean train loss: 2398.0085862583423
INFO:root:final train perplexity: 6.627407073974609
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.95s/it][A100%|██████████| 1/1 [00:20<00:00, 20.95s/it]
INFO:root:eval mean loss: 2224.5747113599846
INFO:root:eval perplexity: 6.044238090515137
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.62s/it][A100%|██████████| 1/1 [00:19<00:00, 19.62s/it]
INFO:root:eval mean loss: 2585.4736025113584
INFO:root:eval perplexity: 8.285204887390137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/3
  2%|▏         | 3/200 [17:35<19:18:07, 352.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2315.6276220703126
INFO:root:current train perplexity6.267160415649414
INFO:root:current mean train loss 2305.918444010417
INFO:root:current train perplexity6.179446220397949
INFO:root:current mean train loss 2311.7859375
INFO:root:current train perplexity6.194015026092529
INFO:root:current mean train loss 2302.227463030134
INFO:root:current train perplexity6.186917781829834
INFO:root:current mean train loss 2305.144265136719
INFO:root:current train perplexity6.182524681091309
INFO:root:current mean train loss 2304.9324063387785
INFO:root:current train perplexity6.1720123291015625
INFO:root:current mean train loss 2308.87255859375
INFO:root:current train perplexity6.180593013763428
INFO:root:current mean train loss 2311.5502052408856
INFO:root:current train perplexity6.182572364807129
INFO:root:current mean train loss 2307.3411387005976
INFO:root:current train perplexity6.1666340827941895
INFO:root:current mean train loss 2306.177331671464
INFO:root:current train perplexity6.165480136871338
INFO:root:current mean train loss 2302.6668212890627
INFO:root:current train perplexity6.154689311981201
INFO:root:current mean train loss 2301.8140984842053
INFO:root:current train perplexity6.1483354568481445
INFO:root:current mean train loss 2300.2021243164063
INFO:root:current train perplexity6.141634464263916
INFO:root:current mean train loss 2300.1033297164354
INFO:root:current train perplexity6.135730266571045
INFO:root:current mean train loss 2298.531111681708
INFO:root:current train perplexity6.1240153312683105
INFO:root:current mean train loss 2296.3771886813256
INFO:root:current train perplexity6.117682933807373
INFO:root:current mean train loss 2295.514910703717
INFO:root:current train perplexity6.113073825836182
INFO:root:current mean train loss 2293.8297826450894
INFO:root:current train perplexity6.105037212371826
INFO:root:current mean train loss 2292.687091361381
INFO:root:current train perplexity6.096723556518555
INFO:root:current mean train loss 2292.2024362104366
INFO:root:current train perplexity6.090968132019043

100%|██████████| 1/1 [05:12<00:00, 312.56s/it][A100%|██████████| 1/1 [05:12<00:00, 312.56s/it]
INFO:root:final mean train loss: 2290.3869848361956
INFO:root:final train perplexity: 6.08810567855835
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 21.00s/it][A100%|██████████| 1/1 [00:20<00:00, 21.00s/it]
INFO:root:eval mean loss: 2154.532407936475
INFO:root:eval perplexity: 5.711371898651123
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.67s/it][A100%|██████████| 1/1 [00:20<00:00, 20.67s/it]
INFO:root:eval mean loss: 2524.8005063753603
INFO:root:eval perplexity: 7.884125232696533
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/4
  2%|▏         | 4/200 [23:31<19:16:31, 354.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2207.1222889458954
INFO:root:current train perplexity5.750941276550293
INFO:root:current mean train loss 2216.5552627654847
INFO:root:current train perplexity5.7475996017456055
INFO:root:current mean train loss 2228.732278316655
INFO:root:current train perplexity5.768048286437988
INFO:root:current mean train loss 2227.09491349306
INFO:root:current train perplexity5.773223876953125
INFO:root:current mean train loss 2222.0933038029475
INFO:root:current train perplexity5.7676005363464355
INFO:root:current mean train loss 2225.289046568425
INFO:root:current train perplexity5.774590492248535
INFO:root:current mean train loss 2221.2497681213104
INFO:root:current train perplexity5.763997554779053
INFO:root:current mean train loss 2221.477038526473
INFO:root:current train perplexity5.759547233581543
INFO:root:current mean train loss 2218.8226277528474
INFO:root:current train perplexity5.755666732788086
INFO:root:current mean train loss 2216.895189571282
INFO:root:current train perplexity5.74609899520874
INFO:root:current mean train loss 2216.5466933245957
INFO:root:current train perplexity5.748262882232666
INFO:root:current mean train loss 2215.0706561169463
INFO:root:current train perplexity5.7491865158081055
INFO:root:current mean train loss 2216.606394730879
INFO:root:current train perplexity5.747801780700684
INFO:root:current mean train loss 2217.02077275955
INFO:root:current train perplexity5.748701095581055
INFO:root:current mean train loss 2217.1820385392703
INFO:root:current train perplexity5.743987560272217
INFO:root:current mean train loss 2216.9634694268007
INFO:root:current train perplexity5.740807056427002
INFO:root:current mean train loss 2216.496575147861
INFO:root:current train perplexity5.740850925445557
INFO:root:current mean train loss 2216.4042215741283
INFO:root:current train perplexity5.739225387573242
INFO:root:current mean train loss 2214.8694564909238
INFO:root:current train perplexity5.7350239753723145
INFO:root:current mean train loss 2214.329632974847
INFO:root:current train perplexity5.729222774505615

100%|██████████| 1/1 [05:07<00:00, 307.22s/it][A100%|██████████| 1/1 [05:07<00:00, 307.22s/it]
INFO:root:final mean train loss: 2212.7947911865112
INFO:root:final train perplexity: 5.726722240447998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.90s/it][A100%|██████████| 1/1 [00:20<00:00, 20.90s/it]
INFO:root:eval mean loss: 2115.0406948657746
INFO:root:eval perplexity: 5.531842231750488
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.98s/it][A100%|██████████| 1/1 [00:19<00:00, 19.98s/it]
INFO:root:eval mean loss: 2489.9934934791945
INFO:root:eval perplexity: 7.662858486175537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/5
  2%|▎         | 5/200 [29:21<19:05:37, 352.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2169.0222749255954
INFO:root:current train perplexity5.5182881355285645
INFO:root:current mean train loss 2151.043389361838
INFO:root:current train perplexity5.462653160095215
INFO:root:current mean train loss 2157.191387767523
INFO:root:current train perplexity5.460372447967529
INFO:root:current mean train loss 2160.1875772476196
INFO:root:current train perplexity5.466277599334717
INFO:root:current mean train loss 2162.678596937952
INFO:root:current train perplexity5.481217384338379
INFO:root:current mean train loss 2160.937042236328
INFO:root:current train perplexity5.475456714630127
INFO:root:current mean train loss 2161.5927561263593
INFO:root:current train perplexity5.47755765914917
INFO:root:current mean train loss 2162.0239075641243
INFO:root:current train perplexity5.4855241775512695
INFO:root:current mean train loss 2166.282577169427
INFO:root:current train perplexity5.4936017990112305
INFO:root:current mean train loss 2165.7418578853453
INFO:root:current train perplexity5.493256568908691
INFO:root:current mean train loss 2164.0685351607544
INFO:root:current train perplexity5.485484600067139
INFO:root:current mean train loss 2162.689084852064
INFO:root:current train perplexity5.48796272277832
INFO:root:current mean train loss 2162.1622138573002
INFO:root:current train perplexity5.488757610321045
INFO:root:current mean train loss 2159.196285181652
INFO:root:current train perplexity5.482766628265381
INFO:root:current mean train loss 2159.5160826814144
INFO:root:current train perplexity5.483831405639648
INFO:root:current mean train loss 2157.3743457216206
INFO:root:current train perplexity5.481801986694336
INFO:root:current mean train loss 2157.1409554017127
INFO:root:current train perplexity5.4803242683410645
INFO:root:current mean train loss 2155.502684520499
INFO:root:current train perplexity5.474370956420898
INFO:root:current mean train loss 2155.1944890437358
INFO:root:current train perplexity5.469527721405029

100%|██████████| 1/1 [05:04<00:00, 304.43s/it][A100%|██████████| 1/1 [05:04<00:00, 304.43s/it]
INFO:root:final mean train loss: 2153.8221887385553
INFO:root:final train perplexity: 5.4664740562438965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.80s/it][A100%|██████████| 1/1 [00:20<00:00, 20.80s/it]
INFO:root:eval mean loss: 2076.8920863807625
INFO:root:eval perplexity: 5.363776683807373
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.39s/it][A100%|██████████| 1/1 [00:19<00:00, 19.39s/it]
INFO:root:eval mean loss: 2457.8946957419103
INFO:root:eval perplexity: 7.464315414428711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/6
  3%|▎         | 6/200 [35:07<18:52:56, 350.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2131.825927734375
INFO:root:current train perplexity5.05272912979126
INFO:root:current mean train loss 2114.205537399443
INFO:root:current train perplexity5.278759002685547
INFO:root:current mean train loss 2112.0588579320197
INFO:root:current train perplexity5.261368274688721
INFO:root:current mean train loss 2114.0853466147996
INFO:root:current train perplexity5.286318302154541
INFO:root:current mean train loss 2112.9447633358013
INFO:root:current train perplexity5.278514385223389
INFO:root:current mean train loss 2110.0057940759107
INFO:root:current train perplexity5.275267601013184
INFO:root:current mean train loss 2109.842250830322
INFO:root:current train perplexity5.283994674682617
INFO:root:current mean train loss 2110.8862271601397
INFO:root:current train perplexity5.282699108123779
INFO:root:current mean train loss 2108.8007512277136
INFO:root:current train perplexity5.275744915008545
INFO:root:current mean train loss 2109.44773236443
INFO:root:current train perplexity5.271773815155029
INFO:root:current mean train loss 2107.945406034395
INFO:root:current train perplexity5.271121978759766
INFO:root:current mean train loss 2108.9337432057505
INFO:root:current train perplexity5.2752461433410645
INFO:root:current mean train loss 2107.5018807569213
INFO:root:current train perplexity5.269632339477539
INFO:root:current mean train loss 2106.3446927843966
INFO:root:current train perplexity5.2692365646362305
INFO:root:current mean train loss 2107.567122552669
INFO:root:current train perplexity5.273043155670166
INFO:root:current mean train loss 2106.3633672928986
INFO:root:current train perplexity5.273454189300537
INFO:root:current mean train loss 2107.050681596097
INFO:root:current train perplexity5.271230220794678
INFO:root:current mean train loss 2106.0850800654944
INFO:root:current train perplexity5.268161773681641
INFO:root:current mean train loss 2105.6786642376414
INFO:root:current train perplexity5.266282081604004
INFO:root:current mean train loss 2106.165052611598
INFO:root:current train perplexity5.266889572143555

100%|██████████| 1/1 [05:15<00:00, 315.27s/it][A100%|██████████| 1/1 [05:15<00:00, 315.27s/it]
INFO:root:final mean train loss: 2105.9009684737257
INFO:root:final train perplexity: 5.263731956481934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.82s/it][A100%|██████████| 1/1 [00:20<00:00, 20.82s/it]
INFO:root:eval mean loss: 2056.575151159408
INFO:root:eval perplexity: 5.276363849639893
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.07s/it][A100%|██████████| 1/1 [00:21<00:00, 21.07s/it]
INFO:root:eval mean loss: 2446.308498084968
INFO:root:eval perplexity: 7.393921852111816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/7
  4%|▎         | 7/200 [41:34<19:25:21, 362.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2097.320760091146
INFO:root:current train perplexity5.192702293395996
INFO:root:current mean train loss 2066.133498369637
INFO:root:current train perplexity5.127291202545166
INFO:root:current mean train loss 2071.8495651385106
INFO:root:current train perplexity5.136226654052734
INFO:root:current mean train loss 2075.783077215998
INFO:root:current train perplexity5.125416278839111
INFO:root:current mean train loss 2073.1124644302295
INFO:root:current train perplexity5.111673831939697
INFO:root:current mean train loss 2071.725214070795
INFO:root:current train perplexity5.113289833068848
INFO:root:current mean train loss 2072.408791156263
INFO:root:current train perplexity5.112821578979492
INFO:root:current mean train loss 2072.377461637959
INFO:root:current train perplexity5.113008499145508
INFO:root:current mean train loss 2072.963964187137
INFO:root:current train perplexity5.116458415985107
INFO:root:current mean train loss 2069.879055447049
INFO:root:current train perplexity5.112881183624268
INFO:root:current mean train loss 2067.242294461413
INFO:root:current train perplexity5.105641841888428
INFO:root:current mean train loss 2065.6898718545604
INFO:root:current train perplexity5.101193428039551
INFO:root:current mean train loss 2066.3029394290716
INFO:root:current train perplexity5.104994297027588
INFO:root:current mean train loss 2066.2672151556867
INFO:root:current train perplexity5.108035564422607
INFO:root:current mean train loss 2066.243759176795
INFO:root:current train perplexity5.105204105377197
INFO:root:current mean train loss 2066.3307508787775
INFO:root:current train perplexity5.10376501083374
INFO:root:current mean train loss 2065.6544027245973
INFO:root:current train perplexity5.099091529846191
INFO:root:current mean train loss 2066.780909936836
INFO:root:current train perplexity5.101339340209961
INFO:root:current mean train loss 2067.160846437427
INFO:root:current train perplexity5.102229595184326
INFO:root:current mean train loss 2067.591569154678
INFO:root:current train perplexity5.101866245269775

100%|██████████| 1/1 [05:10<00:00, 310.44s/it][A100%|██████████| 1/1 [05:10<00:00, 310.44s/it]
INFO:root:final mean train loss: 2066.061874782083
INFO:root:final train perplexity: 5.100919246673584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.69s/it][A100%|██████████| 1/1 [00:21<00:00, 21.69s/it]
INFO:root:eval mean loss: 2032.8141652641566
INFO:root:eval perplexity: 5.175939083099365
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.39s/it][A100%|██████████| 1/1 [00:19<00:00, 19.39s/it]
INFO:root:eval mean loss: 2421.6344050414173
INFO:root:eval perplexity: 7.246214866638184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/8
  4%|▍         | 8/200 [47:27<19:10:13, 359.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2013.6319614955357
INFO:root:current train perplexity4.9060773849487305
INFO:root:current mean train loss 2013.2934461805555
INFO:root:current train perplexity4.908908843994141
INFO:root:current mean train loss 2027.8458594788897
INFO:root:current train perplexity4.949014186859131
INFO:root:current mean train loss 2031.1808870685634
INFO:root:current train perplexity4.944913387298584
INFO:root:current mean train loss 2028.906565699084
INFO:root:current train perplexity4.95389461517334
INFO:root:current mean train loss 2030.2340252172166
INFO:root:current train perplexity4.958052158355713
INFO:root:current mean train loss 2029.8746958815207
INFO:root:current train perplexity4.9611287117004395
INFO:root:current mean train loss 2031.3278594347896
INFO:root:current train perplexity4.966989517211914
INFO:root:current mean train loss 2030.9480106193862
INFO:root:current train perplexity4.968391418457031
INFO:root:current mean train loss 2031.3276354131851
INFO:root:current train perplexity4.967137813568115
INFO:root:current mean train loss 2030.796616588353
INFO:root:current train perplexity4.967902183532715
INFO:root:current mean train loss 2033.4641170283246
INFO:root:current train perplexity4.9755096435546875
INFO:root:current mean train loss 2034.5458504989563
INFO:root:current train perplexity4.97609806060791
INFO:root:current mean train loss 2033.6298328871137
INFO:root:current train perplexity4.975017547607422
INFO:root:current mean train loss 2034.1677434090539
INFO:root:current train perplexity4.973311424255371
INFO:root:current mean train loss 2034.3196687480913
INFO:root:current train perplexity4.9735541343688965
INFO:root:current mean train loss 2034.6084606298978
INFO:root:current train perplexity4.970868110656738
INFO:root:current mean train loss 2033.5802278458214
INFO:root:current train perplexity4.970454216003418
INFO:root:current mean train loss 2033.5594922806326
INFO:root:current train perplexity4.969394683837891
INFO:root:current mean train loss 2032.831393140847
INFO:root:current train perplexity4.965155601501465

100%|██████████| 1/1 [05:04<00:00, 304.20s/it][A100%|██████████| 1/1 [05:04<00:00, 304.20s/it]
INFO:root:final mean train loss: 2031.7930047308864
INFO:root:final train perplexity: 4.964905261993408
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.56s/it][A100%|██████████| 1/1 [00:20<00:00, 20.56s/it]
INFO:root:eval mean loss: 2017.3734148174312
INFO:root:eval perplexity: 5.111705780029297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.78s/it][A100%|██████████| 1/1 [00:19<00:00, 19.78s/it]
INFO:root:eval mean loss: 2411.5545221423426
INFO:root:eval perplexity: 7.18672513961792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/9
  4%|▍         | 9/200 [53:13<18:51:09, 355.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1991.1414794921875
INFO:root:current train perplexity4.799638271331787
INFO:root:current mean train loss 1995.5305256090667
INFO:root:current train perplexity4.784881591796875
INFO:root:current mean train loss 1993.7750287737165
INFO:root:current train perplexity4.817379951477051
INFO:root:current mean train loss 1994.4114175276322
INFO:root:current train perplexity4.813777446746826
INFO:root:current mean train loss 1995.5016908898817
INFO:root:current train perplexity4.829046249389648
INFO:root:current mean train loss 1996.1324515964675
INFO:root:current train perplexity4.834898948669434
INFO:root:current mean train loss 2000.911364970763
INFO:root:current train perplexity4.838027000427246
INFO:root:current mean train loss 2001.6364565910178
INFO:root:current train perplexity4.8440937995910645
INFO:root:current mean train loss 2001.8031272350902
INFO:root:current train perplexity4.843074321746826
INFO:root:current mean train loss 2000.6768026912914
INFO:root:current train perplexity4.84877347946167
INFO:root:current mean train loss 2002.103078283738
INFO:root:current train perplexity4.852062702178955
INFO:root:current mean train loss 2003.0333386527168
INFO:root:current train perplexity4.85026741027832
INFO:root:current mean train loss 2003.5915216317953
INFO:root:current train perplexity4.852324962615967
INFO:root:current mean train loss 2002.856395382853
INFO:root:current train perplexity4.850809097290039
INFO:root:current mean train loss 2002.0484985687845
INFO:root:current train perplexity4.848462104797363
INFO:root:current mean train loss 2001.4892626103667
INFO:root:current train perplexity4.84765625
INFO:root:current mean train loss 2001.4311506442239
INFO:root:current train perplexity4.848783016204834
INFO:root:current mean train loss 2001.574457386313
INFO:root:current train perplexity4.846774101257324
INFO:root:current mean train loss 2000.551681353773
INFO:root:current train perplexity4.844364643096924
INFO:root:current mean train loss 2001.5803561601483
INFO:root:current train perplexity4.844960689544678

100%|██████████| 1/1 [05:08<00:00, 308.33s/it][A100%|██████████| 1/1 [05:08<00:00, 308.33s/it]
INFO:root:final mean train loss: 2000.7568986655124
INFO:root:final train perplexity: 4.844854354858398
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.96s/it][A100%|██████████| 1/1 [00:20<00:00, 20.96s/it]
INFO:root:eval mean loss: 2004.591338462018
INFO:root:eval perplexity: 5.059136390686035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.76s/it][A100%|██████████| 1/1 [00:20<00:00, 20.76s/it]
INFO:root:eval mean loss: 2405.3816454731827
INFO:root:eval perplexity: 7.1505351066589355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/10
  5%|▌         | 10/200 [59:06<18:42:03, 354.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1978.1401508718297
INFO:root:current train perplexity4.716547012329102
INFO:root:current mean train loss 1969.7700231427978
INFO:root:current train perplexity4.7309250831604
INFO:root:current mean train loss 1968.0615937754123
INFO:root:current train perplexity4.749576568603516
INFO:root:current mean train loss 1968.734925143441
INFO:root:current train perplexity4.734786033630371
INFO:root:current mean train loss 1971.4191345344982
INFO:root:current train perplexity4.742843151092529
INFO:root:current mean train loss 1973.2930453389306
INFO:root:current train perplexity4.740000247955322
INFO:root:current mean train loss 1973.1659999328522
INFO:root:current train perplexity4.735420227050781
INFO:root:current mean train loss 1973.024856944388
INFO:root:current train perplexity4.738654136657715
INFO:root:current mean train loss 1975.1236743641668
INFO:root:current train perplexity4.743527412414551
INFO:root:current mean train loss 1975.1158590877758
INFO:root:current train perplexity4.744431972503662
INFO:root:current mean train loss 1974.5478769129297
INFO:root:current train perplexity4.741726875305176
INFO:root:current mean train loss 1973.86776736009
INFO:root:current train perplexity4.741580486297607
INFO:root:current mean train loss 1973.2822031873338
INFO:root:current train perplexity4.735678672790527
INFO:root:current mean train loss 1974.4195391680744
INFO:root:current train perplexity4.7359724044799805
INFO:root:current mean train loss 1974.2126943485682
INFO:root:current train perplexity4.734363079071045
INFO:root:current mean train loss 1973.3413006580126
INFO:root:current train perplexity4.735581398010254
INFO:root:current mean train loss 1974.0405800044
INFO:root:current train perplexity4.737468719482422
INFO:root:current mean train loss 1973.2547791665932
INFO:root:current train perplexity4.737075328826904
INFO:root:current mean train loss 1973.6855880876096
INFO:root:current train perplexity4.739991664886475
INFO:root:current mean train loss 1974.2746464858628
INFO:root:current train perplexity4.74200963973999

100%|██████████| 1/1 [05:06<00:00, 306.10s/it][A100%|██████████| 1/1 [05:06<00:00, 306.10s/it]
INFO:root:final mean train loss: 1973.3018745272316
INFO:root:final train perplexity: 4.7410783767700195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.47s/it][A100%|██████████| 1/1 [00:20<00:00, 20.47s/it]
INFO:root:eval mean loss: 1997.8022149268618
INFO:root:eval perplexity: 5.031434059143066
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.71s/it][A100%|██████████| 1/1 [00:19<00:00, 19.71s/it]
INFO:root:eval mean loss: 2402.5204653909022
INFO:root:eval perplexity: 7.133822441101074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/11
  6%|▌         | 11/200 [1:04:54<18:30:02, 352.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1932.03535922738
INFO:root:current train perplexity4.621697902679443
INFO:root:current mean train loss 1937.8658900107107
INFO:root:current train perplexity4.627457141876221
INFO:root:current mean train loss 1942.4531412191325
INFO:root:current train perplexity4.626211643218994
INFO:root:current mean train loss 1942.853718970106
INFO:root:current train perplexity4.626089572906494
INFO:root:current mean train loss 1941.2688365041474
INFO:root:current train perplexity4.626528739929199
INFO:root:current mean train loss 1940.1770838193927
INFO:root:current train perplexity4.628352642059326
INFO:root:current mean train loss 1942.3704123983578
INFO:root:current train perplexity4.638332843780518
INFO:root:current mean train loss 1943.5643260849038
INFO:root:current train perplexity4.6383514404296875
INFO:root:current mean train loss 1943.8850173433532
INFO:root:current train perplexity4.635972023010254
INFO:root:current mean train loss 1945.156848342617
INFO:root:current train perplexity4.637880802154541
INFO:root:current mean train loss 1945.858708783847
INFO:root:current train perplexity4.639784812927246
INFO:root:current mean train loss 1944.4243911305728
INFO:root:current train perplexity4.639533519744873
INFO:root:current mean train loss 1945.9984741210938
INFO:root:current train perplexity4.643692493438721
INFO:root:current mean train loss 1945.814562336535
INFO:root:current train perplexity4.642605304718018
INFO:root:current mean train loss 1946.4105772529283
INFO:root:current train perplexity4.645017147064209
INFO:root:current mean train loss 1947.4626218548037
INFO:root:current train perplexity4.647845268249512
INFO:root:current mean train loss 1947.732691284035
INFO:root:current train perplexity4.648791790008545
INFO:root:current mean train loss 1947.7108692882575
INFO:root:current train perplexity4.648454666137695
INFO:root:current mean train loss 1948.8468265472768
INFO:root:current train perplexity4.648251533508301

100%|██████████| 1/1 [05:10<00:00, 310.51s/it][A100%|██████████| 1/1 [05:10<00:00, 310.51s/it]
INFO:root:final mean train loss: 1948.2090789287065
INFO:root:final train perplexity: 4.6481757164001465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.38s/it][A100%|██████████| 1/1 [00:21<00:00, 21.38s/it]
INFO:root:eval mean loss: 1989.471574499252
INFO:root:eval perplexity: 4.997650623321533
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.05s/it][A100%|██████████| 1/1 [00:20<00:00, 20.06s/it]
INFO:root:eval mean loss: 2394.939794662151
INFO:root:eval perplexity: 7.089731693267822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/12
  6%|▌         | 12/200 [1:11:45<19:20:50, 370.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2033.6555989583333
INFO:root:current train perplexity4.717243671417236
INFO:root:current mean train loss 1925.2413021939471
INFO:root:current train perplexity4.5627360343933105
INFO:root:current mean train loss 1924.8880158222369
INFO:root:current train perplexity4.547693729400635
INFO:root:current mean train loss 1926.9600338573896
INFO:root:current train perplexity4.537368297576904
INFO:root:current mean train loss 1925.6925233599566
INFO:root:current train perplexity4.544287204742432
INFO:root:current mean train loss 1921.8503383992918
INFO:root:current train perplexity4.5403547286987305
INFO:root:current mean train loss 1920.615979752928
INFO:root:current train perplexity4.536697864532471
INFO:root:current mean train loss 1919.3543546630513
INFO:root:current train perplexity4.536802291870117
INFO:root:current mean train loss 1919.158721505779
INFO:root:current train perplexity4.542255401611328
INFO:root:current mean train loss 1920.1038891358232
INFO:root:current train perplexity4.544387340545654
INFO:root:current mean train loss 1922.1935870854234
INFO:root:current train perplexity4.546687602996826
INFO:root:current mean train loss 1923.126425666152
INFO:root:current train perplexity4.552098751068115
INFO:root:current mean train loss 1923.5731728824098
INFO:root:current train perplexity4.553127288818359
INFO:root:current mean train loss 1924.6511647362756
INFO:root:current train perplexity4.556412696838379
INFO:root:current mean train loss 1925.7188075113875
INFO:root:current train perplexity4.558529853820801
INFO:root:current mean train loss 1925.8919778444413
INFO:root:current train perplexity4.559488296508789
INFO:root:current mean train loss 1924.6305843170032
INFO:root:current train perplexity4.559703826904297
INFO:root:current mean train loss 1925.2127069819344
INFO:root:current train perplexity4.560763359069824
INFO:root:current mean train loss 1924.4220756069528
INFO:root:current train perplexity4.5607171058654785
INFO:root:current mean train loss 1924.4733407546266
INFO:root:current train perplexity4.561966419219971

100%|██████████| 1/1 [05:08<00:00, 308.27s/it][A100%|██████████| 1/1 [05:08<00:00, 308.28s/it]
INFO:root:final mean train loss: 1924.1716463720925
INFO:root:final train perplexity: 4.56088924407959
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.36s/it][A100%|██████████| 1/1 [00:20<00:00, 20.36s/it]
INFO:root:eval mean loss: 1982.209539318761
INFO:root:eval perplexity: 4.968384742736816
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.63s/it][A100%|██████████| 1/1 [00:19<00:00, 19.63s/it]
INFO:root:eval mean loss: 2394.198763367132
INFO:root:eval perplexity: 7.085436820983887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/13
  6%|▋         | 13/200 [1:18:36<19:52:18, 382.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1876.3407043457032
INFO:root:current train perplexity4.449944496154785
INFO:root:current mean train loss 1880.5715779622396
INFO:root:current train perplexity4.429011821746826
INFO:root:current mean train loss 1887.8370677601208
INFO:root:current train perplexity4.444535255432129
INFO:root:current mean train loss 1891.1982418060302
INFO:root:current train perplexity4.444392681121826
INFO:root:current mean train loss 1892.7309305826823
INFO:root:current train perplexity4.44102144241333
INFO:root:current mean train loss 1895.8484313964843
INFO:root:current train perplexity4.448997974395752
INFO:root:current mean train loss 1899.0626254174017
INFO:root:current train perplexity4.454940319061279
INFO:root:current mean train loss 1901.523489379883
INFO:root:current train perplexity4.461193084716797
INFO:root:current mean train loss 1900.8688029963796
INFO:root:current train perplexity4.464844226837158
INFO:root:current mean train loss 1900.5138130519701
INFO:root:current train perplexity4.468747615814209
INFO:root:current mean train loss 1902.0803688198912
INFO:root:current train perplexity4.471816539764404
INFO:root:current mean train loss 1902.2358004978726
INFO:root:current train perplexity4.4747395515441895
INFO:root:current mean train loss 1903.0842679383325
INFO:root:current train perplexity4.476176738739014
INFO:root:current mean train loss 1902.8669196851326
INFO:root:current train perplexity4.478327751159668
INFO:root:current mean train loss 1903.7328569439096
INFO:root:current train perplexity4.479074001312256
INFO:root:current mean train loss 1905.3370489823192
INFO:root:current train perplexity4.4830145835876465
INFO:root:current mean train loss 1903.7570726182726
INFO:root:current train perplexity4.481233596801758
INFO:root:current mean train loss 1902.8182454663654
INFO:root:current train perplexity4.482913970947266
INFO:root:current mean train loss 1903.3203076037732
INFO:root:current train perplexity4.482061386108398
INFO:root:current mean train loss 1903.3340963363648
INFO:root:current train perplexity4.483551979064941

100%|██████████| 1/1 [05:09<00:00, 309.69s/it][A100%|██████████| 1/1 [05:09<00:00, 309.69s/it]
INFO:root:final mean train loss: 1902.8048647881997
INFO:root:final train perplexity: 4.484676361083984
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.83s/it][A100%|██████████| 1/1 [00:20<00:00, 20.83s/it]
INFO:root:eval mean loss: 1973.1284582259807
INFO:root:eval perplexity: 4.932028293609619
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.71s/it][A100%|██████████| 1/1 [00:19<00:00, 19.71s/it]
INFO:root:eval mean loss: 2385.3313494743184
INFO:root:eval perplexity: 7.034237861633301
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/14
  7%|▋         | 14/200 [1:24:39<19:27:50, 376.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1875.6389622043919
INFO:root:current train perplexity4.445241928100586
INFO:root:current mean train loss 1872.2911011633212
INFO:root:current train perplexity4.4102983474731445
INFO:root:current mean train loss 1867.1589087635152
INFO:root:current train perplexity4.397163391113281
INFO:root:current mean train loss 1867.72969503431
INFO:root:current train perplexity4.403648853302002
INFO:root:current mean train loss 1873.241405076784
INFO:root:current train perplexity4.411141395568848
INFO:root:current mean train loss 1870.4378177919868
INFO:root:current train perplexity4.404752731323242
INFO:root:current mean train loss 1872.791854786536
INFO:root:current train perplexity4.401245594024658
INFO:root:current mean train loss 1873.2057905054803
INFO:root:current train perplexity4.399284362792969
INFO:root:current mean train loss 1876.446356587655
INFO:root:current train perplexity4.405384540557861
INFO:root:current mean train loss 1876.526496683648
INFO:root:current train perplexity4.401717662811279
INFO:root:current mean train loss 1876.718510921114
INFO:root:current train perplexity4.40462589263916
INFO:root:current mean train loss 1875.201523592101
INFO:root:current train perplexity4.4011359214782715
INFO:root:current mean train loss 1875.9461068945154
INFO:root:current train perplexity4.404146194458008
INFO:root:current mean train loss 1877.3408204038017
INFO:root:current train perplexity4.4046525955200195
INFO:root:current mean train loss 1878.562102103449
INFO:root:current train perplexity4.409390926361084
INFO:root:current mean train loss 1881.7475079230542
INFO:root:current train perplexity4.413203239440918
INFO:root:current mean train loss 1881.7326441667542
INFO:root:current train perplexity4.412107467651367
INFO:root:current mean train loss 1881.5440510083838
INFO:root:current train perplexity4.410919666290283
INFO:root:current mean train loss 1883.0414111407865
INFO:root:current train perplexity4.41121768951416
INFO:root:current mean train loss 1882.6332794173697
INFO:root:current train perplexity4.4117536544799805

100%|██████████| 1/1 [05:02<00:00, 302.44s/it][A100%|██████████| 1/1 [05:02<00:00, 302.44s/it]
INFO:root:final mean train loss: 1882.3054265086241
INFO:root:final train perplexity: 4.412755012512207
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.27s/it][A100%|██████████| 1/1 [00:21<00:00, 21.28s/it]
INFO:root:eval mean loss: 1971.2366739908855
INFO:root:eval perplexity: 4.9244890213012695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.54s/it][A100%|██████████| 1/1 [00:19<00:00, 19.54s/it]
INFO:root:eval mean loss: 2389.7473516802415
INFO:root:eval perplexity: 7.059689044952393
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/15
  8%|▊         | 15/200 [1:30:24<18:52:04, 367.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1850.994622124566
INFO:root:current train perplexity4.311149597167969
INFO:root:current mean train loss 1842.4392945921265
INFO:root:current train perplexity4.328695297241211
INFO:root:current mean train loss 1855.421516478531
INFO:root:current train perplexity4.335400104522705
INFO:root:current mean train loss 1856.7715685138594
INFO:root:current train perplexity4.340219020843506
INFO:root:current mean train loss 1857.5170696779494
INFO:root:current train perplexity4.337849140167236
INFO:root:current mean train loss 1859.719591712263
INFO:root:current train perplexity4.3488054275512695
INFO:root:current mean train loss 1861.9497242032205
INFO:root:current train perplexity4.343891620635986
INFO:root:current mean train loss 1861.601327263708
INFO:root:current train perplexity4.346258640289307
INFO:root:current mean train loss 1862.5914375251573
INFO:root:current train perplexity4.346088409423828
INFO:root:current mean train loss 1860.5247235887955
INFO:root:current train perplexity4.343649387359619
INFO:root:current mean train loss 1860.8519517583684
INFO:root:current train perplexity4.341081142425537
INFO:root:current mean train loss 1862.438415738904
INFO:root:current train perplexity4.3414740562438965
INFO:root:current mean train loss 1862.7456268845942
INFO:root:current train perplexity4.342576503753662
INFO:root:current mean train loss 1862.6490579489591
INFO:root:current train perplexity4.3430352210998535
INFO:root:current mean train loss 1862.4564975491908
INFO:root:current train perplexity4.344161510467529
INFO:root:current mean train loss 1861.201561415978
INFO:root:current train perplexity4.344529628753662
INFO:root:current mean train loss 1860.2098690526518
INFO:root:current train perplexity4.34410285949707
INFO:root:current mean train loss 1861.6285150543179
INFO:root:current train perplexity4.346359729766846
INFO:root:current mean train loss 1862.337657611604
INFO:root:current train perplexity4.345457553863525
INFO:root:current mean train loss 1862.7747840842303
INFO:root:current train perplexity4.344056606292725

100%|██████████| 1/1 [05:06<00:00, 306.34s/it][A100%|██████████| 1/1 [05:06<00:00, 306.34s/it]
INFO:root:final mean train loss: 1862.4331212277011
INFO:root:final train perplexity: 4.34413480758667
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.99s/it][A100%|██████████| 1/1 [00:19<00:00, 19.99s/it]
INFO:root:eval mean loss: 1967.798208683095
INFO:root:eval perplexity: 4.91081428527832
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.46s/it][A100%|██████████| 1/1 [00:20<00:00, 20.46s/it]
INFO:root:eval mean loss: 2385.8137410481772
INFO:root:eval perplexity: 7.037015438079834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/16
  8%|▊         | 16/200 [1:36:12<18:28:40, 361.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1830.321074149978
INFO:root:current train perplexity4.244903564453125
INFO:root:current mean train loss 1831.4543828239218
INFO:root:current train perplexity4.262007713317871
INFO:root:current mean train loss 1827.5330697935885
INFO:root:current train perplexity4.248167514801025
INFO:root:current mean train loss 1833.7718364376262
INFO:root:current train perplexity4.254566669464111
INFO:root:current mean train loss 1837.7307934933153
INFO:root:current train perplexity4.260555267333984
INFO:root:current mean train loss 1839.3545043731528
INFO:root:current train perplexity4.2660112380981445
INFO:root:current mean train loss 1839.1646486558075
INFO:root:current train perplexity4.2653913497924805
INFO:root:current mean train loss 1838.2549030783885
INFO:root:current train perplexity4.265674114227295
INFO:root:current mean train loss 1840.292793002386
INFO:root:current train perplexity4.266517162322998
INFO:root:current mean train loss 1842.2493342076468
INFO:root:current train perplexity4.270413875579834
INFO:root:current mean train loss 1840.9358507400357
INFO:root:current train perplexity4.267849922180176
INFO:root:current mean train loss 1842.5042369135622
INFO:root:current train perplexity4.272974967956543
INFO:root:current mean train loss 1844.4099851978942
INFO:root:current train perplexity4.276376247406006
INFO:root:current mean train loss 1843.4819083071206
INFO:root:current train perplexity4.275006294250488
INFO:root:current mean train loss 1843.6973252079263
INFO:root:current train perplexity4.276386737823486
INFO:root:current mean train loss 1845.3640085435231
INFO:root:current train perplexity4.278274059295654
INFO:root:current mean train loss 1846.4085916899264
INFO:root:current train perplexity4.280412673950195
INFO:root:current mean train loss 1846.0183888483289
INFO:root:current train perplexity4.2806549072265625
INFO:root:current mean train loss 1845.275779671111
INFO:root:current train perplexity4.280863285064697
INFO:root:current mean train loss 1844.6622414165556
INFO:root:current train perplexity4.280954360961914

100%|██████████| 1/1 [04:55<00:00, 295.77s/it][A100%|██████████| 1/1 [04:55<00:00, 295.77s/it]
INFO:root:final mean train loss: 1844.0279576103915
INFO:root:final train perplexity: 4.281534194946289
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.15s/it][A100%|██████████| 1/1 [00:20<00:00, 20.15s/it]
INFO:root:eval mean loss: 1963.4182124577515
INFO:root:eval perplexity: 4.893449306488037
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:18<00:00, 18.79s/it][A100%|██████████| 1/1 [00:18<00:00, 18.79s/it]
INFO:root:eval mean loss: 2386.3659672297485
INFO:root:eval perplexity: 7.040192127227783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/17
  8%|▊         | 17/200 [1:41:49<17:59:35, 353.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1822.1801965886896
INFO:root:current train perplexity4.171582221984863
INFO:root:current mean train loss 1818.1888362803359
INFO:root:current train perplexity4.166334629058838
INFO:root:current mean train loss 1814.8147523668076
INFO:root:current train perplexity4.183187007904053
INFO:root:current mean train loss 1815.8887319663136
INFO:root:current train perplexity4.195329666137695
INFO:root:current mean train loss 1821.0551712786564
INFO:root:current train perplexity4.199651718139648
INFO:root:current mean train loss 1821.4938628527582
INFO:root:current train perplexity4.204231262207031
INFO:root:current mean train loss 1823.827981638354
INFO:root:current train perplexity4.206140518188477
INFO:root:current mean train loss 1822.5595577646634
INFO:root:current train perplexity4.202467441558838
INFO:root:current mean train loss 1825.9927299430778
INFO:root:current train perplexity4.206411361694336
INFO:root:current mean train loss 1825.3988955107777
INFO:root:current train perplexity4.207181453704834
INFO:root:current mean train loss 1827.2558152815875
INFO:root:current train perplexity4.2094502449035645
INFO:root:current mean train loss 1826.5871651903146
INFO:root:current train perplexity4.21061897277832
INFO:root:current mean train loss 1825.5262513723433
INFO:root:current train perplexity4.2106547355651855
INFO:root:current mean train loss 1826.1101198223894
INFO:root:current train perplexity4.211181640625
INFO:root:current mean train loss 1825.5596338087512
INFO:root:current train perplexity4.211192607879639
INFO:root:current mean train loss 1825.7163477976917
INFO:root:current train perplexity4.214934825897217
INFO:root:current mean train loss 1825.9762541422913
INFO:root:current train perplexity4.216727256774902
INFO:root:current mean train loss 1826.5509401189133
INFO:root:current train perplexity4.2190260887146
INFO:root:current mean train loss 1825.672341168937
INFO:root:current train perplexity4.219135761260986

100%|██████████| 1/1 [05:03<00:00, 303.26s/it][A100%|██████████| 1/1 [05:03<00:00, 303.26s/it]
INFO:root:final mean train loss: 1825.903034435278
INFO:root:final train perplexity: 4.220767974853516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.59s/it][A100%|██████████| 1/1 [00:20<00:00, 20.59s/it]
INFO:root:eval mean loss: 1961.9155766913232
INFO:root:eval perplexity: 4.887505531311035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.92s/it][A100%|██████████| 1/1 [00:19<00:00, 19.92s/it]
INFO:root:eval mean loss: 2386.758669156555
INFO:root:eval perplexity: 7.042454242706299
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/18
  9%|▉         | 18/200 [1:47:34<17:45:57, 351.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1839.0994384765625
INFO:root:current train perplexity4.183740615844727
INFO:root:current mean train loss 1802.319695172991
INFO:root:current train perplexity4.128157138824463
INFO:root:current mean train loss 1798.3022413300305
INFO:root:current train perplexity4.14274263381958
INFO:root:current mean train loss 1798.6748307024845
INFO:root:current train perplexity4.142970561981201
INFO:root:current mean train loss 1793.1310040509259
INFO:root:current train perplexity4.137363433837891
INFO:root:current mean train loss 1797.5116575939821
INFO:root:current train perplexity4.14391565322876
INFO:root:current mean train loss 1798.7112040369964
INFO:root:current train perplexity4.1470561027526855
INFO:root:current mean train loss 1800.5995905017176
INFO:root:current train perplexity4.147439479827881
INFO:root:current mean train loss 1804.2895944536103
INFO:root:current train perplexity4.150613784790039
INFO:root:current mean train loss 1805.4681664904178
INFO:root:current train perplexity4.152222633361816
INFO:root:current mean train loss 1806.1440235346704
INFO:root:current train perplexity4.154371738433838
INFO:root:current mean train loss 1807.485362720058
INFO:root:current train perplexity4.157929420471191
INFO:root:current mean train loss 1807.7818277319436
INFO:root:current train perplexity4.1588921546936035
INFO:root:current mean train loss 1809.0864478568008
INFO:root:current train perplexity4.162230968475342
INFO:root:current mean train loss 1809.1696307307884
INFO:root:current train perplexity4.163482666015625
INFO:root:current mean train loss 1808.1512276785713
INFO:root:current train perplexity4.160184860229492
INFO:root:current mean train loss 1807.5815538447966
INFO:root:current train perplexity4.159792900085449
INFO:root:current mean train loss 1808.0272684315432
INFO:root:current train perplexity4.161410331726074
INFO:root:current mean train loss 1807.9390148215678
INFO:root:current train perplexity4.161753177642822
INFO:root:current mean train loss 1808.7140924889272
INFO:root:current train perplexity4.162984371185303

100%|██████████| 1/1 [05:07<00:00, 307.84s/it][A100%|██████████| 1/1 [05:07<00:00, 307.84s/it]
INFO:root:final mean train loss: 1808.680956803484
INFO:root:final train perplexity: 4.163826942443848
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.66s/it][A100%|██████████| 1/1 [00:20<00:00, 20.66s/it]
INFO:root:eval mean loss: 1962.9399050448803
INFO:root:eval perplexity: 4.891556739807129
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.59s/it][A100%|██████████| 1/1 [00:19<00:00, 19.60s/it]
INFO:root:eval mean loss: 2394.2101786728444
INFO:root:eval perplexity: 7.085503101348877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/19
 10%|▉         | 19/200 [1:53:24<17:38:37, 350.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1795.467440518466
INFO:root:current train perplexity4.020450592041016
INFO:root:current mean train loss 1781.1665659419825
INFO:root:current train perplexity4.050413608551025
INFO:root:current mean train loss 1780.3406723984726
INFO:root:current train perplexity4.067982196807861
INFO:root:current mean train loss 1778.9588194663481
INFO:root:current train perplexity4.056998252868652
INFO:root:current mean train loss 1780.4088296754664
INFO:root:current train perplexity4.066761016845703
INFO:root:current mean train loss 1781.8103146607848
INFO:root:current train perplexity4.071064472198486
INFO:root:current mean train loss 1785.2077501303131
INFO:root:current train perplexity4.084351062774658
INFO:root:current mean train loss 1786.38499979035
INFO:root:current train perplexity4.087205410003662
INFO:root:current mean train loss 1786.956836561217
INFO:root:current train perplexity4.08303689956665
INFO:root:current mean train loss 1785.6316231750357
INFO:root:current train perplexity4.083433151245117
INFO:root:current mean train loss 1786.6775042091563
INFO:root:current train perplexity4.088619709014893
INFO:root:current mean train loss 1787.7640058820048
INFO:root:current train perplexity4.089742660522461
INFO:root:current mean train loss 1788.2826577044548
INFO:root:current train perplexity4.095903396606445
INFO:root:current mean train loss 1789.4366627749444
INFO:root:current train perplexity4.0982866287231445
INFO:root:current mean train loss 1790.273957028503
INFO:root:current train perplexity4.100677967071533
INFO:root:current mean train loss 1790.934278851583
INFO:root:current train perplexity4.103299140930176
INFO:root:current mean train loss 1791.0706005919583
INFO:root:current train perplexity4.104448318481445
INFO:root:current mean train loss 1790.6887054620581
INFO:root:current train perplexity4.105295658111572
INFO:root:current mean train loss 1790.1038852528343
INFO:root:current train perplexity4.104091167449951
INFO:root:current mean train loss 1791.7507225139827
INFO:root:current train perplexity4.107006072998047

100%|██████████| 1/1 [05:05<00:00, 305.08s/it][A100%|██████████| 1/1 [05:05<00:00, 305.08s/it]
INFO:root:final mean train loss: 1792.0258554217194
INFO:root:final train perplexity: 4.109492301940918
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.13s/it][A100%|██████████| 1/1 [00:20<00:00, 20.13s/it]
INFO:root:eval mean loss: 1965.024573359929
INFO:root:eval perplexity: 4.899810791015625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.03s/it][A100%|██████████| 1/1 [00:20<00:00, 20.03s/it]
INFO:root:eval mean loss: 2396.480663543052
INFO:root:eval perplexity: 7.098672389984131
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/20
 10%|█         | 20/200 [1:59:11<17:29:06, 349.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1780.5169646434294
INFO:root:current train perplexity4.055785655975342
INFO:root:current mean train loss 1777.4058548083408
INFO:root:current train perplexity4.042381763458252
INFO:root:current mean train loss 1773.286786067436
INFO:root:current train perplexity4.04987096786499
INFO:root:current mean train loss 1767.8619978913164
INFO:root:current train perplexity4.042953014373779
INFO:root:current mean train loss 1765.0023610456115
INFO:root:current train perplexity4.042327880859375
INFO:root:current mean train loss 1769.1528913678399
INFO:root:current train perplexity4.044761657714844
INFO:root:current mean train loss 1773.3291905840424
INFO:root:current train perplexity4.045869827270508
INFO:root:current mean train loss 1773.7448533900858
INFO:root:current train perplexity4.0491766929626465
INFO:root:current mean train loss 1773.3787041574326
INFO:root:current train perplexity4.047687530517578
INFO:root:current mean train loss 1774.525827036117
INFO:root:current train perplexity4.052844524383545
INFO:root:current mean train loss 1773.8114434279883
INFO:root:current train perplexity4.049811363220215
INFO:root:current mean train loss 1773.7250358172946
INFO:root:current train perplexity4.050336837768555
INFO:root:current mean train loss 1773.2299679562966
INFO:root:current train perplexity4.050819396972656
INFO:root:current mean train loss 1771.8608907139774
INFO:root:current train perplexity4.050721645355225
INFO:root:current mean train loss 1774.7277306085498
INFO:root:current train perplexity4.053192615509033
INFO:root:current mean train loss 1775.6137458945034
INFO:root:current train perplexity4.055351257324219
INFO:root:current mean train loss 1775.860738180555
INFO:root:current train perplexity4.055927276611328
INFO:root:current mean train loss 1775.8341703233943
INFO:root:current train perplexity4.0564727783203125
INFO:root:current mean train loss 1775.5795121143667
INFO:root:current train perplexity4.057069301605225
INFO:root:current mean train loss 1775.9120627251282
INFO:root:current train perplexity4.057112693786621

100%|██████████| 1/1 [05:09<00:00, 309.11s/it][A100%|██████████| 1/1 [05:09<00:00, 309.11s/it]
INFO:root:final mean train loss: 1775.898175630555
INFO:root:final train perplexity: 4.057552814483643
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.34s/it][A100%|██████████| 1/1 [00:20<00:00, 20.34s/it]
INFO:root:eval mean loss: 1965.992068026928
INFO:root:eval perplexity: 4.9036455154418945
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.87s/it][A100%|██████████| 1/1 [00:19<00:00, 19.87s/it]
INFO:root:eval mean loss: 2402.6389220758533
INFO:root:eval perplexity: 7.134513854980469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/21
 10%|█         | 21/200 [2:05:02<17:24:27, 350.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1747.394243512835
INFO:root:current train perplexity3.959101438522339
INFO:root:current mean train loss 1737.8746619591345
INFO:root:current train perplexity3.9510762691497803
INFO:root:current mean train loss 1746.3446798324585
INFO:root:current train perplexity3.966299295425415
INFO:root:current mean train loss 1747.6570753461858
INFO:root:current train perplexity3.9733316898345947
INFO:root:current mean train loss 1751.689592595686
INFO:root:current train perplexity3.978261709213257
INFO:root:current mean train loss 1754.8585547577563
INFO:root:current train perplexity3.9892160892486572
INFO:root:current mean train loss 1753.9273681640625
INFO:root:current train perplexity3.987138509750366
INFO:root:current mean train loss 1753.227198848018
INFO:root:current train perplexity3.9902544021606445
INFO:root:current mean train loss 1757.1599615934854
INFO:root:current train perplexity3.9964091777801514
INFO:root:current mean train loss 1757.5037016928445
INFO:root:current train perplexity3.995544910430908
INFO:root:current mean train loss 1756.7372657313492
INFO:root:current train perplexity3.9946391582489014
INFO:root:current mean train loss 1756.2347263217384
INFO:root:current train perplexity3.992659568786621
INFO:root:current mean train loss 1757.2519059879764
INFO:root:current train perplexity3.995680332183838
INFO:root:current mean train loss 1758.4693273133585
INFO:root:current train perplexity3.998176097869873
INFO:root:current mean train loss 1759.222333551763
INFO:root:current train perplexity3.9984118938446045
INFO:root:current mean train loss 1760.5225727306847
INFO:root:current train perplexity4.000115394592285
INFO:root:current mean train loss 1760.5313680897589
INFO:root:current train perplexity4.001852989196777
INFO:root:current mean train loss 1760.0546950772575
INFO:root:current train perplexity4.00241756439209
INFO:root:current mean train loss 1760.3461922612682
INFO:root:current train perplexity4.003654479980469
INFO:root:current mean train loss 1759.9490877553242
INFO:root:current train perplexity4.004378795623779

100%|██████████| 1/1 [05:03<00:00, 303.89s/it][A100%|██████████| 1/1 [05:03<00:00, 303.89s/it]
INFO:root:final mean train loss: 1759.2826501458687
INFO:root:final train perplexity: 4.004730224609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.91s/it][A100%|██████████| 1/1 [00:20<00:00, 20.91s/it]
INFO:root:eval mean loss: 1968.8564453125
INFO:root:eval perplexity: 4.915018081665039
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.56s/it][A100%|██████████| 1/1 [00:19<00:00, 19.56s/it]
INFO:root:eval mean loss: 2408.8477506164118
INFO:root:eval perplexity: 7.170833110809326
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/22
 11%|█         | 22/200 [2:10:48<17:15:01, 348.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1721.2687001685574
INFO:root:current train perplexity3.903818368911743
INFO:root:current mean train loss 1723.118013062229
INFO:root:current train perplexity3.8773720264434814
INFO:root:current mean train loss 1730.656022403703
INFO:root:current train perplexity3.896799087524414
INFO:root:current mean train loss 1728.9334952428578
INFO:root:current train perplexity3.904006004333496
INFO:root:current mean train loss 1730.2337385826836
INFO:root:current train perplexity3.910675287246704
INFO:root:current mean train loss 1732.5316779917239
INFO:root:current train perplexity3.916351795196533
INFO:root:current mean train loss 1735.0090049074804
INFO:root:current train perplexity3.9242961406707764
INFO:root:current mean train loss 1736.4155631910476
INFO:root:current train perplexity3.9310805797576904
INFO:root:current mean train loss 1735.4778636045335
INFO:root:current train perplexity3.9312222003936768
INFO:root:current mean train loss 1738.4613431046457
INFO:root:current train perplexity3.9377248287200928
INFO:root:current mean train loss 1739.9682143923287
INFO:root:current train perplexity3.942807674407959
INFO:root:current mean train loss 1739.4178495935569
INFO:root:current train perplexity3.943561553955078
INFO:root:current mean train loss 1739.826537127019
INFO:root:current train perplexity3.944577217102051
INFO:root:current mean train loss 1741.788502292397
INFO:root:current train perplexity3.947741985321045
INFO:root:current mean train loss 1742.1754084093102
INFO:root:current train perplexity3.9503862857818604
INFO:root:current mean train loss 1742.7051385005266
INFO:root:current train perplexity3.9513514041900635
INFO:root:current mean train loss 1743.4024283163433
INFO:root:current train perplexity3.9537012577056885
INFO:root:current mean train loss 1743.4436744995462
INFO:root:current train perplexity3.953793525695801
INFO:root:current mean train loss 1743.9773155167595
INFO:root:current train perplexity3.956526756286621
INFO:root:current mean train loss 1744.2425958446845
INFO:root:current train perplexity3.95658802986145

100%|██████████| 1/1 [05:17<00:00, 317.99s/it][A100%|██████████| 1/1 [05:17<00:00, 317.99s/it]
INFO:root:final mean train loss: 1743.9452806435265
INFO:root:final train perplexity: 3.956580638885498
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.80s/it][A100%|██████████| 1/1 [00:21<00:00, 21.80s/it]
INFO:root:eval mean loss: 1968.0747065983765
INFO:root:eval perplexity: 4.911911964416504
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.07s/it][A100%|██████████| 1/1 [00:21<00:00, 21.07s/it]
INFO:root:eval mean loss: 2410.2676045302806
INFO:root:eval perplexity: 7.179163932800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/23
 12%|█▏        | 23/200 [2:16:51<17:21:19, 352.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1710.9750149197048
INFO:root:current train perplexity3.879448890686035
INFO:root:current mean train loss 1713.744357139186
INFO:root:current train perplexity3.8826000690460205
INFO:root:current mean train loss 1715.4814575195312
INFO:root:current train perplexity3.885052442550659
INFO:root:current mean train loss 1719.888246506911
INFO:root:current train perplexity3.8884363174438477
INFO:root:current mean train loss 1720.4758034219547
INFO:root:current train perplexity3.886013984680176
INFO:root:current mean train loss 1723.5863510907707
INFO:root:current train perplexity3.8922181129455566
INFO:root:current mean train loss 1721.0607085739357
INFO:root:current train perplexity3.893338918685913
INFO:root:current mean train loss 1721.493770551078
INFO:root:current train perplexity3.8938066959381104
INFO:root:current mean train loss 1722.2329630991046
INFO:root:current train perplexity3.8906478881835938
INFO:root:current mean train loss 1722.5142999822442
INFO:root:current train perplexity3.89155650138855
INFO:root:current mean train loss 1724.2557360727853
INFO:root:current train perplexity3.8975675106048584
INFO:root:current mean train loss 1725.3944755490086
INFO:root:current train perplexity3.897395133972168
INFO:root:current mean train loss 1724.680973496548
INFO:root:current train perplexity3.8961760997772217
INFO:root:current mean train loss 1726.2523589429238
INFO:root:current train perplexity3.901524782180786
INFO:root:current mean train loss 1726.6538001553324
INFO:root:current train perplexity3.9032301902770996
INFO:root:current mean train loss 1727.339103650747
INFO:root:current train perplexity3.905320882797241
INFO:root:current mean train loss 1727.7751800717688
INFO:root:current train perplexity3.905527114868164
INFO:root:current mean train loss 1728.7954737828431
INFO:root:current train perplexity3.906970262527466
INFO:root:current mean train loss 1728.7480600508432
INFO:root:current train perplexity3.9076876640319824

100%|██████████| 1/1 [05:29<00:00, 329.01s/it][A100%|██████████| 1/1 [05:29<00:00, 329.01s/it]
INFO:root:final mean train loss: 1728.9766261206091
INFO:root:final train perplexity: 3.910146474838257
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.65s/it][A100%|██████████| 1/1 [00:22<00:00, 22.65s/it]
INFO:root:eval mean loss: 1967.7870665205287
INFO:root:eval perplexity: 4.910769462585449
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.51s/it][A100%|██████████| 1/1 [00:20<00:00, 20.51s/it]
INFO:root:eval mean loss: 2412.174376142786
INFO:root:eval perplexity: 7.190369606018066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/24
 12%|█▏        | 24/200 [2:23:05<17:33:55, 359.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1694.814697265625
INFO:root:current train perplexity3.746533155441284
INFO:root:current mean train loss 1696.5188684196116
INFO:root:current train perplexity3.8366453647613525
INFO:root:current mean train loss 1700.7568347580766
INFO:root:current train perplexity3.834388494491577
INFO:root:current mean train loss 1705.75870118778
INFO:root:current train perplexity3.8439674377441406
INFO:root:current mean train loss 1706.4457317980268
INFO:root:current train perplexity3.8489434719085693
INFO:root:current mean train loss 1708.5301376433063
INFO:root:current train perplexity3.8484511375427246
INFO:root:current mean train loss 1712.7530901687346
INFO:root:current train perplexity3.850634813308716
INFO:root:current mean train loss 1711.9128657965546
INFO:root:current train perplexity3.8502166271209717
INFO:root:current mean train loss 1712.7344563802083
INFO:root:current train perplexity3.8545165061950684
INFO:root:current mean train loss 1713.3144612002136
INFO:root:current train perplexity3.858835220336914
INFO:root:current mean train loss 1713.1362679262738
INFO:root:current train perplexity3.85798716545105
INFO:root:current mean train loss 1714.2374547667189
INFO:root:current train perplexity3.859467029571533
INFO:root:current mean train loss 1714.1650987323296
INFO:root:current train perplexity3.8611013889312744
INFO:root:current mean train loss 1714.007801479115
INFO:root:current train perplexity3.8604581356048584
INFO:root:current mean train loss 1713.645527073061
INFO:root:current train perplexity3.860973596572876
INFO:root:current mean train loss 1715.2272707832199
INFO:root:current train perplexity3.8645317554473877
INFO:root:current mean train loss 1714.7650233080615
INFO:root:current train perplexity3.862703561782837
INFO:root:current mean train loss 1714.8351742566226
INFO:root:current train perplexity3.8640871047973633
INFO:root:current mean train loss 1714.8786246843872
INFO:root:current train perplexity3.864786386489868
INFO:root:current mean train loss 1714.8309217351984
INFO:root:current train perplexity3.865504741668701

100%|██████████| 1/1 [05:24<00:00, 324.63s/it][A100%|██████████| 1/1 [05:24<00:00, 324.63s/it]
INFO:root:final mean train loss: 1714.397656293091
INFO:root:final train perplexity: 3.8654465675354004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 1973.5395975315826
INFO:root:eval perplexity: 4.933668613433838
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.78s/it][A100%|██████████| 1/1 [00:20<00:00, 20.78s/it]
INFO:root:eval mean loss: 2424.8422167622452
INFO:root:eval perplexity: 7.265248775482178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/25
 12%|█▎        | 25/200 [2:29:14<17:36:40, 362.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1701.4492797851562
INFO:root:current train perplexity3.7780725955963135
INFO:root:current mean train loss 1691.7927049206148
INFO:root:current train perplexity3.7529969215393066
INFO:root:current mean train loss 1698.4762300763812
INFO:root:current train perplexity3.7734534740448
INFO:root:current mean train loss 1698.4850328233506
INFO:root:current train perplexity3.7913880348205566
INFO:root:current mean train loss 1703.714364393702
INFO:root:current train perplexity3.799225091934204
INFO:root:current mean train loss 1702.6351455804956
INFO:root:current train perplexity3.7978734970092773
INFO:root:current mean train loss 1700.055663279998
INFO:root:current train perplexity3.7975194454193115
INFO:root:current mean train loss 1698.6745507677615
INFO:root:current train perplexity3.8014328479766846
INFO:root:current mean train loss 1697.011922002996
INFO:root:current train perplexity3.8017940521240234
INFO:root:current mean train loss 1696.2494119751505
INFO:root:current train perplexity3.8019821643829346
INFO:root:current mean train loss 1697.3969950675964
INFO:root:current train perplexity3.8048367500305176
INFO:root:current mean train loss 1697.0086152969307
INFO:root:current train perplexity3.803701162338257
INFO:root:current mean train loss 1698.047157337463
INFO:root:current train perplexity3.806933641433716
INFO:root:current mean train loss 1698.7749385776117
INFO:root:current train perplexity3.8094186782836914
INFO:root:current mean train loss 1699.4600574621993
INFO:root:current train perplexity3.813119411468506
INFO:root:current mean train loss 1699.4389560329007
INFO:root:current train perplexity3.8134453296661377
INFO:root:current mean train loss 1699.2254904761103
INFO:root:current train perplexity3.8153746128082275
INFO:root:current mean train loss 1699.1678970230705
INFO:root:current train perplexity3.8166446685791016
INFO:root:current mean train loss 1698.9777535555656
INFO:root:current train perplexity3.81765079498291
INFO:root:current mean train loss 1699.706097513623
INFO:root:current train perplexity3.818398952484131

100%|██████████| 1/1 [05:22<00:00, 322.11s/it][A100%|██████████| 1/1 [05:22<00:00, 322.11s/it]
INFO:root:final mean train loss: 1699.4457711716584
INFO:root:final train perplexity: 3.820132255554199
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.94s/it][A100%|██████████| 1/1 [00:21<00:00, 21.94s/it]
INFO:root:eval mean loss: 1974.202186963237
INFO:root:eval perplexity: 4.936313152313232
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.11s/it][A100%|██████████| 1/1 [00:21<00:00, 21.11s/it]
INFO:root:eval mean loss: 2425.383860486619
INFO:root:eval perplexity: 7.268467903137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/26
 13%|█▎        | 26/200 [2:35:21<17:34:38, 363.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1704.409364281631
INFO:root:current train perplexity3.7597501277923584
INFO:root:current mean train loss 1684.525684978945
INFO:root:current train perplexity3.7416088581085205
INFO:root:current mean train loss 1683.2784504870656
INFO:root:current train perplexity3.7410571575164795
INFO:root:current mean train loss 1687.2724079568366
INFO:root:current train perplexity3.7529709339141846
INFO:root:current mean train loss 1689.4470773986677
INFO:root:current train perplexity3.75773286819458
INFO:root:current mean train loss 1685.8124546467045
INFO:root:current train perplexity3.759265184402466
INFO:root:current mean train loss 1685.7035026371757
INFO:root:current train perplexity3.761023759841919
INFO:root:current mean train loss 1683.791600771761
INFO:root:current train perplexity3.763411521911621
INFO:root:current mean train loss 1683.8717383567275
INFO:root:current train perplexity3.7644448280334473
INFO:root:current mean train loss 1681.6581629105506
INFO:root:current train perplexity3.7652087211608887
INFO:root:current mean train loss 1683.3844302541127
INFO:root:current train perplexity3.767132520675659
INFO:root:current mean train loss 1684.3265120884914
INFO:root:current train perplexity3.7676894664764404
INFO:root:current mean train loss 1683.518075160688
INFO:root:current train perplexity3.7689414024353027
INFO:root:current mean train loss 1683.7238632987276
INFO:root:current train perplexity3.7696566581726074
INFO:root:current mean train loss 1684.3019319808293
INFO:root:current train perplexity3.770601272583008
INFO:root:current mean train loss 1685.1621927091783
INFO:root:current train perplexity3.7742295265197754
INFO:root:current mean train loss 1684.7858234338103
INFO:root:current train perplexity3.773921012878418
INFO:root:current mean train loss 1685.5593769351756
INFO:root:current train perplexity3.776108980178833
INFO:root:current mean train loss 1685.7894328882485
INFO:root:current train perplexity3.77817702293396
INFO:root:current mean train loss 1685.6492314287095
INFO:root:current train perplexity3.776991367340088

100%|██████████| 1/1 [05:30<00:00, 330.98s/it][A100%|██████████| 1/1 [05:30<00:00, 330.98s/it]
INFO:root:final mean train loss: 1685.3482064282239
INFO:root:final train perplexity: 3.777895212173462
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.18s/it][A100%|██████████| 1/1 [00:22<00:00, 22.19s/it]
INFO:root:eval mean loss: 1978.8525442569814
INFO:root:eval perplexity: 4.954914093017578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.71s/it][A100%|██████████| 1/1 [00:20<00:00, 20.71s/it]
INFO:root:eval mean loss: 2432.495076930269
INFO:root:eval perplexity: 7.310863494873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/27
 14%|█▎        | 27/200 [2:42:28<18:23:55, 382.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.977534853179
INFO:root:current train perplexity3.69926381111145
INFO:root:current mean train loss 1654.9618661614913
INFO:root:current train perplexity3.687662124633789
INFO:root:current mean train loss 1656.7586215706758
INFO:root:current train perplexity3.6963553428649902
INFO:root:current mean train loss 1657.3851079674407
INFO:root:current train perplexity3.700122117996216
INFO:root:current mean train loss 1661.2622494093716
INFO:root:current train perplexity3.709836959838867
INFO:root:current mean train loss 1660.037118781852
INFO:root:current train perplexity3.711172342300415
INFO:root:current mean train loss 1662.6559244173277
INFO:root:current train perplexity3.7132465839385986
INFO:root:current mean train loss 1663.607153416938
INFO:root:current train perplexity3.715911388397217
INFO:root:current mean train loss 1663.3212692865402
INFO:root:current train perplexity3.717294692993164
INFO:root:current mean train loss 1664.2421751400623
INFO:root:current train perplexity3.7157692909240723
INFO:root:current mean train loss 1664.4774722584255
INFO:root:current train perplexity3.7176709175109863
INFO:root:current mean train loss 1666.6048788489043
INFO:root:current train perplexity3.723877429962158
INFO:root:current mean train loss 1667.1916097328658
INFO:root:current train perplexity3.724294662475586
INFO:root:current mean train loss 1667.695978223663
INFO:root:current train perplexity3.7239062786102295
INFO:root:current mean train loss 1668.1958955573773
INFO:root:current train perplexity3.7253103256225586
INFO:root:current mean train loss 1669.15864799999
INFO:root:current train perplexity3.7274184226989746
INFO:root:current mean train loss 1669.3027393815025
INFO:root:current train perplexity3.7278025150299072
INFO:root:current mean train loss 1669.675134860615
INFO:root:current train perplexity3.7291829586029053
INFO:root:current mean train loss 1670.9464965426114
INFO:root:current train perplexity3.7327237129211426
INFO:root:current mean train loss 1671.306713817312
INFO:root:current train perplexity3.734724521636963

100%|██████████| 1/1 [05:25<00:00, 325.44s/it][A100%|██████████| 1/1 [05:25<00:00, 325.44s/it]
INFO:root:final mean train loss: 1670.881328696262
INFO:root:final train perplexity: 3.7350361347198486
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.80s/it][A100%|██████████| 1/1 [00:20<00:00, 20.80s/it]
INFO:root:eval mean loss: 1983.6306386095412
INFO:root:eval perplexity: 4.974098205566406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.75s/it][A100%|██████████| 1/1 [00:19<00:00, 19.75s/it]
INFO:root:eval mean loss: 2445.110693099651
INFO:root:eval perplexity: 7.386683464050293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/28
 14%|█▍        | 28/200 [2:48:36<18:04:38, 378.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1632.6079036458334
INFO:root:current train perplexity3.6440913677215576
INFO:root:current mean train loss 1629.6053166852678
INFO:root:current train perplexity3.6327760219573975
INFO:root:current mean train loss 1635.3042915482954
INFO:root:current train perplexity3.6560750007629395
INFO:root:current mean train loss 1640.0918310546874
INFO:root:current train perplexity3.6600546836853027
INFO:root:current mean train loss 1644.6604523026315
INFO:root:current train perplexity3.663724660873413
INFO:root:current mean train loss 1644.6532171365488
INFO:root:current train perplexity3.6633660793304443
INFO:root:current mean train loss 1643.3322404875578
INFO:root:current train perplexity3.6652753353118896
INFO:root:current mean train loss 1642.0559860131048
INFO:root:current train perplexity3.665583372116089
INFO:root:current mean train loss 1643.9043780691964
INFO:root:current train perplexity3.6701245307922363
INFO:root:current mean train loss 1647.285224233774
INFO:root:current train perplexity3.6735379695892334
INFO:root:current mean train loss 1648.520287404615
INFO:root:current train perplexity3.6772985458374023
INFO:root:current mean train loss 1649.6878319273603
INFO:root:current train perplexity3.677896022796631
INFO:root:current mean train loss 1650.612862668505
INFO:root:current train perplexity3.6788318157196045
INFO:root:current mean train loss 1651.6382221235795
INFO:root:current train perplexity3.6785361766815186
INFO:root:current mean train loss 1653.7579931640626
INFO:root:current train perplexity3.685006618499756
INFO:root:current mean train loss 1655.2697778707836
INFO:root:current train perplexity3.687716484069824
INFO:root:current mean train loss 1656.0385989972015
INFO:root:current train perplexity3.688366413116455
INFO:root:current mean train loss 1657.27687431228
INFO:root:current train perplexity3.6910159587860107
INFO:root:current mean train loss 1657.6930240885417
INFO:root:current train perplexity3.6939454078674316
INFO:root:current mean train loss 1657.5292335220531
INFO:root:current train perplexity3.694666862487793

100%|██████████| 1/1 [05:19<00:00, 319.64s/it][A100%|██████████| 1/1 [05:19<00:00, 319.64s/it]
INFO:root:final mean train loss: 1657.0642481355192
INFO:root:final train perplexity: 3.694556713104248
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.78s/it][A100%|██████████| 1/1 [00:21<00:00, 21.78s/it]
INFO:root:eval mean loss: 1985.6341994265292
INFO:root:eval perplexity: 4.982163906097412
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.79s/it][A100%|██████████| 1/1 [00:20<00:00, 20.79s/it]
INFO:root:eval mean loss: 2445.5252512397497
INFO:root:eval perplexity: 7.389188289642334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/29
 14%|█▍        | 29/200 [2:54:40<17:46:08, 374.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1617.9999084472656
INFO:root:current train perplexity3.6234049797058105
INFO:root:current mean train loss 1627.74680519104
INFO:root:current train perplexity3.618779182434082
INFO:root:current mean train loss 1634.6800265377515
INFO:root:current train perplexity3.6268670558929443
INFO:root:current mean train loss 1636.3309438277265
INFO:root:current train perplexity3.6272149085998535
INFO:root:current mean train loss 1635.3915913899739
INFO:root:current train perplexity3.6237072944641113
INFO:root:current mean train loss 1633.584984650483
INFO:root:current train perplexity3.6224238872528076
INFO:root:current mean train loss 1634.9762142820855
INFO:root:current train perplexity3.6234874725341797
INFO:root:current mean train loss 1637.2299243657276
INFO:root:current train perplexity3.6274118423461914
INFO:root:current mean train loss 1637.1916039984324
INFO:root:current train perplexity3.6330623626708984
INFO:root:current mean train loss 1636.3636036534463
INFO:root:current train perplexity3.634119987487793
INFO:root:current mean train loss 1636.4322848477207
INFO:root:current train perplexity3.633348226547241
INFO:root:current mean train loss 1636.1983780828898
INFO:root:current train perplexity3.6342811584472656
INFO:root:current mean train loss 1637.4789567598987
INFO:root:current train perplexity3.636593818664551
INFO:root:current mean train loss 1638.1905383406015
INFO:root:current train perplexity3.64044451713562
INFO:root:current mean train loss 1638.6179110038695
INFO:root:current train perplexity3.6415960788726807
INFO:root:current mean train loss 1639.1073229612418
INFO:root:current train perplexity3.6429851055145264
INFO:root:current mean train loss 1640.1413901759659
INFO:root:current train perplexity3.6451549530029297
INFO:root:current mean train loss 1641.9118154389519
INFO:root:current train perplexity3.649627208709717
INFO:root:current mean train loss 1642.9078554310709
INFO:root:current train perplexity3.653165578842163

100%|██████████| 1/1 [05:24<00:00, 324.10s/it][A100%|██████████| 1/1 [05:24<00:00, 324.10s/it]
INFO:root:final mean train loss: 1643.539761895792
INFO:root:final train perplexity: 3.6553595066070557
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.62s/it][A100%|██████████| 1/1 [00:21<00:00, 21.63s/it]
INFO:root:eval mean loss: 1990.900230461824
INFO:root:eval perplexity: 5.003427982330322
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.85s/it][A100%|██████████| 1/1 [00:20<00:00, 20.85s/it]
INFO:root:eval mean loss: 2451.5339961491577
INFO:root:eval perplexity: 7.425586700439453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/30
 15%|█▌        | 30/200 [3:00:49<17:35:06, 372.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1558.7721761067708
INFO:root:current train perplexity3.5155189037323
INFO:root:current mean train loss 1605.5759355737528
INFO:root:current train perplexity3.567741870880127
INFO:root:current mean train loss 1610.9117466684734
INFO:root:current train perplexity3.5702290534973145
INFO:root:current mean train loss 1613.9364894632383
INFO:root:current train perplexity3.5724194049835205
INFO:root:current mean train loss 1614.694167605822
INFO:root:current train perplexity3.578372001647949
INFO:root:current mean train loss 1617.4941468604188
INFO:root:current train perplexity3.5837349891662598
INFO:root:current mean train loss 1618.0045218131029
INFO:root:current train perplexity3.579942226409912
INFO:root:current mean train loss 1621.0167413665815
INFO:root:current train perplexity3.5889508724212646
INFO:root:current mean train loss 1621.2798933446481
INFO:root:current train perplexity3.5914270877838135
INFO:root:current mean train loss 1621.3818875051568
INFO:root:current train perplexity3.594133138656616
INFO:root:current mean train loss 1622.569916406637
INFO:root:current train perplexity3.5963070392608643
INFO:root:current mean train loss 1623.0658076761863
INFO:root:current train perplexity3.5988290309906006
INFO:root:current mean train loss 1624.0102262410178
INFO:root:current train perplexity3.6005992889404297
INFO:root:current mean train loss 1624.3425451501623
INFO:root:current train perplexity3.6012508869171143
INFO:root:current mean train loss 1626.6009513687122
INFO:root:current train perplexity3.6047589778900146
INFO:root:current mean train loss 1626.534849658559
INFO:root:current train perplexity3.605463981628418
INFO:root:current mean train loss 1627.938342125835
INFO:root:current train perplexity3.6076622009277344
INFO:root:current mean train loss 1628.158806190915
INFO:root:current train perplexity3.6090996265411377
INFO:root:current mean train loss 1629.143761039637
INFO:root:current train perplexity3.6105008125305176
INFO:root:current mean train loss 1629.2937939555436
INFO:root:current train perplexity3.612765073776245

100%|██████████| 1/1 [05:18<00:00, 318.80s/it][A100%|██████████| 1/1 [05:18<00:00, 318.80s/it]
INFO:root:final mean train loss: 1629.1372564934265
INFO:root:final train perplexity: 3.614074468612671
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.44s/it][A100%|██████████| 1/1 [00:21<00:00, 21.44s/it]
INFO:root:eval mean loss: 1990.9288490241302
INFO:root:eval perplexity: 5.003543853759766
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.24s/it][A100%|██████████| 1/1 [00:21<00:00, 21.24s/it]
INFO:root:eval mean loss: 2455.495908480164
INFO:root:eval perplexity: 7.4496870040893555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/31
 16%|█▌        | 31/200 [3:07:22<17:46:14, 378.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1602.905052771935
INFO:root:current train perplexity3.5060625076293945
INFO:root:current mean train loss 1587.3243882921006
INFO:root:current train perplexity3.523252248764038
INFO:root:current mean train loss 1599.8217141480573
INFO:root:current train perplexity3.5348684787750244
INFO:root:current mean train loss 1605.7505515630992
INFO:root:current train perplexity3.5453577041625977
INFO:root:current mean train loss 1603.02343778655
INFO:root:current train perplexity3.543511390686035
INFO:root:current mean train loss 1604.8813126132516
INFO:root:current train perplexity3.5512173175811768
INFO:root:current mean train loss 1605.3212652724392
INFO:root:current train perplexity3.549868106842041
INFO:root:current mean train loss 1608.0230803686725
INFO:root:current train perplexity3.557133913040161
INFO:root:current mean train loss 1608.212101897085
INFO:root:current train perplexity3.5574934482574463
INFO:root:current mean train loss 1609.4156594327922
INFO:root:current train perplexity3.556460380554199
INFO:root:current mean train loss 1610.1027719003182
INFO:root:current train perplexity3.5604467391967773
INFO:root:current mean train loss 1610.7184423307754
INFO:root:current train perplexity3.563469886779785
INFO:root:current mean train loss 1612.2660475664
INFO:root:current train perplexity3.5666115283966064
INFO:root:current mean train loss 1613.1666602225325
INFO:root:current train perplexity3.568735122680664
INFO:root:current mean train loss 1612.9356973656097
INFO:root:current train perplexity3.568911552429199
INFO:root:current mean train loss 1614.0665255205347
INFO:root:current train perplexity3.5702004432678223
INFO:root:current mean train loss 1614.8477465640135
INFO:root:current train perplexity3.5707786083221436
INFO:root:current mean train loss 1615.6651634667176
INFO:root:current train perplexity3.5722312927246094
INFO:root:current mean train loss 1615.7830513727495
INFO:root:current train perplexity3.573744535446167
INFO:root:current mean train loss 1615.74906577549
INFO:root:current train perplexity3.575742244720459

100%|██████████| 1/1 [05:23<00:00, 323.60s/it][A100%|██████████| 1/1 [05:23<00:00, 323.60s/it]
INFO:root:final mean train loss: 1615.9687202980708
INFO:root:final train perplexity: 3.5767343044281006
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.58s/it][A100%|██████████| 1/1 [00:21<00:00, 21.58s/it]
INFO:root:eval mean loss: 1996.199722614694
INFO:root:eval perplexity: 5.024919033050537
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.75s/it][A100%|██████████| 1/1 [00:20<00:00, 20.75s/it]
INFO:root:eval mean loss: 2462.630814789035
INFO:root:eval perplexity: 7.493283748626709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/32
 16%|█▌        | 32/200 [3:13:29<17:30:53, 375.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.1386974246002
INFO:root:current train perplexity3.495915412902832
INFO:root:current mean train loss 1590.0930645282451
INFO:root:current train perplexity3.5135343074798584
INFO:root:current mean train loss 1593.3990187154386
INFO:root:current train perplexity3.5187506675720215
INFO:root:current mean train loss 1591.0100809436499
INFO:root:current train perplexity3.5119221210479736
INFO:root:current mean train loss 1590.1150682160871
INFO:root:current train perplexity3.510842800140381
INFO:root:current mean train loss 1589.7317665619964
INFO:root:current train perplexity3.513425827026367
INFO:root:current mean train loss 1589.8308684495894
INFO:root:current train perplexity3.5135509967803955
INFO:root:current mean train loss 1593.339213026003
INFO:root:current train perplexity3.5202417373657227
INFO:root:current mean train loss 1594.3917874916592
INFO:root:current train perplexity3.5215251445770264
INFO:root:current mean train loss 1594.3998333474865
INFO:root:current train perplexity3.5209062099456787
INFO:root:current mean train loss 1595.8083395441336
INFO:root:current train perplexity3.5230212211608887
INFO:root:current mean train loss 1596.2075144049377
INFO:root:current train perplexity3.522899866104126
INFO:root:current mean train loss 1597.2906063015387
INFO:root:current train perplexity3.5253396034240723
INFO:root:current mean train loss 1597.5865875721333
INFO:root:current train perplexity3.526785135269165
INFO:root:current mean train loss 1599.366132095136
INFO:root:current train perplexity3.530083656311035
INFO:root:current mean train loss 1600.7548195226425
INFO:root:current train perplexity3.532463312149048
INFO:root:current mean train loss 1601.238099296138
INFO:root:current train perplexity3.5352702140808105
INFO:root:current mean train loss 1601.7246562281491
INFO:root:current train perplexity3.5361409187316895
INFO:root:current mean train loss 1602.3112908216901
INFO:root:current train perplexity3.536914110183716
INFO:root:current mean train loss 1602.9604343918875
INFO:root:current train perplexity3.5383670330047607

100%|██████████| 1/1 [05:21<00:00, 321.67s/it][A100%|██████████| 1/1 [05:21<00:00, 321.67s/it]
INFO:root:final mean train loss: 1602.8140334198106
INFO:root:final train perplexity: 3.5398192405700684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.12s/it][A100%|██████████| 1/1 [00:21<00:00, 21.12s/it]
INFO:root:eval mean loss: 2002.6812501731495
INFO:root:eval perplexity: 5.051327705383301
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.38s/it][A100%|██████████| 1/1 [00:21<00:00, 21.38s/it]
INFO:root:eval mean loss: 2470.5504669838765
INFO:root:eval perplexity: 7.541975975036621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/33
 16%|█▋        | 33/200 [3:19:35<17:16:51, 372.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1590.2338216145833
INFO:root:current train perplexity3.4942479133605957
INFO:root:current mean train loss 1580.4576332092286
INFO:root:current train perplexity3.495476722717285
INFO:root:current mean train loss 1575.8873849722056
INFO:root:current train perplexity3.477290391921997
INFO:root:current mean train loss 1573.6854787190755
INFO:root:current train perplexity3.474280595779419
INFO:root:current mean train loss 1577.7449038298234
INFO:root:current train perplexity3.4759562015533447
INFO:root:current mean train loss 1578.6921194893973
INFO:root:current train perplexity3.478013038635254
INFO:root:current mean train loss 1580.8577899354877
INFO:root:current train perplexity3.4805049896240234
INFO:root:current mean train loss 1583.1211954217208
INFO:root:current train perplexity3.4831442832946777
INFO:root:current mean train loss 1584.5615186114644
INFO:root:current train perplexity3.487382173538208
INFO:root:current mean train loss 1584.445655822754
INFO:root:current train perplexity3.489712715148926
INFO:root:current mean train loss 1584.3585095675487
INFO:root:current train perplexity3.492588758468628
INFO:root:current mean train loss 1584.3092405121902
INFO:root:current train perplexity3.494271755218506
INFO:root:current mean train loss 1585.0760754782057
INFO:root:current train perplexity3.495354175567627
INFO:root:current mean train loss 1586.1245669196633
INFO:root:current train perplexity3.497403621673584
INFO:root:current mean train loss 1586.7355089161495
INFO:root:current train perplexity3.4984018802642822
INFO:root:current mean train loss 1588.0430045103415
INFO:root:current train perplexity3.501756429672241
INFO:root:current mean train loss 1588.4272852886154
INFO:root:current train perplexity3.5028841495513916
INFO:root:current mean train loss 1588.5241324684837
INFO:root:current train perplexity3.503739833831787
INFO:root:current mean train loss 1589.7498718261718
INFO:root:current train perplexity3.5050485134124756
INFO:root:current mean train loss 1590.6569042595065
INFO:root:current train perplexity3.505269765853882

100%|██████████| 1/1 [05:25<00:00, 325.53s/it][A100%|██████████| 1/1 [05:25<00:00, 325.53s/it]
INFO:root:final mean train loss: 1590.5925170467528
INFO:root:final train perplexity: 3.5058634281158447
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.69s/it][A100%|██████████| 1/1 [00:21<00:00, 21.69s/it]
INFO:root:eval mean loss: 2004.5474671708776
INFO:root:eval perplexity: 5.058956623077393
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.01s/it][A100%|██████████| 1/1 [00:21<00:00, 21.01s/it]
INFO:root:eval mean loss: 2473.1302892806684
INFO:root:eval perplexity: 7.5579047203063965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/34
 17%|█▋        | 34/200 [3:25:46<17:08:34, 371.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1560.698194627638
INFO:root:current train perplexity3.441072940826416
INFO:root:current mean train loss 1568.4644230557026
INFO:root:current train perplexity3.4369258880615234
INFO:root:current mean train loss 1565.5037238055618
INFO:root:current train perplexity3.4213433265686035
INFO:root:current mean train loss 1567.3869169118866
INFO:root:current train perplexity3.4233901500701904
INFO:root:current mean train loss 1568.7969200406185
INFO:root:current train perplexity3.434417724609375
INFO:root:current mean train loss 1570.0932227916487
INFO:root:current train perplexity3.438185930252075
INFO:root:current mean train loss 1571.1408440774442
INFO:root:current train perplexity3.440095901489258
INFO:root:current mean train loss 1570.0796823783885
INFO:root:current train perplexity3.4398512840270996
INFO:root:current mean train loss 1570.2996203989096
INFO:root:current train perplexity3.442988395690918
INFO:root:current mean train loss 1569.668970301305
INFO:root:current train perplexity3.4430384635925293
INFO:root:current mean train loss 1570.650445142939
INFO:root:current train perplexity3.4465138912200928
INFO:root:current mean train loss 1572.2125965983764
INFO:root:current train perplexity3.4524731636047363
INFO:root:current mean train loss 1573.2196541997541
INFO:root:current train perplexity3.4551808834075928
INFO:root:current mean train loss 1574.385312680845
INFO:root:current train perplexity3.457714319229126
INFO:root:current mean train loss 1574.8055356283323
INFO:root:current train perplexity3.459174156188965
INFO:root:current mean train loss 1575.2093306924244
INFO:root:current train perplexity3.4624385833740234
INFO:root:current mean train loss 1575.8428803673087
INFO:root:current train perplexity3.4626963138580322
INFO:root:current mean train loss 1576.0519621102542
INFO:root:current train perplexity3.4648165702819824
INFO:root:current mean train loss 1576.6586476378322
INFO:root:current train perplexity3.4667868614196777
INFO:root:current mean train loss 1577.4801176666429
INFO:root:current train perplexity3.4685909748077393

100%|██████████| 1/1 [05:26<00:00, 326.39s/it][A100%|██████████| 1/1 [05:26<00:00, 326.39s/it]
INFO:root:final mean train loss: 1577.0254318383506
INFO:root:final train perplexity: 3.4685516357421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.85s/it][A100%|██████████| 1/1 [00:22<00:00, 22.85s/it]
INFO:root:eval mean loss: 2010.6306948830895
INFO:root:eval perplexity: 5.083907604217529
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.90s/it][A100%|██████████| 1/1 [00:21<00:00, 21.90s/it]
INFO:root:eval mean loss: 2482.734091467891
INFO:root:eval perplexity: 7.617499828338623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/35
 18%|█▊        | 35/200 [3:31:58<17:03:18, 372.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1545.432982099817
INFO:root:current train perplexity3.400806188583374
INFO:root:current mean train loss 1548.3685881624517
INFO:root:current train perplexity3.403520107269287
INFO:root:current mean train loss 1547.9848159478636
INFO:root:current train perplexity3.400146484375
INFO:root:current mean train loss 1555.9072879074795
INFO:root:current train perplexity3.406790256500244
INFO:root:current mean train loss 1557.926105947147
INFO:root:current train perplexity3.4095373153686523
INFO:root:current mean train loss 1557.6285981971407
INFO:root:current train perplexity3.408069372177124
INFO:root:current mean train loss 1558.111425921965
INFO:root:current train perplexity3.4094479084014893
INFO:root:current mean train loss 1559.7170573121655
INFO:root:current train perplexity3.412231922149658
INFO:root:current mean train loss 1560.385897984174
INFO:root:current train perplexity3.416008710861206
INFO:root:current mean train loss 1560.7179672271914
INFO:root:current train perplexity3.418384075164795
INFO:root:current mean train loss 1561.2562274828254
INFO:root:current train perplexity3.4197800159454346
INFO:root:current mean train loss 1562.117331653384
INFO:root:current train perplexity3.421556234359741
INFO:root:current mean train loss 1562.0350968185496
INFO:root:current train perplexity3.4238688945770264
INFO:root:current mean train loss 1561.7255327834973
INFO:root:current train perplexity3.424051284790039
INFO:root:current mean train loss 1562.2039774495115
INFO:root:current train perplexity3.425967216491699
INFO:root:current mean train loss 1562.9940788240326
INFO:root:current train perplexity3.4280903339385986
INFO:root:current mean train loss 1563.474038656656
INFO:root:current train perplexity3.429197072982788
INFO:root:current mean train loss 1564.5658037915011
INFO:root:current train perplexity3.432368040084839
INFO:root:current mean train loss 1564.6041426049367
INFO:root:current train perplexity3.433483600616455

100%|██████████| 1/1 [05:22<00:00, 322.81s/it][A100%|██████████| 1/1 [05:22<00:00, 322.81s/it]
INFO:root:final mean train loss: 1564.6427977038222
INFO:root:final train perplexity: 3.4348442554473877
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.16s/it][A100%|██████████| 1/1 [00:22<00:00, 22.16s/it]
INFO:root:eval mean loss: 2018.7935288882425
INFO:root:eval perplexity: 5.117579460144043
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.01s/it][A100%|██████████| 1/1 [00:21<00:00, 21.01s/it]
INFO:root:eval mean loss: 2493.518720478031
INFO:root:eval perplexity: 7.684981822967529
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/36
 18%|█▊        | 36/200 [3:38:06<16:53:33, 370.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.2005837180398
INFO:root:current train perplexity3.3190503120422363
INFO:root:current mean train loss 1535.9867383252392
INFO:root:current train perplexity3.380246162414551
INFO:root:current mean train loss 1540.0806965760146
INFO:root:current train perplexity3.3788554668426514
INFO:root:current mean train loss 1539.637823662957
INFO:root:current train perplexity3.3818233013153076
INFO:root:current mean train loss 1540.7162367177996
INFO:root:current train perplexity3.3798015117645264
INFO:root:current mean train loss 1542.1052903027917
INFO:root:current train perplexity3.376190185546875
INFO:root:current mean train loss 1542.982891975565
INFO:root:current train perplexity3.3777048587799072
INFO:root:current mean train loss 1543.3540979913853
INFO:root:current train perplexity3.3806185722351074
INFO:root:current mean train loss 1544.745134346582
INFO:root:current train perplexity3.3820292949676514
INFO:root:current mean train loss 1545.1327997703845
INFO:root:current train perplexity3.3851466178894043
INFO:root:current mean train loss 1547.005345134188
INFO:root:current train perplexity3.3878214359283447
INFO:root:current mean train loss 1549.223849924055
INFO:root:current train perplexity3.391108512878418
INFO:root:current mean train loss 1550.0065052086022
INFO:root:current train perplexity3.3916189670562744
INFO:root:current mean train loss 1550.299955808871
INFO:root:current train perplexity3.3928160667419434
INFO:root:current mean train loss 1550.5609392994772
INFO:root:current train perplexity3.3938872814178467
INFO:root:current mean train loss 1550.7646506187696
INFO:root:current train perplexity3.395559310913086
INFO:root:current mean train loss 1550.8970331988817
INFO:root:current train perplexity3.39742374420166
INFO:root:current mean train loss 1551.112590564614
INFO:root:current train perplexity3.3982505798339844
INFO:root:current mean train loss 1552.1679343734902
INFO:root:current train perplexity3.400428295135498
INFO:root:current mean train loss 1552.024142646091
INFO:root:current train perplexity3.400691032409668

100%|██████████| 1/1 [05:21<00:00, 321.92s/it][A100%|██████████| 1/1 [05:21<00:00, 321.92s/it]
INFO:root:final mean train loss: 1552.2256730426398
INFO:root:final train perplexity: 3.4013710021972656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2021.4820149739583
INFO:root:eval perplexity: 5.128719329833984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.75s/it][A100%|██████████| 1/1 [00:21<00:00, 21.75s/it]
INFO:root:eval mean loss: 2498.559237432818
INFO:root:eval perplexity: 7.716726779937744
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/37
 18%|█▊        | 37/200 [3:45:04<17:25:45, 384.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1503.5635070800781
INFO:root:current train perplexity3.361111879348755
INFO:root:current mean train loss 1528.0134973526
INFO:root:current train perplexity3.3286800384521484
INFO:root:current mean train loss 1528.8393586811267
INFO:root:current train perplexity3.3331804275512695
INFO:root:current mean train loss 1528.1426399044874
INFO:root:current train perplexity3.3412539958953857
INFO:root:current mean train loss 1525.68346112688
INFO:root:current train perplexity3.3373639583587646
INFO:root:current mean train loss 1529.178399519487
INFO:root:current train perplexity3.3471081256866455
INFO:root:current mean train loss 1533.0333226683792
INFO:root:current train perplexity3.352637767791748
INFO:root:current mean train loss 1533.0621720198747
INFO:root:current train perplexity3.352557897567749
INFO:root:current mean train loss 1532.5270966608168
INFO:root:current train perplexity3.355290651321411
INFO:root:current mean train loss 1533.2183777381633
INFO:root:current train perplexity3.3559741973876953
INFO:root:current mean train loss 1534.2198899562256
INFO:root:current train perplexity3.356729745864868
INFO:root:current mean train loss 1534.636370611529
INFO:root:current train perplexity3.3578805923461914
INFO:root:current mean train loss 1535.5713777324663
INFO:root:current train perplexity3.3593368530273438
INFO:root:current mean train loss 1536.1351117053664
INFO:root:current train perplexity3.3595426082611084
INFO:root:current mean train loss 1537.7166368500525
INFO:root:current train perplexity3.3609163761138916
INFO:root:current mean train loss 1538.0288754607996
INFO:root:current train perplexity3.3619284629821777
INFO:root:current mean train loss 1538.345343737403
INFO:root:current train perplexity3.3648033142089844
INFO:root:current mean train loss 1539.0193857970062
INFO:root:current train perplexity3.3660945892333984
INFO:root:current mean train loss 1539.5282294943245
INFO:root:current train perplexity3.367363214492798
INFO:root:current mean train loss 1540.8845812532418
INFO:root:current train perplexity3.368232011795044

100%|██████████| 1/1 [05:25<00:00, 325.79s/it][A100%|██████████| 1/1 [05:25<00:00, 325.79s/it]
INFO:root:final mean train loss: 1540.1453946065974
INFO:root:final train perplexity: 3.3691186904907227
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.45s/it][A100%|██████████| 1/1 [00:21<00:00, 21.45s/it]
INFO:root:eval mean loss: 2023.257942794908
INFO:root:eval perplexity: 5.1360907554626465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.37s/it][A100%|██████████| 1/1 [00:21<00:00, 21.37s/it]
INFO:root:eval mean loss: 2500.1637958499555
INFO:root:eval perplexity: 7.726861476898193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/38
 19%|█▉        | 38/200 [3:51:15<17:07:39, 380.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1495.4608289930557
INFO:root:current train perplexity3.3021161556243896
INFO:root:current mean train loss 1515.9934578731143
INFO:root:current train perplexity3.308180570602417
INFO:root:current mean train loss 1514.3710474131058
INFO:root:current train perplexity3.311680555343628
INFO:root:current mean train loss 1515.438077799479
INFO:root:current train perplexity3.3124635219573975
INFO:root:current mean train loss 1516.3273848972963
INFO:root:current train perplexity3.313746929168701
INFO:root:current mean train loss 1519.791132319739
INFO:root:current train perplexity3.3182969093322754
INFO:root:current mean train loss 1520.2252723398135
INFO:root:current train perplexity3.3214099407196045
INFO:root:current mean train loss 1521.9795823065226
INFO:root:current train perplexity3.3229098320007324
INFO:root:current mean train loss 1521.6829913438426
INFO:root:current train perplexity3.3214428424835205
INFO:root:current mean train loss 1520.9614935980903
INFO:root:current train perplexity3.3201241493225098
INFO:root:current mean train loss 1522.9673058322742
INFO:root:current train perplexity3.324361801147461
INFO:root:current mean train loss 1523.7165124351802
INFO:root:current train perplexity3.324568748474121
INFO:root:current mean train loss 1523.5386446175326
INFO:root:current train perplexity3.324577569961548
INFO:root:current mean train loss 1524.0090188632666
INFO:root:current train perplexity3.3264284133911133
INFO:root:current mean train loss 1524.4009435317096
INFO:root:current train perplexity3.326838731765747
INFO:root:current mean train loss 1524.8935185009607
INFO:root:current train perplexity3.3273041248321533
INFO:root:current mean train loss 1525.1830938182704
INFO:root:current train perplexity3.3289568424224854
INFO:root:current mean train loss 1526.4792018770147
INFO:root:current train perplexity3.331347942352295
INFO:root:current mean train loss 1527.411580602134
INFO:root:current train perplexity3.3349223136901855
INFO:root:current mean train loss 1528.4112083140867
INFO:root:current train perplexity3.336587429046631

100%|██████████| 1/1 [05:21<00:00, 321.51s/it][A100%|██████████| 1/1 [05:21<00:00, 321.51s/it]
INFO:root:final mean train loss: 1527.686728149968
INFO:root:final train perplexity: 3.336177349090576
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.92s/it][A100%|██████████| 1/1 [00:21<00:00, 21.92s/it]
INFO:root:eval mean loss: 2032.2383937970967
INFO:root:eval perplexity: 5.173530101776123
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.62s/it][A100%|██████████| 1/1 [00:20<00:00, 20.62s/it]
INFO:root:eval mean loss: 2513.0465118191764
INFO:root:eval perplexity: 7.8086981773376465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/39
 20%|█▉        | 39/200 [3:57:20<16:49:26, 376.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1494.3329231508317
INFO:root:current train perplexity3.2442824840545654
INFO:root:current mean train loss 1502.4158264913676
INFO:root:current train perplexity3.2556941509246826
INFO:root:current mean train loss 1505.910262479127
INFO:root:current train perplexity3.2764666080474854
INFO:root:current mean train loss 1507.125047209513
INFO:root:current train perplexity3.2730584144592285
INFO:root:current mean train loss 1507.917478090757
INFO:root:current train perplexity3.277787208557129
INFO:root:current mean train loss 1506.5215918924462
INFO:root:current train perplexity3.2789456844329834
INFO:root:current mean train loss 1509.3606285256562
INFO:root:current train perplexity3.283759832382202
INFO:root:current mean train loss 1508.4242606576033
INFO:root:current train perplexity3.282810688018799
INFO:root:current mean train loss 1509.4524166523165
INFO:root:current train perplexity3.2874701023101807
INFO:root:current mean train loss 1510.596753411878
INFO:root:current train perplexity3.2881689071655273
INFO:root:current mean train loss 1511.7693214775704
INFO:root:current train perplexity3.290675401687622
INFO:root:current mean train loss 1512.6793225496854
INFO:root:current train perplexity3.2937567234039307
INFO:root:current mean train loss 1514.1674546424636
INFO:root:current train perplexity3.2971999645233154
INFO:root:current mean train loss 1516.176083288879
INFO:root:current train perplexity3.2989766597747803
INFO:root:current mean train loss 1516.562614305238
INFO:root:current train perplexity3.3004651069641113
INFO:root:current mean train loss 1516.7727555630302
INFO:root:current train perplexity3.30049991607666
INFO:root:current mean train loss 1516.9811973525873
INFO:root:current train perplexity3.3016374111175537
INFO:root:current mean train loss 1516.5279006178614
INFO:root:current train perplexity3.3025901317596436
INFO:root:current mean train loss 1516.6238575739712
INFO:root:current train perplexity3.3042683601379395
INFO:root:current mean train loss 1516.0996857156083
INFO:root:current train perplexity3.3050060272216797

100%|██████████| 1/1 [05:21<00:00, 321.72s/it][A100%|██████████| 1/1 [05:21<00:00, 321.72s/it]
INFO:root:final mean train loss: 1515.7606790593582
INFO:root:final train perplexity: 3.304945707321167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.70s/it][A100%|██████████| 1/1 [00:21<00:00, 21.70s/it]
INFO:root:eval mean loss: 2036.2103561509587
INFO:root:eval perplexity: 5.190174579620361
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.30s/it][A100%|██████████| 1/1 [00:21<00:00, 21.30s/it]
INFO:root:eval mean loss: 2519.190593746537
INFO:root:eval perplexity: 7.848036766052246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/40
 20%|██        | 40/200 [4:03:27<16:35:25, 373.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1490.1894222211233
INFO:root:current train perplexity3.227724313735962
INFO:root:current mean train loss 1492.960756781381
INFO:root:current train perplexity3.227787494659424
INFO:root:current mean train loss 1493.468494483647
INFO:root:current train perplexity3.2358205318450928
INFO:root:current mean train loss 1494.584890722914
INFO:root:current train perplexity3.2462453842163086
INFO:root:current mean train loss 1493.3927707871217
INFO:root:current train perplexity3.251383066177368
INFO:root:current mean train loss 1497.0697613915424
INFO:root:current train perplexity3.2525572776794434
INFO:root:current mean train loss 1496.8405416542018
INFO:root:current train perplexity3.2545459270477295
INFO:root:current mean train loss 1498.4736878146562
INFO:root:current train perplexity3.2566070556640625
INFO:root:current mean train loss 1499.045553335422
INFO:root:current train perplexity3.2578418254852295
INFO:root:current mean train loss 1499.3425564790284
INFO:root:current train perplexity3.258697986602783
INFO:root:current mean train loss 1499.445070735164
INFO:root:current train perplexity3.26206111907959
INFO:root:current mean train loss 1500.0487339061838
INFO:root:current train perplexity3.2644612789154053
INFO:root:current mean train loss 1500.8424360500453
INFO:root:current train perplexity3.266894817352295
INFO:root:current mean train loss 1501.6075371292036
INFO:root:current train perplexity3.266697406768799
INFO:root:current mean train loss 1502.0173842486213
INFO:root:current train perplexity3.268371105194092
INFO:root:current mean train loss 1502.112855975276
INFO:root:current train perplexity3.2683022022247314
INFO:root:current mean train loss 1503.088728031706
INFO:root:current train perplexity3.271076202392578
INFO:root:current mean train loss 1504.0576477908498
INFO:root:current train perplexity3.272367238998413
INFO:root:current mean train loss 1504.1829628693163
INFO:root:current train perplexity3.2720699310302734
INFO:root:current mean train loss 1503.8988152456259
INFO:root:current train perplexity3.2733523845672607

100%|██████████| 1/1 [05:24<00:00, 324.77s/it][A100%|██████████| 1/1 [05:24<00:00, 324.77s/it]
INFO:root:final mean train loss: 1503.4835384828662
INFO:root:final train perplexity: 3.2731001377105713
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.43s/it][A100%|██████████| 1/1 [00:21<00:00, 21.43s/it]
INFO:root:eval mean loss: 2046.0259122375055
INFO:root:eval perplexity: 5.231539726257324
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.12s/it][A100%|██████████| 1/1 [00:21<00:00, 21.13s/it]
INFO:root:eval mean loss: 2530.909995221077
INFO:root:eval perplexity: 7.923617362976074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/41
 20%|██        | 41/200 [4:09:36<16:25:54, 372.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1477.0815238952637
INFO:root:current train perplexity3.2247464656829834
INFO:root:current mean train loss 1477.2171169981664
INFO:root:current train perplexity3.218956232070923
INFO:root:current mean train loss 1477.9819430789432
INFO:root:current train perplexity3.2111918926239014
INFO:root:current mean train loss 1479.930989275075
INFO:root:current train perplexity3.2134571075439453
INFO:root:current mean train loss 1481.947143308578
INFO:root:current train perplexity3.2169349193573
INFO:root:current mean train loss 1484.346939598954
INFO:root:current train perplexity3.219123363494873
INFO:root:current mean train loss 1486.4151609574242
INFO:root:current train perplexity3.221257448196411
INFO:root:current mean train loss 1486.2388552565071
INFO:root:current train perplexity3.2227253913879395
INFO:root:current mean train loss 1486.752506664821
INFO:root:current train perplexity3.227141857147217
INFO:root:current mean train loss 1486.6509839406453
INFO:root:current train perplexity3.229532480239868
INFO:root:current mean train loss 1486.5839584239209
INFO:root:current train perplexity3.231480121612549
INFO:root:current mean train loss 1487.7534692056204
INFO:root:current train perplexity3.233724355697632
INFO:root:current mean train loss 1488.188478069541
INFO:root:current train perplexity3.2345926761627197
INFO:root:current mean train loss 1488.4300367470116
INFO:root:current train perplexity3.23661732673645
INFO:root:current mean train loss 1489.589416585504
INFO:root:current train perplexity3.2366323471069336
INFO:root:current mean train loss 1490.4017847199787
INFO:root:current train perplexity3.2384791374206543
INFO:root:current mean train loss 1490.387327662054
INFO:root:current train perplexity3.2403407096862793
INFO:root:current mean train loss 1490.8490858184202
INFO:root:current train perplexity3.2415833473205566
INFO:root:current mean train loss 1491.485465713694
INFO:root:current train perplexity3.2413175106048584

100%|██████████| 1/1 [05:30<00:00, 330.46s/it][A100%|██████████| 1/1 [05:30<00:00, 330.46s/it]
INFO:root:final mean train loss: 1491.6337540973273
INFO:root:final train perplexity: 3.2426538467407227
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.73s/it][A100%|██████████| 1/1 [00:21<00:00, 21.73s/it]
INFO:root:eval mean loss: 2053.413256922512
INFO:root:eval perplexity: 5.262889385223389
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.01s/it][A100%|██████████| 1/1 [00:21<00:00, 21.01s/it]
INFO:root:eval mean loss: 2541.3471770590922
INFO:root:eval perplexity: 7.991539478302002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/42
 21%|██        | 42/200 [4:15:51<16:22:01, 372.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1496.8834040715144
INFO:root:current train perplexity3.166008472442627
INFO:root:current mean train loss 1464.6657822870575
INFO:root:current train perplexity3.152939558029175
INFO:root:current mean train loss 1469.596669371699
INFO:root:current train perplexity3.17342209815979
INFO:root:current mean train loss 1470.040023072459
INFO:root:current train perplexity3.1722166538238525
INFO:root:current mean train loss 1471.80809305482
INFO:root:current train perplexity3.174405813217163
INFO:root:current mean train loss 1473.0349549410637
INFO:root:current train perplexity3.1818668842315674
INFO:root:current mean train loss 1471.582783187194
INFO:root:current train perplexity3.1835949420928955
INFO:root:current mean train loss 1471.5597452856548
INFO:root:current train perplexity3.1848747730255127
INFO:root:current mean train loss 1473.9646496086543
INFO:root:current train perplexity3.1916139125823975
INFO:root:current mean train loss 1473.7284259106739
INFO:root:current train perplexity3.1957406997680664
INFO:root:current mean train loss 1474.0751802495295
INFO:root:current train perplexity3.196237325668335
INFO:root:current mean train loss 1475.3606072191922
INFO:root:current train perplexity3.1993210315704346
INFO:root:current mean train loss 1475.0127295284162
INFO:root:current train perplexity3.2009670734405518
INFO:root:current mean train loss 1475.9944190717524
INFO:root:current train perplexity3.2015769481658936
INFO:root:current mean train loss 1476.8374769854697
INFO:root:current train perplexity3.202152967453003
INFO:root:current mean train loss 1477.581678835509
INFO:root:current train perplexity3.204087972640991
INFO:root:current mean train loss 1477.3248088195762
INFO:root:current train perplexity3.205749750137329
INFO:root:current mean train loss 1478.1693420232004
INFO:root:current train perplexity3.2076010704040527
INFO:root:current mean train loss 1479.1946766974843
INFO:root:current train perplexity3.2089040279388428
INFO:root:current mean train loss 1479.6917526157376
INFO:root:current train perplexity3.2111051082611084

100%|██████████| 1/1 [05:13<00:00, 313.80s/it][A100%|██████████| 1/1 [05:13<00:00, 313.81s/it]
INFO:root:final mean train loss: 1479.7149194668352
INFO:root:final train perplexity: 3.2123160362243652
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.99s/it][A100%|██████████| 1/1 [00:21<00:00, 21.99s/it]
INFO:root:eval mean loss: 2056.1686431668327
INFO:root:eval perplexity: 5.274629592895508
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.68s/it][A100%|██████████| 1/1 [00:20<00:00, 20.68s/it]
INFO:root:eval mean loss: 2544.6053388879654
INFO:root:eval perplexity: 8.012862205505371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/43
 22%|██▏       | 43/200 [4:21:49<16:04:16, 368.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1447.0071044921874
INFO:root:current train perplexity3.097034454345703
INFO:root:current mean train loss 1465.4508573091948
INFO:root:current train perplexity3.1469712257385254
INFO:root:current mean train loss 1461.1400905443274
INFO:root:current train perplexity3.1365976333618164
INFO:root:current mean train loss 1462.8360569809422
INFO:root:current train perplexity3.1482460498809814
INFO:root:current mean train loss 1460.6878537200218
INFO:root:current train perplexity3.1502225399017334
INFO:root:current mean train loss 1463.0322330114977
INFO:root:current train perplexity3.153350353240967
INFO:root:current mean train loss 1464.4978199792288
INFO:root:current train perplexity3.1572344303131104
INFO:root:current mean train loss 1466.3596024186643
INFO:root:current train perplexity3.1618216037750244
INFO:root:current mean train loss 1463.9731776225997
INFO:root:current train perplexity3.1612448692321777
INFO:root:current mean train loss 1464.5790029874413
INFO:root:current train perplexity3.1657662391662598
INFO:root:current mean train loss 1464.3476058811818
INFO:root:current train perplexity3.1692397594451904
INFO:root:current mean train loss 1464.8564245713496
INFO:root:current train perplexity3.171114921569824
INFO:root:current mean train loss 1464.0880629128558
INFO:root:current train perplexity3.1710405349731445
INFO:root:current mean train loss 1464.3716334292762
INFO:root:current train perplexity3.1727957725524902
INFO:root:current mean train loss 1465.7044371278137
INFO:root:current train perplexity3.174323797225952
INFO:root:current mean train loss 1466.0596575169782
INFO:root:current train perplexity3.1760809421539307
INFO:root:current mean train loss 1466.400601065232
INFO:root:current train perplexity3.1773650646209717
INFO:root:current mean train loss 1467.2238099202943
INFO:root:current train perplexity3.179896593093872
INFO:root:current mean train loss 1468.239802526255
INFO:root:current train perplexity3.181831121444702
INFO:root:current mean train loss 1468.466292022547
INFO:root:current train perplexity3.1831047534942627

100%|██████████| 1/1 [05:22<00:00, 322.01s/it][A100%|██████████| 1/1 [05:22<00:00, 322.01s/it]
INFO:root:final mean train loss: 1468.6181056127966
INFO:root:final train perplexity: 3.184326171875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.37s/it][A100%|██████████| 1/1 [00:21<00:00, 21.37s/it]
INFO:root:eval mean loss: 2064.3634556979996
INFO:root:eval perplexity: 5.309703350067139
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.28s/it][A100%|██████████| 1/1 [00:20<00:00, 20.28s/it]
INFO:root:eval mean loss: 2554.5474658722574
INFO:root:eval perplexity: 8.078280448913574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/44
 22%|██▏       | 44/200 [4:27:55<15:55:48, 367.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.3154530626662
INFO:root:current train perplexity3.112412929534912
INFO:root:current mean train loss 1452.5018476628932
INFO:root:current train perplexity3.1252524852752686
INFO:root:current mean train loss 1444.6843365503225
INFO:root:current train perplexity3.1289563179016113
INFO:root:current mean train loss 1443.9753755684887
INFO:root:current train perplexity3.1254944801330566
INFO:root:current mean train loss 1446.3168025006116
INFO:root:current train perplexity3.12955904006958
INFO:root:current mean train loss 1447.1211939503114
INFO:root:current train perplexity3.128141164779663
INFO:root:current mean train loss 1447.904792137087
INFO:root:current train perplexity3.132300853729248
INFO:root:current mean train loss 1448.3897384786542
INFO:root:current train perplexity3.1339361667633057
INFO:root:current mean train loss 1448.7864583813737
INFO:root:current train perplexity3.136883020401001
INFO:root:current mean train loss 1449.3884002782224
INFO:root:current train perplexity3.137514114379883
INFO:root:current mean train loss 1451.0975447894282
INFO:root:current train perplexity3.140709161758423
INFO:root:current mean train loss 1451.647161561921
INFO:root:current train perplexity3.1424903869628906
INFO:root:current mean train loss 1452.163548277585
INFO:root:current train perplexity3.144583225250244
INFO:root:current mean train loss 1452.8937505256183
INFO:root:current train perplexity3.1474225521087646
INFO:root:current mean train loss 1454.260121965705
INFO:root:current train perplexity3.150029420852661
INFO:root:current mean train loss 1455.3592753394928
INFO:root:current train perplexity3.1512339115142822
INFO:root:current mean train loss 1456.3335768069076
INFO:root:current train perplexity3.1510653495788574
INFO:root:current mean train loss 1456.758881506131
INFO:root:current train perplexity3.1510541439056396
INFO:root:current mean train loss 1457.3539011345467
INFO:root:current train perplexity3.152139186859131
INFO:root:current mean train loss 1457.5713967125907
INFO:root:current train perplexity3.155555248260498

100%|██████████| 1/1 [05:28<00:00, 328.57s/it][A100%|██████████| 1/1 [05:28<00:00, 328.58s/it]
INFO:root:final mean train loss: 1457.2965280568426
INFO:root:final train perplexity: 3.156019687652588
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.07s/it][A100%|██████████| 1/1 [00:22<00:00, 22.07s/it]
INFO:root:eval mean loss: 2067.4590748455507
INFO:root:eval perplexity: 5.323012828826904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.39s/it][A100%|██████████| 1/1 [00:20<00:00, 20.39s/it]
INFO:root:eval mean loss: 2560.4509545725286
INFO:root:eval perplexity: 8.117378234863281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/45
 22%|██▎       | 45/200 [4:34:08<15:53:42, 369.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1419.422077178955
INFO:root:current train perplexity3.082207202911377
INFO:root:current mean train loss 1426.1031873749523
INFO:root:current train perplexity3.081904649734497
INFO:root:current mean train loss 1431.7631863680754
INFO:root:current train perplexity3.0917563438415527
INFO:root:current mean train loss 1435.6425962343321
INFO:root:current train perplexity3.102506160736084
INFO:root:current mean train loss 1436.3497132926152
INFO:root:current train perplexity3.101043939590454
INFO:root:current mean train loss 1437.8628018426557
INFO:root:current train perplexity3.103546142578125
INFO:root:current mean train loss 1439.3141369187688
INFO:root:current train perplexity3.1055619716644287
INFO:root:current mean train loss 1439.7474352452143
INFO:root:current train perplexity3.1071505546569824
INFO:root:current mean train loss 1439.3926999127423
INFO:root:current train perplexity3.1092700958251953
INFO:root:current mean train loss 1440.5911724676234
INFO:root:current train perplexity3.1131508350372314
INFO:root:current mean train loss 1441.1189362948999
INFO:root:current train perplexity3.1147329807281494
INFO:root:current mean train loss 1441.4682057174211
INFO:root:current train perplexity3.1152286529541016
INFO:root:current mean train loss 1442.5384922269025
INFO:root:current train perplexity3.116049289703369
INFO:root:current mean train loss 1443.9537517290312
INFO:root:current train perplexity3.1183223724365234
INFO:root:current mean train loss 1444.0961819841561
INFO:root:current train perplexity3.1192007064819336
INFO:root:current mean train loss 1443.9593480883352
INFO:root:current train perplexity3.120527505874634
INFO:root:current mean train loss 1444.5825078670796
INFO:root:current train perplexity3.1244776248931885
INFO:root:current mean train loss 1444.7779263520188
INFO:root:current train perplexity3.1257927417755127
INFO:root:current mean train loss 1445.4394052530051
INFO:root:current train perplexity3.1259305477142334
INFO:root:current mean train loss 1446.027759746233
INFO:root:current train perplexity3.1269826889038086

100%|██████████| 1/1 [05:25<00:00, 325.44s/it][A100%|██████████| 1/1 [05:25<00:00, 325.44s/it]
INFO:root:final mean train loss: 1445.5502756708388
INFO:root:final train perplexity: 3.126917839050293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.81s/it][A100%|██████████| 1/1 [00:21<00:00, 21.81s/it]
INFO:root:eval mean loss: 2078.6236888263243
INFO:root:eval perplexity: 5.371293544769287
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.17s/it][A100%|██████████| 1/1 [00:21<00:00, 21.17s/it]
INFO:root:eval mean loss: 2571.6284365823085
INFO:root:eval perplexity: 8.191919326782227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/46
 23%|██▎       | 46/200 [4:40:18<15:48:21, 369.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1427.7742166401426
INFO:root:current train perplexity3.0843617916107178
INFO:root:current mean train loss 1425.1904000129489
INFO:root:current train perplexity3.0765702724456787
INFO:root:current mean train loss 1422.8456722816115
INFO:root:current train perplexity3.0788745880126953
INFO:root:current mean train loss 1423.3772965879266
INFO:root:current train perplexity3.0778214931488037
INFO:root:current mean train loss 1425.500879363062
INFO:root:current train perplexity3.075580596923828
INFO:root:current mean train loss 1425.0538561192313
INFO:root:current train perplexity3.0761241912841797
INFO:root:current mean train loss 1426.4197042636058
INFO:root:current train perplexity3.078237533569336
INFO:root:current mean train loss 1427.4882463950964
INFO:root:current train perplexity3.0806260108947754
INFO:root:current mean train loss 1427.9962137419304
INFO:root:current train perplexity3.083409309387207
INFO:root:current mean train loss 1428.5162859964323
INFO:root:current train perplexity3.087028741836548
INFO:root:current mean train loss 1428.5356743430561
INFO:root:current train perplexity3.0877983570098877
INFO:root:current mean train loss 1430.1413339587414
INFO:root:current train perplexity3.0878241062164307
INFO:root:current mean train loss 1431.2551641173889
INFO:root:current train perplexity3.0891261100769043
INFO:root:current mean train loss 1431.5569996761292
INFO:root:current train perplexity3.089817762374878
INFO:root:current mean train loss 1432.897984661177
INFO:root:current train perplexity3.0929696559906006
INFO:root:current mean train loss 1433.666358827112
INFO:root:current train perplexity3.0946857929229736
INFO:root:current mean train loss 1434.1637274420452
INFO:root:current train perplexity3.096834659576416
INFO:root:current mean train loss 1434.7801017439947
INFO:root:current train perplexity3.098446846008301
INFO:root:current mean train loss 1435.788604590311
INFO:root:current train perplexity3.099900722503662
INFO:root:current mean train loss 1435.6328383190112
INFO:root:current train perplexity3.101389169692993

100%|██████████| 1/1 [05:19<00:00, 319.30s/it][A100%|██████████| 1/1 [05:19<00:00, 319.30s/it]
INFO:root:final mean train loss: 1435.1664759895143
INFO:root:final train perplexity: 3.1014153957366943
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.36s/it][A100%|██████████| 1/1 [00:21<00:00, 21.36s/it]
INFO:root:eval mean loss: 2087.7676097247618
INFO:root:eval perplexity: 5.41116189956665
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.81s/it][A100%|██████████| 1/1 [00:20<00:00, 20.81s/it]
INFO:root:eval mean loss: 2584.0316906651706
INFO:root:eval perplexity: 8.27544116973877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/47
 24%|██▎       | 47/200 [4:46:21<15:37:22, 367.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.946225536113
INFO:root:current train perplexity3.0310280323028564
INFO:root:current mean train loss 1407.8075253265072
INFO:root:current train perplexity3.0296928882598877
INFO:root:current mean train loss 1409.208127425021
INFO:root:current train perplexity3.045597791671753
INFO:root:current mean train loss 1409.1890792463294
INFO:root:current train perplexity3.0459237098693848
INFO:root:current mean train loss 1411.2522455544836
INFO:root:current train perplexity3.052804946899414
INFO:root:current mean train loss 1412.5433067908655
INFO:root:current train perplexity3.053647994995117
INFO:root:current mean train loss 1415.341484004242
INFO:root:current train perplexity3.0550878047943115
INFO:root:current mean train loss 1416.2927547445274
INFO:root:current train perplexity3.0584864616394043
INFO:root:current mean train loss 1415.8622142902195
INFO:root:current train perplexity3.0596117973327637
INFO:root:current mean train loss 1417.7920499201527
INFO:root:current train perplexity3.0622293949127197
INFO:root:current mean train loss 1418.2728438247098
INFO:root:current train perplexity3.0632996559143066
INFO:root:current mean train loss 1419.7640523512496
INFO:root:current train perplexity3.064757823944092
INFO:root:current mean train loss 1420.3227919003996
INFO:root:current train perplexity3.0647244453430176
INFO:root:current mean train loss 1421.1717315368217
INFO:root:current train perplexity3.0672483444213867
INFO:root:current mean train loss 1421.858913447096
INFO:root:current train perplexity3.067690372467041
INFO:root:current mean train loss 1422.3711430975732
INFO:root:current train perplexity3.069286346435547
INFO:root:current mean train loss 1423.4703698399771
INFO:root:current train perplexity3.071254014968872
INFO:root:current mean train loss 1423.656454830997
INFO:root:current train perplexity3.0721547603607178
INFO:root:current mean train loss 1424.332681477007
INFO:root:current train perplexity3.072845935821533

100%|██████████| 1/1 [05:18<00:00, 318.88s/it][A100%|██████████| 1/1 [05:18<00:00, 318.88s/it]
INFO:root:final mean train loss: 1424.1869601635879
INFO:root:final train perplexity: 3.074676275253296
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.66s/it][A100%|██████████| 1/1 [00:22<00:00, 22.66s/it]
INFO:root:eval mean loss: 2088.12477533868
INFO:root:eval perplexity: 5.412725448608398
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.80s/it][A100%|██████████| 1/1 [00:20<00:00, 20.80s/it]
INFO:root:eval mean loss: 2585.4252423225566
INFO:root:eval perplexity: 8.284875869750977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/48
 24%|██▍       | 48/200 [4:52:25<15:28:36, 366.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.7633951822916
INFO:root:current train perplexity3.030134677886963
INFO:root:current mean train loss 1398.5191172724185
INFO:root:current train perplexity3.028045654296875
INFO:root:current mean train loss 1400.1672385992006
INFO:root:current train perplexity3.0364911556243896
INFO:root:current mean train loss 1400.2656393384177
INFO:root:current train perplexity3.0269477367401123
INFO:root:current mean train loss 1400.7989610786897
INFO:root:current train perplexity3.0272445678710938
INFO:root:current mean train loss 1404.2265437746512
INFO:root:current train perplexity3.0280261039733887
INFO:root:current mean train loss 1405.882035616743
INFO:root:current train perplexity3.0300240516662598
INFO:root:current mean train loss 1405.6573249016608
INFO:root:current train perplexity3.0305707454681396
INFO:root:current mean train loss 1408.4709295916412
INFO:root:current train perplexity3.034050941467285
INFO:root:current mean train loss 1408.3550190242913
INFO:root:current train perplexity3.0320348739624023
INFO:root:current mean train loss 1408.3775230670797
INFO:root:current train perplexity3.0332834720611572
INFO:root:current mean train loss 1408.9688349565583
INFO:root:current train perplexity3.0353479385375977
INFO:root:current mean train loss 1409.1704910341114
INFO:root:current train perplexity3.037013292312622
INFO:root:current mean train loss 1409.8597768573254
INFO:root:current train perplexity3.037821054458618
INFO:root:current mean train loss 1410.4250899783292
INFO:root:current train perplexity3.039647102355957
INFO:root:current mean train loss 1410.701877707302
INFO:root:current train perplexity3.040134906768799
INFO:root:current mean train loss 1411.6908851647156
INFO:root:current train perplexity3.0432260036468506
INFO:root:current mean train loss 1412.358745786261
INFO:root:current train perplexity3.043529748916626
INFO:root:current mean train loss 1413.0930746115273
INFO:root:current train perplexity3.045832633972168
INFO:root:current mean train loss 1413.4703101414614
INFO:root:current train perplexity3.0470409393310547

100%|██████████| 1/1 [05:18<00:00, 318.06s/it][A100%|██████████| 1/1 [05:18<00:00, 318.06s/it]
INFO:root:final mean train loss: 1413.0544625963757
INFO:root:final train perplexity: 3.0477993488311768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.51s/it][A100%|██████████| 1/1 [00:22<00:00, 22.51s/it]
INFO:root:eval mean loss: 2098.248743801252
INFO:root:eval perplexity: 5.457225799560547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.77s/it][A100%|██████████| 1/1 [00:20<00:00, 20.77s/it]
INFO:root:eval mean loss: 2596.4092532759864
INFO:root:eval perplexity: 8.359635353088379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/49
 24%|██▍       | 49/200 [4:58:28<15:19:55, 365.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1382.1908988952637
INFO:root:current train perplexity2.9548871517181396
INFO:root:current mean train loss 1388.3338280880089
INFO:root:current train perplexity2.972010850906372
INFO:root:current mean train loss 1386.210741766568
INFO:root:current train perplexity2.9890429973602295
INFO:root:current mean train loss 1388.432423419263
INFO:root:current train perplexity2.9905571937561035
INFO:root:current mean train loss 1388.3823185673466
INFO:root:current train perplexity2.98978853225708
INFO:root:current mean train loss 1391.698332134046
INFO:root:current train perplexity2.996645927429199
INFO:root:current mean train loss 1394.5721711750273
INFO:root:current train perplexity3.0022339820861816
INFO:root:current mean train loss 1395.816793639803
INFO:root:current train perplexity3.005239486694336
INFO:root:current mean train loss 1394.7790534679707
INFO:root:current train perplexity3.0042638778686523
INFO:root:current mean train loss 1395.2877326932587
INFO:root:current train perplexity3.0081934928894043
INFO:root:current mean train loss 1396.9118890096975
INFO:root:current train perplexity3.011197090148926
INFO:root:current mean train loss 1397.3802339264023
INFO:root:current train perplexity3.010502576828003
INFO:root:current mean train loss 1398.5158308945693
INFO:root:current train perplexity3.013084888458252
INFO:root:current mean train loss 1400.1052425716732
INFO:root:current train perplexity3.0148680210113525
INFO:root:current mean train loss 1400.3505812490453
INFO:root:current train perplexity3.01615309715271
INFO:root:current mean train loss 1400.928070865163
INFO:root:current train perplexity3.0181708335876465
INFO:root:current mean train loss 1400.8825474159391
INFO:root:current train perplexity3.0187535285949707
INFO:root:current mean train loss 1401.6247624139587
INFO:root:current train perplexity3.0203256607055664
INFO:root:current mean train loss 1402.3089029903495
INFO:root:current train perplexity3.0217669010162354
INFO:root:current mean train loss 1402.8239425122122
INFO:root:current train perplexity3.022951602935791

100%|██████████| 1/1 [05:25<00:00, 325.84s/it][A100%|██████████| 1/1 [05:25<00:00, 325.84s/it]
INFO:root:final mean train loss: 1402.867459618919
INFO:root:final train perplexity: 3.0234107971191406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.58s/it][A100%|██████████| 1/1 [00:22<00:00, 22.58s/it]
INFO:root:eval mean loss: 2100.0190408043827
INFO:root:eval perplexity: 5.465043544769287
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.09s/it][A100%|██████████| 1/1 [00:21<00:00, 21.09s/it]
INFO:root:eval mean loss: 2597.5799058413677
INFO:root:eval perplexity: 8.367643356323242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/50
 25%|██▌       | 50/200 [5:04:40<15:18:12, 367.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1375.4299166932399
INFO:root:current train perplexity2.9789645671844482
INFO:root:current mean train loss 1383.1418006436136
INFO:root:current train perplexity2.9786674976348877
INFO:root:current mean train loss 1383.5330884083207
INFO:root:current train perplexity2.979949712753296
INFO:root:current mean train loss 1385.9473271848137
INFO:root:current train perplexity2.984724998474121
INFO:root:current mean train loss 1386.0389472264756
INFO:root:current train perplexity2.9789481163024902
INFO:root:current mean train loss 1385.4654456522512
INFO:root:current train perplexity2.9801583290100098
INFO:root:current mean train loss 1385.825941464934
INFO:root:current train perplexity2.983150005340576
INFO:root:current mean train loss 1385.2893302723944
INFO:root:current train perplexity2.983227491378784
INFO:root:current mean train loss 1386.3003310707911
INFO:root:current train perplexity2.985342025756836
INFO:root:current mean train loss 1387.2841499738622
INFO:root:current train perplexity2.984971046447754
INFO:root:current mean train loss 1387.9161367643665
INFO:root:current train perplexity2.986426591873169
INFO:root:current mean train loss 1388.5134656622267
INFO:root:current train perplexity2.986654281616211
INFO:root:current mean train loss 1388.9787420756918
INFO:root:current train perplexity2.9876296520233154
INFO:root:current mean train loss 1389.6355029333072
INFO:root:current train perplexity2.9892213344573975
INFO:root:current mean train loss 1390.131988083107
INFO:root:current train perplexity2.991082191467285
INFO:root:current mean train loss 1390.2683278053632
INFO:root:current train perplexity2.993201494216919
INFO:root:current mean train loss 1391.015149377345
INFO:root:current train perplexity2.9950802326202393
INFO:root:current mean train loss 1391.2058598914782
INFO:root:current train perplexity2.995396375656128
INFO:root:current mean train loss 1392.0559385061392
INFO:root:current train perplexity2.996957540512085
INFO:root:current mean train loss 1392.5994390778324
INFO:root:current train perplexity2.9974563121795654

100%|██████████| 1/1 [05:21<00:00, 321.19s/it][A100%|██████████| 1/1 [05:21<00:00, 321.19s/it]
INFO:root:final mean train loss: 1392.4066669658405
INFO:root:final train perplexity: 2.998570680618286
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.29s/it][A100%|██████████| 1/1 [00:21<00:00, 21.29s/it]
INFO:root:eval mean loss: 2114.16523991578
INFO:root:eval perplexity: 5.527926445007324
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.43s/it][A100%|██████████| 1/1 [00:20<00:00, 20.43s/it]
INFO:root:eval mean loss: 2615.3898631427305
INFO:root:eval perplexity: 8.490411758422852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/51
 26%|██▌       | 51/200 [5:10:44<15:10:08, 366.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1360.6769446170692
INFO:root:current train perplexity2.9444687366485596
INFO:root:current mean train loss 1362.6497765966208
INFO:root:current train perplexity2.9407289028167725
INFO:root:current mean train loss 1364.0641144487195
INFO:root:current train perplexity2.9479310512542725
INFO:root:current mean train loss 1367.0097606221184
INFO:root:current train perplexity2.955801248550415
INFO:root:current mean train loss 1369.3464363327355
INFO:root:current train perplexity2.9549336433410645
INFO:root:current mean train loss 1372.8270265828594
INFO:root:current train perplexity2.9587788581848145
INFO:root:current mean train loss 1374.1949206286365
INFO:root:current train perplexity2.9588847160339355
INFO:root:current mean train loss 1374.3574770138096
INFO:root:current train perplexity2.957977533340454
INFO:root:current mean train loss 1375.5258254828684
INFO:root:current train perplexity2.9596188068389893
INFO:root:current mean train loss 1376.1371928781703
INFO:root:current train perplexity2.9601869583129883
INFO:root:current mean train loss 1377.216997844417
INFO:root:current train perplexity2.9626290798187256
INFO:root:current mean train loss 1377.6489656687193
INFO:root:current train perplexity2.9628536701202393
INFO:root:current mean train loss 1377.7551666790087
INFO:root:current train perplexity2.9646973609924316
INFO:root:current mean train loss 1378.8303857135913
INFO:root:current train perplexity2.9661784172058105
INFO:root:current mean train loss 1379.8842312967631
INFO:root:current train perplexity2.9674885272979736
INFO:root:current mean train loss 1381.0844360195663
INFO:root:current train perplexity2.9696593284606934
INFO:root:current mean train loss 1381.5375242382108
INFO:root:current train perplexity2.97023606300354
INFO:root:current mean train loss 1381.9965694509574
INFO:root:current train perplexity2.9718127250671387
INFO:root:current mean train loss 1382.4611501744835
INFO:root:current train perplexity2.973921775817871
INFO:root:current mean train loss 1382.854546082226
INFO:root:current train perplexity2.975217819213867

100%|██████████| 1/1 [05:19<00:00, 319.29s/it][A100%|██████████| 1/1 [05:19<00:00, 319.29s/it]
INFO:root:final mean train loss: 1382.700167795894
INFO:root:final train perplexity: 2.975703477859497
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.92s/it][A100%|██████████| 1/1 [00:21<00:00, 21.92s/it]
INFO:root:eval mean loss: 2120.7937046348625
INFO:root:eval perplexity: 5.557639122009277
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.67s/it][A100%|██████████| 1/1 [00:21<00:00, 21.67s/it]
INFO:root:eval mean loss: 2627.338652049396
INFO:root:eval perplexity: 8.57379150390625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/52
 26%|██▌       | 52/200 [5:16:49<15:02:43, 365.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1352.9122652720257
INFO:root:current train perplexity2.907224178314209
INFO:root:current mean train loss 1359.051138789276
INFO:root:current train perplexity2.9109046459198
INFO:root:current mean train loss 1357.3854497363627
INFO:root:current train perplexity2.919100284576416
INFO:root:current mean train loss 1356.6633128671672
INFO:root:current train perplexity2.9213411808013916
INFO:root:current mean train loss 1358.9755540930707
INFO:root:current train perplexity2.928600549697876
INFO:root:current mean train loss 1359.2959373827455
INFO:root:current train perplexity2.9304800033569336
INFO:root:current mean train loss 1360.6791350558772
INFO:root:current train perplexity2.9317612648010254
INFO:root:current mean train loss 1363.279941212933
INFO:root:current train perplexity2.9317140579223633
INFO:root:current mean train loss 1364.412722353217
INFO:root:current train perplexity2.934302806854248
INFO:root:current mean train loss 1365.845623524725
INFO:root:current train perplexity2.9362127780914307
INFO:root:current mean train loss 1367.1996856154058
INFO:root:current train perplexity2.936821460723877
INFO:root:current mean train loss 1367.7751498895486
INFO:root:current train perplexity2.9378974437713623
INFO:root:current mean train loss 1368.4196399620337
INFO:root:current train perplexity2.940070629119873
INFO:root:current mean train loss 1368.9963107050457
INFO:root:current train perplexity2.9408462047576904
INFO:root:current mean train loss 1369.8273169323952
INFO:root:current train perplexity2.9429478645324707
INFO:root:current mean train loss 1370.4197305569676
INFO:root:current train perplexity2.9442362785339355
INFO:root:current mean train loss 1370.8841598429144
INFO:root:current train perplexity2.946274518966675
INFO:root:current mean train loss 1371.5615107033002
INFO:root:current train perplexity2.9479141235351562
INFO:root:current mean train loss 1371.9326436371482
INFO:root:current train perplexity2.9485464096069336
INFO:root:current mean train loss 1372.0779770443792
INFO:root:current train perplexity2.9508795738220215

100%|██████████| 1/1 [05:20<00:00, 320.97s/it][A100%|██████████| 1/1 [05:20<00:00, 320.97s/it]
INFO:root:final mean train loss: 1372.0779770443792
INFO:root:final train perplexity: 2.9508795738220215
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.40s/it][A100%|██████████| 1/1 [00:23<00:00, 23.40s/it]
INFO:root:eval mean loss: 2121.9696278673537
INFO:root:eval perplexity: 5.56292724609375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.84s/it][A100%|██████████| 1/1 [00:20<00:00, 20.84s/it]
INFO:root:eval mean loss: 2626.9586822293327
INFO:root:eval perplexity: 8.571124076843262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/53
 26%|██▋       | 53/200 [5:22:56<14:57:24, 366.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1353.9606103515625
INFO:root:current train perplexity2.896388530731201
INFO:root:current mean train loss 1349.2525347900391
INFO:root:current train perplexity2.892228126525879
INFO:root:current mean train loss 1348.637587483724
INFO:root:current train perplexity2.897428274154663
INFO:root:current mean train loss 1352.2102938842772
INFO:root:current train perplexity2.901602029800415
INFO:root:current mean train loss 1351.8904489746094
INFO:root:current train perplexity2.9056613445281982
INFO:root:current mean train loss 1352.1394661458332
INFO:root:current train perplexity2.90785813331604
INFO:root:current mean train loss 1352.2750169154576
INFO:root:current train perplexity2.9108755588531494
INFO:root:current mean train loss 1354.9007078552247
INFO:root:current train perplexity2.913825273513794
INFO:root:current mean train loss 1354.5843840874566
INFO:root:current train perplexity2.915705680847168
INFO:root:current mean train loss 1354.8179161376954
INFO:root:current train perplexity2.9160594940185547
INFO:root:current mean train loss 1355.9854113769532
INFO:root:current train perplexity2.917224645614624
INFO:root:current mean train loss 1356.2667811075846
INFO:root:current train perplexity2.9180777072906494
INFO:root:current mean train loss 1357.849485144982
INFO:root:current train perplexity2.92065691947937
INFO:root:current mean train loss 1357.8318538992746
INFO:root:current train perplexity2.921922445297241
INFO:root:current mean train loss 1358.5552170410156
INFO:root:current train perplexity2.923543691635132
INFO:root:current mean train loss 1359.5847317504883
INFO:root:current train perplexity2.9258499145507812
INFO:root:current mean train loss 1360.7403030934054
INFO:root:current train perplexity2.9263596534729004
INFO:root:current mean train loss 1361.6777313232421
INFO:root:current train perplexity2.927755355834961
INFO:root:current mean train loss 1361.76792030736
INFO:root:current train perplexity2.9271068572998047

100%|██████████| 1/1 [05:31<00:00, 331.49s/it][A100%|██████████| 1/1 [05:31<00:00, 331.49s/it]
INFO:root:final mean train loss: 1361.9689458172788
INFO:root:final train perplexity: 2.9274466037750244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.71s/it][A100%|██████████| 1/1 [00:21<00:00, 21.71s/it]
INFO:root:eval mean loss: 2133.2736058877713
INFO:root:eval perplexity: 5.614017486572266
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.20s/it][A100%|██████████| 1/1 [00:21<00:00, 21.20s/it]
INFO:root:eval mean loss: 2640.231438819398
INFO:root:eval perplexity: 8.66466999053955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/54
 27%|██▋       | 54/200 [5:29:12<14:58:30, 369.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1342.5501278147979
INFO:root:current train perplexity2.8620126247406006
INFO:root:current mean train loss 1340.1107563768696
INFO:root:current train perplexity2.8643171787261963
INFO:root:current mean train loss 1340.0312955654163
INFO:root:current train perplexity2.8685107231140137
INFO:root:current mean train loss 1338.5101526302494
INFO:root:current train perplexity2.8692545890808105
INFO:root:current mean train loss 1338.4548140784248
INFO:root:current train perplexity2.876708745956421
INFO:root:current mean train loss 1341.9693834906159
INFO:root:current train perplexity2.8833961486816406
INFO:root:current mean train loss 1342.1507960092306
INFO:root:current train perplexity2.8856277465820312
INFO:root:current mean train loss 1342.746995061345
INFO:root:current train perplexity2.8859074115753174
INFO:root:current mean train loss 1343.7869196206586
INFO:root:current train perplexity2.8897414207458496
INFO:root:current mean train loss 1345.246118110815
INFO:root:current train perplexity2.89119029045105
INFO:root:current mean train loss 1347.1236671890363
INFO:root:current train perplexity2.8923285007476807
INFO:root:current mean train loss 1347.7760309203993
INFO:root:current train perplexity2.894279718399048
INFO:root:current mean train loss 1347.7147015185267
INFO:root:current train perplexity2.8955023288726807
INFO:root:current mean train loss 1347.9900406567365
INFO:root:current train perplexity2.8958916664123535
INFO:root:current mean train loss 1348.7450303513144
INFO:root:current train perplexity2.8971996307373047
INFO:root:current mean train loss 1349.862413239149
INFO:root:current train perplexity2.8988945484161377
INFO:root:current mean train loss 1350.1286579573236
INFO:root:current train perplexity2.9009363651275635
INFO:root:current mean train loss 1350.8191782485349
INFO:root:current train perplexity2.9017622470855713
INFO:root:current mean train loss 1351.4276288315432
INFO:root:current train perplexity2.903017520904541
INFO:root:current mean train loss 1351.7106560441935
INFO:root:current train perplexity2.9039011001586914

100%|██████████| 1/1 [05:19<00:00, 319.93s/it][A100%|██████████| 1/1 [05:19<00:00, 319.93s/it]
INFO:root:final mean train loss: 1352.1143407008892
INFO:root:final train perplexity: 2.904783010482788
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.08s/it][A100%|██████████| 1/1 [00:22<00:00, 22.08s/it]
INFO:root:eval mean loss: 2138.4620456560283
INFO:root:eval perplexity: 5.637622356414795
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.35s/it][A100%|██████████| 1/1 [00:20<00:00, 20.35s/it]
INFO:root:eval mean loss: 2645.5536529324577
INFO:root:eval perplexity: 8.702466011047363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/55
 28%|██▊       | 55/200 [5:35:16<14:48:38, 367.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1332.052942612592
INFO:root:current train perplexity2.879251718521118
INFO:root:current mean train loss 1330.676865307253
INFO:root:current train perplexity2.8631391525268555
INFO:root:current mean train loss 1331.1697439862112
INFO:root:current train perplexity2.8713364601135254
INFO:root:current mean train loss 1331.1670762478948
INFO:root:current train perplexity2.873141050338745
INFO:root:current mean train loss 1333.4376147573444
INFO:root:current train perplexity2.8713018894195557
INFO:root:current mean train loss 1334.125696760885
INFO:root:current train perplexity2.869499683380127
INFO:root:current mean train loss 1335.3764823648833
INFO:root:current train perplexity2.868664503097534
INFO:root:current mean train loss 1336.6738050081424
INFO:root:current train perplexity2.870068311691284
INFO:root:current mean train loss 1337.4397350283834
INFO:root:current train perplexity2.8701932430267334
INFO:root:current mean train loss 1337.0953004498042
INFO:root:current train perplexity2.869973659515381
INFO:root:current mean train loss 1337.9271162317154
INFO:root:current train perplexity2.8715741634368896
INFO:root:current mean train loss 1338.3037152433312
INFO:root:current train perplexity2.8725903034210205
INFO:root:current mean train loss 1339.2339082838457
INFO:root:current train perplexity2.874764919281006
INFO:root:current mean train loss 1340.0063059290667
INFO:root:current train perplexity2.875652551651001
INFO:root:current mean train loss 1340.4198539446588
INFO:root:current train perplexity2.877852201461792
INFO:root:current mean train loss 1340.7314453920765
INFO:root:current train perplexity2.879972457885742
INFO:root:current mean train loss 1341.39723659408
INFO:root:current train perplexity2.880941390991211
INFO:root:current mean train loss 1341.6102221707847
INFO:root:current train perplexity2.8816421031951904
INFO:root:current mean train loss 1342.4377806152877
INFO:root:current train perplexity2.8831167221069336
INFO:root:current mean train loss 1342.7842799189666
INFO:root:current train perplexity2.8838415145874023

100%|██████████| 1/1 [05:19<00:00, 319.11s/it][A100%|██████████| 1/1 [05:19<00:00, 319.11s/it]
INFO:root:final mean train loss: 1342.950757894742
INFO:root:final train perplexity: 2.88386607170105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.48s/it][A100%|██████████| 1/1 [00:21<00:00, 21.48s/it]
INFO:root:eval mean loss: 2145.461709313359
INFO:root:eval perplexity: 5.669628620147705
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.09s/it][A100%|██████████| 1/1 [00:21<00:00, 21.09s/it]
INFO:root:eval mean loss: 2656.211581182818
INFO:root:eval perplexity: 8.778653144836426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/56
 28%|██▊       | 56/200 [5:41:20<14:39:28, 366.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1322.8531135110295
INFO:root:current train perplexity2.826892137527466
INFO:root:current mean train loss 1324.3885659729408
INFO:root:current train perplexity2.8291566371917725
INFO:root:current mean train loss 1323.4055754520978
INFO:root:current train perplexity2.8275034427642822
INFO:root:current mean train loss 1325.3733595280225
INFO:root:current train perplexity2.836575508117676
INFO:root:current mean train loss 1326.4199908947996
INFO:root:current train perplexity2.8367767333984375
INFO:root:current mean train loss 1326.8795256848343
INFO:root:current train perplexity2.8398561477661133
INFO:root:current mean train loss 1326.237782468078
INFO:root:current train perplexity2.840184211730957
INFO:root:current mean train loss 1327.5443204633405
INFO:root:current train perplexity2.841507911682129
INFO:root:current mean train loss 1328.403631297738
INFO:root:current train perplexity2.8440802097320557
INFO:root:current mean train loss 1328.3553262704554
INFO:root:current train perplexity2.845628023147583
INFO:root:current mean train loss 1328.5192868770814
INFO:root:current train perplexity2.8482143878936768
INFO:root:current mean train loss 1328.9111469179313
INFO:root:current train perplexity2.849891424179077
INFO:root:current mean train loss 1328.9913142728005
INFO:root:current train perplexity2.8504581451416016
INFO:root:current mean train loss 1329.3872691054949
INFO:root:current train perplexity2.850804090499878
INFO:root:current mean train loss 1329.6603637947699
INFO:root:current train perplexity2.8508734703063965
INFO:root:current mean train loss 1330.724354766708
INFO:root:current train perplexity2.853438138961792
INFO:root:current mean train loss 1331.2791613481177
INFO:root:current train perplexity2.856407880783081
INFO:root:current mean train loss 1331.543866186398
INFO:root:current train perplexity2.857736587524414
INFO:root:current mean train loss 1331.6832157343158
INFO:root:current train perplexity2.8591341972351074
INFO:root:current mean train loss 1332.8908009414242
INFO:root:current train perplexity2.860999584197998

100%|██████████| 1/1 [05:25<00:00, 325.07s/it][A100%|██████████| 1/1 [05:25<00:00, 325.07s/it]
INFO:root:final mean train loss: 1332.9018971437886
INFO:root:final train perplexity: 2.8611011505126953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.26s/it][A100%|██████████| 1/1 [00:22<00:00, 22.26s/it]
INFO:root:eval mean loss: 2150.2319175774323
INFO:root:eval perplexity: 5.691542148590088
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.05s/it][A100%|██████████| 1/1 [00:21<00:00, 21.05s/it]
INFO:root:eval mean loss: 2662.3420154760915
INFO:root:eval perplexity: 8.822774887084961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/57
 28%|██▊       | 57/200 [5:47:30<14:36:04, 367.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1321.5786581600414
INFO:root:current train perplexity2.8016891479492188
INFO:root:current mean train loss 1315.6473613920666
INFO:root:current train perplexity2.814211368560791
INFO:root:current mean train loss 1315.7385982684234
INFO:root:current train perplexity2.807776689529419
INFO:root:current mean train loss 1314.537488191024
INFO:root:current train perplexity2.805258274078369
INFO:root:current mean train loss 1315.8838858319143
INFO:root:current train perplexity2.8085343837738037
INFO:root:current mean train loss 1316.2184633066956
INFO:root:current train perplexity2.8138787746429443
INFO:root:current mean train loss 1314.5037474489498
INFO:root:current train perplexity2.816366195678711
INFO:root:current mean train loss 1314.6976941426594
INFO:root:current train perplexity2.8180127143859863
INFO:root:current mean train loss 1316.5939162293887
INFO:root:current train perplexity2.8203694820404053
INFO:root:current mean train loss 1317.6366039938177
INFO:root:current train perplexity2.8214356899261475
INFO:root:current mean train loss 1319.0628883847583
INFO:root:current train perplexity2.82499098777771
INFO:root:current mean train loss 1319.4876154024307
INFO:root:current train perplexity2.826097249984741
INFO:root:current mean train loss 1320.0059229131753
INFO:root:current train perplexity2.8282501697540283
INFO:root:current mean train loss 1320.7402416920802
INFO:root:current train perplexity2.8307666778564453
INFO:root:current mean train loss 1321.1090614256482
INFO:root:current train perplexity2.833804130554199
INFO:root:current mean train loss 1321.9323168384785
INFO:root:current train perplexity2.8356785774230957
INFO:root:current mean train loss 1322.367676366719
INFO:root:current train perplexity2.8365139961242676
INFO:root:current mean train loss 1322.4325278441831
INFO:root:current train perplexity2.837475538253784
INFO:root:current mean train loss 1323.4225724606238
INFO:root:current train perplexity2.8388845920562744
INFO:root:current mean train loss 1323.644315021794
INFO:root:current train perplexity2.8393683433532715

100%|██████████| 1/1 [05:23<00:00, 323.02s/it][A100%|██████████| 1/1 [05:23<00:00, 323.03s/it]
INFO:root:final mean train loss: 1323.3386957473485
INFO:root:final train perplexity: 2.8396036624908447
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.48s/it][A100%|██████████| 1/1 [00:21<00:00, 21.48s/it]
INFO:root:eval mean loss: 2162.531242208278
INFO:root:eval perplexity: 5.748438358306885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.94s/it][A100%|██████████| 1/1 [00:20<00:00, 20.94s/it]
INFO:root:eval mean loss: 2678.039655103751
INFO:root:eval perplexity: 8.936771392822266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/58
 29%|██▉       | 58/200 [5:53:37<14:29:44, 367.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1306.4340734145221
INFO:root:current train perplexity2.7809042930603027
INFO:root:current mean train loss 1302.8272612700591
INFO:root:current train perplexity2.790520668029785
INFO:root:current mean train loss 1302.4486872087446
INFO:root:current train perplexity2.7950592041015625
INFO:root:current mean train loss 1304.318080357143
INFO:root:current train perplexity2.7988574504852295
INFO:root:current mean train loss 1304.6635533283666
INFO:root:current train perplexity2.800553321838379
INFO:root:current mean train loss 1303.6464797843216
INFO:root:current train perplexity2.8015122413635254
INFO:root:current mean train loss 1305.801013272696
INFO:root:current train perplexity2.8063466548919678
INFO:root:current mean train loss 1307.2373746641122
INFO:root:current train perplexity2.8069028854370117
INFO:root:current mean train loss 1308.9053040585275
INFO:root:current train perplexity2.8093674182891846
INFO:root:current mean train loss 1308.955494899072
INFO:root:current train perplexity2.808995485305786
INFO:root:current mean train loss 1310.779677936888
INFO:root:current train perplexity2.8089916706085205
INFO:root:current mean train loss 1310.813801465256
INFO:root:current train perplexity2.809617519378662
INFO:root:current mean train loss 1311.7521790263254
INFO:root:current train perplexity2.8109121322631836
INFO:root:current mean train loss 1311.5697760075868
INFO:root:current train perplexity2.811938762664795
INFO:root:current mean train loss 1312.3594810408774
INFO:root:current train perplexity2.8139734268188477
INFO:root:current mean train loss 1312.9413541101883
INFO:root:current train perplexity2.8151681423187256
INFO:root:current mean train loss 1313.3440253645447
INFO:root:current train perplexity2.8162739276885986
INFO:root:current mean train loss 1313.6698185426515
INFO:root:current train perplexity2.8165791034698486
INFO:root:current mean train loss 1314.6089089957725
INFO:root:current train perplexity2.818572521209717

100%|██████████| 1/1 [05:28<00:00, 328.24s/it][A100%|██████████| 1/1 [05:28<00:00, 328.25s/it]
INFO:root:final mean train loss: 1314.3529659392434
INFO:root:final train perplexity: 2.819551467895508
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.73s/it][A100%|██████████| 1/1 [00:21<00:00, 21.73s/it]
INFO:root:eval mean loss: 2169.3964744189107
INFO:root:eval perplexity: 5.7804436683654785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.16s/it][A100%|██████████| 1/1 [00:21<00:00, 21.16s/it]
INFO:root:eval mean loss: 2684.3759709351452
INFO:root:eval perplexity: 8.98320198059082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/59
 30%|██▉       | 59/200 [5:59:50<14:27:29, 369.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1285.705078125
INFO:root:current train perplexity2.7412009239196777
INFO:root:current mean train loss 1291.8995528875612
INFO:root:current train perplexity2.7750132083892822
INFO:root:current mean train loss 1292.912313026957
INFO:root:current train perplexity2.7824690341949463
INFO:root:current mean train loss 1292.5150049474855
INFO:root:current train perplexity2.7787907123565674
INFO:root:current mean train loss 1294.6379318616875
INFO:root:current train perplexity2.778959274291992
INFO:root:current mean train loss 1295.7871582517587
INFO:root:current train perplexity2.7811925411224365
INFO:root:current mean train loss 1296.1131794571481
INFO:root:current train perplexity2.7834038734436035
INFO:root:current mean train loss 1296.3232289719106
INFO:root:current train perplexity2.784243583679199
INFO:root:current mean train loss 1297.7203665945
INFO:root:current train perplexity2.7860889434814453
INFO:root:current mean train loss 1298.97103171338
INFO:root:current train perplexity2.789170980453491
INFO:root:current mean train loss 1298.9601133426506
INFO:root:current train perplexity2.789942979812622
INFO:root:current mean train loss 1299.7361265206725
INFO:root:current train perplexity2.7904272079467773
INFO:root:current mean train loss 1300.9068089642262
INFO:root:current train perplexity2.79019832611084
INFO:root:current mean train loss 1301.110331217448
INFO:root:current train perplexity2.7907519340515137
INFO:root:current mean train loss 1301.4822097756553
INFO:root:current train perplexity2.7916719913482666
INFO:root:current mean train loss 1302.3633626031178
INFO:root:current train perplexity2.7933971881866455
INFO:root:current mean train loss 1303.6638630880102
INFO:root:current train perplexity2.793748378753662
INFO:root:current mean train loss 1304.3129811808028
INFO:root:current train perplexity2.79498553276062
INFO:root:current mean train loss 1304.759211363459
INFO:root:current train perplexity2.796496629714966
INFO:root:current mean train loss 1305.1137331412042
INFO:root:current train perplexity2.7980687618255615

100%|██████████| 1/1 [05:26<00:00, 326.02s/it][A100%|██████████| 1/1 [05:26<00:00, 326.03s/it]
INFO:root:final mean train loss: 1305.0945777142824
INFO:root:final train perplexity: 2.799038887023926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.11s/it][A100%|██████████| 1/1 [00:22<00:00, 22.11s/it]
INFO:root:eval mean loss: 2173.765608550809
INFO:root:eval perplexity: 5.800905227661133
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.57s/it][A100%|██████████| 1/1 [00:21<00:00, 21.57s/it]
INFO:root:eval mean loss: 2691.9478707820813
INFO:root:eval perplexity: 9.0390043258667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/60
 30%|███       | 60/200 [6:06:02<14:22:59, 369.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1290.2315031352796
INFO:root:current train perplexity2.774104356765747
INFO:root:current mean train loss 1284.9412021156118
INFO:root:current train perplexity2.7621054649353027
INFO:root:current mean train loss 1286.0964227267052
INFO:root:current train perplexity2.752943754196167
INFO:root:current mean train loss 1287.5346840407033
INFO:root:current train perplexity2.7530364990234375
INFO:root:current mean train loss 1287.8687965556944
INFO:root:current train perplexity2.7573864459991455
INFO:root:current mean train loss 1288.514508962172
INFO:root:current train perplexity2.756770610809326
INFO:root:current mean train loss 1288.320735900583
INFO:root:current train perplexity2.757754325866699
INFO:root:current mean train loss 1290.5105421891299
INFO:root:current train perplexity2.760925531387329
INFO:root:current mean train loss 1290.1396968781003
INFO:root:current train perplexity2.7649853229522705
INFO:root:current mean train loss 1291.3189948313384
INFO:root:current train perplexity2.7648565769195557
INFO:root:current mean train loss 1293.095168483386
INFO:root:current train perplexity2.7674002647399902
INFO:root:current mean train loss 1293.1346400638474
INFO:root:current train perplexity2.769270181655884
INFO:root:current mean train loss 1293.8342683712315
INFO:root:current train perplexity2.770371913909912
INFO:root:current mean train loss 1293.7808488986093
INFO:root:current train perplexity2.7703404426574707
INFO:root:current mean train loss 1294.3828911273895
INFO:root:current train perplexity2.7734053134918213
INFO:root:current mean train loss 1294.933354431112
INFO:root:current train perplexity2.775465250015259
INFO:root:current mean train loss 1295.0694077923536
INFO:root:current train perplexity2.7767202854156494
INFO:root:current mean train loss 1295.4415425938046
INFO:root:current train perplexity2.7771520614624023
INFO:root:current mean train loss 1295.94543906658
INFO:root:current train perplexity2.7781550884246826
INFO:root:current mean train loss 1296.861229972581
INFO:root:current train perplexity2.779686212539673

100%|██████████| 1/1 [05:19<00:00, 319.36s/it][A100%|██████████| 1/1 [05:19<00:00, 319.36s/it]
INFO:root:final mean train loss: 1296.6282007599261
INFO:root:final train perplexity: 2.780411720275879
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.08s/it][A100%|██████████| 1/1 [00:22<00:00, 22.08s/it]
INFO:root:eval mean loss: 2183.958344588043
INFO:root:eval perplexity: 5.8489203453063965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.57s/it][A100%|██████████| 1/1 [00:21<00:00, 21.57s/it]
INFO:root:eval mean loss: 2700.4811193587934
INFO:root:eval perplexity: 9.102303504943848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/61
 30%|███       | 61/200 [6:12:07<14:13:20, 368.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1283.5166524251301
INFO:root:current train perplexity2.7223262786865234
INFO:root:current mean train loss 1274.4437255859375
INFO:root:current train perplexity2.731255292892456
INFO:root:current mean train loss 1275.403647729906
INFO:root:current train perplexity2.7364726066589355
INFO:root:current mean train loss 1278.397287641253
INFO:root:current train perplexity2.743058681488037
INFO:root:current mean train loss 1279.5889970971903
INFO:root:current train perplexity2.7451798915863037
INFO:root:current mean train loss 1281.269682699175
INFO:root:current train perplexity2.747225284576416
INFO:root:current mean train loss 1280.1854244208187
INFO:root:current train perplexity2.747817277908325
INFO:root:current mean train loss 1281.499808104142
INFO:root:current train perplexity2.7478232383728027
INFO:root:current mean train loss 1282.6626604436117
INFO:root:current train perplexity2.748924493789673
INFO:root:current mean train loss 1283.6266403850327
INFO:root:current train perplexity2.7514586448669434
INFO:root:current mean train loss 1284.3257181174968
INFO:root:current train perplexity2.7534048557281494
INFO:root:current mean train loss 1284.9951445888466
INFO:root:current train perplexity2.7546584606170654
INFO:root:current mean train loss 1285.1748050825495
INFO:root:current train perplexity2.755183458328247
INFO:root:current mean train loss 1285.4891478943969
INFO:root:current train perplexity2.7546894550323486
INFO:root:current mean train loss 1285.9317227419372
INFO:root:current train perplexity2.755068063735962
INFO:root:current mean train loss 1286.453773578008
INFO:root:current train perplexity2.7567203044891357
INFO:root:current mean train loss 1286.8342734339185
INFO:root:current train perplexity2.756075143814087
INFO:root:current mean train loss 1287.304658248128
INFO:root:current train perplexity2.757509469985962
INFO:root:current mean train loss 1287.6604022522638
INFO:root:current train perplexity2.759267568588257
INFO:root:current mean train loss 1287.7045892132214
INFO:root:current train perplexity2.7602407932281494

100%|██████████| 1/1 [05:13<00:00, 313.88s/it][A100%|██████████| 1/1 [05:13<00:00, 313.88s/it]
INFO:root:final mean train loss: 1287.828947604935
INFO:root:final train perplexity: 2.761183261871338
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.72s/it][A100%|██████████| 1/1 [00:21<00:00, 21.73s/it]
INFO:root:eval mean loss: 2188.008167889101
INFO:root:eval perplexity: 5.868109226226807
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.73s/it][A100%|██████████| 1/1 [00:20<00:00, 20.73s/it]
INFO:root:eval mean loss: 2707.5589482733544
INFO:root:eval perplexity: 9.15514850616455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/62
 31%|███       | 62/200 [6:18:05<14:00:09, 365.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1268.0046225493809
INFO:root:current train perplexity2.750023126602173
INFO:root:current mean train loss 1260.497834648182
INFO:root:current train perplexity2.7297732830047607
INFO:root:current mean train loss 1263.914048990242
INFO:root:current train perplexity2.727846622467041
INFO:root:current mean train loss 1264.9742168826353
INFO:root:current train perplexity2.723733901977539
INFO:root:current mean train loss 1268.546481303032
INFO:root:current train perplexity2.7217633724212646
INFO:root:current mean train loss 1268.6295581010538
INFO:root:current train perplexity2.7198843955993652
INFO:root:current mean train loss 1270.260126040869
INFO:root:current train perplexity2.7193572521209717
INFO:root:current mean train loss 1270.7924208115455
INFO:root:current train perplexity2.721497058868408
INFO:root:current mean train loss 1272.079298191585
INFO:root:current train perplexity2.7218103408813477
INFO:root:current mean train loss 1273.0137405315452
INFO:root:current train perplexity2.7250559329986572
INFO:root:current mean train loss 1272.8596372251156
INFO:root:current train perplexity2.727735757827759
INFO:root:current mean train loss 1273.717515215911
INFO:root:current train perplexity2.7288756370544434
INFO:root:current mean train loss 1274.7557407144727
INFO:root:current train perplexity2.7299458980560303
INFO:root:current mean train loss 1275.8553814151423
INFO:root:current train perplexity2.7328176498413086
INFO:root:current mean train loss 1276.6264924838965
INFO:root:current train perplexity2.7347075939178467
INFO:root:current mean train loss 1276.8329025632092
INFO:root:current train perplexity2.735405445098877
INFO:root:current mean train loss 1276.9583629462766
INFO:root:current train perplexity2.7374675273895264
INFO:root:current mean train loss 1277.433379065018
INFO:root:current train perplexity2.7386374473571777
INFO:root:current mean train loss 1278.0343011649184
INFO:root:current train perplexity2.7395894527435303
INFO:root:current mean train loss 1279.1504969443045
INFO:root:current train perplexity2.741492509841919

100%|██████████| 1/1 [05:19<00:00, 319.55s/it][A100%|██████████| 1/1 [05:19<00:00, 319.56s/it]
INFO:root:final mean train loss: 1278.9916407382675
INFO:root:final train perplexity: 2.7420060634613037
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.12s/it][A100%|██████████| 1/1 [00:22<00:00, 22.12s/it]
INFO:root:eval mean loss: 2196.5089951102614
INFO:root:eval perplexity: 5.908592224121094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.80s/it][A100%|██████████| 1/1 [00:20<00:00, 20.80s/it]
INFO:root:eval mean loss: 2717.479132036791
INFO:root:eval perplexity: 9.22972297668457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/63
 32%|███▏      | 63/200 [6:24:09<13:53:25, 365.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1247.5498116629465
INFO:root:current train perplexity2.688899517059326
INFO:root:current mean train loss 1256.0699613683364
INFO:root:current train perplexity2.692257881164551
INFO:root:current mean train loss 1257.9425460250288
INFO:root:current train perplexity2.691819190979004
INFO:root:current mean train loss 1259.2662432696368
INFO:root:current train perplexity2.6979219913482666
INFO:root:current mean train loss 1261.2668160945811
INFO:root:current train perplexity2.7016725540161133
INFO:root:current mean train loss 1262.318279065584
INFO:root:current train perplexity2.702914237976074
INFO:root:current mean train loss 1263.1841033480061
INFO:root:current train perplexity2.7062039375305176
INFO:root:current mean train loss 1264.8135658165077
INFO:root:current train perplexity2.707432508468628
INFO:root:current mean train loss 1265.1900676858836
INFO:root:current train perplexity2.709462881088257
INFO:root:current mean train loss 1266.262169780928
INFO:root:current train perplexity2.710275888442993
INFO:root:current mean train loss 1267.4832332432827
INFO:root:current train perplexity2.713020086288452
INFO:root:current mean train loss 1268.3027022402512
INFO:root:current train perplexity2.7155187129974365
INFO:root:current mean train loss 1268.171783303088
INFO:root:current train perplexity2.716038703918457
INFO:root:current mean train loss 1267.8456836115704
INFO:root:current train perplexity2.717541217803955
INFO:root:current mean train loss 1268.5961400038532
INFO:root:current train perplexity2.7205018997192383
INFO:root:current mean train loss 1269.4009568912968
INFO:root:current train perplexity2.7197842597961426
INFO:root:current mean train loss 1269.6363083890813
INFO:root:current train perplexity2.7218048572540283
INFO:root:current mean train loss 1270.4260822878045
INFO:root:current train perplexity2.7225208282470703
INFO:root:current mean train loss 1271.002173504345
INFO:root:current train perplexity2.7235662937164307
INFO:root:current mean train loss 1271.5180416823644
INFO:root:current train perplexity2.7252914905548096

100%|██████████| 1/1 [05:23<00:00, 323.50s/it][A100%|██████████| 1/1 [05:23<00:00, 323.50s/it]
INFO:root:final mean train loss: 1271.2356008432516
INFO:root:final train perplexity: 2.7252845764160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.77s/it][A100%|██████████| 1/1 [00:21<00:00, 21.77s/it]
INFO:root:eval mean loss: 2203.1976192826073
INFO:root:eval perplexity: 5.940640449523926
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.51s/it][A100%|██████████| 1/1 [00:21<00:00, 21.51s/it]
INFO:root:eval mean loss: 2725.6284876613754
INFO:root:eval perplexity: 9.291444778442383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/64
 32%|███▏      | 64/200 [6:30:18<13:49:45, 366.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1238.436422413793
INFO:root:current train perplexity2.676889657974243
INFO:root:current mean train loss 1243.6323829691678
INFO:root:current train perplexity2.679352283477783
INFO:root:current mean train loss 1246.8481726031685
INFO:root:current train perplexity2.682805299758911
INFO:root:current mean train loss 1247.0616350987161
INFO:root:current train perplexity2.6813738346099854
INFO:root:current mean train loss 1249.574236296041
INFO:root:current train perplexity2.68166184425354
INFO:root:current mean train loss 1251.2886216327727
INFO:root:current train perplexity2.68522047996521
INFO:root:current mean train loss 1253.3999245545056
INFO:root:current train perplexity2.689818859100342
INFO:root:current mean train loss 1254.6415264900334
INFO:root:current train perplexity2.6920433044433594
INFO:root:current mean train loss 1255.0678577444598
INFO:root:current train perplexity2.6931655406951904
INFO:root:current mean train loss 1256.255939518427
INFO:root:current train perplexity2.6938037872314453
INFO:root:current mean train loss 1257.1310857721942
INFO:root:current train perplexity2.6973111629486084
INFO:root:current mean train loss 1257.905140466216
INFO:root:current train perplexity2.6985809803009033
INFO:root:current mean train loss 1258.7380486809197
INFO:root:current train perplexity2.699863910675049
INFO:root:current mean train loss 1259.3706353042483
INFO:root:current train perplexity2.699465751647949
INFO:root:current mean train loss 1260.192595183649
INFO:root:current train perplexity2.7012698650360107
INFO:root:current mean train loss 1260.0873383472156
INFO:root:current train perplexity2.7023074626922607
INFO:root:current mean train loss 1260.3983199159704
INFO:root:current train perplexity2.704270601272583
INFO:root:current mean train loss 1261.6009856204314
INFO:root:current train perplexity2.705674171447754
INFO:root:current mean train loss 1262.6428444543465
INFO:root:current train perplexity2.707078456878662

100%|██████████| 1/1 [05:21<00:00, 321.46s/it][A100%|██████████| 1/1 [05:21<00:00, 321.46s/it]
INFO:root:final mean train loss: 1262.8082154736155
INFO:root:final train perplexity: 2.7072315216064453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.79s/it][A100%|██████████| 1/1 [00:21<00:00, 21.79s/it]
INFO:root:eval mean loss: 2210.2613330597574
INFO:root:eval perplexity: 5.974674224853516
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.54s/it][A100%|██████████| 1/1 [00:20<00:00, 20.54s/it]
INFO:root:eval mean loss: 2732.9752993752772
INFO:root:eval perplexity: 9.347436904907227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/65
 32%|███▎      | 65/200 [6:36:24<13:43:43, 366.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1260.3183288574219
INFO:root:current train perplexity2.7303714752197266
INFO:root:current mean train loss 1250.730236346905
INFO:root:current train perplexity2.6805286407470703
INFO:root:current mean train loss 1247.1674732881434
INFO:root:current train perplexity2.675126314163208
INFO:root:current mean train loss 1244.5031160053454
INFO:root:current train perplexity2.679346799850464
INFO:root:current mean train loss 1247.3869619841623
INFO:root:current train perplexity2.677567720413208
INFO:root:current mean train loss 1248.1895630851625
INFO:root:current train perplexity2.6731557846069336
INFO:root:current mean train loss 1248.0233578713523
INFO:root:current train perplexity2.6717748641967773
INFO:root:current mean train loss 1247.4950251145797
INFO:root:current train perplexity2.673661231994629
INFO:root:current mean train loss 1248.453113612844
INFO:root:current train perplexity2.6754207611083984
INFO:root:current mean train loss 1248.241909871059
INFO:root:current train perplexity2.6760833263397217
INFO:root:current mean train loss 1248.6544289151987
INFO:root:current train perplexity2.677354097366333
INFO:root:current mean train loss 1249.23372948688
INFO:root:current train perplexity2.6789302825927734
INFO:root:current mean train loss 1250.045074260116
INFO:root:current train perplexity2.6797003746032715
INFO:root:current mean train loss 1251.4610266188172
INFO:root:current train perplexity2.6804819107055664
INFO:root:current mean train loss 1252.0415009501314
INFO:root:current train perplexity2.6823296546936035
INFO:root:current mean train loss 1252.1112000485684
INFO:root:current train perplexity2.684215545654297
INFO:root:current mean train loss 1252.391990224025
INFO:root:current train perplexity2.6848981380462646
INFO:root:current mean train loss 1252.8572906350867
INFO:root:current train perplexity2.685411214828491
INFO:root:current mean train loss 1253.955046186426
INFO:root:current train perplexity2.6867642402648926
INFO:root:current mean train loss 1254.4087785352178
INFO:root:current train perplexity2.688774824142456

100%|██████████| 1/1 [05:22<00:00, 322.69s/it][A100%|██████████| 1/1 [05:22<00:00, 322.69s/it]
INFO:root:final mean train loss: 1254.4968479943288
INFO:root:final train perplexity: 2.6895439624786377
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.14s/it][A100%|██████████| 1/1 [00:20<00:00, 20.14s/it]
INFO:root:eval mean loss: 2215.54150520487
INFO:root:eval perplexity: 6.000241279602051
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.96s/it][A100%|██████████| 1/1 [00:20<00:00, 20.97s/it]
INFO:root:eval mean loss: 2739.613383408134
INFO:root:eval perplexity: 9.398320198059082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/66
 33%|███▎      | 66/200 [6:42:30<13:37:19, 365.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1214.1717064267114
INFO:root:current train perplexity2.63291597366333
INFO:root:current mean train loss 1228.4402137945506
INFO:root:current train perplexity2.6452460289001465
INFO:root:current mean train loss 1230.5989227985365
INFO:root:current train perplexity2.6471340656280518
INFO:root:current mean train loss 1234.1361821730188
INFO:root:current train perplexity2.650306463241577
INFO:root:current mean train loss 1233.531202157716
INFO:root:current train perplexity2.6516060829162598
INFO:root:current mean train loss 1236.469357071362
INFO:root:current train perplexity2.6549525260925293
INFO:root:current mean train loss 1238.3569758564186
INFO:root:current train perplexity2.6548922061920166
INFO:root:current mean train loss 1239.3494627551795
INFO:root:current train perplexity2.6563754081726074
INFO:root:current mean train loss 1240.3350781012105
INFO:root:current train perplexity2.6596148014068604
INFO:root:current mean train loss 1241.3416537306596
INFO:root:current train perplexity2.6609957218170166
INFO:root:current mean train loss 1241.3643229724612
INFO:root:current train perplexity2.662181854248047
INFO:root:current mean train loss 1241.4561808957892
INFO:root:current train perplexity2.6631786823272705
INFO:root:current mean train loss 1242.1479383214003
INFO:root:current train perplexity2.664440631866455
INFO:root:current mean train loss 1242.966189203255
INFO:root:current train perplexity2.666220188140869
INFO:root:current mean train loss 1243.5395961388326
INFO:root:current train perplexity2.6673405170440674
INFO:root:current mean train loss 1243.955663516755
INFO:root:current train perplexity2.668442726135254
INFO:root:current mean train loss 1244.753659548986
INFO:root:current train perplexity2.669161558151245
INFO:root:current mean train loss 1245.201148255352
INFO:root:current train perplexity2.6699204444885254
INFO:root:current mean train loss 1246.0428339430816
INFO:root:current train perplexity2.6711502075195312
INFO:root:current mean train loss 1246.4657199110977
INFO:root:current train perplexity2.671495199203491

100%|██████████| 1/1 [05:17<00:00, 317.40s/it][A100%|██████████| 1/1 [05:17<00:00, 317.40s/it]
INFO:root:final mean train loss: 1246.4524860853387
INFO:root:final train perplexity: 2.672534942626953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.80s/it][A100%|██████████| 1/1 [00:21<00:00, 21.80s/it]
INFO:root:eval mean loss: 2225.786161815021
INFO:root:eval perplexity: 6.0501627922058105
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.09s/it][A100%|██████████| 1/1 [00:21<00:00, 21.09s/it]
INFO:root:eval mean loss: 2751.260272086935
INFO:root:eval perplexity: 9.488269805908203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/67
 34%|███▎      | 67/200 [6:48:32<13:28:38, 364.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1213.7855031866777
INFO:root:current train perplexity2.6236612796783447
INFO:root:current mean train loss 1228.478636810745
INFO:root:current train perplexity2.6374213695526123
INFO:root:current mean train loss 1230.4232234153426
INFO:root:current train perplexity2.629809617996216
INFO:root:current mean train loss 1231.2085426827155
INFO:root:current train perplexity2.63232684135437
INFO:root:current mean train loss 1228.9342124625427
INFO:root:current train perplexity2.632836103439331
INFO:root:current mean train loss 1230.7048151519662
INFO:root:current train perplexity2.634040117263794
INFO:root:current mean train loss 1230.525617354342
INFO:root:current train perplexity2.6337058544158936
INFO:root:current mean train loss 1232.1587679896575
INFO:root:current train perplexity2.6350314617156982
INFO:root:current mean train loss 1232.2399095339536
INFO:root:current train perplexity2.636308193206787
INFO:root:current mean train loss 1232.6423721150802
INFO:root:current train perplexity2.6377336978912354
INFO:root:current mean train loss 1233.1611705625678
INFO:root:current train perplexity2.639352798461914
INFO:root:current mean train loss 1233.599293257943
INFO:root:current train perplexity2.6419131755828857
INFO:root:current mean train loss 1234.4050599623574
INFO:root:current train perplexity2.643927812576294
INFO:root:current mean train loss 1235.121004979885
INFO:root:current train perplexity2.6447386741638184
INFO:root:current mean train loss 1235.63304246417
INFO:root:current train perplexity2.6474976539611816
INFO:root:current mean train loss 1237.2245257989018
INFO:root:current train perplexity2.649806022644043
INFO:root:current mean train loss 1237.6823284069987
INFO:root:current train perplexity2.6523542404174805
INFO:root:current mean train loss 1238.0835997622087
INFO:root:current train perplexity2.6532583236694336
INFO:root:current mean train loss 1238.1854878322863
INFO:root:current train perplexity2.6546709537506104
INFO:root:current mean train loss 1239.0575154622395
INFO:root:current train perplexity2.65602445602417

100%|██████████| 1/1 [05:20<00:00, 320.49s/it][A100%|██████████| 1/1 [05:20<00:00, 320.49s/it]
INFO:root:final mean train loss: 1238.8497741052856
INFO:root:final train perplexity: 2.6565585136413574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.40s/it][A100%|██████████| 1/1 [00:21<00:00, 21.40s/it]
INFO:root:eval mean loss: 2231.4457267598905
INFO:root:eval perplexity: 6.07791805267334
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.44s/it][A100%|██████████| 1/1 [00:20<00:00, 20.44s/it]
INFO:root:eval mean loss: 2756.8805057173927
INFO:root:eval perplexity: 9.531982421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/68
 34%|███▍      | 68/200 [6:54:36<13:22:06, 364.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1215.8296320134943
INFO:root:current train perplexity2.635601282119751
INFO:root:current mean train loss 1212.981271263861
INFO:root:current train perplexity2.6146838665008545
INFO:root:current mean train loss 1217.11316540288
INFO:root:current train perplexity2.6107873916625977
INFO:root:current mean train loss 1219.3379741829885
INFO:root:current train perplexity2.6115078926086426
INFO:root:current mean train loss 1223.0382246415693
INFO:root:current train perplexity2.6160638332366943
INFO:root:current mean train loss 1224.0099763337557
INFO:root:current train perplexity2.6187937259674072
INFO:root:current mean train loss 1224.9495417238193
INFO:root:current train perplexity2.6218180656433105
INFO:root:current mean train loss 1225.8374324167012
INFO:root:current train perplexity2.6240718364715576
INFO:root:current mean train loss 1226.2056092379387
INFO:root:current train perplexity2.6269140243530273
INFO:root:current mean train loss 1226.6035376104385
INFO:root:current train perplexity2.628823757171631
INFO:root:current mean train loss 1227.9082986985338
INFO:root:current train perplexity2.630767583847046
INFO:root:current mean train loss 1228.9410600142046
INFO:root:current train perplexity2.6306533813476562
INFO:root:current mean train loss 1229.4106207007906
INFO:root:current train perplexity2.6318840980529785
INFO:root:current mean train loss 1229.9105856131803
INFO:root:current train perplexity2.6333842277526855
INFO:root:current mean train loss 1229.7792097058903
INFO:root:current train perplexity2.635300397872925
INFO:root:current mean train loss 1230.284402946644
INFO:root:current train perplexity2.6357922554016113
INFO:root:current mean train loss 1230.3360088244665
INFO:root:current train perplexity2.636240243911743
INFO:root:current mean train loss 1230.6621936765491
INFO:root:current train perplexity2.63755202293396
INFO:root:current mean train loss 1230.5960632159704
INFO:root:current train perplexity2.6386637687683105
INFO:root:current mean train loss 1231.1337063918638
INFO:root:current train perplexity2.6400835514068604

100%|██████████| 1/1 [05:22<00:00, 322.10s/it][A100%|██████████| 1/1 [05:22<00:00, 322.11s/it]
INFO:root:final mean train loss: 1231.1033110048695
INFO:root:final train perplexity: 2.640377998352051
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.52s/it][A100%|██████████| 1/1 [00:21<00:00, 21.52s/it]
INFO:root:eval mean loss: 2238.6475405862147
INFO:root:eval perplexity: 6.113422870635986
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.39s/it][A100%|██████████| 1/1 [00:20<00:00, 20.39s/it]
INFO:root:eval mean loss: 2767.3376196462214
INFO:root:eval perplexity: 9.613847732543945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/69
 34%|███▍      | 69/200 [7:00:42<13:16:48, 364.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1201.4701809353298
INFO:root:current train perplexity2.5848143100738525
INFO:root:current mean train loss 1212.86846214117
INFO:root:current train perplexity2.5927422046661377
INFO:root:current mean train loss 1213.5769949520336
INFO:root:current train perplexity2.5997917652130127
INFO:root:current mean train loss 1214.6669249175698
INFO:root:current train perplexity2.6064679622650146
INFO:root:current mean train loss 1214.5857836189916
INFO:root:current train perplexity2.6077308654785156
INFO:root:current mean train loss 1216.1087896173651
INFO:root:current train perplexity2.6074962615966797
INFO:root:current mean train loss 1217.3094556899298
INFO:root:current train perplexity2.6074280738830566
INFO:root:current mean train loss 1217.5886197263096
INFO:root:current train perplexity2.609548807144165
INFO:root:current mean train loss 1217.691232523787
INFO:root:current train perplexity2.609569549560547
INFO:root:current mean train loss 1217.936433140633
INFO:root:current train perplexity2.611311674118042
INFO:root:current mean train loss 1219.1443416538523
INFO:root:current train perplexity2.612914562225342
INFO:root:current mean train loss 1219.6625826578497
INFO:root:current train perplexity2.613901376724243
INFO:root:current mean train loss 1220.094157860714
INFO:root:current train perplexity2.6158394813537598
INFO:root:current mean train loss 1220.7956191527253
INFO:root:current train perplexity2.617974281311035
INFO:root:current mean train loss 1221.4593548152757
INFO:root:current train perplexity2.6193227767944336
INFO:root:current mean train loss 1222.0805271138975
INFO:root:current train perplexity2.620924949645996
INFO:root:current mean train loss 1222.5091249018765
INFO:root:current train perplexity2.621549129486084
INFO:root:current mean train loss 1222.9845051991483
INFO:root:current train perplexity2.6227643489837646
INFO:root:current mean train loss 1223.8282663720286
INFO:root:current train perplexity2.623758554458618
INFO:root:current mean train loss 1223.821954197139
INFO:root:current train perplexity2.6244935989379883

100%|██████████| 1/1 [05:21<00:00, 321.86s/it][A100%|██████████| 1/1 [05:21<00:00, 321.86s/it]
INFO:root:final mean train loss: 1223.4986754170704
INFO:root:final train perplexity: 2.6245899200439453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.19s/it][A100%|██████████| 1/1 [00:21<00:00, 21.19s/it]
INFO:root:eval mean loss: 2247.1804052041775
INFO:root:eval perplexity: 6.155754566192627
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.48s/it][A100%|██████████| 1/1 [00:20<00:00, 20.48s/it]
INFO:root:eval mean loss: 2776.287350485511
INFO:root:eval perplexity: 9.684476852416992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/70
 35%|███▌      | 70/200 [7:06:47<13:10:58, 365.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1211.6501533422577
INFO:root:current train perplexity2.5769801139831543
INFO:root:current mean train loss 1207.7873128255208
INFO:root:current train perplexity2.587009906768799
INFO:root:current mean train loss 1204.7366069014922
INFO:root:current train perplexity2.5889906883239746
INFO:root:current mean train loss 1206.0478820016267
INFO:root:current train perplexity2.591464042663574
INFO:root:current mean train loss 1206.0308343618194
INFO:root:current train perplexity2.596003532409668
INFO:root:current mean train loss 1207.3780252297988
INFO:root:current train perplexity2.5947344303131104
INFO:root:current mean train loss 1208.1417750121893
INFO:root:current train perplexity2.5970098972320557
INFO:root:current mean train loss 1210.0948634854742
INFO:root:current train perplexity2.5998923778533936
INFO:root:current mean train loss 1211.7400095953583
INFO:root:current train perplexity2.601954698562622
INFO:root:current mean train loss 1211.9372535142427
INFO:root:current train perplexity2.60321044921875
INFO:root:current mean train loss 1211.9408254912405
INFO:root:current train perplexity2.604867696762085
INFO:root:current mean train loss 1212.5032074000276
INFO:root:current train perplexity2.6056902408599854
INFO:root:current mean train loss 1212.91069548069
INFO:root:current train perplexity2.6066975593566895
INFO:root:current mean train loss 1213.5549556328463
INFO:root:current train perplexity2.6072306632995605
INFO:root:current mean train loss 1214.1170813340802
INFO:root:current train perplexity2.6076242923736572
INFO:root:current mean train loss 1214.8330982321083
INFO:root:current train perplexity2.60776424407959
INFO:root:current mean train loss 1215.5970044133223
INFO:root:current train perplexity2.6091692447662354
INFO:root:current mean train loss 1216.1853626436732
INFO:root:current train perplexity2.6097233295440674
INFO:root:current mean train loss 1216.470238883007
INFO:root:current train perplexity2.61073899269104

100%|██████████| 1/1 [05:22<00:00, 322.24s/it][A100%|██████████| 1/1 [05:22<00:00, 322.24s/it]
INFO:root:final mean train loss: 1216.8037505811112
INFO:root:final train perplexity: 2.6107685565948486
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.92s/it][A100%|██████████| 1/1 [00:22<00:00, 22.92s/it]
INFO:root:eval mean loss: 2256.443513477948
INFO:root:eval perplexity: 6.20204496383667
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.97s/it][A100%|██████████| 1/1 [00:20<00:00, 20.97s/it]
INFO:root:eval mean loss: 2786.1920140043217
INFO:root:eval perplexity: 9.763239860534668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilbert_bert_final/71
 36%|███▌      | 71/200 [7:12:55<13:06:43, 365.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1201.132303873698
INFO:root:current train perplexity2.5987958908081055
INFO:root:current mean train loss 1195.6986855560879
INFO:root:current train perplexity2.576969623565674
INFO:root:current mean train loss 1195.4492045282161
INFO:root:current train perplexity2.5752451419830322
INFO:root:current mean train loss 1194.8183342428767
INFO:root:current train perplexity2.5751073360443115
INFO:root:current mean train loss 1197.7496283770781
INFO:root:current train perplexity2.577146530151367
INFO:root:current mean train loss 1199.2990478998117
INFO:root:current train perplexity2.5796315670013428
INFO:root:current mean train loss 1200.631593005492
INFO:root:current train perplexity2.5804107189178467
INFO:root:current mean train loss 1202.0039707432388
INFO:root:current train perplexity2.5826756954193115
INFO:root:current mean train loss 1202.6256265569266
INFO:root:current train perplexity2.581684112548828
INFO:root:current mean train loss 1203.7215981725562
INFO:root:current train perplexity2.5821359157562256
INFO:root:current mean train loss 1204.3621117533082
INFO:root:current train perplexity2.584864616394043
INFO:root:current mean train loss 1205.537779216525
INFO:root:current train perplexity2.585824966430664
INFO:root:current mean train loss 1205.9661978599838
INFO:root:current train perplexity2.588318109512329
INFO:root:current mean train loss 1206.6166953865272
INFO:root:current train perplexity2.5884053707122803
INFO:root:current mean train loss 1206.8031972176998
INFO:root:current train perplexity2.589679479598999
INFO:root:current mean train loss 1207.0922128543118
INFO:root:current train perplexity2.5902206897735596
INFO:root:current mean train loss 1207.8887963015889
INFO:root:current train perplexity2.590911626815796
INFO:root:current mean train loss 1208.3133305217568
INFO:root:current train perplexity2.5917909145355225
INFO:root:current mean train loss 1208.9929965706758
INFO:root:current train perplexity2.5928759574890137
INFO:root:current mean train loss 1209.296489959749
INFO:root:current train perplexity2.594343900680542

100%|██████████| 1/1 [05:22<00:00, 322.40s/it][A100%|██████████| 1/1 [05:22<00:00, 322.40s/it]
INFO:root:final mean train loss: 1209.520584698945
INFO:root:final train perplexity: 2.595815420150757
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 26260762 ON ga010 CANCELLED AT 2022-10-25T09:46:13 ***
