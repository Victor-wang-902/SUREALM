INFO:root:Output: large_roberta_gpt2
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Using pad_token, but it is not set yet.
Some weights of RetrievalGenerationModelGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.9.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.1.crossattention.bias', 'h.7.ln_cross_attn.weight', 'h.3.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.masked_bias', 'h.5.ln_cross_attn.weight', 'h.10.crossattention.c_attn_v.bias', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.bias', 'h.11.ln_cross_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.7.crossattention.c_attn_v.weight', 'h.3.crossattention.c_proj.bias', 'h.5.crossattention.c_attn_v.weight', 'h.5.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.9.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.c_attn_v.weight', 'h.4.crossattention.c_proj.bias', 'h.9.crossattention.c_attn_v.bias', 'h.5.crossattention.bias', 'h.1.crossattention.q_attn.weight', 'h.0.crossattention.bias', 'h.4.ln_cross_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.0.crossattention.c_attn_v.bias', 'h.6.crossattention.c_attn_v.bias', 'h.10.crossattention.bias', 'h.0.crossattention.c_proj.bias', 'h.11.crossattention.bias', 'h.2.crossattention.c_proj.weight', 'h.3.crossattention.bias', 'h.7.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.5.crossattention.c_attn_v.bias', 'h.2.crossattention.c_attn_v.bias', 'h.7.crossattention.bias', 'h.1.crossattention.c_attn_v.bias', 'h.9.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.3.crossattention.c_attn_v.bias', 'h.8.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.6.crossattention.bias', 'h.8.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.11.crossattention.masked_bias', 'h.2.crossattention.c_attn_v.weight', 'h.4.crossattention.bias', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.c_attn_v.weight', 'h.8.crossattention.c_attn_v.weight', 'h.4.crossattention.masked_bias', 'h.3.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.weight', 'h.1.crossattention.c_attn_v.weight', 'h.4.crossattention.c_attn_v.weight', 'h.9.crossattention.bias', 'h.10.crossattention.c_attn.weight', 'h.7.crossattention.c_attn_v.bias', 'h.0.ln_cross_attn.weight', 'h.8.crossattention.masked_bias', 'h.10.ln_cross_attn.weight', 'h.8.crossattention.bias', 'h.8.crossattention.c_attn_v.bias', 'h.7.crossattention.c_proj.bias', 'h.11.crossattention.c_attn_v.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.10.crossattention.masked_bias', 'h.9.crossattention.q_attn.weight', 'h.4.crossattention.c_attn_v.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.masked_bias', 'h.3.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.7.crossattention.masked_bias', 'h.9.crossattention.masked_bias', 'h.1.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.masked_bias', 'h.8.crossattention.c_attn.weight', 'h.0.crossattention.c_attn_v.weight', 'h.6.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.3.crossattention.c_attn_v.weight', 'h.10.crossattention.c_attn_v.weight', 'h.11.crossattention.c_attn_v.weight', 'h.2.crossattention.c_attn.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4696.986762152777
INFO:root:current train perplexity61.59632110595703
INFO:root:current mean train loss 4420.113594093515
INFO:root:current train perplexity48.48618698120117
INFO:root:current mean train loss 4172.50769083795
INFO:root:current train perplexity38.886314392089844
INFO:root:current mean train loss 3968.382103329613
INFO:root:current train perplexity32.650909423828125
INFO:root:current mean train loss 3809.474498313032
INFO:root:current train perplexity28.417987823486328
INFO:root:current mean train loss 3685.346209339785
INFO:root:current train perplexity25.420480728149414
INFO:root:current mean train loss 3580.2021589156384
INFO:root:current train perplexity23.165170669555664
INFO:root:current mean train loss 3494.070666946965
INFO:root:current train perplexity21.41863250732422
INFO:root:current mean train loss 3418.797614754241
INFO:root:current train perplexity20.03346061706543
INFO:root:current mean train loss 3352.568118655765
INFO:root:current train perplexity18.895112991333008
INFO:root:current mean train loss 3293.6872778520246
INFO:root:current train perplexity17.9732723236084
INFO:root:current mean train loss 3243.937369275829
INFO:root:current train perplexity17.20562744140625
INFO:root:current mean train loss 3198.869151337868
INFO:root:current train perplexity16.536579132080078
INFO:root:current mean train loss 3155.263944809909
INFO:root:current train perplexity15.944060325622559
INFO:root:current mean train loss 3115.3404022277873
INFO:root:current train perplexity15.420888900756836
INFO:root:current mean train loss 3082.870074436171
INFO:root:current train perplexity14.980961799621582
INFO:root:current mean train loss 3052.2145430647392
INFO:root:current train perplexity14.583742141723633
INFO:root:current mean train loss 3024.485151730888
INFO:root:current train perplexity14.22350025177002
INFO:root:current mean train loss 2998.487594403613
INFO:root:current train perplexity13.899221420288086

100%|██████████| 1/1 [08:18<00:00, 498.72s/it][A100%|██████████| 1/1 [08:18<00:00, 498.72s/it]
INFO:root:final mean train loss: 2977.2234488144345
INFO:root:final train perplexity: 13.647741317749023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.15s/it][A100%|██████████| 1/1 [00:44<00:00, 44.15s/it]
INFO:root:eval mean loss: 2295.2574432589486
INFO:root:eval perplexity: 7.939288139343262
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.29s/it][A100%|██████████| 1/1 [00:41<00:00, 41.29s/it]
INFO:root:eval mean loss: 2501.953507227255
INFO:root:eval perplexity: 9.958596229553223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/1
  0%|          | 1/200 [09:45<32:23:29, 585.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2493.806106567383
INFO:root:current train perplexity8.989863395690918
INFO:root:current mean train loss 2451.6635910560344
INFO:root:current train perplexity8.605669975280762
INFO:root:current mean train loss 2447.2058297616463
INFO:root:current train perplexity8.582070350646973
INFO:root:current mean train loss 2445.1408374641514
INFO:root:current train perplexity8.579461097717285
INFO:root:current mean train loss 2442.6398101219766
INFO:root:current train perplexity8.518669128417969
INFO:root:current mean train loss 2439.052155487297
INFO:root:current train perplexity8.489826202392578
INFO:root:current mean train loss 2429.1664337554535
INFO:root:current train perplexity8.435749053955078
INFO:root:current mean train loss 2420.645433990649
INFO:root:current train perplexity8.394702911376953
INFO:root:current mean train loss 2413.342983320648
INFO:root:current train perplexity8.3419189453125
INFO:root:current mean train loss 2407.5091422135133
INFO:root:current train perplexity8.29427719116211
INFO:root:current mean train loss 2401.0384701706294
INFO:root:current train perplexity8.24816608428955
INFO:root:current mean train loss 2395.693372063312
INFO:root:current train perplexity8.205999374389648
INFO:root:current mean train loss 2391.0439709111265
INFO:root:current train perplexity8.175387382507324
INFO:root:current mean train loss 2386.0213815984753
INFO:root:current train perplexity8.137458801269531
INFO:root:current mean train loss 2381.950846526582
INFO:root:current train perplexity8.099855422973633
INFO:root:current mean train loss 2378.204977714921
INFO:root:current train perplexity8.065625190734863
INFO:root:current mean train loss 2374.4478483294497
INFO:root:current train perplexity8.035691261291504
INFO:root:current mean train loss 2369.634284243995
INFO:root:current train perplexity8.000896453857422
INFO:root:current mean train loss 2364.5642752626395
INFO:root:current train perplexity7.966607570648193
INFO:root:current mean train loss 2360.830318952652
INFO:root:current train perplexity7.940993785858154

100%|██████████| 1/1 [08:53<00:00, 533.80s/it][A100%|██████████| 1/1 [08:53<00:00, 533.80s/it]
INFO:root:final mean train loss: 2357.6240435670975
INFO:root:final train perplexity: 7.922089099884033
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.61s/it][A100%|██████████| 1/1 [00:41<00:00, 41.61s/it]
INFO:root:eval mean loss: 2109.998726053441
INFO:root:eval perplexity: 6.7167158126831055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.93s/it][A100%|██████████| 1/1 [00:38<00:00, 38.93s/it]
INFO:root:eval mean loss: 2356.1324268097574
INFO:root:eval perplexity: 8.710040092468262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/2
  1%|          | 2/200 [20:03<33:15:32, 604.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2264.667029178504
INFO:root:current train perplexity7.3571600914001465
INFO:root:current mean train loss 2261.037762864192
INFO:root:current train perplexity7.2833638191223145
INFO:root:current mean train loss 2250.51241973746
INFO:root:current train perplexity7.241065502166748
INFO:root:current mean train loss 2260.4468451605903
INFO:root:current train perplexity7.22584867477417
INFO:root:current mean train loss 2253.2236330944174
INFO:root:current train perplexity7.197609901428223
INFO:root:current mean train loss 2250.443045839807
INFO:root:current train perplexity7.167575359344482
INFO:root:current mean train loss 2249.885267791025
INFO:root:current train perplexity7.15653133392334
INFO:root:current mean train loss 2242.4751601069556
INFO:root:current train perplexity7.137025833129883
INFO:root:current mean train loss 2237.5865793583057
INFO:root:current train perplexity7.1260199546813965
INFO:root:current mean train loss 2233.2135335548132
INFO:root:current train perplexity7.1156907081604
INFO:root:current mean train loss 2232.476131058847
INFO:root:current train perplexity7.101138591766357
INFO:root:current mean train loss 2228.593068431777
INFO:root:current train perplexity7.079222202301025
INFO:root:current mean train loss 2227.0318672817507
INFO:root:current train perplexity7.069626808166504
INFO:root:current mean train loss 2224.130127960457
INFO:root:current train perplexity7.050730228424072
INFO:root:current mean train loss 2220.41639214334
INFO:root:current train perplexity7.027144908905029
INFO:root:current mean train loss 2217.4608315942496
INFO:root:current train perplexity7.0215044021606445
INFO:root:current mean train loss 2216.09810580962
INFO:root:current train perplexity7.006126403808594
INFO:root:current mean train loss 2214.491431762907
INFO:root:current train perplexity6.993393898010254
INFO:root:current mean train loss 2212.9923418027356
INFO:root:current train perplexity6.9802565574646
INFO:root:current mean train loss 2211.279001392852
INFO:root:current train perplexity6.9653520584106445

100%|██████████| 1/1 [08:21<00:00, 501.24s/it][A100%|██████████| 1/1 [08:21<00:00, 501.24s/it]
INFO:root:final mean train loss: 2209.986030490123
INFO:root:final train perplexity: 6.959099769592285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.64s/it][A100%|██████████| 1/1 [00:43<00:00, 43.64s/it]
INFO:root:eval mean loss: 2023.7254907053414
INFO:root:eval perplexity: 6.213498115539551
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.87s/it][A100%|██████████| 1/1 [00:41<00:00, 41.87s/it]
INFO:root:eval mean loss: 2293.37709121163
INFO:root:eval perplexity: 8.222103118896484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/3
  2%|▏         | 3/200 [29:52<32:41:57, 597.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2150.198430175781
INFO:root:current train perplexity6.504787445068359
INFO:root:current mean train loss 2156.569628092448
INFO:root:current train perplexity6.561443328857422
INFO:root:current mean train loss 2157.3125600585936
INFO:root:current train perplexity6.586038589477539
INFO:root:current mean train loss 2157.6110295758926
INFO:root:current train perplexity6.58447790145874
INFO:root:current mean train loss 2156.948525119358
INFO:root:current train perplexity6.579115390777588
INFO:root:current mean train loss 2152.674591175426
INFO:root:current train perplexity6.571944713592529
INFO:root:current mean train loss 2149.875814866286
INFO:root:current train perplexity6.561539173126221
INFO:root:current mean train loss 2146.894553222656
INFO:root:current train perplexity6.5471367835998535
INFO:root:current mean train loss 2148.564527803309
INFO:root:current train perplexity6.5461745262146
INFO:root:current mean train loss 2145.216287263569
INFO:root:current train perplexity6.530029296875
INFO:root:current mean train loss 2142.038233003162
INFO:root:current train perplexity6.5218024253845215
INFO:root:current mean train loss 2139.8318104619566
INFO:root:current train perplexity6.517301082611084
INFO:root:current mean train loss 2137.6467830078127
INFO:root:current train perplexity6.508009433746338
INFO:root:current mean train loss 2134.91254864728
INFO:root:current train perplexity6.498990058898926
INFO:root:current mean train loss 2132.183056893184
INFO:root:current train perplexity6.490866184234619
INFO:root:current mean train loss 2130.9479029895415
INFO:root:current train perplexity6.490843772888184
INFO:root:current mean train loss 2129.3414105409565
INFO:root:current train perplexity6.479285717010498
INFO:root:current mean train loss 2129.0155144391742
INFO:root:current train perplexity6.471644878387451
INFO:root:current mean train loss 2126.6740294420397
INFO:root:current train perplexity6.465113162994385
INFO:root:current mean train loss 2126.046644193209
INFO:root:current train perplexity6.458748817443848

100%|██████████| 1/1 [08:45<00:00, 525.58s/it][A100%|██████████| 1/1 [08:45<00:00, 525.58s/it]
INFO:root:final mean train loss: 2124.4006950928597
INFO:root:final train perplexity: 6.455410480499268
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.63s/it][A100%|██████████| 1/1 [00:45<00:00, 45.65s/it]
INFO:root:eval mean loss: 1972.5939573463818
INFO:root:eval perplexity: 5.933238506317139
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.53s/it][A100%|██████████| 1/1 [00:44<00:00, 44.54s/it]
INFO:root:eval mean loss: 2258.1326553669383
INFO:root:eval perplexity: 7.960154056549072
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/4
  2%|▏         | 4/200 [40:10<32:58:27, 605.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2069.586628017141
INFO:root:current train perplexity6.215688228607178
INFO:root:current mean train loss 2084.5923122017684
INFO:root:current train perplexity6.189795017242432
INFO:root:current mean train loss 2090.234750354781
INFO:root:current train perplexity6.194448947906494
INFO:root:current mean train loss 2083.6084915701636
INFO:root:current train perplexity6.195135116577148
INFO:root:current mean train loss 2086.177354310258
INFO:root:current train perplexity6.193876266479492
INFO:root:current mean train loss 2079.325437946084
INFO:root:current train perplexity6.180991172790527
INFO:root:current mean train loss 2079.034125332353
INFO:root:current train perplexity6.169260501861572
INFO:root:current mean train loss 2077.951603975259
INFO:root:current train perplexity6.168912887573242
INFO:root:current mean train loss 2073.4180920874783
INFO:root:current train perplexity6.155366897583008
INFO:root:current mean train loss 2073.262997774205
INFO:root:current train perplexity6.157052993774414
INFO:root:current mean train loss 2072.892891595156
INFO:root:current train perplexity6.155258655548096
INFO:root:current mean train loss 2071.803387404101
INFO:root:current train perplexity6.156708717346191
INFO:root:current mean train loss 2069.725060370369
INFO:root:current train perplexity6.151139736175537
INFO:root:current mean train loss 2068.958602894111
INFO:root:current train perplexity6.147674560546875
INFO:root:current mean train loss 2069.2450499531355
INFO:root:current train perplexity6.143744945526123
INFO:root:current mean train loss 2069.6213212977873
INFO:root:current train perplexity6.1446852684021
INFO:root:current mean train loss 2068.5112287845163
INFO:root:current train perplexity6.139993190765381
INFO:root:current mean train loss 2068.5630569501186
INFO:root:current train perplexity6.139472961425781
INFO:root:current mean train loss 2068.4153490439417
INFO:root:current train perplexity6.137264251708984
INFO:root:current mean train loss 2066.281028945372
INFO:root:current train perplexity6.129293441772461

100%|██████████| 1/1 [08:35<00:00, 515.72s/it][A100%|██████████| 1/1 [08:35<00:00, 515.72s/it]
INFO:root:final mean train loss: 2065.2682303362762
INFO:root:final train perplexity: 6.12885856628418
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.45s/it][A100%|██████████| 1/1 [00:47<00:00, 47.45s/it]
INFO:root:eval mean loss: 1939.4343590702572
INFO:root:eval perplexity: 5.758278846740723
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.78s/it][A100%|██████████| 1/1 [00:43<00:00, 43.80s/it]
INFO:root:eval mean loss: 2241.9059283750275
INFO:root:eval perplexity: 7.842374801635742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/5
  2%|▎         | 5/200 [50:20<32:52:59, 607.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2017.2024100167412
INFO:root:current train perplexity5.987755298614502
INFO:root:current mean train loss 2027.9643833326256
INFO:root:current train perplexity6.006793975830078
INFO:root:current mean train loss 2037.555655465999
INFO:root:current train perplexity6.006314754486084
INFO:root:current mean train loss 2033.8165629704793
INFO:root:current train perplexity5.991658687591553
INFO:root:current mean train loss 2034.0716229903796
INFO:root:current train perplexity5.982536315917969
INFO:root:current mean train loss 2032.8206906253345
INFO:root:current train perplexity5.978126049041748
INFO:root:current mean train loss 2034.3126836408649
INFO:root:current train perplexity5.968696117401123
INFO:root:current mean train loss 2031.657941078653
INFO:root:current train perplexity5.959281921386719
INFO:root:current mean train loss 2028.8766631389635
INFO:root:current train perplexity5.940124988555908
INFO:root:current mean train loss 2027.8911612906106
INFO:root:current train perplexity5.937015533447266
INFO:root:current mean train loss 2028.5198418311088
INFO:root:current train perplexity5.937076568603516
INFO:root:current mean train loss 2026.6169371733795
INFO:root:current train perplexity5.927788734436035
INFO:root:current mean train loss 2025.991062437634
INFO:root:current train perplexity5.920930862426758
INFO:root:current mean train loss 2025.4787343637104
INFO:root:current train perplexity5.920445919036865
INFO:root:current mean train loss 2025.286741930198
INFO:root:current train perplexity5.919334411621094
INFO:root:current mean train loss 2023.3858255713876
INFO:root:current train perplexity5.909653663635254
INFO:root:current mean train loss 2022.347003565265
INFO:root:current train perplexity5.903793811798096
INFO:root:current mean train loss 2021.057002867284
INFO:root:current train perplexity5.8975629806518555
INFO:root:current mean train loss 2020.8810694992162
INFO:root:current train perplexity5.894049644470215

100%|██████████| 1/1 [08:59<00:00, 539.30s/it][A100%|██████████| 1/1 [08:59<00:00, 539.30s/it]
INFO:root:final mean train loss: 2020.38585240782
INFO:root:final train perplexity: 5.892075538635254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.64s/it][A100%|██████████| 1/1 [00:47<00:00, 47.66s/it]
INFO:root:eval mean loss: 1912.8670892377272
INFO:root:eval perplexity: 5.621830940246582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.72s/it][A100%|██████████| 1/1 [00:43<00:00, 43.72s/it]
INFO:root:eval mean loss: 2223.7147177838265
INFO:root:eval perplexity: 7.712404251098633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/6
  3%|▎         | 6/200 [1:00:53<33:11:45, 616.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1868.9288330078125
INFO:root:current train perplexity5.0792083740234375
INFO:root:current mean train loss 1974.508382967203
INFO:root:current train perplexity5.768810749053955
INFO:root:current mean train loss 1976.5267370423273
INFO:root:current train perplexity5.717474937438965
INFO:root:current mean train loss 1990.2802681653602
INFO:root:current train perplexity5.746707439422607
INFO:root:current mean train loss 1988.6533723674213
INFO:root:current train perplexity5.745153427124023
INFO:root:current mean train loss 1990.2520366980882
INFO:root:current train perplexity5.741603851318359
INFO:root:current mean train loss 1989.6672639513572
INFO:root:current train perplexity5.742861270904541
INFO:root:current mean train loss 1988.7107107731142
INFO:root:current train perplexity5.732466697692871
INFO:root:current mean train loss 1989.7870282995864
INFO:root:current train perplexity5.731470584869385
INFO:root:current mean train loss 1991.3065620447767
INFO:root:current train perplexity5.731825828552246
INFO:root:current mean train loss 1990.8175068339863
INFO:root:current train perplexity5.725109577178955
INFO:root:current mean train loss 1988.7804505004328
INFO:root:current train perplexity5.725090503692627
INFO:root:current mean train loss 1988.747370152152
INFO:root:current train perplexity5.718347072601318
INFO:root:current mean train loss 1988.0591995978155
INFO:root:current train perplexity5.711065769195557
INFO:root:current mean train loss 1985.140568539213
INFO:root:current train perplexity5.708016395568848
INFO:root:current mean train loss 1984.5819831863394
INFO:root:current train perplexity5.70814847946167
INFO:root:current mean train loss 1985.7532353588822
INFO:root:current train perplexity5.709504127502441
INFO:root:current mean train loss 1984.5586765654855
INFO:root:current train perplexity5.706599235534668
INFO:root:current mean train loss 1984.6161521051672
INFO:root:current train perplexity5.703497409820557
INFO:root:current mean train loss 1983.749705965302
INFO:root:current train perplexity5.699807167053223

100%|██████████| 1/1 [08:48<00:00, 528.54s/it][A100%|██████████| 1/1 [08:48<00:00, 528.54s/it]
INFO:root:final mean train loss: 1982.6510205213553
INFO:root:final train perplexity: 5.7000932693481445
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.27s/it][A100%|██████████| 1/1 [00:45<00:00, 45.29s/it]
INFO:root:eval mean loss: 1892.165081484098
INFO:root:eval perplexity: 5.5177531242370605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.96s/it][A100%|██████████| 1/1 [00:44<00:00, 44.97s/it]
INFO:root:eval mean loss: 2214.8976821358324
INFO:root:eval perplexity: 7.650187969207764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/7
  4%|▎         | 7/200 [1:11:15<33:07:00, 617.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.4708251953125
INFO:root:current train perplexity5.573159694671631
INFO:root:current mean train loss 1954.808980650821
INFO:root:current train perplexity5.590694904327393
INFO:root:current mean train loss 1948.8389814184347
INFO:root:current train perplexity5.545971393585205
INFO:root:current mean train loss 1958.5819951663227
INFO:root:current train perplexity5.555695533752441
INFO:root:current mean train loss 1958.6071070620887
INFO:root:current train perplexity5.559407711029053
INFO:root:current mean train loss 1958.8928729318743
INFO:root:current train perplexity5.567904472351074
INFO:root:current mean train loss 1960.725036423569
INFO:root:current train perplexity5.578074932098389
INFO:root:current mean train loss 1957.2244966554774
INFO:root:current train perplexity5.56935453414917
INFO:root:current mean train loss 1957.889783490841
INFO:root:current train perplexity5.569952487945557
INFO:root:current mean train loss 1955.5787915996477
INFO:root:current train perplexity5.558799743652344
INFO:root:current mean train loss 1954.57109324637
INFO:root:current train perplexity5.556405544281006
INFO:root:current mean train loss 1952.6785350383288
INFO:root:current train perplexity5.556072235107422
INFO:root:current mean train loss 1951.8621283971227
INFO:root:current train perplexity5.5526251792907715
INFO:root:current mean train loss 1951.7621434954121
INFO:root:current train perplexity5.5501708984375
INFO:root:current mean train loss 1952.0447459146906
INFO:root:current train perplexity5.551413536071777
INFO:root:current mean train loss 1951.935157665308
INFO:root:current train perplexity5.549986839294434
INFO:root:current mean train loss 1951.1848825046836
INFO:root:current train perplexity5.545153617858887
INFO:root:current mean train loss 1950.8606244798866
INFO:root:current train perplexity5.545772552490234
INFO:root:current mean train loss 1951.4305346733404
INFO:root:current train perplexity5.547694683074951
INFO:root:current mean train loss 1951.5771492648796
INFO:root:current train perplexity5.5469970703125

100%|██████████| 1/1 [08:41<00:00, 521.81s/it][A100%|██████████| 1/1 [08:41<00:00, 521.82s/it]
INFO:root:final mean train loss: 1951.2387153598556
INFO:root:final train perplexity: 5.545057773590088
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.71s/it][A100%|██████████| 1/1 [00:45<00:00, 45.72s/it]
INFO:root:eval mean loss: 1876.4547755464594
INFO:root:eval perplexity: 5.440057754516602
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.84s/it][A100%|██████████| 1/1 [00:44<00:00, 44.86s/it]
INFO:root:eval mean loss: 2210.3726230918937
INFO:root:eval perplexity: 7.6184515953063965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/8
  4%|▍         | 8/200 [1:21:29<32:53:37, 616.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1945.355259486607
INFO:root:current train perplexity5.472885608673096
INFO:root:current mean train loss 1948.4866165726273
INFO:root:current train perplexity5.45804500579834
INFO:root:current mean train loss 1934.616465986536
INFO:root:current train perplexity5.438507080078125
INFO:root:current mean train loss 1931.1086185284514
INFO:root:current train perplexity5.418373107910156
INFO:root:current mean train loss 1936.1601896439477
INFO:root:current train perplexity5.434425354003906
INFO:root:current mean train loss 1936.438155072649
INFO:root:current train perplexity5.439844608306885
INFO:root:current mean train loss 1932.7852637103224
INFO:root:current train perplexity5.431605339050293
INFO:root:current mean train loss 1928.7011386585884
INFO:root:current train perplexity5.42236328125
INFO:root:current mean train loss 1929.6951954002152
INFO:root:current train perplexity5.42645788192749
INFO:root:current mean train loss 1929.3101606889204
INFO:root:current train perplexity5.42254114151001
INFO:root:current mean train loss 1927.9678414902248
INFO:root:current train perplexity5.4190592765808105
INFO:root:current mean train loss 1925.1985868882502
INFO:root:current train perplexity5.416337013244629
INFO:root:current mean train loss 1927.8611376557756
INFO:root:current train perplexity5.426380157470703
INFO:root:current mean train loss 1926.8861735026042
INFO:root:current train perplexity5.4199442863464355
INFO:root:current mean train loss 1927.1357636242378
INFO:root:current train perplexity5.423412799835205
INFO:root:current mean train loss 1926.2170410951496
INFO:root:current train perplexity5.420622825622559
INFO:root:current mean train loss 1925.8526776328365
INFO:root:current train perplexity5.422722816467285
INFO:root:current mean train loss 1926.2603937066597
INFO:root:current train perplexity5.4226603507995605
INFO:root:current mean train loss 1926.7827109853968
INFO:root:current train perplexity5.423171043395996
INFO:root:current mean train loss 1926.7215335816377
INFO:root:current train perplexity5.422539710998535

100%|██████████| 1/1 [08:38<00:00, 518.62s/it][A100%|██████████| 1/1 [08:38<00:00, 518.62s/it]
INFO:root:final mean train loss: 1925.3650548399187
INFO:root:final train perplexity: 5.420530319213867
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.79s/it][A100%|██████████| 1/1 [00:47<00:00, 47.80s/it]
INFO:root:eval mean loss: 1862.2613503746952
INFO:root:eval perplexity: 5.37080717086792
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.12s/it][A100%|██████████| 1/1 [00:45<00:00, 45.13s/it]
INFO:root:eval mean loss: 2199.527697840481
INFO:root:eval perplexity: 7.542929172515869
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/9
  4%|▍         | 9/200 [1:31:43<32:40:30, 615.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1893.5090918907752
INFO:root:current train perplexity5.2942633628845215
INFO:root:current mean train loss 1909.7991011770148
INFO:root:current train perplexity5.312781810760498
INFO:root:current mean train loss 1915.4986281622023
INFO:root:current train perplexity5.324207305908203
INFO:root:current mean train loss 1914.3687681718307
INFO:root:current train perplexity5.328574180603027
INFO:root:current mean train loss 1915.9103128888967
INFO:root:current train perplexity5.344021797180176
INFO:root:current mean train loss 1909.8267278256624
INFO:root:current train perplexity5.329601287841797
INFO:root:current mean train loss 1910.0115923735261
INFO:root:current train perplexity5.3318190574646
INFO:root:current mean train loss 1910.2061800043634
INFO:root:current train perplexity5.330748081207275
INFO:root:current mean train loss 1907.9843993567524
INFO:root:current train perplexity5.330856800079346
INFO:root:current mean train loss 1906.6959029766692
INFO:root:current train perplexity5.326426982879639
INFO:root:current mean train loss 1905.1427195733945
INFO:root:current train perplexity5.325390815734863
INFO:root:current mean train loss 1903.7894136640762
INFO:root:current train perplexity5.323134422302246
INFO:root:current mean train loss 1904.2901525527905
INFO:root:current train perplexity5.320461273193359
INFO:root:current mean train loss 1902.8388818142682
INFO:root:current train perplexity5.315544605255127
INFO:root:current mean train loss 1903.775601725933
INFO:root:current train perplexity5.31720495223999
INFO:root:current mean train loss 1904.7792627393585
INFO:root:current train perplexity5.3197021484375
INFO:root:current mean train loss 1905.8337993483278
INFO:root:current train perplexity5.317231178283691
INFO:root:current mean train loss 1904.7044647077448
INFO:root:current train perplexity5.312438011169434
INFO:root:current mean train loss 1902.8167343633993
INFO:root:current train perplexity5.310945987701416
INFO:root:current mean train loss 1902.5529084752818
INFO:root:current train perplexity5.310363292694092

100%|██████████| 1/1 [08:41<00:00, 521.33s/it][A100%|██████████| 1/1 [08:41<00:00, 521.33s/it]
INFO:root:final mean train loss: 1901.6898605492881
INFO:root:final train perplexity: 5.3090362548828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.71s/it][A100%|██████████| 1/1 [00:45<00:00, 45.72s/it]
INFO:root:eval mean loss: 1851.4707741162456
INFO:root:eval perplexity: 5.3187479972839355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.33s/it][A100%|██████████| 1/1 [00:43<00:00, 43.34s/it]
INFO:root:eval mean loss: 2195.357275996648
INFO:root:eval perplexity: 7.5140862464904785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/10
  5%|▌         | 10/200 [1:41:56<32:27:10, 614.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1846.526312344316
INFO:root:current train perplexity5.129759311676025
INFO:root:current mean train loss 1853.948318030002
INFO:root:current train perplexity5.150665760040283
INFO:root:current mean train loss 1868.6736193802276
INFO:root:current train perplexity5.184399127960205
INFO:root:current mean train loss 1877.7950237656673
INFO:root:current train perplexity5.198244094848633
INFO:root:current mean train loss 1876.35696873126
INFO:root:current train perplexity5.197001934051514
INFO:root:current mean train loss 1878.9922572238165
INFO:root:current train perplexity5.19713020324707
INFO:root:current mean train loss 1880.548597121987
INFO:root:current train perplexity5.1989593505859375
INFO:root:current mean train loss 1880.624988888268
INFO:root:current train perplexity5.20012092590332
INFO:root:current mean train loss 1879.4383084173169
INFO:root:current train perplexity5.202380180358887
INFO:root:current mean train loss 1879.081608853965
INFO:root:current train perplexity5.206491947174072
INFO:root:current mean train loss 1878.0826249296583
INFO:root:current train perplexity5.207261562347412
INFO:root:current mean train loss 1880.4544104452925
INFO:root:current train perplexity5.207370281219482
INFO:root:current mean train loss 1881.9690634003769
INFO:root:current train perplexity5.211609363555908
INFO:root:current mean train loss 1882.5234570276832
INFO:root:current train perplexity5.213298320770264
INFO:root:current mean train loss 1883.6159595673876
INFO:root:current train perplexity5.2162766456604
INFO:root:current mean train loss 1882.8449346033003
INFO:root:current train perplexity5.2147536277771
INFO:root:current mean train loss 1882.5569582711157
INFO:root:current train perplexity5.21357536315918
INFO:root:current mean train loss 1882.756326264618
INFO:root:current train perplexity5.215349197387695
INFO:root:current mean train loss 1882.577531499235
INFO:root:current train perplexity5.216944217681885
INFO:root:current mean train loss 1881.5395394979605
INFO:root:current train perplexity5.215055465698242

100%|██████████| 1/1 [08:22<00:00, 502.88s/it][A100%|██████████| 1/1 [08:22<00:00, 502.88s/it]
INFO:root:final mean train loss: 1881.0649089034134
INFO:root:final train perplexity: 5.213778018951416
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.79s/it][A100%|██████████| 1/1 [00:39<00:00, 39.79s/it]
INFO:root:eval mean loss: 1843.3851677644338
INFO:root:eval perplexity: 5.280069351196289
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.98s/it][A100%|██████████| 1/1 [00:36<00:00, 36.98s/it]
INFO:root:eval mean loss: 2193.7466885181184
INFO:root:eval perplexity: 7.502976417541504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/11
  6%|▌         | 11/200 [1:51:38<31:45:08, 604.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.578631733739
INFO:root:current train perplexity5.124339580535889
INFO:root:current mean train loss 1861.5970590242775
INFO:root:current train perplexity5.131994247436523
INFO:root:current mean train loss 1867.033360194493
INFO:root:current train perplexity5.1351423263549805
INFO:root:current mean train loss 1867.1556159301126
INFO:root:current train perplexity5.127529144287109
INFO:root:current mean train loss 1871.5356106228298
INFO:root:current train perplexity5.137698650360107
INFO:root:current mean train loss 1872.9816534153024
INFO:root:current train perplexity5.138895511627197
INFO:root:current mean train loss 1870.7328587301272
INFO:root:current train perplexity5.13653039932251
INFO:root:current mean train loss 1868.1167374239624
INFO:root:current train perplexity5.131467819213867
INFO:root:current mean train loss 1866.4606981815657
INFO:root:current train perplexity5.137803554534912
INFO:root:current mean train loss 1865.9751710717626
INFO:root:current train perplexity5.136238098144531
INFO:root:current mean train loss 1865.723925061867
INFO:root:current train perplexity5.136870384216309
INFO:root:current mean train loss 1866.1047013332632
INFO:root:current train perplexity5.140190601348877
INFO:root:current mean train loss 1865.52224375486
INFO:root:current train perplexity5.14044713973999
INFO:root:current mean train loss 1865.1276239832757
INFO:root:current train perplexity5.136532783508301
INFO:root:current mean train loss 1864.5522339360068
INFO:root:current train perplexity5.132814407348633
INFO:root:current mean train loss 1865.4104725860557
INFO:root:current train perplexity5.136026382446289
INFO:root:current mean train loss 1864.8622432903321
INFO:root:current train perplexity5.132692337036133
INFO:root:current mean train loss 1865.6483457763945
INFO:root:current train perplexity5.1346564292907715
INFO:root:current mean train loss 1864.1946740580263
INFO:root:current train perplexity5.131673812866211

100%|██████████| 1/1 [08:22<00:00, 502.64s/it][A100%|██████████| 1/1 [08:22<00:00, 502.64s/it]
INFO:root:final mean train loss: 1862.63279978819
INFO:root:final train perplexity: 5.130093097686768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.99s/it][A100%|██████████| 1/1 [00:43<00:00, 44.00s/it]
INFO:root:eval mean loss: 1834.7793068310893
INFO:root:eval perplexity: 5.239212989807129
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.14s/it][A100%|██████████| 1/1 [00:41<00:00, 41.14s/it]
INFO:root:eval mean loss: 2191.1009452224625
INFO:root:eval perplexity: 7.484763145446777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/12
  6%|▌         | 12/200 [2:01:28<31:21:01, 600.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1852.5820719401042
INFO:root:current train perplexity4.706674575805664
INFO:root:current mean train loss 1859.5358791906856
INFO:root:current train perplexity5.06431770324707
INFO:root:current mean train loss 1851.6928518511393
INFO:root:current train perplexity5.0424652099609375
INFO:root:current mean train loss 1845.2751702538417
INFO:root:current train perplexity5.0439958572387695
INFO:root:current mean train loss 1846.1596055705259
INFO:root:current train perplexity5.04500675201416
INFO:root:current mean train loss 1849.6308698104342
INFO:root:current train perplexity5.056084632873535
INFO:root:current mean train loss 1845.8945118159204
INFO:root:current train perplexity5.046926498413086
INFO:root:current mean train loss 1846.3362153271694
INFO:root:current train perplexity5.047331809997559
INFO:root:current mean train loss 1844.1608707337718
INFO:root:current train perplexity5.0396504402160645
INFO:root:current mean train loss 1844.7696796810112
INFO:root:current train perplexity5.042323589324951
INFO:root:current mean train loss 1846.4154011354608
INFO:root:current train perplexity5.038977146148682
INFO:root:current mean train loss 1843.682392082318
INFO:root:current train perplexity5.036900520324707
INFO:root:current mean train loss 1843.3430783596023
INFO:root:current train perplexity5.038467884063721
INFO:root:current mean train loss 1844.471369124886
INFO:root:current train perplexity5.04521369934082
INFO:root:current mean train loss 1844.5251090715208
INFO:root:current train perplexity5.045523166656494
INFO:root:current mean train loss 1844.8014229516227
INFO:root:current train perplexity5.049161911010742
INFO:root:current mean train loss 1845.0144148057354
INFO:root:current train perplexity5.050468921661377
INFO:root:current mean train loss 1844.664721235274
INFO:root:current train perplexity5.047235488891602
INFO:root:current mean train loss 1845.307105074442
INFO:root:current train perplexity5.0517449378967285
INFO:root:current mean train loss 1844.8951008686943
INFO:root:current train perplexity5.051391124725342

100%|██████████| 1/1 [08:08<00:00, 488.94s/it][A100%|██████████| 1/1 [08:08<00:00, 488.94s/it]
INFO:root:final mean train loss: 1844.6603047288672
INFO:root:final train perplexity: 5.0497894287109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.52s/it][A100%|██████████| 1/1 [00:42<00:00, 42.53s/it]
INFO:root:eval mean loss: 1826.9094974166112
INFO:root:eval perplexity: 5.202126502990723
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.16s/it][A100%|██████████| 1/1 [00:40<00:00, 40.16s/it]
INFO:root:eval mean loss: 2186.092899836547
INFO:root:eval perplexity: 7.450406551361084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/13
  6%|▋         | 13/200 [2:11:02<30:46:12, 592.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1827.482684326172
INFO:root:current train perplexity4.9462890625
INFO:root:current mean train loss 1836.0951517740884
INFO:root:current train perplexity4.935671806335449
INFO:root:current mean train loss 1831.3323158957742
INFO:root:current train perplexity4.951277256011963
INFO:root:current mean train loss 1832.241538619995
INFO:root:current train perplexity4.953861236572266
INFO:root:current mean train loss 1833.0390906924292
INFO:root:current train perplexity4.961956977844238
INFO:root:current mean train loss 1829.843176504282
INFO:root:current train perplexity4.970169544219971
INFO:root:current mean train loss 1830.2224696005544
INFO:root:current train perplexity4.971549034118652
INFO:root:current mean train loss 1828.6432678222657
INFO:root:current train perplexity4.965737342834473
INFO:root:current mean train loss 1830.614442406631
INFO:root:current train perplexity4.97013521194458
INFO:root:current mean train loss 1831.2275598940641
INFO:root:current train perplexity4.97414493560791
INFO:root:current mean train loss 1829.4585564108456
INFO:root:current train perplexity4.975837230682373
INFO:root:current mean train loss 1828.9728454589845
INFO:root:current train perplexity4.9787397384643555
INFO:root:current mean train loss 1828.5533183113473
INFO:root:current train perplexity4.976413726806641
INFO:root:current mean train loss 1828.8608926484078
INFO:root:current train perplexity4.978549480438232
INFO:root:current mean train loss 1829.103690907653
INFO:root:current train perplexity4.977275371551514
INFO:root:current mean train loss 1828.5512578863847
INFO:root:current train perplexity4.974730014801025
INFO:root:current mean train loss 1828.917925121166
INFO:root:current train perplexity4.974714279174805
INFO:root:current mean train loss 1830.7970409304596
INFO:root:current train perplexity4.980551719665527
INFO:root:current mean train loss 1829.343022608495
INFO:root:current train perplexity4.980678081512451
INFO:root:current mean train loss 1828.7435937881469
INFO:root:current train perplexity4.978842258453369

100%|██████████| 1/1 [08:06<00:00, 486.46s/it][A100%|██████████| 1/1 [08:06<00:00, 486.46s/it]
INFO:root:final mean train loss: 1828.8626640346756
INFO:root:final train perplexity: 4.9802422523498535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.61s/it][A100%|██████████| 1/1 [00:41<00:00, 41.63s/it]
INFO:root:eval mean loss: 1821.1517052616634
INFO:root:eval perplexity: 5.1751604080200195
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.07s/it][A100%|██████████| 1/1 [00:40<00:00, 40.07s/it]
INFO:root:eval mean loss: 2185.254473747091
INFO:root:eval perplexity: 7.444669246673584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/14
  7%|▋         | 14/200 [2:20:32<30:15:45, 585.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.8981273754223
INFO:root:current train perplexity5.0433125495910645
INFO:root:current mean train loss 1830.4112833955862
INFO:root:current train perplexity4.97564697265625
INFO:root:current mean train loss 1824.7821498178732
INFO:root:current train perplexity4.934634685516357
INFO:root:current mean train loss 1821.1126415580952
INFO:root:current train perplexity4.907717704772949
INFO:root:current mean train loss 1819.0587945933746
INFO:root:current train perplexity4.917314052581787
INFO:root:current mean train loss 1819.8027807480796
INFO:root:current train perplexity4.912719249725342
INFO:root:current mean train loss 1821.4446931416796
INFO:root:current train perplexity4.916130542755127
INFO:root:current mean train loss 1817.1815631095235
INFO:root:current train perplexity4.912882328033447
INFO:root:current mean train loss 1819.3439247195154
INFO:root:current train perplexity4.9195685386657715
INFO:root:current mean train loss 1818.778590508688
INFO:root:current train perplexity4.919101715087891
INFO:root:current mean train loss 1817.3600719897013
INFO:root:current train perplexity4.919121265411377
INFO:root:current mean train loss 1815.2649408909135
INFO:root:current train perplexity4.915080547332764
INFO:root:current mean train loss 1815.536981285052
INFO:root:current train perplexity4.914190292358398
INFO:root:current mean train loss 1815.6295049149508
INFO:root:current train perplexity4.913593769073486
INFO:root:current mean train loss 1816.5514070621032
INFO:root:current train perplexity4.919010162353516
INFO:root:current mean train loss 1816.783185414083
INFO:root:current train perplexity4.920774459838867
INFO:root:current mean train loss 1815.5478508913743
INFO:root:current train perplexity4.923378944396973
INFO:root:current mean train loss 1815.3169065485347
INFO:root:current train perplexity4.920919418334961
INFO:root:current mean train loss 1815.3895043719378
INFO:root:current train perplexity4.921035289764404
INFO:root:current mean train loss 1814.415523499512
INFO:root:current train perplexity4.9163665771484375

100%|██████████| 1/1 [08:01<00:00, 481.34s/it][A100%|██████████| 1/1 [08:01<00:00, 481.34s/it]
INFO:root:final mean train loss: 1814.3047427178872
INFO:root:final train perplexity: 4.9170002937316895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.02s/it][A100%|██████████| 1/1 [00:42<00:00, 42.04s/it]
INFO:root:eval mean loss: 1815.0556203422816
INFO:root:eval perplexity: 5.146759986877441
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.71s/it][A100%|██████████| 1/1 [00:38<00:00, 38.73s/it]
INFO:root:eval mean loss: 2180.4210417186114
INFO:root:eval perplexity: 7.411687850952148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/15
  8%|▊         | 15/200 [2:29:57<29:46:11, 579.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1784.815739384404
INFO:root:current train perplexity4.818675994873047
INFO:root:current mean train loss 1803.9873736493
INFO:root:current train perplexity4.871628284454346
INFO:root:current mean train loss 1800.8280730960876
INFO:root:current train perplexity4.848479270935059
INFO:root:current mean train loss 1802.1259424241923
INFO:root:current train perplexity4.8585205078125
INFO:root:current mean train loss 1802.8028177269755
INFO:root:current train perplexity4.86181640625
INFO:root:current mean train loss 1801.6907046762185
INFO:root:current train perplexity4.8623151779174805
INFO:root:current mean train loss 1799.9150894585005
INFO:root:current train perplexity4.856203556060791
INFO:root:current mean train loss 1801.7847097381673
INFO:root:current train perplexity4.858635425567627
INFO:root:current mean train loss 1801.3291503048613
INFO:root:current train perplexity4.8597259521484375
INFO:root:current mean train loss 1801.351664481173
INFO:root:current train perplexity4.861174583435059
INFO:root:current mean train loss 1801.0154158358782
INFO:root:current train perplexity4.861591815948486
INFO:root:current mean train loss 1800.5682283133733
INFO:root:current train perplexity4.863919258117676
INFO:root:current mean train loss 1800.9669541841108
INFO:root:current train perplexity4.865592002868652
INFO:root:current mean train loss 1799.7741129437084
INFO:root:current train perplexity4.862075328826904
INFO:root:current mean train loss 1800.154966750532
INFO:root:current train perplexity4.857936859130859
INFO:root:current mean train loss 1800.7008288369982
INFO:root:current train perplexity4.85718297958374
INFO:root:current mean train loss 1800.2588405032545
INFO:root:current train perplexity4.856017112731934
INFO:root:current mean train loss 1799.8766446086402
INFO:root:current train perplexity4.85537052154541
INFO:root:current mean train loss 1799.3750349618856
INFO:root:current train perplexity4.855852127075195
INFO:root:current mean train loss 1799.8874651656058
INFO:root:current train perplexity4.854276657104492

100%|██████████| 1/1 [08:05<00:00, 485.29s/it][A100%|██████████| 1/1 [08:05<00:00, 485.29s/it]
INFO:root:final mean train loss: 1799.9844522865747
INFO:root:final train perplexity: 4.855575084686279
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.64s/it][A100%|██████████| 1/1 [00:40<00:00, 40.65s/it]
INFO:root:eval mean loss: 1811.1287175171765
INFO:root:eval perplexity: 5.128549575805664
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.65s/it][A100%|██████████| 1/1 [00:39<00:00, 39.67s/it]
INFO:root:eval mean loss: 2179.8347319474456
INFO:root:eval perplexity: 7.407695293426514
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/16
  8%|▊         | 16/200 [2:39:25<29:26:01, 575.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1774.0677902866416
INFO:root:current train perplexity4.707767009735107
INFO:root:current mean train loss 1773.8553266915662
INFO:root:current train perplexity4.7497100830078125
INFO:root:current mean train loss 1777.9871929773985
INFO:root:current train perplexity4.75388240814209
INFO:root:current mean train loss 1782.7536275611733
INFO:root:current train perplexity4.778955459594727
INFO:root:current mean train loss 1784.1841792209893
INFO:root:current train perplexity4.7810773849487305
INFO:root:current mean train loss 1784.7616693660382
INFO:root:current train perplexity4.78407096862793
INFO:root:current mean train loss 1783.7022572274357
INFO:root:current train perplexity4.777918815612793
INFO:root:current mean train loss 1783.1075776690175
INFO:root:current train perplexity4.781006813049316
INFO:root:current mean train loss 1782.7294854603185
INFO:root:current train perplexity4.777650833129883
INFO:root:current mean train loss 1783.3310822193212
INFO:root:current train perplexity4.781565189361572
INFO:root:current mean train loss 1783.245989688193
INFO:root:current train perplexity4.784992218017578
INFO:root:current mean train loss 1784.6181599969643
INFO:root:current train perplexity4.788926124572754
INFO:root:current mean train loss 1786.0188054550736
INFO:root:current train perplexity4.794698715209961
INFO:root:current mean train loss 1785.5287961285103
INFO:root:current train perplexity4.7955756187438965
INFO:root:current mean train loss 1785.111449116513
INFO:root:current train perplexity4.793671131134033
INFO:root:current mean train loss 1785.9988625937401
INFO:root:current train perplexity4.793977737426758
INFO:root:current mean train loss 1787.0080988648265
INFO:root:current train perplexity4.796840667724609
INFO:root:current mean train loss 1787.7761763966332
INFO:root:current train perplexity4.79967737197876
INFO:root:current mean train loss 1787.833288228429
INFO:root:current train perplexity4.80103874206543
INFO:root:current mean train loss 1787.3222650056682
INFO:root:current train perplexity4.799313545227051

100%|██████████| 1/1 [08:10<00:00, 490.35s/it][A100%|██████████| 1/1 [08:10<00:00, 490.35s/it]
INFO:root:final mean train loss: 1786.994339674095
INFO:root:final train perplexity: 4.800518989562988
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.08s/it][A100%|██████████| 1/1 [00:41<00:00, 41.08s/it]
INFO:root:eval mean loss: 1807.8455083319482
INFO:root:eval perplexity: 5.113373756408691
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.21s/it][A100%|██████████| 1/1 [00:38<00:00, 38.21s/it]
INFO:root:eval mean loss: 2180.017045690658
INFO:root:eval perplexity: 7.408936977386475
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/17
  8%|▊         | 17/200 [2:48:57<29:12:42, 574.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1752.4685127951882
INFO:root:current train perplexity4.717718124389648
INFO:root:current mean train loss 1760.5406942164643
INFO:root:current train perplexity4.7139177322387695
INFO:root:current mean train loss 1770.5653258429634
INFO:root:current train perplexity4.74690055847168
INFO:root:current mean train loss 1772.0401343906049
INFO:root:current train perplexity4.749162197113037
INFO:root:current mean train loss 1776.3887671798957
INFO:root:current train perplexity4.7589921951293945
INFO:root:current mean train loss 1774.2500817954135
INFO:root:current train perplexity4.756500244140625
INFO:root:current mean train loss 1774.2661555090615
INFO:root:current train perplexity4.755748271942139
INFO:root:current mean train loss 1772.2242197724163
INFO:root:current train perplexity4.751762866973877
INFO:root:current mean train loss 1771.953205830342
INFO:root:current train perplexity4.7489914894104
INFO:root:current mean train loss 1770.4000281206509
INFO:root:current train perplexity4.743565559387207
INFO:root:current mean train loss 1771.3771061616785
INFO:root:current train perplexity4.744198799133301
INFO:root:current mean train loss 1772.6595678875342
INFO:root:current train perplexity4.747454643249512
INFO:root:current mean train loss 1773.8007134858126
INFO:root:current train perplexity4.749669551849365
INFO:root:current mean train loss 1774.762637971114
INFO:root:current train perplexity4.750542640686035
INFO:root:current mean train loss 1775.749159043835
INFO:root:current train perplexity4.75469446182251
INFO:root:current mean train loss 1775.0972522187894
INFO:root:current train perplexity4.752303123474121
INFO:root:current mean train loss 1774.1944157749556
INFO:root:current train perplexity4.750729084014893
INFO:root:current mean train loss 1774.429822678534
INFO:root:current train perplexity4.749125003814697
INFO:root:current mean train loss 1774.6097221374512
INFO:root:current train perplexity4.748342514038086

100%|██████████| 1/1 [08:14<00:00, 494.14s/it][A100%|██████████| 1/1 [08:14<00:00, 494.14s/it]
INFO:root:final mean train loss: 1775.0468440361235
INFO:root:final train perplexity: 4.750433444976807
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.27s/it][A100%|██████████| 1/1 [00:42<00:00, 42.29s/it]
INFO:root:eval mean loss: 1802.0458542844083
INFO:root:eval perplexity: 5.086674690246582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.91s/it][A100%|██████████| 1/1 [00:40<00:00, 40.93s/it]
INFO:root:eval mean loss: 2176.44784177956
INFO:root:eval perplexity: 7.3846821784973145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/18
  9%|▉         | 18/200 [2:58:36<29:07:39, 576.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1766.5009033203125
INFO:root:current train perplexity4.529467582702637
INFO:root:current mean train loss 1763.5919468470981
INFO:root:current train perplexity4.661122798919678
INFO:root:current mean train loss 1758.730399676067
INFO:root:current train perplexity4.6427903175354
INFO:root:current mean train loss 1760.429746734119
INFO:root:current train perplexity4.6685404777526855
INFO:root:current mean train loss 1760.921696566358
INFO:root:current train perplexity4.681921482086182
INFO:root:current mean train loss 1761.2813595006962
INFO:root:current train perplexity4.697706699371338
INFO:root:current mean train loss 1760.29255996578
INFO:root:current train perplexity4.695557117462158
INFO:root:current mean train loss 1760.8365532191933
INFO:root:current train perplexity4.690953254699707
INFO:root:current mean train loss 1758.3775436117041
INFO:root:current train perplexity4.6827311515808105
INFO:root:current mean train loss 1761.3555272897963
INFO:root:current train perplexity4.6850361824035645
INFO:root:current mean train loss 1762.3972924683223
INFO:root:current train perplexity4.687250137329102
INFO:root:current mean train loss 1764.4045004728152
INFO:root:current train perplexity4.688995361328125
INFO:root:current mean train loss 1763.5647472080848
INFO:root:current train perplexity4.692586421966553
INFO:root:current mean train loss 1763.1663016717553
INFO:root:current train perplexity4.693774223327637
INFO:root:current mean train loss 1763.4884810804049
INFO:root:current train perplexity4.694786071777344
INFO:root:current mean train loss 1762.397059524891
INFO:root:current train perplexity4.693851470947266
INFO:root:current mean train loss 1764.0801428488853
INFO:root:current train perplexity4.698620319366455
INFO:root:current mean train loss 1763.9308766295135
INFO:root:current train perplexity4.701131343841553
INFO:root:current mean train loss 1763.333342508332
INFO:root:current train perplexity4.698660850524902
INFO:root:current mean train loss 1764.1983889922694
INFO:root:current train perplexity4.701302528381348

100%|██████████| 1/1 [08:08<00:00, 488.85s/it][A100%|██████████| 1/1 [08:08<00:00, 488.86s/it]
INFO:root:final mean train loss: 1762.9618749729143
INFO:root:final train perplexity: 4.700303554534912
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.31s/it][A100%|██████████| 1/1 [00:43<00:00, 43.32s/it]
INFO:root:eval mean loss: 1800.5954416694372
INFO:root:eval perplexity: 5.080019474029541
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.90s/it][A100%|██████████| 1/1 [00:39<00:00, 39.90s/it]
INFO:root:eval mean loss: 2179.8776916071033
INFO:root:eval perplexity: 7.407990455627441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/19
 10%|▉         | 19/200 [3:08:11<28:56:25, 575.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1776.6541359641335
INFO:root:current train perplexity4.676163196563721
INFO:root:current mean train loss 1760.1499923956198
INFO:root:current train perplexity4.6482462882995605
INFO:root:current mean train loss 1767.2339439048424
INFO:root:current train perplexity4.638037204742432
INFO:root:current mean train loss 1760.8609528156542
INFO:root:current train perplexity4.639769077301025
INFO:root:current mean train loss 1759.8957878221267
INFO:root:current train perplexity4.639883041381836
INFO:root:current mean train loss 1761.965789210294
INFO:root:current train perplexity4.651196479797363
INFO:root:current mean train loss 1761.032118230004
INFO:root:current train perplexity4.650650978088379
INFO:root:current mean train loss 1760.0131364225351
INFO:root:current train perplexity4.653892993927002
INFO:root:current mean train loss 1757.0562935710823
INFO:root:current train perplexity4.657771587371826
INFO:root:current mean train loss 1757.477093280784
INFO:root:current train perplexity4.658644676208496
INFO:root:current mean train loss 1757.7996680451934
INFO:root:current train perplexity4.6600565910339355
INFO:root:current mean train loss 1755.7385926272143
INFO:root:current train perplexity4.654983043670654
INFO:root:current mean train loss 1754.154230145893
INFO:root:current train perplexity4.652559280395508
INFO:root:current mean train loss 1753.8358374060372
INFO:root:current train perplexity4.656436443328857
INFO:root:current mean train loss 1754.5444639825619
INFO:root:current train perplexity4.656383514404297
INFO:root:current mean train loss 1752.9485674142525
INFO:root:current train perplexity4.6553754806518555
INFO:root:current mean train loss 1752.698970018568
INFO:root:current train perplexity4.654716968536377
INFO:root:current mean train loss 1752.5320690478459
INFO:root:current train perplexity4.655890464782715
INFO:root:current mean train loss 1751.9709449876939
INFO:root:current train perplexity4.655356407165527
INFO:root:current mean train loss 1752.2691403328442
INFO:root:current train perplexity4.653952598571777

100%|██████████| 1/1 [08:21<00:00, 501.89s/it][A100%|██████████| 1/1 [08:21<00:00, 501.89s/it]
INFO:root:final mean train loss: 1751.5698113222647
INFO:root:final train perplexity: 4.653532028198242
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.48s/it][A100%|██████████| 1/1 [00:43<00:00, 43.50s/it]
INFO:root:eval mean loss: 1795.9986139392176
INFO:root:eval perplexity: 5.058984279632568
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.76s/it][A100%|██████████| 1/1 [00:39<00:00, 39.77s/it]
INFO:root:eval mean loss: 2176.937104353668
INFO:root:eval perplexity: 7.388002872467041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/20
 10%|█         | 20/200 [3:17:58<28:57:35, 579.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1737.8130634014424
INFO:root:current train perplexity4.635831832885742
INFO:root:current mean train loss 1726.8575123299797
INFO:root:current train perplexity4.565264701843262
INFO:root:current mean train loss 1729.8151656274517
INFO:root:current train perplexity4.556235313415527
INFO:root:current mean train loss 1735.0159343168095
INFO:root:current train perplexity4.578111171722412
INFO:root:current mean train loss 1740.2888558980815
INFO:root:current train perplexity4.589764595031738
INFO:root:current mean train loss 1737.8619063170368
INFO:root:current train perplexity4.58605432510376
INFO:root:current mean train loss 1738.5637208941585
INFO:root:current train perplexity4.583994388580322
INFO:root:current mean train loss 1739.4164612890097
INFO:root:current train perplexity4.591431617736816
INFO:root:current mean train loss 1737.8046415235772
INFO:root:current train perplexity4.5863776206970215
INFO:root:current mean train loss 1736.0093580739567
INFO:root:current train perplexity4.585891246795654
INFO:root:current mean train loss 1738.9037605410476
INFO:root:current train perplexity4.595374584197998
INFO:root:current mean train loss 1739.8607622503293
INFO:root:current train perplexity4.600594520568848
INFO:root:current mean train loss 1741.1701577396716
INFO:root:current train perplexity4.6032514572143555
INFO:root:current mean train loss 1742.1831507778952
INFO:root:current train perplexity4.604690074920654
INFO:root:current mean train loss 1742.4421586917456
INFO:root:current train perplexity4.604404449462891
INFO:root:current mean train loss 1741.6656384681864
INFO:root:current train perplexity4.604053020477295
INFO:root:current mean train loss 1740.8131204061642
INFO:root:current train perplexity4.605556964874268
INFO:root:current mean train loss 1741.2313924551422
INFO:root:current train perplexity4.607966899871826
INFO:root:current mean train loss 1741.0493068477263
INFO:root:current train perplexity4.60598087310791
INFO:root:current mean train loss 1741.7979335932464
INFO:root:current train perplexity4.610320568084717

100%|██████████| 1/1 [08:05<00:00, 485.73s/it][A100%|██████████| 1/1 [08:05<00:00, 485.73s/it]
INFO:root:final mean train loss: 1741.3266560933955
INFO:root:final train perplexity: 4.611874580383301
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.32s/it][A100%|██████████| 1/1 [00:40<00:00, 40.32s/it]
INFO:root:eval mean loss: 1792.631549375277
INFO:root:eval perplexity: 5.043631553649902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.41s/it][A100%|██████████| 1/1 [00:39<00:00, 39.41s/it]
INFO:root:eval mean loss: 2174.949997489334
INFO:root:eval perplexity: 7.374528408050537
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/21
 10%|█         | 21/200 [3:27:26<28:37:44, 575.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1762.0926927839007
INFO:root:current train perplexity4.598866939544678
INFO:root:current mean train loss 1736.9144185384114
INFO:root:current train perplexity4.567718505859375
INFO:root:current mean train loss 1726.7288765907288
INFO:root:current train perplexity4.546335697174072
INFO:root:current mean train loss 1725.559041569742
INFO:root:current train perplexity4.531905174255371
INFO:root:current mean train loss 1728.871387414765
INFO:root:current train perplexity4.54932975769043
INFO:root:current mean train loss 1727.322234229218
INFO:root:current train perplexity4.549970626831055
INFO:root:current mean train loss 1729.1578713393792
INFO:root:current train perplexity4.553457260131836
INFO:root:current mean train loss 1731.5221719085855
INFO:root:current train perplexity4.553546905517578
INFO:root:current mean train loss 1731.3886420704494
INFO:root:current train perplexity4.559239864349365
INFO:root:current mean train loss 1730.9246320524974
INFO:root:current train perplexity4.557219982147217
INFO:root:current mean train loss 1728.8854261456113
INFO:root:current train perplexity4.557975769042969
INFO:root:current mean train loss 1730.2360656104697
INFO:root:current train perplexity4.564077854156494
INFO:root:current mean train loss 1730.9762112562823
INFO:root:current train perplexity4.567038059234619
INFO:root:current mean train loss 1731.0121755234259
INFO:root:current train perplexity4.565679550170898
INFO:root:current mean train loss 1732.2256143590905
INFO:root:current train perplexity4.568830490112305
INFO:root:current mean train loss 1732.8934445417938
INFO:root:current train perplexity4.568692684173584
INFO:root:current mean train loss 1732.6499973610403
INFO:root:current train perplexity4.5681304931640625
INFO:root:current mean train loss 1731.6313237427037
INFO:root:current train perplexity4.566154479980469
INFO:root:current mean train loss 1730.7664767955912
INFO:root:current train perplexity4.565124034881592
INFO:root:current mean train loss 1731.184883289298
INFO:root:current train perplexity4.5680389404296875

100%|██████████| 1/1 [08:10<00:00, 490.04s/it][A100%|██████████| 1/1 [08:10<00:00, 490.04s/it]
INFO:root:final mean train loss: 1730.5281887621934
INFO:root:final train perplexity: 4.568363189697266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.65s/it][A100%|██████████| 1/1 [00:41<00:00, 41.65s/it]
INFO:root:eval mean loss: 1790.2881071482989
INFO:root:eval perplexity: 5.032973766326904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.53s/it][A100%|██████████| 1/1 [00:39<00:00, 39.53s/it]
INFO:root:eval mean loss: 2173.3067271996897
INFO:root:eval perplexity: 7.363404750823975
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/22
 11%|█         | 22/200 [3:36:59<28:26:07, 575.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1686.2857967010916
INFO:root:current train perplexity4.481353759765625
INFO:root:current mean train loss 1692.0231178592396
INFO:root:current train perplexity4.460539817810059
INFO:root:current mean train loss 1709.1056265918326
INFO:root:current train perplexity4.491357326507568
INFO:root:current mean train loss 1713.170166015625
INFO:root:current train perplexity4.511720657348633
INFO:root:current mean train loss 1714.8698771761033
INFO:root:current train perplexity4.521704196929932
INFO:root:current mean train loss 1717.7791481750382
INFO:root:current train perplexity4.5245819091796875
INFO:root:current mean train loss 1719.3321531389302
INFO:root:current train perplexity4.527590274810791
INFO:root:current mean train loss 1719.6748929634439
INFO:root:current train perplexity4.525837421417236
INFO:root:current mean train loss 1721.9240933797341
INFO:root:current train perplexity4.5256123542785645
INFO:root:current mean train loss 1720.4823957798046
INFO:root:current train perplexity4.521519184112549
INFO:root:current mean train loss 1720.603262041844
INFO:root:current train perplexity4.519609451293945
INFO:root:current mean train loss 1720.752603230066
INFO:root:current train perplexity4.520839214324951
INFO:root:current mean train loss 1720.110525414406
INFO:root:current train perplexity4.519949913024902
INFO:root:current mean train loss 1721.1796905228628
INFO:root:current train perplexity4.5231032371521
INFO:root:current mean train loss 1720.4708852774418
INFO:root:current train perplexity4.523587226867676
INFO:root:current mean train loss 1720.7778528289891
INFO:root:current train perplexity4.526192665100098
INFO:root:current mean train loss 1721.2864090576902
INFO:root:current train perplexity4.528796195983887
INFO:root:current mean train loss 1721.192697455099
INFO:root:current train perplexity4.528753280639648
INFO:root:current mean train loss 1722.122059558842
INFO:root:current train perplexity4.532876968383789
INFO:root:current mean train loss 1721.3479513099699
INFO:root:current train perplexity4.53032922744751

100%|██████████| 1/1 [08:10<00:00, 490.86s/it][A100%|██████████| 1/1 [08:10<00:00, 490.86s/it]
INFO:root:final mean train loss: 1720.942450372847
INFO:root:final train perplexity: 4.5300822257995605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.95s/it][A100%|██████████| 1/1 [00:38<00:00, 38.95s/it]
INFO:root:eval mean loss: 1790.9270487034576
INFO:root:eval perplexity: 5.035877704620361
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.26s/it][A100%|██████████| 1/1 [00:37<00:00, 37.26s/it]
INFO:root:eval mean loss: 2180.701259315437
INFO:root:eval perplexity: 7.4135966300964355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/23
 12%|█▏        | 23/200 [3:46:29<28:11:23, 573.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1688.4434244791667
INFO:root:current train perplexity4.457791328430176
INFO:root:current mean train loss 1702.7551102487664
INFO:root:current train perplexity4.468095302581787
INFO:root:current mean train loss 1701.0469974912446
INFO:root:current train perplexity4.467536926269531
INFO:root:current mean train loss 1696.9726684570312
INFO:root:current train perplexity4.454095840454102
INFO:root:current mean train loss 1699.3230994399712
INFO:root:current train perplexity4.46666145324707
INFO:root:current mean train loss 1700.497594801046
INFO:root:current train perplexity4.47352933883667
INFO:root:current mean train loss 1700.368911875849
INFO:root:current train perplexity4.472811222076416
INFO:root:current mean train loss 1699.8989619387855
INFO:root:current train perplexity4.47212553024292
INFO:root:current mean train loss 1699.8507425715413
INFO:root:current train perplexity4.474325180053711
INFO:root:current mean train loss 1703.049547353417
INFO:root:current train perplexity4.477571964263916
INFO:root:current mean train loss 1703.4023015293506
INFO:root:current train perplexity4.4761881828308105
INFO:root:current mean train loss 1705.0382081103926
INFO:root:current train perplexity4.482154369354248
INFO:root:current mean train loss 1707.2466700354287
INFO:root:current train perplexity4.486696243286133
INFO:root:current mean train loss 1707.3113113513095
INFO:root:current train perplexity4.48573637008667
INFO:root:current mean train loss 1707.7894971194683
INFO:root:current train perplexity4.489862442016602
INFO:root:current mean train loss 1709.6313751412638
INFO:root:current train perplexity4.489998817443848
INFO:root:current mean train loss 1710.2401955147466
INFO:root:current train perplexity4.48951530456543
INFO:root:current mean train loss 1711.1211493294998
INFO:root:current train perplexity4.489110946655273
INFO:root:current mean train loss 1711.0326587818288
INFO:root:current train perplexity4.488527297973633

100%|██████████| 1/1 [08:08<00:00, 488.76s/it][A100%|██████████| 1/1 [08:08<00:00, 488.76s/it]
INFO:root:final mean train loss: 1710.590594977967
INFO:root:final train perplexity: 4.489102363586426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.13s/it][A100%|██████████| 1/1 [00:42<00:00, 42.14s/it]
INFO:root:eval mean loss: 1787.9078044485539
INFO:root:eval perplexity: 5.022171974182129
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.36s/it][A100%|██████████| 1/1 [00:39<00:00, 39.37s/it]
INFO:root:eval mean loss: 2177.857441354305
INFO:root:eval perplexity: 7.394253253936768
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/24
 12%|█▏        | 24/200 [3:56:01<28:01:15, 573.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1779.0603899274554
INFO:root:current train perplexity4.63254451751709
INFO:root:current mean train loss 1718.6234883816442
INFO:root:current train perplexity4.495179176330566
INFO:root:current mean train loss 1717.2279052734375
INFO:root:current train perplexity4.477677345275879
INFO:root:current mean train loss 1714.1040023157573
INFO:root:current train perplexity4.4668378829956055
INFO:root:current mean train loss 1713.5360065432087
INFO:root:current train perplexity4.4711833000183105
INFO:root:current mean train loss 1709.1019108939688
INFO:root:current train perplexity4.4542765617370605
INFO:root:current mean train loss 1708.4786224113855
INFO:root:current train perplexity4.457728385925293
INFO:root:current mean train loss 1709.5655581462165
INFO:root:current train perplexity4.458893299102783
INFO:root:current mean train loss 1706.8660823628213
INFO:root:current train perplexity4.458195209503174
INFO:root:current mean train loss 1705.7791102029785
INFO:root:current train perplexity4.4581475257873535
INFO:root:current mean train loss 1705.3323453355806
INFO:root:current train perplexity4.45473051071167
INFO:root:current mean train loss 1704.3368852816945
INFO:root:current train perplexity4.45617151260376
INFO:root:current mean train loss 1702.3837789489696
INFO:root:current train perplexity4.451972484588623
INFO:root:current mean train loss 1702.7706478151001
INFO:root:current train perplexity4.453903675079346
INFO:root:current mean train loss 1702.2110894155062
INFO:root:current train perplexity4.452818870544434
INFO:root:current mean train loss 1703.1984933753163
INFO:root:current train perplexity4.4550299644470215
INFO:root:current mean train loss 1702.274524054916
INFO:root:current train perplexity4.454277992248535
INFO:root:current mean train loss 1702.0736261333159
INFO:root:current train perplexity4.45241641998291
INFO:root:current mean train loss 1701.9262722334151
INFO:root:current train perplexity4.451893329620361
INFO:root:current mean train loss 1702.7470136621453
INFO:root:current train perplexity4.454885005950928

100%|██████████| 1/1 [08:02<00:00, 482.47s/it][A100%|██████████| 1/1 [08:02<00:00, 482.47s/it]
INFO:root:final mean train loss: 1702.0632127539654
INFO:root:final train perplexity: 4.455622673034668
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.03s/it][A100%|██████████| 1/1 [00:42<00:00, 42.03s/it]
INFO:root:eval mean loss: 1786.6317108370733
INFO:root:eval perplexity: 5.016390323638916
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.03s/it][A100%|██████████| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 2179.688893419631
INFO:root:eval perplexity: 7.406702995300293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/25
 12%|█▎        | 25/200 [4:05:28<27:46:11, 571.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.5280303955078
INFO:root:current train perplexity4.4445366859436035
INFO:root:current mean train loss 1695.0454229539441
INFO:root:current train perplexity4.407436847686768
INFO:root:current mean train loss 1683.3057141985212
INFO:root:current train perplexity4.381512641906738
INFO:root:current mean train loss 1681.6429767373168
INFO:root:current train perplexity4.382369518280029
INFO:root:current mean train loss 1684.973229174344
INFO:root:current train perplexity4.3913044929504395
INFO:root:current mean train loss 1685.5450977587518
INFO:root:current train perplexity4.390964031219482
INFO:root:current mean train loss 1686.123795338166
INFO:root:current train perplexity4.3976640701293945
INFO:root:current mean train loss 1688.2609394558226
INFO:root:current train perplexity4.4050774574279785
INFO:root:current mean train loss 1690.453339660052
INFO:root:current train perplexity4.4086527824401855
INFO:root:current mean train loss 1691.827173274317
INFO:root:current train perplexity4.4120306968688965
INFO:root:current mean train loss 1691.9296669960022
INFO:root:current train perplexity4.412263870239258
INFO:root:current mean train loss 1692.087440029158
INFO:root:current train perplexity4.412918567657471
INFO:root:current mean train loss 1691.7211705625448
INFO:root:current train perplexity4.412707328796387
INFO:root:current mean train loss 1692.6041982598895
INFO:root:current train perplexity4.413750648498535
INFO:root:current mean train loss 1692.0473097040413
INFO:root:current train perplexity4.415843486785889
INFO:root:current mean train loss 1691.3201237075284
INFO:root:current train perplexity4.414491653442383
INFO:root:current mean train loss 1691.3456545674742
INFO:root:current train perplexity4.416565418243408
INFO:root:current mean train loss 1692.027012729866
INFO:root:current train perplexity4.415948867797852
INFO:root:current mean train loss 1691.6027351513244
INFO:root:current train perplexity4.414667129516602
INFO:root:current mean train loss 1692.0751501388709
INFO:root:current train perplexity4.417258262634277

100%|██████████| 1/1 [08:00<00:00, 480.28s/it][A100%|██████████| 1/1 [08:00<00:00, 480.28s/it]
INFO:root:final mean train loss: 1692.799017201634
INFO:root:final train perplexity: 4.419533729553223
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.39s/it][A100%|██████████| 1/1 [00:40<00:00, 40.39s/it]
INFO:root:eval mean loss: 1784.2598812022109
INFO:root:eval perplexity: 5.0056610107421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.84s/it][A100%|██████████| 1/1 [00:37<00:00, 37.84s/it]
INFO:root:eval mean loss: 2178.646697348737
INFO:root:eval perplexity: 7.399616241455078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/26
 13%|█▎        | 26/200 [4:14:49<27:27:34, 568.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1666.878700814596
INFO:root:current train perplexity4.319657802581787
INFO:root:current mean train loss 1673.3263346354167
INFO:root:current train perplexity4.346915245056152
INFO:root:current mean train loss 1677.4398284532222
INFO:root:current train perplexity4.3540496826171875
INFO:root:current mean train loss 1678.577725497159
INFO:root:current train perplexity4.363217353820801
INFO:root:current mean train loss 1676.6767830016122
INFO:root:current train perplexity4.360749244689941
INFO:root:current mean train loss 1677.7564279834796
INFO:root:current train perplexity4.362046241760254
INFO:root:current mean train loss 1680.4915085910077
INFO:root:current train perplexity4.3653564453125
INFO:root:current mean train loss 1679.0913277032726
INFO:root:current train perplexity4.3598952293396
INFO:root:current mean train loss 1679.527587890625
INFO:root:current train perplexity4.362521171569824
INFO:root:current mean train loss 1679.2495062703408
INFO:root:current train perplexity4.360332012176514
INFO:root:current mean train loss 1681.232313407143
INFO:root:current train perplexity4.367161273956299
INFO:root:current mean train loss 1681.158408108978
INFO:root:current train perplexity4.369968891143799
INFO:root:current mean train loss 1682.9405633648205
INFO:root:current train perplexity4.375192642211914
INFO:root:current mean train loss 1683.4313834671827
INFO:root:current train perplexity4.376836776733398
INFO:root:current mean train loss 1683.8556269077192
INFO:root:current train perplexity4.379585266113281
INFO:root:current mean train loss 1684.1233163644245
INFO:root:current train perplexity4.381084442138672
INFO:root:current mean train loss 1684.586476364926
INFO:root:current train perplexity4.382809638977051
INFO:root:current mean train loss 1685.0396917125126
INFO:root:current train perplexity4.383016586303711
INFO:root:current mean train loss 1683.9245012688416
INFO:root:current train perplexity4.3824334144592285
INFO:root:current mean train loss 1684.2535062040145
INFO:root:current train perplexity4.384536266326904

100%|██████████| 1/1 [07:57<00:00, 477.36s/it][A100%|██████████| 1/1 [07:57<00:00, 477.36s/it]
INFO:root:final mean train loss: 1683.8862406566657
INFO:root:final train perplexity: 4.38508939743042
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.07s/it][A100%|██████████| 1/1 [00:40<00:00, 40.07s/it]
INFO:root:eval mean loss: 1784.505649864251
INFO:root:eval perplexity: 5.006772994995117
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.41s/it][A100%|██████████| 1/1 [00:37<00:00, 37.41s/it]
INFO:root:eval mean loss: 2180.3189476500165
INFO:root:eval perplexity: 7.410991668701172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/27
 14%|█▎        | 27/200 [4:24:06<27:08:39, 564.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1647.3301602067618
INFO:root:current train perplexity4.270466327667236
INFO:root:current mean train loss 1659.95375852947
INFO:root:current train perplexity4.2914533615112305
INFO:root:current mean train loss 1660.863199396651
INFO:root:current train perplexity4.291282653808594
INFO:root:current mean train loss 1664.4813263109943
INFO:root:current train perplexity4.310060501098633
INFO:root:current mean train loss 1666.7744473786333
INFO:root:current train perplexity4.310906887054443
INFO:root:current mean train loss 1663.8724649627577
INFO:root:current train perplexity4.32019567489624
INFO:root:current mean train loss 1663.4217020979768
INFO:root:current train perplexity4.322124481201172
INFO:root:current mean train loss 1667.7458716722151
INFO:root:current train perplexity4.3275580406188965
INFO:root:current mean train loss 1668.3080903878024
INFO:root:current train perplexity4.331613540649414
INFO:root:current mean train loss 1669.03103618582
INFO:root:current train perplexity4.337836265563965
INFO:root:current mean train loss 1670.6254025551232
INFO:root:current train perplexity4.3385820388793945
INFO:root:current mean train loss 1672.5190379088406
INFO:root:current train perplexity4.339677810668945
INFO:root:current mean train loss 1673.0893362557756
INFO:root:current train perplexity4.340429782867432
INFO:root:current mean train loss 1673.7425847229094
INFO:root:current train perplexity4.341183185577393
INFO:root:current mean train loss 1674.3243955761318
INFO:root:current train perplexity4.34250020980835
INFO:root:current mean train loss 1674.7656449010651
INFO:root:current train perplexity4.345035076141357
INFO:root:current mean train loss 1674.7886405549089
INFO:root:current train perplexity4.345857620239258
INFO:root:current mean train loss 1675.5454570262505
INFO:root:current train perplexity4.349974155426025
INFO:root:current mean train loss 1676.1184214744937
INFO:root:current train perplexity4.3503570556640625
INFO:root:current mean train loss 1675.673206800825
INFO:root:current train perplexity4.351345062255859

100%|██████████| 1/1 [07:49<00:00, 469.53s/it][A100%|██████████| 1/1 [07:49<00:00, 469.53s/it]
INFO:root:final mean train loss: 1675.0587836576722
INFO:root:final train perplexity: 4.3512396812438965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.31s/it][A100%|██████████| 1/1 [00:39<00:00, 39.31s/it]
INFO:root:eval mean loss: 1782.2626546223958
INFO:root:eval perplexity: 4.996645450592041
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.25s/it][A100%|██████████| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2182.6242489645665
INFO:root:eval perplexity: 7.426703929901123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/28
 14%|█▍        | 28/200 [4:33:15<26:45:00, 559.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1682.5503304036458
INFO:root:current train perplexity4.317443370819092
INFO:root:current mean train loss 1654.7870884486608
INFO:root:current train perplexity4.2690887451171875
INFO:root:current mean train loss 1658.4722922585227
INFO:root:current train perplexity4.289361476898193
INFO:root:current mean train loss 1659.6084658203124
INFO:root:current train perplexity4.28971004486084
INFO:root:current mean train loss 1659.119675935444
INFO:root:current train perplexity4.293333053588867
INFO:root:current mean train loss 1661.6303105893342
INFO:root:current train perplexity4.294641017913818
INFO:root:current mean train loss 1663.3734013310186
INFO:root:current train perplexity4.301863193511963
INFO:root:current mean train loss 1664.8951931073589
INFO:root:current train perplexity4.308051109313965
INFO:root:current mean train loss 1667.2169070870536
INFO:root:current train perplexity4.308958053588867
INFO:root:current mean train loss 1667.750896935096
INFO:root:current train perplexity4.3117475509643555
INFO:root:current mean train loss 1664.9432424146075
INFO:root:current train perplexity4.308885097503662
INFO:root:current mean train loss 1664.7283135596742
INFO:root:current train perplexity4.310386657714844
INFO:root:current mean train loss 1664.5229658777573
INFO:root:current train perplexity4.313024520874023
INFO:root:current mean train loss 1665.9273148970171
INFO:root:current train perplexity4.314898490905762
INFO:root:current mean train loss 1666.5779468187236
INFO:root:current train perplexity4.316897392272949
INFO:root:current mean train loss 1666.3771936228918
INFO:root:current train perplexity4.31568717956543
INFO:root:current mean train loss 1667.162628410681
INFO:root:current train perplexity4.318142890930176
INFO:root:current mean train loss 1667.3559871533892
INFO:root:current train perplexity4.318662166595459
INFO:root:current mean train loss 1666.753578515625
INFO:root:current train perplexity4.319332599639893
INFO:root:current mean train loss 1667.6200310274921
INFO:root:current train perplexity4.321640968322754

100%|██████████| 1/1 [07:48<00:00, 468.91s/it][A100%|██████████| 1/1 [07:48<00:00, 468.91s/it]
INFO:root:final mean train loss: 1667.3758847173633
INFO:root:final train perplexity: 4.321991920471191
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.99s/it][A100%|██████████| 1/1 [00:39<00:00, 39.99s/it]
INFO:root:eval mean loss: 1781.1429768014461
INFO:root:eval perplexity: 4.991598606109619
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.86s/it][A100%|██████████| 1/1 [00:35<00:00, 35.86s/it]
INFO:root:eval mean loss: 2181.956685816988
INFO:root:eval perplexity: 7.4221510887146
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/29
 14%|█▍        | 29/200 [4:42:22<26:24:40, 556.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1658.3162775454314
INFO:root:current train perplexity4.284348964691162
INFO:root:current mean train loss 1654.4135697682698
INFO:root:current train perplexity4.277597427368164
INFO:root:current mean train loss 1651.208276199968
INFO:root:current train perplexity4.273409366607666
INFO:root:current mean train loss 1651.9761695083307
INFO:root:current train perplexity4.2796502113342285
INFO:root:current mean train loss 1654.5003121228722
INFO:root:current train perplexity4.288163185119629
INFO:root:current mean train loss 1654.3272416398331
INFO:root:current train perplexity4.289026260375977
INFO:root:current mean train loss 1652.8544583182804
INFO:root:current train perplexity4.290598392486572
INFO:root:current mean train loss 1652.4959286776457
INFO:root:current train perplexity4.2917327880859375
INFO:root:current mean train loss 1654.315556547567
INFO:root:current train perplexity4.294147491455078
INFO:root:current mean train loss 1654.4205451473113
INFO:root:current train perplexity4.292372226715088
INFO:root:current mean train loss 1656.5987234709464
INFO:root:current train perplexity4.2955474853515625
INFO:root:current mean train loss 1656.7971064420356
INFO:root:current train perplexity4.295290946960449
INFO:root:current mean train loss 1656.766839467335
INFO:root:current train perplexity4.293166637420654
INFO:root:current mean train loss 1656.996764347471
INFO:root:current train perplexity4.293943881988525
INFO:root:current mean train loss 1657.2911728764348
INFO:root:current train perplexity4.29286003112793
INFO:root:current mean train loss 1657.8840651008952
INFO:root:current train perplexity4.293376445770264
INFO:root:current mean train loss 1658.6336996019872
INFO:root:current train perplexity4.291611671447754
INFO:root:current mean train loss 1659.0894056728907
INFO:root:current train perplexity4.2897772789001465
INFO:root:current mean train loss 1660.0480118926937
INFO:root:current train perplexity4.292335510253906

100%|██████████| 1/1 [07:46<00:00, 467.00s/it][A100%|██████████| 1/1 [07:46<00:00, 467.00s/it]
INFO:root:final mean train loss: 1659.7048991439442
INFO:root:final train perplexity: 4.292985439300537
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 1782.731895068013
INFO:root:eval perplexity: 4.998762130737305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.39s/it][A100%|██████████| 1/1 [00:37<00:00, 37.39s/it]
INFO:root:eval mean loss: 2188.6792026817375
INFO:root:eval perplexity: 7.468128681182861
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/30
 15%|█▌        | 30/200 [4:51:27<26:06:36, 552.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1661.4504258897568
INFO:root:current train perplexity4.172812461853027
INFO:root:current mean train loss 1641.2345530658688
INFO:root:current train perplexity4.211198329925537
INFO:root:current mean train loss 1643.9837459582461
INFO:root:current train perplexity4.224086761474609
INFO:root:current mean train loss 1644.887982908576
INFO:root:current train perplexity4.225481986999512
INFO:root:current mean train loss 1643.2664478553827
INFO:root:current train perplexity4.22600793838501
INFO:root:current mean train loss 1642.2541405578493
INFO:root:current train perplexity4.229220390319824
INFO:root:current mean train loss 1645.9865442034843
INFO:root:current train perplexity4.240893840789795
INFO:root:current mean train loss 1646.0451272768094
INFO:root:current train perplexity4.23968505859375
INFO:root:current mean train loss 1646.5929946427882
INFO:root:current train perplexity4.246600151062012
INFO:root:current mean train loss 1647.3916989233103
INFO:root:current train perplexity4.247377395629883
INFO:root:current mean train loss 1645.4737877897749
INFO:root:current train perplexity4.244792938232422
INFO:root:current mean train loss 1644.2934182857587
INFO:root:current train perplexity4.242393970489502
INFO:root:current mean train loss 1645.7754253579922
INFO:root:current train perplexity4.245842456817627
INFO:root:current mean train loss 1647.1908806668973
INFO:root:current train perplexity4.2493367195129395
INFO:root:current mean train loss 1647.6667802755167
INFO:root:current train perplexity4.252381801605225
INFO:root:current mean train loss 1649.1984800992225
INFO:root:current train perplexity4.2568864822387695
INFO:root:current mean train loss 1649.136347228359
INFO:root:current train perplexity4.258622646331787
INFO:root:current mean train loss 1650.7459083945519
INFO:root:current train perplexity4.262341022491455
INFO:root:current mean train loss 1651.962297548175
INFO:root:current train perplexity4.263790607452393
INFO:root:current mean train loss 1652.5736030910325
INFO:root:current train perplexity4.263417720794678

100%|██████████| 1/1 [07:47<00:00, 467.16s/it][A100%|██████████| 1/1 [07:47<00:00, 467.16s/it]
INFO:root:final mean train loss: 1651.681087399635
INFO:root:final train perplexity: 4.262852668762207
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.98s/it][A100%|██████████| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 1781.3691739562555
INFO:root:eval perplexity: 4.992617607116699
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.12s/it][A100%|██████████| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2187.989055227726
INFO:root:eval perplexity: 7.463393688201904
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/31
 16%|█▌        | 31/200 [5:00:31<25:49:32, 550.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1623.0665001502405
INFO:root:current train perplexity4.209607124328613
INFO:root:current mean train loss 1617.8354995969742
INFO:root:current train perplexity4.166691303253174
INFO:root:current mean train loss 1630.901864110896
INFO:root:current train perplexity4.1862945556640625
INFO:root:current mean train loss 1633.2218500617091
INFO:root:current train perplexity4.207909107208252
INFO:root:current mean train loss 1635.2560573810704
INFO:root:current train perplexity4.215856075286865
INFO:root:current mean train loss 1638.0539369764438
INFO:root:current train perplexity4.2229695320129395
INFO:root:current mean train loss 1635.2869122294953
INFO:root:current train perplexity4.224132061004639
INFO:root:current mean train loss 1640.3967507102273
INFO:root:current train perplexity4.231601715087891
INFO:root:current mean train loss 1638.6622055829582
INFO:root:current train perplexity4.231271743774414
INFO:root:current mean train loss 1639.7813163081719
INFO:root:current train perplexity4.234051704406738
INFO:root:current mean train loss 1640.394992285537
INFO:root:current train perplexity4.2308831214904785
INFO:root:current mean train loss 1640.8131476448018
INFO:root:current train perplexity4.231175899505615
INFO:root:current mean train loss 1641.593611301839
INFO:root:current train perplexity4.231174468994141
INFO:root:current mean train loss 1642.358098324967
INFO:root:current train perplexity4.233100891113281
INFO:root:current mean train loss 1641.860840699783
INFO:root:current train perplexity4.231462478637695
INFO:root:current mean train loss 1641.7445125954805
INFO:root:current train perplexity4.231685638427734
INFO:root:current mean train loss 1642.8647496222277
INFO:root:current train perplexity4.232595920562744
INFO:root:current mean train loss 1643.4156246605228
INFO:root:current train perplexity4.2323737144470215
INFO:root:current mean train loss 1643.6551747651115
INFO:root:current train perplexity4.233203887939453
INFO:root:current mean train loss 1643.932894349222
INFO:root:current train perplexity4.232268810272217

100%|██████████| 1/1 [07:53<00:00, 473.65s/it][A100%|██████████| 1/1 [07:53<00:00, 473.65s/it]
INFO:root:final mean train loss: 1643.5819261698066
INFO:root:final train perplexity: 4.232652187347412
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.51s/it][A100%|██████████| 1/1 [00:39<00:00, 39.51s/it]
INFO:root:eval mean loss: 1781.3575599616302
INFO:root:eval perplexity: 4.992565155029297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.85s/it][A100%|██████████| 1/1 [00:36<00:00, 36.85s/it]
INFO:root:eval mean loss: 2189.3854764032026
INFO:root:eval perplexity: 7.47297477722168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/32
 16%|█▌        | 32/200 [5:09:43<25:42:14, 550.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.5844357512717
INFO:root:current train perplexity4.134675979614258
INFO:root:current mean train loss 1620.0880767182036
INFO:root:current train perplexity4.173094749450684
INFO:root:current mean train loss 1615.8255047582304
INFO:root:current train perplexity4.16733980178833
INFO:root:current mean train loss 1623.7221238383747
INFO:root:current train perplexity4.172117233276367
INFO:root:current mean train loss 1627.7899969027758
INFO:root:current train perplexity4.176584243774414
INFO:root:current mean train loss 1628.7011583865676
INFO:root:current train perplexity4.175839900970459
INFO:root:current mean train loss 1631.5581853934802
INFO:root:current train perplexity4.182717323303223
INFO:root:current mean train loss 1631.801376979412
INFO:root:current train perplexity4.184892177581787
INFO:root:current mean train loss 1631.499063837995
INFO:root:current train perplexity4.184261322021484
INFO:root:current mean train loss 1632.9173360037778
INFO:root:current train perplexity4.186379432678223
INFO:root:current mean train loss 1634.8909042519326
INFO:root:current train perplexity4.192774295806885
INFO:root:current mean train loss 1632.8368058759502
INFO:root:current train perplexity4.190932750701904
INFO:root:current mean train loss 1632.0668774433705
INFO:root:current train perplexity4.1880693435668945
INFO:root:current mean train loss 1633.6097764777144
INFO:root:current train perplexity4.1883625984191895
INFO:root:current mean train loss 1633.8683379894317
INFO:root:current train perplexity4.1910176277160645
INFO:root:current mean train loss 1634.484874831649
INFO:root:current train perplexity4.190572261810303
INFO:root:current mean train loss 1635.3822386283666
INFO:root:current train perplexity4.194703102111816
INFO:root:current mean train loss 1635.2145657826618
INFO:root:current train perplexity4.197732925415039
INFO:root:current mean train loss 1636.7961171440502
INFO:root:current train perplexity4.202060699462891
INFO:root:current mean train loss 1637.0511754183688
INFO:root:current train perplexity4.203953266143799

100%|██████████| 1/1 [07:47<00:00, 467.20s/it][A100%|██████████| 1/1 [07:47<00:00, 467.20s/it]
INFO:root:final mean train loss: 1636.3602676891765
INFO:root:final train perplexity: 4.205903053283691
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.10s/it][A100%|██████████| 1/1 [00:39<00:00, 39.10s/it]
INFO:root:eval mean loss: 1779.1418444045046
INFO:root:eval perplexity: 4.982590198516846
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.66s/it][A100%|██████████| 1/1 [00:36<00:00, 36.66s/it]
INFO:root:eval mean loss: 2188.118607757785
INFO:root:eval perplexity: 7.464285373687744
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/33
 16%|█▋        | 33/200 [5:18:48<25:28:23, 549.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.0627034505208
INFO:root:current train perplexity4.113010406494141
INFO:root:current mean train loss 1607.156699371338
INFO:root:current train perplexity4.101868152618408
INFO:root:current mean train loss 1611.1623990572416
INFO:root:current train perplexity4.113871097564697
INFO:root:current mean train loss 1620.875552368164
INFO:root:current train perplexity4.138853549957275
INFO:root:current mean train loss 1628.7445766283117
INFO:root:current train perplexity4.160114765167236
INFO:root:current mean train loss 1625.6819880894252
INFO:root:current train perplexity4.1483917236328125
INFO:root:current mean train loss 1627.3575624408145
INFO:root:current train perplexity4.151247978210449
INFO:root:current mean train loss 1631.6085301449425
INFO:root:current train perplexity4.164660930633545
INFO:root:current mean train loss 1631.7283890125364
INFO:root:current train perplexity4.168081760406494
INFO:root:current mean train loss 1632.711747487386
INFO:root:current train perplexity4.173354625701904
INFO:root:current mean train loss 1632.989034746278
INFO:root:current train perplexity4.176067352294922
INFO:root:current mean train loss 1632.4362657218144
INFO:root:current train perplexity4.174246788024902
INFO:root:current mean train loss 1631.8950632246713
INFO:root:current train perplexity4.176712512969971
INFO:root:current mean train loss 1631.7352640488568
INFO:root:current train perplexity4.175741195678711
INFO:root:current mean train loss 1630.4060646370665
INFO:root:current train perplexity4.174774646759033
INFO:root:current mean train loss 1631.0598533434745
INFO:root:current train perplexity4.178192615509033
INFO:root:current mean train loss 1630.0409167186324
INFO:root:current train perplexity4.177718639373779
INFO:root:current mean train loss 1629.7575170343573
INFO:root:current train perplexity4.17930793762207
INFO:root:current mean train loss 1629.6761605867775
INFO:root:current train perplexity4.178393840789795
INFO:root:current mean train loss 1628.858177527603
INFO:root:current train perplexity4.176911354064941

100%|██████████| 1/1 [07:47<00:00, 467.88s/it][A100%|██████████| 1/1 [07:47<00:00, 467.88s/it]
INFO:root:final mean train loss: 1628.2946530494555
INFO:root:final train perplexity: 4.1762285232543945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.21s/it][A100%|██████████| 1/1 [00:39<00:00, 39.21s/it]
INFO:root:eval mean loss: 1781.676189449662
INFO:root:eval perplexity: 4.994001388549805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.87s/it][A100%|██████████| 1/1 [00:36<00:00, 36.87s/it]
INFO:root:eval mean loss: 2195.150958122091
INFO:root:eval perplexity: 7.512660503387451
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/34
 17%|█▋        | 34/200 [5:27:55<25:16:46, 548.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.2200436282467
INFO:root:current train perplexity4.093884468078613
INFO:root:current mean train loss 1617.6847082450565
INFO:root:current train perplexity4.129300594329834
INFO:root:current mean train loss 1615.430129949797
INFO:root:current train perplexity4.128040313720703
INFO:root:current mean train loss 1613.1662610608007
INFO:root:current train perplexity4.129173755645752
INFO:root:current mean train loss 1612.9918517426624
INFO:root:current train perplexity4.124524116516113
INFO:root:current mean train loss 1613.0746078094535
INFO:root:current train perplexity4.119452953338623
INFO:root:current mean train loss 1615.280638386263
INFO:root:current train perplexity4.118978023529053
INFO:root:current mean train loss 1617.16690263785
INFO:root:current train perplexity4.1265130043029785
INFO:root:current mean train loss 1616.9822295133445
INFO:root:current train perplexity4.131126403808594
INFO:root:current mean train loss 1618.4190753792302
INFO:root:current train perplexity4.1337361335754395
INFO:root:current mean train loss 1618.7349656298964
INFO:root:current train perplexity4.137458801269531
INFO:root:current mean train loss 1617.956544213307
INFO:root:current train perplexity4.1397199630737305
INFO:root:current mean train loss 1620.3869727365468
INFO:root:current train perplexity4.143915176391602
INFO:root:current mean train loss 1620.829226203647
INFO:root:current train perplexity4.146406650543213
INFO:root:current mean train loss 1620.055237601557
INFO:root:current train perplexity4.146702289581299
INFO:root:current mean train loss 1620.464803188907
INFO:root:current train perplexity4.147752285003662
INFO:root:current mean train loss 1622.634567924959
INFO:root:current train perplexity4.1539387702941895
INFO:root:current mean train loss 1622.0736970831722
INFO:root:current train perplexity4.151986598968506
INFO:root:current mean train loss 1622.2381660733759
INFO:root:current train perplexity4.152527809143066
INFO:root:current mean train loss 1622.5489203723682
INFO:root:current train perplexity4.153226852416992

100%|██████████| 1/1 [07:52<00:00, 472.95s/it][A100%|██████████| 1/1 [07:52<00:00, 472.95s/it]
INFO:root:final mean train loss: 1621.9829765469872
INFO:root:final train perplexity: 4.153153419494629
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.22s/it][A100%|██████████| 1/1 [00:38<00:00, 38.22s/it]
INFO:root:eval mean loss: 1781.832949807458
INFO:root:eval perplexity: 4.994708061218262
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.59s/it][A100%|██████████| 1/1 [00:36<00:00, 36.59s/it]
INFO:root:eval mean loss: 2197.1002292497783
INFO:root:eval perplexity: 7.526125907897949
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/35
 18%|█▊        | 35/200 [5:37:05<25:09:05, 548.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1605.1714815180353
INFO:root:current train perplexity4.121535778045654
INFO:root:current mean train loss 1600.9977895205782
INFO:root:current train perplexity4.097427845001221
INFO:root:current mean train loss 1611.195079985119
INFO:root:current train perplexity4.101988315582275
INFO:root:current mean train loss 1610.0229160676754
INFO:root:current train perplexity4.106384754180908
INFO:root:current mean train loss 1614.756416351689
INFO:root:current train perplexity4.119084358215332
INFO:root:current mean train loss 1615.2824123395412
INFO:root:current train perplexity4.122089385986328
INFO:root:current mean train loss 1616.3237568528234
INFO:root:current train perplexity4.125615119934082
INFO:root:current mean train loss 1616.3187833925338
INFO:root:current train perplexity4.122076988220215
INFO:root:current mean train loss 1615.690278806409
INFO:root:current train perplexity4.122629165649414
INFO:root:current mean train loss 1615.3067154145576
INFO:root:current train perplexity4.122194766998291
INFO:root:current mean train loss 1615.7755815411836
INFO:root:current train perplexity4.125006198883057
INFO:root:current mean train loss 1616.5020355275728
INFO:root:current train perplexity4.127491474151611
INFO:root:current mean train loss 1616.3549023588437
INFO:root:current train perplexity4.130062580108643
INFO:root:current mean train loss 1615.2646526407818
INFO:root:current train perplexity4.127376556396484
INFO:root:current mean train loss 1615.5647675336763
INFO:root:current train perplexity4.1273884773254395
INFO:root:current mean train loss 1614.823091552428
INFO:root:current train perplexity4.123898983001709
INFO:root:current mean train loss 1615.267272084494
INFO:root:current train perplexity4.125900745391846
INFO:root:current mean train loss 1614.893090697834
INFO:root:current train perplexity4.126328468322754
INFO:root:current mean train loss 1614.4581024911108
INFO:root:current train perplexity4.125894546508789

100%|██████████| 1/1 [07:50<00:00, 470.54s/it][A100%|██████████| 1/1 [07:50<00:00, 470.54s/it]
INFO:root:final mean train loss: 1614.7118121217852
INFO:root:final train perplexity: 4.126728534698486
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.16s/it][A100%|██████████| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 1781.0236773118904
INFO:root:eval perplexity: 4.991060733795166
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.32s/it][A100%|██████████| 1/1 [00:36<00:00, 36.32s/it]
INFO:root:eval mean loss: 2199.3830280709776
INFO:root:eval perplexity: 7.541925430297852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/36
 18%|█▊        | 36/200 [5:46:13<24:59:35, 548.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1566.2367498224432
INFO:root:current train perplexity4.018762111663818
INFO:root:current mean train loss 1601.2577333192567
INFO:root:current train perplexity4.055238723754883
INFO:root:current mean train loss 1597.5877367354117
INFO:root:current train perplexity4.053864002227783
INFO:root:current mean train loss 1597.5324188919312
INFO:root:current train perplexity4.062100410461426
INFO:root:current mean train loss 1601.923095703125
INFO:root:current train perplexity4.082779884338379
INFO:root:current mean train loss 1601.209009457941
INFO:root:current train perplexity4.074537754058838
INFO:root:current mean train loss 1601.9225936365206
INFO:root:current train perplexity4.078569412231445
INFO:root:current mean train loss 1601.9462545531712
INFO:root:current train perplexity4.0824103355407715
INFO:root:current mean train loss 1605.836772424813
INFO:root:current train perplexity4.0881876945495605
INFO:root:current mean train loss 1607.5345747075673
INFO:root:current train perplexity4.08894157409668
INFO:root:current mean train loss 1606.891224364027
INFO:root:current train perplexity4.088893413543701
INFO:root:current mean train loss 1607.2058451572696
INFO:root:current train perplexity4.08951473236084
INFO:root:current mean train loss 1608.5033189819235
INFO:root:current train perplexity4.094144344329834
INFO:root:current mean train loss 1610.2165892344226
INFO:root:current train perplexity4.100141525268555
INFO:root:current mean train loss 1609.7386765294173
INFO:root:current train perplexity4.103081226348877
INFO:root:current mean train loss 1609.821256828182
INFO:root:current train perplexity4.103819847106934
INFO:root:current mean train loss 1609.4700052556157
INFO:root:current train perplexity4.102816581726074
INFO:root:current mean train loss 1608.1596385748467
INFO:root:current train perplexity4.101382732391357
INFO:root:current mean train loss 1608.5361258023881
INFO:root:current train perplexity4.101935386657715
INFO:root:current mean train loss 1609.0792296373177
INFO:root:current train perplexity4.104335784912109

100%|██████████| 1/1 [07:50<00:00, 470.81s/it][A100%|██████████| 1/1 [07:50<00:00, 470.81s/it]
INFO:root:final mean train loss: 1607.918760945084
INFO:root:final train perplexity: 4.102192401885986
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.27s/it][A100%|██████████| 1/1 [00:39<00:00, 39.27s/it]
INFO:root:eval mean loss: 1784.7634974270002
INFO:root:eval perplexity: 5.007937908172607
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.65s/it][A100%|██████████| 1/1 [00:36<00:00, 36.65s/it]
INFO:root:eval mean loss: 2205.3809108869405
INFO:root:eval perplexity: 7.583597183227539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/37
 18%|█▊        | 37/200 [5:55:22<24:50:49, 548.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1559.1080671037946
INFO:root:current train perplexity3.9683384895324707
INFO:root:current mean train loss 1576.5856714248657
INFO:root:current train perplexity4.006835460662842
INFO:root:current mean train loss 1591.6740620930989
INFO:root:current train perplexity4.036264419555664
INFO:root:current mean train loss 1592.3995863751668
INFO:root:current train perplexity4.037120819091797
INFO:root:current mean train loss 1592.40348687573
INFO:root:current train perplexity4.049047946929932
INFO:root:current mean train loss 1592.3048259850705
INFO:root:current train perplexity4.048354148864746
INFO:root:current mean train loss 1592.5196875311008
INFO:root:current train perplexity4.0524115562438965
INFO:root:current mean train loss 1595.4447298154726
INFO:root:current train perplexity4.057221412658691
INFO:root:current mean train loss 1596.8307123598845
INFO:root:current train perplexity4.0603556632995605
INFO:root:current mean train loss 1596.6197455833699
INFO:root:current train perplexity4.061465263366699
INFO:root:current mean train loss 1596.7804782733842
INFO:root:current train perplexity4.062061309814453
INFO:root:current mean train loss 1599.5218967951782
INFO:root:current train perplexity4.06761360168457
INFO:root:current mean train loss 1600.456734523711
INFO:root:current train perplexity4.071105003356934
INFO:root:current mean train loss 1600.8823137398226
INFO:root:current train perplexity4.072502613067627
INFO:root:current mean train loss 1600.5139412332317
INFO:root:current train perplexity4.071657180786133
INFO:root:current mean train loss 1599.0064427240982
INFO:root:current train perplexity4.0710930824279785
INFO:root:current mean train loss 1600.263954856179
INFO:root:current train perplexity4.075965404510498
INFO:root:current mean train loss 1600.407179090712
INFO:root:current train perplexity4.076164722442627
INFO:root:current mean train loss 1600.4952234314098
INFO:root:current train perplexity4.0754594802856445
INFO:root:current mean train loss 1601.4181899074697
INFO:root:current train perplexity4.077175617218018

100%|██████████| 1/1 [07:49<00:00, 469.92s/it][A100%|██████████| 1/1 [07:49<00:00, 469.92s/it]
INFO:root:final mean train loss: 1600.8166299532354
INFO:root:final train perplexity: 4.076695919036865
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.35s/it][A100%|██████████| 1/1 [00:39<00:00, 39.35s/it]
INFO:root:eval mean loss: 1783.73621514503
INFO:root:eval perplexity: 5.003296852111816
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.15s/it][A100%|██████████| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2203.8025369847073
INFO:root:eval perplexity: 7.572606563568115
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/38
 19%|█▉        | 38/200 [6:04:31<24:41:33, 548.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1577.3238226996527
INFO:root:current train perplexity4.020481586456299
INFO:root:current mean train loss 1575.4006170864764
INFO:root:current train perplexity4.000627040863037
INFO:root:current mean train loss 1590.1271100725446
INFO:root:current train perplexity4.025200366973877
INFO:root:current mean train loss 1589.4676453521286
INFO:root:current train perplexity4.031729221343994
INFO:root:current mean train loss 1587.808476617363
INFO:root:current train perplexity4.037234306335449
INFO:root:current mean train loss 1586.9773242635465
INFO:root:current train perplexity4.033077239990234
INFO:root:current mean train loss 1587.541407007025
INFO:root:current train perplexity4.037480354309082
INFO:root:current mean train loss 1587.7634082358954
INFO:root:current train perplexity4.041035175323486
INFO:root:current mean train loss 1587.0717384834966
INFO:root:current train perplexity4.039457321166992
INFO:root:current mean train loss 1588.2865537936095
INFO:root:current train perplexity4.03741455078125
INFO:root:current mean train loss 1590.06047071247
INFO:root:current train perplexity4.036499500274658
INFO:root:current mean train loss 1591.0001086372476
INFO:root:current train perplexity4.040184020996094
INFO:root:current mean train loss 1590.579231476688
INFO:root:current train perplexity4.044005393981934
INFO:root:current mean train loss 1590.8037145678438
INFO:root:current train perplexity4.043487071990967
INFO:root:current mean train loss 1592.00045634867
INFO:root:current train perplexity4.046604633331299
INFO:root:current mean train loss 1592.067109517218
INFO:root:current train perplexity4.045395374298096
INFO:root:current mean train loss 1593.4930529005985
INFO:root:current train perplexity4.047709941864014
INFO:root:current mean train loss 1593.5910075102972
INFO:root:current train perplexity4.048915863037109
INFO:root:current mean train loss 1593.7332012062795
INFO:root:current train perplexity4.0508036613464355
INFO:root:current mean train loss 1593.689327665589
INFO:root:current train perplexity4.051000118255615

100%|██████████| 1/1 [07:53<00:00, 473.33s/it][A100%|██████████| 1/1 [07:53<00:00, 473.33s/it]
INFO:root:final mean train loss: 1593.7131456614622
INFO:root:final train perplexity: 4.0513529777526855
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.48s/it][A100%|██████████| 1/1 [00:40<00:00, 40.48s/it]
INFO:root:eval mean loss: 1784.6396462731327
INFO:root:eval perplexity: 5.007377624511719
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.86s/it][A100%|██████████| 1/1 [00:36<00:00, 36.86s/it]
INFO:root:eval mean loss: 2207.4720913501496
INFO:root:eval perplexity: 7.598178863525391
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/39
 20%|█▉        | 39/200 [6:13:44<24:35:45, 549.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.5980185231854
INFO:root:current train perplexity3.9971160888671875
INFO:root:current mean train loss 1572.3551424756463
INFO:root:current train perplexity3.9969165325164795
INFO:root:current mean train loss 1585.174191074517
INFO:root:current train perplexity4.017358779907227
INFO:root:current mean train loss 1586.5275572044416
INFO:root:current train perplexity4.026615142822266
INFO:root:current mean train loss 1583.5074711258794
INFO:root:current train perplexity4.020498752593994
INFO:root:current mean train loss 1584.4637835628198
INFO:root:current train perplexity4.015145778656006
INFO:root:current mean train loss 1584.9970476317621
INFO:root:current train perplexity4.017795562744141
INFO:root:current mean train loss 1586.5971508276432
INFO:root:current train perplexity4.022734642028809
INFO:root:current mean train loss 1587.3287200573702
INFO:root:current train perplexity4.023051738739014
INFO:root:current mean train loss 1587.8067002643418
INFO:root:current train perplexity4.023860454559326
INFO:root:current mean train loss 1588.555250609662
INFO:root:current train perplexity4.025228023529053
INFO:root:current mean train loss 1589.2373567932443
INFO:root:current train perplexity4.024703025817871
INFO:root:current mean train loss 1587.759371846678
INFO:root:current train perplexity4.022021293640137
INFO:root:current mean train loss 1587.7564329799927
INFO:root:current train perplexity4.023156642913818
INFO:root:current mean train loss 1587.6104559317823
INFO:root:current train perplexity4.023173809051514
INFO:root:current mean train loss 1587.0539045932198
INFO:root:current train perplexity4.024262428283691
INFO:root:current mean train loss 1587.4505527096965
INFO:root:current train perplexity4.023677825927734
INFO:root:current mean train loss 1587.5461014261582
INFO:root:current train perplexity4.025437355041504
INFO:root:current mean train loss 1588.093235757502
INFO:root:current train perplexity4.027090072631836
INFO:root:current mean train loss 1587.9066459507997
INFO:root:current train perplexity4.028472423553467

100%|██████████| 1/1 [07:53<00:00, 473.98s/it][A100%|██████████| 1/1 [07:53<00:00, 473.98s/it]
INFO:root:final mean train loss: 1587.4377501117901
INFO:root:final train perplexity: 4.029096603393555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.22s/it][A100%|██████████| 1/1 [00:40<00:00, 40.22s/it]
INFO:root:eval mean loss: 1784.4677470322197
INFO:root:eval perplexity: 5.006600856781006
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.98s/it][A100%|██████████| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 2209.8103932049257
INFO:root:eval perplexity: 7.614518165588379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/40
 20%|██        | 40/200 [6:22:58<24:30:09, 551.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.7540360462817
INFO:root:current train perplexity3.9307591915130615
INFO:root:current mean train loss 1558.6550790797398
INFO:root:current train perplexity3.9289326667785645
INFO:root:current mean train loss 1558.0446746716789
INFO:root:current train perplexity3.938398838043213
INFO:root:current mean train loss 1562.8165824306357
INFO:root:current train perplexity3.94978404045105
INFO:root:current mean train loss 1569.0591806559075
INFO:root:current train perplexity3.967984199523926
INFO:root:current mean train loss 1570.7286769096097
INFO:root:current train perplexity3.9712448120117188
INFO:root:current mean train loss 1575.0642843120052
INFO:root:current train perplexity3.9828858375549316
INFO:root:current mean train loss 1577.2412764386434
INFO:root:current train perplexity3.9911136627197266
INFO:root:current mean train loss 1578.0085288124822
INFO:root:current train perplexity3.9941916465759277
INFO:root:current mean train loss 1578.756981823369
INFO:root:current train perplexity3.992555856704712
INFO:root:current mean train loss 1580.050114784559
INFO:root:current train perplexity3.994443416595459
INFO:root:current mean train loss 1580.3900102998768
INFO:root:current train perplexity3.998112678527832
INFO:root:current mean train loss 1580.2289197264097
INFO:root:current train perplexity3.996093273162842
INFO:root:current mean train loss 1579.7162522944616
INFO:root:current train perplexity3.9984028339385986
INFO:root:current mean train loss 1581.398424789501
INFO:root:current train perplexity4.001378536224365
INFO:root:current mean train loss 1580.9846587226389
INFO:root:current train perplexity4.003507614135742
INFO:root:current mean train loss 1581.7647058737996
INFO:root:current train perplexity4.005829334259033
INFO:root:current mean train loss 1582.1737614289102
INFO:root:current train perplexity4.008144378662109
INFO:root:current mean train loss 1582.429526515309
INFO:root:current train perplexity4.009642601013184
INFO:root:current mean train loss 1582.0907699176312
INFO:root:current train perplexity4.009152889251709

100%|██████████| 1/1 [07:59<00:00, 479.42s/it][A100%|██████████| 1/1 [07:59<00:00, 479.42s/it]
INFO:root:final mean train loss: 1581.8357932994418
INFO:root:final train perplexity: 4.009330749511719
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.85s/it][A100%|██████████| 1/1 [00:39<00:00, 39.85s/it]
INFO:root:eval mean loss: 1784.3295924409906
INFO:root:eval perplexity: 5.005977153778076
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.84s/it][A100%|██████████| 1/1 [00:37<00:00, 37.84s/it]
INFO:root:eval mean loss: 2209.303402731605
INFO:root:eval perplexity: 7.610973358154297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/41
 20%|██        | 41/200 [6:32:17<24:27:22, 553.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.1730817159016
INFO:root:current train perplexity3.9552574157714844
INFO:root:current mean train loss 1575.457684575295
INFO:root:current train perplexity3.9812543392181396
INFO:root:current mean train loss 1568.9126912709828
INFO:root:current train perplexity3.969651460647583
INFO:root:current mean train loss 1567.0995276865333
INFO:root:current train perplexity3.9641470909118652
INFO:root:current mean train loss 1565.297867805727
INFO:root:current train perplexity3.9615776538848877
INFO:root:current mean train loss 1566.9090910021891
INFO:root:current train perplexity3.9612526893615723
INFO:root:current mean train loss 1570.75244140625
INFO:root:current train perplexity3.965322971343994
INFO:root:current mean train loss 1570.6845299802235
INFO:root:current train perplexity3.963745355606079
INFO:root:current mean train loss 1570.462099484035
INFO:root:current train perplexity3.9657154083251953
INFO:root:current mean train loss 1571.304861413427
INFO:root:current train perplexity3.9692578315734863
INFO:root:current mean train loss 1572.1061408049868
INFO:root:current train perplexity3.97243595123291
INFO:root:current mean train loss 1572.6709915212166
INFO:root:current train perplexity3.9725310802459717
INFO:root:current mean train loss 1572.108851209099
INFO:root:current train perplexity3.9730019569396973
INFO:root:current mean train loss 1573.0692785749463
INFO:root:current train perplexity3.9768869876861572
INFO:root:current mean train loss 1572.3182743500897
INFO:root:current train perplexity3.975584030151367
INFO:root:current mean train loss 1573.0834528031505
INFO:root:current train perplexity3.9765467643737793
INFO:root:current mean train loss 1574.0181809911187
INFO:root:current train perplexity3.9793412685394287
INFO:root:current mean train loss 1574.6922230200141
INFO:root:current train perplexity3.983037233352661
INFO:root:current mean train loss 1574.9104962570254
INFO:root:current train perplexity3.984938859939575

100%|██████████| 1/1 [07:47<00:00, 467.57s/it][A100%|██████████| 1/1 [07:47<00:00, 467.57s/it]
INFO:root:final mean train loss: 1575.3109353700793
INFO:root:final train perplexity: 3.986431360244751
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.97s/it][A100%|██████████| 1/1 [00:40<00:00, 40.97s/it]
INFO:root:eval mean loss: 1786.2964057651818
INFO:root:eval perplexity: 5.014871597290039
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.01s/it][A100%|██████████| 1/1 [00:38<00:00, 38.01s/it]
INFO:root:eval mean loss: 2214.7358627860426
INFO:root:eval perplexity: 7.649051666259766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/42
 21%|██        | 42/200 [6:41:26<24:14:14, 552.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1577.8821833683894
INFO:root:current train perplexity4.064822673797607
INFO:root:current mean train loss 1567.125093983338
INFO:root:current train perplexity3.951120138168335
INFO:root:current mean train loss 1566.560341132079
INFO:root:current train perplexity3.9457461833953857
INFO:root:current mean train loss 1564.997826914437
INFO:root:current train perplexity3.9441139698028564
INFO:root:current mean train loss 1564.0542807960048
INFO:root:current train perplexity3.9440369606018066
INFO:root:current mean train loss 1559.835968909905
INFO:root:current train perplexity3.9423770904541016
INFO:root:current mean train loss 1561.9613445337989
INFO:root:current train perplexity3.949126958847046
INFO:root:current mean train loss 1563.743331502564
INFO:root:current train perplexity3.9505362510681152
INFO:root:current mean train loss 1565.1558063127018
INFO:root:current train perplexity3.9529216289520264
INFO:root:current mean train loss 1562.9562746279864
INFO:root:current train perplexity3.9520821571350098
INFO:root:current mean train loss 1562.8748143037003
INFO:root:current train perplexity3.950796604156494
INFO:root:current mean train loss 1562.7961656102593
INFO:root:current train perplexity3.947500228881836
INFO:root:current mean train loss 1564.6464666632316
INFO:root:current train perplexity3.9484546184539795
INFO:root:current mean train loss 1565.4187102829874
INFO:root:current train perplexity3.9504904747009277
INFO:root:current mean train loss 1565.8252672761025
INFO:root:current train perplexity3.9508743286132812
INFO:root:current mean train loss 1566.613723865819
INFO:root:current train perplexity3.9543163776397705
INFO:root:current mean train loss 1568.066100657984
INFO:root:current train perplexity3.9578943252563477
INFO:root:current mean train loss 1568.7161001549503
INFO:root:current train perplexity3.960467576980591
INFO:root:current mean train loss 1569.3387676729265
INFO:root:current train perplexity3.9619522094726562
INFO:root:current mean train loss 1568.5435881627147
INFO:root:current train perplexity3.962432622909546

100%|██████████| 1/1 [08:02<00:00, 482.74s/it][A100%|██████████| 1/1 [08:02<00:00, 482.74s/it]
INFO:root:final mean train loss: 1568.726739080278
INFO:root:final train perplexity: 3.963456869125366
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.04s/it][A100%|██████████| 1/1 [00:44<00:00, 44.06s/it]
INFO:root:eval mean loss: 1788.9203776907414
INFO:root:eval perplexity: 5.026763916015625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.91s/it][A100%|██████████| 1/1 [00:41<00:00, 41.93s/it]
INFO:root:eval mean loss: 2220.7940617554577
INFO:root:eval perplexity: 7.6917405128479
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/43
 22%|██▏       | 43/200 [6:50:57<24:19:53, 557.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1545.1915730794271
INFO:root:current train perplexity3.9317400455474854
INFO:root:current mean train loss 1563.6718637319711
INFO:root:current train perplexity3.9340293407440186
INFO:root:current mean train loss 1558.4233727496603
INFO:root:current train perplexity3.9274795055389404
INFO:root:current mean train loss 1561.4066535718514
INFO:root:current train perplexity3.9350247383117676
INFO:root:current mean train loss 1556.7969437000363
INFO:root:current train perplexity3.9274556636810303
INFO:root:current mean train loss 1560.327944428066
INFO:root:current train perplexity3.928145408630371
INFO:root:current mean train loss 1561.5887156653025
INFO:root:current train perplexity3.92757511138916
INFO:root:current mean train loss 1561.1838745117188
INFO:root:current train perplexity3.9253344535827637
INFO:root:current mean train loss 1562.2301657803087
INFO:root:current train perplexity3.928964376449585
INFO:root:current mean train loss 1563.1713672925068
INFO:root:current train perplexity3.9340856075286865
INFO:root:current mean train loss 1564.2125323545586
INFO:root:current train perplexity3.9393844604492188
INFO:root:current mean train loss 1562.5816418132952
INFO:root:current train perplexity3.9369986057281494
INFO:root:current mean train loss 1562.3921351983295
INFO:root:current train perplexity3.9379961490631104
INFO:root:current mean train loss 1562.7159815738075
INFO:root:current train perplexity3.940227508544922
INFO:root:current mean train loss 1563.4618234914499
INFO:root:current train perplexity3.942225694656372
INFO:root:current mean train loss 1563.6119267482384
INFO:root:current train perplexity3.943617105484009
INFO:root:current mean train loss 1562.7408027882957
INFO:root:current train perplexity3.9429872035980225
INFO:root:current mean train loss 1562.9968113540915
INFO:root:current train perplexity3.942126512527466
INFO:root:current mean train loss 1562.1278307638534
INFO:root:current train perplexity3.9408326148986816
INFO:root:current mean train loss 1562.5217923337314
INFO:root:current train perplexity3.941136598587036

100%|██████████| 1/1 [08:19<00:00, 499.23s/it][A100%|██████████| 1/1 [08:19<00:00, 499.23s/it]
INFO:root:final mean train loss: 1562.8513710841469
INFO:root:final train perplexity: 3.9430675506591797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.47s/it][A100%|██████████| 1/1 [00:43<00:00, 43.49s/it]
INFO:root:eval mean loss: 1788.263064120678
INFO:root:eval perplexity: 5.023782730102539
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.65s/it][A100%|██████████| 1/1 [00:41<00:00, 41.65s/it]
INFO:root:eval mean loss: 2221.203929711741
INFO:root:eval perplexity: 7.694636344909668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/44
 22%|██▏       | 44/200 [7:00:44<24:33:16, 566.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1539.6595199260305
INFO:root:current train perplexity3.8887245655059814
INFO:root:current mean train loss 1544.9400875584608
INFO:root:current train perplexity3.8947296142578125
INFO:root:current mean train loss 1546.55746546448
INFO:root:current train perplexity3.9082210063934326
INFO:root:current mean train loss 1552.4836253405304
INFO:root:current train perplexity3.921647787094116
INFO:root:current mean train loss 1551.9184196182011
INFO:root:current train perplexity3.918941020965576
INFO:root:current mean train loss 1552.9641648873114
INFO:root:current train perplexity3.9163873195648193
INFO:root:current mean train loss 1554.6933486207377
INFO:root:current train perplexity3.9167842864990234
INFO:root:current mean train loss 1554.8291721573796
INFO:root:current train perplexity3.918417453765869
INFO:root:current mean train loss 1556.0689433524572
INFO:root:current train perplexity3.9155611991882324
INFO:root:current mean train loss 1555.2656420150806
INFO:root:current train perplexity3.9168291091918945
INFO:root:current mean train loss 1556.0407581930515
INFO:root:current train perplexity3.918818235397339
INFO:root:current mean train loss 1555.879494358585
INFO:root:current train perplexity3.917452096939087
INFO:root:current mean train loss 1555.3956505378533
INFO:root:current train perplexity3.914781332015991
INFO:root:current mean train loss 1555.9562061199367
INFO:root:current train perplexity3.913900136947632
INFO:root:current mean train loss 1556.0825021528917
INFO:root:current train perplexity3.9134774208068848
INFO:root:current mean train loss 1556.1870043803278
INFO:root:current train perplexity3.9141530990600586
INFO:root:current mean train loss 1556.3145604460763
INFO:root:current train perplexity3.917536497116089
INFO:root:current mean train loss 1556.541407270164
INFO:root:current train perplexity3.917126417160034
INFO:root:current mean train loss 1556.8299582356947
INFO:root:current train perplexity3.9189224243164062
INFO:root:current mean train loss 1556.825337571123
INFO:root:current train perplexity3.919780731201172

100%|██████████| 1/1 [08:29<00:00, 509.18s/it][A100%|██████████| 1/1 [08:29<00:00, 509.18s/it]
INFO:root:final mean train loss: 1556.3362676377135
INFO:root:final train perplexity: 3.920579433441162
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.71s/it][A100%|██████████| 1/1 [00:44<00:00, 44.71s/it]
INFO:root:eval mean loss: 1791.2119036735373
INFO:root:eval perplexity: 5.037172794342041
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.49s/it][A100%|██████████| 1/1 [00:41<00:00, 41.49s/it]
INFO:root:eval mean loss: 2226.3588715681794
INFO:root:eval perplexity: 7.731160640716553
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/45
 22%|██▎       | 45/200 [7:10:42<24:48:03, 576.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1542.7456951141357
INFO:root:current train perplexity3.8350577354431152
INFO:root:current mean train loss 1540.8764596334318
INFO:root:current train perplexity3.8433282375335693
INFO:root:current mean train loss 1543.4742542613637
INFO:root:current train perplexity3.858339548110962
INFO:root:current mean train loss 1545.6720604529748
INFO:root:current train perplexity3.8635730743408203
INFO:root:current mean train loss 1547.5245224525188
INFO:root:current train perplexity3.873544931411743
INFO:root:current mean train loss 1548.6681211214539
INFO:root:current train perplexity3.8738603591918945
INFO:root:current mean train loss 1550.8394643025226
INFO:root:current train perplexity3.8775877952575684
INFO:root:current mean train loss 1550.4848751048144
INFO:root:current train perplexity3.883920192718506
INFO:root:current mean train loss 1550.0891189575195
INFO:root:current train perplexity3.889371395111084
INFO:root:current mean train loss 1549.563375892481
INFO:root:current train perplexity3.890761137008667
INFO:root:current mean train loss 1550.2247808929674
INFO:root:current train perplexity3.8917644023895264
INFO:root:current mean train loss 1550.7051709361913
INFO:root:current train perplexity3.89457368850708
INFO:root:current mean train loss 1550.1835631358472
INFO:root:current train perplexity3.8973443508148193
INFO:root:current mean train loss 1550.2035942189505
INFO:root:current train perplexity3.8960776329040527
INFO:root:current mean train loss 1550.9769646483041
INFO:root:current train perplexity3.8969593048095703
INFO:root:current mean train loss 1551.6762742923045
INFO:root:current train perplexity3.898587942123413
INFO:root:current mean train loss 1551.2044840592605
INFO:root:current train perplexity3.901263952255249
INFO:root:current mean train loss 1550.9969932919457
INFO:root:current train perplexity3.9004693031311035
INFO:root:current mean train loss 1550.8585746666904
INFO:root:current train perplexity3.900885820388794
INFO:root:current mean train loss 1551.106144052416
INFO:root:current train perplexity3.900437593460083

100%|██████████| 1/1 [08:24<00:00, 504.46s/it][A100%|██████████| 1/1 [08:24<00:00, 504.46s/it]
INFO:root:final mean train loss: 1550.5606022467832
INFO:root:final train perplexity: 3.9007515907287598
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.06s/it][A100%|██████████| 1/1 [00:39<00:00, 39.06s/it]
INFO:root:eval mean loss: 1791.6081231299868
INFO:root:eval perplexity: 5.038974285125732
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.39s/it][A100%|██████████| 1/1 [00:37<00:00, 37.39s/it]
INFO:root:eval mean loss: 2228.021331570673
INFO:root:eval perplexity: 7.742977619171143
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/46
 23%|██▎       | 46/200 [7:20:25<24:44:03, 578.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1554.8319769965278
INFO:root:current train perplexity3.8616690635681152
INFO:root:current mean train loss 1540.704089422911
INFO:root:current train perplexity3.8534090518951416
INFO:root:current mean train loss 1542.5267208004336
INFO:root:current train perplexity3.8586113452911377
INFO:root:current mean train loss 1544.5669877660557
INFO:root:current train perplexity3.856196641921997
INFO:root:current mean train loss 1540.236356295072
INFO:root:current train perplexity3.8591959476470947
INFO:root:current mean train loss 1540.7068883794104
INFO:root:current train perplexity3.86671781539917
INFO:root:current mean train loss 1542.0428006120364
INFO:root:current train perplexity3.870473623275757
INFO:root:current mean train loss 1541.6657860202765
INFO:root:current train perplexity3.8739798069000244
INFO:root:current mean train loss 1541.271026161012
INFO:root:current train perplexity3.8713839054107666
INFO:root:current mean train loss 1541.3076293820877
INFO:root:current train perplexity3.8749401569366455
INFO:root:current mean train loss 1541.3913211734289
INFO:root:current train perplexity3.8753323554992676
INFO:root:current mean train loss 1542.692111280992
INFO:root:current train perplexity3.875405788421631
INFO:root:current mean train loss 1544.4044988961261
INFO:root:current train perplexity3.8786890506744385
INFO:root:current mean train loss 1545.2015590350063
INFO:root:current train perplexity3.8813745975494385
INFO:root:current mean train loss 1545.1309373483393
INFO:root:current train perplexity3.8810877799987793
INFO:root:current mean train loss 1545.5672681544265
INFO:root:current train perplexity3.8826591968536377
INFO:root:current mean train loss 1546.0173812584817
INFO:root:current train perplexity3.8842484951019287
INFO:root:current mean train loss 1545.5795422082265
INFO:root:current train perplexity3.882935047149658
INFO:root:current mean train loss 1545.403917684763
INFO:root:current train perplexity3.8827967643737793
INFO:root:current mean train loss 1545.9535592400264
INFO:root:current train perplexity3.8833818435668945

100%|██████████| 1/1 [08:18<00:00, 498.32s/it][A100%|██████████| 1/1 [08:18<00:00, 498.32s/it]
INFO:root:final mean train loss: 1545.3598472760652
INFO:root:final train perplexity: 3.8829832077026367
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.63s/it][A100%|██████████| 1/1 [00:43<00:00, 43.63s/it]
INFO:root:eval mean loss: 1795.3502431017287
INFO:root:eval perplexity: 5.056023120880127
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.22s/it][A100%|██████████| 1/1 [00:39<00:00, 39.22s/it]
INFO:root:eval mean loss: 2234.9187998670213
INFO:root:eval perplexity: 7.792196273803711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/47
 24%|██▎       | 47/200 [7:30:09<24:38:21, 579.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1517.007332938058
INFO:root:current train perplexity3.8343091011047363
INFO:root:current mean train loss 1521.9662062519728
INFO:root:current train perplexity3.8259668350219727
INFO:root:current mean train loss 1524.7669280391412
INFO:root:current train perplexity3.827108144760132
INFO:root:current mean train loss 1528.6028058996153
INFO:root:current train perplexity3.83054780960083
INFO:root:current mean train loss 1527.7398877737512
INFO:root:current train perplexity3.8319780826568604
INFO:root:current mean train loss 1531.080858313519
INFO:root:current train perplexity3.8311679363250732
INFO:root:current mean train loss 1532.0963725879747
INFO:root:current train perplexity3.8350560665130615
INFO:root:current mean train loss 1533.0188407419917
INFO:root:current train perplexity3.8404431343078613
INFO:root:current mean train loss 1533.5327353700497
INFO:root:current train perplexity3.844852924346924
INFO:root:current mean train loss 1534.8728845630715
INFO:root:current train perplexity3.847630500793457
INFO:root:current mean train loss 1535.0289259947062
INFO:root:current train perplexity3.847174644470215
INFO:root:current mean train loss 1534.5564060625131
INFO:root:current train perplexity3.8497915267944336
INFO:root:current mean train loss 1535.5043663177726
INFO:root:current train perplexity3.8511106967926025
INFO:root:current mean train loss 1535.877283273678
INFO:root:current train perplexity3.852389097213745
INFO:root:current mean train loss 1536.2495680275524
INFO:root:current train perplexity3.85534405708313
INFO:root:current mean train loss 1537.7388221635688
INFO:root:current train perplexity3.8561851978302
INFO:root:current mean train loss 1538.7230962926283
INFO:root:current train perplexity3.860578775405884
INFO:root:current mean train loss 1539.2354375141215
INFO:root:current train perplexity3.8602187633514404
INFO:root:current mean train loss 1539.9792617460196
INFO:root:current train perplexity3.862110137939453

100%|██████████| 1/1 [08:06<00:00, 486.13s/it][A100%|██████████| 1/1 [08:06<00:00, 486.13s/it]
INFO:root:final mean train loss: 1539.318212619768
INFO:root:final train perplexity: 3.8624439239501953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.53s/it][A100%|██████████| 1/1 [00:44<00:00, 44.53s/it]
INFO:root:eval mean loss: 1795.2684391102891
INFO:root:eval perplexity: 5.05565071105957
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.04s/it][A100%|██████████| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 2234.486346305685
INFO:root:eval perplexity: 7.789101600646973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/48
 24%|██▍       | 48/200 [7:39:43<24:24:24, 578.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.8255615234375
INFO:root:current train perplexity3.816089153289795
INFO:root:current mean train loss 1516.0178434952445
INFO:root:current train perplexity3.810336112976074
INFO:root:current mean train loss 1527.651601108285
INFO:root:current train perplexity3.8209922313690186
INFO:root:current mean train loss 1527.9457643539185
INFO:root:current train perplexity3.828824758529663
INFO:root:current mean train loss 1526.3055522872742
INFO:root:current train perplexity3.8289713859558105
INFO:root:current mean train loss 1525.4183830779732
INFO:root:current train perplexity3.8271875381469727
INFO:root:current mean train loss 1528.9303617647993
INFO:root:current train perplexity3.8316946029663086
INFO:root:current mean train loss 1529.740657950448
INFO:root:current train perplexity3.824603796005249
INFO:root:current mean train loss 1528.2083157592024
INFO:root:current train perplexity3.8258471488952637
INFO:root:current mean train loss 1530.272756174223
INFO:root:current train perplexity3.8282740116119385
INFO:root:current mean train loss 1529.3412545941733
INFO:root:current train perplexity3.8259284496307373
INFO:root:current mean train loss 1532.0639889293723
INFO:root:current train perplexity3.8299989700317383
INFO:root:current mean train loss 1532.8491168740354
INFO:root:current train perplexity3.831934690475464
INFO:root:current mean train loss 1531.2814883852186
INFO:root:current train perplexity3.8325400352478027
INFO:root:current mean train loss 1532.438724930295
INFO:root:current train perplexity3.8350038528442383
INFO:root:current mean train loss 1533.6489970896503
INFO:root:current train perplexity3.837646245956421
INFO:root:current mean train loss 1533.1453082672213
INFO:root:current train perplexity3.839876174926758
INFO:root:current mean train loss 1532.6723890476949
INFO:root:current train perplexity3.83834171295166
INFO:root:current mean train loss 1532.3140771618887
INFO:root:current train perplexity3.8384177684783936
INFO:root:current mean train loss 1533.6045289042102
INFO:root:current train perplexity3.8408496379852295

100%|██████████| 1/1 [08:16<00:00, 496.36s/it][A100%|██████████| 1/1 [08:16<00:00, 496.37s/it]
INFO:root:final mean train loss: 1533.4786976224175
INFO:root:final train perplexity: 3.8426942825317383
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.66s/it][A100%|██████████| 1/1 [00:44<00:00, 44.66s/it]
INFO:root:eval mean loss: 1796.5268784110428
INFO:root:eval perplexity: 5.061396598815918
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.61s/it][A100%|██████████| 1/1 [00:42<00:00, 42.61s/it]
INFO:root:eval mean loss: 2239.8252130603114
INFO:root:eval perplexity: 7.827397346496582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/49
 24%|██▍       | 49/200 [7:49:29<24:20:48, 580.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1544.7800598144531
INFO:root:current train perplexity3.7924020290374756
INFO:root:current mean train loss 1523.897627397017
INFO:root:current train perplexity3.7929928302764893
INFO:root:current mean train loss 1519.0568016315328
INFO:root:current train perplexity3.807352066040039
INFO:root:current mean train loss 1520.4807242887566
INFO:root:current train perplexity3.7993125915527344
INFO:root:current mean train loss 1521.2168183503327
INFO:root:current train perplexity3.799387216567993
INFO:root:current mean train loss 1522.9266045362428
INFO:root:current train perplexity3.8065474033355713
INFO:root:current mean train loss 1529.1743650798555
INFO:root:current train perplexity3.813770294189453
INFO:root:current mean train loss 1527.5341036436987
INFO:root:current train perplexity3.811976671218872
INFO:root:current mean train loss 1526.6784019470215
INFO:root:current train perplexity3.811142921447754
INFO:root:current mean train loss 1527.1847710998272
INFO:root:current train perplexity3.814213991165161
INFO:root:current mean train loss 1526.9170057902963
INFO:root:current train perplexity3.818398952484131
INFO:root:current mean train loss 1526.7874859381902
INFO:root:current train perplexity3.817138910293579
INFO:root:current mean train loss 1525.8988405698306
INFO:root:current train perplexity3.816120147705078
INFO:root:current mean train loss 1526.961605587521
INFO:root:current train perplexity3.8184244632720947
INFO:root:current mean train loss 1527.1879758355337
INFO:root:current train perplexity3.819192409515381
INFO:root:current mean train loss 1528.4140981171213
INFO:root:current train perplexity3.820944309234619
INFO:root:current mean train loss 1528.5647101308784
INFO:root:current train perplexity3.8224501609802246
INFO:root:current mean train loss 1528.6750519996979
INFO:root:current train perplexity3.8231632709503174
INFO:root:current mean train loss 1528.7968733341934
INFO:root:current train perplexity3.8232808113098145
INFO:root:current mean train loss 1528.6081573928611
INFO:root:current train perplexity3.8241541385650635

100%|██████████| 1/1 [08:16<00:00, 496.33s/it][A100%|██████████| 1/1 [08:16<00:00, 496.33s/it]
INFO:root:final mean train loss: 1528.027163353101
INFO:root:final train perplexity: 3.8243489265441895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.90s/it][A100%|██████████| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 1798.5498661555298
INFO:root:eval perplexity: 5.070647239685059
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.82s/it][A100%|██████████| 1/1 [00:40<00:00, 40.82s/it]
INFO:root:eval mean loss: 2243.8079310207504
INFO:root:eval perplexity: 7.856090545654297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/50
 25%|██▌       | 50/200 [7:59:12<24:13:25, 581.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1514.1233981385524
INFO:root:current train perplexity3.7624523639678955
INFO:root:current mean train loss 1522.3371491912228
INFO:root:current train perplexity3.807772397994995
INFO:root:current mean train loss 1518.7157232445406
INFO:root:current train perplexity3.788454055786133
INFO:root:current mean train loss 1515.1452720663951
INFO:root:current train perplexity3.7849597930908203
INFO:root:current mean train loss 1519.5750653579134
INFO:root:current train perplexity3.7942306995391846
INFO:root:current mean train loss 1520.0128116017047
INFO:root:current train perplexity3.7954978942871094
INFO:root:current mean train loss 1521.9191497661668
INFO:root:current train perplexity3.7990663051605225
INFO:root:current mean train loss 1520.7123288407981
INFO:root:current train perplexity3.800250291824341
INFO:root:current mean train loss 1522.397984876509
INFO:root:current train perplexity3.8024375438690186
INFO:root:current mean train loss 1523.3066359943032
INFO:root:current train perplexity3.799663543701172
INFO:root:current mean train loss 1522.1079800935788
INFO:root:current train perplexity3.800243854522705
INFO:root:current mean train loss 1520.822029452411
INFO:root:current train perplexity3.7990126609802246
INFO:root:current mean train loss 1520.4743914272042
INFO:root:current train perplexity3.7989416122436523
INFO:root:current mean train loss 1521.9218296647691
INFO:root:current train perplexity3.799607038497925
INFO:root:current mean train loss 1522.235938915308
INFO:root:current train perplexity3.8005940914154053
INFO:root:current mean train loss 1522.9158167189516
INFO:root:current train perplexity3.8015949726104736
INFO:root:current mean train loss 1522.2715397471004
INFO:root:current train perplexity3.80122971534729
INFO:root:current mean train loss 1521.9737497180308
INFO:root:current train perplexity3.8023247718811035
INFO:root:current mean train loss 1522.0097998891927
INFO:root:current train perplexity3.8052730560302734
INFO:root:current mean train loss 1523.0664446435871
INFO:root:current train perplexity3.8058269023895264

100%|██████████| 1/1 [08:23<00:00, 503.28s/it][A100%|██████████| 1/1 [08:23<00:00, 503.28s/it]
INFO:root:final mean train loss: 1522.7297328808545
INFO:root:final train perplexity: 3.806605100631714
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.54s/it][A100%|██████████| 1/1 [00:44<00:00, 44.55s/it]
INFO:root:eval mean loss: 1799.0440132736314
INFO:root:eval perplexity: 5.072910308837891
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 42.00s/it][A100%|██████████| 1/1 [00:41<00:00, 42.00s/it]
INFO:root:eval mean loss: 2245.042954465176
INFO:root:eval perplexity: 7.865007400512695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/51
 26%|██▌       | 51/200 [8:09:05<24:11:51, 584.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1526.774893095999
INFO:root:current train perplexity3.7858808040618896
INFO:root:current mean train loss 1510.4754190100246
INFO:root:current train perplexity3.784207582473755
INFO:root:current mean train loss 1503.0445519927748
INFO:root:current train perplexity3.7745981216430664
INFO:root:current mean train loss 1503.0276519275103
INFO:root:current train perplexity3.780996322631836
INFO:root:current mean train loss 1506.4882110464725
INFO:root:current train perplexity3.7799596786499023
INFO:root:current mean train loss 1508.2425642788621
INFO:root:current train perplexity3.785449743270874
INFO:root:current mean train loss 1509.3878976632882
INFO:root:current train perplexity3.787322998046875
INFO:root:current mean train loss 1510.282246004508
INFO:root:current train perplexity3.781371593475342
INFO:root:current mean train loss 1510.532621388072
INFO:root:current train perplexity3.78208589553833
INFO:root:current mean train loss 1512.8801476772774
INFO:root:current train perplexity3.7837584018707275
INFO:root:current mean train loss 1512.777666446192
INFO:root:current train perplexity3.7852964401245117
INFO:root:current mean train loss 1513.9200280322013
INFO:root:current train perplexity3.783694267272949
INFO:root:current mean train loss 1512.8721071457223
INFO:root:current train perplexity3.780879497528076
INFO:root:current mean train loss 1512.469928791722
INFO:root:current train perplexity3.780813217163086
INFO:root:current mean train loss 1512.9815857849537
INFO:root:current train perplexity3.7828121185302734
INFO:root:current mean train loss 1514.3515253176633
INFO:root:current train perplexity3.785349130630493
INFO:root:current mean train loss 1515.2219054369796
INFO:root:current train perplexity3.786311388015747
INFO:root:current mean train loss 1515.609263781918
INFO:root:current train perplexity3.787214756011963
INFO:root:current mean train loss 1516.4244465229979
INFO:root:current train perplexity3.7868552207946777
INFO:root:current mean train loss 1517.295793752583
INFO:root:current train perplexity3.7869086265563965

100%|██████████| 1/1 [08:20<00:00, 500.39s/it][A100%|██████████| 1/1 [08:20<00:00, 500.39s/it]
INFO:root:final mean train loss: 1516.9126260654527
INFO:root:final train perplexity: 3.7872159481048584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.58s/it][A100%|██████████| 1/1 [00:40<00:00, 40.58s/it]
INFO:root:eval mean loss: 1800.9615080272051
INFO:root:eval perplexity: 5.081697463989258
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.04s/it][A100%|██████████| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 2248.757770944149
INFO:root:eval perplexity: 7.891892433166504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/52
 26%|██▌       | 52/200 [8:18:49<24:02:00, 584.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1496.9062691194465
INFO:root:current train perplexity3.7117207050323486
INFO:root:current mean train loss 1502.4752997726691
INFO:root:current train perplexity3.7295079231262207
INFO:root:current mean train loss 1503.2811628685402
INFO:root:current train perplexity3.739553213119507
INFO:root:current mean train loss 1503.9627127784352
INFO:root:current train perplexity3.7447476387023926
INFO:root:current mean train loss 1501.2438969898421
INFO:root:current train perplexity3.7465081214904785
INFO:root:current mean train loss 1502.3016409767636
INFO:root:current train perplexity3.745765447616577
INFO:root:current mean train loss 1504.7554379375229
INFO:root:current train perplexity3.747685194015503
INFO:root:current mean train loss 1507.0282054136235
INFO:root:current train perplexity3.7526354789733887
INFO:root:current mean train loss 1507.0258988135263
INFO:root:current train perplexity3.7568531036376953
INFO:root:current mean train loss 1508.0311379883806
INFO:root:current train perplexity3.758444309234619
INFO:root:current mean train loss 1508.8377626935091
INFO:root:current train perplexity3.7604739665985107
INFO:root:current mean train loss 1509.397131357975
INFO:root:current train perplexity3.7608630657196045
INFO:root:current mean train loss 1509.7973266506417
INFO:root:current train perplexity3.7626376152038574
INFO:root:current mean train loss 1509.5198150215508
INFO:root:current train perplexity3.7620842456817627
INFO:root:current mean train loss 1510.3212601706055
INFO:root:current train perplexity3.762132167816162
INFO:root:current mean train loss 1510.877619075233
INFO:root:current train perplexity3.764930009841919
INFO:root:current mean train loss 1512.3342671023192
INFO:root:current train perplexity3.7678420543670654
INFO:root:current mean train loss 1511.9461474116438
INFO:root:current train perplexity3.7675609588623047
INFO:root:current mean train loss 1511.9808435830041
INFO:root:current train perplexity3.766749858856201
INFO:root:current mean train loss 1511.4376153604467
INFO:root:current train perplexity3.769057273864746

100%|██████████| 1/1 [08:25<00:00, 505.56s/it][A100%|██████████| 1/1 [08:25<00:00, 505.56s/it]
INFO:root:final mean train loss: 1511.4376153604467
INFO:root:final train perplexity: 3.769057273864746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.77s/it][A100%|██████████| 1/1 [00:40<00:00, 40.77s/it]
INFO:root:eval mean loss: 1804.4842936197917
INFO:root:eval perplexity: 5.097882270812988
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.90s/it][A100%|██████████| 1/1 [00:36<00:00, 36.90s/it]
INFO:root:eval mean loss: 2255.627986826795
INFO:root:eval perplexity: 7.941858768463135
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/53
 26%|██▋       | 53/200 [8:28:35<23:52:56, 584.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1502.1294885253906
INFO:root:current train perplexity3.709623336791992
INFO:root:current mean train loss 1506.0187237548828
INFO:root:current train perplexity3.7237095832824707
INFO:root:current mean train loss 1504.6980859375
INFO:root:current train perplexity3.72748064994812
INFO:root:current mean train loss 1503.6571002197265
INFO:root:current train perplexity3.7314393520355225
INFO:root:current mean train loss 1503.900940673828
INFO:root:current train perplexity3.7344653606414795
INFO:root:current mean train loss 1501.0143583170573
INFO:root:current train perplexity3.732318878173828
INFO:root:current mean train loss 1505.089844970703
INFO:root:current train perplexity3.7409019470214844
INFO:root:current mean train loss 1503.5795040893554
INFO:root:current train perplexity3.7405242919921875
INFO:root:current mean train loss 1504.1444239637588
INFO:root:current train perplexity3.737746477127075
INFO:root:current mean train loss 1505.7307325439454
INFO:root:current train perplexity3.742516279220581
INFO:root:current mean train loss 1505.412373490767
INFO:root:current train perplexity3.7440409660339355
INFO:root:current mean train loss 1504.5984508260092
INFO:root:current train perplexity3.7460711002349854
INFO:root:current mean train loss 1504.5264338566706
INFO:root:current train perplexity3.745983362197876
INFO:root:current mean train loss 1504.8191239711216
INFO:root:current train perplexity3.746403694152832
INFO:root:current mean train loss 1506.6899356282552
INFO:root:current train perplexity3.748809814453125
INFO:root:current mean train loss 1506.5895297241211
INFO:root:current train perplexity3.7482213973999023
INFO:root:current mean train loss 1506.8309338378906
INFO:root:current train perplexity3.7500789165496826
INFO:root:current mean train loss 1506.9853594292535
INFO:root:current train perplexity3.7516844272613525
INFO:root:current mean train loss 1506.7937057976974
INFO:root:current train perplexity3.7529566287994385

100%|██████████| 1/1 [08:27<00:00, 507.40s/it][A100%|██████████| 1/1 [08:27<00:00, 507.41s/it]
INFO:root:final mean train loss: 1507.087980838839
INFO:root:final train perplexity: 3.7546935081481934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.33s/it][A100%|██████████| 1/1 [00:44<00:00, 44.33s/it]
INFO:root:eval mean loss: 1807.1568400065105
INFO:root:eval perplexity: 5.110194683074951
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.38s/it][A100%|██████████| 1/1 [00:37<00:00, 37.38s/it]
INFO:root:eval mean loss: 2259.6401613925364
INFO:root:eval perplexity: 7.971184253692627
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/54
 27%|██▋       | 54/200 [8:38:26<23:47:55, 586.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1493.888614430147
INFO:root:current train perplexity3.818589210510254
INFO:root:current mean train loss 1502.9683264055823
INFO:root:current train perplexity3.7353806495666504
INFO:root:current mean train loss 1506.749255202333
INFO:root:current train perplexity3.7259981632232666
INFO:root:current mean train loss 1500.6220914918918
INFO:root:current train perplexity3.726443290710449
INFO:root:current mean train loss 1500.3779680357277
INFO:root:current train perplexity3.728619337081909
INFO:root:current mean train loss 1501.9620245632857
INFO:root:current train perplexity3.7323200702667236
INFO:root:current mean train loss 1501.0735729841977
INFO:root:current train perplexity3.7291038036346436
INFO:root:current mean train loss 1500.9022559002353
INFO:root:current train perplexity3.726515769958496
INFO:root:current mean train loss 1500.630347486517
INFO:root:current train perplexity3.727489948272705
INFO:root:current mean train loss 1499.345238672088
INFO:root:current train perplexity3.728113889694214
INFO:root:current mean train loss 1498.782253809266
INFO:root:current train perplexity3.727890968322754
INFO:root:current mean train loss 1500.6027578492194
INFO:root:current train perplexity3.732693910598755
INFO:root:current mean train loss 1500.1557070529157
INFO:root:current train perplexity3.7307255268096924
INFO:root:current mean train loss 1501.162705359897
INFO:root:current train perplexity3.730491876602173
INFO:root:current mean train loss 1501.2112140319007
INFO:root:current train perplexity3.730043649673462
INFO:root:current mean train loss 1501.817556382483
INFO:root:current train perplexity3.73044490814209
INFO:root:current mean train loss 1502.5401126670483
INFO:root:current train perplexity3.732536792755127
INFO:root:current mean train loss 1502.4931632804537
INFO:root:current train perplexity3.7345290184020996
INFO:root:current mean train loss 1502.3952585660172
INFO:root:current train perplexity3.7345926761627197
INFO:root:current mean train loss 1501.7156034259667
INFO:root:current train perplexity3.7337276935577393

100%|██████████| 1/1 [08:17<00:00, 497.93s/it][A100%|██████████| 1/1 [08:17<00:00, 497.94s/it]
INFO:root:final mean train loss: 1501.2054593254086
INFO:root:final train perplexity: 3.735353708267212
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.32s/it][A100%|██████████| 1/1 [00:43<00:00, 43.34s/it]
INFO:root:eval mean loss: 1805.3578309404088
INFO:root:eval perplexity: 5.101904392242432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.38s/it][A100%|██████████| 1/1 [00:41<00:00, 41.39s/it]
INFO:root:eval mean loss: 2260.597177491966
INFO:root:eval perplexity: 7.978198528289795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/55
 28%|██▊       | 55/200 [8:48:12<23:37:30, 586.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1503.130719353171
INFO:root:current train perplexity3.7557249069213867
INFO:root:current mean train loss 1487.1496199422809
INFO:root:current train perplexity3.707247734069824
INFO:root:current mean train loss 1485.2742919921875
INFO:root:current train perplexity3.70121431350708
INFO:root:current mean train loss 1487.3053493842394
INFO:root:current train perplexity3.711369752883911
INFO:root:current mean train loss 1484.4583073629212
INFO:root:current train perplexity3.6966845989227295
INFO:root:current mean train loss 1486.006166836742
INFO:root:current train perplexity3.698228597640991
INFO:root:current mean train loss 1488.6971820626725
INFO:root:current train perplexity3.7061939239501953
INFO:root:current mean train loss 1489.0682785491529
INFO:root:current train perplexity3.7096569538116455
INFO:root:current mean train loss 1490.1705868215583
INFO:root:current train perplexity3.709960460662842
INFO:root:current mean train loss 1491.267361561287
INFO:root:current train perplexity3.7116479873657227
INFO:root:current mean train loss 1492.2304263677543
INFO:root:current train perplexity3.7104742527008057
INFO:root:current mean train loss 1492.399980064002
INFO:root:current train perplexity3.7108237743377686
INFO:root:current mean train loss 1494.3268393048204
INFO:root:current train perplexity3.711170196533203
INFO:root:current mean train loss 1496.0673812568814
INFO:root:current train perplexity3.7149388790130615
INFO:root:current mean train loss 1495.2579846242481
INFO:root:current train perplexity3.7149171829223633
INFO:root:current mean train loss 1495.0207220323705
INFO:root:current train perplexity3.7161576747894287
INFO:root:current mean train loss 1495.7935686426617
INFO:root:current train perplexity3.7163407802581787
INFO:root:current mean train loss 1496.6526596966912
INFO:root:current train perplexity3.7191145420074463
INFO:root:current mean train loss 1496.6863539101903
INFO:root:current train perplexity3.7193069458007812
INFO:root:current mean train loss 1496.8088869964695
INFO:root:current train perplexity3.72070574760437

100%|██████████| 1/1 [08:21<00:00, 501.35s/it][A100%|██████████| 1/1 [08:21<00:00, 501.35s/it]
INFO:root:final mean train loss: 1496.568231887548
INFO:root:final train perplexity: 3.720179319381714
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.27s/it][A100%|██████████| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 1811.0279614604112
INFO:root:eval perplexity: 5.128082752227783
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.83s/it][A100%|██████████| 1/1 [00:40<00:00, 40.83s/it]
INFO:root:eval mean loss: 2265.800787743102
INFO:root:eval perplexity: 8.016426086425781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/56
 28%|██▊       | 56/200 [8:57:59<23:28:01, 586.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1475.4330073337928
INFO:root:current train perplexity3.6961936950683594
INFO:root:current mean train loss 1487.2346724958609
INFO:root:current train perplexity3.6864075660705566
INFO:root:current mean train loss 1489.648671427571
INFO:root:current train perplexity3.685889482498169
INFO:root:current mean train loss 1488.86082941039
INFO:root:current train perplexity3.69071102142334
INFO:root:current mean train loss 1486.8319208183204
INFO:root:current train perplexity3.6863510608673096
INFO:root:current mean train loss 1486.6880733538453
INFO:root:current train perplexity3.6907260417938232
INFO:root:current mean train loss 1485.4273526005663
INFO:root:current train perplexity3.6906750202178955
INFO:root:current mean train loss 1487.0596733652005
INFO:root:current train perplexity3.6958694458007812
INFO:root:current mean train loss 1487.7258292174647
INFO:root:current train perplexity3.6954824924468994
INFO:root:current mean train loss 1488.2188488371614
INFO:root:current train perplexity3.6992461681365967
INFO:root:current mean train loss 1489.266440815295
INFO:root:current train perplexity3.697070598602295
INFO:root:current mean train loss 1488.478592939733
INFO:root:current train perplexity3.6975271701812744
INFO:root:current mean train loss 1488.7131660882233
INFO:root:current train perplexity3.698582172393799
INFO:root:current mean train loss 1488.4961692872178
INFO:root:current train perplexity3.697406768798828
INFO:root:current mean train loss 1490.2022270975567
INFO:root:current train perplexity3.700645685195923
INFO:root:current mean train loss 1489.8161413314494
INFO:root:current train perplexity3.702385187149048
INFO:root:current mean train loss 1490.3932641882525
INFO:root:current train perplexity3.7034714221954346
INFO:root:current mean train loss 1490.4865164242085
INFO:root:current train perplexity3.7030062675476074
INFO:root:current mean train loss 1491.086909512067
INFO:root:current train perplexity3.7014636993408203
INFO:root:current mean train loss 1491.2405216875961
INFO:root:current train perplexity3.703272819519043

100%|██████████| 1/1 [08:15<00:00, 495.71s/it][A100%|██████████| 1/1 [08:15<00:00, 495.71s/it]
INFO:root:final mean train loss: 1491.2454800435046
INFO:root:final train perplexity: 3.702836275100708
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.38s/it][A100%|██████████| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 1809.2520808226673
INFO:root:eval perplexity: 5.119869232177734
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.57s/it][A100%|██████████| 1/1 [00:41<00:00, 41.58s/it]
INFO:root:eval mean loss: 2265.8325584898603
INFO:root:eval perplexity: 8.016658782958984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/57
 28%|██▊       | 57/200 [9:07:42<23:15:39, 585.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1493.5619165757123
INFO:root:current train perplexity3.6949076652526855
INFO:root:current mean train loss 1492.9454890659877
INFO:root:current train perplexity3.6849942207336426
INFO:root:current mean train loss 1488.1972633475689
INFO:root:current train perplexity3.6789309978485107
INFO:root:current mean train loss 1487.5200805664062
INFO:root:current train perplexity3.6785433292388916
INFO:root:current mean train loss 1486.8775715624165
INFO:root:current train perplexity3.679715156555176
INFO:root:current mean train loss 1487.2058573978047
INFO:root:current train perplexity3.6817312240600586
INFO:root:current mean train loss 1487.278176861609
INFO:root:current train perplexity3.6823577880859375
INFO:root:current mean train loss 1487.449546178182
INFO:root:current train perplexity3.6790103912353516
INFO:root:current mean train loss 1485.2715383784562
INFO:root:current train perplexity3.674039363861084
INFO:root:current mean train loss 1485.0452289423667
INFO:root:current train perplexity3.6731209754943848
INFO:root:current mean train loss 1485.5145133372102
INFO:root:current train perplexity3.67568302154541
INFO:root:current mean train loss 1485.744106449493
INFO:root:current train perplexity3.6775383949279785
INFO:root:current mean train loss 1486.4704378049832
INFO:root:current train perplexity3.679307699203491
INFO:root:current mean train loss 1484.5629640099598
INFO:root:current train perplexity3.677049160003662
INFO:root:current mean train loss 1484.7748650075305
INFO:root:current train perplexity3.678788900375366
INFO:root:current mean train loss 1484.802951968446
INFO:root:current train perplexity3.680032253265381
INFO:root:current mean train loss 1485.3324759138002
INFO:root:current train perplexity3.6818525791168213
INFO:root:current mean train loss 1485.7334141105548
INFO:root:current train perplexity3.682652711868286
INFO:root:current mean train loss 1485.296381882988
INFO:root:current train perplexity3.6830055713653564
INFO:root:current mean train loss 1486.2212834552051
INFO:root:current train perplexity3.684967041015625

100%|██████████| 1/1 [08:23<00:00, 503.15s/it][A100%|██████████| 1/1 [08:23<00:00, 503.15s/it]
INFO:root:final mean train loss: 1485.8570940149473
INFO:root:final train perplexity: 3.6853628158569336
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.08s/it][A100%|██████████| 1/1 [00:43<00:00, 43.10s/it]
INFO:root:eval mean loss: 1811.2467564792498
INFO:root:eval perplexity: 5.129096508026123
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.95s/it][A100%|██████████| 1/1 [00:40<00:00, 40.98s/it]
INFO:root:eval mean loss: 2267.722189179549
INFO:root:eval perplexity: 8.03058910369873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/58
 29%|██▉       | 58/200 [9:17:32<23:08:50, 586.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1473.682341452206
INFO:root:current train perplexity3.6412622928619385
INFO:root:current mean train loss 1468.2563667915963
INFO:root:current train perplexity3.650047540664673
INFO:root:current mean train loss 1471.4313609340734
INFO:root:current train perplexity3.65065336227417
INFO:root:current mean train loss 1470.1522816051136
INFO:root:current train perplexity3.653240203857422
INFO:root:current mean train loss 1473.354848330783
INFO:root:current train perplexity3.657548427581787
INFO:root:current mean train loss 1474.9935874482505
INFO:root:current train perplexity3.6595494747161865
INFO:root:current mean train loss 1475.3947112368842
INFO:root:current train perplexity3.6602659225463867
INFO:root:current mean train loss 1473.7063145339869
INFO:root:current train perplexity3.656350612640381
INFO:root:current mean train loss 1474.4184611692267
INFO:root:current train perplexity3.656785011291504
INFO:root:current mean train loss 1474.7000245379918
INFO:root:current train perplexity3.659693956375122
INFO:root:current mean train loss 1474.098622236823
INFO:root:current train perplexity3.660916805267334
INFO:root:current mean train loss 1474.9046144638382
INFO:root:current train perplexity3.6601550579071045
INFO:root:current mean train loss 1476.2488596637888
INFO:root:current train perplexity3.6626698970794678
INFO:root:current mean train loss 1476.6464713306634
INFO:root:current train perplexity3.6618707180023193
INFO:root:current mean train loss 1477.483209207965
INFO:root:current train perplexity3.66365909576416
INFO:root:current mean train loss 1478.6899924678382
INFO:root:current train perplexity3.6640870571136475
INFO:root:current mean train loss 1479.3135135095974
INFO:root:current train perplexity3.6679556369781494
INFO:root:current mean train loss 1479.7854795140713
INFO:root:current train perplexity3.6670851707458496
INFO:root:current mean train loss 1480.8836789078043
INFO:root:current train perplexity3.6684160232543945

100%|██████████| 1/1 [08:25<00:00, 505.13s/it][A100%|██████████| 1/1 [08:25<00:00, 505.13s/it]
INFO:root:final mean train loss: 1480.8548498418195
INFO:root:final train perplexity: 3.6692142486572266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.81s/it][A100%|██████████| 1/1 [00:43<00:00, 43.81s/it]
INFO:root:eval mean loss: 1812.3337436973625
INFO:root:eval perplexity: 5.134130954742432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.07s/it][A100%|██████████| 1/1 [00:42<00:00, 42.07s/it]
INFO:root:eval mean loss: 2272.3648616709606
INFO:root:eval perplexity: 8.064913749694824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/59
 30%|██▉       | 59/200 [9:27:25<23:03:43, 588.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1434.3447265625
INFO:root:current train perplexity3.748544931411743
INFO:root:current mean train loss 1485.4661446365656
INFO:root:current train perplexity3.6565818786621094
INFO:root:current mean train loss 1478.5268270662516
INFO:root:current train perplexity3.666585922241211
INFO:root:current mean train loss 1473.2780098820365
INFO:root:current train perplexity3.6575241088867188
INFO:root:current mean train loss 1472.8665267412935
INFO:root:current train perplexity3.6562392711639404
INFO:root:current mean train loss 1476.130894877521
INFO:root:current train perplexity3.6580564975738525
INFO:root:current mean train loss 1475.1732686698635
INFO:root:current train perplexity3.6517906188964844
INFO:root:current mean train loss 1477.8157432099692
INFO:root:current train perplexity3.6541082859039307
INFO:root:current mean train loss 1477.452360614577
INFO:root:current train perplexity3.651014566421509
INFO:root:current mean train loss 1476.419771790769
INFO:root:current train perplexity3.6504745483398438
INFO:root:current mean train loss 1476.0286265847212
INFO:root:current train perplexity3.6492626667022705
INFO:root:current mean train loss 1475.5702984320058
INFO:root:current train perplexity3.6500701904296875
INFO:root:current mean train loss 1475.2722777304753
INFO:root:current train perplexity3.652254343032837
INFO:root:current mean train loss 1475.27003687611
INFO:root:current train perplexity3.651501178741455
INFO:root:current mean train loss 1475.1861594032798
INFO:root:current train perplexity3.653212308883667
INFO:root:current mean train loss 1475.6222937288044
INFO:root:current train perplexity3.6513683795928955
INFO:root:current mean train loss 1476.3490999105122
INFO:root:current train perplexity3.6530885696411133
INFO:root:current mean train loss 1476.5528890786804
INFO:root:current train perplexity3.6545045375823975
INFO:root:current mean train loss 1477.314249629318
INFO:root:current train perplexity3.655027389526367
INFO:root:current mean train loss 1477.0037963738828
INFO:root:current train perplexity3.655207872390747

100%|██████████| 1/1 [08:21<00:00, 501.37s/it][A100%|██████████| 1/1 [08:21<00:00, 501.37s/it]
INFO:root:final mean train loss: 1476.7740474822122
INFO:root:final train perplexity: 3.6560933589935303
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.81s/it][A100%|██████████| 1/1 [00:38<00:00, 38.81s/it]
INFO:root:eval mean loss: 1817.7504965058456
INFO:root:eval perplexity: 5.159295082092285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.57s/it][A100%|██████████| 1/1 [00:36<00:00, 36.57s/it]
INFO:root:eval mean loss: 2279.6931399081614
INFO:root:eval perplexity: 8.119391441345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/60
 30%|███       | 60/200 [9:37:04<22:47:04, 585.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1436.4015149568256
INFO:root:current train perplexity3.602825880050659
INFO:root:current mean train loss 1463.9168762719933
INFO:root:current train perplexity3.606229066848755
INFO:root:current mean train loss 1469.1065088559503
INFO:root:current train perplexity3.613757371902466
INFO:root:current mean train loss 1465.9760045736186
INFO:root:current train perplexity3.6280834674835205
INFO:root:current mean train loss 1466.7597271684815
INFO:root:current train perplexity3.6329896450042725
INFO:root:current mean train loss 1465.2725037444304
INFO:root:current train perplexity3.633324384689331
INFO:root:current mean train loss 1467.59777615105
INFO:root:current train perplexity3.6346867084503174
INFO:root:current mean train loss 1468.6362228287444
INFO:root:current train perplexity3.632657766342163
INFO:root:current mean train loss 1466.9942322895204
INFO:root:current train perplexity3.6289865970611572
INFO:root:current mean train loss 1468.9470309152696
INFO:root:current train perplexity3.62959885597229
INFO:root:current mean train loss 1469.4009792938082
INFO:root:current train perplexity3.633371114730835
INFO:root:current mean train loss 1470.442744005355
INFO:root:current train perplexity3.6373112201690674
INFO:root:current mean train loss 1469.7294596420927
INFO:root:current train perplexity3.6363651752471924
INFO:root:current mean train loss 1470.5530879069133
INFO:root:current train perplexity3.635782241821289
INFO:root:current mean train loss 1471.4261980439846
INFO:root:current train perplexity3.635690450668335
INFO:root:current mean train loss 1471.2013207059538
INFO:root:current train perplexity3.636692523956299
INFO:root:current mean train loss 1472.4179474875984
INFO:root:current train perplexity3.6381490230560303
INFO:root:current mean train loss 1472.5141588070144
INFO:root:current train perplexity3.639726400375366
INFO:root:current mean train loss 1472.621307691812
INFO:root:current train perplexity3.6398205757141113
INFO:root:current mean train loss 1472.0498826114879
INFO:root:current train perplexity3.6396872997283936

100%|██████████| 1/1 [08:24<00:00, 504.92s/it][A100%|██████████| 1/1 [08:24<00:00, 504.92s/it]
INFO:root:final mean train loss: 1471.943250231952
INFO:root:final train perplexity: 3.6406216621398926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.05s/it][A100%|██████████| 1/1 [00:45<00:00, 45.06s/it]
INFO:root:eval mean loss: 1816.1943835535794
INFO:root:eval perplexity: 5.1520538330078125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.48s/it][A100%|██████████| 1/1 [00:42<00:00, 42.50s/it]
INFO:root:eval mean loss: 2278.9856238398993
INFO:root:eval perplexity: 8.114115715026855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/61
 30%|███       | 61/200 [9:46:59<22:43:27, 588.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1458.8006286621094
INFO:root:current train perplexity3.595043659210205
INFO:root:current mean train loss 1471.5401656206916
INFO:root:current train perplexity3.6270320415496826
INFO:root:current mean train loss 1466.0524193715241
INFO:root:current train perplexity3.6132311820983887
INFO:root:current mean train loss 1463.088163103376
INFO:root:current train perplexity3.610004186630249
INFO:root:current mean train loss 1464.1257643393421
INFO:root:current train perplexity3.6106278896331787
INFO:root:current mean train loss 1464.0957010753118
INFO:root:current train perplexity3.614489793777466
INFO:root:current mean train loss 1464.1865967564613
INFO:root:current train perplexity3.6153345108032227
INFO:root:current mean train loss 1464.6436573526134
INFO:root:current train perplexity3.619077682495117
INFO:root:current mean train loss 1464.0185767360852
INFO:root:current train perplexity3.6141438484191895
INFO:root:current mean train loss 1464.2782419318826
INFO:root:current train perplexity3.614032745361328
INFO:root:current mean train loss 1464.573447091239
INFO:root:current train perplexity3.615725517272949
INFO:root:current mean train loss 1464.8538354148327
INFO:root:current train perplexity3.616201400756836
INFO:root:current mean train loss 1465.5586568591664
INFO:root:current train perplexity3.618586540222168
INFO:root:current mean train loss 1466.2424019453767
INFO:root:current train perplexity3.620232105255127
INFO:root:current mean train loss 1466.2182361315884
INFO:root:current train perplexity3.618846893310547
INFO:root:current mean train loss 1467.0256956418355
INFO:root:current train perplexity3.6210896968841553
INFO:root:current mean train loss 1466.6327623586492
INFO:root:current train perplexity3.6220903396606445
INFO:root:current mean train loss 1467.3258935603128
INFO:root:current train perplexity3.624185085296631
INFO:root:current mean train loss 1468.096474707776
INFO:root:current train perplexity3.625094413757324
INFO:root:current mean train loss 1467.7312873651174
INFO:root:current train perplexity3.6253249645233154

100%|██████████| 1/1 [08:34<00:00, 514.99s/it][A100%|██████████| 1/1 [08:34<00:00, 514.99s/it]
INFO:root:final mean train loss: 1467.092990984895
INFO:root:final train perplexity: 3.6251540184020996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.34s/it][A100%|██████████| 1/1 [00:47<00:00, 47.38s/it]
INFO:root:eval mean loss: 1822.406128362561
INFO:root:eval perplexity: 5.181022644042969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.69s/it][A100%|██████████| 1/1 [00:43<00:00, 43.69s/it]
INFO:root:eval mean loss: 2286.8004236965317
INFO:root:eval perplexity: 8.172576904296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/62
 31%|███       | 62/200 [9:57:07<22:47:21, 594.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.0170092312794
INFO:root:current train perplexity3.574693441390991
INFO:root:current mean train loss 1448.9695199205985
INFO:root:current train perplexity3.5799973011016846
INFO:root:current mean train loss 1453.142304069911
INFO:root:current train perplexity3.5874640941619873
INFO:root:current mean train loss 1457.4790761801744
INFO:root:current train perplexity3.592275857925415
INFO:root:current mean train loss 1457.8857403012037
INFO:root:current train perplexity3.5989513397216797
INFO:root:current mean train loss 1459.3868593626385
INFO:root:current train perplexity3.6001031398773193
INFO:root:current mean train loss 1460.6554127060801
INFO:root:current train perplexity3.601599931716919
INFO:root:current mean train loss 1459.347121280503
INFO:root:current train perplexity3.59786057472229
INFO:root:current mean train loss 1459.7603224545262
INFO:root:current train perplexity3.602184772491455
INFO:root:current mean train loss 1461.1387980442107
INFO:root:current train perplexity3.602400302886963
INFO:root:current mean train loss 1460.9594885381425
INFO:root:current train perplexity3.6033644676208496
INFO:root:current mean train loss 1462.0997553723641
INFO:root:current train perplexity3.603360176086426
INFO:root:current mean train loss 1461.0325135689968
INFO:root:current train perplexity3.606792688369751
INFO:root:current mean train loss 1461.2990248990957
INFO:root:current train perplexity3.60664439201355
INFO:root:current mean train loss 1461.9115822799274
INFO:root:current train perplexity3.609300136566162
INFO:root:current mean train loss 1462.3343788829836
INFO:root:current train perplexity3.6105329990386963
INFO:root:current mean train loss 1461.9652473278934
INFO:root:current train perplexity3.609480381011963
INFO:root:current mean train loss 1461.8764491062198
INFO:root:current train perplexity3.6093406677246094
INFO:root:current mean train loss 1462.653030848413
INFO:root:current train perplexity3.609696865081787
INFO:root:current mean train loss 1462.7151204552092
INFO:root:current train perplexity3.6105313301086426

100%|██████████| 1/1 [08:38<00:00, 518.74s/it][A100%|██████████| 1/1 [08:38<00:00, 518.74s/it]
INFO:root:final mean train loss: 1462.4249632927244
INFO:root:final train perplexity: 3.6103296279907227
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.90s/it][A100%|██████████| 1/1 [00:45<00:00, 45.91s/it]
INFO:root:eval mean loss: 1823.90337052582
INFO:root:eval perplexity: 5.188029766082764
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.14s/it][A100%|██████████| 1/1 [00:44<00:00, 44.16s/it]
INFO:root:eval mean loss: 2291.9994918065713
INFO:root:eval perplexity: 8.211702346801758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/63
 32%|███▏      | 63/200 [10:07:19<22:48:51, 599.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1455.1460641043527
INFO:root:current train perplexity3.571261167526245
INFO:root:current mean train loss 1457.6244492474725
INFO:root:current train perplexity3.5934271812438965
INFO:root:current mean train loss 1459.8252581561053
INFO:root:current train perplexity3.5964136123657227
INFO:root:current mean train loss 1461.3715170370565
INFO:root:current train perplexity3.5939149856567383
INFO:root:current mean train loss 1461.4202093895447
INFO:root:current train perplexity3.5888566970825195
INFO:root:current mean train loss 1459.536052074767
INFO:root:current train perplexity3.5895581245422363
INFO:root:current mean train loss 1459.8376062193913
INFO:root:current train perplexity3.5878219604492188
INFO:root:current mean train loss 1457.332692173549
INFO:root:current train perplexity3.585081100463867
INFO:root:current mean train loss 1457.2791350967582
INFO:root:current train perplexity3.5841803550720215
INFO:root:current mean train loss 1457.6522062006684
INFO:root:current train perplexity3.58876895904541
INFO:root:current mean train loss 1456.8670590409608
INFO:root:current train perplexity3.586604595184326
INFO:root:current mean train loss 1455.0076673719618
INFO:root:current train perplexity3.58427095413208
INFO:root:current mean train loss 1456.0570586437316
INFO:root:current train perplexity3.586648941040039
INFO:root:current mean train loss 1455.5790189645586
INFO:root:current train perplexity3.5849668979644775
INFO:root:current mean train loss 1456.7709629603794
INFO:root:current train perplexity3.588247537612915
INFO:root:current mean train loss 1457.144972180409
INFO:root:current train perplexity3.590428590774536
INFO:root:current mean train loss 1457.5254328744854
INFO:root:current train perplexity3.5920095443725586
INFO:root:current mean train loss 1456.7599316268318
INFO:root:current train perplexity3.591825485229492
INFO:root:current mean train loss 1456.5578531683448
INFO:root:current train perplexity3.5916857719421387
INFO:root:current mean train loss 1457.9540615953165
INFO:root:current train perplexity3.594493865966797

100%|██████████| 1/1 [08:25<00:00, 505.39s/it][A100%|██████████| 1/1 [08:25<00:00, 505.39s/it]
INFO:root:final mean train loss: 1457.5589386617303
INFO:root:final train perplexity: 3.594939947128296
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.18s/it][A100%|██████████| 1/1 [00:45<00:00, 45.20s/it]
INFO:root:eval mean loss: 1825.4247986272717
INFO:root:eval perplexity: 5.195158958435059
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.83s/it][A100%|██████████| 1/1 [00:42<00:00, 42.84s/it]
INFO:root:eval mean loss: 2293.7729189176084
INFO:root:eval perplexity: 8.225091934204102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/64
 32%|███▏      | 64/200 [10:17:14<22:36:22, 598.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.3244334253773
INFO:root:current train perplexity3.5618739128112793
INFO:root:current mean train loss 1441.0729027406417
INFO:root:current train perplexity3.555736541748047
INFO:root:current mean train loss 1440.2695346526568
INFO:root:current train perplexity3.5458619594573975
INFO:root:current mean train loss 1442.061404206032
INFO:root:current train perplexity3.549030303955078
INFO:root:current mean train loss 1446.4122347038629
INFO:root:current train perplexity3.5524542331695557
INFO:root:current mean train loss 1446.2281252911387
INFO:root:current train perplexity3.5516319274902344
INFO:root:current mean train loss 1448.3707394440275
INFO:root:current train perplexity3.555717706680298
INFO:root:current mean train loss 1447.9525658342102
INFO:root:current train perplexity3.5570855140686035
INFO:root:current mean train loss 1450.022812147689
INFO:root:current train perplexity3.562891960144043
INFO:root:current mean train loss 1450.87094904659
INFO:root:current train perplexity3.565181016921997
INFO:root:current mean train loss 1450.5661220182053
INFO:root:current train perplexity3.5669360160827637
INFO:root:current mean train loss 1451.6027144035975
INFO:root:current train perplexity3.569188117980957
INFO:root:current mean train loss 1451.841665888907
INFO:root:current train perplexity3.5722498893737793
INFO:root:current mean train loss 1452.343712507604
INFO:root:current train perplexity3.5753297805786133
INFO:root:current mean train loss 1451.7795635087425
INFO:root:current train perplexity3.575251579284668
INFO:root:current mean train loss 1451.7533835859965
INFO:root:current train perplexity3.5738489627838135
INFO:root:current mean train loss 1451.332998622856
INFO:root:current train perplexity3.5747601985931396
INFO:root:current mean train loss 1452.5071113641927
INFO:root:current train perplexity3.576414108276367
INFO:root:current mean train loss 1452.7600134529634
INFO:root:current train perplexity3.5781705379486084

100%|██████████| 1/1 [08:46<00:00, 526.79s/it][A100%|██████████| 1/1 [08:46<00:00, 526.79s/it]
INFO:root:final mean train loss: 1453.053414503012
INFO:root:final train perplexity: 3.5807487964630127
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:48<00:00, 48.01s/it][A100%|██████████| 1/1 [00:48<00:00, 48.04s/it]
INFO:root:eval mean loss: 1822.95308734001
INFO:root:eval perplexity: 5.1835808753967285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.70s/it][A100%|██████████| 1/1 [00:43<00:00, 43.72s/it]
INFO:root:eval mean loss: 2292.8452572653478
INFO:root:eval perplexity: 8.218086242675781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/65
 32%|███▎      | 65/200 [10:27:35<22:41:37, 605.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1434.7894897460938
INFO:root:current train perplexity3.4176111221313477
INFO:root:current mean train loss 1425.043201153095
INFO:root:current train perplexity3.5287342071533203
INFO:root:current mean train loss 1428.8645151175704
INFO:root:current train perplexity3.5220839977264404
INFO:root:current mean train loss 1436.5992865311473
INFO:root:current train perplexity3.5323739051818848
INFO:root:current mean train loss 1434.8168268487004
INFO:root:current train perplexity3.5327441692352295
INFO:root:current mean train loss 1437.4188079833984
INFO:root:current train perplexity3.536829948425293
INFO:root:current mean train loss 1439.1410229411347
INFO:root:current train perplexity3.5360026359558105
INFO:root:current mean train loss 1441.8059772144663
INFO:root:current train perplexity3.5358970165252686
INFO:root:current mean train loss 1442.7746381617303
INFO:root:current train perplexity3.5397815704345703
INFO:root:current mean train loss 1444.775114211361
INFO:root:current train perplexity3.5459511280059814
INFO:root:current mean train loss 1444.0469128126167
INFO:root:current train perplexity3.549525737762451
INFO:root:current mean train loss 1444.1676338306372
INFO:root:current train perplexity3.551466226577759
INFO:root:current mean train loss 1442.5256515959172
INFO:root:current train perplexity3.5491881370544434
INFO:root:current mean train loss 1444.0135387584476
INFO:root:current train perplexity3.5535550117492676
INFO:root:current mean train loss 1444.2891105804008
INFO:root:current train perplexity3.5560171604156494
INFO:root:current mean train loss 1444.932739744795
INFO:root:current train perplexity3.5580811500549316
INFO:root:current mean train loss 1446.4423206357885
INFO:root:current train perplexity3.560105800628662
INFO:root:current mean train loss 1446.7142955797938
INFO:root:current train perplexity3.5608227252960205
INFO:root:current mean train loss 1446.2772122063816
INFO:root:current train perplexity3.560189962387085
INFO:root:current mean train loss 1447.6534746314296
INFO:root:current train perplexity3.5622870922088623

100%|██████████| 1/1 [08:36<00:00, 516.19s/it][A100%|██████████| 1/1 [08:36<00:00, 516.19s/it]
INFO:root:final mean train loss: 1447.6622993442306
INFO:root:final train perplexity: 3.563842535018921
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.64s/it][A100%|██████████| 1/1 [00:46<00:00, 46.65s/it]
INFO:root:eval mean loss: 1828.0202619403813
INFO:root:eval perplexity: 5.207345485687256
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.47s/it][A100%|██████████| 1/1 [00:44<00:00, 44.48s/it]
INFO:root:eval mean loss: 2302.64168943581
INFO:root:eval perplexity: 8.292379379272461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/66
 33%|███▎      | 66/200 [10:37:45<22:34:35, 606.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1425.026576450893
INFO:root:current train perplexity3.5067429542541504
INFO:root:current mean train loss 1439.5612369253615
INFO:root:current train perplexity3.527144193649292
INFO:root:current mean train loss 1434.711859379419
INFO:root:current train perplexity3.528087854385376
INFO:root:current mean train loss 1438.4389503930588
INFO:root:current train perplexity3.530052900314331
INFO:root:current mean train loss 1437.741556851711
INFO:root:current train perplexity3.5309460163116455
INFO:root:current mean train loss 1436.2717196122242
INFO:root:current train perplexity3.5317933559417725
INFO:root:current mean train loss 1438.3444344586603
INFO:root:current train perplexity3.5340731143951416
INFO:root:current mean train loss 1438.5747615480886
INFO:root:current train perplexity3.53737473487854
INFO:root:current mean train loss 1438.1130601555362
INFO:root:current train perplexity3.5340869426727295
INFO:root:current mean train loss 1439.0423907384552
INFO:root:current train perplexity3.535005569458008
INFO:root:current mean train loss 1439.2906967596489
INFO:root:current train perplexity3.5359108448028564
INFO:root:current mean train loss 1440.392610684343
INFO:root:current train perplexity3.539984941482544
INFO:root:current mean train loss 1440.5668453432124
INFO:root:current train perplexity3.540475368499756
INFO:root:current mean train loss 1439.7913369258906
INFO:root:current train perplexity3.5390355587005615
INFO:root:current mean train loss 1440.8716935353075
INFO:root:current train perplexity3.5422160625457764
INFO:root:current mean train loss 1442.0054791994114
INFO:root:current train perplexity3.545849323272705
INFO:root:current mean train loss 1442.9781132673938
INFO:root:current train perplexity3.547957181930542
INFO:root:current mean train loss 1443.2634100019065
INFO:root:current train perplexity3.548274278640747
INFO:root:current mean train loss 1443.1677912419343
INFO:root:current train perplexity3.5470423698425293
INFO:root:current mean train loss 1443.785397404001
INFO:root:current train perplexity3.549769878387451

100%|██████████| 1/1 [08:41<00:00, 521.94s/it][A100%|██████████| 1/1 [08:41<00:00, 521.94s/it]
INFO:root:final mean train loss: 1443.8270858326046
INFO:root:final train perplexity: 3.5518643856048584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.64s/it][A100%|██████████| 1/1 [00:42<00:00, 42.66s/it]
INFO:root:eval mean loss: 1831.1772608114472
INFO:root:eval perplexity: 5.222205638885498
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.87s/it][A100%|██████████| 1/1 [00:42<00:00, 42.88s/it]
INFO:root:eval mean loss: 2305.7923570998173
INFO:root:eval perplexity: 8.31641674041748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/67
 34%|███▎      | 67/200 [10:47:55<22:26:40, 607.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1435.453677528783
INFO:root:current train perplexity3.5064144134521484
INFO:root:current mean train loss 1424.309770224751
INFO:root:current train perplexity3.4939773082733154
INFO:root:current mean train loss 1430.8253691857603
INFO:root:current train perplexity3.5028631687164307
INFO:root:current mean train loss 1432.6386350372134
INFO:root:current train perplexity3.500715970993042
INFO:root:current mean train loss 1434.054716763431
INFO:root:current train perplexity3.512347459793091
INFO:root:current mean train loss 1436.405222158893
INFO:root:current train perplexity3.5157313346862793
INFO:root:current mean train loss 1437.371591597889
INFO:root:current train perplexity3.5163943767547607
INFO:root:current mean train loss 1436.1795723767784
INFO:root:current train perplexity3.5164108276367188
INFO:root:current mean train loss 1436.816236983051
INFO:root:current train perplexity3.5182807445526123
INFO:root:current mean train loss 1435.8604990099032
INFO:root:current train perplexity3.5200064182281494
INFO:root:current mean train loss 1436.1256018842575
INFO:root:current train perplexity3.527122974395752
INFO:root:current mean train loss 1437.1296037026993
INFO:root:current train perplexity3.5283780097961426
INFO:root:current mean train loss 1438.008828503635
INFO:root:current train perplexity3.53078556060791
INFO:root:current mean train loss 1438.71194316596
INFO:root:current train perplexity3.532522678375244
INFO:root:current mean train loss 1438.2497809865047
INFO:root:current train perplexity3.533006429672241
INFO:root:current mean train loss 1438.798663036411
INFO:root:current train perplexity3.5344324111938477
INFO:root:current mean train loss 1438.5771781725762
INFO:root:current train perplexity3.5338804721832275
INFO:root:current mean train loss 1438.4818247980572
INFO:root:current train perplexity3.5353291034698486
INFO:root:current mean train loss 1439.433053997318
INFO:root:current train perplexity3.536717653274536
INFO:root:current mean train loss 1439.3151150005644
INFO:root:current train perplexity3.5372016429901123

100%|██████████| 1/1 [08:42<00:00, 522.74s/it][A100%|██████████| 1/1 [08:42<00:00, 522.74s/it]
INFO:root:final mean train loss: 1439.0858395298021
INFO:root:final train perplexity: 3.537111520767212
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.46s/it][A100%|██████████| 1/1 [00:43<00:00, 43.46s/it]
INFO:root:eval mean loss: 1833.798975734846
INFO:root:eval perplexity: 5.234578609466553
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.01s/it][A100%|██████████| 1/1 [00:40<00:00, 40.01s/it]
INFO:root:eval mean loss: 2310.7569571420654
INFO:root:eval perplexity: 8.354432106018066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/68
 34%|███▍      | 68/200 [10:58:03<22:17:09, 607.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1441.8382790305398
INFO:root:current train perplexity3.539994716644287
INFO:root:current mean train loss 1423.4313744329638
INFO:root:current train perplexity3.4994077682495117
INFO:root:current mean train loss 1424.6262331495097
INFO:root:current train perplexity3.5031754970550537
INFO:root:current mean train loss 1425.0077378823723
INFO:root:current train perplexity3.5109965801239014
INFO:root:current mean train loss 1424.758120761075
INFO:root:current train perplexity3.5125060081481934
INFO:root:current mean train loss 1427.0833766627957
INFO:root:current train perplexity3.514423131942749
INFO:root:current mean train loss 1428.4099972790434
INFO:root:current train perplexity3.517699718475342
INFO:root:current mean train loss 1427.2829766077712
INFO:root:current train perplexity3.5148463249206543
INFO:root:current mean train loss 1428.9565746641995
INFO:root:current train perplexity3.5139355659484863
INFO:root:current mean train loss 1430.5952264755808
INFO:root:current train perplexity3.5149471759796143
INFO:root:current mean train loss 1431.4865116354413
INFO:root:current train perplexity3.5169386863708496
INFO:root:current mean train loss 1432.0680074320212
INFO:root:current train perplexity3.5177114009857178
INFO:root:current mean train loss 1432.8802255820467
INFO:root:current train perplexity3.5192952156066895
INFO:root:current mean train loss 1433.7844482421874
INFO:root:current train perplexity3.5210447311401367
INFO:root:current mean train loss 1434.0947676720898
INFO:root:current train perplexity3.5207326412200928
INFO:root:current mean train loss 1434.3906837978548
INFO:root:current train perplexity3.5218334197998047
INFO:root:current mean train loss 1434.9314838144355
INFO:root:current train perplexity3.5232787132263184
INFO:root:current mean train loss 1435.3739597940037
INFO:root:current train perplexity3.5239243507385254
INFO:root:current mean train loss 1434.8473296543336
INFO:root:current train perplexity3.524259090423584
INFO:root:current mean train loss 1434.7099162304187
INFO:root:current train perplexity3.5234668254852295

100%|██████████| 1/1 [08:31<00:00, 511.58s/it][A100%|██████████| 1/1 [08:31<00:00, 511.58s/it]
INFO:root:final mean train loss: 1434.6024334898393
INFO:root:final train perplexity: 3.5232174396514893
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.81s/it][A100%|██████████| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 1837.6397241903535
INFO:root:eval perplexity: 5.252757549285889
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.76s/it][A100%|██████████| 1/1 [00:41<00:00, 41.76s/it]
INFO:root:eval mean loss: 2317.8740095855496
INFO:root:eval perplexity: 8.40923023223877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/69
 34%|███▍      | 69/200 [11:08:03<22:01:36, 605.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.372804429796
INFO:root:current train perplexity3.5105018615722656
INFO:root:current mean train loss 1424.78839040357
INFO:root:current train perplexity3.484344959259033
INFO:root:current mean train loss 1425.0546453139361
INFO:root:current train perplexity3.4879233837127686
INFO:root:current mean train loss 1425.5174439132854
INFO:root:current train perplexity3.488873243331909
INFO:root:current mean train loss 1426.8977386991855
INFO:root:current train perplexity3.4901371002197266
INFO:root:current mean train loss 1426.5892227279555
INFO:root:current train perplexity3.4883005619049072
INFO:root:current mean train loss 1428.5860007149834
INFO:root:current train perplexity3.491421937942505
INFO:root:current mean train loss 1429.3817749023438
INFO:root:current train perplexity3.4989564418792725
INFO:root:current mean train loss 1428.277665724448
INFO:root:current train perplexity3.4974093437194824
INFO:root:current mean train loss 1427.7621770160188
INFO:root:current train perplexity3.500483751296997
INFO:root:current mean train loss 1428.6692529934555
INFO:root:current train perplexity3.50215220451355
INFO:root:current mean train loss 1429.5696267398143
INFO:root:current train perplexity3.5033392906188965
INFO:root:current mean train loss 1429.1504908147847
INFO:root:current train perplexity3.503319263458252
INFO:root:current mean train loss 1429.2957720075335
INFO:root:current train perplexity3.5027313232421875
INFO:root:current mean train loss 1429.7742429816205
INFO:root:current train perplexity3.5048961639404297
INFO:root:current mean train loss 1429.8217872833175
INFO:root:current train perplexity3.5059611797332764
INFO:root:current mean train loss 1430.1515733636738
INFO:root:current train perplexity3.50775146484375
INFO:root:current mean train loss 1430.2987037124806
INFO:root:current train perplexity3.509265422821045
INFO:root:current mean train loss 1430.436370523567
INFO:root:current train perplexity3.5098307132720947
INFO:root:current mean train loss 1430.743327854613
INFO:root:current train perplexity3.5097591876983643

100%|██████████| 1/1 [08:03<00:00, 483.97s/it][A100%|██████████| 1/1 [08:03<00:00, 483.97s/it]
INFO:root:final mean train loss: 1430.3184623622076
INFO:root:final train perplexity: 3.509993076324463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.65s/it][A100%|██████████| 1/1 [00:42<00:00, 42.67s/it]
INFO:root:eval mean loss: 1839.0948187645445
INFO:root:eval perplexity: 5.2596611976623535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.72s/it][A100%|██████████| 1/1 [00:43<00:00, 43.72s/it]
INFO:root:eval mean loss: 2318.8867356320643
INFO:root:eval perplexity: 8.417058944702148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/70
 35%|███▌      | 70/200 [11:17:36<21:30:21, 595.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.8528819566363
INFO:root:current train perplexity3.447510242462158
INFO:root:current mean train loss 1416.8342143063824
INFO:root:current train perplexity3.4714620113372803
INFO:root:current mean train loss 1416.3944662021518
INFO:root:current train perplexity3.4726264476776123
INFO:root:current mean train loss 1420.4010367503815
INFO:root:current train perplexity3.4836068153381348
INFO:root:current mean train loss 1423.000396416475
INFO:root:current train perplexity3.4873034954071045
INFO:root:current mean train loss 1420.5473255617308
INFO:root:current train perplexity3.4854238033294678
INFO:root:current mean train loss 1422.4721748783904
INFO:root:current train perplexity3.4861409664154053
INFO:root:current mean train loss 1423.6930536577154
INFO:root:current train perplexity3.4860966205596924
INFO:root:current mean train loss 1422.8601983223778
INFO:root:current train perplexity3.486829996109009
INFO:root:current mean train loss 1421.305789095085
INFO:root:current train perplexity3.4865856170654297
INFO:root:current mean train loss 1421.412702688282
INFO:root:current train perplexity3.4883813858032227
INFO:root:current mean train loss 1422.8559580579138
INFO:root:current train perplexity3.4897584915161133
INFO:root:current mean train loss 1423.225102201925
INFO:root:current train perplexity3.4905269145965576
INFO:root:current mean train loss 1424.0957286112423
INFO:root:current train perplexity3.4905943870544434
INFO:root:current mean train loss 1423.9149385532971
INFO:root:current train perplexity3.4927587509155273
INFO:root:current mean train loss 1425.0232594263787
INFO:root:current train perplexity3.4941792488098145
INFO:root:current mean train loss 1424.858061497419
INFO:root:current train perplexity3.493151903152466
INFO:root:current mean train loss 1425.3200876695473
INFO:root:current train perplexity3.4946341514587402
INFO:root:current mean train loss 1425.6476526053384
INFO:root:current train perplexity3.495046615600586

100%|██████████| 1/1 [08:21<00:00, 501.44s/it][A100%|██████████| 1/1 [08:21<00:00, 501.46s/it]
INFO:root:final mean train loss: 1425.6209536122963
INFO:root:final train perplexity: 3.4955482482910156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.81s/it][A100%|██████████| 1/1 [00:42<00:00, 42.82s/it]
INFO:root:eval mean loss: 1839.8125740213598
INFO:root:eval perplexity: 5.263070106506348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.45s/it][A100%|██████████| 1/1 [00:42<00:00, 42.47s/it]
INFO:root:eval mean loss: 2320.4889712502772
INFO:root:eval perplexity: 8.429455757141113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/71
 36%|███▌      | 71/200 [11:27:25<21:16:18, 593.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1391.0004069010417
INFO:root:current train perplexity3.482395648956299
INFO:root:current mean train loss 1409.326184542674
INFO:root:current train perplexity3.4683241844177246
INFO:root:current mean train loss 1411.478565401244
INFO:root:current train perplexity3.4736545085906982
INFO:root:current mean train loss 1411.035692401961
INFO:root:current train perplexity3.4631686210632324
INFO:root:current mean train loss 1413.20211626626
INFO:root:current train perplexity3.467919111251831
INFO:root:current mean train loss 1413.4464745804255
INFO:root:current train perplexity3.465421199798584
INFO:root:current mean train loss 1413.364704396465
INFO:root:current train perplexity3.4694323539733887
INFO:root:current mean train loss 1413.6848568146356
INFO:root:current train perplexity3.4715917110443115
INFO:root:current mean train loss 1417.8680728883955
INFO:root:current train perplexity3.474914312362671
INFO:root:current mean train loss 1418.2903977282526
INFO:root:current train perplexity3.476144552230835
INFO:root:current mean train loss 1418.2447594705206
INFO:root:current train perplexity3.475571870803833
INFO:root:current mean train loss 1418.9772422749138
INFO:root:current train perplexity3.4765734672546387
INFO:root:current mean train loss 1419.9097496526158
INFO:root:current train perplexity3.4783647060394287
INFO:root:current mean train loss 1420.9060705398163
INFO:root:current train perplexity3.4798154830932617
INFO:root:current mean train loss 1420.7034301757812
INFO:root:current train perplexity3.4816484451293945
INFO:root:current mean train loss 1420.8204832849592
INFO:root:current train perplexity3.483135223388672
INFO:root:current mean train loss 1421.5014719885878
INFO:root:current train perplexity3.4844155311584473
INFO:root:current mean train loss 1422.6787670354631
INFO:root:current train perplexity3.487135171890259
INFO:root:current mean train loss 1422.5793742267529
INFO:root:current train perplexity3.48712158203125
INFO:root:current mean train loss 1423.04355604526
INFO:root:current train perplexity3.485718011856079

100%|██████████| 1/1 [08:15<00:00, 495.20s/it][A100%|██████████| 1/1 [08:15<00:00, 495.21s/it]
INFO:root:final mean train loss: 1422.5224775890479
INFO:root:final train perplexity: 3.486053228378296
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.21s/it][A100%|██████████| 1/1 [00:42<00:00, 42.23s/it]
INFO:root:eval mean loss: 1842.8105087821366
INFO:root:eval perplexity: 5.277331829071045
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.17s/it][A100%|██████████| 1/1 [00:40<00:00, 40.19s/it]
INFO:root:eval mean loss: 2326.5296734749004
INFO:root:eval perplexity: 8.476365089416504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/72
 36%|███▌      | 72/200 [11:37:05<20:57:38, 589.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.8252696161685
INFO:root:current train perplexity3.4953625202178955
INFO:root:current mean train loss 1412.7273614154599
INFO:root:current train perplexity3.455687999725342
INFO:root:current mean train loss 1416.8779647211322
INFO:root:current train perplexity3.47092866897583
INFO:root:current mean train loss 1413.5379997702205
INFO:root:current train perplexity3.460099935531616
INFO:root:current mean train loss 1416.1663030529699
INFO:root:current train perplexity3.467010498046875
INFO:root:current mean train loss 1414.951279240858
INFO:root:current train perplexity3.4632606506347656
INFO:root:current mean train loss 1416.1006219903693
INFO:root:current train perplexity3.4667739868164062
INFO:root:current mean train loss 1415.540063037582
INFO:root:current train perplexity3.4670019149780273
INFO:root:current mean train loss 1416.0502787296857
INFO:root:current train perplexity3.466405153274536
INFO:root:current mean train loss 1416.958494242196
INFO:root:current train perplexity3.466403007507324
INFO:root:current mean train loss 1415.648316384294
INFO:root:current train perplexity3.465038776397705
INFO:root:current mean train loss 1415.779167630475
INFO:root:current train perplexity3.4645419120788574
INFO:root:current mean train loss 1416.265059863361
INFO:root:current train perplexity3.4642598628997803
INFO:root:current mean train loss 1417.5408862655306
INFO:root:current train perplexity3.464550256729126
INFO:root:current mean train loss 1417.7821007177013
INFO:root:current train perplexity3.4654366970062256
INFO:root:current mean train loss 1418.0541476815138
INFO:root:current train perplexity3.4644224643707275
INFO:root:current mean train loss 1417.6646538979464
INFO:root:current train perplexity3.4657344818115234
INFO:root:current mean train loss 1418.2508288453778
INFO:root:current train perplexity3.4666717052459717
INFO:root:current mean train loss 1417.949202411461
INFO:root:current train perplexity3.468062162399292
INFO:root:current mean train loss 1418.2758629729956
INFO:root:current train perplexity3.4708425998687744

100%|██████████| 1/1 [08:03<00:00, 483.34s/it][A100%|██████████| 1/1 [08:03<00:00, 483.35s/it]
INFO:root:final mean train loss: 1417.920515482677
INFO:root:final train perplexity: 3.471998453140259
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.60s/it][A100%|██████████| 1/1 [00:42<00:00, 42.61s/it]
INFO:root:eval mean loss: 1846.213606597684
INFO:root:eval perplexity: 5.293567180633545
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.55s/it][A100%|██████████| 1/1 [00:41<00:00, 41.56s/it]
INFO:root:eval mean loss: 2332.542009502438
INFO:root:eval perplexity: 8.52331256866455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/73
 36%|███▋      | 73/200 [11:46:35<20:35:20, 583.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1429.6666015625
INFO:root:current train perplexity3.4533185958862305
INFO:root:current mean train loss 1415.9904758998325
INFO:root:current train perplexity3.451542615890503
INFO:root:current mean train loss 1409.6720860799153
INFO:root:current train perplexity3.457432270050049
INFO:root:current mean train loss 1409.791578584559
INFO:root:current train perplexity3.445671558380127
INFO:root:current mean train loss 1408.8618419300426
INFO:root:current train perplexity3.4392309188842773
INFO:root:current mean train loss 1407.3801845974392
INFO:root:current train perplexity3.437652349472046
INFO:root:current mean train loss 1408.189817047119
INFO:root:current train perplexity3.4393575191497803
INFO:root:current mean train loss 1409.5003596125423
INFO:root:current train perplexity3.443708658218384
INFO:root:current mean train loss 1409.7335549490792
INFO:root:current train perplexity3.444082021713257
INFO:root:current mean train loss 1410.535310915683
INFO:root:current train perplexity3.444669485092163
INFO:root:current mean train loss 1411.4729040292593
INFO:root:current train perplexity3.449557065963745
INFO:root:current mean train loss 1410.841097326446
INFO:root:current train perplexity3.4505741596221924
INFO:root:current mean train loss 1410.705009017452
INFO:root:current train perplexity3.4515981674194336
INFO:root:current mean train loss 1410.5389082723589
INFO:root:current train perplexity3.450904607772827
INFO:root:current mean train loss 1411.6100473192002
INFO:root:current train perplexity3.451213836669922
INFO:root:current mean train loss 1412.4997964437905
INFO:root:current train perplexity3.453591823577881
INFO:root:current mean train loss 1412.7432670035014
INFO:root:current train perplexity3.455512523651123
INFO:root:current mean train loss 1412.9820317410874
INFO:root:current train perplexity3.4563698768615723
INFO:root:current mean train loss 1412.9853227698284
INFO:root:current train perplexity3.4566454887390137
INFO:root:current mean train loss 1413.0063789289022
INFO:root:current train perplexity3.4560093879699707

100%|██████████| 1/1 [08:21<00:00, 501.89s/it][A100%|██████████| 1/1 [08:21<00:00, 501.89s/it]
INFO:root:final mean train loss: 1413.0099036389388
INFO:root:final train perplexity: 3.457063913345337
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.64s/it][A100%|██████████| 1/1 [00:39<00:00, 39.65s/it]
INFO:root:eval mean loss: 1848.6709867436834
INFO:root:eval perplexity: 5.305322170257568
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.94s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 2335.2133689501607
INFO:root:eval perplexity: 8.544255256652832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/74
 37%|███▋      | 74/200 [11:56:18<20:25:44, 583.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.3330335115131
INFO:root:current train perplexity3.5108768939971924
INFO:root:current mean train loss 1414.0091646036524
INFO:root:current train perplexity3.473972797393799
INFO:root:current mean train loss 1410.7577930257478
INFO:root:current train perplexity3.4636974334716797
INFO:root:current mean train loss 1408.5714101070116
INFO:root:current train perplexity3.461355447769165
INFO:root:current mean train loss 1408.753696299747
INFO:root:current train perplexity3.455167055130005
INFO:root:current mean train loss 1406.793245983295
INFO:root:current train perplexity3.449662685394287
INFO:root:current mean train loss 1406.7869245044353
INFO:root:current train perplexity3.4462311267852783
INFO:root:current mean train loss 1409.3576847212476
INFO:root:current train perplexity3.4458539485931396
INFO:root:current mean train loss 1408.114907049938
INFO:root:current train perplexity3.4436612129211426
INFO:root:current mean train loss 1407.8817488173083
INFO:root:current train perplexity3.443713665008545
INFO:root:current mean train loss 1406.9866984934883
INFO:root:current train perplexity3.441401958465576
INFO:root:current mean train loss 1407.066005538637
INFO:root:current train perplexity3.4417803287506104
INFO:root:current mean train loss 1408.4563023241722
INFO:root:current train perplexity3.4441146850585938
INFO:root:current mean train loss 1409.6384889944213
INFO:root:current train perplexity3.4459707736968994
INFO:root:current mean train loss 1409.861322427827
INFO:root:current train perplexity3.4464242458343506
INFO:root:current mean train loss 1410.144643833795
INFO:root:current train perplexity3.446387529373169
INFO:root:current mean train loss 1410.7055272140917
INFO:root:current train perplexity3.4476478099823
INFO:root:current mean train loss 1410.4214231244664
INFO:root:current train perplexity3.447659492492676
INFO:root:current mean train loss 1409.6534825470349
INFO:root:current train perplexity3.4467623233795166
INFO:root:current mean train loss 1409.6030833576185
INFO:root:current train perplexity3.445671558380127

100%|██████████| 1/1 [08:18<00:00, 498.21s/it][A100%|██████████| 1/1 [08:18<00:00, 498.21s/it]
INFO:root:final mean train loss: 1409.4435980369271
INFO:root:final train perplexity: 3.4462573528289795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.57s/it][A100%|██████████| 1/1 [00:40<00:00, 40.59s/it]
INFO:root:eval mean loss: 1850.2590539810506
INFO:root:eval perplexity: 5.312933444976807
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.49s/it][A100%|██████████| 1/1 [00:37<00:00, 37.49s/it]
INFO:root:eval mean loss: 2337.0783708721187
INFO:root:eval perplexity: 8.558906555175781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/75
 38%|███▊      | 75/200 [12:05:57<20:12:47, 582.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1403.952781883446
INFO:root:current train perplexity3.431354522705078
INFO:root:current mean train loss 1400.9014106838183
INFO:root:current train perplexity3.418584108352661
INFO:root:current mean train loss 1398.8466337997547
INFO:root:current train perplexity3.415476083755493
INFO:root:current mean train loss 1396.9322992824616
INFO:root:current train perplexity3.414177894592285
INFO:root:current mean train loss 1397.11789030566
INFO:root:current train perplexity3.418851852416992
INFO:root:current mean train loss 1395.3923703502694
INFO:root:current train perplexity3.4175515174865723
INFO:root:current mean train loss 1398.4498636941883
INFO:root:current train perplexity3.419780731201172
INFO:root:current mean train loss 1401.0954709706073
INFO:root:current train perplexity3.421515464782715
INFO:root:current mean train loss 1402.672560912248
INFO:root:current train perplexity3.425487756729126
INFO:root:current mean train loss 1402.8442343960553
INFO:root:current train perplexity3.4247543811798096
INFO:root:current mean train loss 1402.4108668492493
INFO:root:current train perplexity3.4220995903015137
INFO:root:current mean train loss 1403.336778371054
INFO:root:current train perplexity3.4243342876434326
INFO:root:current mean train loss 1404.1886292941156
INFO:root:current train perplexity3.424656391143799
INFO:root:current mean train loss 1404.8251238827102
INFO:root:current train perplexity3.4273457527160645
INFO:root:current mean train loss 1405.0744684392755
INFO:root:current train perplexity3.4282147884368896
INFO:root:current mean train loss 1405.1862743334061
INFO:root:current train perplexity3.4286322593688965
INFO:root:current mean train loss 1404.6399350183412
INFO:root:current train perplexity3.4297444820404053
INFO:root:current mean train loss 1405.0675791296374
INFO:root:current train perplexity3.4312288761138916
INFO:root:current mean train loss 1405.3976781106173
INFO:root:current train perplexity3.431114912033081
INFO:root:current mean train loss 1405.267048473416
INFO:root:current train perplexity3.432661771774292

100%|██████████| 1/1 [07:53<00:00, 473.53s/it][A100%|██████████| 1/1 [07:53<00:00, 473.53s/it]
INFO:root:final mean train loss: 1405.079711083024
INFO:root:final train perplexity: 3.4330804347991943
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.42s/it][A100%|██████████| 1/1 [00:42<00:00, 42.44s/it]
INFO:root:eval mean loss: 1851.8405869937112
INFO:root:eval perplexity: 5.320524215698242
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.68s/it][A100%|██████████| 1/1 [00:40<00:00, 40.69s/it]
INFO:root:eval mean loss: 2342.3144145992633
INFO:root:eval perplexity: 8.600172996520996
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/76
 38%|███▊      | 76/200 [12:15:16<19:48:46, 575.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.9010184151787
INFO:root:current train perplexity3.389895439147949
INFO:root:current mean train loss 1393.8598651985847
INFO:root:current train perplexity3.401967763900757
INFO:root:current mean train loss 1394.230851740362
INFO:root:current train perplexity3.4062070846557617
INFO:root:current mean train loss 1392.3994989809783
INFO:root:current train perplexity3.4051625728607178
INFO:root:current mean train loss 1395.497918340679
INFO:root:current train perplexity3.4076473712921143
INFO:root:current mean train loss 1394.9211113892634
INFO:root:current train perplexity3.4061150550842285
INFO:root:current mean train loss 1393.3830856124503
INFO:root:current train perplexity3.405975818634033
INFO:root:current mean train loss 1394.586571154492
INFO:root:current train perplexity3.4070241451263428
INFO:root:current mean train loss 1394.891073961183
INFO:root:current train perplexity3.4091808795928955
INFO:root:current mean train loss 1395.8569885315496
INFO:root:current train perplexity3.4105477333068848
INFO:root:current mean train loss 1395.6589647497638
INFO:root:current train perplexity3.4111971855163574
INFO:root:current mean train loss 1397.197659714296
INFO:root:current train perplexity3.412435531616211
INFO:root:current mean train loss 1399.270123541586
INFO:root:current train perplexity3.4156792163848877
INFO:root:current mean train loss 1400.1401127610252
INFO:root:current train perplexity3.415783643722534
INFO:root:current mean train loss 1399.9223500180772
INFO:root:current train perplexity3.4170730113983154
INFO:root:current mean train loss 1401.3467122191232
INFO:root:current train perplexity3.4186692237854004
INFO:root:current mean train loss 1401.3790930009748
INFO:root:current train perplexity3.4190871715545654
INFO:root:current mean train loss 1401.5969723563564
INFO:root:current train perplexity3.420895576477051
INFO:root:current mean train loss 1401.8430058294223
INFO:root:current train perplexity3.4214465618133545

100%|██████████| 1/1 [08:06<00:00, 486.88s/it][A100%|██████████| 1/1 [08:06<00:00, 486.88s/it]
INFO:root:final mean train loss: 1401.6562977693204
INFO:root:final train perplexity: 3.422778606414795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.38s/it][A100%|██████████| 1/1 [00:42<00:00, 42.40s/it]
INFO:root:eval mean loss: 1854.296988845717
INFO:root:eval perplexity: 5.332332611083984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.51s/it][A100%|██████████| 1/1 [00:40<00:00, 40.51s/it]
INFO:root:eval mean loss: 2346.1852875838044
INFO:root:eval perplexity: 8.630809783935547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_roberta_gpt2/77
 38%|███▊      | 77/200 [12:24:48<19:37:18, 574.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 26219444 ON ga004 CANCELLED AT 2022-10-24T11:44:59 ***
