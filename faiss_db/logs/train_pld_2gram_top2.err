INFO:root:Output: pld_22
INFO:root:Steps per epochs:1983
INFO:root:Total steps:198300
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.decoder.weight', 'encoder.layer.0.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'cls.predictions.bias', 'encoder.layer.2.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/100 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 12228.761827256945
INFO:root:current train perplexity16576.244140625
INFO:root:current mean train loss 10543.044161235866
INFO:root:current train perplexity4108.59765625
INFO:root:current mean train loss 9172.639305497492
INFO:root:current train perplexity1383.0350341796875
INFO:root:current mean train loss 8225.111154350721
INFO:root:current train perplexity654.4259643554688
INFO:root:current mean train loss 7532.585866068073
INFO:root:current train perplexity379.795654296875
INFO:root:current mean train loss 7006.952090561092
INFO:root:current train perplexity250.86387634277344
INFO:root:current mean train loss 6597.715545785273
INFO:root:current train perplexity181.0919952392578
INFO:root:current mean train loss 6272.371719837785
INFO:root:current train perplexity139.6580047607422
INFO:root:current mean train loss 5995.657815595888
INFO:root:current train perplexity112.84175872802734
INFO:root:current mean train loss 5771.794359056322
INFO:root:current train perplexity94.08992767333984
INFO:root:current mean train loss 5572.646618552377
INFO:root:current train perplexity80.60281372070312
INFO:root:current mean train loss 5403.190026723116
INFO:root:current train perplexity70.5628662109375
INFO:root:current mean train loss 5256.656562552625
INFO:root:current train perplexity62.69076156616211
INFO:root:current mean train loss 5120.943494097346
INFO:root:current train perplexity56.47312545776367
INFO:root:current mean train loss 5002.2035740298215
INFO:root:current train perplexity51.49870681762695
INFO:root:current mean train loss 4895.535150295351
INFO:root:current train perplexity47.37029266357422
INFO:root:current mean train loss 4799.996841116327
INFO:root:current train perplexity43.90169143676758
INFO:root:current mean train loss 4711.477960439176
INFO:root:current train perplexity40.98872756958008
INFO:root:current mean train loss 4628.73730327331
INFO:root:current train perplexity38.45895767211914

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.04s/it]
INFO:root:final mean train loss: 4564.40337042104
INFO:root:final train perplexity: 36.589569091796875
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:28<00:00, 28.91s/it]
INFO:root:eval mean loss: 3475.844120976445
INFO:root:eval perplexity: 17.325576782226562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/1
  1%|          | 1/100 [04:58<8:12:06, 298.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3086.0945587158203
INFO:root:current train perplexity11.320915222167969
INFO:root:current mean train loss 3087.507907209725
INFO:root:current train perplexity11.437697410583496
INFO:root:current mean train loss 3067.6555051450378
INFO:root:current train perplexity11.357008934020996
INFO:root:current mean train loss 3065.8102069323577
INFO:root:current train perplexity11.332111358642578
INFO:root:current mean train loss 3059.271645766038
INFO:root:current train perplexity11.242206573486328
INFO:root:current mean train loss 3049.4087817761324
INFO:root:current train perplexity11.105359077453613
INFO:root:current mean train loss 3031.056527670328
INFO:root:current train perplexity10.972288131713867
INFO:root:current mean train loss 3019.5249207565903
INFO:root:current train perplexity10.867166519165039
INFO:root:current mean train loss 3010.3500467936196
INFO:root:current train perplexity10.787784576416016
INFO:root:current mean train loss 3003.986961131533
INFO:root:current train perplexity10.70903491973877
INFO:root:current mean train loss 2990.9114550492895
INFO:root:current train perplexity10.618985176086426
INFO:root:current mean train loss 2981.9300272404935
INFO:root:current train perplexity10.523077011108398
INFO:root:current mean train loss 2974.5408028050474
INFO:root:current train perplexity10.446637153625488
INFO:root:current mean train loss 2965.2053411883785
INFO:root:current train perplexity10.3623628616333
INFO:root:current mean train loss 2955.799941413147
INFO:root:current train perplexity10.288276672363281
INFO:root:current mean train loss 2947.4170411444593
INFO:root:current train perplexity10.213785171508789
INFO:root:current mean train loss 2937.8747890963414
INFO:root:current train perplexity10.13963508605957
INFO:root:current mean train loss 2929.9945005759214
INFO:root:current train perplexity10.074748039245605
INFO:root:current mean train loss 2919.4321160001377
INFO:root:current train perplexity10.001972198486328
INFO:root:current mean train loss 2910.9358438957706
INFO:root:current train perplexity9.928441047668457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.62s/it]
INFO:root:final mean train loss: 2904.8284023821625
INFO:root:final train perplexity: 9.884053230285645
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.23s/it]
INFO:root:eval mean loss: 3238.5341672238646
INFO:root:eval perplexity: 14.259930610656738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/2
  2%|â–         | 2/100 [09:56<8:07:10, 298.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2769.8855498342805
INFO:root:current train perplexity8.862183570861816
INFO:root:current mean train loss 2718.9400405310153
INFO:root:current train perplexity8.607181549072266
INFO:root:current mean train loss 2716.1784322190183
INFO:root:current train perplexity8.568958282470703
INFO:root:current mean train loss 2702.947689388607
INFO:root:current train perplexity8.493468284606934
INFO:root:current mean train loss 2704.307084363272
INFO:root:current train perplexity8.453010559082031
INFO:root:current mean train loss 2699.2010601108113
INFO:root:current train perplexity8.404500007629395
INFO:root:current mean train loss 2694.157392022734
INFO:root:current train perplexity8.36167049407959
INFO:root:current mean train loss 2687.3161214747824
INFO:root:current train perplexity8.320472717285156
INFO:root:current mean train loss 2682.05961046528
INFO:root:current train perplexity8.285429000854492
INFO:root:current mean train loss 2674.8805796468882
INFO:root:current train perplexity8.247518539428711
INFO:root:current mean train loss 2669.3417401530737
INFO:root:current train perplexity8.212302207946777
INFO:root:current mean train loss 2663.4218413848744
INFO:root:current train perplexity8.182928085327148
INFO:root:current mean train loss 2657.207515967153
INFO:root:current train perplexity8.146841049194336
INFO:root:current mean train loss 2649.9848385558303
INFO:root:current train perplexity8.104283332824707
INFO:root:current mean train loss 2648.1485549703148
INFO:root:current train perplexity8.082256317138672
INFO:root:current mean train loss 2645.9917884821684
INFO:root:current train perplexity8.053621292114258
INFO:root:current mean train loss 2640.856754412771
INFO:root:current train perplexity8.021136283874512
INFO:root:current mean train loss 2636.5681922239028
INFO:root:current train perplexity7.9889678955078125
INFO:root:current mean train loss 2631.423335781399
INFO:root:current train perplexity7.952539920806885
INFO:root:current mean train loss 2625.750105651129
INFO:root:current train perplexity7.9216790199279785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.35s/it]
INFO:root:final mean train loss: 2621.486198052095
INFO:root:final train perplexity: 7.904735088348389
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 3164.5076754000092
INFO:root:eval perplexity: 13.41950511932373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/3
  3%|â–Ž         | 3/100 [14:55<8:03:02, 298.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2527.7419775390626
INFO:root:current train perplexity7.300004482269287
INFO:root:current mean train loss 2515.9297770182293
INFO:root:current train perplexity7.266881465911865
INFO:root:current mean train loss 2508.44303125
INFO:root:current train perplexity7.241140842437744
INFO:root:current mean train loss 2500.887220982143
INFO:root:current train perplexity7.187216758728027
INFO:root:current mean train loss 2504.2383132595487
INFO:root:current train perplexity7.176339626312256
INFO:root:current mean train loss 2495.896500355114
INFO:root:current train perplexity7.136981010437012
INFO:root:current mean train loss 2491.166844576322
INFO:root:current train perplexity7.116553783416748
INFO:root:current mean train loss 2487.497448079427
INFO:root:current train perplexity7.098750114440918
INFO:root:current mean train loss 2484.3980111155793
INFO:root:current train perplexity7.090059757232666
INFO:root:current mean train loss 2480.9010514751235
INFO:root:current train perplexity7.068458080291748
INFO:root:current mean train loss 2476.4522424897696
INFO:root:current train perplexity7.045144557952881
INFO:root:current mean train loss 2475.060566087806
INFO:root:current train perplexity7.033499240875244
INFO:root:current mean train loss 2471.1015794921873
INFO:root:current train perplexity7.015934944152832
INFO:root:current mean train loss 2467.9086100260415
INFO:root:current train perplexity6.997170448303223
INFO:root:current mean train loss 2466.439832974138
INFO:root:current train perplexity6.987290382385254
INFO:root:current mean train loss 2462.9092337922125
INFO:root:current train perplexity6.97052526473999
INFO:root:current mean train loss 2460.6952615263967
INFO:root:current train perplexity6.956502914428711
INFO:root:current mean train loss 2457.0084017159597
INFO:root:current train perplexity6.935529708862305
INFO:root:current mean train loss 2454.8967960172085
INFO:root:current train perplexity6.9239182472229
INFO:root:current mean train loss 2452.004694949419
INFO:root:current train perplexity6.910523414611816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.95s/it]
INFO:root:final mean train loss: 2450.2558400456614
INFO:root:final train perplexity: 6.906197547912598
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it]
INFO:root:eval mean loss: 3146.090354759056
INFO:root:eval perplexity: 13.218229293823242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/4
  4%|â–         | 4/100 [19:55<7:58:46, 299.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2414.955992741371
INFO:root:current train perplexity6.584133625030518
INFO:root:current mean train loss 2377.707652565962
INFO:root:current train perplexity6.493829250335693
INFO:root:current mean train loss 2374.533747183696
INFO:root:current train perplexity6.496562957763672
INFO:root:current mean train loss 2373.373576733332
INFO:root:current train perplexity6.497097492218018
INFO:root:current mean train loss 2374.13318106347
INFO:root:current train perplexity6.505749702453613
INFO:root:current mean train loss 2368.3821003155313
INFO:root:current train perplexity6.471138000488281
INFO:root:current mean train loss 2368.565558529329
INFO:root:current train perplexity6.46536922454834
INFO:root:current mean train loss 2364.600385723089
INFO:root:current train perplexity6.447103977203369
INFO:root:current mean train loss 2364.175927255668
INFO:root:current train perplexity6.434261798858643
INFO:root:current mean train loss 2359.9841226540284
INFO:root:current train perplexity6.4165778160095215
INFO:root:current mean train loss 2358.7069337767985
INFO:root:current train perplexity6.408780574798584
INFO:root:current mean train loss 2357.178007594928
INFO:root:current train perplexity6.397776126861572
INFO:root:current mean train loss 2353.529707983148
INFO:root:current train perplexity6.383942604064941
INFO:root:current mean train loss 2352.671434850351
INFO:root:current train perplexity6.377382278442383
INFO:root:current mean train loss 2349.631594376411
INFO:root:current train perplexity6.3699049949646
INFO:root:current mean train loss 2346.760952051841
INFO:root:current train perplexity6.362110614776611
INFO:root:current mean train loss 2342.6827912493673
INFO:root:current train perplexity6.345025062561035
INFO:root:current mean train loss 2340.826829203253
INFO:root:current train perplexity6.3325395584106445
INFO:root:current mean train loss 2338.220692794465
INFO:root:current train perplexity6.320199966430664
INFO:root:current mean train loss 2335.692887911775
INFO:root:current train perplexity6.305949687957764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.83s/it]
INFO:root:final mean train loss: 2334.60547607545
INFO:root:final train perplexity: 6.304163455963135
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.48s/it]
INFO:root:eval mean loss: 3130.667531056447
INFO:root:eval perplexity: 13.051996231079102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/5
  5%|â–Œ         | 5/100 [24:55<7:54:05, 299.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2284.8732067289807
INFO:root:current train perplexity6.010538578033447
INFO:root:current mean train loss 2280.058825948964
INFO:root:current train perplexity6.024328708648682
INFO:root:current mean train loss 2277.1457184267715
INFO:root:current train perplexity6.038846969604492
INFO:root:current mean train loss 2277.4075816472373
INFO:root:current train perplexity6.037190914154053
INFO:root:current mean train loss 2282.618240482551
INFO:root:current train perplexity6.046113967895508
INFO:root:current mean train loss 2275.5475873555224
INFO:root:current train perplexity6.012948989868164
INFO:root:current mean train loss 2272.9228358575474
INFO:root:current train perplexity5.991352558135986
INFO:root:current mean train loss 2272.278001746353
INFO:root:current train perplexity5.993650436401367
INFO:root:current mean train loss 2270.1462297396424
INFO:root:current train perplexity5.980921745300293
INFO:root:current mean train loss 2265.494532515363
INFO:root:current train perplexity5.962458610534668
INFO:root:current mean train loss 2264.1766547734446
INFO:root:current train perplexity5.958999156951904
INFO:root:current mean train loss 2262.11616938823
INFO:root:current train perplexity5.95029878616333
INFO:root:current mean train loss 2260.182986345618
INFO:root:current train perplexity5.941990375518799
INFO:root:current mean train loss 2259.645064954813
INFO:root:current train perplexity5.9367899894714355
INFO:root:current mean train loss 2259.517704883997
INFO:root:current train perplexity5.936158657073975
INFO:root:current mean train loss 2258.2409244113496
INFO:root:current train perplexity5.926601409912109
INFO:root:current mean train loss 2257.0967924068204
INFO:root:current train perplexity5.920155048370361
INFO:root:current mean train loss 2253.5050544225583
INFO:root:current train perplexity5.91152286529541
INFO:root:current mean train loss 2252.5504617549304
INFO:root:current train perplexity5.9040422439575195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.28s/it]
INFO:root:final mean train loss: 2249.7289269582466
INFO:root:final train perplexity: 5.895985126495361
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.33s/it]
INFO:root:eval mean loss: 3150.1145708696977
INFO:root:eval perplexity: 13.26195240020752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/6
  6%|â–Œ         | 6/100 [29:55<7:49:26, 299.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2310.45947265625
INFO:root:current train perplexity5.440577507019043
INFO:root:current mean train loss 2183.304718924041
INFO:root:current train perplexity5.6100287437438965
INFO:root:current mean train loss 2194.9538149098257
INFO:root:current train perplexity5.660536766052246
INFO:root:current mean train loss 2203.501422666632
INFO:root:current train perplexity5.672144412994385
INFO:root:current mean train loss 2199.4595283641484
INFO:root:current train perplexity5.663606643676758
INFO:root:current mean train loss 2196.3598598701037
INFO:root:current train perplexity5.661659240722656
INFO:root:current mean train loss 2193.157478015157
INFO:root:current train perplexity5.650925636291504
INFO:root:current mean train loss 2194.9344957816957
INFO:root:current train perplexity5.652096271514893
INFO:root:current mean train loss 2194.8361659436932
INFO:root:current train perplexity5.6499152183532715
INFO:root:current mean train loss 2193.953225934942
INFO:root:current train perplexity5.643449306488037
INFO:root:current mean train loss 2190.7997991998236
INFO:root:current train perplexity5.633018493652344
INFO:root:current mean train loss 2189.1044198988134
INFO:root:current train perplexity5.623239994049072
INFO:root:current mean train loss 2188.596863148711
INFO:root:current train perplexity5.619812488555908
INFO:root:current mean train loss 2187.8680013646353
INFO:root:current train perplexity5.617550849914551
INFO:root:current mean train loss 2187.3137076334983
INFO:root:current train perplexity5.615667819976807
INFO:root:current mean train loss 2187.4811927139717
INFO:root:current train perplexity5.611639499664307
INFO:root:current mean train loss 2185.9833143378405
INFO:root:current train perplexity5.605120658874512
INFO:root:current mean train loss 2185.5202765176045
INFO:root:current train perplexity5.60380744934082
INFO:root:current mean train loss 2185.0511320072833
INFO:root:current train perplexity5.5991997718811035
INFO:root:current mean train loss 2184.6116682651605
INFO:root:current train perplexity5.597126483917236

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.18s/it]
INFO:root:final mean train loss: 2183.044742093685
INFO:root:final train perplexity: 5.59391975402832
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 3148.395498281485
INFO:root:eval perplexity: 13.243252754211426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/7
  7%|â–‹         | 7/100 [34:55<7:44:46, 299.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2136.5328504774307
INFO:root:current train perplexity5.378752708435059
INFO:root:current mean train loss 2129.2979798397773
INFO:root:current train perplexity5.420951843261719
INFO:root:current mean train loss 2144.7492401403024
INFO:root:current train perplexity5.458853721618652
INFO:root:current mean train loss 2147.8156216219536
INFO:root:current train perplexity5.457690715789795
INFO:root:current mean train loss 2146.1701744846177
INFO:root:current train perplexity5.433563709259033
INFO:root:current mean train loss 2145.429373133597
INFO:root:current train perplexity5.427840232849121
INFO:root:current mean train loss 2143.6701034002704
INFO:root:current train perplexity5.415911674499512
INFO:root:current mean train loss 2144.297717591182
INFO:root:current train perplexity5.417454719543457
INFO:root:current mean train loss 2142.3015688870532
INFO:root:current train perplexity5.405254364013672
INFO:root:current mean train loss 2140.807908799913
INFO:root:current train perplexity5.401796340942383
INFO:root:current mean train loss 2138.1947871659736
INFO:root:current train perplexity5.394341468811035
INFO:root:current mean train loss 2138.2209482483017
INFO:root:current train perplexity5.388731002807617
INFO:root:current mean train loss 2137.758905620606
INFO:root:current train perplexity5.384532451629639
INFO:root:current mean train loss 2138.0576596064707
INFO:root:current train perplexity5.388769149780273
INFO:root:current mean train loss 2137.809013850934
INFO:root:current train perplexity5.384987831115723
INFO:root:current mean train loss 2137.6132839037023
INFO:root:current train perplexity5.380800247192383
INFO:root:current mean train loss 2135.5358200167548
INFO:root:current train perplexity5.374532699584961
INFO:root:current mean train loss 2133.1188456809564
INFO:root:current train perplexity5.365984916687012
INFO:root:current mean train loss 2131.8805977765746
INFO:root:current train perplexity5.36427640914917
INFO:root:current mean train loss 2129.308232821563
INFO:root:current train perplexity5.359250068664551

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.78s/it]
INFO:root:final mean train loss: 2128.736882150624
INFO:root:final train perplexity: 5.35938835144043
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 3137.564771314283
INFO:root:eval perplexity: 13.12607479095459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/8
  8%|â–Š         | 8/100 [40:13<7:48:32, 305.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2092.398095703125
INFO:root:current train perplexity5.184288024902344
INFO:root:current mean train loss 2098.4561315465858
INFO:root:current train perplexity5.194396018981934
INFO:root:current mean train loss 2089.6968157829124
INFO:root:current train perplexity5.192260265350342
INFO:root:current mean train loss 2092.156608923158
INFO:root:current train perplexity5.206761837005615
INFO:root:current mean train loss 2090.0219586251796
INFO:root:current train perplexity5.212362289428711
INFO:root:current mean train loss 2088.365057087836
INFO:root:current train perplexity5.199651718139648
INFO:root:current mean train loss 2090.868864380844
INFO:root:current train perplexity5.196493148803711
INFO:root:current mean train loss 2092.829880487351
INFO:root:current train perplexity5.198962688446045
INFO:root:current mean train loss 2093.226301401104
INFO:root:current train perplexity5.202209949493408
INFO:root:current mean train loss 2095.02816547251
INFO:root:current train perplexity5.205413818359375
INFO:root:current mean train loss 2093.315651772909
INFO:root:current train perplexity5.2026238441467285
INFO:root:current mean train loss 2089.8495007485544
INFO:root:current train perplexity5.193077564239502
INFO:root:current mean train loss 2087.6451220307754
INFO:root:current train perplexity5.188193321228027
INFO:root:current mean train loss 2088.20820778836
INFO:root:current train perplexity5.189154624938965
INFO:root:current mean train loss 2087.637782250381
INFO:root:current train perplexity5.18717622756958
INFO:root:current mean train loss 2088.000018847338
INFO:root:current train perplexity5.1848835945129395
INFO:root:current mean train loss 2086.9026040173453
INFO:root:current train perplexity5.182663440704346
INFO:root:current mean train loss 2085.301418689211
INFO:root:current train perplexity5.177099227905273
INFO:root:current mean train loss 2083.7281198777036
INFO:root:current train perplexity5.171645164489746
INFO:root:current mean train loss 2083.7289948219477
INFO:root:current train perplexity5.171280860900879

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.36s/it]
INFO:root:final mean train loss: 2083.993748159404
INFO:root:final train perplexity: 5.173570156097412
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.38s/it]
INFO:root:eval mean loss: 3144.2403186878287
INFO:root:eval perplexity: 13.198175430297852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/9
  9%|â–‰         | 9/100 [45:14<7:41:22, 304.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2034.242182804988
INFO:root:current train perplexity5.006889343261719
INFO:root:current mean train loss 2043.3172310277034
INFO:root:current train perplexity4.9812235832214355
INFO:root:current mean train loss 2053.9687064034597
INFO:root:current train perplexity5.027670383453369
INFO:root:current mean train loss 2048.656540263783
INFO:root:current train perplexity5.029294490814209
INFO:root:current mean train loss 2054.620942512445
INFO:root:current train perplexity5.042940139770508
INFO:root:current mean train loss 2056.7923431396484
INFO:root:current train perplexity5.049798965454102
INFO:root:current mean train loss 2059.5191564267398
INFO:root:current train perplexity5.056537628173828
INFO:root:current mean train loss 2055.8208543493392
INFO:root:current train perplexity5.047101974487305
INFO:root:current mean train loss 2054.9100379048377
INFO:root:current train perplexity5.046868801116943
INFO:root:current mean train loss 2051.6534175071397
INFO:root:current train perplexity5.039977550506592
INFO:root:current mean train loss 2052.5128190073224
INFO:root:current train perplexity5.041966438293457
INFO:root:current mean train loss 2051.897692574395
INFO:root:current train perplexity5.035872936248779
INFO:root:current mean train loss 2051.348817282972
INFO:root:current train perplexity5.034877300262451
INFO:root:current mean train loss 2051.695176525229
INFO:root:current train perplexity5.038859844207764
INFO:root:current mean train loss 2050.1034502181797
INFO:root:current train perplexity5.0337677001953125
INFO:root:current mean train loss 2047.3415467567052
INFO:root:current train perplexity5.025991439819336
INFO:root:current mean train loss 2046.7415103496708
INFO:root:current train perplexity5.021075248718262
INFO:root:current mean train loss 2048.0417949380394
INFO:root:current train perplexity5.023731231689453
INFO:root:current mean train loss 2046.2531267664604
INFO:root:current train perplexity5.019144058227539
INFO:root:current mean train loss 2046.2354968336763
INFO:root:current train perplexity5.018815040588379

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.61s/it]
INFO:root:final mean train loss: 2045.4098991513313
INFO:root:final train perplexity: 5.0185112953186035
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 3148.1850629926803
INFO:root:eval perplexity: 13.24096393585205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/10
 10%|â–ˆ         | 10/100 [50:15<7:34:42, 303.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2030.748628920403
INFO:root:current train perplexity4.916243076324463
INFO:root:current mean train loss 2032.738273304595
INFO:root:current train perplexity4.906794548034668
INFO:root:current mean train loss 2014.9292137401255
INFO:root:current train perplexity4.872582912445068
INFO:root:current mean train loss 2013.9150420398246
INFO:root:current train perplexity4.8761701583862305
INFO:root:current mean train loss 2015.0752687108543
INFO:root:current train perplexity4.8781352043151855
INFO:root:current mean train loss 2018.9281482126676
INFO:root:current train perplexity4.889784812927246
INFO:root:current mean train loss 2014.658329027116
INFO:root:current train perplexity4.881712436676025
INFO:root:current mean train loss 2016.3866054103341
INFO:root:current train perplexity4.892393589019775
INFO:root:current mean train loss 2017.5908496711827
INFO:root:current train perplexity4.8890767097473145
INFO:root:current mean train loss 2016.748746795182
INFO:root:current train perplexity4.8900837898254395
INFO:root:current mean train loss 2015.2620766249854
INFO:root:current train perplexity4.89089822769165
INFO:root:current mean train loss 2015.1075825817672
INFO:root:current train perplexity4.890703201293945
INFO:root:current mean train loss 2013.941820269405
INFO:root:current train perplexity4.885906219482422
INFO:root:current mean train loss 2014.557022351083
INFO:root:current train perplexity4.888266086578369
INFO:root:current mean train loss 2015.3155728645922
INFO:root:current train perplexity4.889603137969971
INFO:root:current mean train loss 2013.3828903791477
INFO:root:current train perplexity4.886048316955566
INFO:root:current mean train loss 2012.3548306053167
INFO:root:current train perplexity4.884302139282227
INFO:root:current mean train loss 2010.9114844633268
INFO:root:current train perplexity4.881718158721924
INFO:root:current mean train loss 2010.0719442188963
INFO:root:current train perplexity4.8808913230896
INFO:root:current mean train loss 2011.5799452053707
INFO:root:current train perplexity4.884695053100586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.11s/it]
INFO:root:final mean train loss: 2011.2030785849645
INFO:root:final train perplexity: 4.884933948516846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.00s/it]
INFO:root:eval mean loss: 3156.080564206785
INFO:root:eval perplexity: 13.327033996582031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/11
 11%|â–ˆ         | 11/100 [55:16<7:28:30, 302.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2013.3548782703488
INFO:root:current train perplexity4.830019950866699
INFO:root:current mean train loss 1982.8939537130377
INFO:root:current train perplexity4.775053977966309
INFO:root:current mean train loss 1983.1255890105988
INFO:root:current train perplexity4.771252632141113
INFO:root:current mean train loss 1992.191994148215
INFO:root:current train perplexity4.796381950378418
INFO:root:current mean train loss 1986.7422605914835
INFO:root:current train perplexity4.777042865753174
INFO:root:current mean train loss 1985.58927318586
INFO:root:current train perplexity4.770376682281494
INFO:root:current mean train loss 1986.8824829457453
INFO:root:current train perplexity4.774555683135986
INFO:root:current mean train loss 1986.5151691776498
INFO:root:current train perplexity4.7792134284973145
INFO:root:current mean train loss 1984.0837672386428
INFO:root:current train perplexity4.775442123413086
INFO:root:current mean train loss 1984.113014453323
INFO:root:current train perplexity4.77822208404541
INFO:root:current mean train loss 1980.8622771486173
INFO:root:current train perplexity4.770692348480225
INFO:root:current mean train loss 1983.6926740932624
INFO:root:current train perplexity4.77484655380249
INFO:root:current mean train loss 1982.2606769757544
INFO:root:current train perplexity4.772250175476074
INFO:root:current mean train loss 1982.3704197210666
INFO:root:current train perplexity4.77335786819458
INFO:root:current mean train loss 1981.9540805328902
INFO:root:current train perplexity4.771846771240234
INFO:root:current mean train loss 1981.56488999202
INFO:root:current train perplexity4.769620418548584
INFO:root:current mean train loss 1981.494436678089
INFO:root:current train perplexity4.769192218780518
INFO:root:current mean train loss 1981.5507940995067
INFO:root:current train perplexity4.77014684677124
INFO:root:current mean train loss 1981.461956651188
INFO:root:current train perplexity4.769660472869873

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.33s/it]
INFO:root:final mean train loss: 1981.0714933440593
INFO:root:final train perplexity: 4.770219326019287
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.53s/it]
INFO:root:eval mean loss: 3169.2510872689095
INFO:root:eval perplexity: 13.471844673156738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/12
 12%|â–ˆâ–        | 12/100 [1:00:17<7:23:00, 302.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1854.3543701171875
INFO:root:current train perplexity4.380578994750977
INFO:root:current mean train loss 1951.736431232934
INFO:root:current train perplexity4.653078556060791
INFO:root:current mean train loss 1960.3691622729373
INFO:root:current train perplexity4.685442924499512
INFO:root:current mean train loss 1956.1286347140574
INFO:root:current train perplexity4.6797590255737305
INFO:root:current mean train loss 1951.394551544568
INFO:root:current train perplexity4.674025058746338
INFO:root:current mean train loss 1955.3093227742918
INFO:root:current train perplexity4.68540096282959
INFO:root:current mean train loss 1953.2381751723156
INFO:root:current train perplexity4.682608604431152
INFO:root:current mean train loss 1951.2736387510558
INFO:root:current train perplexity4.677030086517334
INFO:root:current mean train loss 1950.9855642354355
INFO:root:current train perplexity4.672808647155762
INFO:root:current mean train loss 1954.6624659879394
INFO:root:current train perplexity4.67888069152832
INFO:root:current mean train loss 1954.1443976176938
INFO:root:current train perplexity4.676056385040283
INFO:root:current mean train loss 1952.7765536684399
INFO:root:current train perplexity4.672176361083984
INFO:root:current mean train loss 1951.3040533026158
INFO:root:current train perplexity4.67193603515625
INFO:root:current mean train loss 1950.0955448926456
INFO:root:current train perplexity4.668697357177734
INFO:root:current mean train loss 1950.280581440997
INFO:root:current train perplexity4.667781352996826
INFO:root:current mean train loss 1950.9314977629376
INFO:root:current train perplexity4.665207386016846
INFO:root:current mean train loss 1952.8234330984628
INFO:root:current train perplexity4.666508674621582
INFO:root:current mean train loss 1951.8047172470226
INFO:root:current train perplexity4.665148735046387
INFO:root:current mean train loss 1952.8631764442075
INFO:root:current train perplexity4.667557716369629
INFO:root:current mean train loss 1953.8037963161578
INFO:root:current train perplexity4.6691575050354

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.28s/it]
INFO:root:final mean train loss: 1954.1547378178384
INFO:root:final train perplexity: 4.670023441314697
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 3154.4064281566725
INFO:root:eval perplexity: 13.308735847473145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/13
 13%|â–ˆâ–Ž        | 13/100 [1:05:19<7:17:40, 301.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1992.3802795410156
INFO:root:current train perplexity4.641422271728516
INFO:root:current mean train loss 1936.216362508138
INFO:root:current train perplexity4.571415424346924
INFO:root:current mean train loss 1943.5650446111506
INFO:root:current train perplexity4.599400043487549
INFO:root:current mean train loss 1933.7635093688964
INFO:root:current train perplexity4.58189582824707
INFO:root:current mean train loss 1935.6070292154948
INFO:root:current train perplexity4.578860282897949
INFO:root:current mean train loss 1935.6297393798827
INFO:root:current train perplexity4.588339328765869
INFO:root:current mean train loss 1935.7535790228076
INFO:root:current train perplexity4.5885114669799805
INFO:root:current mean train loss 1936.3423216078018
INFO:root:current train perplexity4.589047908782959
INFO:root:current mean train loss 1936.7667645710271
INFO:root:current train perplexity4.590713977813721
INFO:root:current mean train loss 1936.9010382610818
INFO:root:current train perplexity4.59005880355835
INFO:root:current mean train loss 1934.9436864516315
INFO:root:current train perplexity4.589778423309326
INFO:root:current mean train loss 1935.4116431100028
INFO:root:current train perplexity4.591695785522461
INFO:root:current mean train loss 1932.8030776727394
INFO:root:current train perplexity4.590262413024902
INFO:root:current mean train loss 1931.7315404718572
INFO:root:current train perplexity4.589469909667969
INFO:root:current mean train loss 1931.1110030913017
INFO:root:current train perplexity4.58439826965332
INFO:root:current mean train loss 1929.8506371749074
INFO:root:current train perplexity4.581840515136719
INFO:root:current mean train loss 1929.7094483175395
INFO:root:current train perplexity4.579897880554199
INFO:root:current mean train loss 1929.2168796982876
INFO:root:current train perplexity4.577820301055908
INFO:root:current mean train loss 1929.5386612776872
INFO:root:current train perplexity4.579406261444092
INFO:root:current mean train loss 1930.377824083964
INFO:root:current train perplexity4.580138683319092

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.02s/it]
INFO:root:final mean train loss: 1929.3311983032534
INFO:root:final train perplexity: 4.579485893249512
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 3177.259860201999
INFO:root:eval perplexity: 13.560667037963867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/14
 14%|â–ˆâ–        | 14/100 [1:10:20<7:12:21, 301.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1887.776195629223
INFO:root:current train perplexity4.455186367034912
INFO:root:current mean train loss 1887.8187050923814
INFO:root:current train perplexity4.477201461791992
INFO:root:current mean train loss 1902.9532481004417
INFO:root:current train perplexity4.499606132507324
INFO:root:current mean train loss 1899.1230508594908
INFO:root:current train perplexity4.492903232574463
INFO:root:current mean train loss 1898.27551101929
INFO:root:current train perplexity4.499714374542236
INFO:root:current mean train loss 1901.1331057415327
INFO:root:current train perplexity4.500993728637695
INFO:root:current mean train loss 1900.2298583984375
INFO:root:current train perplexity4.492744445800781
INFO:root:current mean train loss 1897.7880168692227
INFO:root:current train perplexity4.490899562835693
INFO:root:current mean train loss 1898.7783623151881
INFO:root:current train perplexity4.491372108459473
INFO:root:current mean train loss 1900.3633995422558
INFO:root:current train perplexity4.4961934089660645
INFO:root:current mean train loss 1900.9969314089622
INFO:root:current train perplexity4.497981071472168
INFO:root:current mean train loss 1903.658988368857
INFO:root:current train perplexity4.500921726226807
INFO:root:current mean train loss 1905.4505278726886
INFO:root:current train perplexity4.504482746124268
INFO:root:current mean train loss 1905.527127365078
INFO:root:current train perplexity4.504286766052246
INFO:root:current mean train loss 1905.7434206904848
INFO:root:current train perplexity4.500683784484863
INFO:root:current mean train loss 1906.6931585983248
INFO:root:current train perplexity4.50489616394043
INFO:root:current mean train loss 1906.7498659985636
INFO:root:current train perplexity4.5040764808654785
INFO:root:current mean train loss 1907.3320611877969
INFO:root:current train perplexity4.501330375671387
INFO:root:current mean train loss 1907.458072801485
INFO:root:current train perplexity4.500231742858887
INFO:root:current mean train loss 1908.4548813126169
INFO:root:current train perplexity4.501659870147705

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it]
INFO:root:final mean train loss: 1907.3629597920212
INFO:root:final train perplexity: 4.500826835632324
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 3175.1462314365144
INFO:root:eval perplexity: 13.537172317504883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/15
 15%|â–ˆâ–Œ        | 15/100 [1:15:22<7:07:45, 301.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1894.1328215422454
INFO:root:current train perplexity4.409695625305176
INFO:root:current mean train loss 1888.3209815087257
INFO:root:current train perplexity4.391208171844482
INFO:root:current mean train loss 1883.4425399660126
INFO:root:current train perplexity4.40814208984375
INFO:root:current mean train loss 1880.3022912666622
INFO:root:current train perplexity4.414844512939453
INFO:root:current mean train loss 1881.4854972940184
INFO:root:current train perplexity4.416106224060059
INFO:root:current mean train loss 1879.7020413505472
INFO:root:current train perplexity4.41366720199585
INFO:root:current mean train loss 1881.809080724687
INFO:root:current train perplexity4.415529727935791
INFO:root:current mean train loss 1882.9286669015253
INFO:root:current train perplexity4.420355796813965
INFO:root:current mean train loss 1884.0727820653267
INFO:root:current train perplexity4.420337200164795
INFO:root:current mean train loss 1881.9766346673546
INFO:root:current train perplexity4.416264057159424
INFO:root:current mean train loss 1882.7810045853964
INFO:root:current train perplexity4.417369842529297
INFO:root:current mean train loss 1882.1032399618853
INFO:root:current train perplexity4.4183244705200195
INFO:root:current mean train loss 1883.5941527152177
INFO:root:current train perplexity4.420949935913086
INFO:root:current mean train loss 1885.7026539384174
INFO:root:current train perplexity4.42595100402832
INFO:root:current mean train loss 1885.3216541820248
INFO:root:current train perplexity4.423354625701904
INFO:root:current mean train loss 1885.8278810164798
INFO:root:current train perplexity4.425727844238281
INFO:root:current mean train loss 1885.2227832651197
INFO:root:current train perplexity4.424522399902344
INFO:root:current mean train loss 1885.1190249574634
INFO:root:current train perplexity4.423475742340088
INFO:root:current mean train loss 1884.7644372176712
INFO:root:current train perplexity4.422835350036621
INFO:root:current mean train loss 1886.1955788806615
INFO:root:current train perplexity4.42460298538208

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.59s/it]
INFO:root:final mean train loss: 1885.9592763157248
INFO:root:final train perplexity: 4.425489902496338
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.76s/it]
INFO:root:eval mean loss: 3175.758717946462
INFO:root:eval perplexity: 13.543974876403809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/16
 16%|â–ˆâ–Œ        | 16/100 [1:20:25<7:03:06, 302.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1884.5120316626321
INFO:root:current train perplexity4.428347110748291
INFO:root:current mean train loss 1861.8855051854898
INFO:root:current train perplexity4.366635799407959
INFO:root:current mean train loss 1858.997145086197
INFO:root:current train perplexity4.3656134605407715
INFO:root:current mean train loss 1860.6488925491703
INFO:root:current train perplexity4.361547470092773
INFO:root:current mean train loss 1862.8021805230726
INFO:root:current train perplexity4.360883712768555
INFO:root:current mean train loss 1863.3924592614383
INFO:root:current train perplexity4.355423927307129
INFO:root:current mean train loss 1862.0249407294966
INFO:root:current train perplexity4.352832317352295
INFO:root:current mean train loss 1864.383381369822
INFO:root:current train perplexity4.352907180786133
INFO:root:current mean train loss 1863.769749743246
INFO:root:current train perplexity4.355104446411133
INFO:root:current mean train loss 1865.0331227924257
INFO:root:current train perplexity4.355910301208496
INFO:root:current mean train loss 1864.761552342291
INFO:root:current train perplexity4.353338718414307
INFO:root:current mean train loss 1865.8765180084463
INFO:root:current train perplexity4.3537821769714355
INFO:root:current mean train loss 1864.5349723281681
INFO:root:current train perplexity4.352458953857422
INFO:root:current mean train loss 1864.868794981708
INFO:root:current train perplexity4.355452537536621
INFO:root:current mean train loss 1864.6083591028105
INFO:root:current train perplexity4.355700492858887
INFO:root:current mean train loss 1865.4755874138436
INFO:root:current train perplexity4.358219146728516
INFO:root:current mean train loss 1865.2711479693812
INFO:root:current train perplexity4.354792594909668
INFO:root:current mean train loss 1866.163023627188
INFO:root:current train perplexity4.357700347900391
INFO:root:current mean train loss 1866.1794924876194
INFO:root:current train perplexity4.358747959136963
INFO:root:current mean train loss 1867.731007197132
INFO:root:current train perplexity4.36123514175415

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.95s/it]
INFO:root:final mean train loss: 1867.5884322158267
INFO:root:final train perplexity: 4.3618340492248535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.55s/it]
INFO:root:eval mean loss: 3175.892481348536
INFO:root:eval perplexity: 13.545463562011719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/17
 17%|â–ˆâ–‹        | 17/100 [1:25:27<6:57:57, 302.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1833.0503415194426
INFO:root:current train perplexity4.258409023284912
INFO:root:current mean train loss 1839.6381693089263
INFO:root:current train perplexity4.2708964347839355
INFO:root:current mean train loss 1847.8145039876301
INFO:root:current train perplexity4.2845001220703125
INFO:root:current mean train loss 1840.8352280449622
INFO:root:current train perplexity4.277834415435791
INFO:root:current mean train loss 1844.6982364341861
INFO:root:current train perplexity4.292379379272461
INFO:root:current mean train loss 1847.0071012535873
INFO:root:current train perplexity4.295073986053467
INFO:root:current mean train loss 1848.0173341618029
INFO:root:current train perplexity4.299599647521973
INFO:root:current mean train loss 1845.8892182480865
INFO:root:current train perplexity4.296798229217529
INFO:root:current mean train loss 1846.7442261292053
INFO:root:current train perplexity4.2973504066467285
INFO:root:current mean train loss 1850.4668837327224
INFO:root:current train perplexity4.301526069641113
INFO:root:current mean train loss 1851.6843645432416
INFO:root:current train perplexity4.300906658172607
INFO:root:current mean train loss 1851.612453781796
INFO:root:current train perplexity4.298038482666016
INFO:root:current mean train loss 1849.2485859556969
INFO:root:current train perplexity4.294956207275391
INFO:root:current mean train loss 1848.317590455157
INFO:root:current train perplexity4.292396068572998
INFO:root:current mean train loss 1848.823622918898
INFO:root:current train perplexity4.2933502197265625
INFO:root:current mean train loss 1849.182886003547
INFO:root:current train perplexity4.296868324279785
INFO:root:current mean train loss 1850.2072539849305
INFO:root:current train perplexity4.300151824951172
INFO:root:current mean train loss 1849.986786025216
INFO:root:current train perplexity4.301243782043457
INFO:root:current mean train loss 1849.846274618375
INFO:root:current train perplexity4.301161766052246

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.93s/it]
INFO:root:final mean train loss: 1849.9552376843797
INFO:root:final train perplexity: 4.301595687866211
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.94s/it]
INFO:root:eval mean loss: 3187.5148793813346
INFO:root:eval perplexity: 13.675264358520508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/18
 18%|â–ˆâ–Š        | 18/100 [1:30:30<6:53:00, 302.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1856.894091796875
INFO:root:current train perplexity4.318086624145508
INFO:root:current mean train loss 1835.2399658203126
INFO:root:current train perplexity4.259660720825195
INFO:root:current mean train loss 1836.274824337843
INFO:root:current train perplexity4.251585960388184
INFO:root:current mean train loss 1839.490281602203
INFO:root:current train perplexity4.260772228240967
INFO:root:current mean train loss 1834.6799409842786
INFO:root:current train perplexity4.2527241706848145
INFO:root:current mean train loss 1831.867291924505
INFO:root:current train perplexity4.244340419769287
INFO:root:current mean train loss 1835.9296140560434
INFO:root:current train perplexity4.252019882202148
INFO:root:current mean train loss 1835.6783177152593
INFO:root:current train perplexity4.255018711090088
INFO:root:current mean train loss 1835.9814961119469
INFO:root:current train perplexity4.251582622528076
INFO:root:current mean train loss 1837.4356947082183
INFO:root:current train perplexity4.255880832672119
INFO:root:current mean train loss 1839.8329176869559
INFO:root:current train perplexity4.260617256164551
INFO:root:current mean train loss 1837.5449541324942
INFO:root:current train perplexity4.253609657287598
INFO:root:current mean train loss 1836.2996288252075
INFO:root:current train perplexity4.249279499053955
INFO:root:current mean train loss 1835.23116029469
INFO:root:current train perplexity4.247917652130127
INFO:root:current mean train loss 1834.7175105301935
INFO:root:current train perplexity4.245758533477783
INFO:root:current mean train loss 1833.1870025533378
INFO:root:current train perplexity4.244141101837158
INFO:root:current mean train loss 1832.2601446133908
INFO:root:current train perplexity4.242753505706787
INFO:root:current mean train loss 1832.7760729300312
INFO:root:current train perplexity4.243419647216797
INFO:root:current mean train loss 1832.0317171810077
INFO:root:current train perplexity4.241844177246094
INFO:root:current mean train loss 1832.2184738840017
INFO:root:current train perplexity4.242522239685059

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.91s/it]
INFO:root:final mean train loss: 1833.1691567040548
INFO:root:final train perplexity: 4.24502420425415
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 3186.216996293168
INFO:root:eval perplexity: 13.660700798034668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/19
 19%|â–ˆâ–‰        | 19/100 [1:35:32<6:47:57, 302.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.3075339577415
INFO:root:current train perplexity4.128362655639648
INFO:root:current mean train loss 1804.0425935338756
INFO:root:current train perplexity4.13353157043457
INFO:root:current mean train loss 1800.9264245935387
INFO:root:current train perplexity4.139213562011719
INFO:root:current mean train loss 1802.169355499078
INFO:root:current train perplexity4.14223051071167
INFO:root:current mean train loss 1811.2566933292912
INFO:root:current train perplexity4.151662826538086
INFO:root:current mean train loss 1812.8052929406879
INFO:root:current train perplexity4.1608781814575195
INFO:root:current mean train loss 1812.349063198666
INFO:root:current train perplexity4.165224552154541
INFO:root:current mean train loss 1811.3204748095568
INFO:root:current train perplexity4.1687493324279785
INFO:root:current mean train loss 1813.481570798405
INFO:root:current train perplexity4.169863700866699
INFO:root:current mean train loss 1814.8933330544164
INFO:root:current train perplexity4.172861576080322
INFO:root:current mean train loss 1814.09920362857
INFO:root:current train perplexity4.1754255294799805
INFO:root:current mean train loss 1814.0326830750055
INFO:root:current train perplexity4.177280902862549
INFO:root:current mean train loss 1815.4878119485859
INFO:root:current train perplexity4.18131160736084
INFO:root:current mean train loss 1817.370984606952
INFO:root:current train perplexity4.1866559982299805
INFO:root:current mean train loss 1817.4343981950763
INFO:root:current train perplexity4.185804843902588
INFO:root:current mean train loss 1818.3284091623634
INFO:root:current train perplexity4.188899517059326
INFO:root:current mean train loss 1818.159213328332
INFO:root:current train perplexity4.188697814941406
INFO:root:current mean train loss 1818.3312566493585
INFO:root:current train perplexity4.191762447357178
INFO:root:current mean train loss 1818.8336564869048
INFO:root:current train perplexity4.193641185760498
INFO:root:current mean train loss 1819.1720649012666
INFO:root:current train perplexity4.194546699523926

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.13s/it]
INFO:root:final mean train loss: 1817.2854235365844
INFO:root:final train perplexity: 4.192178249359131
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 3196.8592320347693
INFO:root:eval perplexity: 13.780523300170898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/20
 20%|â–ˆâ–ˆ        | 20/100 [1:40:34<6:42:56, 302.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1782.7595746945112
INFO:root:current train perplexity4.106926918029785
INFO:root:current mean train loss 1814.2583675247301
INFO:root:current train perplexity4.13943338394165
INFO:root:current mean train loss 1809.8414878685603
INFO:root:current train perplexity4.134618282318115
INFO:root:current mean train loss 1808.8222944321533
INFO:root:current train perplexity4.1350603103637695
INFO:root:current mean train loss 1806.4030375209104
INFO:root:current train perplexity4.134549617767334
INFO:root:current mean train loss 1806.7561528872914
INFO:root:current train perplexity4.141245365142822
INFO:root:current mean train loss 1804.7279062286043
INFO:root:current train perplexity4.1439738273620605
INFO:root:current mean train loss 1806.0934797604448
INFO:root:current train perplexity4.1455583572387695
INFO:root:current mean train loss 1804.2939575340808
INFO:root:current train perplexity4.144245147705078
INFO:root:current mean train loss 1804.3954885724509
INFO:root:current train perplexity4.142481803894043
INFO:root:current mean train loss 1804.067336287145
INFO:root:current train perplexity4.142251968383789
INFO:root:current mean train loss 1804.6282536721837
INFO:root:current train perplexity4.143460273742676
INFO:root:current mean train loss 1802.8890283321352
INFO:root:current train perplexity4.139408111572266
INFO:root:current mean train loss 1804.0635487654033
INFO:root:current train perplexity4.141639232635498
INFO:root:current mean train loss 1804.752745436827
INFO:root:current train perplexity4.1456217765808105
INFO:root:current mean train loss 1805.9620868962952
INFO:root:current train perplexity4.149343013763428
INFO:root:current mean train loss 1804.6085759198397
INFO:root:current train perplexity4.146960735321045
INFO:root:current mean train loss 1804.9013478134884
INFO:root:current train perplexity4.148612976074219
INFO:root:current mean train loss 1804.7664290444238
INFO:root:current train perplexity4.147556781768799
INFO:root:current mean train loss 1803.7230784281926
INFO:root:current train perplexity4.146198749542236

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.57s/it]
INFO:root:final mean train loss: 1802.7682250422538
INFO:root:final train perplexity: 4.144455432891846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 3196.2603316206832
INFO:root:eval perplexity: 13.773749351501465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/21
 21%|â–ˆâ–ˆ        | 21/100 [1:45:37<6:38:10, 302.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1760.3062286376953
INFO:root:current train perplexity4.069389820098877
INFO:root:current mean train loss 1772.258314083784
INFO:root:current train perplexity4.078976631164551
INFO:root:current mean train loss 1778.3929286003113
INFO:root:current train perplexity4.09083890914917
INFO:root:current mean train loss 1783.4597617160068
INFO:root:current train perplexity4.099100589752197
INFO:root:current mean train loss 1784.4308070132606
INFO:root:current train perplexity4.098373889923096
INFO:root:current mean train loss 1784.339764711668
INFO:root:current train perplexity4.089245796203613
INFO:root:current mean train loss 1783.5343261346584
INFO:root:current train perplexity4.09005069732666
INFO:root:current mean train loss 1784.6161142177682
INFO:root:current train perplexity4.088992595672607
INFO:root:current mean train loss 1784.6063489111784
INFO:root:current train perplexity4.089945316314697
INFO:root:current mean train loss 1786.2547317568726
INFO:root:current train perplexity4.090822696685791
INFO:root:current mean train loss 1786.179248347427
INFO:root:current train perplexity4.091334342956543
INFO:root:current mean train loss 1786.640520775607
INFO:root:current train perplexity4.091517448425293
INFO:root:current mean train loss 1786.5222694737138
INFO:root:current train perplexity4.090099811553955
INFO:root:current mean train loss 1787.449821629707
INFO:root:current train perplexity4.09282112121582
INFO:root:current mean train loss 1787.944823212676
INFO:root:current train perplexity4.0925517082214355
INFO:root:current mean train loss 1788.445540008937
INFO:root:current train perplexity4.0939555168151855
INFO:root:current mean train loss 1788.9262383502462
INFO:root:current train perplexity4.093416690826416
INFO:root:current mean train loss 1789.2666718433006
INFO:root:current train perplexity4.095227241516113
INFO:root:current mean train loss 1789.632721999596
INFO:root:current train perplexity4.0966691970825195
INFO:root:current mean train loss 1789.2516631768037
INFO:root:current train perplexity4.098355770111084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.61s/it]
INFO:root:final mean train loss: 1788.8969499104203
INFO:root:final train perplexity: 4.099363803863525
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.86s/it]
INFO:root:eval mean loss: 3201.7960949230483
INFO:root:eval perplexity: 13.836463928222656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/22
 22%|â–ˆâ–ˆâ–       | 22/100 [1:50:40<6:33:21, 302.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1768.622966609589
INFO:root:current train perplexity4.05225133895874
INFO:root:current mean train loss 1774.4656961253613
INFO:root:current train perplexity4.04624605178833
INFO:root:current mean train loss 1772.6666528052026
INFO:root:current train perplexity4.043367862701416
INFO:root:current mean train loss 1771.0212097986134
INFO:root:current train perplexity4.0463995933532715
INFO:root:current mean train loss 1768.4666097693414
INFO:root:current train perplexity4.031950950622559
INFO:root:current mean train loss 1771.3892558099503
INFO:root:current train perplexity4.041011333465576
INFO:root:current mean train loss 1772.1330013190125
INFO:root:current train perplexity4.045064926147461
INFO:root:current mean train loss 1770.560848971388
INFO:root:current train perplexity4.044182300567627
INFO:root:current mean train loss 1770.7551958885936
INFO:root:current train perplexity4.044888496398926
INFO:root:current mean train loss 1773.4497962316532
INFO:root:current train perplexity4.0494866371154785
INFO:root:current mean train loss 1775.5449561183962
INFO:root:current train perplexity4.054362773895264
INFO:root:current mean train loss 1775.9164213813074
INFO:root:current train perplexity4.055643081665039
INFO:root:current mean train loss 1775.3774859959556
INFO:root:current train perplexity4.055704116821289
INFO:root:current mean train loss 1774.5758552745756
INFO:root:current train perplexity4.054723262786865
INFO:root:current mean train loss 1775.5619579348852
INFO:root:current train perplexity4.05471658706665
INFO:root:current mean train loss 1777.2371963530077
INFO:root:current train perplexity4.060736179351807
INFO:root:current mean train loss 1777.5391826002597
INFO:root:current train perplexity4.059997081756592
INFO:root:current mean train loss 1776.686531079804
INFO:root:current train perplexity4.0578484535217285
INFO:root:current mean train loss 1776.7016319360444
INFO:root:current train perplexity4.057784080505371
INFO:root:current mean train loss 1777.175684794036
INFO:root:current train perplexity4.059620380401611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.13s/it]
INFO:root:final mean train loss: 1776.3239383206967
INFO:root:final train perplexity: 4.058916091918945
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.28s/it]
INFO:root:eval mean loss: 3217.0053315033783
INFO:root:eval perplexity: 14.010228157043457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/23
 23%|â–ˆâ–ˆâ–Ž       | 23/100 [1:55:43<6:28:26, 302.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1781.306465657552
INFO:root:current train perplexity4.02549934387207
INFO:root:current mean train loss 1761.963675729852
INFO:root:current train perplexity3.9886670112609863
INFO:root:current mean train loss 1763.8445623989762
INFO:root:current train perplexity4.002030372619629
INFO:root:current mean train loss 1768.370375726162
INFO:root:current train perplexity4.01291036605835
INFO:root:current mean train loss 1760.5077733876753
INFO:root:current train perplexity4.001583099365234
INFO:root:current mean train loss 1763.276890434653
INFO:root:current train perplexity4.008652687072754
INFO:root:current mean train loss 1765.4690512836844
INFO:root:current train perplexity4.0152506828308105
INFO:root:current mean train loss 1766.278224665002
INFO:root:current train perplexity4.013621807098389
INFO:root:current mean train loss 1765.819099340546
INFO:root:current train perplexity4.0149407386779785
INFO:root:current mean train loss 1767.586906910906
INFO:root:current train perplexity4.02108097076416
INFO:root:current mean train loss 1765.4594173326404
INFO:root:current train perplexity4.019646167755127
INFO:root:current mean train loss 1763.5697368205094
INFO:root:current train perplexity4.012101173400879
INFO:root:current mean train loss 1762.4226483958637
INFO:root:current train perplexity4.008092880249023
INFO:root:current mean train loss 1762.202999504693
INFO:root:current train perplexity4.0098700523376465
INFO:root:current mean train loss 1762.344325532849
INFO:root:current train perplexity4.011505603790283
INFO:root:current mean train loss 1764.0676545916863
INFO:root:current train perplexity4.016231536865234
INFO:root:current mean train loss 1763.460402196399
INFO:root:current train perplexity4.017148017883301
INFO:root:current mean train loss 1763.779908726868
INFO:root:current train perplexity4.018326759338379
INFO:root:current mean train loss 1764.2773606073288
INFO:root:current train perplexity4.019554615020752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.89s/it]
INFO:root:final mean train loss: 1764.0790477912353
INFO:root:final train perplexity: 4.019906997680664
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.90s/it]
INFO:root:eval mean loss: 3208.2652268968186
INFO:root:eval perplexity: 13.910104751586914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/24
 24%|â–ˆâ–ˆâ–       | 24/100 [2:00:45<6:23:14, 302.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1687.3084891183037
INFO:root:current train perplexity3.8880844116210938
INFO:root:current mean train loss 1728.1435911945093
INFO:root:current train perplexity3.934065103530884
INFO:root:current mean train loss 1735.5078237045213
INFO:root:current train perplexity3.944988965988159
INFO:root:current mean train loss 1740.3889009059446
INFO:root:current train perplexity3.9580514430999756
INFO:root:current mean train loss 1741.5457808660933
INFO:root:current train perplexity3.9581024646759033
INFO:root:current mean train loss 1741.5121545434233
INFO:root:current train perplexity3.955915689468384
INFO:root:current mean train loss 1743.0915991894692
INFO:root:current train perplexity3.9623918533325195
INFO:root:current mean train loss 1745.9544641475866
INFO:root:current train perplexity3.9691617488861084
INFO:root:current mean train loss 1745.0657133081145
INFO:root:current train perplexity3.9669291973114014
INFO:root:current mean train loss 1746.4672605268486
INFO:root:current train perplexity3.9697787761688232
INFO:root:current mean train loss 1746.8791742713117
INFO:root:current train perplexity3.973011016845703
INFO:root:current mean train loss 1747.8206752925278
INFO:root:current train perplexity3.9728376865386963
INFO:root:current mean train loss 1751.248359484226
INFO:root:current train perplexity3.9820644855499268
INFO:root:current mean train loss 1750.7820057712079
INFO:root:current train perplexity3.9802801609039307
INFO:root:current mean train loss 1750.8671476774887
INFO:root:current train perplexity3.9805850982666016
INFO:root:current mean train loss 1751.967056163036
INFO:root:current train perplexity3.9815893173217773
INFO:root:current mean train loss 1754.1413016660508
INFO:root:current train perplexity3.985267162322998
INFO:root:current mean train loss 1753.3339174401362
INFO:root:current train perplexity3.9848358631134033
INFO:root:current mean train loss 1753.185076292932
INFO:root:current train perplexity3.984617233276367
INFO:root:current mean train loss 1752.7498685839792
INFO:root:current train perplexity3.9837262630462646

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it]
INFO:root:final mean train loss: 1751.9780120157077
INFO:root:final train perplexity: 3.9817252159118652
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it]
INFO:root:eval mean loss: 3230.224635035426
INFO:root:eval perplexity: 14.163029670715332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/25
 25%|â–ˆâ–ˆâ–Œ       | 25/100 [2:05:48<6:18:18, 302.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1719.243896484375
INFO:root:current train perplexity3.897345781326294
INFO:root:current mean train loss 1714.0911481303554
INFO:root:current train perplexity3.9229238033294678
INFO:root:current mean train loss 1728.1095041547503
INFO:root:current train perplexity3.9428329467773438
INFO:root:current mean train loss 1735.1099363727335
INFO:root:current train perplexity3.9384829998016357
INFO:root:current mean train loss 1732.6696849319171
INFO:root:current train perplexity3.9307849407196045
INFO:root:current mean train loss 1734.0072741326485
INFO:root:current train perplexity3.9298417568206787
INFO:root:current mean train loss 1737.7170279087165
INFO:root:current train perplexity3.9384918212890625
INFO:root:current mean train loss 1739.8715100367424
INFO:root:current train perplexity3.9442291259765625
INFO:root:current mean train loss 1740.9742676077537
INFO:root:current train perplexity3.9447808265686035
INFO:root:current mean train loss 1739.9121824322324
INFO:root:current train perplexity3.9403445720672607
INFO:root:current mean train loss 1740.8968983888626
INFO:root:current train perplexity3.9461846351623535
INFO:root:current mean train loss 1741.501394142884
INFO:root:current train perplexity3.9440674781799316
INFO:root:current mean train loss 1740.2483390857972
INFO:root:current train perplexity3.940046787261963
INFO:root:current mean train loss 1740.2421467484305
INFO:root:current train perplexity3.9418346881866455
INFO:root:current mean train loss 1741.44693113177
INFO:root:current train perplexity3.944340705871582
INFO:root:current mean train loss 1741.8391041192483
INFO:root:current train perplexity3.9453070163726807
INFO:root:current mean train loss 1742.4358525769464
INFO:root:current train perplexity3.9457311630249023
INFO:root:current mean train loss 1743.377186927884
INFO:root:current train perplexity3.948066473007202
INFO:root:current mean train loss 1742.5782179581493
INFO:root:current train perplexity3.9491922855377197
INFO:root:current mean train loss 1742.918671352204
INFO:root:current train perplexity3.9495749473571777

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it]
INFO:root:final mean train loss: 1741.4436401120954
INFO:root:final train perplexity: 3.948781728744507
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 3223.0883209870026
INFO:root:eval perplexity: 14.08033275604248
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/26
 26%|â–ˆâ–ˆâ–Œ       | 26/100 [2:10:50<6:13:06, 302.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1741.0069699171113
INFO:root:current train perplexity3.9155173301696777
INFO:root:current mean train loss 1724.8570296916555
INFO:root:current train perplexity3.9081530570983887
INFO:root:current mean train loss 1725.1737222631937
INFO:root:current train perplexity3.9118692874908447
INFO:root:current mean train loss 1735.1128223229014
INFO:root:current train perplexity3.924395799636841
INFO:root:current mean train loss 1728.3058171347966
INFO:root:current train perplexity3.917358160018921
INFO:root:current mean train loss 1729.901090554962
INFO:root:current train perplexity3.91753888130188
INFO:root:current mean train loss 1730.3005435842433
INFO:root:current train perplexity3.9196479320526123
INFO:root:current mean train loss 1730.0221695172802
INFO:root:current train perplexity3.9206414222717285
INFO:root:current mean train loss 1734.1079346864317
INFO:root:current train perplexity3.9275119304656982
INFO:root:current mean train loss 1732.2884957357116
INFO:root:current train perplexity3.921908378601074
INFO:root:current mean train loss 1730.2627871290752
INFO:root:current train perplexity3.9178428649902344
INFO:root:current mean train loss 1731.2617015253547
INFO:root:current train perplexity3.9161007404327393
INFO:root:current mean train loss 1730.9657203741942
INFO:root:current train perplexity3.9137356281280518
INFO:root:current mean train loss 1731.1956940038626
INFO:root:current train perplexity3.9150071144104004
INFO:root:current mean train loss 1731.6323049890755
INFO:root:current train perplexity3.917081594467163
INFO:root:current mean train loss 1731.4775005640108
INFO:root:current train perplexity3.9173707962036133
INFO:root:current mean train loss 1730.6588617542182
INFO:root:current train perplexity3.9154434204101562
INFO:root:current mean train loss 1730.744186971052
INFO:root:current train perplexity3.9154891967773438
INFO:root:current mean train loss 1730.8574596697192
INFO:root:current train perplexity3.914961814880371
INFO:root:current mean train loss 1730.9743205192838
INFO:root:current train perplexity3.915220022201538

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.44s/it]
INFO:root:final mean train loss: 1730.7113503722067
INFO:root:final train perplexity: 3.9154999256134033
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.12s/it]
INFO:root:eval mean loss: 3225.2184200802367
INFO:root:eval perplexity: 14.104968070983887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/27
 27%|â–ˆâ–ˆâ–‹       | 27/100 [2:15:51<6:07:31, 302.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1738.3124600114493
INFO:root:current train perplexity3.878065347671509
INFO:root:current mean train loss 1716.1330411886868
INFO:root:current train perplexity3.858024835586548
INFO:root:current mean train loss 1705.0873181246973
INFO:root:current train perplexity3.8584506511688232
INFO:root:current mean train loss 1704.9276286716567
INFO:root:current train perplexity3.8548080921173096
INFO:root:current mean train loss 1708.7018158092249
INFO:root:current train perplexity3.864267110824585
INFO:root:current mean train loss 1714.2189016034527
INFO:root:current train perplexity3.872623920440674
INFO:root:current mean train loss 1715.7529717999028
INFO:root:current train perplexity3.8745992183685303
INFO:root:current mean train loss 1717.5428346014903
INFO:root:current train perplexity3.878166675567627
INFO:root:current mean train loss 1718.254924783062
INFO:root:current train perplexity3.8805737495422363
INFO:root:current mean train loss 1717.621007103014
INFO:root:current train perplexity3.8776743412017822
INFO:root:current mean train loss 1716.6408719097058
INFO:root:current train perplexity3.8754894733428955
INFO:root:current mean train loss 1718.0901387005479
INFO:root:current train perplexity3.8771164417266846
INFO:root:current mean train loss 1719.4695497643208
INFO:root:current train perplexity3.8791756629943848
INFO:root:current mean train loss 1718.8468266572797
INFO:root:current train perplexity3.8764023780822754
INFO:root:current mean train loss 1719.1045824425048
INFO:root:current train perplexity3.8773744106292725
INFO:root:current mean train loss 1720.0341531266297
INFO:root:current train perplexity3.879525661468506
INFO:root:current mean train loss 1719.9231409088982
INFO:root:current train perplexity3.8793835639953613
INFO:root:current mean train loss 1719.3796632525864
INFO:root:current train perplexity3.8781023025512695
INFO:root:current mean train loss 1720.2103442303974
INFO:root:current train perplexity3.8819494247436523
INFO:root:current mean train loss 1720.7330608426369
INFO:root:current train perplexity3.883272647857666

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.55s/it]
INFO:root:final mean train loss: 1720.61982854015
INFO:root:final train perplexity: 3.8844611644744873
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it]
INFO:root:eval mean loss: 3241.047439529373
INFO:root:eval perplexity: 14.289372444152832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/28
 28%|â–ˆâ–ˆâ–Š       | 28/100 [2:20:53<6:02:17, 301.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1732.368857421875
INFO:root:current train perplexity3.893979072570801
INFO:root:current mean train loss 1717.102470703125
INFO:root:current train perplexity3.847266912460327
INFO:root:current mean train loss 1712.1108047762784
INFO:root:current train perplexity3.8450119495391846
INFO:root:current mean train loss 1713.255955078125
INFO:root:current train perplexity3.852633476257324
INFO:root:current mean train loss 1713.749972502056
INFO:root:current train perplexity3.8566150665283203
INFO:root:current mean train loss 1709.1157007897418
INFO:root:current train perplexity3.8462681770324707
INFO:root:current mean train loss 1708.893185944734
INFO:root:current train perplexity3.84698748588562
INFO:root:current mean train loss 1712.559881079889
INFO:root:current train perplexity3.854130506515503
INFO:root:current mean train loss 1712.390758231027
INFO:root:current train perplexity3.85212779045105
INFO:root:current mean train loss 1713.5067755909456
INFO:root:current train perplexity3.8523786067962646
INFO:root:current mean train loss 1714.123410814862
INFO:root:current train perplexity3.85318922996521
INFO:root:current mean train loss 1712.62849048371
INFO:root:current train perplexity3.8534207344055176
INFO:root:current mean train loss 1710.6875830078125
INFO:root:current train perplexity3.8502752780914307
INFO:root:current mean train loss 1709.5815929509943
INFO:root:current train perplexity3.8506290912628174
INFO:root:current mean train loss 1709.5927872583422
INFO:root:current train perplexity3.8495121002197266
INFO:root:current mean train loss 1711.14384765625
INFO:root:current train perplexity3.8515658378601074
INFO:root:current mean train loss 1711.5791874125466
INFO:root:current train perplexity3.8530402183532715
INFO:root:current mean train loss 1710.8900882344851
INFO:root:current train perplexity3.8524258136749268
INFO:root:current mean train loss 1711.0932927083334
INFO:root:current train perplexity3.853576898574829
INFO:root:current mean train loss 1710.413098484474
INFO:root:current train perplexity3.852337121963501

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.63s/it]
INFO:root:final mean train loss: 1710.1050640650608
INFO:root:final train perplexity: 3.8523826599121094
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3240.499903956691
INFO:root:eval perplexity: 14.282952308654785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/29
 29%|â–ˆâ–ˆâ–‰       | 29/100 [2:25:54<5:57:11, 301.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1693.1602159583051
INFO:root:current train perplexity3.796443462371826
INFO:root:current mean train loss 1693.054157892863
INFO:root:current train perplexity3.805640935897827
INFO:root:current mean train loss 1699.968719900471
INFO:root:current train perplexity3.808333158493042
INFO:root:current mean train loss 1700.8865375129544
INFO:root:current train perplexity3.810068130493164
INFO:root:current mean train loss 1708.4102946955984
INFO:root:current train perplexity3.821035861968994
INFO:root:current mean train loss 1708.5110413834855
INFO:root:current train perplexity3.8262736797332764
INFO:root:current mean train loss 1707.5587085878228
INFO:root:current train perplexity3.820686101913452
INFO:root:current mean train loss 1705.3986865727588
INFO:root:current train perplexity3.819155693054199
INFO:root:current mean train loss 1706.5889349283125
INFO:root:current train perplexity3.820232629776001
INFO:root:current mean train loss 1706.3166705716042
INFO:root:current train perplexity3.821958065032959
INFO:root:current mean train loss 1705.2235656291136
INFO:root:current train perplexity3.8195180892944336
INFO:root:current mean train loss 1704.7671725893981
INFO:root:current train perplexity3.8206450939178467
INFO:root:current mean train loss 1704.4935229983491
INFO:root:current train perplexity3.8210718631744385
INFO:root:current mean train loss 1704.0019118210364
INFO:root:current train perplexity3.8225178718566895
INFO:root:current mean train loss 1705.415964653281
INFO:root:current train perplexity3.825847625732422
INFO:root:current mean train loss 1704.5711628516115
INFO:root:current train perplexity3.8248088359832764
INFO:root:current mean train loss 1703.4344070470643
INFO:root:current train perplexity3.8236632347106934
INFO:root:current mean train loss 1701.4420880590167
INFO:root:current train perplexity3.8231208324432373
INFO:root:current mean train loss 1701.3473613972897
INFO:root:current train perplexity3.8244168758392334

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.46s/it]
INFO:root:final mean train loss: 1700.911810939864
INFO:root:final train perplexity: 3.824552297592163
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 3241.286917288382
INFO:root:eval perplexity: 14.292177200317383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/30
 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [2:30:56<5:52:02, 301.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1652.94873046875
INFO:root:current train perplexity3.8300974369049072
INFO:root:current mean train loss 1680.5010986328125
INFO:root:current train perplexity3.795102834701538
INFO:root:current mean train loss 1691.5018246299342
INFO:root:current train perplexity3.8039259910583496
INFO:root:current mean train loss 1690.3498602314673
INFO:root:current train perplexity3.804391384124756
INFO:root:current mean train loss 1693.359307846405
INFO:root:current train perplexity3.798720121383667
INFO:root:current mean train loss 1688.3366006127978
INFO:root:current train perplexity3.7847354412078857
INFO:root:current mean train loss 1688.8998331906173
INFO:root:current train perplexity3.7850112915039062
INFO:root:current mean train loss 1687.9385550731665
INFO:root:current train perplexity3.78487491607666
INFO:root:current mean train loss 1689.5057329288666
INFO:root:current train perplexity3.786149740219116
INFO:root:current mean train loss 1689.8980711547717
INFO:root:current train perplexity3.788032293319702
INFO:root:current mean train loss 1691.3149520526201
INFO:root:current train perplexity3.791008949279785
INFO:root:current mean train loss 1690.6425732818136
INFO:root:current train perplexity3.7900784015655518
INFO:root:current mean train loss 1691.486608412169
INFO:root:current train perplexity3.794227123260498
INFO:root:current mean train loss 1691.967134363511
INFO:root:current train perplexity3.794748544692993
INFO:root:current mean train loss 1692.6267110636422
INFO:root:current train perplexity3.794907808303833
INFO:root:current mean train loss 1692.6080048841068
INFO:root:current train perplexity3.796104669570923
INFO:root:current mean train loss 1692.9467242367155
INFO:root:current train perplexity3.7970454692840576
INFO:root:current mean train loss 1692.9623639869578
INFO:root:current train perplexity3.798598527908325
INFO:root:current mean train loss 1692.690610599887
INFO:root:current train perplexity3.7986667156219482
INFO:root:current mean train loss 1692.3894567314776
INFO:root:current train perplexity3.7981278896331787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.71s/it]
INFO:root:final mean train loss: 1692.0640301879944
INFO:root:final train perplexity: 3.7979576587677
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 3251.0182540939377
INFO:root:eval perplexity: 14.40676498413086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/31
 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [2:35:58<5:47:01, 301.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1678.4432185246394
INFO:root:current train perplexity3.7703030109405518
INFO:root:current mean train loss 1678.1475926959326
INFO:root:current train perplexity3.753723621368408
INFO:root:current mean train loss 1679.278838841261
INFO:root:current train perplexity3.751087188720703
INFO:root:current mean train loss 1670.4945124526696
INFO:root:current train perplexity3.731367826461792
INFO:root:current mean train loss 1669.8444594978725
INFO:root:current train perplexity3.7306969165802
INFO:root:current mean train loss 1671.6719248956601
INFO:root:current train perplexity3.7344207763671875
INFO:root:current mean train loss 1673.9325854024187
INFO:root:current train perplexity3.7412123680114746
INFO:root:current mean train loss 1676.8334414479489
INFO:root:current train perplexity3.744678020477295
INFO:root:current mean train loss 1680.4758973202463
INFO:root:current train perplexity3.75117826461792
INFO:root:current mean train loss 1678.639662542817
INFO:root:current train perplexity3.7496957778930664
INFO:root:current mean train loss 1679.3832786515443
INFO:root:current train perplexity3.7552671432495117
INFO:root:current mean train loss 1681.3001503004275
INFO:root:current train perplexity3.7613584995269775
INFO:root:current mean train loss 1681.5244640456133
INFO:root:current train perplexity3.7635254859924316
INFO:root:current mean train loss 1682.3488578048407
INFO:root:current train perplexity3.763854742050171
INFO:root:current mean train loss 1681.5519977585632
INFO:root:current train perplexity3.7649192810058594
INFO:root:current mean train loss 1680.5812653907785
INFO:root:current train perplexity3.7653517723083496
INFO:root:current mean train loss 1681.8027094504343
INFO:root:current train perplexity3.767732858657837
INFO:root:current mean train loss 1682.803387726997
INFO:root:current train perplexity3.7701847553253174
INFO:root:current mean train loss 1682.6020180910075
INFO:root:current train perplexity3.76935076713562
INFO:root:current mean train loss 1682.9001743716738
INFO:root:current train perplexity3.7711119651794434

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.06s/it]
INFO:root:final mean train loss: 1683.0808095003822
INFO:root:final train perplexity: 3.7711448669433594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.15s/it]
INFO:root:eval mean loss: 3259.36413684192
INFO:root:eval perplexity: 14.505763053894043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/32
 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [2:40:59<5:41:58, 301.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1648.6242874500363
INFO:root:current train perplexity3.6491804122924805
INFO:root:current mean train loss 1669.1611131788134
INFO:root:current train perplexity3.7016282081604004
INFO:root:current mean train loss 1676.8897268036264
INFO:root:current train perplexity3.7277557849884033
INFO:root:current mean train loss 1676.9242797495672
INFO:root:current train perplexity3.734886407852173
INFO:root:current mean train loss 1672.8911320189052
INFO:root:current train perplexity3.7321839332580566
INFO:root:current mean train loss 1669.2160631042818
INFO:root:current train perplexity3.728006362915039
INFO:root:current mean train loss 1666.429513981702
INFO:root:current train perplexity3.7278900146484375
INFO:root:current mean train loss 1668.0406229627565
INFO:root:current train perplexity3.7317399978637695
INFO:root:current mean train loss 1669.0042776739047
INFO:root:current train perplexity3.7353665828704834
INFO:root:current mean train loss 1671.849725490663
INFO:root:current train perplexity3.739006757736206
INFO:root:current mean train loss 1672.5315502016792
INFO:root:current train perplexity3.741292715072632
INFO:root:current mean train loss 1672.6225393700788
INFO:root:current train perplexity3.741546392440796
INFO:root:current mean train loss 1674.000778971616
INFO:root:current train perplexity3.742771863937378
INFO:root:current mean train loss 1674.282897903772
INFO:root:current train perplexity3.7422125339508057
INFO:root:current mean train loss 1674.4381342072993
INFO:root:current train perplexity3.742900848388672
INFO:root:current mean train loss 1675.737980069391
INFO:root:current train perplexity3.7443859577178955
INFO:root:current mean train loss 1676.4153788236315
INFO:root:current train perplexity3.745191812515259
INFO:root:current mean train loss 1675.8552953443236
INFO:root:current train perplexity3.745717763900757
INFO:root:current mean train loss 1675.3460151560591
INFO:root:current train perplexity3.746840000152588
INFO:root:current mean train loss 1675.6427816174046
INFO:root:current train perplexity3.7475364208221436

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.72s/it]
INFO:root:final mean train loss: 1674.8019685269123
INFO:root:final train perplexity: 3.74660325050354
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 3265.6839302681587
INFO:root:eval perplexity: 14.581186294555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/33
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [2:46:01<5:36:59, 301.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.4549743652344
INFO:root:current train perplexity3.6612303256988525
INFO:root:current mean train loss 1647.757664489746
INFO:root:current train perplexity3.696911573410034
INFO:root:current mean train loss 1650.5710773174578
INFO:root:current train perplexity3.7032017707824707
INFO:root:current mean train loss 1653.9749304877387
INFO:root:current train perplexity3.709653854370117
INFO:root:current mean train loss 1652.7931253184443
INFO:root:current train perplexity3.709667205810547
INFO:root:current mean train loss 1654.9464115687779
INFO:root:current train perplexity3.708350419998169
INFO:root:current mean train loss 1657.2405478737571
INFO:root:current train perplexity3.712646007537842
INFO:root:current mean train loss 1659.8101886950042
INFO:root:current train perplexity3.712714195251465
INFO:root:current mean train loss 1662.6193884561228
INFO:root:current train perplexity3.7148001194000244
INFO:root:current mean train loss 1663.2848283131918
INFO:root:current train perplexity3.7120754718780518
INFO:root:current mean train loss 1662.934944124042
INFO:root:current train perplexity3.7113096714019775
INFO:root:current mean train loss 1662.3606610528354
INFO:root:current train perplexity3.7112419605255127
INFO:root:current mean train loss 1663.1592378162202
INFO:root:current train perplexity3.713606357574463
INFO:root:current mean train loss 1662.685673792222
INFO:root:current train perplexity3.7133607864379883
INFO:root:current mean train loss 1662.4918020588077
INFO:root:current train perplexity3.712398052215576
INFO:root:current mean train loss 1661.9956165802785
INFO:root:current train perplexity3.7125003337860107
INFO:root:current mean train loss 1662.2022260918675
INFO:root:current train perplexity3.7126007080078125
INFO:root:current mean train loss 1664.397628922896
INFO:root:current train perplexity3.7161002159118652
INFO:root:current mean train loss 1665.6090734338247
INFO:root:current train perplexity3.718907117843628
INFO:root:current mean train loss 1667.0987684600207
INFO:root:current train perplexity3.7227070331573486

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.69s/it]
INFO:root:final mean train loss: 1666.3264508576808
INFO:root:final train perplexity: 3.7216429710388184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.45s/it]
INFO:root:eval mean loss: 3266.8125161294106
INFO:root:eval perplexity: 14.594691276550293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/34
 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [2:51:04<5:32:14, 302.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.4545153332995
INFO:root:current train perplexity3.6904265880584717
INFO:root:current mean train loss 1670.1398815435205
INFO:root:current train perplexity3.6971890926361084
INFO:root:current mean train loss 1659.6082045351986
INFO:root:current train perplexity3.6858372688293457
INFO:root:current mean train loss 1657.45336460751
INFO:root:current train perplexity3.6864516735076904
INFO:root:current mean train loss 1655.2481955602234
INFO:root:current train perplexity3.690258502960205
INFO:root:current mean train loss 1656.6476052216476
INFO:root:current train perplexity3.6927597522735596
INFO:root:current mean train loss 1655.434437062927
INFO:root:current train perplexity3.6892096996307373
INFO:root:current mean train loss 1655.6123632875342
INFO:root:current train perplexity3.685154676437378
INFO:root:current mean train loss 1654.8039799932742
INFO:root:current train perplexity3.6842052936553955
INFO:root:current mean train loss 1655.1530134499744
INFO:root:current train perplexity3.685694932937622
INFO:root:current mean train loss 1655.859094929673
INFO:root:current train perplexity3.6886279582977295
INFO:root:current mean train loss 1656.9525365319005
INFO:root:current train perplexity3.690850257873535
INFO:root:current mean train loss 1658.6868731111124
INFO:root:current train perplexity3.696221351623535
INFO:root:current mean train loss 1658.9344790922012
INFO:root:current train perplexity3.6969361305236816
INFO:root:current mean train loss 1658.352718242214
INFO:root:current train perplexity3.6967756748199463
INFO:root:current mean train loss 1658.2761933321278
INFO:root:current train perplexity3.69622540473938
INFO:root:current mean train loss 1659.1539807441907
INFO:root:current train perplexity3.6981098651885986
INFO:root:current mean train loss 1659.2940820147633
INFO:root:current train perplexity3.6987109184265137
INFO:root:current mean train loss 1659.5419198688066
INFO:root:current train perplexity3.6998684406280518
INFO:root:current mean train loss 1659.190147635306
INFO:root:current train perplexity3.699636459350586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.90s/it]
INFO:root:final mean train loss: 1658.7418773072088
INFO:root:final train perplexity: 3.6994481086730957
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.81s/it]
INFO:root:eval mean loss: 3267.2287040458427
INFO:root:eval perplexity: 14.599678993225098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/35
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [2:56:06<5:27:15, 302.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1673.7585007687833
INFO:root:current train perplexity3.7161834239959717
INFO:root:current mean train loss 1658.179998338837
INFO:root:current train perplexity3.6832327842712402
INFO:root:current mean train loss 1656.1156504105547
INFO:root:current train perplexity3.6732873916625977
INFO:root:current mean train loss 1652.043198019115
INFO:root:current train perplexity3.6651980876922607
INFO:root:current mean train loss 1650.389328188259
INFO:root:current train perplexity3.6698975563049316
INFO:root:current mean train loss 1651.6900797115031
INFO:root:current train perplexity3.671063184738159
INFO:root:current mean train loss 1650.9982250554417
INFO:root:current train perplexity3.6731390953063965
INFO:root:current mean train loss 1652.098491985792
INFO:root:current train perplexity3.6728084087371826
INFO:root:current mean train loss 1652.227663590604
INFO:root:current train perplexity3.6754696369171143
INFO:root:current mean train loss 1650.2771804164834
INFO:root:current train perplexity3.6722493171691895
INFO:root:current mean train loss 1651.7925436239573
INFO:root:current train perplexity3.674105167388916
INFO:root:current mean train loss 1652.8561251079616
INFO:root:current train perplexity3.67440128326416
INFO:root:current mean train loss 1652.7008433983167
INFO:root:current train perplexity3.677502393722534
INFO:root:current mean train loss 1652.9597229266612
INFO:root:current train perplexity3.6776535511016846
INFO:root:current mean train loss 1653.1571468164325
INFO:root:current train perplexity3.677757501602173
INFO:root:current mean train loss 1652.0768731283574
INFO:root:current train perplexity3.6766319274902344
INFO:root:current mean train loss 1651.640586519748
INFO:root:current train perplexity3.676327705383301
INFO:root:current mean train loss 1652.213948023359
INFO:root:current train perplexity3.677626848220825
INFO:root:current mean train loss 1652.218473956099
INFO:root:current train perplexity3.6786692142486572

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.56s/it]
INFO:root:final mean train loss: 1651.6032546173726
INFO:root:final train perplexity: 3.6786787509918213
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.49s/it]
INFO:root:eval mean loss: 3274.2588902378943
INFO:root:eval perplexity: 14.684147834777832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/36
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [3:01:08<5:22:02, 301.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.1738947088068
INFO:root:current train perplexity3.7520902156829834
INFO:root:current mean train loss 1627.327095650338
INFO:root:current train perplexity3.6385819911956787
INFO:root:current mean train loss 1620.5452938712604
INFO:root:current train perplexity3.6117961406707764
INFO:root:current mean train loss 1628.1554999152181
INFO:root:current train perplexity3.6273560523986816
INFO:root:current mean train loss 1630.0218146479622
INFO:root:current train perplexity3.619946002960205
INFO:root:current mean train loss 1632.8321182041952
INFO:root:current train perplexity3.628518581390381
INFO:root:current mean train loss 1636.9565503608965
INFO:root:current train perplexity3.6377861499786377
INFO:root:current mean train loss 1637.4999364753648
INFO:root:current train perplexity3.6380255222320557
INFO:root:current mean train loss 1636.4790373213048
INFO:root:current train perplexity3.6399385929107666
INFO:root:current mean train loss 1635.9691393922371
INFO:root:current train perplexity3.6417505741119385
INFO:root:current mean train loss 1636.6175771349144
INFO:root:current train perplexity3.643268585205078
INFO:root:current mean train loss 1636.2852371174617
INFO:root:current train perplexity3.6431288719177246
INFO:root:current mean train loss 1637.7047802573093
INFO:root:current train perplexity3.644033193588257
INFO:root:current mean train loss 1637.3541912483315
INFO:root:current train perplexity3.6444027423858643
INFO:root:current mean train loss 1640.5385313946504
INFO:root:current train perplexity3.647261619567871
INFO:root:current mean train loss 1640.8638285063182
INFO:root:current train perplexity3.649453639984131
INFO:root:current mean train loss 1641.7966362392583
INFO:root:current train perplexity3.651085138320923
INFO:root:current mean train loss 1643.1618162207544
INFO:root:current train perplexity3.6525990962982178
INFO:root:current mean train loss 1643.5195745239596
INFO:root:current train perplexity3.654144763946533
INFO:root:current mean train loss 1643.3749859469028
INFO:root:current train perplexity3.6541178226470947

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.14s/it]
INFO:root:final mean train loss: 1643.93000427954
INFO:root:final train perplexity: 3.6564838886260986
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.70s/it]
INFO:root:eval mean loss: 3284.526750627581
INFO:root:eval perplexity: 14.808387756347656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/37
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [3:06:10<5:17:08, 302.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1628.1329345703125
INFO:root:current train perplexity3.604339599609375
INFO:root:current mean train loss 1629.0653057098389
INFO:root:current train perplexity3.613828659057617
INFO:root:current mean train loss 1629.0794228001644
INFO:root:current train perplexity3.6087610721588135
INFO:root:current mean train loss 1621.8677096483184
INFO:root:current train perplexity3.600029230117798
INFO:root:current mean train loss 1622.457684953636
INFO:root:current train perplexity3.6060333251953125
INFO:root:current mean train loss 1624.9685125639944
INFO:root:current train perplexity3.6127099990844727
INFO:root:current mean train loss 1626.6529999751194
INFO:root:current train perplexity3.6158499717712402
INFO:root:current mean train loss 1625.8045060713212
INFO:root:current train perplexity3.6161935329437256
INFO:root:current mean train loss 1626.9488633013002
INFO:root:current train perplexity3.621300220489502
INFO:root:current mean train loss 1627.1421153627593
INFO:root:current train perplexity3.622157335281372
INFO:root:current mean train loss 1629.565523615143
INFO:root:current train perplexity3.624638080596924
INFO:root:current mean train loss 1631.027855947508
INFO:root:current train perplexity3.6256320476531982
INFO:root:current mean train loss 1630.664887568073
INFO:root:current train perplexity3.6268115043640137
INFO:root:current mean train loss 1631.3941187111727
INFO:root:current train perplexity3.6268768310546875
INFO:root:current mean train loss 1630.881720791344
INFO:root:current train perplexity3.6264572143554688
INFO:root:current mean train loss 1633.0919375594374
INFO:root:current train perplexity3.6303365230560303
INFO:root:current mean train loss 1635.6964430000623
INFO:root:current train perplexity3.634211540222168
INFO:root:current mean train loss 1637.3410620512786
INFO:root:current train perplexity3.636145830154419
INFO:root:current mean train loss 1638.1831846675414
INFO:root:current train perplexity3.6381771564483643
INFO:root:current mean train loss 1637.8584543441837
INFO:root:current train perplexity3.637392282485962

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.74s/it]
INFO:root:final mean train loss: 1637.527941943778
INFO:root:final train perplexity: 3.6380693912506104
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.28s/it]
INFO:root:eval mean loss: 3287.2234443916573
INFO:root:eval perplexity: 14.841191291809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/38
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [3:11:12<5:12:15, 302.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1611.320890299479
INFO:root:current train perplexity3.596639633178711
INFO:root:current mean train loss 1634.087281957166
INFO:root:current train perplexity3.6134886741638184
INFO:root:current mean train loss 1635.37478774713
INFO:root:current train perplexity3.615844488143921
INFO:root:current mean train loss 1627.3051506595334
INFO:root:current train perplexity3.6052374839782715
INFO:root:current mean train loss 1628.7874550122892
INFO:root:current train perplexity3.607152223587036
INFO:root:current mean train loss 1627.6778318072677
INFO:root:current train perplexity3.606597423553467
INFO:root:current mean train loss 1628.1218055429385
INFO:root:current train perplexity3.6053855419158936
INFO:root:current mean train loss 1625.7074895461933
INFO:root:current train perplexity3.5999841690063477
INFO:root:current mean train loss 1626.2693498058431
INFO:root:current train perplexity3.5989482402801514
INFO:root:current mean train loss 1626.4629660631613
INFO:root:current train perplexity3.5994069576263428
INFO:root:current mean train loss 1627.0770602431594
INFO:root:current train perplexity3.6024913787841797
INFO:root:current mean train loss 1628.7517971521902
INFO:root:current train perplexity3.6067376136779785
INFO:root:current mean train loss 1629.2571528300703
INFO:root:current train perplexity3.608715534210205
INFO:root:current mean train loss 1627.8308782527881
INFO:root:current train perplexity3.6096134185791016
INFO:root:current mean train loss 1629.295776874054
INFO:root:current train perplexity3.6108908653259277
INFO:root:current mean train loss 1629.2071232175365
INFO:root:current train perplexity3.610541343688965
INFO:root:current mean train loss 1629.5491110016148
INFO:root:current train perplexity3.611341953277588
INFO:root:current mean train loss 1630.2787103778653
INFO:root:current train perplexity3.6130924224853516
INFO:root:current mean train loss 1630.464391328972
INFO:root:current train perplexity3.614348888397217
INFO:root:current mean train loss 1630.2858408479274
INFO:root:current train perplexity3.61488676071167

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.39s/it]
INFO:root:final mean train loss: 1629.983406070742
INFO:root:final train perplexity: 3.6164863109588623
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.83s/it]
INFO:root:eval mean loss: 3287.5899009360924
INFO:root:eval perplexity: 14.845661163330078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/39
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [3:16:14<5:07:04, 302.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1608.0452782415575
INFO:root:current train perplexity3.5640244483947754
INFO:root:current mean train loss 1611.5044842001832
INFO:root:current train perplexity3.5666279792785645
INFO:root:current mean train loss 1618.7254237983063
INFO:root:current train perplexity3.5730228424072266
INFO:root:current mean train loss 1619.8666847186853
INFO:root:current train perplexity3.5778794288635254
INFO:root:current mean train loss 1617.1372543268906
INFO:root:current train perplexity3.5810506343841553
INFO:root:current mean train loss 1619.3681271373166
INFO:root:current train perplexity3.5899786949157715
INFO:root:current mean train loss 1618.8281397516994
INFO:root:current train perplexity3.5897433757781982
INFO:root:current mean train loss 1618.786837520249
INFO:root:current train perplexity3.589287519454956
INFO:root:current mean train loss 1620.9366819023255
INFO:root:current train perplexity3.593954086303711
INFO:root:current mean train loss 1620.350501554176
INFO:root:current train perplexity3.5939042568206787
INFO:root:current mean train loss 1621.7860621220648
INFO:root:current train perplexity3.593555212020874
INFO:root:current mean train loss 1621.8850510510233
INFO:root:current train perplexity3.5918219089508057
INFO:root:current mean train loss 1621.0034025890513
INFO:root:current train perplexity3.590853452682495
INFO:root:current mean train loss 1622.0968703215342
INFO:root:current train perplexity3.5937416553497314
INFO:root:current mean train loss 1621.648903237485
INFO:root:current train perplexity3.594977617263794
INFO:root:current mean train loss 1622.5860896580655
INFO:root:current train perplexity3.5962023735046387
INFO:root:current mean train loss 1623.683725735771
INFO:root:current train perplexity3.5976216793060303
INFO:root:current mean train loss 1623.9336343477316
INFO:root:current train perplexity3.5985982418060303
INFO:root:current mean train loss 1624.5395273112329
INFO:root:current train perplexity3.599278211593628
INFO:root:current mean train loss 1624.736536428469
INFO:root:current train perplexity3.5993881225585938

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.70s/it]
INFO:root:final mean train loss: 1624.168519882379
INFO:root:final train perplexity: 3.5999391078948975
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 3291.889339779232
INFO:root:eval perplexity: 14.898123741149902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/40
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [3:21:16<5:01:57, 301.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.8589392553401
INFO:root:current train perplexity3.5792930126190186
INFO:root:current mean train loss 1618.5347368464124
INFO:root:current train perplexity3.5745625495910645
INFO:root:current mean train loss 1616.5970760878697
INFO:root:current train perplexity3.576336622238159
INFO:root:current mean train loss 1616.5454053249712
INFO:root:current train perplexity3.580003261566162
INFO:root:current mean train loss 1615.0004388414993
INFO:root:current train perplexity3.5789005756378174
INFO:root:current mean train loss 1617.7894980738613
INFO:root:current train perplexity3.5770609378814697
INFO:root:current mean train loss 1617.8341957957475
INFO:root:current train perplexity3.576336145401001
INFO:root:current mean train loss 1619.5994285417
INFO:root:current train perplexity3.5805447101593018
INFO:root:current mean train loss 1618.8919963814972
INFO:root:current train perplexity3.5806822776794434
INFO:root:current mean train loss 1619.712307954346
INFO:root:current train perplexity3.5822205543518066
INFO:root:current mean train loss 1619.655168676509
INFO:root:current train perplexity3.5810797214508057
INFO:root:current mean train loss 1619.3318557545126
INFO:root:current train perplexity3.5827529430389404
INFO:root:current mean train loss 1619.9026876275104
INFO:root:current train perplexity3.581435203552246
INFO:root:current mean train loss 1621.0020774083348
INFO:root:current train perplexity3.585298538208008
INFO:root:current mean train loss 1621.4661342783343
INFO:root:current train perplexity3.5862343311309814
INFO:root:current mean train loss 1620.1809622418511
INFO:root:current train perplexity3.585270404815674
INFO:root:current mean train loss 1619.3972783046083
INFO:root:current train perplexity3.582736015319824
INFO:root:current mean train loss 1618.4012285804
INFO:root:current train perplexity3.5820484161376953
INFO:root:current mean train loss 1617.9533428945333
INFO:root:current train perplexity3.5819296836853027
INFO:root:current mean train loss 1617.9731044374132
INFO:root:current train perplexity3.5811290740966797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.42s/it]
INFO:root:final mean train loss: 1617.4850559898296
INFO:root:final train perplexity: 3.5810139179229736
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 3292.0990140531158
INFO:root:eval perplexity: 14.900689125061035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/41
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [3:26:18<4:56:48, 301.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1595.7231572469075
INFO:root:current train perplexity3.528343915939331
INFO:root:current mean train loss 1595.957446662747
INFO:root:current train perplexity3.5290229320526123
INFO:root:current mean train loss 1602.2684614851669
INFO:root:current train perplexity3.5440516471862793
INFO:root:current mean train loss 1603.9900931926688
INFO:root:current train perplexity3.546011209487915
INFO:root:current mean train loss 1603.6359666393648
INFO:root:current train perplexity3.550837278366089
INFO:root:current mean train loss 1606.0490638681706
INFO:root:current train perplexity3.547269105911255
INFO:root:current mean train loss 1602.9059614861148
INFO:root:current train perplexity3.5434436798095703
INFO:root:current mean train loss 1604.2338660158703
INFO:root:current train perplexity3.546856641769409
INFO:root:current mean train loss 1605.567592212132
INFO:root:current train perplexity3.548978567123413
INFO:root:current mean train loss 1605.6562215659512
INFO:root:current train perplexity3.550548553466797
INFO:root:current mean train loss 1605.4781829388473
INFO:root:current train perplexity3.54663348197937
INFO:root:current mean train loss 1606.5499401283903
INFO:root:current train perplexity3.5484201908111572
INFO:root:current mean train loss 1608.0435859774366
INFO:root:current train perplexity3.5506362915039062
INFO:root:current mean train loss 1607.7939344695783
INFO:root:current train perplexity3.551816940307617
INFO:root:current mean train loss 1608.9685505749708
INFO:root:current train perplexity3.555394411087036
INFO:root:current mean train loss 1611.264538681298
INFO:root:current train perplexity3.558765172958374
INFO:root:current mean train loss 1611.6732040981076
INFO:root:current train perplexity3.560610055923462
INFO:root:current mean train loss 1611.6548752816589
INFO:root:current train perplexity3.561574935913086
INFO:root:current mean train loss 1611.875595479072
INFO:root:current train perplexity3.562354326248169

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.92s/it]
INFO:root:final mean train loss: 1611.2752433773007
INFO:root:final train perplexity: 3.563519239425659
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 3300.565770604589
INFO:root:eval perplexity: 15.004571914672852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/42
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [3:31:20<4:51:50, 301.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1616.774423452524
INFO:root:current train perplexity3.6128153800964355
INFO:root:current mean train loss 1589.8777428010924
INFO:root:current train perplexity3.544851541519165
INFO:root:current mean train loss 1590.0841969951218
INFO:root:current train perplexity3.537471294403076
INFO:root:current mean train loss 1593.5310744995506
INFO:root:current train perplexity3.5267388820648193
INFO:root:current mean train loss 1593.427336242528
INFO:root:current train perplexity3.5278568267822266
INFO:root:current mean train loss 1595.9105912295931
INFO:root:current train perplexity3.5256035327911377
INFO:root:current mean train loss 1597.392870655651
INFO:root:current train perplexity3.5318918228149414
INFO:root:current mean train loss 1600.8657568975718
INFO:root:current train perplexity3.534191131591797
INFO:root:current mean train loss 1600.300686206325
INFO:root:current train perplexity3.536027669906616
INFO:root:current mean train loss 1602.1917310131862
INFO:root:current train perplexity3.536393880844116
INFO:root:current mean train loss 1603.5382644035737
INFO:root:current train perplexity3.5383269786834717
INFO:root:current mean train loss 1603.8924592353155
INFO:root:current train perplexity3.5408008098602295
INFO:root:current mean train loss 1603.9920589890444
INFO:root:current train perplexity3.5423130989074707
INFO:root:current mean train loss 1603.6927935563237
INFO:root:current train perplexity3.5411088466644287
INFO:root:current mean train loss 1605.4868786076831
INFO:root:current train perplexity3.544346809387207
INFO:root:current mean train loss 1605.4871317881743
INFO:root:current train perplexity3.545271635055542
INFO:root:current mean train loss 1604.2160150347033
INFO:root:current train perplexity3.5420567989349365
INFO:root:current mean train loss 1604.5887406989975
INFO:root:current train perplexity3.543398380279541
INFO:root:current mean train loss 1603.811645440482
INFO:root:current train perplexity3.5418617725372314
INFO:root:current mean train loss 1604.7298460829277
INFO:root:current train perplexity3.5437989234924316

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.70s/it]
INFO:root:final mean train loss: 1604.8397158931975
INFO:root:final train perplexity: 3.5454790592193604
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 3312.186936936937
INFO:root:eval perplexity: 15.148347854614258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/43
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [3:36:21<4:46:46, 301.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.5278483072916
INFO:root:current train perplexity3.563682794570923
INFO:root:current mean train loss 1603.9422241210937
INFO:root:current train perplexity3.532637119293213
INFO:root:current mean train loss 1609.2125923488452
INFO:root:current train perplexity3.5397284030914307
INFO:root:current mean train loss 1605.2921046401516
INFO:root:current train perplexity3.5258824825286865
INFO:root:current mean train loss 1601.3485961914062
INFO:root:current train perplexity3.5245273113250732
INFO:root:current mean train loss 1601.138313264667
INFO:root:current train perplexity3.520159959793091
INFO:root:current mean train loss 1599.022291201637
INFO:root:current train perplexity3.5197670459747314
INFO:root:current mean train loss 1600.5006275751819
INFO:root:current train perplexity3.5240962505340576
INFO:root:current mean train loss 1601.463053875659
INFO:root:current train perplexity3.5231618881225586
INFO:root:current mean train loss 1601.6197248561407
INFO:root:current train perplexity3.5247113704681396
INFO:root:current mean train loss 1600.7251228999166
INFO:root:current train perplexity3.525388240814209
INFO:root:current mean train loss 1600.5638601657563
INFO:root:current train perplexity3.5253078937530518
INFO:root:current mean train loss 1600.6211183625508
INFO:root:current train perplexity3.526547431945801
INFO:root:current mean train loss 1600.207755962171
INFO:root:current train perplexity3.525761365890503
INFO:root:current mean train loss 1600.7368014675753
INFO:root:current train perplexity3.5284695625305176
INFO:root:current mean train loss 1599.2897274241727
INFO:root:current train perplexity3.5269839763641357
INFO:root:current mean train loss 1599.310560729606
INFO:root:current train perplexity3.5292277336120605
INFO:root:current mean train loss 1600.134665640241
INFO:root:current train perplexity3.5297255516052246
INFO:root:current mean train loss 1599.934571112961
INFO:root:current train perplexity3.5304152965545654
INFO:root:current mean train loss 1599.2617788364232
INFO:root:current train perplexity3.5287909507751465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.81s/it]
INFO:root:final mean train loss: 1599.1556825546443
INFO:root:final train perplexity: 3.5296201705932617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.52s/it]
INFO:root:eval mean loss: 3319.3011170350037
INFO:root:eval perplexity: 15.237030029296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/44
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [3:41:23<4:41:43, 301.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.205348238032
INFO:root:current train perplexity3.484219551086426
INFO:root:current mean train loss 1566.009168560002
INFO:root:current train perplexity3.4638988971710205
INFO:root:current mean train loss 1578.9563932225772
INFO:root:current train perplexity3.4728357791900635
INFO:root:current mean train loss 1586.354815480345
INFO:root:current train perplexity3.4939751625061035
INFO:root:current mean train loss 1586.7844017080013
INFO:root:current train perplexity3.4941396713256836
INFO:root:current mean train loss 1587.837665676417
INFO:root:current train perplexity3.4976136684417725
INFO:root:current mean train loss 1588.2354668406467
INFO:root:current train perplexity3.500190496444702
INFO:root:current mean train loss 1588.7444918018427
INFO:root:current train perplexity3.495553731918335
INFO:root:current mean train loss 1591.3997159955634
INFO:root:current train perplexity3.501460313796997
INFO:root:current mean train loss 1591.569081613607
INFO:root:current train perplexity3.5000853538513184
INFO:root:current mean train loss 1591.6414874203454
INFO:root:current train perplexity3.5044596195220947
INFO:root:current mean train loss 1593.5794692633976
INFO:root:current train perplexity3.5078015327453613
INFO:root:current mean train loss 1591.8614988472334
INFO:root:current train perplexity3.506228446960449
INFO:root:current mean train loss 1590.6703730910995
INFO:root:current train perplexity3.5050055980682373
INFO:root:current mean train loss 1591.4243068734613
INFO:root:current train perplexity3.505899429321289
INFO:root:current mean train loss 1592.5347232041854
INFO:root:current train perplexity3.5102529525756836
INFO:root:current mean train loss 1592.0535568487448
INFO:root:current train perplexity3.5100154876708984
INFO:root:current mean train loss 1591.8078622364903
INFO:root:current train perplexity3.508737564086914
INFO:root:current mean train loss 1592.5212537830562
INFO:root:current train perplexity3.510711908340454
INFO:root:current mean train loss 1593.6623205999012
INFO:root:current train perplexity3.51297926902771

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.38s/it]
INFO:root:final mean train loss: 1593.1575176106278
INFO:root:final train perplexity: 3.512963056564331
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.57s/it]
INFO:root:eval mean loss: 3313.3929754950263
INFO:root:eval perplexity: 15.163347244262695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/45
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [3:46:25<4:36:34, 301.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.7926616668701
INFO:root:current train perplexity3.474811553955078
INFO:root:current mean train loss 1589.1038051698265
INFO:root:current train perplexity3.489619493484497
INFO:root:current mean train loss 1589.6852185798414
INFO:root:current train perplexity3.497868776321411
INFO:root:current mean train loss 1591.1405368008457
INFO:root:current train perplexity3.51135516166687
INFO:root:current mean train loss 1589.5965428845636
INFO:root:current train perplexity3.503354787826538
INFO:root:current mean train loss 1588.6734621304993
INFO:root:current train perplexity3.5014328956604004
INFO:root:current mean train loss 1585.8049404649848
INFO:root:current train perplexity3.4958107471466064
INFO:root:current mean train loss 1585.0937124521945
INFO:root:current train perplexity3.495858669281006
INFO:root:current mean train loss 1587.1874214454933
INFO:root:current train perplexity3.4986841678619385
INFO:root:current mean train loss 1587.8790430092713
INFO:root:current train perplexity3.4993348121643066
INFO:root:current mean train loss 1586.732245882651
INFO:root:current train perplexity3.498964309692383
INFO:root:current mean train loss 1587.8868024373792
INFO:root:current train perplexity3.4987525939941406
INFO:root:current mean train loss 1588.9240638636336
INFO:root:current train perplexity3.499593496322632
INFO:root:current mean train loss 1589.691928002142
INFO:root:current train perplexity3.499110460281372
INFO:root:current mean train loss 1590.5757866531121
INFO:root:current train perplexity3.4994571208953857
INFO:root:current mean train loss 1590.0284805492977
INFO:root:current train perplexity3.4984588623046875
INFO:root:current mean train loss 1588.873005940364
INFO:root:current train perplexity3.4988908767700195
INFO:root:current mean train loss 1588.7120988287893
INFO:root:current train perplexity3.4982128143310547
INFO:root:current mean train loss 1588.3889659832475
INFO:root:current train perplexity3.4986581802368164
INFO:root:current mean train loss 1588.294898940201
INFO:root:current train perplexity3.4979729652404785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it]
INFO:root:final mean train loss: 1587.651341825438
INFO:root:final train perplexity: 3.497741222381592
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.38s/it]
INFO:root:eval mean loss: 3317.8610055367867
INFO:root:eval perplexity: 15.219037055969238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/46
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [3:51:26<4:31:29, 301.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1588.7375578703704
INFO:root:current train perplexity3.4860777854919434
INFO:root:current mean train loss 1590.1769972321738
INFO:root:current train perplexity3.4831550121307373
INFO:root:current mean train loss 1589.1962126056494
INFO:root:current train perplexity3.4810123443603516
INFO:root:current mean train loss 1587.5114960758078
INFO:root:current train perplexity3.4760780334472656
INFO:root:current mean train loss 1582.9740114588747
INFO:root:current train perplexity3.4735124111175537
INFO:root:current mean train loss 1583.2625751331218
INFO:root:current train perplexity3.4748194217681885
INFO:root:current mean train loss 1584.2852073366946
INFO:root:current train perplexity3.4811770915985107
INFO:root:current mean train loss 1585.9690677579326
INFO:root:current train perplexity3.4831745624542236
INFO:root:current mean train loss 1588.3361662605969
INFO:root:current train perplexity3.4850854873657227
INFO:root:current mean train loss 1585.6409766520928
INFO:root:current train perplexity3.4829587936401367
INFO:root:current mean train loss 1585.2590383976064
INFO:root:current train perplexity3.4824297428131104
INFO:root:current mean train loss 1585.3757164007925
INFO:root:current train perplexity3.4827821254730225
INFO:root:current mean train loss 1585.9459341914276
INFO:root:current train perplexity3.486072063446045
INFO:root:current mean train loss 1585.3911901828951
INFO:root:current train perplexity3.485800266265869
INFO:root:current mean train loss 1584.601760895167
INFO:root:current train perplexity3.4854142665863037
INFO:root:current mean train loss 1583.379323574503
INFO:root:current train perplexity3.4834682941436768
INFO:root:current mean train loss 1583.7435315805556
INFO:root:current train perplexity3.483086585998535
INFO:root:current mean train loss 1583.8605794590687
INFO:root:current train perplexity3.4843356609344482
INFO:root:current mean train loss 1582.7236006887335
INFO:root:current train perplexity3.483261823654175
INFO:root:current mean train loss 1583.0670567781622
INFO:root:current train perplexity3.4837851524353027

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.80s/it]
INFO:root:final mean train loss: 1582.538242480518
INFO:root:final train perplexity: 3.4836645126342773
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 3317.36902551966
INFO:root:eval perplexity: 15.212894439697266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/47
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [3:56:28<4:26:30, 301.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.973236706792
INFO:root:current train perplexity3.4357051849365234
INFO:root:current mean train loss 1571.5972037267204
INFO:root:current train perplexity3.436366558074951
INFO:root:current mean train loss 1570.4951401268877
INFO:root:current train perplexity3.4494004249572754
INFO:root:current mean train loss 1568.5236819473344
INFO:root:current train perplexity3.448716163635254
INFO:root:current mean train loss 1569.615010579427
INFO:root:current train perplexity3.4480671882629395
INFO:root:current mean train loss 1570.2961019560644
INFO:root:current train perplexity3.4498543739318848
INFO:root:current mean train loss 1574.2653359137155
INFO:root:current train perplexity3.4585134983062744
INFO:root:current mean train loss 1574.5216468294761
INFO:root:current train perplexity3.4611172676086426
INFO:root:current mean train loss 1575.4029918917038
INFO:root:current train perplexity3.463620901107788
INFO:root:current mean train loss 1574.461353248489
INFO:root:current train perplexity3.462507724761963
INFO:root:current mean train loss 1576.7097065687615
INFO:root:current train perplexity3.463864326477051
INFO:root:current mean train loss 1577.156389290582
INFO:root:current train perplexity3.465494394302368
INFO:root:current mean train loss 1575.257245503148
INFO:root:current train perplexity3.462890148162842
INFO:root:current mean train loss 1575.8282998975938
INFO:root:current train perplexity3.464998245239258
INFO:root:current mean train loss 1576.0210489800202
INFO:root:current train perplexity3.4666059017181396
INFO:root:current mean train loss 1576.6551453324223
INFO:root:current train perplexity3.4690515995025635
INFO:root:current mean train loss 1577.1561170742004
INFO:root:current train perplexity3.4700260162353516
INFO:root:current mean train loss 1577.0292278964475
INFO:root:current train perplexity3.4690258502960205
INFO:root:current mean train loss 1576.9804966628112
INFO:root:current train perplexity3.4682443141937256

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.27s/it]
INFO:root:final mean train loss: 1577.0630060716285
INFO:root:final train perplexity: 3.4686546325683594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 3331.3908559438346
INFO:root:eval perplexity: 15.38894271850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/48
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [4:01:30<4:21:40, 301.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1509.472265625
INFO:root:current train perplexity3.2747998237609863
INFO:root:current mean train loss 1566.7513459578804
INFO:root:current train perplexity3.4369771480560303
INFO:root:current mean train loss 1573.085758085029
INFO:root:current train perplexity3.4385032653808594
INFO:root:current mean train loss 1571.4953322637648
INFO:root:current train perplexity3.435814380645752
INFO:root:current mean train loss 1575.9784973879894
INFO:root:current train perplexity3.447307586669922
INFO:root:current mean train loss 1571.3416283468598
INFO:root:current train perplexity3.438532590866089
INFO:root:current mean train loss 1567.8294413744918
INFO:root:current train perplexity3.436145067214966
INFO:root:current mean train loss 1569.6376828493771
INFO:root:current train perplexity3.442823886871338
INFO:root:current mean train loss 1568.5853794214916
INFO:root:current train perplexity3.4436821937561035
INFO:root:current mean train loss 1567.7589142012466
INFO:root:current train perplexity3.4446818828582764
INFO:root:current mean train loss 1569.4316085138933
INFO:root:current train perplexity3.4472267627716064
INFO:root:current mean train loss 1570.9070864279709
INFO:root:current train perplexity3.4471211433410645
INFO:root:current mean train loss 1571.9239776234567
INFO:root:current train perplexity3.4475533962249756
INFO:root:current mean train loss 1571.973645901408
INFO:root:current train perplexity3.4483203887939453
INFO:root:current mean train loss 1571.2175476721234
INFO:root:current train perplexity3.448993682861328
INFO:root:current mean train loss 1570.5314419283725
INFO:root:current train perplexity3.4499287605285645
INFO:root:current mean train loss 1571.4404924989117
INFO:root:current train perplexity3.451292037963867
INFO:root:current mean train loss 1570.9543701883656
INFO:root:current train perplexity3.451385021209717
INFO:root:current mean train loss 1571.962447540031
INFO:root:current train perplexity3.4530508518218994
INFO:root:current mean train loss 1571.9547715149722
INFO:root:current train perplexity3.454540967941284

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.62s/it]
INFO:root:final mean train loss: 1572.2719359982211
INFO:root:final train perplexity: 3.4555723667144775
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.94s/it]
INFO:root:eval mean loss: 3339.8722022804054
INFO:root:eval perplexity: 15.496415138244629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/49
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [4:06:32<4:16:39, 301.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1586.4981002807617
INFO:root:current train perplexity3.4736204147338867
INFO:root:current mean train loss 1568.2184586958451
INFO:root:current train perplexity3.4172005653381348
INFO:root:current mean train loss 1562.1627818140491
INFO:root:current train perplexity3.408860921859741
INFO:root:current mean train loss 1564.32566153285
INFO:root:current train perplexity3.419095993041992
INFO:root:current mean train loss 1566.0826885082104
INFO:root:current train perplexity3.425727367401123
INFO:root:current mean train loss 1561.9784770764802
INFO:root:current train perplexity3.4238951206207275
INFO:root:current mean train loss 1562.6058173843578
INFO:root:current train perplexity3.42767596244812
INFO:root:current mean train loss 1563.5280506571785
INFO:root:current train perplexity3.429722785949707
INFO:root:current mean train loss 1563.449767919687
INFO:root:current train perplexity3.4307501316070557
INFO:root:current mean train loss 1563.713559523161
INFO:root:current train perplexity3.432077407836914
INFO:root:current mean train loss 1562.8613918807155
INFO:root:current train perplexity3.4306273460388184
INFO:root:current mean train loss 1564.810763301782
INFO:root:current train perplexity3.4332022666931152
INFO:root:current mean train loss 1565.4588076108462
INFO:root:current train perplexity3.434211254119873
INFO:root:current mean train loss 1564.261908453864
INFO:root:current train perplexity3.432158946990967
INFO:root:current mean train loss 1564.004884687882
INFO:root:current train perplexity3.432584285736084
INFO:root:current mean train loss 1563.9704083076656
INFO:root:current train perplexity3.4320836067199707
INFO:root:current mean train loss 1565.138214260924
INFO:root:current train perplexity3.434138536453247
INFO:root:current mean train loss 1564.8293048250757
INFO:root:current train perplexity3.4350571632385254
INFO:root:current mean train loss 1566.1411134145146
INFO:root:current train perplexity3.437285900115967
INFO:root:current mean train loss 1566.1718064460201
INFO:root:current train perplexity3.438697576522827

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.32s/it]
INFO:root:final mean train loss: 1566.619163986414
INFO:root:final train perplexity: 3.440201759338379
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 3337.4676854588965
INFO:root:eval perplexity: 15.465871810913086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/50
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [4:11:34<4:11:31, 301.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1540.442233338648
INFO:root:current train perplexity3.390077829360962
INFO:root:current mean train loss 1552.2231732054845
INFO:root:current train perplexity3.3834612369537354
INFO:root:current mean train loss 1552.4764374882343
INFO:root:current train perplexity3.396545171737671
INFO:root:current mean train loss 1554.883948558381
INFO:root:current train perplexity3.4056756496429443
INFO:root:current mean train loss 1558.594970159382
INFO:root:current train perplexity3.4062466621398926
INFO:root:current mean train loss 1558.477795877092
INFO:root:current train perplexity3.4072537422180176
INFO:root:current mean train loss 1556.9475470074153
INFO:root:current train perplexity3.408721685409546
INFO:root:current mean train loss 1559.660123654456
INFO:root:current train perplexity3.4147822856903076
INFO:root:current mean train loss 1559.515815941549
INFO:root:current train perplexity3.4148080348968506
INFO:root:current mean train loss 1561.7173735253803
INFO:root:current train perplexity3.4203104972839355
INFO:root:current mean train loss 1562.3610637362965
INFO:root:current train perplexity3.423994779586792
INFO:root:current mean train loss 1561.8971465719171
INFO:root:current train perplexity3.4254751205444336
INFO:root:current mean train loss 1562.1050418459768
INFO:root:current train perplexity3.427791118621826
INFO:root:current mean train loss 1560.867309479823
INFO:root:current train perplexity3.4263947010040283
INFO:root:current mean train loss 1560.6321096479523
INFO:root:current train perplexity3.426361560821533
INFO:root:current mean train loss 1560.5071771669727
INFO:root:current train perplexity3.4262261390686035
INFO:root:current mean train loss 1560.7730276872346
INFO:root:current train perplexity3.4267194271087646
INFO:root:current mean train loss 1560.8111586224495
INFO:root:current train perplexity3.426818370819092
INFO:root:current mean train loss 1561.6691544627163
INFO:root:current train perplexity3.427800178527832
INFO:root:current mean train loss 1563.357902264583
INFO:root:current train perplexity3.4302797317504883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it]
INFO:root:final mean train loss: 1562.680093970133
INFO:root:final train perplexity: 3.4295310974121094
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 3331.533299901464
INFO:root:eval perplexity: 15.39074420928955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/51
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [4:16:36<4:06:39, 302.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1555.3643336440578
INFO:root:current train perplexity3.3874168395996094
INFO:root:current mean train loss 1548.2207082725433
INFO:root:current train perplexity3.392840623855591
INFO:root:current mean train loss 1552.5473573154077
INFO:root:current train perplexity3.3893654346466064
INFO:root:current mean train loss 1552.1590642876963
INFO:root:current train perplexity3.3995442390441895
INFO:root:current mean train loss 1552.3411448728373
INFO:root:current train perplexity3.3984181880950928
INFO:root:current mean train loss 1557.9986846168977
INFO:root:current train perplexity3.402811050415039
INFO:root:current mean train loss 1558.6705457899307
INFO:root:current train perplexity3.4041011333465576
INFO:root:current mean train loss 1556.4923406456533
INFO:root:current train perplexity3.4034557342529297
INFO:root:current mean train loss 1558.7468339246084
INFO:root:current train perplexity3.4086642265319824
INFO:root:current mean train loss 1557.1913172877846
INFO:root:current train perplexity3.4064865112304688
INFO:root:current mean train loss 1557.625243224525
INFO:root:current train perplexity3.411860942840576
INFO:root:current mean train loss 1556.5796962103345
INFO:root:current train perplexity3.4100725650787354
INFO:root:current mean train loss 1557.219948140366
INFO:root:current train perplexity3.4109628200531006
INFO:root:current mean train loss 1556.4261566653608
INFO:root:current train perplexity3.411909341812134
INFO:root:current mean train loss 1555.689833657966
INFO:root:current train perplexity3.411391496658325
INFO:root:current mean train loss 1557.0519064638959
INFO:root:current train perplexity3.414036273956299
INFO:root:current mean train loss 1557.0815143928664
INFO:root:current train perplexity3.414313554763794
INFO:root:current mean train loss 1557.85100640959
INFO:root:current train perplexity3.4148287773132324
INFO:root:current mean train loss 1557.7030905900406
INFO:root:current train perplexity3.4151549339294434
INFO:root:current mean train loss 1557.6771175660049
INFO:root:current train perplexity3.414912700653076

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.10s/it]
INFO:root:final mean train loss: 1557.6060690367638
INFO:root:final train perplexity: 3.415834903717041
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.51s/it]
INFO:root:eval mean loss: 3353.169146930133
INFO:root:eval perplexity: 15.666421890258789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/52
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [4:21:39<4:01:38, 302.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1548.1975362387047
INFO:root:current train perplexity3.3977532386779785
INFO:root:current mean train loss 1548.2544498964737
INFO:root:current train perplexity3.3978397846221924
INFO:root:current mean train loss 1542.300657885656
INFO:root:current train perplexity3.3818647861480713
INFO:root:current mean train loss 1544.3206685118514
INFO:root:current train perplexity3.388470411300659
INFO:root:current mean train loss 1543.1974453185655
INFO:root:current train perplexity3.3843460083007812
INFO:root:current mean train loss 1543.862486431979
INFO:root:current train perplexity3.3898890018463135
INFO:root:current mean train loss 1546.7529180702668
INFO:root:current train perplexity3.3930234909057617
INFO:root:current mean train loss 1546.8816730211825
INFO:root:current train perplexity3.390726089477539
INFO:root:current mean train loss 1548.0848911237879
INFO:root:current train perplexity3.39080810546875
INFO:root:current mean train loss 1548.7002973896076
INFO:root:current train perplexity3.39324688911438
INFO:root:current mean train loss 1548.5502744834948
INFO:root:current train perplexity3.392860174179077
INFO:root:current mean train loss 1549.3231973011214
INFO:root:current train perplexity3.396716833114624
INFO:root:current mean train loss 1548.244897213562
INFO:root:current train perplexity3.3968796730041504
INFO:root:current mean train loss 1549.464051749339
INFO:root:current train perplexity3.398968458175659
INFO:root:current mean train loss 1550.850948032783
INFO:root:current train perplexity3.3998353481292725
INFO:root:current mean train loss 1552.0744456943648
INFO:root:current train perplexity3.4009621143341064
INFO:root:current mean train loss 1552.0088539055537
INFO:root:current train perplexity3.3998069763183594
INFO:root:current mean train loss 1553.085342347321
INFO:root:current train perplexity3.4021100997924805
INFO:root:current mean train loss 1553.1924221628344
INFO:root:current train perplexity3.4027583599090576
INFO:root:current mean train loss 1553.0890045550755
INFO:root:current train perplexity3.4036877155303955

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.30s/it]
INFO:root:final mean train loss: 1553.0890045550755
INFO:root:final train perplexity: 3.4036877155303955
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.60s/it]
INFO:root:eval mean loss: 3363.6897837779184
INFO:root:eval perplexity: 15.802261352539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/53
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [4:26:40<3:56:27, 301.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1539.324219970703
INFO:root:current train perplexity3.4167418479919434
INFO:root:current mean train loss 1551.5541723632812
INFO:root:current train perplexity3.4004006385803223
INFO:root:current mean train loss 1541.1717622884114
INFO:root:current train perplexity3.3848419189453125
INFO:root:current mean train loss 1542.284951171875
INFO:root:current train perplexity3.383744955062866
INFO:root:current mean train loss 1542.0061577148438
INFO:root:current train perplexity3.3820438385009766
INFO:root:current mean train loss 1547.3824186197917
INFO:root:current train perplexity3.3899424076080322
INFO:root:current mean train loss 1545.482653111049
INFO:root:current train perplexity3.384228467941284
INFO:root:current mean train loss 1545.4479315185547
INFO:root:current train perplexity3.3862645626068115
INFO:root:current mean train loss 1546.9359978569878
INFO:root:current train perplexity3.3887722492218018
INFO:root:current mean train loss 1546.7413868408203
INFO:root:current train perplexity3.389134168624878
INFO:root:current mean train loss 1549.535352228338
INFO:root:current train perplexity3.3931257724761963
INFO:root:current mean train loss 1549.9536510213215
INFO:root:current train perplexity3.3936262130737305
INFO:root:current mean train loss 1550.6146668419472
INFO:root:current train perplexity3.3936448097229004
INFO:root:current mean train loss 1548.2799976457868
INFO:root:current train perplexity3.3906638622283936
INFO:root:current mean train loss 1548.2773050130209
INFO:root:current train perplexity3.3907954692840576
INFO:root:current mean train loss 1548.526110534668
INFO:root:current train perplexity3.390995979309082
INFO:root:current mean train loss 1549.1859270881205
INFO:root:current train perplexity3.3911075592041016
INFO:root:current mean train loss 1550.6357728407118
INFO:root:current train perplexity3.3935697078704834
INFO:root:current mean train loss 1549.2436119320519
INFO:root:current train perplexity3.3919191360473633

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it]
INFO:root:final mean train loss: 1548.549188764902
INFO:root:final train perplexity: 3.3915228843688965
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it]
INFO:root:eval mean loss: 3351.2783291103606
INFO:root:eval perplexity: 15.642135620117188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/54
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [4:31:42<3:51:34, 302.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1549.3045151654412
INFO:root:current train perplexity3.4331045150756836
INFO:root:current mean train loss 1532.8909943409456
INFO:root:current train perplexity3.3648886680603027
INFO:root:current mean train loss 1538.8682068152361
INFO:root:current train perplexity3.3644988536834717
INFO:root:current mean train loss 1537.4274609683064
INFO:root:current train perplexity3.3669607639312744
INFO:root:current mean train loss 1542.8274777638826
INFO:root:current train perplexity3.37699031829834
INFO:root:current mean train loss 1541.4454161535148
INFO:root:current train perplexity3.372413396835327
INFO:root:current mean train loss 1541.165185665582
INFO:root:current train perplexity3.372896671295166
INFO:root:current mean train loss 1540.911256074573
INFO:root:current train perplexity3.3758902549743652
INFO:root:current mean train loss 1541.2564788407474
INFO:root:current train perplexity3.3765757083892822
INFO:root:current mean train loss 1542.1196520689919
INFO:root:current train perplexity3.3784611225128174
INFO:root:current mean train loss 1542.65720807791
INFO:root:current train perplexity3.38002347946167
INFO:root:current mean train loss 1543.0367891726582
INFO:root:current train perplexity3.381814479827881
INFO:root:current mean train loss 1543.2253000702933
INFO:root:current train perplexity3.382791519165039
INFO:root:current mean train loss 1542.1771319019315
INFO:root:current train perplexity3.3818418979644775
INFO:root:current mean train loss 1543.301450784558
INFO:root:current train perplexity3.3829822540283203
INFO:root:current mean train loss 1543.007717949824
INFO:root:current train perplexity3.3806169033050537
INFO:root:current mean train loss 1543.5087489008388
INFO:root:current train perplexity3.381718635559082
INFO:root:current mean train loss 1543.46656503383
INFO:root:current train perplexity3.3811776638031006
INFO:root:current mean train loss 1543.8668114132586
INFO:root:current train perplexity3.3808274269104004
INFO:root:current mean train loss 1544.313027570443
INFO:root:current train perplexity3.3800313472747803

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.25s/it]
INFO:root:final mean train loss: 1544.3838189183252
INFO:root:final train perplexity: 3.3804001808166504
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.84s/it]
INFO:root:eval mean loss: 3359.0450831691064
INFO:root:eval perplexity: 15.742144584655762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/55
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [4:36:45<3:46:39, 302.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1522.6120425953584
INFO:root:current train perplexity3.345838785171509
INFO:root:current mean train loss 1533.0022027314599
INFO:root:current train perplexity3.3504650592803955
INFO:root:current mean train loss 1542.7498800163596
INFO:root:current train perplexity3.3621363639831543
INFO:root:current mean train loss 1544.174773256222
INFO:root:current train perplexity3.3658759593963623
INFO:root:current mean train loss 1543.4413691226239
INFO:root:current train perplexity3.36678409576416
INFO:root:current mean train loss 1542.4681885680009
INFO:root:current train perplexity3.364708662033081
INFO:root:current mean train loss 1540.0804449135574
INFO:root:current train perplexity3.3599741458892822
INFO:root:current mean train loss 1539.914226978936
INFO:root:current train perplexity3.3603525161743164
INFO:root:current mean train loss 1539.2460152971373
INFO:root:current train perplexity3.360856533050537
INFO:root:current mean train loss 1536.842557788661
INFO:root:current train perplexity3.359347343444824
INFO:root:current mean train loss 1536.0387500047223
INFO:root:current train perplexity3.3610341548919678
INFO:root:current mean train loss 1536.080144650091
INFO:root:current train perplexity3.3603553771972656
INFO:root:current mean train loss 1537.1619790941236
INFO:root:current train perplexity3.3636832237243652
INFO:root:current mean train loss 1537.398194274445
INFO:root:current train perplexity3.364793539047241
INFO:root:current mean train loss 1537.6209807030161
INFO:root:current train perplexity3.3664402961730957
INFO:root:current mean train loss 1538.7221331938304
INFO:root:current train perplexity3.367373466491699
INFO:root:current mean train loss 1539.5029605412572
INFO:root:current train perplexity3.3689496517181396
INFO:root:current mean train loss 1540.5759567383939
INFO:root:current train perplexity3.371410369873047
INFO:root:current mean train loss 1540.5620629696446
INFO:root:current train perplexity3.3709235191345215
INFO:root:current mean train loss 1541.4240912010405
INFO:root:current train perplexity3.371417284011841

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.99s/it]
INFO:root:final mean train loss: 1541.035184505307
INFO:root:final train perplexity: 3.3714845180511475
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 3368.273731495167
INFO:root:eval perplexity: 15.861810684204102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/56
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [4:41:48<3:41:50, 302.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1541.3380533854167
INFO:root:current train perplexity3.3682165145874023
INFO:root:current mean train loss 1536.3287660712438
INFO:root:current train perplexity3.3528645038604736
INFO:root:current mean train loss 1533.7731165183018
INFO:root:current train perplexity3.3545804023742676
INFO:root:current mean train loss 1532.7923166649973
INFO:root:current train perplexity3.3479065895080566
INFO:root:current mean train loss 1531.8571490437917
INFO:root:current train perplexity3.3455991744995117
INFO:root:current mean train loss 1532.9178544337
INFO:root:current train perplexity3.345665693283081
INFO:root:current mean train loss 1534.3820298249088
INFO:root:current train perplexity3.3493568897247314
INFO:root:current mean train loss 1532.1301041970082
INFO:root:current train perplexity3.3476932048797607
INFO:root:current mean train loss 1531.5848108957293
INFO:root:current train perplexity3.3476274013519287
INFO:root:current mean train loss 1531.2450695659586
INFO:root:current train perplexity3.348177671432495
INFO:root:current mean train loss 1532.4735484899054
INFO:root:current train perplexity3.3510055541992188
INFO:root:current mean train loss 1533.447842250794
INFO:root:current train perplexity3.3509678840637207
INFO:root:current mean train loss 1535.2704800027166
INFO:root:current train perplexity3.3531782627105713
INFO:root:current mean train loss 1536.2764643196879
INFO:root:current train perplexity3.355186939239502
INFO:root:current mean train loss 1536.42969751128
INFO:root:current train perplexity3.356304407119751
INFO:root:current mean train loss 1536.5370999304885
INFO:root:current train perplexity3.3572998046875
INFO:root:current mean train loss 1536.1654401717944
INFO:root:current train perplexity3.356701612472534
INFO:root:current mean train loss 1536.0608749520363
INFO:root:current train perplexity3.3568153381347656
INFO:root:current mean train loss 1536.782033861553
INFO:root:current train perplexity3.357856273651123
INFO:root:current mean train loss 1536.0347125797869
INFO:root:current train perplexity3.357029438018799

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.77s/it]
INFO:root:final mean train loss: 1535.524642136382
INFO:root:final train perplexity: 3.3568637371063232
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 3366.9087317297767
INFO:root:eval perplexity: 15.844053268432617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/57
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [4:46:50<3:36:39, 302.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1513.0100133559283
INFO:root:current train perplexity3.28887939453125
INFO:root:current mean train loss 1511.7064020066034
INFO:root:current train perplexity3.306168794631958
INFO:root:current mean train loss 1523.0844608136078
INFO:root:current train perplexity3.3203928470611572
INFO:root:current mean train loss 1520.8472814145296
INFO:root:current train perplexity3.3203656673431396
INFO:root:current mean train loss 1523.072556976579
INFO:root:current train perplexity3.3226499557495117
INFO:root:current mean train loss 1524.1557137932575
INFO:root:current train perplexity3.3256499767303467
INFO:root:current mean train loss 1523.6740627631457
INFO:root:current train perplexity3.324915647506714
INFO:root:current mean train loss 1524.1775398254395
INFO:root:current train perplexity3.32619571685791
INFO:root:current mean train loss 1526.000992454142
INFO:root:current train perplexity3.328564167022705
INFO:root:current mean train loss 1528.97371629447
INFO:root:current train perplexity3.33486008644104
INFO:root:current mean train loss 1527.7548062328096
INFO:root:current train perplexity3.33483624458313
INFO:root:current mean train loss 1527.7975982247967
INFO:root:current train perplexity3.3366119861602783
INFO:root:current mean train loss 1529.4998038980864
INFO:root:current train perplexity3.339053153991699
INFO:root:current mean train loss 1528.3961228933947
INFO:root:current train perplexity3.3398022651672363
INFO:root:current mean train loss 1528.9440369151268
INFO:root:current train perplexity3.342284917831421
INFO:root:current mean train loss 1529.6517029587103
INFO:root:current train perplexity3.3429806232452393
INFO:root:current mean train loss 1530.0372300548233
INFO:root:current train perplexity3.3431925773620605
INFO:root:current mean train loss 1530.7157169531913
INFO:root:current train perplexity3.3440048694610596
INFO:root:current mean train loss 1531.0561487496027
INFO:root:current train perplexity3.344575881958008
INFO:root:current mean train loss 1532.0198539175638
INFO:root:current train perplexity3.3469908237457275

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.97s/it]
INFO:root:final mean train loss: 1531.81153962736
INFO:root:final train perplexity: 3.3470475673675537
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.05s/it]
INFO:root:eval mean loss: 3371.4199907915727
INFO:root:eval perplexity: 15.902815818786621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/58
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [4:51:52<3:31:27, 302.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1527.0691104664522
INFO:root:current train perplexity3.3404433727264404
INFO:root:current mean train loss 1524.3927397856842
INFO:root:current train perplexity3.3262102603912354
INFO:root:current mean train loss 1517.3874970017819
INFO:root:current train perplexity3.3078458309173584
INFO:root:current mean train loss 1516.517276278409
INFO:root:current train perplexity3.3059921264648438
INFO:root:current mean train loss 1519.0076587165754
INFO:root:current train perplexity3.3106601238250732
INFO:root:current mean train loss 1519.9725266676683
INFO:root:current train perplexity3.3142409324645996
INFO:root:current mean train loss 1516.9696554587706
INFO:root:current train perplexity3.31254243850708
INFO:root:current mean train loss 1518.4588168976413
INFO:root:current train perplexity3.312750816345215
INFO:root:current mean train loss 1520.020292223914
INFO:root:current train perplexity3.3171846866607666
INFO:root:current mean train loss 1520.585265307741
INFO:root:current train perplexity3.320222854614258
INFO:root:current mean train loss 1521.3506490540394
INFO:root:current train perplexity3.3200981616973877
INFO:root:current mean train loss 1522.5852086835773
INFO:root:current train perplexity3.322599172592163
INFO:root:current mean train loss 1521.9415535893422
INFO:root:current train perplexity3.3233745098114014
INFO:root:current mean train loss 1523.1740372750733
INFO:root:current train perplexity3.3243024349212646
INFO:root:current mean train loss 1523.5477369725904
INFO:root:current train perplexity3.326320171356201
INFO:root:current mean train loss 1524.297794570682
INFO:root:current train perplexity3.3301730155944824
INFO:root:current mean train loss 1524.348755245039
INFO:root:current train perplexity3.3309314250946045
INFO:root:current mean train loss 1526.2417350533963
INFO:root:current train perplexity3.33362078666687
INFO:root:current mean train loss 1526.9736741286058
INFO:root:current train perplexity3.3349294662475586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.73s/it]
INFO:root:final mean train loss: 1528.1381140019757
INFO:root:final train perplexity: 3.3373653888702393
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.79s/it]
INFO:root:eval mean loss: 3380.5583532751502
INFO:root:eval perplexity: 16.02251434326172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/59
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [4:56:55<3:26:36, 302.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1611.864013671875
INFO:root:current train perplexity3.5385303497314453
INFO:root:current mean train loss 1526.1377024931066
INFO:root:current train perplexity3.311476707458496
INFO:root:current mean train loss 1517.6985262124845
INFO:root:current train perplexity3.306241273880005
INFO:root:current mean train loss 1514.16138301622
INFO:root:current train perplexity3.2971303462982178
INFO:root:current mean train loss 1517.3465239112056
INFO:root:current train perplexity3.3044779300689697
INFO:root:current mean train loss 1521.037799971987
INFO:root:current train perplexity3.310504198074341
INFO:root:current mean train loss 1519.9681118683166
INFO:root:current train perplexity3.3111226558685303
INFO:root:current mean train loss 1523.77095696865
INFO:root:current train perplexity3.317911386489868
INFO:root:current mean train loss 1523.050420214113
INFO:root:current train perplexity3.3218767642974854
INFO:root:current mean train loss 1522.1957402874255
INFO:root:current train perplexity3.322608709335327
INFO:root:current mean train loss 1521.6541204699975
INFO:root:current train perplexity3.322735071182251
INFO:root:current mean train loss 1522.369417332478
INFO:root:current train perplexity3.3227860927581787
INFO:root:current mean train loss 1521.7744433106282
INFO:root:current train perplexity3.322504997253418
INFO:root:current mean train loss 1521.9466687743015
INFO:root:current train perplexity3.323187828063965
INFO:root:current mean train loss 1522.4008182193686
INFO:root:current train perplexity3.3245699405670166
INFO:root:current mean train loss 1524.0258719981432
INFO:root:current train perplexity3.3250672817230225
INFO:root:current mean train loss 1524.6994008648858
INFO:root:current train perplexity3.3256797790527344
INFO:root:current mean train loss 1524.4092149745704
INFO:root:current train perplexity3.3259470462799072
INFO:root:current mean train loss 1524.370610075177
INFO:root:current train perplexity3.325836420059204
INFO:root:current mean train loss 1524.2061670666362
INFO:root:current train perplexity3.326475143432617

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.73s/it]
INFO:root:final mean train loss: 1524.1517405392121
INFO:root:final train perplexity: 3.3268892765045166
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.68s/it]
INFO:root:eval mean loss: 3378.8731069937126
INFO:root:eval perplexity: 16.000370025634766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/60
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [5:01:57<3:21:28, 302.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1488.165347450658
INFO:root:current train perplexity3.298430919647217
INFO:root:current mean train loss 1510.4875160024947
INFO:root:current train perplexity3.311264753341675
INFO:root:current mean train loss 1522.627493244328
INFO:root:current train perplexity3.3194937705993652
INFO:root:current mean train loss 1522.3628216686668
INFO:root:current train perplexity3.320774555206299
INFO:root:current mean train loss 1523.1211365765773
INFO:root:current train perplexity3.322810411453247
INFO:root:current mean train loss 1520.7575615384906
INFO:root:current train perplexity3.322418689727783
INFO:root:current mean train loss 1519.2176943580246
INFO:root:current train perplexity3.3180956840515137
INFO:root:current mean train loss 1515.7639394449757
INFO:root:current train perplexity3.3130486011505127
INFO:root:current mean train loss 1515.1454215137076
INFO:root:current train perplexity3.311542272567749
INFO:root:current mean train loss 1514.8224474685885
INFO:root:current train perplexity3.3090310096740723
INFO:root:current mean train loss 1517.3464337499618
INFO:root:current train perplexity3.311433792114258
INFO:root:current mean train loss 1517.2748540174332
INFO:root:current train perplexity3.312620162963867
INFO:root:current mean train loss 1517.3339353065396
INFO:root:current train perplexity3.3115170001983643
INFO:root:current mean train loss 1516.4387226466251
INFO:root:current train perplexity3.310080051422119
INFO:root:current mean train loss 1517.85436246091
INFO:root:current train perplexity3.3132541179656982
INFO:root:current mean train loss 1518.0669499812275
INFO:root:current train perplexity3.3137450218200684
INFO:root:current mean train loss 1518.1912977514332
INFO:root:current train perplexity3.3132035732269287
INFO:root:current mean train loss 1519.4669114179983
INFO:root:current train perplexity3.3145110607147217
INFO:root:current mean train loss 1520.0905793930817
INFO:root:current train perplexity3.31595516204834
INFO:root:current mean train loss 1521.313001512425
INFO:root:current train perplexity3.3179738521575928

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.37s/it]
INFO:root:final mean train loss: 1521.0485351070033
INFO:root:final train perplexity: 3.3187570571899414
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 3376.3988597972975
INFO:root:eval perplexity: 15.967920303344727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/61
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [5:06:59<3:16:30, 302.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1528.5177103678386
INFO:root:current train perplexity3.298031806945801
INFO:root:current mean train loss 1507.1685333251953
INFO:root:current train perplexity3.2724497318267822
INFO:root:current mean train loss 1511.9228650109242
INFO:root:current train perplexity3.2873623371124268
INFO:root:current mean train loss 1514.751634506952
INFO:root:current train perplexity3.303215742111206
INFO:root:current mean train loss 1516.2611669137937
INFO:root:current train perplexity3.3050808906555176
INFO:root:current mean train loss 1517.5560361947587
INFO:root:current train perplexity3.3063783645629883
INFO:root:current mean train loss 1516.6748591968849
INFO:root:current train perplexity3.306049346923828
INFO:root:current mean train loss 1518.0190477785857
INFO:root:current train perplexity3.308436870574951
INFO:root:current mean train loss 1520.0942997544576
INFO:root:current train perplexity3.3139922618865967
INFO:root:current mean train loss 1520.3251011514255
INFO:root:current train perplexity3.314072847366333
INFO:root:current mean train loss 1519.1140619815546
INFO:root:current train perplexity3.311192512512207
INFO:root:current mean train loss 1518.98018710714
INFO:root:current train perplexity3.3097970485687256
INFO:root:current mean train loss 1518.4815559263754
INFO:root:current train perplexity3.3084282875061035
INFO:root:current mean train loss 1518.8905345437056
INFO:root:current train perplexity3.309100866317749
INFO:root:current mean train loss 1518.19065920689
INFO:root:current train perplexity3.3093504905700684
INFO:root:current mean train loss 1517.893088499705
INFO:root:current train perplexity3.309648036956787
INFO:root:current mean train loss 1517.282104939878
INFO:root:current train perplexity3.308908224105835
INFO:root:current mean train loss 1517.6382603799143
INFO:root:current train perplexity3.3094232082366943
INFO:root:current mean train loss 1517.7961538809318
INFO:root:current train perplexity3.3094215393066406
INFO:root:current mean train loss 1518.1708198105994
INFO:root:current train perplexity3.310145854949951

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.54s/it]
INFO:root:final mean train loss: 1517.711425042549
INFO:root:final train perplexity: 3.3100342750549316
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 3379.986627252252
INFO:root:eval perplexity: 16.015003204345703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/62
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [5:12:01<3:11:21, 302.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.3967722766804
INFO:root:current train perplexity3.2785024642944336
INFO:root:current mean train loss 1503.5500903160744
INFO:root:current train perplexity3.2819037437438965
INFO:root:current mean train loss 1505.8893179309227
INFO:root:current train perplexity3.2831127643585205
INFO:root:current mean train loss 1508.9995106813253
INFO:root:current train perplexity3.2896928787231445
INFO:root:current mean train loss 1508.2350818221405
INFO:root:current train perplexity3.2883849143981934
INFO:root:current mean train loss 1510.1508380689845
INFO:root:current train perplexity3.2916998863220215
INFO:root:current mean train loss 1510.163594781896
INFO:root:current train perplexity3.290065050125122
INFO:root:current mean train loss 1511.4078669047767
INFO:root:current train perplexity3.2927677631378174
INFO:root:current mean train loss 1513.2513920022623
INFO:root:current train perplexity3.294459104537964
INFO:root:current mean train loss 1513.4049366873935
INFO:root:current train perplexity3.293944835662842
INFO:root:current mean train loss 1514.019211061773
INFO:root:current train perplexity3.294811487197876
INFO:root:current mean train loss 1514.1445719048136
INFO:root:current train perplexity3.2967658042907715
INFO:root:current mean train loss 1513.397612624233
INFO:root:current train perplexity3.2969610691070557
INFO:root:current mean train loss 1513.5984690596417
INFO:root:current train perplexity3.297098159790039
INFO:root:current mean train loss 1514.49507459311
INFO:root:current train perplexity3.299241065979004
INFO:root:current mean train loss 1514.9407200466335
INFO:root:current train perplexity3.300636053085327
INFO:root:current mean train loss 1514.4243587210044
INFO:root:current train perplexity3.3002593517303467
INFO:root:current mean train loss 1514.092722394979
INFO:root:current train perplexity3.2994604110717773
INFO:root:current mean train loss 1513.9493348913713
INFO:root:current train perplexity3.299604892730713
INFO:root:current mean train loss 1513.909188313052
INFO:root:current train perplexity3.298842668533325

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.39s/it]
INFO:root:final mean train loss: 1513.3656138825525
INFO:root:final train perplexity: 3.298708915710449
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 3377.3626360735734
INFO:root:eval perplexity: 15.98055362701416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/63
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [5:17:04<3:06:36, 302.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1510.7307739257812
INFO:root:current train perplexity3.2916460037231445
INFO:root:current mean train loss 1510.6156723920037
INFO:root:current train perplexity3.292736053466797
INFO:root:current mean train loss 1510.1645109953704
INFO:root:current train perplexity3.286377429962158
INFO:root:current mean train loss 1506.5473339183911
INFO:root:current train perplexity3.27815580368042
INFO:root:current mean train loss 1509.1139767910572
INFO:root:current train perplexity3.28829288482666
INFO:root:current mean train loss 1507.780393366228
INFO:root:current train perplexity3.2899231910705566
INFO:root:current mean train loss 1508.9202230425024
INFO:root:current train perplexity3.2864420413970947
INFO:root:current mean train loss 1510.4399510767553
INFO:root:current train perplexity3.289217948913574
INFO:root:current mean train loss 1511.8419641253593
INFO:root:current train perplexity3.290438175201416
INFO:root:current mean train loss 1512.6523370801788
INFO:root:current train perplexity3.2903544902801514
INFO:root:current mean train loss 1512.3318767797166
INFO:root:current train perplexity3.2892887592315674
INFO:root:current mean train loss 1511.7472306732438
INFO:root:current train perplexity3.2865278720855713
INFO:root:current mean train loss 1511.8209319828063
INFO:root:current train perplexity3.285813570022583
INFO:root:current mean train loss 1510.878355062443
INFO:root:current train perplexity3.2850239276885986
INFO:root:current mean train loss 1510.3623008676127
INFO:root:current train perplexity3.2851133346557617
INFO:root:current mean train loss 1509.591916846014
INFO:root:current train perplexity3.2841811180114746
INFO:root:current mean train loss 1510.0004647443395
INFO:root:current train perplexity3.2860796451568604
INFO:root:current mean train loss 1510.3646756102137
INFO:root:current train perplexity3.2866249084472656
INFO:root:current mean train loss 1510.0167792498746
INFO:root:current train perplexity3.2871298789978027
INFO:root:current mean train loss 1510.2796417081418
INFO:root:current train perplexity3.289348602294922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.87s/it]
INFO:root:final mean train loss: 1509.7956071431386
INFO:root:final train perplexity: 3.2894344329833984
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it]
INFO:root:eval mean loss: 3388.7016432936844
INFO:root:eval perplexity: 16.129941940307617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/64
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [5:22:07<3:01:35, 302.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1504.6949350642062
INFO:root:current train perplexity3.261817693710327
INFO:root:current mean train loss 1508.0549460018383
INFO:root:current train perplexity3.276416540145874
INFO:root:current mean train loss 1508.919108640026
INFO:root:current train perplexity3.274886131286621
INFO:root:current mean train loss 1500.6475987791707
INFO:root:current train perplexity3.2688214778900146
INFO:root:current mean train loss 1500.297232437917
INFO:root:current train perplexity3.271270751953125
INFO:root:current mean train loss 1499.1101540040727
INFO:root:current train perplexity3.2700304985046387
INFO:root:current mean train loss 1499.6392537967954
INFO:root:current train perplexity3.2701213359832764
INFO:root:current mean train loss 1499.0879714364776
INFO:root:current train perplexity3.268080711364746
INFO:root:current mean train loss 1502.515238283452
INFO:root:current train perplexity3.271822929382324
INFO:root:current mean train loss 1504.0129025970427
INFO:root:current train perplexity3.2740468978881836
INFO:root:current mean train loss 1503.3243829328858
INFO:root:current train perplexity3.2725114822387695
INFO:root:current mean train loss 1504.8894600358046
INFO:root:current train perplexity3.27698016166687
INFO:root:current mean train loss 1504.7234455811115
INFO:root:current train perplexity3.2766802310943604
INFO:root:current mean train loss 1505.6552572436012
INFO:root:current train perplexity3.2786686420440674
INFO:root:current mean train loss 1506.5224170184567
INFO:root:current train perplexity3.279334545135498
INFO:root:current mean train loss 1507.429961100568
INFO:root:current train perplexity3.281660795211792
INFO:root:current mean train loss 1507.0710738656362
INFO:root:current train perplexity3.282528877258301
INFO:root:current mean train loss 1506.743286269433
INFO:root:current train perplexity3.2812678813934326
INFO:root:current mean train loss 1506.82131060432
INFO:root:current train perplexity3.2814202308654785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.23s/it]
INFO:root:final mean train loss: 1507.1910576447656
INFO:root:final train perplexity: 3.282684564590454
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.06s/it]
INFO:root:eval mean loss: 3392.0716937640764
INFO:root:eval perplexity: 16.174598693847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/65
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [5:27:10<2:56:34, 302.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.57080078125
INFO:root:current train perplexity3.282163143157959
INFO:root:current mean train loss 1490.667256281926
INFO:root:current train perplexity3.2483632564544678
INFO:root:current mean train loss 1505.9826624253217
INFO:root:current train perplexity3.2757768630981445
INFO:root:current mean train loss 1501.308339570698
INFO:root:current train perplexity3.2627105712890625
INFO:root:current mean train loss 1500.4414119909306
INFO:root:current train perplexity3.2624106407165527
INFO:root:current mean train loss 1502.2289186507937
INFO:root:current train perplexity3.265424966812134
INFO:root:current mean train loss 1501.6524973484065
INFO:root:current train perplexity3.262859582901001
INFO:root:current mean train loss 1501.4563241438432
INFO:root:current train perplexity3.2644004821777344
INFO:root:current mean train loss 1503.496749953844
INFO:root:current train perplexity3.268667221069336
INFO:root:current mean train loss 1503.3307561283618
INFO:root:current train perplexity3.2667315006256104
INFO:root:current mean train loss 1503.0361925102325
INFO:root:current train perplexity3.265383005142212
INFO:root:current mean train loss 1502.2283854830093
INFO:root:current train perplexity3.265850782394409
INFO:root:current mean train loss 1503.099903398178
INFO:root:current train perplexity3.2683753967285156
INFO:root:current mean train loss 1503.432742721464
INFO:root:current train perplexity3.2695682048797607
INFO:root:current mean train loss 1503.2131625009738
INFO:root:current train perplexity3.270311117172241
INFO:root:current mean train loss 1502.7935997983243
INFO:root:current train perplexity3.2697627544403076
INFO:root:current mean train loss 1502.9259984499201
INFO:root:current train perplexity3.2691524028778076
INFO:root:current mean train loss 1502.297548535844
INFO:root:current train perplexity3.270031690597534
INFO:root:current mean train loss 1502.5984691273081
INFO:root:current train perplexity3.271225929260254
INFO:root:current mean train loss 1503.4204899122735
INFO:root:current train perplexity3.272555112838745

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it]
INFO:root:final mean train loss: 1503.972895650628
INFO:root:final train perplexity: 3.2743635177612305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.67s/it]
INFO:root:eval mean loss: 3393.873047608155
INFO:root:eval perplexity: 16.198530197143555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/66
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [5:32:12<2:51:25, 302.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1465.157703218006
INFO:root:current train perplexity3.1848392486572266
INFO:root:current mean train loss 1500.8204335614669
INFO:root:current train perplexity3.2394871711730957
INFO:root:current mean train loss 1492.1264107130232
INFO:root:current train perplexity3.2431118488311768
INFO:root:current mean train loss 1497.9011032722449
INFO:root:current train perplexity3.256938934326172
INFO:root:current mean train loss 1500.413503180207
INFO:root:current train perplexity3.262211561203003
INFO:root:current mean train loss 1500.7395040618253
INFO:root:current train perplexity3.2649776935577393
INFO:root:current mean train loss 1499.7991994467718
INFO:root:current train perplexity3.2617530822753906
INFO:root:current mean train loss 1498.9828121275248
INFO:root:current train perplexity3.256333351135254
INFO:root:current mean train loss 1499.3947041705524
INFO:root:current train perplexity3.256584882736206
INFO:root:current mean train loss 1500.3270819018899
INFO:root:current train perplexity3.2582857608795166
INFO:root:current mean train loss 1499.3263888756044
INFO:root:current train perplexity3.258598566055298
INFO:root:current mean train loss 1498.779815973287
INFO:root:current train perplexity3.2577764987945557
INFO:root:current mean train loss 1498.9935200759176
INFO:root:current train perplexity3.2580642700195312
INFO:root:current mean train loss 1496.9708864614875
INFO:root:current train perplexity3.2575089931488037
INFO:root:current mean train loss 1497.9336127348972
INFO:root:current train perplexity3.2595276832580566
INFO:root:current mean train loss 1497.813981135843
INFO:root:current train perplexity3.260439395904541
INFO:root:current mean train loss 1498.6703430589962
INFO:root:current train perplexity3.2610208988189697
INFO:root:current mean train loss 1498.9432031164883
INFO:root:current train perplexity3.2625887393951416
INFO:root:current mean train loss 1500.0881467648485
INFO:root:current train perplexity3.2634129524230957
INFO:root:current mean train loss 1499.6150764651995
INFO:root:current train perplexity3.262849807739258

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.36s/it]
INFO:root:final mean train loss: 1500.2509282391538
INFO:root:final train perplexity: 3.264765977859497
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.71s/it]
INFO:root:eval mean loss: 3398.9481094864395
INFO:root:eval perplexity: 16.266128540039062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/67
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [5:37:15<2:46:23, 302.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1483.8488062808387
INFO:root:current train perplexity3.2338929176330566
INFO:root:current mean train loss 1483.2579407622848
INFO:root:current train perplexity3.230499505996704
INFO:root:current mean train loss 1485.8424113297663
INFO:root:current train perplexity3.233198881149292
INFO:root:current mean train loss 1486.2319140913924
INFO:root:current train perplexity3.2380969524383545
INFO:root:current mean train loss 1488.5989658582157
INFO:root:current train perplexity3.2413430213928223
INFO:root:current mean train loss 1493.018716691595
INFO:root:current train perplexity3.245954751968384
INFO:root:current mean train loss 1496.7674964259038
INFO:root:current train perplexity3.250192642211914
INFO:root:current mean train loss 1493.7544747536099
INFO:root:current train perplexity3.247756004333496
INFO:root:current mean train loss 1494.8020592008968
INFO:root:current train perplexity3.2485811710357666
INFO:root:current mean train loss 1497.0687501821944
INFO:root:current train perplexity3.2514758110046387
INFO:root:current mean train loss 1496.9966122548244
INFO:root:current train perplexity3.2528574466705322
INFO:root:current mean train loss 1498.2482712784215
INFO:root:current train perplexity3.2531650066375732
INFO:root:current mean train loss 1497.9325534900672
INFO:root:current train perplexity3.2528252601623535
INFO:root:current mean train loss 1497.0070358299176
INFO:root:current train perplexity3.2527031898498535
INFO:root:current mean train loss 1497.2897218324877
INFO:root:current train perplexity3.2538399696350098
INFO:root:current mean train loss 1498.250156119834
INFO:root:current train perplexity3.255389928817749
INFO:root:current mean train loss 1498.078334263393
INFO:root:current train perplexity3.2557694911956787
INFO:root:current mean train loss 1498.5685285737243
INFO:root:current train perplexity3.257495403289795
INFO:root:current mean train loss 1499.0366068145786
INFO:root:current train perplexity3.2583937644958496
INFO:root:current mean train loss 1497.8210837853337
INFO:root:current train perplexity3.2575435638427734

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.80s/it]
INFO:root:final mean train loss: 1497.675793346226
INFO:root:final train perplexity: 3.2581419944763184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.93s/it]
INFO:root:eval mean loss: 3398.9819196638045
INFO:root:eval perplexity: 16.266582489013672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/68
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [5:42:17<2:41:18, 302.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1477.9136541193182
INFO:root:current train perplexity3.2244136333465576
INFO:root:current mean train loss 1498.050025201613
INFO:root:current train perplexity3.23899245262146
INFO:root:current mean train loss 1494.8062337239583
INFO:root:current train perplexity3.241475820541382
INFO:root:current mean train loss 1496.665164915273
INFO:root:current train perplexity3.2463998794555664
INFO:root:current mean train loss 1496.6986346905048
INFO:root:current train perplexity3.25111722946167
INFO:root:current mean train loss 1494.4370370125985
INFO:root:current train perplexity3.2462446689605713
INFO:root:current mean train loss 1494.319243313156
INFO:root:current train perplexity3.2464778423309326
INFO:root:current mean train loss 1494.1414476407285
INFO:root:current train perplexity3.2426869869232178
INFO:root:current mean train loss 1492.6559627421418
INFO:root:current train perplexity3.2409956455230713
INFO:root:current mean train loss 1493.3728479834751
INFO:root:current train perplexity3.2425479888916016
INFO:root:current mean train loss 1492.989591047097
INFO:root:current train perplexity3.24271559715271
INFO:root:current mean train loss 1494.9737530861066
INFO:root:current train perplexity3.2463669776916504
INFO:root:current mean train loss 1495.451763259462
INFO:root:current train perplexity3.247403383255005
INFO:root:current mean train loss 1495.2495171240776
INFO:root:current train perplexity3.2479825019836426
INFO:root:current mean train loss 1495.9851882987007
INFO:root:current train perplexity3.2494914531707764
INFO:root:current mean train loss 1496.2501796121383
INFO:root:current train perplexity3.2495641708374023
INFO:root:current mean train loss 1497.2013297181836
INFO:root:current train perplexity3.251720666885376
INFO:root:current mean train loss 1496.026269253027
INFO:root:current train perplexity3.250265598297119
INFO:root:current mean train loss 1495.4777365466011
INFO:root:current train perplexity3.2498316764831543
INFO:root:current mean train loss 1495.020217029152
INFO:root:current train perplexity3.249786138534546

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.86s/it]
INFO:root:final mean train loss: 1494.625447160237
INFO:root:final train perplexity: 3.2503139972686768
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.30s/it]
INFO:root:eval mean loss: 3407.3943149692664
INFO:root:eval perplexity: 16.379253387451172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/69
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [5:47:19<2:36:08, 302.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1471.0118882921006
INFO:root:current train perplexity3.212238311767578
INFO:root:current mean train loss 1479.9288550088572
INFO:root:current train perplexity3.2273619174957275
INFO:root:current mean train loss 1480.827849444221
INFO:root:current train perplexity3.230022668838501
INFO:root:current mean train loss 1479.7271551316785
INFO:root:current train perplexity3.224947929382324
INFO:root:current mean train loss 1483.920912661795
INFO:root:current train perplexity3.226722240447998
INFO:root:current mean train loss 1487.0262837443317
INFO:root:current train perplexity3.2348737716674805
INFO:root:current mean train loss 1488.5072333926246
INFO:root:current train perplexity3.2388110160827637
INFO:root:current mean train loss 1487.3820993690292
INFO:root:current train perplexity3.2364141941070557
INFO:root:current mean train loss 1485.7729510386055
INFO:root:current train perplexity3.235751152038574
INFO:root:current mean train loss 1486.5495034049077
INFO:root:current train perplexity3.237083673477173
INFO:root:current mean train loss 1486.4944885026164
INFO:root:current train perplexity3.2370660305023193
INFO:root:current mean train loss 1487.8348909449658
INFO:root:current train perplexity3.238496780395508
INFO:root:current mean train loss 1488.025801556665
INFO:root:current train perplexity3.238055944442749
INFO:root:current mean train loss 1487.3802741136913
INFO:root:current train perplexity3.238079071044922
INFO:root:current mean train loss 1488.3266026040783
INFO:root:current train perplexity3.2395050525665283
INFO:root:current mean train loss 1488.2569344013398
INFO:root:current train perplexity3.2392613887786865
INFO:root:current mean train loss 1489.8887104235198
INFO:root:current train perplexity3.240609645843506
INFO:root:current mean train loss 1490.5406545806954
INFO:root:current train perplexity3.241002082824707
INFO:root:current mean train loss 1491.5853525797527
INFO:root:current train perplexity3.241849184036255
INFO:root:current mean train loss 1491.899768264492
INFO:root:current train perplexity3.242540121078491

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.69s/it]
INFO:root:final mean train loss: 1491.8413884965566
INFO:root:final train perplexity: 3.24318528175354
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 3406.871540241413
INFO:root:eval perplexity: 16.372234344482422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/70
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [5:52:21<2:31:11, 302.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.696722480688
INFO:root:current train perplexity3.2484290599823
INFO:root:current mean train loss 1471.6564146980406
INFO:root:current train perplexity3.220726490020752
INFO:root:current mean train loss 1477.806874628298
INFO:root:current train perplexity3.2232422828674316
INFO:root:current mean train loss 1476.3836742097124
INFO:root:current train perplexity3.219491958618164
INFO:root:current mean train loss 1481.374658502684
INFO:root:current train perplexity3.226046562194824
INFO:root:current mean train loss 1481.292531037776
INFO:root:current train perplexity3.226700782775879
INFO:root:current mean train loss 1482.0739802788235
INFO:root:current train perplexity3.225681781768799
INFO:root:current mean train loss 1481.65765976513
INFO:root:current train perplexity3.223050355911255
INFO:root:current mean train loss 1483.7661077887724
INFO:root:current train perplexity3.226292610168457
INFO:root:current mean train loss 1486.3110039289606
INFO:root:current train perplexity3.227971076965332
INFO:root:current mean train loss 1484.7379542719452
INFO:root:current train perplexity3.227226495742798
INFO:root:current mean train loss 1484.1348991939458
INFO:root:current train perplexity3.227808713912964
INFO:root:current mean train loss 1484.8352347197138
INFO:root:current train perplexity3.2293105125427246
INFO:root:current mean train loss 1485.0418782903619
INFO:root:current train perplexity3.230398178100586
INFO:root:current mean train loss 1485.6466802285772
INFO:root:current train perplexity3.2318100929260254
INFO:root:current mean train loss 1485.5171596750363
INFO:root:current train perplexity3.2320632934570312
INFO:root:current mean train loss 1486.7387844919099
INFO:root:current train perplexity3.232696533203125
INFO:root:current mean train loss 1487.5934815681335
INFO:root:current train perplexity3.232694149017334
INFO:root:current mean train loss 1488.8148055715242
INFO:root:current train perplexity3.2340967655181885

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it]
INFO:root:final mean train loss: 1489.1490296610546
INFO:root:final train perplexity: 3.2363054752349854
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.63s/it]
INFO:root:eval mean loss: 3411.9931589304147
INFO:root:eval perplexity: 16.441181182861328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/71
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [5:57:24<2:26:06, 302.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1519.5655110677083
INFO:root:current train perplexity3.2310454845428467
INFO:root:current mean train loss 1483.5935715009582
INFO:root:current train perplexity3.2076644897460938
INFO:root:current mean train loss 1475.94030109887
INFO:root:current train perplexity3.2020609378814697
INFO:root:current mean train loss 1474.6040035073274
INFO:root:current train perplexity3.1969971656799316
INFO:root:current mean train loss 1477.9808085023476
INFO:root:current train perplexity3.2081596851348877
INFO:root:current mean train loss 1476.670001727319
INFO:root:current train perplexity3.2065064907073975
INFO:root:current mean train loss 1479.484791167105
INFO:root:current train perplexity3.2129135131835938
INFO:root:current mean train loss 1482.2032858008365
INFO:root:current train perplexity3.2183115482330322
INFO:root:current mean train loss 1483.3090229649697
INFO:root:current train perplexity3.218886613845825
INFO:root:current mean train loss 1483.240299856426
INFO:root:current train perplexity3.218726634979248
INFO:root:current mean train loss 1484.239153822184
INFO:root:current train perplexity3.2223970890045166
INFO:root:current mean train loss 1479.932202192586
INFO:root:current train perplexity3.217398166656494
INFO:root:current mean train loss 1481.8379878966173
INFO:root:current train perplexity3.2191648483276367
INFO:root:current mean train loss 1483.9517313795104
INFO:root:current train perplexity3.222050189971924
INFO:root:current mean train loss 1484.4949495361675
INFO:root:current train perplexity3.2244644165039062
INFO:root:current mean train loss 1484.7577696213843
INFO:root:current train perplexity3.2238082885742188
INFO:root:current mean train loss 1484.711493809226
INFO:root:current train perplexity3.225558042526245
INFO:root:current mean train loss 1483.9218292057444
INFO:root:current train perplexity3.2249279022216797
INFO:root:current mean train loss 1485.0868891888151
INFO:root:current train perplexity3.227585554122925
INFO:root:current mean train loss 1487.1726015297088
INFO:root:current train perplexity3.2298498153686523

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.06s/it]
INFO:root:final mean train loss: 1487.0797853224576
INFO:root:final train perplexity: 3.2310285568237305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.73s/it]
INFO:root:eval mean loss: 3409.655271238035
INFO:root:eval perplexity: 16.40967559814453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/72
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [6:02:27<2:21:12, 302.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1506.2653118631115
INFO:root:current train perplexity3.2303407192230225
INFO:root:current mean train loss 1494.5800265180387
INFO:root:current train perplexity3.2306253910064697
INFO:root:current mean train loss 1484.4175784534402
INFO:root:current train perplexity3.2153289318084717
INFO:root:current mean train loss 1484.2620097535314
INFO:root:current train perplexity3.2159526348114014
INFO:root:current mean train loss 1483.7045878236738
INFO:root:current train perplexity3.21659517288208
INFO:root:current mean train loss 1484.959685987542
INFO:root:current train perplexity3.2183449268341064
INFO:root:current mean train loss 1483.244736477039
INFO:root:current train perplexity3.214637517929077
INFO:root:current mean train loss 1483.1397073621736
INFO:root:current train perplexity3.2202556133270264
INFO:root:current mean train loss 1483.0779872073872
INFO:root:current train perplexity3.220439910888672
INFO:root:current mean train loss 1483.9807787530472
INFO:root:current train perplexity3.2224080562591553
INFO:root:current mean train loss 1485.0593964547822
INFO:root:current train perplexity3.222918748855591
INFO:root:current mean train loss 1484.5631126342664
INFO:root:current train perplexity3.2221696376800537
INFO:root:current mean train loss 1484.1663283366017
INFO:root:current train perplexity3.2223055362701416
INFO:root:current mean train loss 1483.6688655931123
INFO:root:current train perplexity3.2218241691589355
INFO:root:current mean train loss 1483.6351338089644
INFO:root:current train perplexity3.2223782539367676
INFO:root:current mean train loss 1484.423078069856
INFO:root:current train perplexity3.223918914794922
INFO:root:current mean train loss 1484.9707916504208
INFO:root:current train perplexity3.2241313457489014
INFO:root:current mean train loss 1484.7668001481563
INFO:root:current train perplexity3.2235264778137207
INFO:root:current mean train loss 1484.7072019341615
INFO:root:current train perplexity3.2221646308898926
INFO:root:current mean train loss 1484.453048888557
INFO:root:current train perplexity3.222790002822876

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.17s/it]
INFO:root:final mean train loss: 1484.1511007011748
INFO:root:final train perplexity: 3.2235748767852783
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.64s/it]
INFO:root:eval mean loss: 3407.743457324512
INFO:root:eval perplexity: 16.383947372436523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/73
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [6:07:30<2:16:15, 302.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1470.0745880126954
INFO:root:current train perplexity3.2304201126098633
INFO:root:current mean train loss 1475.3168962751115
INFO:root:current train perplexity3.2086074352264404
INFO:root:current mean train loss 1470.9699661254883
INFO:root:current train perplexity3.201573133468628
INFO:root:current mean train loss 1474.2435984892004
INFO:root:current train perplexity3.2062036991119385
INFO:root:current mean train loss 1479.0670199307528
INFO:root:current train perplexity3.211348056793213
INFO:root:current mean train loss 1479.5135012026187
INFO:root:current train perplexity3.2152843475341797
INFO:root:current mean train loss 1479.7147651672362
INFO:root:current train perplexity3.2112720012664795
INFO:root:current mean train loss 1479.4535072120461
INFO:root:current train perplexity3.21142315864563
INFO:root:current mean train loss 1480.3266017368862
INFO:root:current train perplexity3.2131567001342773
INFO:root:current mean train loss 1480.1427031821393
INFO:root:current train perplexity3.2146401405334473
INFO:root:current mean train loss 1480.9173627413236
INFO:root:current train perplexity3.2166008949279785
INFO:root:current mean train loss 1481.1097524542558
INFO:root:current train perplexity3.21899151802063
INFO:root:current mean train loss 1482.035267294607
INFO:root:current train perplexity3.220184803009033
INFO:root:current mean train loss 1481.9686714741722
INFO:root:current train perplexity3.2190496921539307
INFO:root:current mean train loss 1480.9864144219293
INFO:root:current train perplexity3.216426372528076
INFO:root:current mean train loss 1480.792275723544
INFO:root:current train perplexity3.216562509536743
INFO:root:current mean train loss 1481.2450114180401
INFO:root:current train perplexity3.2164840698242188
INFO:root:current mean train loss 1481.4228369000314
INFO:root:current train perplexity3.216409921646118
INFO:root:current mean train loss 1481.9573019276495
INFO:root:current train perplexity3.217658758163452
INFO:root:current mean train loss 1482.2314779694548
INFO:root:current train perplexity3.2175726890563965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.53s/it]
INFO:root:final mean train loss: 1481.602836081793
INFO:root:final train perplexity: 3.217102527618408
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.46s/it]
INFO:root:eval mean loss: 3419.2509926919106
INFO:root:eval perplexity: 16.53939437866211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/74
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [6:12:34<2:11:18, 303.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1468.2528440241229
INFO:root:current train perplexity3.183382272720337
INFO:root:current mean train loss 1484.6991853167297
INFO:root:current train perplexity3.2149908542633057
INFO:root:current mean train loss 1482.2870652016964
INFO:root:current train perplexity3.2047791481018066
INFO:root:current mean train loss 1478.5782911797533
INFO:root:current train perplexity3.203002452850342
INFO:root:current mean train loss 1475.178674877342
INFO:root:current train perplexity3.199840784072876
INFO:root:current mean train loss 1478.4024719567017
INFO:root:current train perplexity3.208458423614502
INFO:root:current mean train loss 1478.5653780352216
INFO:root:current train perplexity3.205124616622925
INFO:root:current mean train loss 1476.9620287150656
INFO:root:current train perplexity3.2037312984466553
INFO:root:current mean train loss 1477.1566219085016
INFO:root:current train perplexity3.205824136734009
INFO:root:current mean train loss 1476.967897293585
INFO:root:current train perplexity3.2056548595428467
INFO:root:current mean train loss 1475.6676127019646
INFO:root:current train perplexity3.2019808292388916
INFO:root:current mean train loss 1477.4654285691377
INFO:root:current train perplexity3.2029478549957275
INFO:root:current mean train loss 1476.0879020842656
INFO:root:current train perplexity3.201850652694702
INFO:root:current mean train loss 1477.16135968157
INFO:root:current train perplexity3.2052907943725586
INFO:root:current mean train loss 1477.382568108029
INFO:root:current train perplexity3.206130027770996
INFO:root:current mean train loss 1477.5863596892311
INFO:root:current train perplexity3.207252264022827
INFO:root:current mean train loss 1477.9096288502612
INFO:root:current train perplexity3.2078123092651367
INFO:root:current mean train loss 1477.9000082260245
INFO:root:current train perplexity3.2076525688171387
INFO:root:current mean train loss 1479.4346925800182
INFO:root:current train perplexity3.2115323543548584
INFO:root:current mean train loss 1479.0963006894322
INFO:root:current train perplexity3.21048641204834

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.81s/it]
INFO:root:final mean train loss: 1478.9837704349275
INFO:root:final train perplexity: 3.2104642391204834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.78s/it]
INFO:root:eval mean loss: 3425.1165899786506
INFO:root:eval perplexity: 16.619186401367188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/75
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [6:17:37<2:06:15, 303.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1478.8300946209881
INFO:root:current train perplexity3.212272882461548
INFO:root:current mean train loss 1466.6083149526312
INFO:root:current train perplexity3.188307285308838
INFO:root:current mean train loss 1473.4677164119526
INFO:root:current train perplexity3.1929805278778076
INFO:root:current mean train loss 1473.0496535683699
INFO:root:current train perplexity3.189509153366089
INFO:root:current mean train loss 1476.2655407869363
INFO:root:current train perplexity3.198118209838867
INFO:root:current mean train loss 1474.4179334474359
INFO:root:current train perplexity3.1990506649017334
INFO:root:current mean train loss 1473.0215238576827
INFO:root:current train perplexity3.195659875869751
INFO:root:current mean train loss 1475.2890574531655
INFO:root:current train perplexity3.200471878051758
INFO:root:current mean train loss 1474.02591549941
INFO:root:current train perplexity3.196288585662842
INFO:root:current mean train loss 1475.8192121125835
INFO:root:current train perplexity3.199345350265503
INFO:root:current mean train loss 1475.581768355556
INFO:root:current train perplexity3.1976730823516846
INFO:root:current mean train loss 1475.5934975411253
INFO:root:current train perplexity3.1966116428375244
INFO:root:current mean train loss 1474.3083046714028
INFO:root:current train perplexity3.19580340385437
INFO:root:current mean train loss 1476.3673466178527
INFO:root:current train perplexity3.200430154800415
INFO:root:current mean train loss 1475.7942696187033
INFO:root:current train perplexity3.200500726699829
INFO:root:current mean train loss 1475.410057523502
INFO:root:current train perplexity3.2006335258483887
INFO:root:current mean train loss 1475.8568113775948
INFO:root:current train perplexity3.2018394470214844
INFO:root:current mean train loss 1475.893742884966
INFO:root:current train perplexity3.202239990234375
INFO:root:current mean train loss 1476.4439404140542
INFO:root:current train perplexity3.2031357288360596
INFO:root:current mean train loss 1476.779900918978
INFO:root:current train perplexity3.204127311706543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it]
INFO:root:final mean train loss: 1476.509224465082
INFO:root:final train perplexity: 3.204204559326172
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.16s/it]
INFO:root:eval mean loss: 3425.5598972996436
INFO:root:eval perplexity: 16.6252384185791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/76
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [6:22:40<2:01:13, 303.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1469.5242933336194
INFO:root:current train perplexity3.199202299118042
INFO:root:current mean train loss 1477.4410464301784
INFO:root:current train perplexity3.2134597301483154
INFO:root:current mean train loss 1471.4182695211823
INFO:root:current train perplexity3.1971938610076904
INFO:root:current mean train loss 1472.9177870494325
INFO:root:current train perplexity3.198092460632324
INFO:root:current mean train loss 1473.526073820965
INFO:root:current train perplexity3.1991117000579834
INFO:root:current mean train loss 1471.8848034234215
INFO:root:current train perplexity3.1923627853393555
INFO:root:current mean train loss 1471.2046827655797
INFO:root:current train perplexity3.1908390522003174
INFO:root:current mean train loss 1473.9730142817637
INFO:root:current train perplexity3.1943933963775635
INFO:root:current mean train loss 1473.7536682745424
INFO:root:current train perplexity3.195307493209839
INFO:root:current mean train loss 1473.6000287992322
INFO:root:current train perplexity3.196011781692505
INFO:root:current mean train loss 1474.1332402495918
INFO:root:current train perplexity3.195620536804199
INFO:root:current mean train loss 1475.0640436616093
INFO:root:current train perplexity3.197352170944214
INFO:root:current mean train loss 1474.4366981748644
INFO:root:current train perplexity3.197244882583618
INFO:root:current mean train loss 1474.8744621359128
INFO:root:current train perplexity3.198709726333618
INFO:root:current mean train loss 1475.0376153404804
INFO:root:current train perplexity3.1982555389404297
INFO:root:current mean train loss 1475.3903636728571
INFO:root:current train perplexity3.1989946365356445
INFO:root:current mean train loss 1474.4035827889377
INFO:root:current train perplexity3.1965479850769043
INFO:root:current mean train loss 1474.6266048395187
INFO:root:current train perplexity3.197455883026123
INFO:root:current mean train loss 1474.1778562516527
INFO:root:current train perplexity3.1972298622131348

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.79s/it]
INFO:root:final mean train loss: 1474.024565096041
INFO:root:final train perplexity: 3.197932004928589
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.74s/it]
INFO:root:eval mean loss: 3434.1852910038947
INFO:root:eval perplexity: 16.743328094482422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/77
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [6:27:42<1:56:03, 302.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1419.5874328613281
INFO:root:current train perplexity3.0822248458862305
INFO:root:current mean train loss 1460.5340564869068
INFO:root:current train perplexity3.1734704971313477
INFO:root:current mean train loss 1465.8241536067083
INFO:root:current train perplexity3.1713638305664062
INFO:root:current mean train loss 1469.9528963163302
INFO:root:current train perplexity3.183173179626465
INFO:root:current mean train loss 1470.1655252494063
INFO:root:current train perplexity3.1801254749298096
INFO:root:current mean train loss 1468.8575703778604
INFO:root:current train perplexity3.179865837097168
INFO:root:current mean train loss 1469.3060511538856
INFO:root:current train perplexity3.182466506958008
INFO:root:current mean train loss 1468.6581127791756
INFO:root:current train perplexity3.1842212677001953
INFO:root:current mean train loss 1471.478615486976
INFO:root:current train perplexity3.187596082687378
INFO:root:current mean train loss 1471.95604662118
INFO:root:current train perplexity3.1892852783203125
INFO:root:current mean train loss 1472.3894977872335
INFO:root:current train perplexity3.1887099742889404
INFO:root:current mean train loss 1470.7875269259787
INFO:root:current train perplexity3.1876144409179688
INFO:root:current mean train loss 1469.9364945367472
INFO:root:current train perplexity3.1860077381134033
INFO:root:current mean train loss 1470.242565749982
INFO:root:current train perplexity3.186784267425537
INFO:root:current mean train loss 1470.925786972046
INFO:root:current train perplexity3.1890807151794434
INFO:root:current mean train loss 1470.8966161591304
INFO:root:current train perplexity3.188854932785034
INFO:root:current mean train loss 1470.520787329223
INFO:root:current train perplexity3.1891298294067383
INFO:root:current mean train loss 1472.1008114959932
INFO:root:current train perplexity3.1910665035247803
INFO:root:current mean train loss 1472.0391332575705
INFO:root:current train perplexity3.191493511199951
INFO:root:current mean train loss 1471.6285345369415
INFO:root:current train perplexity3.1914169788360596

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.22s/it]
INFO:root:final mean train loss: 1472.150664559892
INFO:root:final train perplexity: 3.193209409713745
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.58s/it]
INFO:root:eval mean loss: 3430.8006485489395
INFO:root:eval perplexity: 16.696882247924805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/78
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [6:32:44<1:50:57, 302.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1471.3899853515625
INFO:root:current train perplexity3.2034289836883545
INFO:root:current mean train loss 1457.7357421875
INFO:root:current train perplexity3.16630220413208
INFO:root:current mean train loss 1467.74875
INFO:root:current train perplexity3.1912286281585693
INFO:root:current mean train loss 1464.1442439152645
INFO:root:current train perplexity3.1878137588500977
INFO:root:current mean train loss 1466.705772633272
INFO:root:current train perplexity3.186135768890381
INFO:root:current mean train loss 1465.2778352864584
INFO:root:current train perplexity3.181917667388916
INFO:root:current mean train loss 1466.7765162109374
INFO:root:current train perplexity3.181875228881836
INFO:root:current mean train loss 1467.2393180899785
INFO:root:current train perplexity3.1810598373413086
INFO:root:current mean train loss 1466.5583089192708
INFO:root:current train perplexity3.181633949279785
INFO:root:current mean train loss 1464.840352882179
INFO:root:current train perplexity3.1795098781585693
INFO:root:current mean train loss 1464.0655150771722
INFO:root:current train perplexity3.178657293319702
INFO:root:current mean train loss 1466.1468896484375
INFO:root:current train perplexity3.181185007095337
INFO:root:current mean train loss 1467.127375538106
INFO:root:current train perplexity3.1832773685455322
INFO:root:current mean train loss 1468.3232291052477
INFO:root:current train perplexity3.184401273727417
INFO:root:current mean train loss 1467.816557274534
INFO:root:current train perplexity3.1832492351531982
INFO:root:current mean train loss 1468.1209635950308
INFO:root:current train perplexity3.1838064193725586
INFO:root:current mean train loss 1468.0521572265625
INFO:root:current train perplexity3.1824283599853516
INFO:root:current mean train loss 1469.5113642861186
INFO:root:current train perplexity3.184541702270508
INFO:root:current mean train loss 1469.4625167219606
INFO:root:current train perplexity3.1869635581970215
INFO:root:current mean train loss 1470.001683048397
INFO:root:current train perplexity3.1873855590820312

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.05s/it]
INFO:root:final mean train loss: 1469.9550774478575
INFO:root:final train perplexity: 3.187685489654541
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it]
INFO:root:eval mean loss: 3438.526565139358
INFO:root:eval perplexity: 16.80307388305664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/79
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [6:37:47<1:45:54, 302.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1464.4176286969866
INFO:root:current train perplexity3.1589455604553223
INFO:root:current mean train loss 1463.5732249944983
INFO:root:current train perplexity3.1721532344818115
INFO:root:current mean train loss 1464.630245492478
INFO:root:current train perplexity3.175384521484375
INFO:root:current mean train loss 1471.2484569884184
INFO:root:current train perplexity3.185044050216675
INFO:root:current mean train loss 1469.8015156051154
INFO:root:current train perplexity3.1791415214538574
INFO:root:current mean train loss 1471.0639952487172
INFO:root:current train perplexity3.1811211109161377
INFO:root:current mean train loss 1470.7742676541814
INFO:root:current train perplexity3.179431915283203
INFO:root:current mean train loss 1469.5827182656672
INFO:root:current train perplexity3.1802315711975098
INFO:root:current mean train loss 1470.4411729826213
INFO:root:current train perplexity3.184573173522949
INFO:root:current mean train loss 1471.9158678965964
INFO:root:current train perplexity3.188309907913208
INFO:root:current mean train loss 1470.2416091303908
INFO:root:current train perplexity3.1863343715667725
INFO:root:current mean train loss 1468.485733272733
INFO:root:current train perplexity3.18294620513916
INFO:root:current mean train loss 1469.351147932707
INFO:root:current train perplexity3.183838367462158
INFO:root:current mean train loss 1468.809002258028
INFO:root:current train perplexity3.1836421489715576
INFO:root:current mean train loss 1468.7506518317657
INFO:root:current train perplexity3.1827404499053955
INFO:root:current mean train loss 1469.1145694796987
INFO:root:current train perplexity3.18363881111145
INFO:root:current mean train loss 1470.3607815592645
INFO:root:current train perplexity3.18619704246521
INFO:root:current mean train loss 1469.7196049126364
INFO:root:current train perplexity3.1856529712677
INFO:root:current mean train loss 1468.5018126977513
INFO:root:current train perplexity3.184750556945801
INFO:root:current mean train loss 1468.3345583946157
INFO:root:current train perplexity3.183789014816284

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.88s/it]
INFO:root:final mean train loss: 1468.0850780831404
INFO:root:final train perplexity: 3.1829872131347656
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.21s/it]
INFO:root:eval mean loss: 3431.5436659804336
INFO:root:eval perplexity: 16.707073211669922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/80
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [6:42:48<1:40:45, 302.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.2588780289989
INFO:root:current train perplexity3.1817917823791504
INFO:root:current mean train loss 1468.1644985750786
INFO:root:current train perplexity3.1868629455566406
INFO:root:current mean train loss 1461.0313819679054
INFO:root:current train perplexity3.171008825302124
INFO:root:current mean train loss 1458.6390761691548
INFO:root:current train perplexity3.168295383453369
INFO:root:current mean train loss 1459.0622622421365
INFO:root:current train perplexity3.1656832695007324
INFO:root:current mean train loss 1459.5696071563339
INFO:root:current train perplexity3.163137435913086
INFO:root:current mean train loss 1460.0840690277053
INFO:root:current train perplexity3.1614084243774414
INFO:root:current mean train loss 1461.1170791002758
INFO:root:current train perplexity3.1643264293670654
INFO:root:current mean train loss 1461.8193043896429
INFO:root:current train perplexity3.167287588119507
INFO:root:current mean train loss 1461.1270998894113
INFO:root:current train perplexity3.167524576187134
INFO:root:current mean train loss 1460.681904361426
INFO:root:current train perplexity3.167076826095581
INFO:root:current mean train loss 1462.87093787074
INFO:root:current train perplexity3.169940710067749
INFO:root:current mean train loss 1464.1325184259272
INFO:root:current train perplexity3.172498941421509
INFO:root:current mean train loss 1464.4800421775835
INFO:root:current train perplexity3.1745588779449463
INFO:root:current mean train loss 1465.0170274280865
INFO:root:current train perplexity3.1761066913604736
INFO:root:current mean train loss 1465.522128708933
INFO:root:current train perplexity3.17651104927063
INFO:root:current mean train loss 1466.0227659293296
INFO:root:current train perplexity3.178182363510132
INFO:root:current mean train loss 1465.5814950150343
INFO:root:current train perplexity3.176983594894409
INFO:root:current mean train loss 1465.7548528038217
INFO:root:current train perplexity3.1768836975097656
INFO:root:current mean train loss 1466.0648954195779
INFO:root:current train perplexity3.1776328086853027

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.15s/it]
INFO:root:final mean train loss: 1466.0455162213777
INFO:root:final train perplexity: 3.1778717041015625
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.69s/it]
INFO:root:eval mean loss: 3429.174920525995
INFO:root:eval perplexity: 16.67462921142578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/81
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [6:47:52<1:35:49, 302.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1465.1004879600123
INFO:root:current train perplexity3.1941945552825928
INFO:root:current mean train loss 1470.5863432450728
INFO:root:current train perplexity3.188647747039795
INFO:root:current mean train loss 1470.4584554036458
INFO:root:current train perplexity3.1818456649780273
INFO:root:current mean train loss 1464.8583107806267
INFO:root:current train perplexity3.1718087196350098
INFO:root:current mean train loss 1463.43429898815
INFO:root:current train perplexity3.1705174446105957
INFO:root:current mean train loss 1464.0589362250435
INFO:root:current train perplexity3.1709301471710205
INFO:root:current mean train loss 1463.577984871949
INFO:root:current train perplexity3.1696314811706543
INFO:root:current mean train loss 1462.5647834699178
INFO:root:current train perplexity3.1699485778808594
INFO:root:current mean train loss 1464.388380216137
INFO:root:current train perplexity3.1685242652893066
INFO:root:current mean train loss 1465.3789157554752
INFO:root:current train perplexity3.1711220741271973
INFO:root:current mean train loss 1463.0280282967153
INFO:root:current train perplexity3.1686980724334717
INFO:root:current mean train loss 1462.954493723759
INFO:root:current train perplexity3.1704154014587402
INFO:root:current mean train loss 1464.2522112329177
INFO:root:current train perplexity3.1717846393585205
INFO:root:current mean train loss 1465.1082667860874
INFO:root:current train perplexity3.1725683212280273
INFO:root:current mean train loss 1465.407643884178
INFO:root:current train perplexity3.171785593032837
INFO:root:current mean train loss 1465.9353973853406
INFO:root:current train perplexity3.173189401626587
INFO:root:current mean train loss 1466.5329525749553
INFO:root:current train perplexity3.1738457679748535
INFO:root:current mean train loss 1465.7385122625678
INFO:root:current train perplexity3.173853874206543
INFO:root:current mean train loss 1465.181942937725
INFO:root:current train perplexity3.174349546432495
INFO:root:current mean train loss 1465.0703684694854
INFO:root:current train perplexity3.174276828765869

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it]
INFO:root:final mean train loss: 1464.4698060036187
INFO:root:final train perplexity: 3.1739249229431152
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 3435.4271456515107
INFO:root:eval perplexity: 16.760393142700195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/82
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [6:52:53<1:30:42, 302.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.622293451781
INFO:root:current train perplexity3.1609292030334473
INFO:root:current mean train loss 1473.0008228677543
INFO:root:current train perplexity3.176764726638794
INFO:root:current mean train loss 1460.7365589337137
INFO:root:current train perplexity3.16646146774292
INFO:root:current mean train loss 1461.1631415369552
INFO:root:current train perplexity3.1676039695739746
INFO:root:current mean train loss 1460.5940070161955
INFO:root:current train perplexity3.1651058197021484
INFO:root:current mean train loss 1460.0832266333132
INFO:root:current train perplexity3.167677164077759
INFO:root:current mean train loss 1461.4850415426588
INFO:root:current train perplexity3.1719307899475098
INFO:root:current mean train loss 1461.307657518423
INFO:root:current train perplexity3.1728134155273438
INFO:root:current mean train loss 1462.085935449547
INFO:root:current train perplexity3.175898313522339
INFO:root:current mean train loss 1464.2004047866315
INFO:root:current train perplexity3.1787216663360596
INFO:root:current mean train loss 1463.2373790688614
INFO:root:current train perplexity3.17594313621521
INFO:root:current mean train loss 1461.8956965968475
INFO:root:current train perplexity3.172719717025757
INFO:root:current mean train loss 1461.9553960931457
INFO:root:current train perplexity3.1724061965942383
INFO:root:current mean train loss 1462.3971536147535
INFO:root:current train perplexity3.171191453933716
INFO:root:current mean train loss 1462.4629252102259
INFO:root:current train perplexity3.170789957046509
INFO:root:current mean train loss 1463.2002575354088
INFO:root:current train perplexity3.1714019775390625
INFO:root:current mean train loss 1463.3360946844543
INFO:root:current train perplexity3.170781135559082
INFO:root:current mean train loss 1463.110377569672
INFO:root:current train perplexity3.1711599826812744
INFO:root:current mean train loss 1463.5511774465135
INFO:root:current train perplexity3.170457601547241

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.20s/it]
INFO:root:final mean train loss: 1462.5408611134094
INFO:root:final train perplexity: 3.169100284576416
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.97s/it]
INFO:root:eval mean loss: 3441.1068639440223
INFO:root:eval perplexity: 16.838693618774414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/83
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [6:57:56<1:25:41, 302.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1483.5208374023437
INFO:root:current train perplexity3.1766912937164307
INFO:root:current mean train loss 1463.7407137784091
INFO:root:current train perplexity3.1586341857910156
INFO:root:current mean train loss 1464.6545136951265
INFO:root:current train perplexity3.1575634479522705
INFO:root:current mean train loss 1464.0089205834174
INFO:root:current train perplexity3.1627283096313477
INFO:root:current mean train loss 1458.5952231802592
INFO:root:current train perplexity3.1581923961639404
INFO:root:current mean train loss 1457.9119104721967
INFO:root:current train perplexity3.16290020942688
INFO:root:current mean train loss 1458.8820782770877
INFO:root:current train perplexity3.1661319732666016
INFO:root:current mean train loss 1458.0718027893927
INFO:root:current train perplexity3.1652462482452393
INFO:root:current mean train loss 1458.1536212685667
INFO:root:current train perplexity3.1633152961730957
INFO:root:current mean train loss 1458.908391193767
INFO:root:current train perplexity3.1629390716552734
INFO:root:current mean train loss 1457.9386309028853
INFO:root:current train perplexity3.1595451831817627
INFO:root:current mean train loss 1458.570412355715
INFO:root:current train perplexity3.1618916988372803
INFO:root:current mean train loss 1459.7403705691504
INFO:root:current train perplexity3.1642026901245117
INFO:root:current mean train loss 1460.8490417946387
INFO:root:current train perplexity3.1652305126190186
INFO:root:current mean train loss 1460.5455220107492
INFO:root:current train perplexity3.1657965183258057
INFO:root:current mean train loss 1461.5780874088111
INFO:root:current train perplexity3.1668143272399902
INFO:root:current mean train loss 1461.4981855499077
INFO:root:current train perplexity3.165876865386963
INFO:root:current mean train loss 1461.2419750548245
INFO:root:current train perplexity3.1651782989501953
INFO:root:current mean train loss 1460.0738211110151
INFO:root:current train perplexity3.1645591259002686
INFO:root:current mean train loss 1460.7624691309104
INFO:root:current train perplexity3.1643638610839844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.52s/it]
INFO:root:final mean train loss: 1460.5966144355932
INFO:root:final train perplexity: 3.1642446517944336
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 3439.3029103322074
INFO:root:eval perplexity: 16.813785552978516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/84
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [7:02:59<1:20:40, 302.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1441.2535445601852
INFO:root:current train perplexity3.1431710720062256
INFO:root:current mean train loss 1447.9090066744586
INFO:root:current train perplexity3.153308629989624
INFO:root:current mean train loss 1452.998954604901
INFO:root:current train perplexity3.157867908477783
INFO:root:current mean train loss 1455.164159558964
INFO:root:current train perplexity3.1544926166534424
INFO:root:current mean train loss 1454.9673672035092
INFO:root:current train perplexity3.1504323482513428
INFO:root:current mean train loss 1453.8502621153048
INFO:root:current train perplexity3.1524055004119873
INFO:root:current mean train loss 1454.0377859209902
INFO:root:current train perplexity3.1547791957855225
INFO:root:current mean train loss 1453.9117942085948
INFO:root:current train perplexity3.154020309448242
INFO:root:current mean train loss 1453.8753508598945
INFO:root:current train perplexity3.153470993041992
INFO:root:current mean train loss 1455.4528641356105
INFO:root:current train perplexity3.154695749282837
INFO:root:current mean train loss 1455.3221914556962
INFO:root:current train perplexity3.1534488201141357
INFO:root:current mean train loss 1456.9109908123405
INFO:root:current train perplexity3.155722141265869
INFO:root:current mean train loss 1458.014583373128
INFO:root:current train perplexity3.157241106033325
INFO:root:current mean train loss 1458.3352242119795
INFO:root:current train perplexity3.1580700874328613
INFO:root:current mean train loss 1459.5111002718224
INFO:root:current train perplexity3.1594245433807373
INFO:root:current mean train loss 1460.2352027118636
INFO:root:current train perplexity3.1601898670196533
INFO:root:current mean train loss 1460.287379475261
INFO:root:current train perplexity3.160214900970459
INFO:root:current mean train loss 1459.9556447941925
INFO:root:current train perplexity3.160299301147461
INFO:root:current mean train loss 1459.1216517188996
INFO:root:current train perplexity3.1593499183654785
INFO:root:current mean train loss 1458.8193692581976
INFO:root:current train perplexity3.1590874195098877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.18s/it]
INFO:root:final mean train loss: 1458.9077670452755
INFO:root:final train perplexity: 3.1600329875946045
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.17s/it]
INFO:root:eval mean loss: 3438.816454638232
INFO:root:eval perplexity: 16.807079315185547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/85
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [7:08:02<1:15:39, 302.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1428.389315518466
INFO:root:current train perplexity3.1327855587005615
INFO:root:current mean train loss 1434.0250057644314
INFO:root:current train perplexity3.1464385986328125
INFO:root:current mean train loss 1451.3336706943198
INFO:root:current train perplexity3.164794683456421
INFO:root:current mean train loss 1451.6743742477063
INFO:root:current train perplexity3.154827356338501
INFO:root:current mean train loss 1454.5819773631054
INFO:root:current train perplexity3.1580965518951416
INFO:root:current mean train loss 1458.2852172851562
INFO:root:current train perplexity3.1607985496520996
INFO:root:current mean train loss 1460.9606742148073
INFO:root:current train perplexity3.1600589752197266
INFO:root:current mean train loss 1458.6043046520601
INFO:root:current train perplexity3.1561648845672607
INFO:root:current mean train loss 1460.0970625312407
INFO:root:current train perplexity3.158003807067871
INFO:root:current mean train loss 1458.672091338594
INFO:root:current train perplexity3.156358003616333
INFO:root:current mean train loss 1457.621299422107
INFO:root:current train perplexity3.158156394958496
INFO:root:current mean train loss 1457.7968518450543
INFO:root:current train perplexity3.157456874847412
INFO:root:current mean train loss 1457.5167575848448
INFO:root:current train perplexity3.156949281692505
INFO:root:current mean train loss 1456.6442825680688
INFO:root:current train perplexity3.1566104888916016
INFO:root:current mean train loss 1457.4950239440411
INFO:root:current train perplexity3.1575396060943604
INFO:root:current mean train loss 1457.7100137503037
INFO:root:current train perplexity3.1574082374572754
INFO:root:current mean train loss 1458.2756361021613
INFO:root:current train perplexity3.157848358154297
INFO:root:current mean train loss 1458.2259202309704
INFO:root:current train perplexity3.1568357944488525
INFO:root:current mean train loss 1458.2613842482162
INFO:root:current train perplexity3.1572742462158203
INFO:root:current mean train loss 1458.586301764343
INFO:root:current train perplexity3.157150983810425

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.56s/it]
INFO:root:final mean train loss: 1457.6921745604238
INFO:root:final train perplexity: 3.1570048332214355
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.11s/it]
INFO:root:eval mean loss: 3442.854347755959
INFO:root:eval perplexity: 16.86285972595215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/86
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [7:13:05<1:10:38, 302.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1450.8014256211577
INFO:root:current train perplexity3.139169216156006
INFO:root:current mean train loss 1436.914565945264
INFO:root:current train perplexity3.128361701965332
INFO:root:current mean train loss 1443.2949405830939
INFO:root:current train perplexity3.1341662406921387
INFO:root:current mean train loss 1447.2266230279388
INFO:root:current train perplexity3.140017032623291
INFO:root:current mean train loss 1449.2754724465326
INFO:root:current train perplexity3.1485273838043213
INFO:root:current mean train loss 1450.0361963499888
INFO:root:current train perplexity3.15201997756958
INFO:root:current mean train loss 1450.1601689925894
INFO:root:current train perplexity3.1499645709991455
INFO:root:current mean train loss 1451.265165271333
INFO:root:current train perplexity3.1511118412017822
INFO:root:current mean train loss 1452.5788172988803
INFO:root:current train perplexity3.151644706726074
INFO:root:current mean train loss 1452.707395809622
INFO:root:current train perplexity3.1497838497161865
INFO:root:current mean train loss 1453.874954899564
INFO:root:current train perplexity3.1527297496795654
INFO:root:current mean train loss 1454.7258378586616
INFO:root:current train perplexity3.152357816696167
INFO:root:current mean train loss 1453.9352686592363
INFO:root:current train perplexity3.1494252681732178
INFO:root:current mean train loss 1454.6473290907995
INFO:root:current train perplexity3.1510870456695557
INFO:root:current mean train loss 1455.6895576492716
INFO:root:current train perplexity3.154376745223999
INFO:root:current mean train loss 1456.2035669086072
INFO:root:current train perplexity3.1544835567474365
INFO:root:current mean train loss 1455.7134004394238
INFO:root:current train perplexity3.1525933742523193
INFO:root:current mean train loss 1455.7848928387634
INFO:root:current train perplexity3.1525771617889404
INFO:root:current mean train loss 1456.4086949483224
INFO:root:current train perplexity3.1531097888946533
INFO:root:current mean train loss 1456.7991239323048
INFO:root:current train perplexity3.153838872909546

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.45s/it]
INFO:root:final mean train loss: 1456.3726671827724
INFO:root:final train perplexity: 3.1537210941314697
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.24s/it]
INFO:root:eval mean loss: 3446.687456743853
INFO:root:eval perplexity: 16.91597557067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/87
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [7:18:09<1:05:41, 303.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.5577110877405
INFO:root:current train perplexity3.1605746746063232
INFO:root:current mean train loss 1455.402140070883
INFO:root:current train perplexity3.136835813522339
INFO:root:current mean train loss 1458.1473107646696
INFO:root:current train perplexity3.1413698196411133
INFO:root:current mean train loss 1460.2629772367932
INFO:root:current train perplexity3.1501781940460205
INFO:root:current mean train loss 1460.851819409486
INFO:root:current train perplexity3.1520140171051025
INFO:root:current mean train loss 1458.9403988531303
INFO:root:current train perplexity3.151092767715454
INFO:root:current mean train loss 1457.8995708814414
INFO:root:current train perplexity3.1504478454589844
INFO:root:current mean train loss 1455.3695674003857
INFO:root:current train perplexity3.148561954498291
INFO:root:current mean train loss 1454.0818684432393
INFO:root:current train perplexity3.146047353744507
INFO:root:current mean train loss 1452.9095990701687
INFO:root:current train perplexity3.144610643386841
INFO:root:current mean train loss 1453.82453819374
INFO:root:current train perplexity3.145477771759033
INFO:root:current mean train loss 1454.527794726231
INFO:root:current train perplexity3.144888401031494
INFO:root:current mean train loss 1454.749466539362
INFO:root:current train perplexity3.1449661254882812
INFO:root:current mean train loss 1453.516663040582
INFO:root:current train perplexity3.144423246383667
INFO:root:current mean train loss 1454.2373303734723
INFO:root:current train perplexity3.1472816467285156
INFO:root:current mean train loss 1454.6342973793717
INFO:root:current train perplexity3.147817850112915
INFO:root:current mean train loss 1455.8294031009061
INFO:root:current train perplexity3.1502697467803955
INFO:root:current mean train loss 1455.450170733663
INFO:root:current train perplexity3.1507694721221924
INFO:root:current mean train loss 1455.5562443189854
INFO:root:current train perplexity3.1510562896728516
INFO:root:current mean train loss 1455.6106113167696
INFO:root:current train perplexity3.1509861946105957

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.77s/it]
INFO:root:final mean train loss: 1455.2430343897247
INFO:root:final train perplexity: 3.1509127616882324
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.62s/it]
INFO:root:eval mean loss: 3446.718463336383
INFO:root:eval perplexity: 16.91640853881836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/88
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [7:23:11<1:00:33, 302.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1475.338972553454
INFO:root:current train perplexity3.182215929031372
INFO:root:current mean train loss 1464.4991254757613
INFO:root:current train perplexity3.1574482917785645
INFO:root:current mean train loss 1460.0956331931939
INFO:root:current train perplexity3.1540234088897705
INFO:root:current mean train loss 1455.4295842810523
INFO:root:current train perplexity3.1467883586883545
INFO:root:current mean train loss 1453.3490808968593
INFO:root:current train perplexity3.1404314041137695
INFO:root:current mean train loss 1451.6204386735162
INFO:root:current train perplexity3.1375317573547363
INFO:root:current mean train loss 1452.8154447926033
INFO:root:current train perplexity3.1433064937591553
INFO:root:current mean train loss 1455.2655732544713
INFO:root:current train perplexity3.1470961570739746
INFO:root:current mean train loss 1455.5657808953824
INFO:root:current train perplexity3.150386333465576
INFO:root:current mean train loss 1454.5953025626177
INFO:root:current train perplexity3.147728204727173
INFO:root:current mean train loss 1455.0418382339826
INFO:root:current train perplexity3.1480064392089844
INFO:root:current mean train loss 1453.9420803437174
INFO:root:current train perplexity3.145430564880371
INFO:root:current mean train loss 1453.810430931769
INFO:root:current train perplexity3.1449759006500244
INFO:root:current mean train loss 1452.18026906222
INFO:root:current train perplexity3.144247055053711
INFO:root:current mean train loss 1452.6120683855038
INFO:root:current train perplexity3.1425282955169678
INFO:root:current mean train loss 1452.565956617971
INFO:root:current train perplexity3.1429061889648438
INFO:root:current mean train loss 1452.673492521663
INFO:root:current train perplexity3.143507719039917
INFO:root:current mean train loss 1453.2104856018236
INFO:root:current train perplexity3.145655632019043
INFO:root:current mean train loss 1453.8868087406208
INFO:root:current train perplexity3.1469638347625732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.04s/it]
INFO:root:final mean train loss: 1453.7221404829231
INFO:root:final train perplexity: 3.1471359729766846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.92s/it]
INFO:root:eval mean loss: 3447.5847644519517
INFO:root:eval perplexity: 16.928438186645508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/89
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [7:28:14<55:32, 302.99s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1440.9558614095051
INFO:root:current train perplexity3.1255838871002197
INFO:root:current mean train loss 1459.8540006365095
INFO:root:current train perplexity3.154191493988037
INFO:root:current mean train loss 1450.7250135889594
INFO:root:current train perplexity3.1422653198242188
INFO:root:current mean train loss 1449.372995621119
INFO:root:current train perplexity3.1415154933929443
INFO:root:current mean train loss 1451.8284731374204
INFO:root:current train perplexity3.146620512008667
INFO:root:current mean train loss 1451.0718302726746
INFO:root:current train perplexity3.144609212875366
INFO:root:current mean train loss 1448.8799143273845
INFO:root:current train perplexity3.141396999359131
INFO:root:current mean train loss 1448.3101047130112
INFO:root:current train perplexity3.1397898197174072
INFO:root:current mean train loss 1447.336431644233
INFO:root:current train perplexity3.1354730129241943
INFO:root:current mean train loss 1447.2099571897272
INFO:root:current train perplexity3.1348164081573486
INFO:root:current mean train loss 1448.5972065680583
INFO:root:current train perplexity3.135935068130493
INFO:root:current mean train loss 1449.0488518364996
INFO:root:current train perplexity3.1388392448425293
INFO:root:current mean train loss 1449.4337575175975
INFO:root:current train perplexity3.140425443649292
INFO:root:current mean train loss 1451.097285945241
INFO:root:current train perplexity3.141394853591919
INFO:root:current mean train loss 1452.4236850468362
INFO:root:current train perplexity3.142632246017456
INFO:root:current mean train loss 1452.2876563501106
INFO:root:current train perplexity3.1424760818481445
INFO:root:current mean train loss 1451.3646293999834
INFO:root:current train perplexity3.1416678428649902
INFO:root:current mean train loss 1451.7778751694154
INFO:root:current train perplexity3.142796516418457
INFO:root:current mean train loss 1452.0306508988472
INFO:root:current train perplexity3.1430912017822266
INFO:root:current mean train loss 1453.1629099187492
INFO:root:current train perplexity3.1450252532958984

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.65s/it]
INFO:root:final mean train loss: 1453.007172107937
INFO:root:final train perplexity: 3.14536190032959
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.59s/it]
INFO:root:eval mean loss: 3451.9499167135887
INFO:root:eval perplexity: 16.989177703857422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/90
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [7:33:16<50:26, 302.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1432.6560563712285
INFO:root:current train perplexity3.152561902999878
INFO:root:current mean train loss 1444.0694352970568
INFO:root:current train perplexity3.12713623046875
INFO:root:current mean train loss 1453.1056473244746
INFO:root:current train perplexity3.136786460876465
INFO:root:current mean train loss 1455.3510430518618
INFO:root:current train perplexity3.140507698059082
INFO:root:current mean train loss 1457.3291331471264
INFO:root:current train perplexity3.1385867595672607
INFO:root:current mean train loss 1455.1974352311997
INFO:root:current train perplexity3.1389245986938477
INFO:root:current mean train loss 1455.8142159709112
INFO:root:current train perplexity3.1419520378112793
INFO:root:current mean train loss 1454.949535061139
INFO:root:current train perplexity3.1372756958007812
INFO:root:current mean train loss 1455.2041847587925
INFO:root:current train perplexity3.139164924621582
INFO:root:current mean train loss 1453.577835395082
INFO:root:current train perplexity3.138068199157715
INFO:root:current mean train loss 1453.5158554981701
INFO:root:current train perplexity3.138136625289917
INFO:root:current mean train loss 1451.4437040695582
INFO:root:current train perplexity3.136314868927002
INFO:root:current mean train loss 1451.8569775946844
INFO:root:current train perplexity3.1375887393951416
INFO:root:current mean train loss 1450.69433740712
INFO:root:current train perplexity3.1371753215789795
INFO:root:current mean train loss 1450.2392779724676
INFO:root:current train perplexity3.138320207595825
INFO:root:current mean train loss 1450.7318666107597
INFO:root:current train perplexity3.1398468017578125
INFO:root:current mean train loss 1450.248250550328
INFO:root:current train perplexity3.1384947299957275
INFO:root:current mean train loss 1450.835665189303
INFO:root:current train perplexity3.1398661136627197
INFO:root:current mean train loss 1451.801960706841
INFO:root:current train perplexity3.1409735679626465
INFO:root:current mean train loss 1451.4107918509428
INFO:root:current train perplexity3.140078067779541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.16s/it]
INFO:root:final mean train loss: 1450.625633035834
INFO:root:final train perplexity: 3.1394593715667725
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.14s/it]
INFO:root:eval mean loss: 3446.13097594665
INFO:root:eval perplexity: 16.908254623413086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/91
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [7:38:19<45:23, 302.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1460.006366232167
INFO:root:current train perplexity3.173168659210205
INFO:root:current mean train loss 1449.696233043932
INFO:root:current train perplexity3.1424431800842285
INFO:root:current mean train loss 1451.5319387544462
INFO:root:current train perplexity3.1438558101654053
INFO:root:current mean train loss 1448.871546045204
INFO:root:current train perplexity3.137373208999634
INFO:root:current mean train loss 1449.2864505784928
INFO:root:current train perplexity3.1404452323913574
INFO:root:current mean train loss 1449.709720150455
INFO:root:current train perplexity3.1406919956207275
INFO:root:current mean train loss 1449.2876127733166
INFO:root:current train perplexity3.139017343521118
INFO:root:current mean train loss 1447.398243922011
INFO:root:current train perplexity3.1357686519622803
INFO:root:current mean train loss 1446.2090078944573
INFO:root:current train perplexity3.135591983795166
INFO:root:current mean train loss 1447.608775229585
INFO:root:current train perplexity3.137286901473999
INFO:root:current mean train loss 1449.402319709384
INFO:root:current train perplexity3.1401143074035645
INFO:root:current mean train loss 1449.6950450318022
INFO:root:current train perplexity3.1401426792144775
INFO:root:current mean train loss 1449.5832292241423
INFO:root:current train perplexity3.1375396251678467
INFO:root:current mean train loss 1449.9066916659895
INFO:root:current train perplexity3.139430522918701
INFO:root:current mean train loss 1450.5309907483197
INFO:root:current train perplexity3.1391243934631348
INFO:root:current mean train loss 1450.631894682851
INFO:root:current train perplexity3.1392951011657715
INFO:root:current mean train loss 1450.1556931932525
INFO:root:current train perplexity3.138848304748535
INFO:root:current mean train loss 1450.5303081848913
INFO:root:current train perplexity3.139712333679199
INFO:root:current mean train loss 1450.305783950624
INFO:root:current train perplexity3.138627052307129
INFO:root:current mean train loss 1450.20370480262
INFO:root:current train perplexity3.138026714324951

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.96s/it]
INFO:root:final mean train loss: 1450.0780314312278
INFO:root:final train perplexity: 3.138104200363159
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.85s/it]
INFO:root:eval mean loss: 3447.5972339527025
INFO:root:eval perplexity: 16.928617477416992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/92
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [7:43:21<40:20, 302.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1444.3174486917162
INFO:root:current train perplexity3.1228692531585693
INFO:root:current mean train loss 1447.1793729629985
INFO:root:current train perplexity3.1183290481567383
INFO:root:current mean train loss 1447.6696081125237
INFO:root:current train perplexity3.132331132888794
INFO:root:current mean train loss 1446.0068809992683
INFO:root:current train perplexity3.1341702938079834
INFO:root:current mean train loss 1452.2787698370848
INFO:root:current train perplexity3.1425352096557617
INFO:root:current mean train loss 1452.1331964902724
INFO:root:current train perplexity3.141939401626587
INFO:root:current mean train loss 1451.8834272703973
INFO:root:current train perplexity3.1406052112579346
INFO:root:current mean train loss 1452.52863772731
INFO:root:current train perplexity3.1377065181732178
INFO:root:current mean train loss 1452.3777032279747
INFO:root:current train perplexity3.139904260635376
INFO:root:current mean train loss 1452.9153936114762
INFO:root:current train perplexity3.141439437866211
INFO:root:current mean train loss 1452.8538677111505
INFO:root:current train perplexity3.141530752182007
INFO:root:current mean train loss 1451.4508517421941
INFO:root:current train perplexity3.1391448974609375
INFO:root:current mean train loss 1450.441033273487
INFO:root:current train perplexity3.136852741241455
INFO:root:current mean train loss 1450.0490693997042
INFO:root:current train perplexity3.135045051574707
INFO:root:current mean train loss 1448.9162554268305
INFO:root:current train perplexity3.1343159675598145
INFO:root:current mean train loss 1449.3767710738814
INFO:root:current train perplexity3.1356024742126465
INFO:root:current mean train loss 1448.9364473912922
INFO:root:current train perplexity3.1341891288757324
INFO:root:current mean train loss 1448.6256444869364
INFO:root:current train perplexity3.1337473392486572
INFO:root:current mean train loss 1448.2420913114768
INFO:root:current train perplexity3.1325488090515137
INFO:root:current mean train loss 1449.321846618497
INFO:root:current train perplexity3.134894847869873

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.49s/it]
INFO:root:final mean train loss: 1448.8659164421017
INFO:root:final train perplexity: 3.135105609893799
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.65s/it]
INFO:root:eval mean loss: 3448.597043332395
INFO:root:eval perplexity: 16.942502975463867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/93
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [7:48:24<35:18, 302.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1433.434796142578
INFO:root:current train perplexity3.1148159503936768
INFO:root:current mean train loss 1443.9955641004774
INFO:root:current train perplexity3.1184706687927246
INFO:root:current mean train loss 1449.3303366524833
INFO:root:current train perplexity3.1254260540008545
INFO:root:current mean train loss 1450.142870130037
INFO:root:current train perplexity3.138017416000366
INFO:root:current mean train loss 1448.704736582438
INFO:root:current train perplexity3.1349551677703857
INFO:root:current mean train loss 1448.6916078764816
INFO:root:current train perplexity3.135632038116455
INFO:root:current mean train loss 1448.6460267908433
INFO:root:current train perplexity3.1353964805603027
INFO:root:current mean train loss 1448.057503098708
INFO:root:current train perplexity3.1354801654815674
INFO:root:current mean train loss 1447.2472889293324
INFO:root:current train perplexity3.132854461669922
INFO:root:current mean train loss 1447.0547841597577
INFO:root:current train perplexity3.13279128074646
INFO:root:current mean train loss 1447.1549410219545
INFO:root:current train perplexity3.132917881011963
INFO:root:current mean train loss 1445.9174701238082
INFO:root:current train perplexity3.13114070892334
INFO:root:current mean train loss 1446.9384570121765
INFO:root:current train perplexity3.1329100131988525
INFO:root:current mean train loss 1447.4622341874717
INFO:root:current train perplexity3.1322479248046875
INFO:root:current mean train loss 1449.2499487799566
INFO:root:current train perplexity3.134401321411133
INFO:root:current mean train loss 1449.4399553902542
INFO:root:current train perplexity3.134307384490967
INFO:root:current mean train loss 1448.8804462977819
INFO:root:current train perplexity3.134183406829834
INFO:root:current mean train loss 1449.476423147823
INFO:root:current train perplexity3.134891986846924
INFO:root:current mean train loss 1448.6623546843832
INFO:root:current train perplexity3.1336047649383545
INFO:root:current mean train loss 1448.6975313437106
INFO:root:current train perplexity3.1337947845458984

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.16s/it]
INFO:root:final mean train loss: 1448.2939257677072
INFO:root:final train perplexity: 3.1336917877197266
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.54s/it]
INFO:root:eval mean loss: 3449.448413012622
INFO:root:eval perplexity: 16.954345703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/94
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [7:53:26<30:14, 302.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.694328386759
INFO:root:current train perplexity3.1511919498443604
INFO:root:current mean train loss 1451.323198812262
INFO:root:current train perplexity3.1456682682037354
INFO:root:current mean train loss 1450.9844115799926
INFO:root:current train perplexity3.135178565979004
INFO:root:current mean train loss 1450.388495687874
INFO:root:current train perplexity3.1386828422546387
INFO:root:current mean train loss 1449.9726338990977
INFO:root:current train perplexity3.1362764835357666
INFO:root:current mean train loss 1451.6713891724246
INFO:root:current train perplexity3.1337544918060303
INFO:root:current mean train loss 1450.4395878051582
INFO:root:current train perplexity3.1306614875793457
INFO:root:current mean train loss 1448.4319509317165
INFO:root:current train perplexity3.1299545764923096
INFO:root:current mean train loss 1448.2238852544506
INFO:root:current train perplexity3.1283693313598633
INFO:root:current mean train loss 1448.564863291045
INFO:root:current train perplexity3.1301801204681396
INFO:root:current mean train loss 1449.3210213312586
INFO:root:current train perplexity3.133183002471924
INFO:root:current mean train loss 1449.2465719352092
INFO:root:current train perplexity3.133810520172119
INFO:root:current mean train loss 1449.3876624655154
INFO:root:current train perplexity3.13484787940979
INFO:root:current mean train loss 1449.1555870454824
INFO:root:current train perplexity3.133272409439087
INFO:root:current mean train loss 1448.8683760750669
INFO:root:current train perplexity3.1331844329833984
INFO:root:current mean train loss 1449.0028070821265
INFO:root:current train perplexity3.133390426635742
INFO:root:current mean train loss 1448.4428887173367
INFO:root:current train perplexity3.1328561305999756
INFO:root:current mean train loss 1448.2842257440786
INFO:root:current train perplexity3.1331539154052734
INFO:root:current mean train loss 1448.4490429481582
INFO:root:current train perplexity3.1338300704956055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.33s/it]
INFO:root:final mean train loss: 1447.2961086902244
INFO:root:final train perplexity: 3.131226062774658
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.11s/it]
INFO:root:eval mean loss: 3450.0368293097786
INFO:root:eval perplexity: 16.962533950805664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/95
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [7:58:28<25:11, 302.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1413.6590314592634
INFO:root:current train perplexity3.064997434616089
INFO:root:current mean train loss 1435.7873706483003
INFO:root:current train perplexity3.1004080772399902
INFO:root:current mean train loss 1439.826794205425
INFO:root:current train perplexity3.1006600856781006
INFO:root:current mean train loss 1440.762187593302
INFO:root:current train perplexity3.1073853969573975
INFO:root:current mean train loss 1439.0510094684103
INFO:root:current train perplexity3.1051883697509766
INFO:root:current mean train loss 1442.273338228812
INFO:root:current train perplexity3.113846778869629
INFO:root:current mean train loss 1443.2734714967808
INFO:root:current train perplexity3.119938373565674
INFO:root:current mean train loss 1445.3943818249957
INFO:root:current train perplexity3.1209099292755127
INFO:root:current mean train loss 1445.0003209219517
INFO:root:current train perplexity3.1216182708740234
INFO:root:current mean train loss 1445.600573650335
INFO:root:current train perplexity3.1224236488342285
INFO:root:current mean train loss 1445.665829630293
INFO:root:current train perplexity3.1212592124938965
INFO:root:current mean train loss 1446.8183700041027
INFO:root:current train perplexity3.1235170364379883
INFO:root:current mean train loss 1446.913216353641
INFO:root:current train perplexity3.124789237976074
INFO:root:current mean train loss 1446.3645294514602
INFO:root:current train perplexity3.1249427795410156
INFO:root:current mean train loss 1446.4856859177478
INFO:root:current train perplexity3.1267917156219482
INFO:root:current mean train loss 1447.754266736372
INFO:root:current train perplexity3.128056287765503
INFO:root:current mean train loss 1447.593681779788
INFO:root:current train perplexity3.129042148590088
INFO:root:current mean train loss 1447.760577883993
INFO:root:current train perplexity3.1287989616394043
INFO:root:current mean train loss 1448.0598315456605
INFO:root:current train perplexity3.1289756298065186
INFO:root:current mean train loss 1447.9929771941534
INFO:root:current train perplexity3.1295299530029297

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.01s/it]
INFO:root:final mean train loss: 1446.5490237576037
INFO:root:final train perplexity: 3.129382371902466
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.66s/it]
INFO:root:eval mean loss: 3451.0284816066064
INFO:root:eval perplexity: 16.976341247558594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/96
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [8:03:31<20:10, 302.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1465.5266940209174
INFO:root:current train perplexity3.174254894256592
INFO:root:current mean train loss 1453.2621380755008
INFO:root:current train perplexity3.1600701808929443
INFO:root:current mean train loss 1450.308666146679
INFO:root:current train perplexity3.1459271907806396
INFO:root:current mean train loss 1449.0595149936273
INFO:root:current train perplexity3.1362009048461914
INFO:root:current mean train loss 1452.7091027633774
INFO:root:current train perplexity3.1398234367370605
INFO:root:current mean train loss 1450.833048042829
INFO:root:current train perplexity3.1352274417877197
INFO:root:current mean train loss 1450.3025724528898
INFO:root:current train perplexity3.1330740451812744
INFO:root:current mean train loss 1450.1635386496987
INFO:root:current train perplexity3.132558584213257
INFO:root:current mean train loss 1449.4438134295558
INFO:root:current train perplexity3.132122039794922
INFO:root:current mean train loss 1448.1309961304628
INFO:root:current train perplexity3.1304521560668945
INFO:root:current mean train loss 1446.8301207489694
INFO:root:current train perplexity3.128810405731201
INFO:root:current mean train loss 1446.858872471817
INFO:root:current train perplexity3.1288273334503174
INFO:root:current mean train loss 1446.9722789327466
INFO:root:current train perplexity3.1284170150756836
INFO:root:current mean train loss 1446.7744250680878
INFO:root:current train perplexity3.127519130706787
INFO:root:current mean train loss 1445.2470865202984
INFO:root:current train perplexity3.12666916847229
INFO:root:current mean train loss 1445.3043433749388
INFO:root:current train perplexity3.1279120445251465
INFO:root:current mean train loss 1447.1052907713347
INFO:root:current train perplexity3.130951404571533
INFO:root:current mean train loss 1447.0000629744593
INFO:root:current train perplexity3.129580497741699
INFO:root:current mean train loss 1446.6289165169733
INFO:root:current train perplexity3.129542350769043
INFO:root:current mean train loss 1446.690149450475
INFO:root:current train perplexity3.129188299179077

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.40s/it]
INFO:root:final mean train loss: 1446.2434346732382
INFO:root:final train perplexity: 3.1286280155181885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.77s/it]
INFO:root:eval mean loss: 3450.9749378284537
INFO:root:eval perplexity: 16.975601196289062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/97
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [8:08:33<15:06, 302.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1447.6911951700847
INFO:root:current train perplexity3.1383719444274902
INFO:root:current mean train loss 1443.8747088458088
INFO:root:current train perplexity3.140916347503662
INFO:root:current mean train loss 1446.5657299410911
INFO:root:current train perplexity3.142169237136841
INFO:root:current mean train loss 1444.7965961324758
INFO:root:current train perplexity3.131394386291504
INFO:root:current mean train loss 1448.3924072810582
INFO:root:current train perplexity3.134155750274658
INFO:root:current mean train loss 1449.157182234047
INFO:root:current train perplexity3.128567695617676
INFO:root:current mean train loss 1451.1998353181061
INFO:root:current train perplexity3.1315207481384277
INFO:root:current mean train loss 1450.2254287801325
INFO:root:current train perplexity3.1284568309783936
INFO:root:current mean train loss 1448.2339273128869
INFO:root:current train perplexity3.128592014312744
INFO:root:current mean train loss 1448.7406796483551
INFO:root:current train perplexity3.1300580501556396
INFO:root:current mean train loss 1448.2627660154387
INFO:root:current train perplexity3.1300160884857178
INFO:root:current mean train loss 1446.277334499027
INFO:root:current train perplexity3.1289894580841064
INFO:root:current mean train loss 1446.9105766492012
INFO:root:current train perplexity3.1286587715148926
INFO:root:current mean train loss 1447.4644798029778
INFO:root:current train perplexity3.1285386085510254
INFO:root:current mean train loss 1447.0885594826377
INFO:root:current train perplexity3.1282708644866943
INFO:root:current mean train loss 1446.374053245367
INFO:root:current train perplexity3.1269659996032715
INFO:root:current mean train loss 1446.405580835435
INFO:root:current train perplexity3.126342296600342
INFO:root:current mean train loss 1446.2198217466166
INFO:root:current train perplexity3.1261563301086426
INFO:root:current mean train loss 1445.4984722715435
INFO:root:current train perplexity3.126195192337036
INFO:root:current mean train loss 1445.1065581711412
INFO:root:current train perplexity3.1257922649383545

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it]
INFO:root:final mean train loss: 1445.1942950011621
INFO:root:final train perplexity: 3.1260409355163574
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.80s/it]
INFO:root:eval mean loss: 3451.148583397851
INFO:root:eval perplexity: 16.978010177612305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/98
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [8:13:35<10:04, 302.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1448.71552734375
INFO:root:current train perplexity3.1125402450561523
INFO:root:current mean train loss 1444.463592714252
INFO:root:current train perplexity3.12688946723938
INFO:root:current mean train loss 1444.2988308888562
INFO:root:current train perplexity3.1300549507141113
INFO:root:current mean train loss 1442.4150136451199
INFO:root:current train perplexity3.127037525177002
INFO:root:current mean train loss 1445.378610656082
INFO:root:current train perplexity3.1284525394439697
INFO:root:current mean train loss 1448.0273284101909
INFO:root:current train perplexity3.127049684524536
INFO:root:current mean train loss 1446.9117002099977
INFO:root:current train perplexity3.1229310035705566
INFO:root:current mean train loss 1447.5679566227532
INFO:root:current train perplexity3.123562812805176
INFO:root:current mean train loss 1447.6813485029804
INFO:root:current train perplexity3.124903678894043
INFO:root:current mean train loss 1447.3662918960492
INFO:root:current train perplexity3.1247739791870117
INFO:root:current mean train loss 1446.8164381143633
INFO:root:current train perplexity3.122898817062378
INFO:root:current mean train loss 1446.3262344294862
INFO:root:current train perplexity3.1243181228637695
INFO:root:current mean train loss 1446.117283322783
INFO:root:current train perplexity3.1241824626922607
INFO:root:current mean train loss 1446.3767982343177
INFO:root:current train perplexity3.125241994857788
INFO:root:current mean train loss 1445.1324159589644
INFO:root:current train perplexity3.123500347137451
INFO:root:current mean train loss 1444.7520702813
INFO:root:current train perplexity3.123175621032715
INFO:root:current mean train loss 1444.6874200127863
INFO:root:current train perplexity3.1228811740875244
INFO:root:current mean train loss 1444.9434260468308
INFO:root:current train perplexity3.1230368614196777
INFO:root:current mean train loss 1444.3750471263404
INFO:root:current train perplexity3.122191905975342
INFO:root:current mean train loss 1444.7567269128697
INFO:root:current train perplexity3.1236042976379395

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.72s/it]
INFO:root:final mean train loss: 1444.2936670377408
INFO:root:final train perplexity: 3.1238205432891846
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.61s/it]
INFO:root:eval mean loss: 3452.243236644848
INFO:root:eval perplexity: 16.99327278137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/99
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [8:18:38<05:02, 302.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1439.3205104920921
INFO:root:current train perplexity3.092421293258667
INFO:root:current mean train loss 1432.6354256095467
INFO:root:current train perplexity3.0955357551574707
INFO:root:current mean train loss 1437.7060516573858
INFO:root:current train perplexity3.104931116104126
INFO:root:current mean train loss 1439.668509118844
INFO:root:current train perplexity3.1054701805114746
INFO:root:current mean train loss 1442.2537586006385
INFO:root:current train perplexity3.1111412048339844
INFO:root:current mean train loss 1442.0968508376288
INFO:root:current train perplexity3.1150994300842285
INFO:root:current mean train loss 1443.9456774580165
INFO:root:current train perplexity3.1194069385528564
INFO:root:current mean train loss 1444.241409652983
INFO:root:current train perplexity3.120116710662842
INFO:root:current mean train loss 1443.281522374575
INFO:root:current train perplexity3.118680238723755
INFO:root:current mean train loss 1443.8482539221613
INFO:root:current train perplexity3.1198110580444336
INFO:root:current mean train loss 1443.7101188842998
INFO:root:current train perplexity3.119541883468628
INFO:root:current mean train loss 1444.0072420123468
INFO:root:current train perplexity3.119753837585449
INFO:root:current mean train loss 1444.0795232859118
INFO:root:current train perplexity3.1200215816497803
INFO:root:current mean train loss 1444.2788991307038
INFO:root:current train perplexity3.1206696033477783
INFO:root:current mean train loss 1445.849665550407
INFO:root:current train perplexity3.123037815093994
INFO:root:current mean train loss 1444.131351282865
INFO:root:current train perplexity3.120986223220825
INFO:root:current mean train loss 1443.089012626802
INFO:root:current train perplexity3.1203534603118896
INFO:root:current mean train loss 1442.8230020199828
INFO:root:current train perplexity3.119964599609375
INFO:root:current mean train loss 1443.665155619541
INFO:root:current train perplexity3.121807336807251
INFO:root:current mean train loss 1444.4183304033174
INFO:root:current train perplexity3.123155117034912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.11s/it]
INFO:root:final mean train loss: 1443.9849226543295
INFO:root:final train perplexity: 3.1230602264404297
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:30<00:00, 30.31s/it]
INFO:root:eval mean loss: 3451.748185441301
INFO:root:eval perplexity: 16.986373901367188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: pld_22/100
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [8:23:41<00:00, 302.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [8:23:41<00:00, 302.21s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:29<00:00, 29.46s/it]
INFO:root:eval mean loss: 3451.748185441301
INFO:root:eval perplexity: 16.986373901367188
INFO:root:evalaution complete
INFO:root:save model final: pld_22/final
