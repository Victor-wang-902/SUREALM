INFO:root:Output: multiqa_l6_baseline
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading: 100%|██████████| 232k/232k [00:00<00:00, 9.05MB/s]
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 12306.521119397095
INFO:root:current train perplexity17618.76953125
INFO:root:current mean train loss 10365.70459965845
INFO:root:current train perplexity3618.953369140625
INFO:root:current mean train loss 9056.70327850648
INFO:root:current train perplexity1252.97021484375
INFO:root:current mean train loss 8121.81094337406
INFO:root:current train perplexity605.5045776367188
INFO:root:current mean train loss 7450.311169213427
INFO:root:current train perplexity356.4046325683594
INFO:root:current mean train loss 6937.7082641805355
INFO:root:current train perplexity238.2783966064453
INFO:root:current mean train loss 6536.574905766609
INFO:root:current train perplexity174.278076171875
INFO:root:current mean train loss 6210.85617489391
INFO:root:current train perplexity134.8467559814453
INFO:root:current mean train loss 5949.236084527513
INFO:root:current train perplexity109.25801849365234
INFO:root:current mean train loss 5724.293821653685
INFO:root:current train perplexity91.5838394165039
INFO:root:current mean train loss 5530.445708145544
INFO:root:current train perplexity78.69841003417969
INFO:root:current mean train loss 5364.695394558942
INFO:root:current train perplexity69.0345230102539
INFO:root:current mean train loss 5218.223520609303
INFO:root:current train perplexity61.46613693237305
INFO:root:current mean train loss 5091.581081038633
INFO:root:current train perplexity55.472808837890625
INFO:root:current mean train loss 4976.717447048032
INFO:root:current train perplexity50.622169494628906
INFO:root:current mean train loss 4874.965777713855
INFO:root:current train perplexity46.649593353271484
INFO:root:current mean train loss 4781.942844222474
INFO:root:current train perplexity43.326995849609375
INFO:root:current mean train loss 4694.68793264053
INFO:root:current train perplexity40.48394775390625
INFO:root:current mean train loss 4614.092587664354
INFO:root:current train perplexity38.04413604736328

100%|██████████| 1/1 [02:20<00:00, 140.83s/it][A100%|██████████| 1/1 [02:20<00:00, 140.83s/it]
INFO:root:final mean train loss: 4551.567903965937
INFO:root:final train perplexity: 36.221038818359375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2943.1921256856717
INFO:root:eval perplexity: 10.807941436767578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/1
  0%|          | 1/200 [02:28<8:12:06, 148.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3298.5472106933594
INFO:root:current train perplexity12.482882499694824
INFO:root:current mean train loss 3132.5585242962015
INFO:root:current train perplexity11.783483505249023
INFO:root:current mean train loss 3117.095893012153
INFO:root:current train perplexity11.629426956176758
INFO:root:current mean train loss 3090.896793413766
INFO:root:current train perplexity11.446764945983887
INFO:root:current mean train loss 3081.6516811664287
INFO:root:current train perplexity11.323274612426758
INFO:root:current mean train loss 3068.233446697856
INFO:root:current train perplexity11.242548942565918
INFO:root:current mean train loss 3057.48474715592
INFO:root:current train perplexity11.154428482055664
INFO:root:current mean train loss 3047.5849490032515
INFO:root:current train perplexity11.055070877075195
INFO:root:current mean train loss 3035.678430893842
INFO:root:current train perplexity10.963539123535156
INFO:root:current mean train loss 3028.4657331574954
INFO:root:current train perplexity10.899391174316406
INFO:root:current mean train loss 3019.614337590736
INFO:root:current train perplexity10.827156066894531
INFO:root:current mean train loss 3011.0666902056732
INFO:root:current train perplexity10.750859260559082
INFO:root:current mean train loss 3001.009842119719
INFO:root:current train perplexity10.658364295959473
INFO:root:current mean train loss 2991.254672436004
INFO:root:current train perplexity10.584463119506836
INFO:root:current mean train loss 2982.4205310196526
INFO:root:current train perplexity10.507743835449219
INFO:root:current mean train loss 2972.1533781268035
INFO:root:current train perplexity10.427180290222168
INFO:root:current mean train loss 2961.822021635452
INFO:root:current train perplexity10.347705841064453
INFO:root:current mean train loss 2954.845452439813
INFO:root:current train perplexity10.287519454956055
INFO:root:current mean train loss 2947.277972250783
INFO:root:current train perplexity10.225481033325195
INFO:root:current mean train loss 2941.41372113636
INFO:root:current train perplexity10.16788387298584

100%|██████████| 1/1 [02:17<00:00, 137.96s/it][A100%|██████████| 1/1 [02:17<00:00, 137.96s/it]
INFO:root:final mean train loss: 2936.5403051179164
INFO:root:final train perplexity: 10.134366035461426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2621.4290529075242
INFO:root:eval perplexity: 8.331619262695312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/2
  1%|          | 2/200 [04:53<8:03:57, 146.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2749.195009173769
INFO:root:current train perplexity8.681672096252441
INFO:root:current mean train loss 2765.8394123737075
INFO:root:current train perplexity8.738948822021484
INFO:root:current mean train loss 2765.1070216101125
INFO:root:current train perplexity8.759346008300781
INFO:root:current mean train loss 2753.9364149305557
INFO:root:current train perplexity8.703999519348145
INFO:root:current mean train loss 2749.5449275133515
INFO:root:current train perplexity8.702131271362305
INFO:root:current mean train loss 2750.114404388485
INFO:root:current train perplexity8.687902450561523
INFO:root:current mean train loss 2746.3800417160346
INFO:root:current train perplexity8.682178497314453
INFO:root:current mean train loss 2741.523753250767
INFO:root:current train perplexity8.664703369140625
INFO:root:current mean train loss 2737.947171251313
INFO:root:current train perplexity8.643819808959961
INFO:root:current mean train loss 2733.343731159566
INFO:root:current train perplexity8.623807907104492
INFO:root:current mean train loss 2727.4334528905492
INFO:root:current train perplexity8.584824562072754
INFO:root:current mean train loss 2726.1262605672164
INFO:root:current train perplexity8.571773529052734
INFO:root:current mean train loss 2721.678522040374
INFO:root:current train perplexity8.543381690979004
INFO:root:current mean train loss 2719.9542781203113
INFO:root:current train perplexity8.524760246276855
INFO:root:current mean train loss 2717.2434015586837
INFO:root:current train perplexity8.500909805297852
INFO:root:current mean train loss 2713.4918210501774
INFO:root:current train perplexity8.478385925292969
INFO:root:current mean train loss 2708.754737793268
INFO:root:current train perplexity8.457365036010742
INFO:root:current mean train loss 2705.790004829279
INFO:root:current train perplexity8.440887451171875
INFO:root:current mean train loss 2703.017677219722
INFO:root:current train perplexity8.423860549926758
INFO:root:current mean train loss 2699.3279111717134
INFO:root:current train perplexity8.400779724121094

100%|██████████| 1/1 [02:18<00:00, 138.76s/it][A100%|██████████| 1/1 [02:18<00:00, 138.76s/it]
INFO:root:final mean train loss: 2697.819938409587
INFO:root:final train perplexity: 8.395223617553711
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2484.315216713763
INFO:root:eval perplexity: 7.457111358642578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/3
  2%|▏         | 3/200 [07:20<8:00:53, 146.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2594.7395458984374
INFO:root:current train perplexity7.805403709411621
INFO:root:current mean train loss 2619.2051888020833
INFO:root:current train perplexity7.851296901702881
INFO:root:current mean train loss 2621.0642724609374
INFO:root:current train perplexity7.840249061584473
INFO:root:current mean train loss 2616.1617961774555
INFO:root:current train perplexity7.80507230758667
INFO:root:current mean train loss 2611.848687065972
INFO:root:current train perplexity7.811573028564453
INFO:root:current mean train loss 2604.512725497159
INFO:root:current train perplexity7.795037269592285
INFO:root:current mean train loss 2605.0383743990383
INFO:root:current train perplexity7.792672157287598
INFO:root:current mean train loss 2603.0803170572917
INFO:root:current train perplexity7.787790775299072
INFO:root:current mean train loss 2598.2190866268384
INFO:root:current train perplexity7.754520416259766
INFO:root:current mean train loss 2595.1467094983554
INFO:root:current train perplexity7.735698223114014
INFO:root:current mean train loss 2595.962559291295
INFO:root:current train perplexity7.738221645355225
INFO:root:current mean train loss 2592.979452912704
INFO:root:current train perplexity7.717929840087891
INFO:root:current mean train loss 2589.9661966796875
INFO:root:current train perplexity7.699864864349365
INFO:root:current mean train loss 2589.014037000868
INFO:root:current train perplexity7.692562103271484
INFO:root:current mean train loss 2584.2659945783944
INFO:root:current train perplexity7.675516605377197
INFO:root:current mean train loss 2582.6781119266634
INFO:root:current train perplexity7.663393497467041
INFO:root:current mean train loss 2578.9557849491002
INFO:root:current train perplexity7.643342018127441
INFO:root:current mean train loss 2577.7166337890626
INFO:root:current train perplexity7.6352715492248535
INFO:root:current mean train loss 2577.180082611909
INFO:root:current train perplexity7.626258850097656
INFO:root:current mean train loss 2574.854575445713
INFO:root:current train perplexity7.614893913269043

100%|██████████| 1/1 [02:18<00:00, 138.20s/it][A100%|██████████| 1/1 [02:18<00:00, 138.20s/it]
INFO:root:final mean train loss: 2573.1990771656738
INFO:root:final train perplexity: 7.609364032745361
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.36s/it][A100%|██████████| 1/1 [00:07<00:00,  7.36s/it]
INFO:root:eval mean loss: 2405.3871243524213
INFO:root:eval perplexity: 6.9959821701049805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/4
  2%|▏         | 4/200 [09:46<7:57:53, 146.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2531.5865332760027
INFO:root:current train perplexity7.341297626495361
INFO:root:current mean train loss 2510.4440998374344
INFO:root:current train perplexity7.292350769042969
INFO:root:current mean train loss 2511.374473771799
INFO:root:current train perplexity7.263895511627197
INFO:root:current mean train loss 2511.1127959623
INFO:root:current train perplexity7.256005764007568
INFO:root:current mean train loss 2508.88999216868
INFO:root:current train perplexity7.231545925140381
INFO:root:current mean train loss 2514.126718672495
INFO:root:current train perplexity7.246757507324219
INFO:root:current mean train loss 2513.6458266228215
INFO:root:current train perplexity7.244361400604248
INFO:root:current mean train loss 2513.9189911485496
INFO:root:current train perplexity7.237007141113281
INFO:root:current mean train loss 2513.048949772924
INFO:root:current train perplexity7.223607063293457
INFO:root:current mean train loss 2510.519369415315
INFO:root:current train perplexity7.206135272979736
INFO:root:current mean train loss 2508.978808044605
INFO:root:current train perplexity7.1997880935668945
INFO:root:current mean train loss 2506.199546153666
INFO:root:current train perplexity7.187568187713623
INFO:root:current mean train loss 2504.579739179965
INFO:root:current train perplexity7.17766809463501
INFO:root:current mean train loss 2501.0006345513098
INFO:root:current train perplexity7.167942047119141
INFO:root:current mean train loss 2499.035856552488
INFO:root:current train perplexity7.162389755249023
INFO:root:current mean train loss 2498.6497208352444
INFO:root:current train perplexity7.160708904266357
INFO:root:current mean train loss 2495.460959907628
INFO:root:current train perplexity7.153614521026611
INFO:root:current mean train loss 2493.683598033169
INFO:root:current train perplexity7.140598297119141
INFO:root:current mean train loss 2492.631478291753
INFO:root:current train perplexity7.136073112487793
INFO:root:current mean train loss 2491.4547716770066
INFO:root:current train perplexity7.12905740737915

100%|██████████| 1/1 [02:18<00:00, 138.07s/it][A100%|██████████| 1/1 [02:18<00:00, 138.07s/it]
INFO:root:final mean train loss: 2490.532994442015
INFO:root:final train perplexity: 7.129094123840332
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2351.499027333361
INFO:root:eval perplexity: 6.697634220123291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/5
  2%|▎         | 5/200 [12:11<7:54:35, 146.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2453.2677873883927
INFO:root:current train perplexity6.994516849517822
INFO:root:current mean train loss 2456.2716316554856
INFO:root:current train perplexity6.963795185089111
INFO:root:current mean train loss 2463.0275724169232
INFO:root:current train perplexity6.950184345245361
INFO:root:current mean train loss 2460.3125578562417
INFO:root:current train perplexity6.9372429847717285
INFO:root:current mean train loss 2455.667325106534
INFO:root:current train perplexity6.919708251953125
INFO:root:current mean train loss 2449.574328278842
INFO:root:current train perplexity6.890251636505127
INFO:root:current mean train loss 2441.937230874223
INFO:root:current train perplexity6.873211860656738
INFO:root:current mean train loss 2438.490185640296
INFO:root:current train perplexity6.862938404083252
INFO:root:current mean train loss 2439.3046252220465
INFO:root:current train perplexity6.862122535705566
INFO:root:current mean train loss 2439.1578138397963
INFO:root:current train perplexity6.850635528564453
INFO:root:current mean train loss 2439.609991432556
INFO:root:current train perplexity6.846338272094727
INFO:root:current mean train loss 2439.0036520055824
INFO:root:current train perplexity6.841057300567627
INFO:root:current mean train loss 2438.018138469565
INFO:root:current train perplexity6.837930202484131
INFO:root:current mean train loss 2436.140492874763
INFO:root:current train perplexity6.832293510437012
INFO:root:current mean train loss 2434.9833278604597
INFO:root:current train perplexity6.826302528381348
INFO:root:current mean train loss 2433.6155975804186
INFO:root:current train perplexity6.821928977966309
INFO:root:current mean train loss 2432.7191148336597
INFO:root:current train perplexity6.816551685333252
INFO:root:current mean train loss 2431.981856547129
INFO:root:current train perplexity6.810061454772949
INFO:root:current mean train loss 2430.553672450363
INFO:root:current train perplexity6.801858901977539

100%|██████████| 1/1 [02:18<00:00, 138.00s/it][A100%|██████████| 1/1 [02:18<00:00, 138.00s/it]
INFO:root:final mean train loss: 2430.676319054985
INFO:root:final train perplexity: 6.800374984741211
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.13s/it][A100%|██████████| 1/1 [00:07<00:00,  7.13s/it]
INFO:root:eval mean loss: 2309.608146505153
INFO:root:eval perplexity: 6.474526405334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/6
  3%|▎         | 6/200 [14:37<7:51:33, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2404.615478515625
INFO:root:current train perplexity6.431176662445068
INFO:root:current mean train loss 2422.93483620823
INFO:root:current train perplexity6.658016204833984
INFO:root:current mean train loss 2403.0606513331777
INFO:root:current train perplexity6.6304802894592285
INFO:root:current mean train loss 2404.591352393065
INFO:root:current train perplexity6.647700786590576
INFO:root:current mean train loss 2403.5222493692527
INFO:root:current train perplexity6.636135101318359
INFO:root:current mean train loss 2400.8583321637975
INFO:root:current train perplexity6.61773681640625
INFO:root:current mean train loss 2401.0425052484143
INFO:root:current train perplexity6.6126275062561035
INFO:root:current mean train loss 2394.245877819633
INFO:root:current train perplexity6.5910844802856445
INFO:root:current mean train loss 2390.6085224889785
INFO:root:current train perplexity6.57349967956543
INFO:root:current mean train loss 2390.2569769754527
INFO:root:current train perplexity6.577464580535889
INFO:root:current mean train loss 2390.271802782179
INFO:root:current train perplexity6.569183349609375
INFO:root:current mean train loss 2389.72839067201
INFO:root:current train perplexity6.569987773895264
INFO:root:current mean train loss 2389.500115768598
INFO:root:current train perplexity6.570896148681641
INFO:root:current mean train loss 2389.9670139931422
INFO:root:current train perplexity6.5709614753723145
INFO:root:current mean train loss 2389.7935694300386
INFO:root:current train perplexity6.569398880004883
INFO:root:current mean train loss 2387.595012748662
INFO:root:current train perplexity6.562039852142334
INFO:root:current mean train loss 2385.856164192424
INFO:root:current train perplexity6.557879447937012
INFO:root:current mean train loss 2384.2919681466096
INFO:root:current train perplexity6.552818775177002
INFO:root:current mean train loss 2384.4203946348166
INFO:root:current train perplexity6.552452564239502
INFO:root:current mean train loss 2384.9565924775407
INFO:root:current train perplexity6.555738925933838

100%|██████████| 1/1 [02:18<00:00, 138.23s/it][A100%|██████████| 1/1 [02:18<00:00, 138.23s/it]
INFO:root:final mean train loss: 2383.180704567929
INFO:root:final train perplexity: 6.550357818603516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2279.9717303336934
INFO:root:eval perplexity: 6.3211894035339355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/7
  4%|▎         | 7/200 [17:02<7:48:59, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2356.214857313368
INFO:root:current train perplexity6.4195876121521
INFO:root:current mean train loss 2349.4797404661017
INFO:root:current train perplexity6.347964763641357
INFO:root:current mean train loss 2358.863875362851
INFO:root:current train perplexity6.390295028686523
INFO:root:current mean train loss 2350.4216746204306
INFO:root:current train perplexity6.372905731201172
INFO:root:current mean train loss 2352.554449492094
INFO:root:current train perplexity6.361661434173584
INFO:root:current mean train loss 2351.478196781114
INFO:root:current train perplexity6.371885776519775
INFO:root:current mean train loss 2354.705218565117
INFO:root:current train perplexity6.374000072479248
INFO:root:current mean train loss 2356.7590071909276
INFO:root:current train perplexity6.387591361999512
INFO:root:current mean train loss 2355.2284904050944
INFO:root:current train perplexity6.3862223625183105
INFO:root:current mean train loss 2351.7290682657613
INFO:root:current train perplexity6.380975246429443
INFO:root:current mean train loss 2351.9911267593475
INFO:root:current train perplexity6.383243083953857
INFO:root:current mean train loss 2351.9227934753744
INFO:root:current train perplexity6.384956359863281
INFO:root:current mean train loss 2350.8129973012237
INFO:root:current train perplexity6.376302719116211
INFO:root:current mean train loss 2351.356758082944
INFO:root:current train perplexity6.372655868530273
INFO:root:current mean train loss 2350.205400001515
INFO:root:current train perplexity6.367159843444824
INFO:root:current mean train loss 2349.4454366611085
INFO:root:current train perplexity6.365911483764648
INFO:root:current mean train loss 2348.8630667593334
INFO:root:current train perplexity6.367359161376953
INFO:root:current mean train loss 2348.2047501409706
INFO:root:current train perplexity6.366945743560791
INFO:root:current mean train loss 2347.0300975165874
INFO:root:current train perplexity6.360360145568848
INFO:root:current mean train loss 2346.2094030927196
INFO:root:current train perplexity6.359873294830322

100%|██████████| 1/1 [02:18<00:00, 138.43s/it][A100%|██████████| 1/1 [02:18<00:00, 138.43s/it]
INFO:root:final mean train loss: 2345.2249165514295
INFO:root:final train perplexity: 6.357183456420898
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2254.666059778092
INFO:root:eval perplexity: 6.193135738372803
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/8
  4%|▍         | 8/200 [19:28<7:46:45, 145.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2366.9372907366073
INFO:root:current train perplexity6.330506801605225
INFO:root:current mean train loss 2353.049993670428
INFO:root:current train perplexity6.24650764465332
INFO:root:current mean train loss 2342.8685832571477
INFO:root:current train perplexity6.2471184730529785
INFO:root:current mean train loss 2336.4805361619638
INFO:root:current train perplexity6.263219833374023
INFO:root:current mean train loss 2339.3409176881287
INFO:root:current train perplexity6.273992538452148
INFO:root:current mean train loss 2334.767837781104
INFO:root:current train perplexity6.251768589019775
INFO:root:current mean train loss 2329.6189826064224
INFO:root:current train perplexity6.236908435821533
INFO:root:current mean train loss 2328.3385041321217
INFO:root:current train perplexity6.2377471923828125
INFO:root:current mean train loss 2326.3154201850207
INFO:root:current train perplexity6.232542514801025
INFO:root:current mean train loss 2325.000418694644
INFO:root:current train perplexity6.227230548858643
INFO:root:current mean train loss 2321.2650539232336
INFO:root:current train perplexity6.2172369956970215
INFO:root:current mean train loss 2318.69643382606
INFO:root:current train perplexity6.209370136260986
INFO:root:current mean train loss 2316.170456809843
INFO:root:current train perplexity6.202911376953125
INFO:root:current mean train loss 2316.825687434164
INFO:root:current train perplexity6.200331211090088
INFO:root:current mean train loss 2316.945632179606
INFO:root:current train perplexity6.202394485473633
INFO:root:current mean train loss 2316.2380884822883
INFO:root:current train perplexity6.202402114868164
INFO:root:current mean train loss 2315.151477834719
INFO:root:current train perplexity6.20017671585083
INFO:root:current mean train loss 2315.8477127470956
INFO:root:current train perplexity6.202180862426758
INFO:root:current mean train loss 2314.4810256833275
INFO:root:current train perplexity6.199114799499512
INFO:root:current mean train loss 2314.124641296229
INFO:root:current train perplexity6.199660778045654

100%|██████████| 1/1 [02:18<00:00, 138.03s/it][A100%|██████████| 1/1 [02:18<00:00, 138.03s/it]
INFO:root:final mean train loss: 2312.711730002876
INFO:root:final train perplexity: 6.196245193481445
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2236.603650681516
INFO:root:eval perplexity: 6.103324890136719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/9
  4%|▍         | 9/200 [21:54<7:43:54, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2254.5470346304087
INFO:root:current train perplexity6.034200668334961
INFO:root:current mean train loss 2268.8992277446546
INFO:root:current train perplexity6.033151149749756
INFO:root:current mean train loss 2285.5154612707715
INFO:root:current train perplexity6.071688175201416
INFO:root:current mean train loss 2283.5497759038753
INFO:root:current train perplexity6.061494827270508
INFO:root:current mean train loss 2288.705780569431
INFO:root:current train perplexity6.0726165771484375
INFO:root:current mean train loss 2296.329311426135
INFO:root:current train perplexity6.085710048675537
INFO:root:current mean train loss 2295.025718454934
INFO:root:current train perplexity6.086444854736328
INFO:root:current mean train loss 2294.345716760514
INFO:root:current train perplexity6.089337348937988
INFO:root:current mean train loss 2293.565878711396
INFO:root:current train perplexity6.090858459472656
INFO:root:current mean train loss 2291.1851175773045
INFO:root:current train perplexity6.080840110778809
INFO:root:current mean train loss 2289.658309762469
INFO:root:current train perplexity6.076767921447754
INFO:root:current mean train loss 2287.591391245524
INFO:root:current train perplexity6.06879997253418
INFO:root:current mean train loss 2286.135416439166
INFO:root:current train perplexity6.062689304351807
INFO:root:current mean train loss 2288.621900479469
INFO:root:current train perplexity6.070502758026123
INFO:root:current mean train loss 2288.5907220341287
INFO:root:current train perplexity6.070864677429199
INFO:root:current mean train loss 2286.53184989064
INFO:root:current train perplexity6.067230224609375
INFO:root:current mean train loss 2287.0614324020125
INFO:root:current train perplexity6.068058013916016
INFO:root:current mean train loss 2286.7811359422935
INFO:root:current train perplexity6.069753170013428
INFO:root:current mean train loss 2286.52303015954
INFO:root:current train perplexity6.066831111907959
INFO:root:current mean train loss 2285.3106633170705
INFO:root:current train perplexity6.062062740325928

100%|██████████| 1/1 [02:18<00:00, 138.17s/it][A100%|██████████| 1/1 [02:18<00:00, 138.17s/it]
INFO:root:final mean train loss: 2284.742273620205
INFO:root:final train perplexity: 6.061063289642334
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.31s/it][A100%|██████████| 1/1 [00:07<00:00,  7.31s/it]
INFO:root:eval mean loss: 2217.2047941600176
INFO:root:eval perplexity: 6.00831937789917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/10
  5%|▌         | 10/200 [24:20<7:41:30, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2230.472504104393
INFO:root:current train perplexity5.884105205535889
INFO:root:current mean train loss 2249.0141883263223
INFO:root:current train perplexity5.957826137542725
INFO:root:current mean train loss 2253.3064886043508
INFO:root:current train perplexity5.958700180053711
INFO:root:current mean train loss 2258.307482215447
INFO:root:current train perplexity5.948082447052002
INFO:root:current mean train loss 2263.3286338432004
INFO:root:current train perplexity5.95960807800293
INFO:root:current mean train loss 2262.3366669183874
INFO:root:current train perplexity5.958730220794678
INFO:root:current mean train loss 2261.9519927932897
INFO:root:current train perplexity5.965015888214111
INFO:root:current mean train loss 2261.3605849088713
INFO:root:current train perplexity5.954626560211182
INFO:root:current mean train loss 2262.2438349575664
INFO:root:current train perplexity5.959964275360107
INFO:root:current mean train loss 2263.472403920964
INFO:root:current train perplexity5.956724643707275
INFO:root:current mean train loss 2261.9775282143432
INFO:root:current train perplexity5.950369834899902
INFO:root:current mean train loss 2261.7918482928117
INFO:root:current train perplexity5.94783353805542
INFO:root:current mean train loss 2261.6645547252083
INFO:root:current train perplexity5.946750164031982
INFO:root:current mean train loss 2260.4457291797444
INFO:root:current train perplexity5.945377349853516
INFO:root:current mean train loss 2260.9844546074605
INFO:root:current train perplexity5.946588516235352
INFO:root:current mean train loss 2260.930536701696
INFO:root:current train perplexity5.948227882385254
INFO:root:current mean train loss 2261.1934684264297
INFO:root:current train perplexity5.948775768280029
INFO:root:current mean train loss 2260.451725504235
INFO:root:current train perplexity5.940545082092285
INFO:root:current mean train loss 2260.836181967191
INFO:root:current train perplexity5.94387149810791
INFO:root:current mean train loss 2260.209749840794
INFO:root:current train perplexity5.942899227142334

100%|██████████| 1/1 [02:17<00:00, 137.94s/it][A100%|██████████| 1/1 [02:17<00:00, 137.94s/it]
INFO:root:final mean train loss: 2259.8571419073846
INFO:root:final train perplexity: 5.9432692527771
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2202.8306837149544
INFO:root:eval perplexity: 5.938878059387207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/11
  6%|▌         | 11/200 [26:45<7:38:46, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2254.2590828829034
INFO:root:current train perplexity5.828672885894775
INFO:root:current mean train loss 2251.3576916110133
INFO:root:current train perplexity5.8682355880737305
INFO:root:current mean train loss 2245.671490862653
INFO:root:current train perplexity5.865728855133057
INFO:root:current mean train loss 2239.3269352888196
INFO:root:current train perplexity5.847700595855713
INFO:root:current mean train loss 2233.7104894065073
INFO:root:current train perplexity5.828534126281738
INFO:root:current mean train loss 2236.9625171231737
INFO:root:current train perplexity5.844015121459961
INFO:root:current mean train loss 2234.3433494456654
INFO:root:current train perplexity5.838701248168945
INFO:root:current mean train loss 2235.238789410385
INFO:root:current train perplexity5.843142032623291
INFO:root:current mean train loss 2239.220967518826
INFO:root:current train perplexity5.849729061126709
INFO:root:current mean train loss 2239.1718041843624
INFO:root:current train perplexity5.8454270362854
INFO:root:current mean train loss 2242.3618900306096
INFO:root:current train perplexity5.851165771484375
INFO:root:current mean train loss 2241.103373689957
INFO:root:current train perplexity5.848646640777588
INFO:root:current mean train loss 2241.556753962444
INFO:root:current train perplexity5.850805282592773
INFO:root:current mean train loss 2244.0449319154154
INFO:root:current train perplexity5.855116367340088
INFO:root:current mean train loss 2242.070521563893
INFO:root:current train perplexity5.850989818572998
INFO:root:current mean train loss 2240.0517536562597
INFO:root:current train perplexity5.846821308135986
INFO:root:current mean train loss 2240.4587864270557
INFO:root:current train perplexity5.847077369689941
INFO:root:current mean train loss 2240.233025801809
INFO:root:current train perplexity5.846973896026611
INFO:root:current mean train loss 2239.5400822337083
INFO:root:current train perplexity5.845125675201416

100%|██████████| 1/1 [02:18<00:00, 138.55s/it][A100%|██████████| 1/1 [02:18<00:00, 138.55s/it]
INFO:root:final mean train loss: 2238.0493808886768
INFO:root:final train perplexity: 5.841925144195557
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2189.603173222102
INFO:root:eval perplexity: 5.875683784484863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/12
  6%|▌         | 12/200 [29:11<7:36:46, 145.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2269.034912109375
INFO:root:current train perplexity5.798383712768555
INFO:root:current mean train loss 2191.7855900144114
INFO:root:current train perplexity5.655547618865967
INFO:root:current mean train loss 2212.8117958407097
INFO:root:current train perplexity5.710329055786133
INFO:root:current mean train loss 2210.967078885623
INFO:root:current train perplexity5.712584972381592
INFO:root:current mean train loss 2207.85449370202
INFO:root:current train perplexity5.71015739440918
INFO:root:current mean train loss 2215.8284691266463
INFO:root:current train perplexity5.726877212524414
INFO:root:current mean train loss 2219.724639335873
INFO:root:current train perplexity5.739604473114014
INFO:root:current mean train loss 2220.6148143350483
INFO:root:current train perplexity5.746685981750488
INFO:root:current mean train loss 2217.4422382435496
INFO:root:current train perplexity5.740783214569092
INFO:root:current mean train loss 2218.2492879907686
INFO:root:current train perplexity5.739535808563232
INFO:root:current mean train loss 2218.398802859001
INFO:root:current train perplexity5.741237640380859
INFO:root:current mean train loss 2216.9377975948055
INFO:root:current train perplexity5.739504337310791
INFO:root:current mean train loss 2218.6658768118764
INFO:root:current train perplexity5.74388313293457
INFO:root:current mean train loss 2219.119915860484
INFO:root:current train perplexity5.744370937347412
INFO:root:current mean train loss 2218.5675324639165
INFO:root:current train perplexity5.746949195861816
INFO:root:current mean train loss 2218.312885946856
INFO:root:current train perplexity5.744024753570557
INFO:root:current mean train loss 2218.343202625448
INFO:root:current train perplexity5.746380805969238
INFO:root:current mean train loss 2217.559972506583
INFO:root:current train perplexity5.742668151855469
INFO:root:current mean train loss 2218.3158206103976
INFO:root:current train perplexity5.7461347579956055
INFO:root:current mean train loss 2218.843789578236
INFO:root:current train perplexity5.749240875244141

100%|██████████| 1/1 [02:18<00:00, 138.28s/it][A100%|██████████| 1/1 [02:18<00:00, 138.28s/it]
INFO:root:final mean train loss: 2217.902054671741
INFO:root:final train perplexity: 5.749834060668945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2179.2370181252772
INFO:root:eval perplexity: 5.82663106918335
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/13
  6%|▋         | 13/200 [31:37<7:34:21, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2143.1712951660156
INFO:root:current train perplexity5.724912643432617
INFO:root:current mean train loss 2209.097118123372
INFO:root:current train perplexity5.697963714599609
INFO:root:current mean train loss 2209.111260986328
INFO:root:current train perplexity5.675883769989014
INFO:root:current mean train loss 2198.1513259887697
INFO:root:current train perplexity5.673169136047363
INFO:root:current mean train loss 2200.8469488234746
INFO:root:current train perplexity5.679267406463623
INFO:root:current mean train loss 2200.5663301908053
INFO:root:current train perplexity5.672639846801758
INFO:root:current mean train loss 2198.29803703062
INFO:root:current train perplexity5.667963027954102
INFO:root:current mean train loss 2197.6681801689997
INFO:root:current train perplexity5.6550164222717285
INFO:root:current mean train loss 2202.342176186166
INFO:root:current train perplexity5.6658430099487305
INFO:root:current mean train loss 2202.6128934113876
INFO:root:current train perplexity5.662745952606201
INFO:root:current mean train loss 2200.953510119868
INFO:root:current train perplexity5.664919376373291
INFO:root:current mean train loss 2202.1422306605746
INFO:root:current train perplexity5.668572425842285
INFO:root:current mean train loss 2202.0110246501986
INFO:root:current train perplexity5.6691575050354
INFO:root:current mean train loss 2203.1796444054808
INFO:root:current train perplexity5.676468849182129
INFO:root:current mean train loss 2202.5438889194543
INFO:root:current train perplexity5.672971248626709
INFO:root:current mean train loss 2202.368780838816
INFO:root:current train perplexity5.673492431640625
INFO:root:current mean train loss 2200.522664915485
INFO:root:current train perplexity5.666940212249756
INFO:root:current mean train loss 2199.7488660235736
INFO:root:current train perplexity5.666749477386475
INFO:root:current mean train loss 2199.139811153202
INFO:root:current train perplexity5.664069652557373
INFO:root:current mean train loss 2199.7510340372723
INFO:root:current train perplexity5.666055202484131

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 2199.260615284853
INFO:root:final train perplexity: 5.665920257568359
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.35s/it][A100%|██████████| 1/1 [00:07<00:00,  7.35s/it]
INFO:root:eval mean loss: 2168.4396457536845
INFO:root:eval perplexity: 5.775972843170166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/14
  7%|▋         | 14/200 [34:03<7:32:06, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2203.7708476298562
INFO:root:current train perplexity5.616064548492432
INFO:root:current mean train loss 2197.4208155722513
INFO:root:current train perplexity5.562893867492676
INFO:root:current mean train loss 2196.038362527195
INFO:root:current train perplexity5.577131748199463
INFO:root:current mean train loss 2196.8220754561157
INFO:root:current train perplexity5.587360858917236
INFO:root:current mean train loss 2193.4130566071044
INFO:root:current train perplexity5.574824810028076
INFO:root:current mean train loss 2186.169602719099
INFO:root:current train perplexity5.565019130706787
INFO:root:current mean train loss 2189.783392841812
INFO:root:current train perplexity5.587843418121338
INFO:root:current mean train loss 2190.804249736315
INFO:root:current train perplexity5.593817234039307
INFO:root:current mean train loss 2189.88248114546
INFO:root:current train perplexity5.59415340423584
INFO:root:current mean train loss 2187.0321246852486
INFO:root:current train perplexity5.5845112800598145
INFO:root:current mean train loss 2184.6524778272283
INFO:root:current train perplexity5.580763339996338
INFO:root:current mean train loss 2184.831070791763
INFO:root:current train perplexity5.586365222930908
INFO:root:current mean train loss 2184.494001383924
INFO:root:current train perplexity5.585890293121338
INFO:root:current mean train loss 2184.6993744193214
INFO:root:current train perplexity5.5859293937683105
INFO:root:current mean train loss 2185.8308616006384
INFO:root:current train perplexity5.591989994049072
INFO:root:current mean train loss 2184.263562432651
INFO:root:current train perplexity5.590213298797607
INFO:root:current mean train loss 2185.6725604430744
INFO:root:current train perplexity5.593011856079102
INFO:root:current mean train loss 2184.523904346912
INFO:root:current train perplexity5.593440055847168
INFO:root:current mean train loss 2184.5506687486177
INFO:root:current train perplexity5.594911098480225
INFO:root:current mean train loss 2183.2498295301007
INFO:root:current train perplexity5.591838359832764

100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
INFO:root:final mean train loss: 2182.3060569282256
INFO:root:final train perplexity: 5.590662479400635
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.31s/it][A100%|██████████| 1/1 [00:07<00:00,  7.31s/it]
INFO:root:eval mean loss: 2160.322684646498
INFO:root:eval perplexity: 5.738180637359619
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/15
  8%|▊         | 15/200 [36:29<7:29:40, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2169.172772442853
INFO:root:current train perplexity5.563309192657471
INFO:root:current mean train loss 2146.0229064148743
INFO:root:current train perplexity5.470380783081055
INFO:root:current mean train loss 2160.7175696665845
INFO:root:current train perplexity5.5173139572143555
INFO:root:current mean train loss 2159.493647171279
INFO:root:current train perplexity5.511406898498535
INFO:root:current mean train loss 2161.913938278669
INFO:root:current train perplexity5.514684200286865
INFO:root:current mean train loss 2164.0206503747604
INFO:root:current train perplexity5.523439884185791
INFO:root:current mean train loss 2162.512796664457
INFO:root:current train perplexity5.514192581176758
INFO:root:current mean train loss 2163.3433169256155
INFO:root:current train perplexity5.512917518615723
INFO:root:current mean train loss 2166.9046320680714
INFO:root:current train perplexity5.520119667053223
INFO:root:current mean train loss 2166.890008378579
INFO:root:current train perplexity5.522528648376465
INFO:root:current mean train loss 2166.5281167075577
INFO:root:current train perplexity5.518582820892334
INFO:root:current mean train loss 2167.383100433614
INFO:root:current train perplexity5.516443729400635
INFO:root:current mean train loss 2168.972190747421
INFO:root:current train perplexity5.524285793304443
INFO:root:current mean train loss 2168.9965697701255
INFO:root:current train perplexity5.5213623046875
INFO:root:current mean train loss 2169.200313352981
INFO:root:current train perplexity5.519825458526611
INFO:root:current mean train loss 2168.225817902515
INFO:root:current train perplexity5.519778251647949
INFO:root:current mean train loss 2167.2555020942364
INFO:root:current train perplexity5.519049644470215
INFO:root:current mean train loss 2167.6727110494094
INFO:root:current train perplexity5.521921634674072
INFO:root:current mean train loss 2168.3899823202155
INFO:root:current train perplexity5.523940086364746
INFO:root:current mean train loss 2167.8816931889514
INFO:root:current train perplexity5.523653030395508

100%|██████████| 1/1 [02:18<00:00, 138.42s/it][A100%|██████████| 1/1 [02:18<00:00, 138.42s/it]
INFO:root:final mean train loss: 2167.2394618170465
INFO:root:final train perplexity: 5.524624347686768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.13s/it]
INFO:root:eval mean loss: 2150.3658810879324
INFO:root:eval perplexity: 5.692158222198486
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/16
  8%|▊         | 16/200 [38:54<7:27:12, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2138.785909303477
INFO:root:current train perplexity5.432082653045654
INFO:root:current mean train loss 2148.805252164428
INFO:root:current train perplexity5.4323410987854
INFO:root:current mean train loss 2141.5070341328415
INFO:root:current train perplexity5.4405975341796875
INFO:root:current mean train loss 2142.1667816079853
INFO:root:current train perplexity5.4440765380859375
INFO:root:current mean train loss 2143.4721583793626
INFO:root:current train perplexity5.442258834838867
INFO:root:current mean train loss 2144.5000081237686
INFO:root:current train perplexity5.441022872924805
INFO:root:current mean train loss 2145.9902070865546
INFO:root:current train perplexity5.444881916046143
INFO:root:current mean train loss 2146.457669308832
INFO:root:current train perplexity5.448892116546631
INFO:root:current mean train loss 2147.700526626184
INFO:root:current train perplexity5.448760986328125
INFO:root:current mean train loss 2147.4759667315025
INFO:root:current train perplexity5.447276592254639
INFO:root:current mean train loss 2147.621016017084
INFO:root:current train perplexity5.44336462020874
INFO:root:current mean train loss 2148.572115929894
INFO:root:current train perplexity5.447758674621582
INFO:root:current mean train loss 2149.605515042597
INFO:root:current train perplexity5.449769496917725
INFO:root:current mean train loss 2149.687877785803
INFO:root:current train perplexity5.4519243240356445
INFO:root:current mean train loss 2151.1897408325276
INFO:root:current train perplexity5.455993175506592
INFO:root:current mean train loss 2152.114870184322
INFO:root:current train perplexity5.457467555999756
INFO:root:current mean train loss 2152.125824613817
INFO:root:current train perplexity5.459551811218262
INFO:root:current mean train loss 2151.8840812454782
INFO:root:current train perplexity5.462172985076904
INFO:root:current mean train loss 2152.5549633488945
INFO:root:current train perplexity5.4595160484313965
INFO:root:current mean train loss 2153.060109874429
INFO:root:current train perplexity5.459926128387451

100%|██████████| 1/1 [02:18<00:00, 138.73s/it][A100%|██████████| 1/1 [02:18<00:00, 138.74s/it]
INFO:root:final mean train loss: 2152.234515968742
INFO:root:final train perplexity: 5.4596333503723145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2144.5070947958225
INFO:root:eval perplexity: 5.665252208709717
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/17
  8%|▊         | 17/200 [41:21<7:25:12, 145.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2136.4643221768465
INFO:root:current train perplexity5.351903915405273
INFO:root:current mean train loss 2134.2986313840174
INFO:root:current train perplexity5.390027046203613
INFO:root:current mean train loss 2138.3316264682344
INFO:root:current train perplexity5.402499675750732
INFO:root:current mean train loss 2140.7159074606357
INFO:root:current train perplexity5.396164894104004
INFO:root:current mean train loss 2140.9752672539385
INFO:root:current train perplexity5.3960280418396
INFO:root:current mean train loss 2140.312153511307
INFO:root:current train perplexity5.3984246253967285
INFO:root:current mean train loss 2139.8417523406274
INFO:root:current train perplexity5.396413326263428
INFO:root:current mean train loss 2140.7553382524984
INFO:root:current train perplexity5.40103006362915
INFO:root:current mean train loss 2137.6807740357544
INFO:root:current train perplexity5.3989691734313965
INFO:root:current mean train loss 2137.750633455964
INFO:root:current train perplexity5.400672912597656
INFO:root:current mean train loss 2137.395550447352
INFO:root:current train perplexity5.398295879364014
INFO:root:current mean train loss 2139.004314897839
INFO:root:current train perplexity5.398502349853516
INFO:root:current mean train loss 2140.688488598936
INFO:root:current train perplexity5.404181480407715
INFO:root:current mean train loss 2140.8967417076615
INFO:root:current train perplexity5.4071736335754395
INFO:root:current mean train loss 2139.912936056814
INFO:root:current train perplexity5.403367042541504
INFO:root:current mean train loss 2140.1317302405982
INFO:root:current train perplexity5.40455436706543
INFO:root:current mean train loss 2140.9425421258284
INFO:root:current train perplexity5.407771587371826
INFO:root:current mean train loss 2140.79434081212
INFO:root:current train perplexity5.408107757568359
INFO:root:current mean train loss 2140.3741832668497
INFO:root:current train perplexity5.405417442321777

100%|██████████| 1/1 [02:18<00:00, 138.04s/it][A100%|██████████| 1/1 [02:18<00:00, 138.04s/it]
INFO:root:final mean train loss: 2138.760354954368
INFO:root:final train perplexity: 5.401924133300781
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.29s/it][A100%|██████████| 1/1 [00:07<00:00,  7.29s/it]
INFO:root:eval mean loss: 2139.907299718113
INFO:root:eval perplexity: 5.644217014312744
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/18
  9%|▉         | 18/200 [43:46<7:22:25, 145.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2021.86416015625
INFO:root:current train perplexity5.099904537200928
INFO:root:current mean train loss 2121.998074776786
INFO:root:current train perplexity5.362967014312744
INFO:root:current mean train loss 2105.562213581364
INFO:root:current train perplexity5.288437843322754
INFO:root:current mean train loss 2115.205347880379
INFO:root:current train perplexity5.307464122772217
INFO:root:current mean train loss 2116.3757827570407
INFO:root:current train perplexity5.3239641189575195
INFO:root:current mean train loss 2117.9461309754024
INFO:root:current train perplexity5.33553409576416
INFO:root:current mean train loss 2120.093141867898
INFO:root:current train perplexity5.3378729820251465
INFO:root:current mean train loss 2119.25478238586
INFO:root:current train perplexity5.339034080505371
INFO:root:current mean train loss 2118.40215920395
INFO:root:current train perplexity5.336177349090576
INFO:root:current mean train loss 2121.5055232432665
INFO:root:current train perplexity5.341646194458008
INFO:root:current mean train loss 2121.7650605614504
INFO:root:current train perplexity5.3461127281188965
INFO:root:current mean train loss 2123.3641046998728
INFO:root:current train perplexity5.351125240325928
INFO:root:current mean train loss 2124.0273762683155
INFO:root:current train perplexity5.351250648498535
INFO:root:current mean train loss 2124.4954616970485
INFO:root:current train perplexity5.3486151695251465
INFO:root:current mean train loss 2124.7054419901024
INFO:root:current train perplexity5.351434707641602
INFO:root:current mean train loss 2125.901419422238
INFO:root:current train perplexity5.354671478271484
INFO:root:current mean train loss 2126.4246157637267
INFO:root:current train perplexity5.3544793128967285
INFO:root:current mean train loss 2126.4443558410467
INFO:root:current train perplexity5.351807117462158
INFO:root:current mean train loss 2126.07603966034
INFO:root:current train perplexity5.3486552238464355
INFO:root:current mean train loss 2126.330098822486
INFO:root:current train perplexity5.347050666809082

100%|██████████| 1/1 [02:18<00:00, 138.23s/it][A100%|██████████| 1/1 [02:18<00:00, 138.23s/it]
INFO:root:final mean train loss: 2126.1071319657026
INFO:root:final train perplexity: 5.348285675048828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.13s/it][A100%|██████████| 1/1 [00:07<00:00,  7.13s/it]
INFO:root:eval mean loss: 2132.260371647828
INFO:root:eval perplexity: 5.60941743850708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/19
 10%|▉         | 19/200 [46:12<7:19:48, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2122.5693969726562
INFO:root:current train perplexity5.290373802185059
INFO:root:current mean train loss 2105.717282154521
INFO:root:current train perplexity5.288857936859131
INFO:root:current mean train loss 2102.6370200767174
INFO:root:current train perplexity5.263974189758301
INFO:root:current mean train loss 2099.7337191563956
INFO:root:current train perplexity5.2674431800842285
INFO:root:current mean train loss 2108.1417843787026
INFO:root:current train perplexity5.273889541625977
INFO:root:current mean train loss 2108.1650979929955
INFO:root:current train perplexity5.281473159790039
INFO:root:current mean train loss 2110.717293791449
INFO:root:current train perplexity5.282608509063721
INFO:root:current mean train loss 2111.6848452243116
INFO:root:current train perplexity5.290151119232178
INFO:root:current mean train loss 2113.4696260252717
INFO:root:current train perplexity5.293323516845703
INFO:root:current mean train loss 2114.016073826854
INFO:root:current train perplexity5.293355941772461
INFO:root:current mean train loss 2114.7297734747663
INFO:root:current train perplexity5.290731430053711
INFO:root:current mean train loss 2115.885070909578
INFO:root:current train perplexity5.299108505249023
INFO:root:current mean train loss 2115.214668935723
INFO:root:current train perplexity5.296695709228516
INFO:root:current mean train loss 2114.928252758309
INFO:root:current train perplexity5.295395851135254
INFO:root:current mean train loss 2113.654626602194
INFO:root:current train perplexity5.291188716888428
INFO:root:current mean train loss 2113.356649030367
INFO:root:current train perplexity5.292612552642822
INFO:root:current mean train loss 2112.907952135794
INFO:root:current train perplexity5.293529510498047
INFO:root:current mean train loss 2112.866092553399
INFO:root:current train perplexity5.294070243835449
INFO:root:current mean train loss 2113.6562121461434
INFO:root:current train perplexity5.297576904296875
INFO:root:current mean train loss 2113.7344254921427
INFO:root:current train perplexity5.296751499176025

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 2113.5781950534624
INFO:root:final train perplexity: 5.295699119567871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2127.725954312805
INFO:root:eval perplexity: 5.5888848304748535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/20
 10%|█         | 20/200 [48:38<7:17:16, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2087.5690385867388
INFO:root:current train perplexity5.18454647064209
INFO:root:current mean train loss 2096.0909028636465
INFO:root:current train perplexity5.161530017852783
INFO:root:current mean train loss 2096.980335953844
INFO:root:current train perplexity5.185817241668701
INFO:root:current mean train loss 2099.2409289874863
INFO:root:current train perplexity5.210419654846191
INFO:root:current mean train loss 2101.739005886069
INFO:root:current train perplexity5.213855743408203
INFO:root:current mean train loss 2102.943023964735
INFO:root:current train perplexity5.223160743713379
INFO:root:current mean train loss 2103.472588051093
INFO:root:current train perplexity5.235389709472656
INFO:root:current mean train loss 2103.1937573010932
INFO:root:current train perplexity5.237954139709473
INFO:root:current mean train loss 2100.2814660600884
INFO:root:current train perplexity5.235121250152588
INFO:root:current mean train loss 2101.5445243339823
INFO:root:current train perplexity5.242400169372559
INFO:root:current mean train loss 2100.7859689163633
INFO:root:current train perplexity5.239504337310791
INFO:root:current mean train loss 2101.7880682539167
INFO:root:current train perplexity5.239371299743652
INFO:root:current mean train loss 2102.796588592899
INFO:root:current train perplexity5.243371486663818
INFO:root:current mean train loss 2100.8457708608044
INFO:root:current train perplexity5.242312908172607
INFO:root:current mean train loss 2101.557047639148
INFO:root:current train perplexity5.244807720184326
INFO:root:current mean train loss 2101.3316940694294
INFO:root:current train perplexity5.245522975921631
INFO:root:current mean train loss 2101.7502638029573
INFO:root:current train perplexity5.244722366333008
INFO:root:current mean train loss 2103.667956044579
INFO:root:current train perplexity5.2494354248046875
INFO:root:current mean train loss 2102.6760707272338
INFO:root:current train perplexity5.248979091644287
INFO:root:current mean train loss 2103.0999500260887
INFO:root:current train perplexity5.250235557556152

100%|██████████| 1/1 [02:18<00:00, 138.23s/it][A100%|██████████| 1/1 [02:18<00:00, 138.23s/it]
INFO:root:final mean train loss: 2102.4257198147143
INFO:root:final train perplexity: 5.249324798583984
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.25s/it][A100%|██████████| 1/1 [00:07<00:00,  7.25s/it]
INFO:root:eval mean loss: 2120.3278449308787
INFO:root:eval perplexity: 5.555545806884766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/21
 10%|█         | 21/200 [51:03<7:14:51, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2078.966262817383
INFO:root:current train perplexity5.155656814575195
INFO:root:current mean train loss 2083.8326228215146
INFO:root:current train perplexity5.194840908050537
INFO:root:current mean train loss 2079.569257259369
INFO:root:current train perplexity5.177011013031006
INFO:root:current mean train loss 2081.6157291712384
INFO:root:current train perplexity5.188731670379639
INFO:root:current mean train loss 2083.8290560538308
INFO:root:current train perplexity5.186667442321777
INFO:root:current mean train loss 2084.7363478845828
INFO:root:current train perplexity5.183045387268066
INFO:root:current mean train loss 2089.4415634899606
INFO:root:current train perplexity5.196776390075684
INFO:root:current mean train loss 2086.6352345300097
INFO:root:current train perplexity5.195758819580078
INFO:root:current mean train loss 2089.240126850449
INFO:root:current train perplexity5.195367813110352
INFO:root:current mean train loss 2090.404340544505
INFO:root:current train perplexity5.196219444274902
INFO:root:current mean train loss 2090.5303066138067
INFO:root:current train perplexity5.200928211212158
INFO:root:current mean train loss 2090.9992105556607
INFO:root:current train perplexity5.200728416442871
INFO:root:current mean train loss 2089.3600045951307
INFO:root:current train perplexity5.199580192565918
INFO:root:current mean train loss 2089.228875354328
INFO:root:current train perplexity5.197109699249268
INFO:root:current mean train loss 2089.471833532983
INFO:root:current train perplexity5.196883678436279
INFO:root:current mean train loss 2089.787902125969
INFO:root:current train perplexity5.198750972747803
INFO:root:current mean train loss 2091.5018653316774
INFO:root:current train perplexity5.20208740234375
INFO:root:current mean train loss 2091.858033686401
INFO:root:current train perplexity5.20138692855835
INFO:root:current mean train loss 2091.4287406658304
INFO:root:current train perplexity5.202643394470215
INFO:root:current mean train loss 2092.1707208239472
INFO:root:current train perplexity5.204190254211426

100%|██████████| 1/1 [02:18<00:00, 138.08s/it][A100%|██████████| 1/1 [02:18<00:00, 138.08s/it]
INFO:root:final mean train loss: 2091.467500118192
INFO:root:final train perplexity: 5.204153537750244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.25s/it][A100%|██████████| 1/1 [00:07<00:00,  7.25s/it]
INFO:root:eval mean loss: 2116.864558226673
INFO:root:eval perplexity: 5.540007591247559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/22
 11%|█         | 22/200 [53:29<7:12:16, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2065.5413333422516
INFO:root:current train perplexity5.130817413330078
INFO:root:current mean train loss 2085.956738422372
INFO:root:current train perplexity5.18751335144043
INFO:root:current mean train loss 2080.896978469122
INFO:root:current train perplexity5.165889739990234
INFO:root:current mean train loss 2080.1056683824145
INFO:root:current train perplexity5.155333995819092
INFO:root:current mean train loss 2076.754827584071
INFO:root:current train perplexity5.149635314941406
INFO:root:current mean train loss 2076.94230079318
INFO:root:current train perplexity5.14678955078125
INFO:root:current mean train loss 2074.6843486632847
INFO:root:current train perplexity5.135794162750244
INFO:root:current mean train loss 2073.7078524215717
INFO:root:current train perplexity5.136145114898682
INFO:root:current mean train loss 2074.1127602488723
INFO:root:current train perplexity5.137035369873047
INFO:root:current mean train loss 2074.3332859521533
INFO:root:current train perplexity5.140697002410889
INFO:root:current mean train loss 2074.3589861497408
INFO:root:current train perplexity5.143613815307617
INFO:root:current mean train loss 2074.9888314509735
INFO:root:current train perplexity5.144705772399902
INFO:root:current mean train loss 2075.9936939608087
INFO:root:current train perplexity5.14518404006958
INFO:root:current mean train loss 2076.9527602115863
INFO:root:current train perplexity5.147615432739258
INFO:root:current mean train loss 2077.6856118465716
INFO:root:current train perplexity5.148593902587891
INFO:root:current mean train loss 2077.981576384819
INFO:root:current train perplexity5.153779983520508
INFO:root:current mean train loss 2078.074799842629
INFO:root:current train perplexity5.152597427368164
INFO:root:current mean train loss 2079.1665340623676
INFO:root:current train perplexity5.157130241394043
INFO:root:current mean train loss 2079.976384380318
INFO:root:current train perplexity5.159088611602783
INFO:root:current mean train loss 2081.787261823682
INFO:root:current train perplexity5.161858081817627

100%|██████████| 1/1 [02:18<00:00, 138.12s/it][A100%|██████████| 1/1 [02:18<00:00, 138.12s/it]
INFO:root:final mean train loss: 2080.9886842728624
INFO:root:final train perplexity: 5.161323547363281
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2111.99821006829
INFO:root:eval perplexity: 5.518246650695801
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/23
 12%|█▏        | 23/200 [55:55<7:09:46, 145.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2084.800682237413
INFO:root:current train perplexity5.135288238525391
INFO:root:current mean train loss 2078.5898688065377
INFO:root:current train perplexity5.149847030639648
INFO:root:current mean train loss 2068.6520036368534
INFO:root:current train perplexity5.121788024902344
INFO:root:current mean train loss 2070.7279209234775
INFO:root:current train perplexity5.117334365844727
INFO:root:current mean train loss 2069.9958232023278
INFO:root:current train perplexity5.114912986755371
INFO:root:current mean train loss 2070.536775233382
INFO:root:current train perplexity5.108609676361084
INFO:root:current mean train loss 2070.1181401791778
INFO:root:current train perplexity5.105153560638428
INFO:root:current mean train loss 2070.1655415595333
INFO:root:current train perplexity5.107067108154297
INFO:root:current mean train loss 2069.318073538448
INFO:root:current train perplexity5.109462738037109
INFO:root:current mean train loss 2068.7618106109926
INFO:root:current train perplexity5.113091468811035
INFO:root:current mean train loss 2069.5851755124713
INFO:root:current train perplexity5.1158246994018555
INFO:root:current mean train loss 2070.1954996060927
INFO:root:current train perplexity5.114969253540039
INFO:root:current mean train loss 2070.463475805475
INFO:root:current train perplexity5.113437175750732
INFO:root:current mean train loss 2070.2872969593077
INFO:root:current train perplexity5.111647605895996
INFO:root:current mean train loss 2070.2415814086094
INFO:root:current train perplexity5.113449573516846
INFO:root:current mean train loss 2070.838898511203
INFO:root:current train perplexity5.117360591888428
INFO:root:current mean train loss 2071.607486449473
INFO:root:current train perplexity5.118450164794922
INFO:root:current mean train loss 2072.3718767048927
INFO:root:current train perplexity5.118065357208252
INFO:root:current mean train loss 2071.763097886181
INFO:root:current train perplexity5.119995594024658

100%|██████████| 1/1 [02:18<00:00, 138.55s/it][A100%|██████████| 1/1 [02:18<00:00, 138.55s/it]
INFO:root:final mean train loss: 2070.80498193384
INFO:root:final train perplexity: 5.120035648345947
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2108.2320569626827
INFO:root:eval perplexity: 5.50146484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/24
 12%|█▏        | 24/200 [58:21<7:07:39, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2070.943603515625
INFO:root:current train perplexity5.095120429992676
INFO:root:current mean train loss 2064.303684698087
INFO:root:current train perplexity5.057476997375488
INFO:root:current mean train loss 2059.969674078163
INFO:root:current train perplexity5.059392929077148
INFO:root:current mean train loss 2057.7182179802016
INFO:root:current train perplexity5.035948276519775
INFO:root:current mean train loss 2055.757033289504
INFO:root:current train perplexity5.038102626800537
INFO:root:current mean train loss 2058.281292134723
INFO:root:current train perplexity5.059042930603027
INFO:root:current mean train loss 2055.245923816863
INFO:root:current train perplexity5.0556416511535645
INFO:root:current mean train loss 2057.002836106016
INFO:root:current train perplexity5.061318397521973
INFO:root:current mean train loss 2055.4086712880944
INFO:root:current train perplexity5.06231164932251
INFO:root:current mean train loss 2055.0854243201748
INFO:root:current train perplexity5.060852527618408
INFO:root:current mean train loss 2056.6331524058155
INFO:root:current train perplexity5.058785915374756
INFO:root:current mean train loss 2058.1073304601045
INFO:root:current train perplexity5.061448574066162
INFO:root:current mean train loss 2058.724886182328
INFO:root:current train perplexity5.063270092010498
INFO:root:current mean train loss 2058.7177603618734
INFO:root:current train perplexity5.066679954528809
INFO:root:current mean train loss 2060.188853011033
INFO:root:current train perplexity5.066098690032959
INFO:root:current mean train loss 2061.6098086047664
INFO:root:current train perplexity5.069098472595215
INFO:root:current mean train loss 2062.367115792237
INFO:root:current train perplexity5.07302188873291
INFO:root:current mean train loss 2062.743125303209
INFO:root:current train perplexity5.073660373687744
INFO:root:current mean train loss 2062.068745109081
INFO:root:current train perplexity5.075839042663574
INFO:root:current mean train loss 2062.96033252542
INFO:root:current train perplexity5.0795416831970215

100%|██████████| 1/1 [02:18<00:00, 138.50s/it][A100%|██████████| 1/1 [02:18<00:00, 138.50s/it]
INFO:root:final mean train loss: 2060.8870833911983
INFO:root:final train perplexity: 5.080143928527832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2105.148143146055
INFO:root:eval perplexity: 5.487760066986084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/25
 12%|█▎        | 25/200 [1:00:47<7:05:26, 145.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2074.2359059651694
INFO:root:current train perplexity5.093789577484131
INFO:root:current mean train loss 2036.922124062815
INFO:root:current train perplexity4.9956841468811035
INFO:root:current mean train loss 2040.6165809631348
INFO:root:current train perplexity5.012502193450928
INFO:root:current mean train loss 2046.9630605438608
INFO:root:current train perplexity5.025932788848877
INFO:root:current mean train loss 2051.27594397203
INFO:root:current train perplexity5.032587051391602
INFO:root:current mean train loss 2050.7462801168895
INFO:root:current train perplexity5.034396171569824
INFO:root:current mean train loss 2052.687229254307
INFO:root:current train perplexity5.035143852233887
INFO:root:current mean train loss 2054.5783526172954
INFO:root:current train perplexity5.040687561035156
INFO:root:current mean train loss 2053.9991246195673
INFO:root:current train perplexity5.03967809677124
INFO:root:current mean train loss 2051.2192787071326
INFO:root:current train perplexity5.038720607757568
INFO:root:current mean train loss 2051.3143006563187
INFO:root:current train perplexity5.039490222930908
INFO:root:current mean train loss 2052.068071575776
INFO:root:current train perplexity5.04147481918335
INFO:root:current mean train loss 2050.3639275045957
INFO:root:current train perplexity5.0375447273254395
INFO:root:current mean train loss 2050.022136400113
INFO:root:current train perplexity5.036196231842041
INFO:root:current mean train loss 2049.130872662148
INFO:root:current train perplexity5.034368515014648
INFO:root:current mean train loss 2050.008901040385
INFO:root:current train perplexity5.037026882171631
INFO:root:current mean train loss 2051.5314189741766
INFO:root:current train perplexity5.040574550628662
INFO:root:current mean train loss 2052.0793818144125
INFO:root:current train perplexity5.040611743927002
INFO:root:current mean train loss 2050.7838081226014
INFO:root:current train perplexity5.039396286010742
INFO:root:current mean train loss 2051.8633008548477
INFO:root:current train perplexity5.041906356811523

100%|██████████| 1/1 [02:18<00:00, 138.13s/it][A100%|██████████| 1/1 [02:18<00:00, 138.13s/it]
INFO:root:final mean train loss: 2051.371347216723
INFO:root:final train perplexity: 5.04216194152832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2101.2431385229665
INFO:root:eval perplexity: 5.470456123352051
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/26
 13%|█▎        | 26/200 [1:03:12<7:02:50, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2029.1143888147865
INFO:root:current train perplexity4.947104454040527
INFO:root:current mean train loss 2035.290662400266
INFO:root:current train perplexity4.9953203201293945
INFO:root:current mean train loss 2032.788136589082
INFO:root:current train perplexity4.993800163269043
INFO:root:current mean train loss 2035.941341098103
INFO:root:current train perplexity4.998890399932861
INFO:root:current mean train loss 2034.704345703125
INFO:root:current train perplexity4.996073246002197
INFO:root:current mean train loss 2036.5226274585548
INFO:root:current train perplexity4.9887566566467285
INFO:root:current mean train loss 2038.896760318655
INFO:root:current train perplexity4.989856243133545
INFO:root:current mean train loss 2038.456602933114
INFO:root:current train perplexity4.991715908050537
INFO:root:current mean train loss 2039.6449562462842
INFO:root:current train perplexity4.994936466217041
INFO:root:current mean train loss 2040.5439728139945
INFO:root:current train perplexity4.9975690841674805
INFO:root:current mean train loss 2041.2846278649586
INFO:root:current train perplexity4.997166156768799
INFO:root:current mean train loss 2041.9763502410167
INFO:root:current train perplexity4.999428749084473
INFO:root:current mean train loss 2041.3472431192083
INFO:root:current train perplexity5.002354621887207
INFO:root:current mean train loss 2041.0060287805568
INFO:root:current train perplexity5.004158020019531
INFO:root:current mean train loss 2042.2194213443627
INFO:root:current train perplexity5.003744602203369
INFO:root:current mean train loss 2042.6826742222988
INFO:root:current train perplexity5.004344940185547
INFO:root:current mean train loss 2042.7559290019424
INFO:root:current train perplexity5.003152370452881
INFO:root:current mean train loss 2042.5139425191162
INFO:root:current train perplexity5.006398677825928
INFO:root:current mean train loss 2043.9049671013545
INFO:root:current train perplexity5.008269309997559
INFO:root:current mean train loss 2044.386414926363
INFO:root:current train perplexity5.011013031005859

100%|██████████| 1/1 [02:18<00:00, 138.38s/it][A100%|██████████| 1/1 [02:18<00:00, 138.38s/it]
INFO:root:final mean train loss: 2043.1983792472836
INFO:root:final train perplexity: 5.009765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2098.7156558205897
INFO:root:eval perplexity: 5.459285259246826
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/27
 14%|█▎        | 27/200 [1:05:38<7:00:29, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2035.1288178542566
INFO:root:current train perplexity4.927977085113525
INFO:root:current mean train loss 2035.6928054230123
INFO:root:current train perplexity4.957557201385498
INFO:root:current mean train loss 2034.5596029592116
INFO:root:current train perplexity4.955345153808594
INFO:root:current mean train loss 2033.0812207440424
INFO:root:current train perplexity4.9567975997924805
INFO:root:current mean train loss 2029.5245875729224
INFO:root:current train perplexity4.94931697845459
INFO:root:current mean train loss 2028.5518236604644
INFO:root:current train perplexity4.949522495269775
INFO:root:current mean train loss 2033.0292406632907
INFO:root:current train perplexity4.963848114013672
INFO:root:current mean train loss 2034.1554132225017
INFO:root:current train perplexity4.965412139892578
INFO:root:current mean train loss 2034.3407367981954
INFO:root:current train perplexity4.96995210647583
INFO:root:current mean train loss 2034.4044430280776
INFO:root:current train perplexity4.967258930206299
INFO:root:current mean train loss 2034.6849854438651
INFO:root:current train perplexity4.965818405151367
INFO:root:current mean train loss 2034.3793084073766
INFO:root:current train perplexity4.967098712921143
INFO:root:current mean train loss 2035.0705383980028
INFO:root:current train perplexity4.966587066650391
INFO:root:current mean train loss 2035.2277832930147
INFO:root:current train perplexity4.970130443572998
INFO:root:current mean train loss 2034.5413709517532
INFO:root:current train perplexity4.971863746643066
INFO:root:current mean train loss 2034.7187678639482
INFO:root:current train perplexity4.9718523025512695
INFO:root:current mean train loss 2034.4010455933353
INFO:root:current train perplexity4.9704155921936035
INFO:root:current mean train loss 2034.7492000853242
INFO:root:current train perplexity4.972302436828613
INFO:root:current mean train loss 2035.809079928855
INFO:root:current train perplexity4.974104404449463
INFO:root:current mean train loss 2035.5400217931044
INFO:root:current train perplexity4.976043224334717

100%|██████████| 1/1 [02:18<00:00, 138.31s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 2034.7010681183124
INFO:root:final train perplexity: 4.97630500793457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2096.86613561752
INFO:root:eval perplexity: 5.451127052307129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/28
 14%|█▍        | 28/200 [1:08:04<6:57:59, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1997.760478515625
INFO:root:current train perplexity4.893758773803711
INFO:root:current mean train loss 2017.1390464564731
INFO:root:current train perplexity4.910825252532959
INFO:root:current mean train loss 2019.7560422585227
INFO:root:current train perplexity4.901269435882568
INFO:root:current mean train loss 2015.7681572265626
INFO:root:current train perplexity4.891043663024902
INFO:root:current mean train loss 2016.0069741981908
INFO:root:current train perplexity4.900388240814209
INFO:root:current mean train loss 2021.3442764945653
INFO:root:current train perplexity4.912955284118652
INFO:root:current mean train loss 2023.7450218822337
INFO:root:current train perplexity4.92121696472168
INFO:root:current mean train loss 2027.796482642389
INFO:root:current train perplexity4.920333385467529
INFO:root:current mean train loss 2028.8234026227678
INFO:root:current train perplexity4.924186706542969
INFO:root:current mean train loss 2028.9671784855768
INFO:root:current train perplexity4.927326679229736
INFO:root:current mean train loss 2028.6192637172965
INFO:root:current train perplexity4.932766914367676
INFO:root:current mean train loss 2028.8644823179854
INFO:root:current train perplexity4.935915470123291
INFO:root:current mean train loss 2028.017615368413
INFO:root:current train perplexity4.936639308929443
INFO:root:current mean train loss 2027.7158117897727
INFO:root:current train perplexity4.9365010261535645
INFO:root:current mean train loss 2028.2087682070974
INFO:root:current train perplexity4.937305450439453
INFO:root:current mean train loss 2028.114554811508
INFO:root:current train perplexity4.938663005828857
INFO:root:current mean train loss 2026.4677852437032
INFO:root:current train perplexity4.935557842254639
INFO:root:current mean train loss 2025.4976111355634
INFO:root:current train perplexity4.935809135437012
INFO:root:current mean train loss 2025.1113792317708
INFO:root:current train perplexity4.938263893127441
INFO:root:current mean train loss 2026.1357468848892
INFO:root:current train perplexity4.94091796875

100%|██████████| 1/1 [02:18<00:00, 138.41s/it][A100%|██████████| 1/1 [02:18<00:00, 138.41s/it]
INFO:root:final mean train loss: 2025.616583796745
INFO:root:final train perplexity: 4.940779209136963
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2093.4432740989305
INFO:root:eval perplexity: 5.436057090759277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/29
 14%|█▍        | 29/200 [1:10:30<6:55:38, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2018.9489109205163
INFO:root:current train perplexity4.880888938903809
INFO:root:current mean train loss 2029.5117778778076
INFO:root:current train perplexity4.90333890914917
INFO:root:current mean train loss 2028.2875571054956
INFO:root:current train perplexity4.896571159362793
INFO:root:current mean train loss 2026.1590408013792
INFO:root:current train perplexity4.899492263793945
INFO:root:current mean train loss 2022.6376340292334
INFO:root:current train perplexity4.893789768218994
INFO:root:current mean train loss 2024.5994897790856
INFO:root:current train perplexity4.90070104598999
INFO:root:current mean train loss 2021.939163296209
INFO:root:current train perplexity4.898479461669922
INFO:root:current mean train loss 2021.1380274608882
INFO:root:current train perplexity4.898183822631836
INFO:root:current mean train loss 2022.2170525110355
INFO:root:current train perplexity4.90261173248291
INFO:root:current mean train loss 2021.7309746280794
INFO:root:current train perplexity4.907991886138916
INFO:root:current mean train loss 2021.2415279625973
INFO:root:current train perplexity4.908884525299072
INFO:root:current mean train loss 2021.7885773933974
INFO:root:current train perplexity4.912014484405518
INFO:root:current mean train loss 2020.492733226103
INFO:root:current train perplexity4.909160614013672
INFO:root:current mean train loss 2020.388975121509
INFO:root:current train perplexity4.911380767822266
INFO:root:current mean train loss 2018.6984468925414
INFO:root:current train perplexity4.907318115234375
INFO:root:current mean train loss 2017.6062321495172
INFO:root:current train perplexity4.9080729484558105
INFO:root:current mean train loss 2017.164918940118
INFO:root:current train perplexity4.909000396728516
INFO:root:current mean train loss 2016.9026953833443
INFO:root:current train perplexity4.91074800491333
INFO:root:current mean train loss 2018.3600191596195
INFO:root:current train perplexity4.911666393280029

100%|██████████| 1/1 [02:18<00:00, 138.11s/it][A100%|██████████| 1/1 [02:18<00:00, 138.11s/it]
INFO:root:final mean train loss: 2017.643292664159
INFO:root:final train perplexity: 4.9098076820373535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.31s/it][A100%|██████████| 1/1 [00:07<00:00,  7.31s/it]
INFO:root:eval mean loss: 2091.8536822812775
INFO:root:eval perplexity: 5.429073810577393
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/30
 15%|█▌        | 30/200 [1:12:56<6:53:04, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1971.5449761284722
INFO:root:current train perplexity4.746650695800781
INFO:root:current mean train loss 2014.7282558056193
INFO:root:current train perplexity4.84497594833374
INFO:root:current mean train loss 2011.9101451526988
INFO:root:current train perplexity4.862185955047607
INFO:root:current mean train loss 2011.688703715994
INFO:root:current train perplexity4.863419532775879
INFO:root:current mean train loss 2008.3923387597417
INFO:root:current train perplexity4.853038311004639
INFO:root:current mean train loss 2008.0171845741497
INFO:root:current train perplexity4.864208221435547
INFO:root:current mean train loss 2010.1019020519038
INFO:root:current train perplexity4.866299629211426
INFO:root:current mean train loss 2008.9296358482459
INFO:root:current train perplexity4.862229824066162
INFO:root:current mean train loss 2010.3894148592012
INFO:root:current train perplexity4.864382743835449
INFO:root:current mean train loss 2011.578323616053
INFO:root:current train perplexity4.869074821472168
INFO:root:current mean train loss 2012.1481950531156
INFO:root:current train perplexity4.870763301849365
INFO:root:current mean train loss 2011.400618695052
INFO:root:current train perplexity4.871928691864014
INFO:root:current mean train loss 2010.8832460565939
INFO:root:current train perplexity4.872775077819824
INFO:root:current mean train loss 2010.9889953007125
INFO:root:current train perplexity4.872974395751953
INFO:root:current mean train loss 2011.4008230259437
INFO:root:current train perplexity4.874240398406982
INFO:root:current mean train loss 2012.044858938815
INFO:root:current train perplexity4.8766770362854
INFO:root:current mean train loss 2012.5214880924925
INFO:root:current train perplexity4.8794660568237305
INFO:root:current mean train loss 2012.2199955600406
INFO:root:current train perplexity4.880378246307373
INFO:root:current mean train loss 2011.606863077732
INFO:root:current train perplexity4.879615783691406
INFO:root:current mean train loss 2011.3911685933613
INFO:root:current train perplexity4.881551265716553

100%|██████████| 1/1 [02:18<00:00, 138.18s/it][A100%|██████████| 1/1 [02:18<00:00, 138.18s/it]
INFO:root:final mean train loss: 2010.0134717486328
INFO:root:final train perplexity: 4.880352973937988
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2088.995028448443
INFO:root:eval perplexity: 5.416536331176758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/31
 16%|█▌        | 31/200 [1:15:21<6:50:29, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1958.3162794846755
INFO:root:current train perplexity4.762373924255371
INFO:root:current mean train loss 1972.1449342757937
INFO:root:current train perplexity4.769926071166992
INFO:root:current mean train loss 1988.5753832791759
INFO:root:current train perplexity4.8118085861206055
INFO:root:current mean train loss 1996.8122150444547
INFO:root:current train perplexity4.830087184906006
INFO:root:current mean train loss 1996.3374765602075
INFO:root:current train perplexity4.820364952087402
INFO:root:current mean train loss 1999.7496361097908
INFO:root:current train perplexity4.826279640197754
INFO:root:current mean train loss 1998.1251604854108
INFO:root:current train perplexity4.831426620483398
INFO:root:current mean train loss 1995.8906337433282
INFO:root:current train perplexity4.827902317047119
INFO:root:current mean train loss 1995.8937954290727
INFO:root:current train perplexity4.8279337882995605
INFO:root:current mean train loss 1997.330299723483
INFO:root:current train perplexity4.834265232086182
INFO:root:current mean train loss 1998.8378328022204
INFO:root:current train perplexity4.83919620513916
INFO:root:current mean train loss 1999.2612599564277
INFO:root:current train perplexity4.841864109039307
INFO:root:current mean train loss 2000.0471838597955
INFO:root:current train perplexity4.843000411987305
INFO:root:current mean train loss 1999.6189656207284
INFO:root:current train perplexity4.845166206359863
INFO:root:current mean train loss 2000.331306104406
INFO:root:current train perplexity4.847415924072266
INFO:root:current mean train loss 2001.7532844593452
INFO:root:current train perplexity4.849593162536621
INFO:root:current mean train loss 2002.015031314864
INFO:root:current train perplexity4.848545551300049
INFO:root:current mean train loss 2002.6599847433326
INFO:root:current train perplexity4.851715087890625
INFO:root:current mean train loss 2003.380351840601
INFO:root:current train perplexity4.852710723876953
INFO:root:current mean train loss 2002.3303064205688
INFO:root:current train perplexity4.849442481994629

100%|██████████| 1/1 [02:18<00:00, 138.48s/it][A100%|██████████| 1/1 [02:18<00:00, 138.48s/it]
INFO:root:final mean train loss: 2002.0003761833987
INFO:root:final train perplexity: 4.849608421325684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2087.298912102449
INFO:root:eval perplexity: 5.409111499786377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/32
 16%|█▌        | 32/200 [1:17:47<6:48:15, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1983.8902587890625
INFO:root:current train perplexity4.757756233215332
INFO:root:current mean train loss 1978.4650324041193
INFO:root:current train perplexity4.784285068511963
INFO:root:current mean train loss 1980.1734794962063
INFO:root:current train perplexity4.796319484710693
INFO:root:current mean train loss 1986.2956923771183
INFO:root:current train perplexity4.80094575881958
INFO:root:current mean train loss 1988.3345878377186
INFO:root:current train perplexity4.801551818847656
INFO:root:current mean train loss 1992.899610094383
INFO:root:current train perplexity4.805783271789551
INFO:root:current mean train loss 1990.739322359788
INFO:root:current train perplexity4.797675609588623
INFO:root:current mean train loss 1989.9015027299063
INFO:root:current train perplexity4.7992377281188965
INFO:root:current mean train loss 1990.437957582666
INFO:root:current train perplexity4.79967737197876
INFO:root:current mean train loss 1990.9924258154244
INFO:root:current train perplexity4.805807590484619
INFO:root:current mean train loss 1989.5071569718884
INFO:root:current train perplexity4.808096408843994
INFO:root:current mean train loss 1990.4296595188785
INFO:root:current train perplexity4.80716609954834
INFO:root:current mean train loss 1989.568393452553
INFO:root:current train perplexity4.807082653045654
INFO:root:current mean train loss 1989.8956642406517
INFO:root:current train perplexity4.806313991546631
INFO:root:current mean train loss 1991.2186544078634
INFO:root:current train perplexity4.810257434844971
INFO:root:current mean train loss 1992.019508861213
INFO:root:current train perplexity4.81174898147583
INFO:root:current mean train loss 1992.5634143014397
INFO:root:current train perplexity4.812152862548828
INFO:root:current mean train loss 1992.4706546610548
INFO:root:current train perplexity4.811936855316162
INFO:root:current mean train loss 1993.800724420739
INFO:root:current train perplexity4.817083358764648
INFO:root:current mean train loss 1994.4303129925534
INFO:root:current train perplexity4.818981170654297

100%|██████████| 1/1 [02:17<00:00, 137.99s/it][A100%|██████████| 1/1 [02:17<00:00, 137.99s/it]
INFO:root:final mean train loss: 1994.1647821485064
INFO:root:final train perplexity: 4.819732189178467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2086.1910417705562
INFO:root:eval perplexity: 5.404267311096191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/33
 16%|█▋        | 33/200 [1:20:13<6:45:28, 145.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1993.8715372721354
INFO:root:current train perplexity4.7679762840271
INFO:root:current mean train loss 1988.0673500061034
INFO:root:current train perplexity4.77407169342041
INFO:root:current mean train loss 1988.3999272273138
INFO:root:current train perplexity4.7692413330078125
INFO:root:current mean train loss 1982.8743421766494
INFO:root:current train perplexity4.775948524475098
INFO:root:current mean train loss 1984.8107480256454
INFO:root:current train perplexity4.784841537475586
INFO:root:current mean train loss 1989.2034576416015
INFO:root:current train perplexity4.782681941986084
INFO:root:current mean train loss 1989.2662048339844
INFO:root:current train perplexity4.789519309997559
INFO:root:current mean train loss 1986.7827133981805
INFO:root:current train perplexity4.78622579574585
INFO:root:current mean train loss 1987.68759637877
INFO:root:current train perplexity4.7919230461120605
INFO:root:current mean train loss 1985.428370920817
INFO:root:current train perplexity4.791145324707031
INFO:root:current mean train loss 1984.891071477926
INFO:root:current train perplexity4.790009498596191
INFO:root:current mean train loss 1985.539427658607
INFO:root:current train perplexity4.796275615692139
INFO:root:current mean train loss 1985.4973906986297
INFO:root:current train perplexity4.7932610511779785
INFO:root:current mean train loss 1986.0092606488397
INFO:root:current train perplexity4.793031692504883
INFO:root:current mean train loss 1987.8413669533925
INFO:root:current train perplexity4.794074535369873
INFO:root:current mean train loss 1987.6379505646535
INFO:root:current train perplexity4.795036315917969
INFO:root:current mean train loss 1987.7892082490116
INFO:root:current train perplexity4.795835494995117
INFO:root:current mean train loss 1987.5396375482733
INFO:root:current train perplexity4.794149875640869
INFO:root:current mean train loss 1988.4345662434896
INFO:root:current train perplexity4.795252799987793
INFO:root:current mean train loss 1988.053627730389
INFO:root:current train perplexity4.794524669647217

100%|██████████| 1/1 [02:18<00:00, 138.17s/it][A100%|██████████| 1/1 [02:18<00:00, 138.17s/it]
INFO:root:final mean train loss: 1987.6223527424515
INFO:root:final train perplexity: 4.794927597045898
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2083.8573361660574
INFO:root:eval perplexity: 5.394076824188232
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/34
 17%|█▋        | 34/200 [1:22:38<6:43:01, 145.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1956.6957548067166
INFO:root:current train perplexity4.673331260681152
INFO:root:current mean train loss 1959.738588149938
INFO:root:current train perplexity4.711029529571533
INFO:root:current mean train loss 1972.8551047424978
INFO:root:current train perplexity4.728782653808594
INFO:root:current mean train loss 1975.2499307080984
INFO:root:current train perplexity4.738964557647705
INFO:root:current mean train loss 1973.8607837988895
INFO:root:current train perplexity4.73807430267334
INFO:root:current mean train loss 1974.1780589085517
INFO:root:current train perplexity4.739047050476074
INFO:root:current mean train loss 1971.9150913525896
INFO:root:current train perplexity4.7415313720703125
INFO:root:current mean train loss 1972.7472563244048
INFO:root:current train perplexity4.742586612701416
INFO:root:current mean train loss 1974.15393734522
INFO:root:current train perplexity4.748151779174805
INFO:root:current mean train loss 1973.464967319641
INFO:root:current train perplexity4.747431755065918
INFO:root:current mean train loss 1975.2983677261054
INFO:root:current train perplexity4.751091957092285
INFO:root:current mean train loss 1975.9063278885342
INFO:root:current train perplexity4.7561235427856445
INFO:root:current mean train loss 1977.6239975322105
INFO:root:current train perplexity4.761292934417725
INFO:root:current mean train loss 1979.0474679762674
INFO:root:current train perplexity4.763927459716797
INFO:root:current mean train loss 1980.7932951248572
INFO:root:current train perplexity4.765521049499512
INFO:root:current mean train loss 1981.3403654709298
INFO:root:current train perplexity4.769974231719971
INFO:root:current mean train loss 1980.840429862198
INFO:root:current train perplexity4.767697334289551
INFO:root:current mean train loss 1980.7709511674786
INFO:root:current train perplexity4.76786994934082
INFO:root:current mean train loss 1980.9365179095423
INFO:root:current train perplexity4.769613742828369
INFO:root:current mean train loss 1981.3907309548085
INFO:root:current train perplexity4.769228935241699

100%|██████████| 1/1 [02:18<00:00, 138.29s/it][A100%|██████████| 1/1 [02:18<00:00, 138.29s/it]
INFO:root:final mean train loss: 1980.7817799255095
INFO:root:final train perplexity: 4.769129276275635
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.09s/it][A100%|██████████| 1/1 [00:07<00:00,  7.09s/it]
INFO:root:eval mean loss: 2082.9605695575688
INFO:root:eval perplexity: 5.39016580581665
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/35
 18%|█▊        | 35/200 [1:25:04<6:40:34, 145.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1960.7529530626662
INFO:root:current train perplexity4.697831153869629
INFO:root:current mean train loss 1965.1928899706024
INFO:root:current train perplexity4.716376781463623
INFO:root:current mean train loss 1965.4868836694834
INFO:root:current train perplexity4.704538345336914
INFO:root:current mean train loss 1965.0118206818092
INFO:root:current train perplexity4.707516670227051
INFO:root:current mean train loss 1967.3859139260976
INFO:root:current train perplexity4.711294651031494
INFO:root:current mean train loss 1971.0099524295692
INFO:root:current train perplexity4.716270446777344
INFO:root:current mean train loss 1973.3359063667936
INFO:root:current train perplexity4.727461338043213
INFO:root:current mean train loss 1971.5492942675535
INFO:root:current train perplexity4.7278618812561035
INFO:root:current mean train loss 1973.7795340518824
INFO:root:current train perplexity4.732100486755371
INFO:root:current mean train loss 1974.70400270274
INFO:root:current train perplexity4.732537269592285
INFO:root:current mean train loss 1976.9527267651322
INFO:root:current train perplexity4.737720966339111
INFO:root:current mean train loss 1975.9500539194999
INFO:root:current train perplexity4.732722282409668
INFO:root:current mean train loss 1975.1102605286117
INFO:root:current train perplexity4.733409881591797
INFO:root:current mean train loss 1976.737054679794
INFO:root:current train perplexity4.7384033203125
INFO:root:current mean train loss 1977.20790690431
INFO:root:current train perplexity4.738556861877441
INFO:root:current mean train loss 1977.167060497863
INFO:root:current train perplexity4.740067005157471
INFO:root:current mean train loss 1976.1242676501854
INFO:root:current train perplexity4.74042272567749
INFO:root:current mean train loss 1976.073853831892
INFO:root:current train perplexity4.741429328918457
INFO:root:current mean train loss 1975.5160367133876
INFO:root:current train perplexity4.7410173416137695

100%|██████████| 1/1 [02:18<00:00, 138.02s/it][A100%|██████████| 1/1 [02:18<00:00, 138.02s/it]
INFO:root:final mean train loss: 1973.6955172432472
INFO:root:final train perplexity: 4.742550373077393
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2080.661523697224
INFO:root:eval perplexity: 5.380153656005859
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/36
 18%|█▊        | 36/200 [1:27:29<6:38:03, 145.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1954.3540593927557
INFO:root:current train perplexity4.578303337097168
INFO:root:current mean train loss 1959.8779010944538
INFO:root:current train perplexity4.651660442352295
INFO:root:current mean train loss 1964.0904436879814
INFO:root:current train perplexity4.655505657196045
INFO:root:current mean train loss 1971.8458550259998
INFO:root:current train perplexity4.6771559715271
INFO:root:current mean train loss 1970.1589922754144
INFO:root:current train perplexity4.6874680519104
INFO:root:current mean train loss 1973.281554100798
INFO:root:current train perplexity4.698114395141602
INFO:root:current mean train loss 1971.2874604020687
INFO:root:current train perplexity4.701577663421631
INFO:root:current mean train loss 1968.5145352949742
INFO:root:current train perplexity4.705216884613037
INFO:root:current mean train loss 1966.8520188713778
INFO:root:current train perplexity4.703811168670654
INFO:root:current mean train loss 1965.9544456641054
INFO:root:current train perplexity4.703521251678467
INFO:root:current mean train loss 1966.0897150388694
INFO:root:current train perplexity4.708237648010254
INFO:root:current mean train loss 1967.7738591974432
INFO:root:current train perplexity4.713119983673096
INFO:root:current mean train loss 1966.2764284746593
INFO:root:current train perplexity4.716071128845215
INFO:root:current mean train loss 1966.8535912322416
INFO:root:current train perplexity4.717010974884033
INFO:root:current mean train loss 1966.4224947815158
INFO:root:current train perplexity4.715324878692627
INFO:root:current mean train loss 1966.849708259224
INFO:root:current train perplexity4.718614101409912
INFO:root:current mean train loss 1967.6529669829736
INFO:root:current train perplexity4.71823263168335
INFO:root:current mean train loss 1967.2619867196631
INFO:root:current train perplexity4.718664169311523
INFO:root:current mean train loss 1968.0308403128884
INFO:root:current train perplexity4.7179975509643555
INFO:root:current mean train loss 1967.3175119093612
INFO:root:current train perplexity4.717548847198486

100%|██████████| 1/1 [02:18<00:00, 138.16s/it][A100%|██████████| 1/1 [02:18<00:00, 138.16s/it]
INFO:root:final mean train loss: 1967.0584734341019
INFO:root:final train perplexity: 4.7177910804748535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.29s/it][A100%|██████████| 1/1 [00:07<00:00,  7.29s/it]
INFO:root:eval mean loss: 2080.494267024047
INFO:root:eval perplexity: 5.379425525665283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/37
 18%|█▊        | 37/200 [1:29:55<6:35:43, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1980.1449977329798
INFO:root:current train perplexity4.672914981842041
INFO:root:current mean train loss 1960.047423362732
INFO:root:current train perplexity4.663313388824463
INFO:root:current mean train loss 1950.231220981531
INFO:root:current train perplexity4.636143207550049
INFO:root:current mean train loss 1953.5501414973562
INFO:root:current train perplexity4.650273323059082
INFO:root:current mean train loss 1954.5323155483352
INFO:root:current train perplexity4.653271675109863
INFO:root:current mean train loss 1953.9857508341472
INFO:root:current train perplexity4.659663677215576
INFO:root:current mean train loss 1959.4673568822775
INFO:root:current train perplexity4.670636177062988
INFO:root:current mean train loss 1958.1110209370706
INFO:root:current train perplexity4.670298099517822
INFO:root:current mean train loss 1957.5068745636133
INFO:root:current train perplexity4.669166564941406
INFO:root:current mean train loss 1957.8157011887122
INFO:root:current train perplexity4.665927410125732
INFO:root:current mean train loss 1957.4023769987232
INFO:root:current train perplexity4.667014122009277
INFO:root:current mean train loss 1957.2413293283882
INFO:root:current train perplexity4.67331600189209
INFO:root:current mean train loss 1958.2314070412701
INFO:root:current train perplexity4.677257061004639
INFO:root:current mean train loss 1959.7857308445207
INFO:root:current train perplexity4.682118892669678
INFO:root:current mean train loss 1959.0646469158953
INFO:root:current train perplexity4.682799816131592
INFO:root:current mean train loss 1959.8584514837614
INFO:root:current train perplexity4.68664026260376
INFO:root:current mean train loss 1960.7900847263945
INFO:root:current train perplexity4.6899847984313965
INFO:root:current mean train loss 1959.793139634309
INFO:root:current train perplexity4.689432621002197
INFO:root:current mean train loss 1959.5292998800132
INFO:root:current train perplexity4.687716484069824
INFO:root:current mean train loss 1961.0653077438164
INFO:root:current train perplexity4.692026138305664

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1960.5288880656478
INFO:root:final train perplexity: 4.693558216094971
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2078.820561835106
INFO:root:eval perplexity: 5.37214994430542
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/38
 19%|█▉        | 38/200 [1:32:21<6:33:28, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1968.6953938802083
INFO:root:current train perplexity4.672825336456299
INFO:root:current mean train loss 1958.5548297750538
INFO:root:current train perplexity4.630000591278076
INFO:root:current mean train loss 1949.9293920400191
INFO:root:current train perplexity4.633249759674072
INFO:root:current mean train loss 1952.4534544129303
INFO:root:current train perplexity4.639995098114014
INFO:root:current mean train loss 1951.6317840919066
INFO:root:current train perplexity4.649938106536865
INFO:root:current mean train loss 1951.972487815367
INFO:root:current train perplexity4.6495208740234375
INFO:root:current mean train loss 1952.6184943147407
INFO:root:current train perplexity4.651819705963135
INFO:root:current mean train loss 1956.3520086710885
INFO:root:current train perplexity4.657301902770996
INFO:root:current mean train loss 1956.4103158804087
INFO:root:current train perplexity4.6535844802856445
INFO:root:current mean train loss 1956.6395042782738
INFO:root:current train perplexity4.653213977813721
INFO:root:current mean train loss 1957.3490834797399
INFO:root:current train perplexity4.658645153045654
INFO:root:current mean train loss 1957.459785348151
INFO:root:current train perplexity4.658051490783691
INFO:root:current mean train loss 1956.3209632475214
INFO:root:current train perplexity4.656233310699463
INFO:root:current mean train loss 1956.572631291386
INFO:root:current train perplexity4.657166004180908
INFO:root:current mean train loss 1957.5230498317203
INFO:root:current train perplexity4.66202974319458
INFO:root:current mean train loss 1956.6675601897502
INFO:root:current train perplexity4.664708614349365
INFO:root:current mean train loss 1956.5972585011398
INFO:root:current train perplexity4.667943477630615
INFO:root:current mean train loss 1955.854164241583
INFO:root:current train perplexity4.669572353363037
INFO:root:current mean train loss 1954.7849965992336
INFO:root:current train perplexity4.668245792388916
INFO:root:current mean train loss 1954.5245954420388
INFO:root:current train perplexity4.669279098510742

100%|██████████| 1/1 [02:18<00:00, 138.38s/it][A100%|██████████| 1/1 [02:18<00:00, 138.38s/it]
INFO:root:final mean train loss: 1954.020240895266
INFO:root:final train perplexity: 4.669527053833008
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2079.1646429832945
INFO:root:eval perplexity: 5.37364387512207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/39
 20%|█▉        | 39/200 [1:34:47<6:31:06, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1930.7430794008317
INFO:root:current train perplexity4.552731037139893
INFO:root:current mean train loss 1933.8709935317804
INFO:root:current train perplexity4.575361251831055
INFO:root:current mean train loss 1943.057560811516
INFO:root:current train perplexity4.601022720336914
INFO:root:current mean train loss 1944.8236131193887
INFO:root:current train perplexity4.615871429443359
INFO:root:current mean train loss 1941.7518585337189
INFO:root:current train perplexity4.618886947631836
INFO:root:current mean train loss 1942.584352758007
INFO:root:current train perplexity4.621731758117676
INFO:root:current mean train loss 1937.113586241385
INFO:root:current train perplexity4.6089067459106445
INFO:root:current mean train loss 1939.6207860110626
INFO:root:current train perplexity4.617212295532227
INFO:root:current mean train loss 1941.9658160641131
INFO:root:current train perplexity4.620677471160889
INFO:root:current mean train loss 1941.5596512697343
INFO:root:current train perplexity4.623889446258545
INFO:root:current mean train loss 1940.9735628117276
INFO:root:current train perplexity4.62064790725708
INFO:root:current mean train loss 1939.862695585635
INFO:root:current train perplexity4.624312400817871
INFO:root:current mean train loss 1940.7625959731886
INFO:root:current train perplexity4.628017902374268
INFO:root:current mean train loss 1942.2520851437741
INFO:root:current train perplexity4.632224082946777
INFO:root:current mean train loss 1943.7171832083352
INFO:root:current train perplexity4.635231018066406
INFO:root:current mean train loss 1944.528983415318
INFO:root:current train perplexity4.637298583984375
INFO:root:current mean train loss 1945.1710180840337
INFO:root:current train perplexity4.637182712554932
INFO:root:current mean train loss 1946.835394557299
INFO:root:current train perplexity4.641566753387451
INFO:root:current mean train loss 1947.3912572481706
INFO:root:current train perplexity4.643475532531738
INFO:root:current mean train loss 1948.5496360164414
INFO:root:current train perplexity4.647319316864014

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1947.8898705094377
INFO:root:final train perplexity: 4.647006034851074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2077.5708068414783
INFO:root:eval perplexity: 5.366721153259277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/40
 20%|██        | 40/200 [1:37:13<6:28:36, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1951.3613327605815
INFO:root:current train perplexity4.579394817352295
INFO:root:current mean train loss 1934.1972438023743
INFO:root:current train perplexity4.58951473236084
INFO:root:current mean train loss 1944.8859119483648
INFO:root:current train perplexity4.607123851776123
INFO:root:current mean train loss 1940.4445810443808
INFO:root:current train perplexity4.601798057556152
INFO:root:current mean train loss 1942.000859843913
INFO:root:current train perplexity4.610532283782959
INFO:root:current mean train loss 1942.0093226721044
INFO:root:current train perplexity4.6137518882751465
INFO:root:current mean train loss 1944.4774452175764
INFO:root:current train perplexity4.620210647583008
INFO:root:current mean train loss 1944.1088384547497
INFO:root:current train perplexity4.622577667236328
INFO:root:current mean train loss 1942.835987078045
INFO:root:current train perplexity4.621391296386719
INFO:root:current mean train loss 1944.2894157433047
INFO:root:current train perplexity4.6248273849487305
INFO:root:current mean train loss 1944.9875553898285
INFO:root:current train perplexity4.626032829284668
INFO:root:current mean train loss 1945.2379072737754
INFO:root:current train perplexity4.628547668457031
INFO:root:current mean train loss 1944.3045240078613
INFO:root:current train perplexity4.626937389373779
INFO:root:current mean train loss 1942.256809292711
INFO:root:current train perplexity4.6223907470703125
INFO:root:current mean train loss 1941.6189042427316
INFO:root:current train perplexity4.622025489807129
INFO:root:current mean train loss 1943.2096014524125
INFO:root:current train perplexity4.625550746917725
INFO:root:current mean train loss 1942.887960101111
INFO:root:current train perplexity4.62626314163208
INFO:root:current mean train loss 1942.4433859299247
INFO:root:current train perplexity4.625916957855225
INFO:root:current mean train loss 1941.6737437996858
INFO:root:current train perplexity4.624989986419678
INFO:root:current mean train loss 1942.6229487993069
INFO:root:current train perplexity4.626184940338135

100%|██████████| 1/1 [02:18<00:00, 138.31s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 1942.1269582343475
INFO:root:final train perplexity: 4.625933647155762
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2076.388428600122
INFO:root:eval perplexity: 5.3615922927856445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/41
 20%|██        | 41/200 [1:39:38<6:26:14, 145.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1906.5262451171875
INFO:root:current train perplexity4.50768518447876
INFO:root:current mean train loss 1921.7419713857223
INFO:root:current train perplexity4.550319671630859
INFO:root:current mean train loss 1928.8505071691566
INFO:root:current train perplexity4.571323871612549
INFO:root:current mean train loss 1930.5851945973407
INFO:root:current train perplexity4.571869373321533
INFO:root:current mean train loss 1930.8536920855122
INFO:root:current train perplexity4.580294132232666
INFO:root:current mean train loss 1924.582395207962
INFO:root:current train perplexity4.571616172790527
INFO:root:current mean train loss 1923.5280709102235
INFO:root:current train perplexity4.57722806930542
INFO:root:current mean train loss 1925.683522133372
INFO:root:current train perplexity4.581121921539307
INFO:root:current mean train loss 1928.2328324999128
INFO:root:current train perplexity4.581967830657959
INFO:root:current mean train loss 1929.6163115597153
INFO:root:current train perplexity4.584633827209473
INFO:root:current mean train loss 1931.676568247106
INFO:root:current train perplexity4.590508937835693
INFO:root:current mean train loss 1930.7556353412742
INFO:root:current train perplexity4.589104175567627
INFO:root:current mean train loss 1933.2705025378568
INFO:root:current train perplexity4.592177391052246
INFO:root:current mean train loss 1934.1346168845978
INFO:root:current train perplexity4.594228267669678
INFO:root:current mean train loss 1934.609440196644
INFO:root:current train perplexity4.595435619354248
INFO:root:current mean train loss 1933.8199614331238
INFO:root:current train perplexity4.595900535583496
INFO:root:current mean train loss 1933.7363566992417
INFO:root:current train perplexity4.600456714630127
INFO:root:current mean train loss 1935.022568598622
INFO:root:current train perplexity4.602363586425781
INFO:root:current mean train loss 1935.7430725097656
INFO:root:current train perplexity4.604638576507568

100%|██████████| 1/1 [02:18<00:00, 138.16s/it][A100%|██████████| 1/1 [02:18<00:00, 138.16s/it]
INFO:root:final mean train loss: 1935.6963718585516
INFO:root:final train perplexity: 4.602532386779785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2077.1421954648713
INFO:root:eval perplexity: 5.364861965179443
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/42
 21%|██        | 42/200 [1:42:04<6:23:44, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1916.318096454327
INFO:root:current train perplexity4.52015495300293
INFO:root:current mean train loss 1910.10087652966
INFO:root:current train perplexity4.533452987670898
INFO:root:current mean train loss 1909.198474866124
INFO:root:current train perplexity4.548983573913574
INFO:root:current mean train loss 1913.8138751435204
INFO:root:current train perplexity4.535649299621582
INFO:root:current mean train loss 1915.0958272643009
INFO:root:current train perplexity4.548470973968506
INFO:root:current mean train loss 1919.2456647192525
INFO:root:current train perplexity4.5589919090271
INFO:root:current mean train loss 1919.1880703252448
INFO:root:current train perplexity4.567832946777344
INFO:root:current mean train loss 1921.1171825350084
INFO:root:current train perplexity4.566831588745117
INFO:root:current mean train loss 1923.39007110408
INFO:root:current train perplexity4.570615291595459
INFO:root:current mean train loss 1924.5436214852307
INFO:root:current train perplexity4.572647571563721
INFO:root:current mean train loss 1924.2854084643773
INFO:root:current train perplexity4.570789337158203
INFO:root:current mean train loss 1926.8413144066221
INFO:root:current train perplexity4.574591636657715
INFO:root:current mean train loss 1927.2972879055994
INFO:root:current train perplexity4.574810981750488
INFO:root:current mean train loss 1928.6064074734923
INFO:root:current train perplexity4.575505256652832
INFO:root:current mean train loss 1929.6760342888856
INFO:root:current train perplexity4.578189373016357
INFO:root:current mean train loss 1930.110161558808
INFO:root:current train perplexity4.578770637512207
INFO:root:current mean train loss 1931.4482220568718
INFO:root:current train perplexity4.582616329193115
INFO:root:current mean train loss 1930.7148032736793
INFO:root:current train perplexity4.581210613250732
INFO:root:current mean train loss 1930.3502741296581
INFO:root:current train perplexity4.581124782562256
INFO:root:current mean train loss 1930.0476628097638
INFO:root:current train perplexity4.581027507781982

100%|██████████| 1/1 [02:18<00:00, 138.63s/it][A100%|██████████| 1/1 [02:18<00:00, 138.63s/it]
INFO:root:final mean train loss: 1930.062746110494
INFO:root:final train perplexity: 4.582128524780273
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2076.0995444439827
INFO:root:eval perplexity: 5.360340595245361
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/43
 22%|██▏       | 43/200 [1:44:30<6:21:40, 145.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1883.4859619140625
INFO:root:current train perplexity4.563656330108643
INFO:root:current mean train loss 1907.6952608548677
INFO:root:current train perplexity4.5559000968933105
INFO:root:current mean train loss 1910.8356153405232
INFO:root:current train perplexity4.552128314971924
INFO:root:current mean train loss 1917.0853582208806
INFO:root:current train perplexity4.555917263031006
INFO:root:current mean train loss 1918.4710946016532
INFO:root:current train perplexity4.552354335784912
INFO:root:current mean train loss 1919.8617397092423
INFO:root:current train perplexity4.555699825286865
INFO:root:current mean train loss 1919.105391826327
INFO:root:current train perplexity4.55160665512085
INFO:root:current mean train loss 1921.3276332071382
INFO:root:current train perplexity4.552089214324951
INFO:root:current mean train loss 1921.2022382988987
INFO:root:current train perplexity4.552794933319092
INFO:root:current mean train loss 1924.0200871293264
INFO:root:current train perplexity4.55477237701416
INFO:root:current mean train loss 1925.9269910497574
INFO:root:current train perplexity4.558253288269043
INFO:root:current mean train loss 1925.694141921322
INFO:root:current train perplexity4.557795524597168
INFO:root:current mean train loss 1924.2706868489583
INFO:root:current train perplexity4.560744285583496
INFO:root:current mean train loss 1924.1054699431684
INFO:root:current train perplexity4.5580573081970215
INFO:root:current mean train loss 1924.8732726623962
INFO:root:current train perplexity4.5608391761779785
INFO:root:current mean train loss 1926.00516501034
INFO:root:current train perplexity4.564158916473389
INFO:root:current mean train loss 1925.0681261682803
INFO:root:current train perplexity4.563425064086914
INFO:root:current mean train loss 1926.3339235515264
INFO:root:current train perplexity4.563281536102295
INFO:root:current mean train loss 1925.837481322575
INFO:root:current train perplexity4.562918663024902
INFO:root:current mean train loss 1925.1056003076426
INFO:root:current train perplexity4.562172889709473

100%|██████████| 1/1 [02:18<00:00, 138.15s/it][A100%|██████████| 1/1 [02:18<00:00, 138.15s/it]
INFO:root:final mean train loss: 1924.8580206228037
INFO:root:final train perplexity: 4.563358306884766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2074.482593725759
INFO:root:eval perplexity: 5.353334426879883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/44
 22%|██▏       | 44/200 [1:46:56<6:19:06, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1923.0544459566156
INFO:root:current train perplexity4.4932942390441895
INFO:root:current mean train loss 1917.7712078483737
INFO:root:current train perplexity4.502785682678223
INFO:root:current mean train loss 1924.5405431585273
INFO:root:current train perplexity4.507693767547607
INFO:root:current mean train loss 1918.2246100785753
INFO:root:current train perplexity4.508604526519775
INFO:root:current mean train loss 1918.8215312915095
INFO:root:current train perplexity4.512940883636475
INFO:root:current mean train loss 1915.750656546361
INFO:root:current train perplexity4.508680820465088
INFO:root:current mean train loss 1914.2321747156348
INFO:root:current train perplexity4.513434886932373
INFO:root:current mean train loss 1916.0811862358287
INFO:root:current train perplexity4.516848564147949
INFO:root:current mean train loss 1918.2384034644333
INFO:root:current train perplexity4.52650260925293
INFO:root:current mean train loss 1916.958170615884
INFO:root:current train perplexity4.529346942901611
INFO:root:current mean train loss 1917.7444951768819
INFO:root:current train perplexity4.532645225524902
INFO:root:current mean train loss 1918.4585187624305
INFO:root:current train perplexity4.531163692474365
INFO:root:current mean train loss 1918.774287097628
INFO:root:current train perplexity4.532498836517334
INFO:root:current mean train loss 1919.35761445066
INFO:root:current train perplexity4.535745143890381
INFO:root:current mean train loss 1921.1632009214916
INFO:root:current train perplexity4.540478706359863
INFO:root:current mean train loss 1920.981007690035
INFO:root:current train perplexity4.541402339935303
INFO:root:current mean train loss 1921.082481657583
INFO:root:current train perplexity4.542228698730469
INFO:root:current mean train loss 1919.9290453556544
INFO:root:current train perplexity4.54102087020874
INFO:root:current mean train loss 1919.3335203491938
INFO:root:current train perplexity4.5395426750183105
INFO:root:current mean train loss 1919.738973295254
INFO:root:current train perplexity4.542849063873291

100%|██████████| 1/1 [02:18<00:00, 138.07s/it][A100%|██████████| 1/1 [02:18<00:00, 138.07s/it]
INFO:root:final mean train loss: 1919.202615450322
INFO:root:final train perplexity: 4.5430498123168945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2074.8500275307515
INFO:root:eval perplexity: 5.354925632476807
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/45
 22%|██▎       | 45/200 [1:49:22<6:16:32, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1892.3856201171875
INFO:root:current train perplexity4.408734321594238
INFO:root:current mean train loss 1897.1385014231612
INFO:root:current train perplexity4.46904182434082
INFO:root:current mean train loss 1902.302132808801
INFO:root:current train perplexity4.470302104949951
INFO:root:current mean train loss 1912.6108012775799
INFO:root:current train perplexity4.496781826019287
INFO:root:current mean train loss 1906.0075920368063
INFO:root:current train perplexity4.487406253814697
INFO:root:current mean train loss 1908.3337980229803
INFO:root:current train perplexity4.496280193328857
INFO:root:current mean train loss 1909.1433822447996
INFO:root:current train perplexity4.506589412689209
INFO:root:current mean train loss 1912.6076489193902
INFO:root:current train perplexity4.508308410644531
INFO:root:current mean train loss 1913.7571832162362
INFO:root:current train perplexity4.505634307861328
INFO:root:current mean train loss 1912.1547567913642
INFO:root:current train perplexity4.504006862640381
INFO:root:current mean train loss 1912.0766494865704
INFO:root:current train perplexity4.50486421585083
INFO:root:current mean train loss 1910.6908709863617
INFO:root:current train perplexity4.502774238586426
INFO:root:current mean train loss 1909.4087245313426
INFO:root:current train perplexity4.506233215332031
INFO:root:current mean train loss 1909.0362385948383
INFO:root:current train perplexity4.5090131759643555
INFO:root:current mean train loss 1910.7842029508997
INFO:root:current train perplexity4.51397705078125
INFO:root:current mean train loss 1911.8665325037964
INFO:root:current train perplexity4.514481544494629
INFO:root:current mean train loss 1914.5415831345779
INFO:root:current train perplexity4.519191741943359
INFO:root:current mean train loss 1914.1943468712354
INFO:root:current train perplexity4.520007133483887
INFO:root:current mean train loss 1915.3911864972422
INFO:root:current train perplexity4.522698879241943
INFO:root:current mean train loss 1914.4281030099407
INFO:root:current train perplexity4.522706031799316

100%|██████████| 1/1 [02:18<00:00, 138.11s/it][A100%|██████████| 1/1 [02:18<00:00, 138.11s/it]
INFO:root:final mean train loss: 1913.5163983274335
INFO:root:final train perplexity: 4.522723197937012
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2075.630265905502
INFO:root:eval perplexity: 5.358306407928467
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/46
 23%|██▎       | 46/200 [1:51:47<6:13:58, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1912.0930266203704
INFO:root:current train perplexity4.4785027503967285
INFO:root:current mean train loss 1896.4896220001726
INFO:root:current train perplexity4.473073959350586
INFO:root:current mean train loss 1903.8838881088745
INFO:root:current train perplexity4.484894275665283
INFO:root:current mean train loss 1898.7129002368356
INFO:root:current train perplexity4.474732875823975
INFO:root:current mean train loss 1900.0614254767088
INFO:root:current train perplexity4.475744724273682
INFO:root:current mean train loss 1899.0488249734428
INFO:root:current train perplexity4.477725982666016
INFO:root:current mean train loss 1898.3881826974923
INFO:root:current train perplexity4.473300933837891
INFO:root:current mean train loss 1900.237938015165
INFO:root:current train perplexity4.476468563079834
INFO:root:current mean train loss 1901.9077839845968
INFO:root:current train perplexity4.484784126281738
INFO:root:current mean train loss 1903.8880818062723
INFO:root:current train perplexity4.489136219024658
INFO:root:current mean train loss 1905.3107278913837
INFO:root:current train perplexity4.493745803833008
INFO:root:current mean train loss 1907.88988151841
INFO:root:current train perplexity4.4957990646362305
INFO:root:current mean train loss 1907.9943657070282
INFO:root:current train perplexity4.500041484832764
INFO:root:current mean train loss 1907.5623520306276
INFO:root:current train perplexity4.500034809112549
INFO:root:current mean train loss 1907.4296889836364
INFO:root:current train perplexity4.498935699462891
INFO:root:current mean train loss 1907.3891324375643
INFO:root:current train perplexity4.500412940979004
INFO:root:current mean train loss 1906.9459961954146
INFO:root:current train perplexity4.499058723449707
INFO:root:current mean train loss 1908.0420005494193
INFO:root:current train perplexity4.501907825469971
INFO:root:current mean train loss 1907.8817815542347
INFO:root:current train perplexity4.502145290374756
INFO:root:current mean train loss 1908.6342820885325
INFO:root:current train perplexity4.503698825836182

100%|██████████| 1/1 [02:18<00:00, 138.67s/it][A100%|██████████| 1/1 [02:18<00:00, 138.67s/it]
INFO:root:final mean train loss: 1908.2028010181268
INFO:root:final train perplexity: 4.503809452056885
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2076.11654857879
INFO:root:eval perplexity: 5.360413551330566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/47
 24%|██▎       | 47/200 [1:54:13<6:11:52, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1897.9450123066804
INFO:root:current train perplexity4.474496841430664
INFO:root:current mean train loss 1897.2672914447207
INFO:root:current train perplexity4.460155963897705
INFO:root:current mean train loss 1895.8959059747274
INFO:root:current train perplexity4.460696220397949
INFO:root:current mean train loss 1900.026594459112
INFO:root:current train perplexity4.4685516357421875
INFO:root:current mean train loss 1898.215983563159
INFO:root:current train perplexity4.464324474334717
INFO:root:current mean train loss 1899.0627904783523
INFO:root:current train perplexity4.467922210693359
INFO:root:current mean train loss 1900.5869231565634
INFO:root:current train perplexity4.477015018463135
INFO:root:current mean train loss 1899.9284664909344
INFO:root:current train perplexity4.476500988006592
INFO:root:current mean train loss 1898.3201569894904
INFO:root:current train perplexity4.474492073059082
INFO:root:current mean train loss 1897.9502793428653
INFO:root:current train perplexity4.476752758026123
INFO:root:current mean train loss 1898.4633267651056
INFO:root:current train perplexity4.476525783538818
INFO:root:current mean train loss 1899.1347707197542
INFO:root:current train perplexity4.481044769287109
INFO:root:current mean train loss 1899.559583008565
INFO:root:current train perplexity4.482438087463379
INFO:root:current mean train loss 1900.5552726167125
INFO:root:current train perplexity4.481865406036377
INFO:root:current mean train loss 1901.2545394184433
INFO:root:current train perplexity4.481588840484619
INFO:root:current mean train loss 1902.0024691356139
INFO:root:current train perplexity4.482753276824951
INFO:root:current mean train loss 1902.2825297972338
INFO:root:current train perplexity4.48405647277832
INFO:root:current mean train loss 1903.3291369343758
INFO:root:current train perplexity4.484555721282959
INFO:root:current mean train loss 1903.88790079967
INFO:root:current train perplexity4.488165855407715

100%|██████████| 1/1 [02:18<00:00, 138.09s/it][A100%|██████████| 1/1 [02:18<00:00, 138.09s/it]
INFO:root:final mean train loss: 1903.153788864282
INFO:root:final train perplexity: 4.4859113693237305
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2075.206339085356
INFO:root:eval perplexity: 5.35646915435791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/48
 24%|██▍       | 48/200 [1:56:39<6:09:17, 145.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1879.1338704427083
INFO:root:current train perplexity4.405370712280273
INFO:root:current mean train loss 1885.1454324473505
INFO:root:current train perplexity4.440588474273682
INFO:root:current mean train loss 1890.8586579078851
INFO:root:current train perplexity4.44939661026001
INFO:root:current mean train loss 1892.0727248418898
INFO:root:current train perplexity4.446124076843262
INFO:root:current mean train loss 1899.952616128577
INFO:root:current train perplexity4.450654029846191
INFO:root:current mean train loss 1898.7067093636226
INFO:root:current train perplexity4.450616836547852
INFO:root:current mean train loss 1898.1804492981453
INFO:root:current train perplexity4.454517841339111
INFO:root:current mean train loss 1896.445295939412
INFO:root:current train perplexity4.456967830657959
INFO:root:current mean train loss 1895.3967771939704
INFO:root:current train perplexity4.456321716308594
INFO:root:current mean train loss 1894.4448454309681
INFO:root:current train perplexity4.460620880126953
INFO:root:current mean train loss 1895.0702219394627
INFO:root:current train perplexity4.459219932556152
INFO:root:current mean train loss 1894.1058757970152
INFO:root:current train perplexity4.459170818328857
INFO:root:current mean train loss 1896.3650859817064
INFO:root:current train perplexity4.462363243103027
INFO:root:current mean train loss 1895.7761109790874
INFO:root:current train perplexity4.459864616394043
INFO:root:current mean train loss 1896.8327944698267
INFO:root:current train perplexity4.461547374725342
INFO:root:current mean train loss 1896.9546101485148
INFO:root:current train perplexity4.463183403015137
INFO:root:current mean train loss 1897.3297022391398
INFO:root:current train perplexity4.464573383331299
INFO:root:current mean train loss 1897.5177137191372
INFO:root:current train perplexity4.463537693023682
INFO:root:current mean train loss 1897.455858298898
INFO:root:current train perplexity4.46259880065918
INFO:root:current mean train loss 1897.6404152812909
INFO:root:current train perplexity4.463836669921875

100%|██████████| 1/1 [02:18<00:00, 138.42s/it][A100%|██████████| 1/1 [02:18<00:00, 138.42s/it]
INFO:root:final mean train loss: 1897.2257914502272
INFO:root:final train perplexity: 4.464987277984619
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2075.2993588278478
INFO:root:eval perplexity: 5.356871604919434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/49
 24%|██▍       | 49/200 [1:59:05<6:06:55, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1885.6939125061035
INFO:root:current train perplexity4.373238563537598
INFO:root:current mean train loss 1878.3847720984256
INFO:root:current train perplexity4.412435531616211
INFO:root:current mean train loss 1887.4320189377356
INFO:root:current train perplexity4.430964469909668
INFO:root:current mean train loss 1883.4135131835938
INFO:root:current train perplexity4.423922061920166
INFO:root:current mean train loss 1884.8414558128075
INFO:root:current train perplexity4.422711372375488
INFO:root:current mean train loss 1883.9093416830651
INFO:root:current train perplexity4.426364898681641
INFO:root:current mean train loss 1883.2847056328496
INFO:root:current train perplexity4.422609806060791
INFO:root:current mean train loss 1881.8268439287697
INFO:root:current train perplexity4.421334266662598
INFO:root:current mean train loss 1885.0751058138335
INFO:root:current train perplexity4.426783561706543
INFO:root:current mean train loss 1886.7514053803145
INFO:root:current train perplexity4.43374490737915
INFO:root:current mean train loss 1887.6832465829775
INFO:root:current train perplexity4.436703205108643
INFO:root:current mean train loss 1888.8455570072672
INFO:root:current train perplexity4.4405927658081055
INFO:root:current mean train loss 1888.6562548550692
INFO:root:current train perplexity4.439481258392334
INFO:root:current mean train loss 1890.1232814846096
INFO:root:current train perplexity4.442907810211182
INFO:root:current mean train loss 1890.9374130504757
INFO:root:current train perplexity4.443923473358154
INFO:root:current mean train loss 1891.3681978469729
INFO:root:current train perplexity4.444757461547852
INFO:root:current mean train loss 1891.141056210387
INFO:root:current train perplexity4.444578170776367
INFO:root:current mean train loss 1891.676814336975
INFO:root:current train perplexity4.443041801452637
INFO:root:current mean train loss 1892.7086510804022
INFO:root:current train perplexity4.444491386413574
INFO:root:current mean train loss 1892.8304543820968
INFO:root:current train perplexity4.448147296905518

100%|██████████| 1/1 [02:18<00:00, 138.04s/it][A100%|██████████| 1/1 [02:18<00:00, 138.04s/it]
INFO:root:final mean train loss: 1892.553332722674
INFO:root:final train perplexity: 4.448564052581787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2075.7540503968585
INFO:root:eval perplexity: 5.3588433265686035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/50
 25%|██▌       | 50/200 [2:01:30<6:04:18, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1881.309418347417
INFO:root:current train perplexity4.365755081176758
INFO:root:current mean train loss 1877.603089607802
INFO:root:current train perplexity4.383577823638916
INFO:root:current mean train loss 1870.6903723291605
INFO:root:current train perplexity4.3715033531188965
INFO:root:current mean train loss 1874.5608347370837
INFO:root:current train perplexity4.378116130828857
INFO:root:current mean train loss 1877.1817292551154
INFO:root:current train perplexity4.3810577392578125
INFO:root:current mean train loss 1879.8823195493937
INFO:root:current train perplexity4.391144275665283
INFO:root:current mean train loss 1879.82016221621
INFO:root:current train perplexity4.39609956741333
INFO:root:current mean train loss 1881.2986807931409
INFO:root:current train perplexity4.400518417358398
INFO:root:current mean train loss 1884.1057251120344
INFO:root:current train perplexity4.411837100982666
INFO:root:current mean train loss 1883.0779910956846
INFO:root:current train perplexity4.415513515472412
INFO:root:current mean train loss 1884.634971480465
INFO:root:current train perplexity4.415773391723633
INFO:root:current mean train loss 1883.4419506262238
INFO:root:current train perplexity4.413654327392578
INFO:root:current mean train loss 1882.8240525037218
INFO:root:current train perplexity4.416128158569336
INFO:root:current mean train loss 1884.0299626966155
INFO:root:current train perplexity4.4180779457092285
INFO:root:current mean train loss 1884.1999349126822
INFO:root:current train perplexity4.4188432693481445
INFO:root:current mean train loss 1886.9058815824978
INFO:root:current train perplexity4.425678730010986
INFO:root:current mean train loss 1887.2022984159435
INFO:root:current train perplexity4.427293300628662
INFO:root:current mean train loss 1888.3686369191994
INFO:root:current train perplexity4.430254936218262
INFO:root:current mean train loss 1888.3657556000499
INFO:root:current train perplexity4.432020664215088
INFO:root:current mean train loss 1888.586570148654
INFO:root:current train perplexity4.431562900543213

100%|██████████| 1/1 [02:18<00:00, 138.31s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 1887.880948203775
INFO:root:final train perplexity: 4.432201862335205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2074.1205405377327
INFO:root:eval perplexity: 5.3517680168151855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/51
 26%|██▌       | 51/200 [2:03:56<6:01:58, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1875.1889814897017
INFO:root:current train perplexity4.379797458648682
INFO:root:current mean train loss 1878.5467661662274
INFO:root:current train perplexity4.390320301055908
INFO:root:current mean train loss 1877.198739646969
INFO:root:current train perplexity4.393165588378906
INFO:root:current mean train loss 1877.3403256942665
INFO:root:current train perplexity4.389227867126465
INFO:root:current mean train loss 1875.2951348431632
INFO:root:current train perplexity4.387520790100098
INFO:root:current mean train loss 1878.9081636570368
INFO:root:current train perplexity4.388241291046143
INFO:root:current mean train loss 1874.8641756991367
INFO:root:current train perplexity4.387242794036865
INFO:root:current mean train loss 1877.3328015997267
INFO:root:current train perplexity4.394228458404541
INFO:root:current mean train loss 1878.498157245733
INFO:root:current train perplexity4.397952556610107
INFO:root:current mean train loss 1878.6472044129303
INFO:root:current train perplexity4.401549816131592
INFO:root:current mean train loss 1879.7335147821882
INFO:root:current train perplexity4.4039998054504395
INFO:root:current mean train loss 1878.7090947198622
INFO:root:current train perplexity4.401120662689209
INFO:root:current mean train loss 1877.612062475316
INFO:root:current train perplexity4.398075580596924
INFO:root:current mean train loss 1877.699465839615
INFO:root:current train perplexity4.401684284210205
INFO:root:current mean train loss 1879.5358643577335
INFO:root:current train perplexity4.406708240509033
INFO:root:current mean train loss 1879.832277962988
INFO:root:current train perplexity4.408874988555908
INFO:root:current mean train loss 1879.7651229437088
INFO:root:current train perplexity4.409437656402588
INFO:root:current mean train loss 1879.7621858106463
INFO:root:current train perplexity4.409361362457275
INFO:root:current mean train loss 1881.6170233003836
INFO:root:current train perplexity4.412608623504639
INFO:root:current mean train loss 1882.6727080708968
INFO:root:current train perplexity4.413508892059326

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1882.7730003430033
INFO:root:final train perplexity: 4.4143829345703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2074.6750609485816
INFO:root:eval perplexity: 5.35416841506958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/52
 26%|██▌       | 52/200 [2:06:22<5:59:39, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1876.7522899214043
INFO:root:current train perplexity4.371101379394531
INFO:root:current mean train loss 1869.6685110623719
INFO:root:current train perplexity4.376649379730225
INFO:root:current mean train loss 1875.2131541760987
INFO:root:current train perplexity4.390720367431641
INFO:root:current mean train loss 1872.2398060133812
INFO:root:current train perplexity4.378566265106201
INFO:root:current mean train loss 1876.3017423957524
INFO:root:current train perplexity4.377188205718994
INFO:root:current mean train loss 1878.2837318171769
INFO:root:current train perplexity4.37506628036499
INFO:root:current mean train loss 1878.5159116775715
INFO:root:current train perplexity4.378190040588379
INFO:root:current mean train loss 1878.409369418752
INFO:root:current train perplexity4.377655029296875
INFO:root:current mean train loss 1878.598833820693
INFO:root:current train perplexity4.378077983856201
INFO:root:current mean train loss 1878.3516065843958
INFO:root:current train perplexity4.379361152648926
INFO:root:current mean train loss 1878.5171394383367
INFO:root:current train perplexity4.384143829345703
INFO:root:current mean train loss 1878.5338335980425
INFO:root:current train perplexity4.385848522186279
INFO:root:current mean train loss 1879.4067882320794
INFO:root:current train perplexity4.388695240020752
INFO:root:current mean train loss 1879.251559905013
INFO:root:current train perplexity4.388347148895264
INFO:root:current mean train loss 1878.9510177848954
INFO:root:current train perplexity4.389832019805908
INFO:root:current mean train loss 1878.965322777657
INFO:root:current train perplexity4.391562461853027
INFO:root:current mean train loss 1878.892698382028
INFO:root:current train perplexity4.392381191253662
INFO:root:current mean train loss 1877.680710959956
INFO:root:current train perplexity4.39324426651001
INFO:root:current mean train loss 1878.1909428625365
INFO:root:current train perplexity4.396988868713379
INFO:root:current mean train loss 1877.8895865713052
INFO:root:current train perplexity4.397414207458496

100%|██████████| 1/1 [02:18<00:00, 138.40s/it][A100%|██████████| 1/1 [02:18<00:00, 138.40s/it]
INFO:root:final mean train loss: 1877.8895865713052
INFO:root:final train perplexity: 4.397414207458496
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2075.83379737367
INFO:root:eval perplexity: 5.359188079833984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/53
 26%|██▋       | 53/200 [2:08:48<5:57:17, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1876.4244152832032
INFO:root:current train perplexity4.36704683303833
INFO:root:current mean train loss 1873.0876776123048
INFO:root:current train perplexity4.366699695587158
INFO:root:current mean train loss 1875.6941349283854
INFO:root:current train perplexity4.369365692138672
INFO:root:current mean train loss 1875.340135192871
INFO:root:current train perplexity4.364213466644287
INFO:root:current mean train loss 1871.1183908691405
INFO:root:current train perplexity4.362115859985352
INFO:root:current mean train loss 1872.9342045084636
INFO:root:current train perplexity4.358339309692383
INFO:root:current mean train loss 1872.1332632882254
INFO:root:current train perplexity4.3612141609191895
INFO:root:current mean train loss 1870.4353730773926
INFO:root:current train perplexity4.3593058586120605
INFO:root:current mean train loss 1872.618771158854
INFO:root:current train perplexity4.362837314605713
INFO:root:current mean train loss 1872.884368408203
INFO:root:current train perplexity4.3656110763549805
INFO:root:current mean train loss 1871.7562756347656
INFO:root:current train perplexity4.369060516357422
INFO:root:current mean train loss 1871.2322032674153
INFO:root:current train perplexity4.367618560791016
INFO:root:current mean train loss 1871.1788738544171
INFO:root:current train perplexity4.369416236877441
INFO:root:current mean train loss 1871.0634403773718
INFO:root:current train perplexity4.370378017425537
INFO:root:current mean train loss 1869.9942025553385
INFO:root:current train perplexity4.370680332183838
INFO:root:current mean train loss 1871.4828274536133
INFO:root:current train perplexity4.373387336730957
INFO:root:current mean train loss 1872.1379255227482
INFO:root:current train perplexity4.376108169555664
INFO:root:current mean train loss 1872.6435342068141
INFO:root:current train perplexity4.3775954246521
INFO:root:current mean train loss 1873.7163248483757
INFO:root:current train perplexity4.379586219787598

100%|██████████| 1/1 [02:18<00:00, 138.45s/it][A100%|██████████| 1/1 [02:18<00:00, 138.45s/it]
INFO:root:final mean train loss: 1873.6818345339684
INFO:root:final train perplexity: 4.382845878601074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2075.5058875117743
INFO:root:eval perplexity: 5.357767581939697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/54
 27%|██▋       | 54/200 [2:11:14<5:54:54, 145.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1850.7469985064338
INFO:root:current train perplexity4.273805141448975
INFO:root:current mean train loss 1867.3854187533386
INFO:root:current train perplexity4.340679168701172
INFO:root:current mean train loss 1862.025117232503
INFO:root:current train perplexity4.353854656219482
INFO:root:current mean train loss 1862.4252602369627
INFO:root:current train perplexity4.351562023162842
INFO:root:current mean train loss 1863.3336945677834
INFO:root:current train perplexity4.3503642082214355
INFO:root:current mean train loss 1862.5660549141683
INFO:root:current train perplexity4.350393772125244
INFO:root:current mean train loss 1861.9115089156833
INFO:root:current train perplexity4.343233585357666
INFO:root:current mean train loss 1863.447400634425
INFO:root:current train perplexity4.341065883636475
INFO:root:current mean train loss 1864.970034950658
INFO:root:current train perplexity4.342765808105469
INFO:root:current mean train loss 1866.3492391704863
INFO:root:current train perplexity4.341395854949951
INFO:root:current mean train loss 1867.250069857347
INFO:root:current train perplexity4.349587917327881
INFO:root:current mean train loss 1868.0878059298414
INFO:root:current train perplexity4.348945140838623
INFO:root:current mean train loss 1867.6926529319344
INFO:root:current train perplexity4.349186420440674
INFO:root:current mean train loss 1868.2726521531831
INFO:root:current train perplexity4.352510929107666
INFO:root:current mean train loss 1867.6799798829504
INFO:root:current train perplexity4.353333473205566
INFO:root:current mean train loss 1866.8915196458368
INFO:root:current train perplexity4.354896068572998
INFO:root:current mean train loss 1867.3598609410028
INFO:root:current train perplexity4.357885837554932
INFO:root:current mean train loss 1868.1003693106845
INFO:root:current train perplexity4.360815525054932
INFO:root:current mean train loss 1868.525293680883
INFO:root:current train perplexity4.362768650054932
INFO:root:current mean train loss 1868.231338715889
INFO:root:current train perplexity4.36257791519165

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1868.3571605903599
INFO:root:final train perplexity: 4.364479064941406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2075.855846215647
INFO:root:eval perplexity: 5.359283924102783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/55
 28%|██▊       | 55/200 [2:13:40<5:52:29, 145.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1850.4522848690258
INFO:root:current train perplexity4.347081661224365
INFO:root:current mean train loss 1865.6083364913713
INFO:root:current train perplexity4.3169732093811035
INFO:root:current mean train loss 1856.1477755033052
INFO:root:current train perplexity4.303217887878418
INFO:root:current mean train loss 1852.3650677161302
INFO:root:current train perplexity4.301858425140381
INFO:root:current mean train loss 1855.6730377619167
INFO:root:current train perplexity4.313368797302246
INFO:root:current mean train loss 1855.9793884048747
INFO:root:current train perplexity4.320089817047119
INFO:root:current mean train loss 1856.8149899263112
INFO:root:current train perplexity4.325418949127197
INFO:root:current mean train loss 1858.362218872403
INFO:root:current train perplexity4.335410118103027
INFO:root:current mean train loss 1859.5439865880733
INFO:root:current train perplexity4.337916851043701
INFO:root:current mean train loss 1860.3262382687033
INFO:root:current train perplexity4.338751316070557
INFO:root:current mean train loss 1859.9525852461618
INFO:root:current train perplexity4.340241432189941
INFO:root:current mean train loss 1859.3682036761463
INFO:root:current train perplexity4.338838577270508
INFO:root:current mean train loss 1860.1642232489933
INFO:root:current train perplexity4.3382368087768555
INFO:root:current mean train loss 1860.3854295081462
INFO:root:current train perplexity4.334860324859619
INFO:root:current mean train loss 1862.0378392431028
INFO:root:current train perplexity4.341724872589111
INFO:root:current mean train loss 1862.4609208685167
INFO:root:current train perplexity4.344119548797607
INFO:root:current mean train loss 1862.1205220216636
INFO:root:current train perplexity4.3440141677856445
INFO:root:current mean train loss 1862.62827463821
INFO:root:current train perplexity4.3460373878479
INFO:root:current mean train loss 1863.7051824904581
INFO:root:current train perplexity4.348240375518799
INFO:root:current mean train loss 1864.0363812451526
INFO:root:current train perplexity4.348976135253906

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1864.090280537646
INFO:root:final train perplexity: 4.349816799163818
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2076.6427538716202
INFO:root:eval perplexity: 5.362695217132568
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/56
 28%|██▊       | 56/200 [2:16:05<5:49:54, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1815.8301283892463
INFO:root:current train perplexity4.2639007568359375
INFO:root:current mean train loss 1836.363356432378
INFO:root:current train perplexity4.2799072265625
INFO:root:current mean train loss 1840.0935394165526
INFO:root:current train perplexity4.276663780212402
INFO:root:current mean train loss 1847.4704019486735
INFO:root:current train perplexity4.289910316467285
INFO:root:current mean train loss 1846.415169252789
INFO:root:current train perplexity4.295751094818115
INFO:root:current mean train loss 1847.860666153868
INFO:root:current train perplexity4.298058032989502
INFO:root:current mean train loss 1850.7491181310604
INFO:root:current train perplexity4.301555633544922
INFO:root:current mean train loss 1852.3215596977468
INFO:root:current train perplexity4.304039001464844
INFO:root:current mean train loss 1850.736811098845
INFO:root:current train perplexity4.307093143463135
INFO:root:current mean train loss 1851.0917217844292
INFO:root:current train perplexity4.306295394897461
INFO:root:current mean train loss 1853.7199337684349
INFO:root:current train perplexity4.309045314788818
INFO:root:current mean train loss 1854.9934190208244
INFO:root:current train perplexity4.311985969543457
INFO:root:current mean train loss 1855.1115797791454
INFO:root:current train perplexity4.314911842346191
INFO:root:current mean train loss 1855.208509737434
INFO:root:current train perplexity4.316980361938477
INFO:root:current mean train loss 1856.2600252452512
INFO:root:current train perplexity4.320817947387695
INFO:root:current mean train loss 1856.7617821856363
INFO:root:current train perplexity4.321664810180664
INFO:root:current mean train loss 1857.8231440728393
INFO:root:current train perplexity4.323855876922607
INFO:root:current mean train loss 1858.5385025521132
INFO:root:current train perplexity4.327422618865967
INFO:root:current mean train loss 1858.556308047698
INFO:root:current train perplexity4.328658580780029
INFO:root:current mean train loss 1859.173958141458
INFO:root:current train perplexity4.330668926239014

100%|██████████| 1/1 [02:17<00:00, 137.61s/it][A100%|██████████| 1/1 [02:17<00:00, 137.61s/it]
INFO:root:final mean train loss: 1858.9648079537887
INFO:root:final train perplexity: 4.33227014541626
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2076.705615753823
INFO:root:eval perplexity: 5.362968444824219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/57
 28%|██▊       | 57/200 [2:18:31<5:47:00, 145.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1849.572468477137
INFO:root:current train perplexity4.259382247924805
INFO:root:current mean train loss 1858.1028812953405
INFO:root:current train perplexity4.268106937408447
INFO:root:current mean train loss 1852.7129261529267
INFO:root:current train perplexity4.283710956573486
INFO:root:current mean train loss 1850.6708483488662
INFO:root:current train perplexity4.284689426422119
INFO:root:current mean train loss 1850.2447775816306
INFO:root:current train perplexity4.2901201248168945
INFO:root:current mean train loss 1848.423990383954
INFO:root:current train perplexity4.289900302886963
INFO:root:current mean train loss 1848.8588481606123
INFO:root:current train perplexity4.287329196929932
INFO:root:current mean train loss 1851.3967943191528
INFO:root:current train perplexity4.290913105010986
INFO:root:current mean train loss 1850.426293860932
INFO:root:current train perplexity4.292117118835449
INFO:root:current mean train loss 1851.0157639684755
INFO:root:current train perplexity4.295304775238037
INFO:root:current mean train loss 1850.8798921849398
INFO:root:current train perplexity4.296998023986816
INFO:root:current mean train loss 1850.8985665726334
INFO:root:current train perplexity4.299703121185303
INFO:root:current mean train loss 1851.8710529315358
INFO:root:current train perplexity4.302271366119385
INFO:root:current mean train loss 1853.2731549893206
INFO:root:current train perplexity4.306950569152832
INFO:root:current mean train loss 1853.7323190132993
INFO:root:current train perplexity4.308078765869141
INFO:root:current mean train loss 1854.907774399738
INFO:root:current train perplexity4.31096887588501
INFO:root:current mean train loss 1854.041035384583
INFO:root:current train perplexity4.312686920166016
INFO:root:current mean train loss 1854.461401132437
INFO:root:current train perplexity4.313209056854248
INFO:root:current mean train loss 1855.1741389207207
INFO:root:current train perplexity4.3156328201293945
INFO:root:current mean train loss 1854.9039549168533
INFO:root:current train perplexity4.316497325897217

100%|██████████| 1/1 [02:18<00:00, 138.30s/it][A100%|██████████| 1/1 [02:18<00:00, 138.30s/it]
INFO:root:final mean train loss: 1854.2583106305945
INFO:root:final train perplexity: 4.316219329833984
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2077.1839235995676
INFO:root:eval perplexity: 5.3650431632995605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/58
 29%|██▉       | 58/200 [2:20:56<5:44:43, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1849.1395565257353
INFO:root:current train perplexity4.244441032409668
INFO:root:current mean train loss 1842.5292124155405
INFO:root:current train perplexity4.248640537261963
INFO:root:current mean train loss 1838.2398947197094
INFO:root:current train perplexity4.25811243057251
INFO:root:current mean train loss 1843.3661814503855
INFO:root:current train perplexity4.267736911773682
INFO:root:current mean train loss 1840.1908298767719
INFO:root:current train perplexity4.263497352600098
INFO:root:current mean train loss 1842.2904904096554
INFO:root:current train perplexity4.272418975830078
INFO:root:current mean train loss 1847.6678242258781
INFO:root:current train perplexity4.287031173706055
INFO:root:current mean train loss 1847.3938280627985
INFO:root:current train perplexity4.292964935302734
INFO:root:current mean train loss 1847.294128486935
INFO:root:current train perplexity4.294106960296631
INFO:root:current mean train loss 1847.5735064046637
INFO:root:current train perplexity4.297929286956787
INFO:root:current mean train loss 1847.2570317000288
INFO:root:current train perplexity4.298671722412109
INFO:root:current mean train loss 1847.5901044757054
INFO:root:current train perplexity4.301546573638916
INFO:root:current mean train loss 1847.4122440798274
INFO:root:current train perplexity4.302316665649414
INFO:root:current mean train loss 1849.1738103212433
INFO:root:current train perplexity4.304065704345703
INFO:root:current mean train loss 1850.0634229666457
INFO:root:current train perplexity4.30643892288208
INFO:root:current mean train loss 1848.6955903736198
INFO:root:current train perplexity4.30563497543335
INFO:root:current mean train loss 1849.8998553992026
INFO:root:current train perplexity4.305365562438965
INFO:root:current mean train loss 1851.3721680371368
INFO:root:current train perplexity4.305492401123047
INFO:root:current mean train loss 1851.3979403467963
INFO:root:current train perplexity4.305293083190918

100%|██████████| 1/1 [02:18<00:00, 138.14s/it][A100%|██████████| 1/1 [02:18<00:00, 138.14s/it]
INFO:root:final mean train loss: 1850.7843450025903
INFO:root:final train perplexity: 4.304408550262451
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2077.528922439467
INFO:root:eval perplexity: 5.366539478302002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/59
 30%|██▉       | 59/200 [2:23:22<5:42:14, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1943.8109130859375
INFO:root:current train perplexity4.286596298217773
INFO:root:current mean train loss 1836.7396862553614
INFO:root:current train perplexity4.258439064025879
INFO:root:current mean train loss 1844.945546367381
INFO:root:current train perplexity4.2616963386535645
INFO:root:current mean train loss 1837.117770365532
INFO:root:current train perplexity4.256426811218262
INFO:root:current mean train loss 1838.5418883366372
INFO:root:current train perplexity4.264193058013916
INFO:root:current mean train loss 1836.6939339808735
INFO:root:current train perplexity4.260580062866211
INFO:root:current mean train loss 1838.5518748134473
INFO:root:current train perplexity4.270873546600342
INFO:root:current mean train loss 1840.103065251625
INFO:root:current train perplexity4.268964767456055
INFO:root:current mean train loss 1842.1997123585081
INFO:root:current train perplexity4.2715067863464355
INFO:root:current mean train loss 1841.01512981677
INFO:root:current train perplexity4.272940158843994
INFO:root:current mean train loss 1842.3425681595793
INFO:root:current train perplexity4.275862216949463
INFO:root:current mean train loss 1843.6235483380715
INFO:root:current train perplexity4.275003433227539
INFO:root:current mean train loss 1845.4688252529963
INFO:root:current train perplexity4.2773613929748535
INFO:root:current mean train loss 1845.328128843996
INFO:root:current train perplexity4.280045986175537
INFO:root:current mean train loss 1845.465396549154
INFO:root:current train perplexity4.282309055328369
INFO:root:current mean train loss 1845.540126104647
INFO:root:current train perplexity4.282918453216553
INFO:root:current mean train loss 1845.3821884326721
INFO:root:current train perplexity4.283449172973633
INFO:root:current mean train loss 1844.8011864775356
INFO:root:current train perplexity4.280998706817627
INFO:root:current mean train loss 1846.5088742813973
INFO:root:current train perplexity4.284704208374023
INFO:root:current mean train loss 1846.1148507071093
INFO:root:current train perplexity4.283863067626953

100%|██████████| 1/1 [02:18<00:00, 138.22s/it][A100%|██████████| 1/1 [02:18<00:00, 138.22s/it]
INFO:root:final mean train loss: 1845.4014614641937
INFO:root:final train perplexity: 4.286174774169922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.09s/it][A100%|██████████| 1/1 [00:07<00:00,  7.09s/it]
INFO:root:eval mean loss: 2078.8796382390015
INFO:root:eval perplexity: 5.372406005859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/60
 30%|███       | 60/200 [2:25:47<5:39:45, 145.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1834.8578009354442
INFO:root:current train perplexity4.158044338226318
INFO:root:current mean train loss 1822.472935267857
INFO:root:current train perplexity4.208315372467041
INFO:root:current mean train loss 1817.6414649998217
INFO:root:current train perplexity4.232365131378174
INFO:root:current mean train loss 1826.6366552275176
INFO:root:current train perplexity4.239248752593994
INFO:root:current mean train loss 1829.1228848914827
INFO:root:current train perplexity4.252171039581299
INFO:root:current mean train loss 1829.9830806783627
INFO:root:current train perplexity4.252582550048828
INFO:root:current mean train loss 1834.2546406439317
INFO:root:current train perplexity4.256298542022705
INFO:root:current mean train loss 1836.1279442883997
INFO:root:current train perplexity4.2619147300720215
INFO:root:current mean train loss 1836.5405453785581
INFO:root:current train perplexity4.261534690856934
INFO:root:current mean train loss 1837.6177524238728
INFO:root:current train perplexity4.260380268096924
INFO:root:current mean train loss 1839.302302636623
INFO:root:current train perplexity4.263131618499756
INFO:root:current mean train loss 1839.6232150898543
INFO:root:current train perplexity4.263613224029541
INFO:root:current mean train loss 1839.763730156314
INFO:root:current train perplexity4.2646989822387695
INFO:root:current mean train loss 1840.357098791254
INFO:root:current train perplexity4.2669453620910645
INFO:root:current mean train loss 1839.7677789947531
INFO:root:current train perplexity4.267465591430664
INFO:root:current mean train loss 1839.7558527049302
INFO:root:current train perplexity4.26859712600708
INFO:root:current mean train loss 1840.0876175313176
INFO:root:current train perplexity4.2684149742126465
INFO:root:current mean train loss 1840.9773385945
INFO:root:current train perplexity4.2689595222473145
INFO:root:current mean train loss 1841.0912503435954
INFO:root:current train perplexity4.270495414733887
INFO:root:current mean train loss 1841.47180029475
INFO:root:current train perplexity4.272076606750488

100%|██████████| 1/1 [02:18<00:00, 138.16s/it][A100%|██████████| 1/1 [02:18<00:00, 138.16s/it]
INFO:root:final mean train loss: 1841.6568155678247
INFO:root:final train perplexity: 4.273534297943115
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2078.9269348577404
INFO:root:eval perplexity: 5.372611045837402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/61
 30%|███       | 61/200 [2:28:13<5:37:19, 145.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1838.8638543023003
INFO:root:current train perplexity4.252657890319824
INFO:root:current mean train loss 1824.4606484805836
INFO:root:current train perplexity4.219502925872803
INFO:root:current mean train loss 1832.4658285884534
INFO:root:current train perplexity4.2379374504089355
INFO:root:current mean train loss 1836.8266194661458
INFO:root:current train perplexity4.230548858642578
INFO:root:current mean train loss 1830.754622153186
INFO:root:current train perplexity4.221724987030029
INFO:root:current mean train loss 1831.754799458518
INFO:root:current train perplexity4.232213020324707
INFO:root:current mean train loss 1831.430986512382
INFO:root:current train perplexity4.232903003692627
INFO:root:current mean train loss 1832.0299024167268
INFO:root:current train perplexity4.237383842468262
INFO:root:current mean train loss 1833.1094582297585
INFO:root:current train perplexity4.2405548095703125
INFO:root:current mean train loss 1832.455822284405
INFO:root:current train perplexity4.240787506103516
INFO:root:current mean train loss 1834.539728584437
INFO:root:current train perplexity4.24449348449707
INFO:root:current mean train loss 1834.7211763623734
INFO:root:current train perplexity4.24457311630249
INFO:root:current mean train loss 1834.6421638167792
INFO:root:current train perplexity4.247836112976074
INFO:root:current mean train loss 1834.767058777952
INFO:root:current train perplexity4.2487993240356445
INFO:root:current mean train loss 1835.0901826736322
INFO:root:current train perplexity4.250334739685059
INFO:root:current mean train loss 1836.4194162686665
INFO:root:current train perplexity4.253176689147949
INFO:root:current mean train loss 1838.0258213033885
INFO:root:current train perplexity4.255411148071289
INFO:root:current mean train loss 1838.5967751076694
INFO:root:current train perplexity4.25806999206543
INFO:root:current mean train loss 1838.006538341248
INFO:root:current train perplexity4.259735584259033
INFO:root:current mean train loss 1837.9605333312484
INFO:root:current train perplexity4.259016036987305

100%|██████████| 1/1 [02:18<00:00, 138.49s/it][A100%|██████████| 1/1 [02:18<00:00, 138.49s/it]
INFO:root:final mean train loss: 1837.8129798169696
INFO:root:final train perplexity: 4.2606000900268555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2079.261632608184
INFO:root:eval perplexity: 5.374064922332764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/62
 31%|███       | 62/200 [2:30:39<5:35:06, 145.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1802.3796294590213
INFO:root:current train perplexity4.167329788208008
INFO:root:current mean train loss 1816.1024297577103
INFO:root:current train perplexity4.207700252532959
INFO:root:current mean train loss 1818.899237953156
INFO:root:current train perplexity4.220019817352295
INFO:root:current mean train loss 1822.4525367801655
INFO:root:current train perplexity4.216700553894043
INFO:root:current mean train loss 1823.0286808645487
INFO:root:current train perplexity4.218762397766113
INFO:root:current mean train loss 1823.6239938580752
INFO:root:current train perplexity4.216805934906006
INFO:root:current mean train loss 1824.211010779575
INFO:root:current train perplexity4.219212055206299
INFO:root:current mean train loss 1823.5914815672206
INFO:root:current train perplexity4.2176666259765625
INFO:root:current mean train loss 1826.3144595648173
INFO:root:current train perplexity4.224726676940918
INFO:root:current mean train loss 1828.6304042692075
INFO:root:current train perplexity4.227723598480225
INFO:root:current mean train loss 1830.0564448256098
INFO:root:current train perplexity4.227182865142822
INFO:root:current mean train loss 1831.4809560784029
INFO:root:current train perplexity4.233042240142822
INFO:root:current mean train loss 1832.7794051113265
INFO:root:current train perplexity4.239109992980957
INFO:root:current mean train loss 1833.267030658144
INFO:root:current train perplexity4.242648601531982
INFO:root:current mean train loss 1832.2337257506022
INFO:root:current train perplexity4.240047931671143
INFO:root:current mean train loss 1833.0328313332561
INFO:root:current train perplexity4.240603923797607
INFO:root:current mean train loss 1833.9144108397848
INFO:root:current train perplexity4.24016809463501
INFO:root:current mean train loss 1834.5874070789362
INFO:root:current train perplexity4.242734909057617
INFO:root:current mean train loss 1834.2047140880077
INFO:root:current train perplexity4.244049549102783
INFO:root:current mean train loss 1834.0620860985102
INFO:root:current train perplexity4.245824337005615

100%|██████████| 1/1 [02:18<00:00, 138.33s/it][A100%|██████████| 1/1 [02:18<00:00, 138.33s/it]
INFO:root:final mean train loss: 1833.6081034496344
INFO:root:final train perplexity: 4.246493816375732
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2078.9844996675533
INFO:root:eval perplexity: 5.372861385345459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/63
 32%|███▏      | 63/200 [2:33:05<5:32:46, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1833.8265084402901
INFO:root:current train perplexity4.215409278869629
INFO:root:current mean train loss 1832.8768877814798
INFO:root:current train perplexity4.214674949645996
INFO:root:current mean train loss 1828.34661593967
INFO:root:current train perplexity4.211060047149658
INFO:root:current mean train loss 1825.8594465925887
INFO:root:current train perplexity4.210355281829834
INFO:root:current mean train loss 1824.2514983481549
INFO:root:current train perplexity4.211434841156006
INFO:root:current mean train loss 1821.5104340135006
INFO:root:current train perplexity4.206779479980469
INFO:root:current mean train loss 1821.799142774895
INFO:root:current train perplexity4.212316513061523
INFO:root:current mean train loss 1824.1003435407365
INFO:root:current train perplexity4.216892242431641
INFO:root:current mean train loss 1824.8321822243174
INFO:root:current train perplexity4.218766212463379
INFO:root:current mean train loss 1825.7121962085212
INFO:root:current train perplexity4.219994068145752
INFO:root:current mean train loss 1825.9008034964588
INFO:root:current train perplexity4.2195258140563965
INFO:root:current mean train loss 1824.9973904079861
INFO:root:current train perplexity4.218555450439453
INFO:root:current mean train loss 1826.0622583584523
INFO:root:current train perplexity4.221339225769043
INFO:root:current mean train loss 1826.8685505887888
INFO:root:current train perplexity4.2207465171813965
INFO:root:current mean train loss 1828.6609524473852
INFO:root:current train perplexity4.223524570465088
INFO:root:current mean train loss 1828.8267200251294
INFO:root:current train perplexity4.224455833435059
INFO:root:current mean train loss 1829.0191540015671
INFO:root:current train perplexity4.224844455718994
INFO:root:current mean train loss 1828.9390014648438
INFO:root:current train perplexity4.22625732421875
INFO:root:current mean train loss 1829.0353527375084
INFO:root:current train perplexity4.227383136749268
INFO:root:current mean train loss 1828.9525664508644
INFO:root:current train perplexity4.230127811431885

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1828.9141679187646
INFO:root:final train perplexity: 4.230802536010742
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2080.4622378518397
INFO:root:eval perplexity: 5.3792853355407715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/64
 32%|███▏      | 64/200 [2:35:30<5:30:16, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1799.284916318696
INFO:root:current train perplexity4.160560607910156
INFO:root:current mean train loss 1815.759928820605
INFO:root:current train perplexity4.172607898712158
INFO:root:current mean train loss 1812.030175185785
INFO:root:current train perplexity4.176567554473877
INFO:root:current mean train loss 1820.9886260118903
INFO:root:current train perplexity4.1981587409973145
INFO:root:current mean train loss 1822.6415764967273
INFO:root:current train perplexity4.202374458312988
INFO:root:current mean train loss 1822.1149817081691
INFO:root:current train perplexity4.200655460357666
INFO:root:current mean train loss 1820.2399157839225
INFO:root:current train perplexity4.2010416984558105
INFO:root:current mean train loss 1821.359595253931
INFO:root:current train perplexity4.203250408172607
INFO:root:current mean train loss 1824.024656964531
INFO:root:current train perplexity4.2071404457092285
INFO:root:current mean train loss 1825.321450215101
INFO:root:current train perplexity4.208605766296387
INFO:root:current mean train loss 1824.866448789314
INFO:root:current train perplexity4.210180759429932
INFO:root:current mean train loss 1824.9578060416886
INFO:root:current train perplexity4.208712100982666
INFO:root:current mean train loss 1825.8984203323803
INFO:root:current train perplexity4.209819316864014
INFO:root:current mean train loss 1825.3572404857325
INFO:root:current train perplexity4.211753845214844
INFO:root:current mean train loss 1825.380889174276
INFO:root:current train perplexity4.213174343109131
INFO:root:current mean train loss 1825.3250868568348
INFO:root:current train perplexity4.216150760650635
INFO:root:current mean train loss 1825.2178541905935
INFO:root:current train perplexity4.2157697677612305
INFO:root:current mean train loss 1825.2471192909074
INFO:root:current train perplexity4.216196537017822
INFO:root:current mean train loss 1824.801295860141
INFO:root:current train perplexity4.2158074378967285

100%|██████████| 1/1 [02:18<00:00, 138.81s/it][A100%|██████████| 1/1 [02:18<00:00, 138.81s/it]
INFO:root:final mean train loss: 1825.147481436448
INFO:root:final train perplexity: 4.218253135681152
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2080.898750900377
INFO:root:eval perplexity: 5.381185054779053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/65
 32%|███▎      | 65/200 [2:37:57<5:28:16, 145.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1853.1834716796875
INFO:root:current train perplexity4.121699810028076
INFO:root:current mean train loss 1825.2369208702673
INFO:root:current train perplexity4.226777076721191
INFO:root:current mean train loss 1824.2886759440105
INFO:root:current train perplexity4.201560020446777
INFO:root:current mean train loss 1815.7394995438426
INFO:root:current train perplexity4.203305721282959
INFO:root:current mean train loss 1819.258637381072
INFO:root:current train perplexity4.207694053649902
INFO:root:current mean train loss 1819.7213980054098
INFO:root:current train perplexity4.210651874542236
INFO:root:current mean train loss 1817.7744144667063
INFO:root:current train perplexity4.2020416259765625
INFO:root:current mean train loss 1818.8619950034401
INFO:root:current train perplexity4.198724746704102
INFO:root:current mean train loss 1816.7908528645833
INFO:root:current train perplexity4.196417808532715
INFO:root:current mean train loss 1816.696505521251
INFO:root:current train perplexity4.196627616882324
INFO:root:current mean train loss 1816.9320839201787
INFO:root:current train perplexity4.196236610412598
INFO:root:current mean train loss 1818.2102633490078
INFO:root:current train perplexity4.201352119445801
INFO:root:current mean train loss 1817.9591961730755
INFO:root:current train perplexity4.198350429534912
INFO:root:current mean train loss 1819.6633913941178
INFO:root:current train perplexity4.197892189025879
INFO:root:current mean train loss 1820.5457194184305
INFO:root:current train perplexity4.200639247894287
INFO:root:current mean train loss 1821.1085390943163
INFO:root:current train perplexity4.200706481933594
INFO:root:current mean train loss 1820.475409681363
INFO:root:current train perplexity4.198930740356445
INFO:root:current mean train loss 1821.0646373766688
INFO:root:current train perplexity4.202116012573242
INFO:root:current mean train loss 1822.0361524357763
INFO:root:current train perplexity4.204170227050781
INFO:root:current mean train loss 1822.5165959847075
INFO:root:current train perplexity4.205690383911133

100%|██████████| 1/1 [02:18<00:00, 138.03s/it][A100%|██████████| 1/1 [02:18<00:00, 138.03s/it]
INFO:root:final mean train loss: 1821.1429488912593
INFO:root:final train perplexity: 4.204952239990234
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2082.3504036977783
INFO:root:eval perplexity: 5.38750696182251
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/66
 33%|███▎      | 66/200 [2:40:22<5:25:31, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1812.907970610119
INFO:root:current train perplexity4.15598201751709
INFO:root:current mean train loss 1803.8048125968492
INFO:root:current train perplexity4.144647121429443
INFO:root:current mean train loss 1806.0224294533018
INFO:root:current train perplexity4.161116600036621
INFO:root:current mean train loss 1808.9775637807875
INFO:root:current train perplexity4.163989543914795
INFO:root:current mean train loss 1809.641405264159
INFO:root:current train perplexity4.163073539733887
INFO:root:current mean train loss 1809.9674880600708
INFO:root:current train perplexity4.163269996643066
INFO:root:current mean train loss 1810.2904923541919
INFO:root:current train perplexity4.1628031730651855
INFO:root:current mean train loss 1808.3469446528802
INFO:root:current train perplexity4.16227388381958
INFO:root:current mean train loss 1808.8880689576831
INFO:root:current train perplexity4.166694641113281
INFO:root:current mean train loss 1809.6765432285304
INFO:root:current train perplexity4.167623996734619
INFO:root:current mean train loss 1810.4460879633173
INFO:root:current train perplexity4.1694464683532715
INFO:root:current mean train loss 1811.4262725802855
INFO:root:current train perplexity4.170928478240967
INFO:root:current mean train loss 1812.736526776688
INFO:root:current train perplexity4.172929286956787
INFO:root:current mean train loss 1814.1474659275052
INFO:root:current train perplexity4.1787214279174805
INFO:root:current mean train loss 1815.6116974285
INFO:root:current train perplexity4.181787967681885
INFO:root:current mean train loss 1815.7700975406806
INFO:root:current train perplexity4.182499885559082
INFO:root:current mean train loss 1816.9420221741739
INFO:root:current train perplexity4.186336040496826
INFO:root:current mean train loss 1818.3639054328878
INFO:root:current train perplexity4.189795017242432
INFO:root:current mean train loss 1818.3293317598932
INFO:root:current train perplexity4.190800189971924
INFO:root:current mean train loss 1817.665791437565
INFO:root:current train perplexity4.191198825836182

100%|██████████| 1/1 [02:18<00:00, 138.43s/it][A100%|██████████| 1/1 [02:18<00:00, 138.44s/it]
INFO:root:final mean train loss: 1817.0188125248696
INFO:root:final train perplexity: 4.191298007965088
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2082.36165321296
INFO:root:eval perplexity: 5.387556552886963
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/67
 34%|███▎      | 67/200 [2:42:48<5:23:13, 145.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1814.9672144839637
INFO:root:current train perplexity4.15811824798584
INFO:root:current mean train loss 1797.5894023508265
INFO:root:current train perplexity4.15110969543457
INFO:root:current mean train loss 1808.9633199226957
INFO:root:current train perplexity4.173768997192383
INFO:root:current mean train loss 1805.0041352221247
INFO:root:current train perplexity4.173305988311768
INFO:root:current mean train loss 1806.8969559342893
INFO:root:current train perplexity4.1756911277771
INFO:root:current mean train loss 1808.8087759478829
INFO:root:current train perplexity4.175416469573975
INFO:root:current mean train loss 1809.5966618935515
INFO:root:current train perplexity4.1770453453063965
INFO:root:current mean train loss 1810.0987618299034
INFO:root:current train perplexity4.181804180145264
INFO:root:current mean train loss 1811.3058805260853
INFO:root:current train perplexity4.1804938316345215
INFO:root:current mean train loss 1811.8202566704008
INFO:root:current train perplexity4.1836371421813965
INFO:root:current mean train loss 1812.7960460273287
INFO:root:current train perplexity4.18142032623291
INFO:root:current mean train loss 1813.445110193665
INFO:root:current train perplexity4.1814398765563965
INFO:root:current mean train loss 1813.4223987782714
INFO:root:current train perplexity4.179396152496338
INFO:root:current mean train loss 1814.722285112458
INFO:root:current train perplexity4.181879997253418
INFO:root:current mean train loss 1814.3156857125782
INFO:root:current train perplexity4.181139945983887
INFO:root:current mean train loss 1813.0517257472163
INFO:root:current train perplexity4.18000602722168
INFO:root:current mean train loss 1814.0907287970276
INFO:root:current train perplexity4.1817193031311035
INFO:root:current mean train loss 1814.4640719256824
INFO:root:current train perplexity4.181670665740967
INFO:root:current mean train loss 1813.5796732606773
INFO:root:current train perplexity4.180136680603027
INFO:root:current mean train loss 1813.7107438755725
INFO:root:current train perplexity4.179287910461426

100%|██████████| 1/1 [02:18<00:00, 138.20s/it][A100%|██████████| 1/1 [02:18<00:00, 138.20s/it]
INFO:root:final mean train loss: 1813.3415854834452
INFO:root:final train perplexity: 4.179159641265869
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2083.933479904283
INFO:root:eval perplexity: 5.3944091796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/68
 34%|███▍      | 68/200 [2:45:14<5:20:41, 145.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.1759011008523
INFO:root:current train perplexity4.108833312988281
INFO:root:current mean train loss 1787.5937043220765
INFO:root:current train perplexity4.145954608917236
INFO:root:current mean train loss 1799.1788344439337
INFO:root:current train perplexity4.161216735839844
INFO:root:current mean train loss 1803.4629731514085
INFO:root:current train perplexity4.157248497009277
INFO:root:current mean train loss 1802.5640493539663
INFO:root:current train perplexity4.159584045410156
INFO:root:current mean train loss 1800.6445710603182
INFO:root:current train perplexity4.15248966217041
INFO:root:current mean train loss 1803.54773713323
INFO:root:current train perplexity4.155437469482422
INFO:root:current mean train loss 1803.5592419352752
INFO:root:current train perplexity4.152879238128662
INFO:root:current mean train loss 1804.9658961245889
INFO:root:current train perplexity4.155744552612305
INFO:root:current mean train loss 1806.3997821907722
INFO:root:current train perplexity4.158380031585693
INFO:root:current mean train loss 1806.3472030278065
INFO:root:current train perplexity4.1576128005981445
INFO:root:current mean train loss 1806.8971984070618
INFO:root:current train perplexity4.158957481384277
INFO:root:current mean train loss 1807.6258185033305
INFO:root:current train perplexity4.163435459136963
INFO:root:current mean train loss 1809.2707866373098
INFO:root:current train perplexity4.165846824645996
INFO:root:current mean train loss 1809.3628700702052
INFO:root:current train perplexity4.166740894317627
INFO:root:current mean train loss 1809.3813426321342
INFO:root:current train perplexity4.167936325073242
INFO:root:current mean train loss 1809.7369701927162
INFO:root:current train perplexity4.1673383712768555
INFO:root:current mean train loss 1810.3863133096288
INFO:root:current train perplexity4.168519020080566
INFO:root:current mean train loss 1809.2277499052393
INFO:root:current train perplexity4.166730880737305
INFO:root:current mean train loss 1809.912373121803
INFO:root:current train perplexity4.1676506996154785

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1809.8763826017241
INFO:root:final train perplexity: 4.167754650115967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2083.163135285073
INFO:root:eval perplexity: 5.391049385070801
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/69
 34%|███▍      | 69/200 [2:47:40<5:18:16, 145.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1792.3065372043186
INFO:root:current train perplexity4.084888458251953
INFO:root:current mean train loss 1796.2968366755995
INFO:root:current train perplexity4.101386070251465
INFO:root:current mean train loss 1794.1265914019416
INFO:root:current train perplexity4.106434345245361
INFO:root:current mean train loss 1792.895995109312
INFO:root:current train perplexity4.1123762130737305
INFO:root:current mean train loss 1796.0002948308395
INFO:root:current train perplexity4.121792316436768
INFO:root:current mean train loss 1798.054706280048
INFO:root:current train perplexity4.126913547515869
INFO:root:current mean train loss 1800.6248492286318
INFO:root:current train perplexity4.127877712249756
INFO:root:current mean train loss 1801.7949221912443
INFO:root:current train perplexity4.1304426193237305
INFO:root:current mean train loss 1801.8129131072158
INFO:root:current train perplexity4.130651473999023
INFO:root:current mean train loss 1800.3843947673531
INFO:root:current train perplexity4.133647918701172
INFO:root:current mean train loss 1801.0833907625567
INFO:root:current train perplexity4.133655071258545
INFO:root:current mean train loss 1802.4199305199113
INFO:root:current train perplexity4.137661457061768
INFO:root:current mean train loss 1802.5193441139077
INFO:root:current train perplexity4.139345169067383
INFO:root:current mean train loss 1803.6456257011037
INFO:root:current train perplexity4.14232063293457
INFO:root:current mean train loss 1804.8218742038894
INFO:root:current train perplexity4.14636754989624
INFO:root:current mean train loss 1804.936223619767
INFO:root:current train perplexity4.147186756134033
INFO:root:current mean train loss 1805.117779818448
INFO:root:current train perplexity4.147295951843262
INFO:root:current mean train loss 1804.7593992762856
INFO:root:current train perplexity4.1485137939453125
INFO:root:current mean train loss 1805.5787105723325
INFO:root:current train perplexity4.151881217956543
INFO:root:current mean train loss 1806.1439095085098
INFO:root:current train perplexity4.154151439666748

100%|██████████| 1/1 [02:20<00:00, 141.00s/it][A100%|██████████| 1/1 [02:20<00:00, 141.00s/it]
INFO:root:final mean train loss: 1805.733504256394
INFO:root:final train perplexity: 4.154159069061279
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.13s/it][A100%|██████████| 1/1 [00:07<00:00,  7.13s/it]
INFO:root:eval mean loss: 2083.361484392315
INFO:root:eval perplexity: 5.391915321350098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/70
 35%|███▌      | 70/200 [2:50:08<5:17:32, 146.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1803.277057090502
INFO:root:current train perplexity4.093616485595703
INFO:root:current mean train loss 1804.8786440248844
INFO:root:current train perplexity4.104224681854248
INFO:root:current mean train loss 1807.5396158290982
INFO:root:current train perplexity4.109768867492676
INFO:root:current mean train loss 1808.8557329741725
INFO:root:current train perplexity4.113637447357178
INFO:root:current mean train loss 1808.365805284621
INFO:root:current train perplexity4.112990379333496
INFO:root:current mean train loss 1806.4369609424741
INFO:root:current train perplexity4.11157751083374
INFO:root:current mean train loss 1804.3157197152236
INFO:root:current train perplexity4.117381572723389
INFO:root:current mean train loss 1802.3255106221284
INFO:root:current train perplexity4.121039867401123
INFO:root:current mean train loss 1801.7529684094663
INFO:root:current train perplexity4.125467300415039
INFO:root:current mean train loss 1800.2117696517157
INFO:root:current train perplexity4.124164581298828
INFO:root:current mean train loss 1800.3638247935678
INFO:root:current train perplexity4.122822284698486
INFO:root:current mean train loss 1802.1974274271972
INFO:root:current train perplexity4.12928581237793
INFO:root:current mean train loss 1804.0147933308888
INFO:root:current train perplexity4.133995056152344
INFO:root:current mean train loss 1803.3488344174652
INFO:root:current train perplexity4.133590221405029
INFO:root:current mean train loss 1803.3076118587087
INFO:root:current train perplexity4.134342670440674
INFO:root:current mean train loss 1803.9473819336552
INFO:root:current train perplexity4.138166904449463
INFO:root:current mean train loss 1803.756209468898
INFO:root:current train perplexity4.139459609985352
INFO:root:current mean train loss 1802.516807151014
INFO:root:current train perplexity4.13986873626709
INFO:root:current mean train loss 1802.265780608953
INFO:root:current train perplexity4.140213489532471

100%|██████████| 1/1 [02:18<00:00, 138.09s/it][A100%|██████████| 1/1 [02:18<00:00, 138.09s/it]
INFO:root:final mean train loss: 1802.208804593685
INFO:root:final train perplexity: 4.142627716064453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2084.4708174035904
INFO:root:eval perplexity: 5.396753311157227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/71
 36%|███▌      | 71/200 [2:52:34<5:14:28, 146.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1749.96826171875
INFO:root:current train perplexity4.168575763702393
INFO:root:current mean train loss 1788.7171124152417
INFO:root:current train perplexity4.086828708648682
INFO:root:current mean train loss 1800.5681928616125
INFO:root:current train perplexity4.114603519439697
INFO:root:current mean train loss 1802.9826021880106
INFO:root:current train perplexity4.125533103942871
INFO:root:current mean train loss 1797.2174279725023
INFO:root:current train perplexity4.117006301879883
INFO:root:current mean train loss 1797.107258310431
INFO:root:current train perplexity4.119375228881836
INFO:root:current mean train loss 1798.434185770872
INFO:root:current train perplexity4.122894287109375
INFO:root:current mean train loss 1795.8370556709788
INFO:root:current train perplexity4.119556427001953
INFO:root:current mean train loss 1796.066418971968
INFO:root:current train perplexity4.1228742599487305
INFO:root:current mean train loss 1794.995407138176
INFO:root:current train perplexity4.11881685256958
INFO:root:current mean train loss 1795.7460374471918
INFO:root:current train perplexity4.118710041046143
INFO:root:current mean train loss 1798.0366469205612
INFO:root:current train perplexity4.122382640838623
INFO:root:current mean train loss 1797.8845752317513
INFO:root:current train perplexity4.12202787399292
INFO:root:current mean train loss 1798.5118553079835
INFO:root:current train perplexity4.123404026031494
INFO:root:current mean train loss 1798.73816291299
INFO:root:current train perplexity4.124506950378418
INFO:root:current mean train loss 1798.8985654873993
INFO:root:current train perplexity4.1267781257629395
INFO:root:current mean train loss 1799.2760405772055
INFO:root:current train perplexity4.125915050506592
INFO:root:current mean train loss 1799.3185470598944
INFO:root:current train perplexity4.126790523529053
INFO:root:current mean train loss 1798.6008406224046
INFO:root:current train perplexity4.127060413360596
INFO:root:current mean train loss 1799.0302889364589
INFO:root:current train perplexity4.129281997680664

100%|██████████| 1/1 [02:18<00:00, 138.11s/it][A100%|██████████| 1/1 [02:18<00:00, 138.11s/it]
INFO:root:final mean train loss: 1798.4186040019363
INFO:root:final train perplexity: 4.130262851715088
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2085.8348990365967
INFO:root:eval perplexity: 5.402709484100342
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/72
 36%|███▌      | 72/200 [2:54:59<5:11:35, 146.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1809.0591669497283
INFO:root:current train perplexity4.171271324157715
INFO:root:current mean train loss 1794.9336274930133
INFO:root:current train perplexity4.11790657043457
INFO:root:current mean train loss 1792.440988583415
INFO:root:current train perplexity4.105401039123535
INFO:root:current mean train loss 1787.2529587878532
INFO:root:current train perplexity4.091733932495117
INFO:root:current mean train loss 1786.102445561835
INFO:root:current train perplexity4.095510959625244
INFO:root:current mean train loss 1788.0178143298876
INFO:root:current train perplexity4.103182315826416
INFO:root:current mean train loss 1788.2102585696102
INFO:root:current train perplexity4.105660438537598
INFO:root:current mean train loss 1789.856825874719
INFO:root:current train perplexity4.110951900482178
INFO:root:current mean train loss 1789.737573598164
INFO:root:current train perplexity4.108139991760254
INFO:root:current mean train loss 1790.3877135635325
INFO:root:current train perplexity4.107985496520996
INFO:root:current mean train loss 1791.3452217646475
INFO:root:current train perplexity4.112356662750244
INFO:root:current mean train loss 1791.9307029771678
INFO:root:current train perplexity4.113548755645752
INFO:root:current mean train loss 1791.6685801994965
INFO:root:current train perplexity4.113043785095215
INFO:root:current mean train loss 1793.8202499424249
INFO:root:current train perplexity4.112896919250488
INFO:root:current mean train loss 1794.2509744179056
INFO:root:current train perplexity4.115828514099121
INFO:root:current mean train loss 1794.923835018005
INFO:root:current train perplexity4.118271350860596
INFO:root:current mean train loss 1795.376234241422
INFO:root:current train perplexity4.118697643280029
INFO:root:current mean train loss 1796.4274629580718
INFO:root:current train perplexity4.1200175285339355
INFO:root:current mean train loss 1796.540036116206
INFO:root:current train perplexity4.1203131675720215
INFO:root:current mean train loss 1796.1481630163644
INFO:root:current train perplexity4.119922161102295

100%|██████████| 1/1 [02:18<00:00, 138.36s/it][A100%|██████████| 1/1 [02:18<00:00, 138.36s/it]
INFO:root:final mean train loss: 1795.0722755051236
INFO:root:final train perplexity: 4.119377136230469
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2085.576462333084
INFO:root:eval perplexity: 5.401582717895508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/73
 36%|███▋      | 73/200 [2:57:25<5:09:03, 146.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1780.1570587158203
INFO:root:current train perplexity4.046586036682129
INFO:root:current mean train loss 1776.8459010532924
INFO:root:current train perplexity4.062680244445801
INFO:root:current mean train loss 1774.8779012044272
INFO:root:current train perplexity4.06793212890625
INFO:root:current mean train loss 1776.66665577608
INFO:root:current train perplexity4.075309753417969
INFO:root:current mean train loss 1775.8959605823864
INFO:root:current train perplexity4.078014850616455
INFO:root:current mean train loss 1778.0144524468317
INFO:root:current train perplexity4.0760626792907715
INFO:root:current mean train loss 1778.7317636489868
INFO:root:current train perplexity4.078071117401123
INFO:root:current mean train loss 1781.6651921452703
INFO:root:current train perplexity4.085250377655029
INFO:root:current mean train loss 1783.6464951288133
INFO:root:current train perplexity4.086735248565674
INFO:root:current mean train loss 1786.5969939536237
INFO:root:current train perplexity4.089010238647461
INFO:root:current mean train loss 1784.8933224017803
INFO:root:current train perplexity4.0888166427612305
INFO:root:current mean train loss 1785.775989626165
INFO:root:current train perplexity4.092574596405029
INFO:root:current mean train loss 1786.4280131678427
INFO:root:current train perplexity4.093384265899658
INFO:root:current mean train loss 1787.638347659894
INFO:root:current train perplexity4.095983982086182
INFO:root:current mean train loss 1788.236749352349
INFO:root:current train perplexity4.097775459289551
INFO:root:current mean train loss 1788.419010628044
INFO:root:current train perplexity4.0999627113342285
INFO:root:current mean train loss 1789.643244599133
INFO:root:current train perplexity4.102412223815918
INFO:root:current mean train loss 1791.0881135085533
INFO:root:current train perplexity4.104641914367676
INFO:root:current mean train loss 1791.5909866333009
INFO:root:current train perplexity4.103909492492676
INFO:root:current mean train loss 1791.666537255356
INFO:root:current train perplexity4.105754852294922

100%|██████████| 1/1 [02:17<00:00, 137.99s/it][A100%|██████████| 1/1 [02:17<00:00, 137.99s/it]
INFO:root:final mean train loss: 1791.3345852465684
INFO:root:final train perplexity: 4.10725212097168
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2086.5933781617077
INFO:root:eval perplexity: 5.406025409698486
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/74
 37%|███▋      | 74/200 [2:59:51<5:06:19, 145.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1775.1354273745887
INFO:root:current train perplexity4.050836086273193
INFO:root:current mean train loss 1793.0706802659734
INFO:root:current train perplexity4.080978870391846
INFO:root:current mean train loss 1779.6436682081408
INFO:root:current train perplexity4.065008163452148
INFO:root:current mean train loss 1780.8509258195465
INFO:root:current train perplexity4.074867248535156
INFO:root:current mean train loss 1784.598134113871
INFO:root:current train perplexity4.076816082000732
INFO:root:current mean train loss 1781.9560625771433
INFO:root:current train perplexity4.0732340812683105
INFO:root:current mean train loss 1783.4075478099435
INFO:root:current train perplexity4.083933353424072
INFO:root:current mean train loss 1784.4263907952857
INFO:root:current train perplexity4.0833001136779785
INFO:root:current mean train loss 1783.9302720700846
INFO:root:current train perplexity4.083068370819092
INFO:root:current mean train loss 1784.716056927369
INFO:root:current train perplexity4.084847450256348
INFO:root:current mean train loss 1785.7205315105398
INFO:root:current train perplexity4.086452484130859
INFO:root:current mean train loss 1785.2200849660019
INFO:root:current train perplexity4.088529586791992
INFO:root:current mean train loss 1785.5583035780878
INFO:root:current train perplexity4.0881123542785645
INFO:root:current mean train loss 1786.2887545805602
INFO:root:current train perplexity4.088747024536133
INFO:root:current mean train loss 1786.344143188728
INFO:root:current train perplexity4.090442657470703
INFO:root:current mean train loss 1786.321961037226
INFO:root:current train perplexity4.090514659881592
INFO:root:current mean train loss 1788.1635884369578
INFO:root:current train perplexity4.0937724113464355
INFO:root:current mean train loss 1787.9368923580232
INFO:root:current train perplexity4.093719005584717
INFO:root:current mean train loss 1787.7006089842698
INFO:root:current train perplexity4.093221664428711
INFO:root:current mean train loss 1787.9727359044655
INFO:root:current train perplexity4.094672679901123

100%|██████████| 1/1 [02:18<00:00, 138.38s/it][A100%|██████████| 1/1 [02:18<00:00, 138.38s/it]
INFO:root:final mean train loss: 1787.5983417643722
INFO:root:final train perplexity: 4.095167636871338
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2086.93368162331
INFO:root:eval perplexity: 5.407513618469238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/75
 38%|███▊      | 75/200 [3:02:17<5:03:55, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.740252520587
INFO:root:current train perplexity4.07875394821167
INFO:root:current mean train loss 1781.881596006196
INFO:root:current train perplexity4.056262493133545
INFO:root:current mean train loss 1782.441755085966
INFO:root:current train perplexity4.074009418487549
INFO:root:current mean train loss 1784.6256031709559
INFO:root:current train perplexity4.079416751861572
INFO:root:current mean train loss 1782.356703874934
INFO:root:current train perplexity4.075560092926025
INFO:root:current mean train loss 1781.581889614411
INFO:root:current train perplexity4.068594932556152
INFO:root:current mean train loss 1784.7817880873863
INFO:root:current train perplexity4.071572780609131
INFO:root:current mean train loss 1783.0784415311591
INFO:root:current train perplexity4.068804740905762
INFO:root:current mean train loss 1782.4021987740452
INFO:root:current train perplexity4.069888114929199
INFO:root:current mean train loss 1781.454618168073
INFO:root:current train perplexity4.069351673126221
INFO:root:current mean train loss 1780.8934184097488
INFO:root:current train perplexity4.068684101104736
INFO:root:current mean train loss 1780.8014696891303
INFO:root:current train perplexity4.069211483001709
INFO:root:current mean train loss 1781.039690385995
INFO:root:current train perplexity4.074007034301758
INFO:root:current mean train loss 1780.5615328548604
INFO:root:current train perplexity4.073660373687744
INFO:root:current mean train loss 1781.6662837821723
INFO:root:current train perplexity4.075589179992676
INFO:root:current mean train loss 1781.895134078805
INFO:root:current train perplexity4.076642990112305
INFO:root:current mean train loss 1782.6761415543094
INFO:root:current train perplexity4.079796314239502
INFO:root:current mean train loss 1782.7769531800486
INFO:root:current train perplexity4.0803327560424805
INFO:root:current mean train loss 1783.2472192200423
INFO:root:current train perplexity4.080419063568115
INFO:root:current mean train loss 1783.8564894655917
INFO:root:current train perplexity4.0811285972595215

100%|██████████| 1/1 [02:18<00:00, 138.33s/it][A100%|██████████| 1/1 [02:18<00:00, 138.33s/it]
INFO:root:final mean train loss: 1783.4348093129984
INFO:root:final train perplexity: 4.081742286682129
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2089.8049662705007
INFO:root:eval perplexity: 5.4200849533081055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/76
 38%|███▊      | 76/200 [3:04:42<5:01:25, 145.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1786.5558371072286
INFO:root:current train perplexity4.054521560668945
INFO:root:current mean train loss 1776.6653995214333
INFO:root:current train perplexity4.038558006286621
INFO:root:current mean train loss 1780.2667462850354
INFO:root:current train perplexity4.047121524810791
INFO:root:current mean train loss 1778.0515330282929
INFO:root:current train perplexity4.051481246948242
INFO:root:current mean train loss 1779.7764509709934
INFO:root:current train perplexity4.057821750640869
INFO:root:current mean train loss 1781.063440829566
INFO:root:current train perplexity4.060669898986816
INFO:root:current mean train loss 1779.3702389044975
INFO:root:current train perplexity4.0604400634765625
INFO:root:current mean train loss 1778.39065138941
INFO:root:current train perplexity4.057674884796143
INFO:root:current mean train loss 1778.0433250966698
INFO:root:current train perplexity4.0564680099487305
INFO:root:current mean train loss 1778.5282353190432
INFO:root:current train perplexity4.0600905418396
INFO:root:current mean train loss 1780.4779757631688
INFO:root:current train perplexity4.06488037109375
INFO:root:current mean train loss 1782.5206200433918
INFO:root:current train perplexity4.068550109863281
INFO:root:current mean train loss 1782.1978117738188
INFO:root:current train perplexity4.068873405456543
INFO:root:current mean train loss 1781.1870990371979
INFO:root:current train perplexity4.069784641265869
INFO:root:current mean train loss 1781.4833786246122
INFO:root:current train perplexity4.070128440856934
INFO:root:current mean train loss 1781.8064859463389
INFO:root:current train perplexity4.0711588859558105
INFO:root:current mean train loss 1781.039835347289
INFO:root:current train perplexity4.0699286460876465
INFO:root:current mean train loss 1780.9390655125671
INFO:root:current train perplexity4.070708274841309
INFO:root:current mean train loss 1782.3130732334082
INFO:root:current train perplexity4.07517671585083

100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
INFO:root:final mean train loss: 1781.163557413306
INFO:root:final train perplexity: 4.074438095092773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2090.851069457142
INFO:root:eval perplexity: 5.424672603607178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/77
 38%|███▊      | 77/200 [3:07:08<4:58:53, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1776.9088439941406
INFO:root:current train perplexity4.045590877532959
INFO:root:current mean train loss 1786.5585361056858
INFO:root:current train perplexity4.054098129272461
INFO:root:current mean train loss 1772.7946249154897
INFO:root:current train perplexity4.039381504058838
INFO:root:current mean train loss 1765.3440452674768
INFO:root:current train perplexity4.036602973937988
INFO:root:current mean train loss 1765.5029488357843
INFO:root:current train perplexity4.039364814758301
INFO:root:current mean train loss 1764.4118154931257
INFO:root:current train perplexity4.037299156188965
INFO:root:current mean train loss 1764.1386003996197
INFO:root:current train perplexity4.036975860595703
INFO:root:current mean train loss 1767.1307090285134
INFO:root:current train perplexity4.044620037078857
INFO:root:current mean train loss 1768.322222568021
INFO:root:current train perplexity4.039087295532227
INFO:root:current mean train loss 1770.5967427392363
INFO:root:current train perplexity4.044258117675781
INFO:root:current mean train loss 1770.8868629818871
INFO:root:current train perplexity4.044753551483154
INFO:root:current mean train loss 1771.4036652602874
INFO:root:current train perplexity4.047252178192139
INFO:root:current mean train loss 1772.9178811382774
INFO:root:current train perplexity4.047036170959473
INFO:root:current mean train loss 1773.557341036082
INFO:root:current train perplexity4.051621913909912
INFO:root:current mean train loss 1774.0145689357412
INFO:root:current train perplexity4.053621768951416
INFO:root:current mean train loss 1773.5946486900593
INFO:root:current train perplexity4.054961204528809
INFO:root:current mean train loss 1775.807384813603
INFO:root:current train perplexity4.057120323181152
INFO:root:current mean train loss 1776.785283894952
INFO:root:current train perplexity4.05948543548584
INFO:root:current mean train loss 1777.703359890828
INFO:root:current train perplexity4.060217380523682
INFO:root:current mean train loss 1777.8074112418312
INFO:root:current train perplexity4.060876846313477

100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
INFO:root:final mean train loss: 1777.5163891244524
INFO:root:final train perplexity: 4.062734603881836
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2091.4113033646386
INFO:root:eval perplexity: 5.427131175994873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/78
 39%|███▉      | 78/200 [3:09:34<4:56:25, 145.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1782.325830078125
INFO:root:current train perplexity4.022916316986084
INFO:root:current mean train loss 1784.465890625
INFO:root:current train perplexity4.031577110290527
INFO:root:current mean train loss 1776.1386528862847
INFO:root:current train perplexity4.0294623374938965
INFO:root:current mean train loss 1782.98172663762
INFO:root:current train perplexity4.025610446929932
INFO:root:current mean train loss 1778.4366377527574
INFO:root:current train perplexity4.032894134521484
INFO:root:current mean train loss 1780.5834763299852
INFO:root:current train perplexity4.034530162811279
INFO:root:current mean train loss 1779.5717169921875
INFO:root:current train perplexity4.037497520446777
INFO:root:current mean train loss 1776.494030004041
INFO:root:current train perplexity4.035147666931152
INFO:root:current mean train loss 1775.5683425071022
INFO:root:current train perplexity4.039253234863281
INFO:root:current mean train loss 1774.9265935124577
INFO:root:current train perplexity4.03834342956543
INFO:root:current mean train loss 1773.4017894912347
INFO:root:current train perplexity4.039590835571289
INFO:root:current mean train loss 1773.0802880859376
INFO:root:current train perplexity4.043718338012695
INFO:root:current mean train loss 1773.1527444395726
INFO:root:current train perplexity4.043435573577881
INFO:root:current mean train loss 1772.5409555571935
INFO:root:current train perplexity4.044215202331543
INFO:root:current mean train loss 1773.4295809347589
INFO:root:current train perplexity4.044816017150879
INFO:root:current mean train loss 1772.0512305487962
INFO:root:current train perplexity4.043754577636719
INFO:root:current mean train loss 1772.1701397235577
INFO:root:current train perplexity4.046641826629639
INFO:root:current mean train loss 1774.0034462749095
INFO:root:current train perplexity4.049692153930664
INFO:root:current mean train loss 1774.8302044092466
INFO:root:current train perplexity4.052671909332275
INFO:root:current mean train loss 1775.5514307908888
INFO:root:current train perplexity4.054014682769775

100%|██████████| 1/1 [02:18<00:00, 138.23s/it][A100%|██████████| 1/1 [02:18<00:00, 138.23s/it]
INFO:root:final mean train loss: 1774.3107201619034
INFO:root:final train perplexity: 4.052476406097412
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.31s/it][A100%|██████████| 1/1 [00:07<00:00,  7.31s/it]
INFO:root:eval mean loss: 2092.913606684259
INFO:root:eval perplexity: 5.43372917175293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/79
 40%|███▉      | 79/200 [3:12:00<4:54:00, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1788.729989188058
INFO:root:current train perplexity4.039499282836914
INFO:root:current mean train loss 1776.624894263039
INFO:root:current train perplexity4.042974472045898
INFO:root:current mean train loss 1766.3726620004197
INFO:root:current train perplexity4.013847827911377
INFO:root:current mean train loss 1769.5674117238898
INFO:root:current train perplexity4.020602226257324
INFO:root:current mean train loss 1766.3682212311758
INFO:root:current train perplexity4.012781620025635
INFO:root:current mean train loss 1766.8622168509282
INFO:root:current train perplexity4.0154266357421875
INFO:root:current mean train loss 1769.4847876052618
INFO:root:current train perplexity4.025233268737793
INFO:root:current mean train loss 1768.2088315403364
INFO:root:current train perplexity4.026412487030029
INFO:root:current mean train loss 1769.5122110905952
INFO:root:current train perplexity4.028608798980713
INFO:root:current mean train loss 1770.0465770810526
INFO:root:current train perplexity4.030315399169922
INFO:root:current mean train loss 1769.7172056113918
INFO:root:current train perplexity4.031942844390869
INFO:root:current mean train loss 1770.1007044803866
INFO:root:current train perplexity4.0325093269348145
INFO:root:current mean train loss 1770.1134148196898
INFO:root:current train perplexity4.032415866851807
INFO:root:current mean train loss 1769.419364553983
INFO:root:current train perplexity4.032869815826416
INFO:root:current mean train loss 1770.3075998335373
INFO:root:current train perplexity4.036607265472412
INFO:root:current mean train loss 1769.5062840878577
INFO:root:current train perplexity4.035011291503906
INFO:root:current mean train loss 1769.2742192109233
INFO:root:current train perplexity4.03523063659668
INFO:root:current mean train loss 1770.1583753688737
INFO:root:current train perplexity4.038521766662598
INFO:root:current mean train loss 1769.8682570400508
INFO:root:current train perplexity4.039062976837158
INFO:root:current mean train loss 1770.3517320909903
INFO:root:current train perplexity4.039680480957031

100%|██████████| 1/1 [02:17<00:00, 137.97s/it][A100%|██████████| 1/1 [02:17<00:00, 137.97s/it]
INFO:root:final mean train loss: 1770.621050936131
INFO:root:final train perplexity: 4.040701389312744
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2092.5689095571533
INFO:root:eval perplexity: 5.432214736938477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/80
 40%|████      | 80/200 [3:14:25<4:51:21, 145.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1751.304085424391
INFO:root:current train perplexity4.039425849914551
INFO:root:current mean train loss 1751.0961392000786
INFO:root:current train perplexity4.017282485961914
INFO:root:current mean train loss 1752.7717780035896
INFO:root:current train perplexity4.006236553192139
INFO:root:current mean train loss 1757.7244468412691
INFO:root:current train perplexity4.0102152824401855
INFO:root:current mean train loss 1759.9045500578704
INFO:root:current train perplexity4.017967700958252
INFO:root:current mean train loss 1761.567846636013
INFO:root:current train perplexity4.018383026123047
INFO:root:current mean train loss 1762.0373414753058
INFO:root:current train perplexity4.015773296356201
INFO:root:current mean train loss 1761.0641306599452
INFO:root:current train perplexity4.013922214508057
INFO:root:current mean train loss 1762.0756419562626
INFO:root:current train perplexity4.017581462860107
INFO:root:current mean train loss 1763.3723662598165
INFO:root:current train perplexity4.019458293914795
INFO:root:current mean train loss 1764.075031053581
INFO:root:current train perplexity4.019873142242432
INFO:root:current mean train loss 1764.482033756707
INFO:root:current train perplexity4.020172595977783
INFO:root:current mean train loss 1765.3129666595823
INFO:root:current train perplexity4.022156715393066
INFO:root:current mean train loss 1766.1142899693593
INFO:root:current train perplexity4.021850109100342
INFO:root:current mean train loss 1766.1969980575843
INFO:root:current train perplexity4.0228729248046875
INFO:root:current mean train loss 1767.773843644138
INFO:root:current train perplexity4.025007247924805
INFO:root:current mean train loss 1766.2055838448662
INFO:root:current train perplexity4.02398681640625
INFO:root:current mean train loss 1767.4928256105875
INFO:root:current train perplexity4.0278401374816895
INFO:root:current mean train loss 1767.5996602649905
INFO:root:current train perplexity4.030434608459473
INFO:root:current mean train loss 1768.17607654924
INFO:root:current train perplexity4.031209945678711

100%|██████████| 1/1 [02:17<00:00, 137.84s/it][A100%|██████████| 1/1 [02:17<00:00, 137.84s/it]
INFO:root:final mean train loss: 1767.583599265633
INFO:root:final train perplexity: 4.031033039093018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2093.720933846548
INFO:root:eval perplexity: 5.437277793884277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/81
 40%|████      | 81/200 [3:16:50<4:48:43, 145.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1738.53514661287
INFO:root:current train perplexity3.964360237121582
INFO:root:current mean train loss 1747.216074856845
INFO:root:current train perplexity3.9739015102386475
INFO:root:current mean train loss 1746.0466627038043
INFO:root:current train perplexity3.9745614528656006
INFO:root:current mean train loss 1748.7721129072474
INFO:root:current train perplexity3.975189685821533
INFO:root:current mean train loss 1753.162714854008
INFO:root:current train perplexity3.985440492630005
INFO:root:current mean train loss 1755.5696383582222
INFO:root:current train perplexity3.9900598526000977
INFO:root:current mean train loss 1756.3529686560998
INFO:root:current train perplexity3.9914052486419678
INFO:root:current mean train loss 1758.5264522906432
INFO:root:current train perplexity3.999199390411377
INFO:root:current mean train loss 1757.8503249355647
INFO:root:current train perplexity3.9996070861816406
INFO:root:current mean train loss 1758.4632154370918
INFO:root:current train perplexity4.002021312713623
INFO:root:current mean train loss 1757.9735099480497
INFO:root:current train perplexity4.003650188446045
INFO:root:current mean train loss 1758.913869533409
INFO:root:current train perplexity4.007344722747803
INFO:root:current mean train loss 1760.5195197700334
INFO:root:current train perplexity4.009918689727783
INFO:root:current mean train loss 1761.8369625889977
INFO:root:current train perplexity4.012035846710205
INFO:root:current mean train loss 1762.099122334302
INFO:root:current train perplexity4.013442516326904
INFO:root:current mean train loss 1762.768875199526
INFO:root:current train perplexity4.015462875366211
INFO:root:current mean train loss 1762.9164264105384
INFO:root:current train perplexity4.016229152679443
INFO:root:current mean train loss 1763.428625020895
INFO:root:current train perplexity4.015776634216309
INFO:root:current mean train loss 1764.4455658154193
INFO:root:current train perplexity4.019672393798828
INFO:root:current mean train loss 1764.8261392570219
INFO:root:current train perplexity4.020599365234375

100%|██████████| 1/1 [02:18<00:00, 138.18s/it][A100%|██████████| 1/1 [02:18<00:00, 138.18s/it]
INFO:root:final mean train loss: 1764.2297705238177
INFO:root:final train perplexity: 4.0203857421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2094.6125916825963
INFO:root:eval perplexity: 5.441200256347656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/82
 41%|████      | 82/200 [3:19:16<4:46:19, 145.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1753.2272319178428
INFO:root:current train perplexity3.9760749340057373
INFO:root:current mean train loss 1757.308923276595
INFO:root:current train perplexity3.9780197143554688
INFO:root:current mean train loss 1751.513871853669
INFO:root:current train perplexity3.9786581993103027
INFO:root:current mean train loss 1746.2106588815004
INFO:root:current train perplexity3.966369152069092
INFO:root:current mean train loss 1750.5130196778334
INFO:root:current train perplexity3.977494955062866
INFO:root:current mean train loss 1752.1598149471702
INFO:root:current train perplexity3.9819092750549316
INFO:root:current mean train loss 1752.0206212515782
INFO:root:current train perplexity3.983973264694214
INFO:root:current mean train loss 1752.872741930121
INFO:root:current train perplexity3.986639976501465
INFO:root:current mean train loss 1757.7559541059368
INFO:root:current train perplexity3.995079755783081
INFO:root:current mean train loss 1758.566415592743
INFO:root:current train perplexity3.9960508346557617
INFO:root:current mean train loss 1757.4213242875471
INFO:root:current train perplexity3.9947686195373535
INFO:root:current mean train loss 1758.0179704280831
INFO:root:current train perplexity3.999828338623047
INFO:root:current mean train loss 1759.220724272525
INFO:root:current train perplexity4.001802921295166
INFO:root:current mean train loss 1757.6240425411095
INFO:root:current train perplexity4.001548767089844
INFO:root:current mean train loss 1758.5194536580866
INFO:root:current train perplexity4.002282619476318
INFO:root:current mean train loss 1758.456447795286
INFO:root:current train perplexity4.003103733062744
INFO:root:current mean train loss 1758.3450851305236
INFO:root:current train perplexity4.004130840301514
INFO:root:current mean train loss 1759.1347110235552
INFO:root:current train perplexity4.005054950714111
INFO:root:current mean train loss 1760.1379262336775
INFO:root:current train perplexity4.008100509643555

100%|██████████| 1/1 [02:18<00:00, 138.30s/it][A100%|██████████| 1/1 [02:18<00:00, 138.30s/it]
INFO:root:final mean train loss: 1760.5871007691353
INFO:root:final train perplexity: 4.008852005004883
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2095.305189200327
INFO:root:eval perplexity: 5.444249153137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/83
 42%|████▏     | 83/200 [3:21:43<4:44:40, 145.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1763.8438842773437
INFO:root:current train perplexity3.936931610107422
INFO:root:current mean train loss 1750.4847389914773
INFO:root:current train perplexity3.971696376800537
INFO:root:current mean train loss 1741.588625372024
INFO:root:current train perplexity3.9603819847106934
INFO:root:current mean train loss 1741.8044547788559
INFO:root:current train perplexity3.9635984897613525
INFO:root:current mean train loss 1743.1597837866807
INFO:root:current train perplexity3.9683196544647217
INFO:root:current mean train loss 1747.747347244562
INFO:root:current train perplexity3.976478338241577
INFO:root:current mean train loss 1752.3176897893186
INFO:root:current train perplexity3.984527111053467
INFO:root:current mean train loss 1752.1524732133032
INFO:root:current train perplexity3.9813995361328125
INFO:root:current mean train loss 1750.7724109037422
INFO:root:current train perplexity3.9809889793395996
INFO:root:current mean train loss 1752.0786916208792
INFO:root:current train perplexity3.9842777252197266
INFO:root:current mean train loss 1752.3155339911432
INFO:root:current train perplexity3.984755039215088
INFO:root:current mean train loss 1753.7710208377323
INFO:root:current train perplexity3.989023447036743
INFO:root:current mean train loss 1754.2998098326123
INFO:root:current train perplexity3.991661310195923
INFO:root:current mean train loss 1754.4635820461594
INFO:root:current train perplexity3.994274854660034
INFO:root:current mean train loss 1755.175690519725
INFO:root:current train perplexity3.996044635772705
INFO:root:current mean train loss 1757.0272662232253
INFO:root:current train perplexity3.9979419708251953
INFO:root:current mean train loss 1757.4080309376213
INFO:root:current train perplexity3.9992423057556152
INFO:root:current mean train loss 1757.7016724346674
INFO:root:current train perplexity4.000241756439209
INFO:root:current mean train loss 1757.5215441961973
INFO:root:current train perplexity4.000729084014893
INFO:root:current mean train loss 1758.2231366062663
INFO:root:current train perplexity4.001086711883545

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1757.7210508376374
INFO:root:final train perplexity: 3.999800682067871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2097.724849619764
INFO:root:eval perplexity: 5.45491361618042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/84
 42%|████▏     | 84/200 [3:24:08<4:42:02, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1777.7029215494792
INFO:root:current train perplexity4.004044532775879
INFO:root:current mean train loss 1735.8140167476624
INFO:root:current train perplexity3.943419933319092
INFO:root:current mean train loss 1751.3276173595816
INFO:root:current train perplexity3.9687955379486084
INFO:root:current mean train loss 1749.8758533722764
INFO:root:current train perplexity3.970522403717041
INFO:root:current mean train loss 1748.3015902874342
INFO:root:current train perplexity3.9744760990142822
INFO:root:current mean train loss 1750.7322562577829
INFO:root:current train perplexity3.9678995609283447
INFO:root:current mean train loss 1749.3986024020012
INFO:root:current train perplexity3.971230983734131
INFO:root:current mean train loss 1747.4833991091386
INFO:root:current train perplexity3.970323085784912
INFO:root:current mean train loss 1747.6300941845525
INFO:root:current train perplexity3.9720585346221924
INFO:root:current mean train loss 1747.2582707574838
INFO:root:current train perplexity3.9713990688323975
INFO:root:current mean train loss 1747.9122848139302
INFO:root:current train perplexity3.974000930786133
INFO:root:current mean train loss 1748.4801416405558
INFO:root:current train perplexity3.9766788482666016
INFO:root:current mean train loss 1749.094370897164
INFO:root:current train perplexity3.978316068649292
INFO:root:current mean train loss 1751.163073610882
INFO:root:current train perplexity3.9790873527526855
INFO:root:current mean train loss 1752.0403270868462
INFO:root:current train perplexity3.979975461959839
INFO:root:current mean train loss 1753.6690845701846
INFO:root:current train perplexity3.9832539558410645
INFO:root:current mean train loss 1753.780552240992
INFO:root:current train perplexity3.986295461654663
INFO:root:current mean train loss 1755.4832707125117
INFO:root:current train perplexity3.9883289337158203
INFO:root:current mean train loss 1755.1724951733117
INFO:root:current train perplexity3.9904963970184326
INFO:root:current mean train loss 1755.3685506079316
INFO:root:current train perplexity3.991058349609375

100%|██████████| 1/1 [02:18<00:00, 138.32s/it][A100%|██████████| 1/1 [02:18<00:00, 138.32s/it]
INFO:root:final mean train loss: 1754.9100696988858
INFO:root:final train perplexity: 3.990943670272827
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2098.057336685505
INFO:root:eval perplexity: 5.456380844116211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/85
 42%|████▎     | 85/200 [3:26:34<4:39:34, 145.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.0953868519175
INFO:root:current train perplexity3.9057414531707764
INFO:root:current mean train loss 1742.7300092909072
INFO:root:current train perplexity3.9286093711853027
INFO:root:current mean train loss 1741.6692269747375
INFO:root:current train perplexity3.9444680213928223
INFO:root:current mean train loss 1739.7723140272983
INFO:root:current train perplexity3.953308582305908
INFO:root:current mean train loss 1739.3596540571334
INFO:root:current train perplexity3.95061993598938
INFO:root:current mean train loss 1740.1825698403752
INFO:root:current train perplexity3.958345890045166
INFO:root:current mean train loss 1743.1960453009754
INFO:root:current train perplexity3.9634618759155273
INFO:root:current mean train loss 1745.330060241043
INFO:root:current train perplexity3.967390775680542
INFO:root:current mean train loss 1748.0750068556076
INFO:root:current train perplexity3.967691421508789
INFO:root:current mean train loss 1747.72255771443
INFO:root:current train perplexity3.968233108520508
INFO:root:current mean train loss 1747.7099935597387
INFO:root:current train perplexity3.9709224700927734
INFO:root:current mean train loss 1749.8423725474966
INFO:root:current train perplexity3.973151683807373
INFO:root:current mean train loss 1749.7098004994284
INFO:root:current train perplexity3.9731862545013428
INFO:root:current mean train loss 1749.4900606246222
INFO:root:current train perplexity3.9723634719848633
INFO:root:current mean train loss 1749.5678693184893
INFO:root:current train perplexity3.974491834640503
INFO:root:current mean train loss 1749.1155010480338
INFO:root:current train perplexity3.9749555587768555
INFO:root:current mean train loss 1751.0572509765625
INFO:root:current train perplexity3.979726791381836
INFO:root:current mean train loss 1750.9068653211682
INFO:root:current train perplexity3.9789626598358154
INFO:root:current mean train loss 1751.1807268850198
INFO:root:current train perplexity3.979562759399414
INFO:root:current mean train loss 1752.3621602627475
INFO:root:current train perplexity3.980541944503784

100%|██████████| 1/1 [02:18<00:00, 138.03s/it][A100%|██████████| 1/1 [02:18<00:00, 138.03s/it]
INFO:root:final mean train loss: 1751.579207196719
INFO:root:final train perplexity: 3.980473279953003
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.29s/it][A100%|██████████| 1/1 [00:07<00:00,  7.29s/it]
INFO:root:eval mean loss: 2100.149205417498
INFO:root:eval perplexity: 5.465619087219238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/86
 43%|████▎     | 86/200 [3:29:00<4:36:59, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1729.082883741035
INFO:root:current train perplexity3.953829765319824
INFO:root:current mean train loss 1732.829446543818
INFO:root:current train perplexity3.969089984893799
INFO:root:current mean train loss 1729.9587453791007
INFO:root:current train perplexity3.9514310359954834
INFO:root:current mean train loss 1736.3572511118205
INFO:root:current train perplexity3.952498197555542
INFO:root:current mean train loss 1732.2624813584598
INFO:root:current train perplexity3.947354316711426
INFO:root:current mean train loss 1737.7271928702235
INFO:root:current train perplexity3.9570541381835938
INFO:root:current mean train loss 1737.0925307742766
INFO:root:current train perplexity3.9526150226593018
INFO:root:current mean train loss 1738.430388000729
INFO:root:current train perplexity3.954179048538208
INFO:root:current mean train loss 1740.763657555486
INFO:root:current train perplexity3.9576807022094727
INFO:root:current mean train loss 1741.737967881154
INFO:root:current train perplexity3.962103843688965
INFO:root:current mean train loss 1741.711555099847
INFO:root:current train perplexity3.959270477294922
INFO:root:current mean train loss 1742.2458627521735
INFO:root:current train perplexity3.9605941772460938
INFO:root:current mean train loss 1743.060239133903
INFO:root:current train perplexity3.961897373199463
INFO:root:current mean train loss 1743.6596603449611
INFO:root:current train perplexity3.9622488021850586
INFO:root:current mean train loss 1744.3176050623504
INFO:root:current train perplexity3.964478015899658
INFO:root:current mean train loss 1745.603789012452
INFO:root:current train perplexity3.964923143386841
INFO:root:current mean train loss 1746.4501038883768
INFO:root:current train perplexity3.9663338661193848
INFO:root:current mean train loss 1747.4964449187696
INFO:root:current train perplexity3.9678966999053955
INFO:root:current mean train loss 1747.8146987086916
INFO:root:current train perplexity3.9690165519714355
INFO:root:current mean train loss 1748.3341268007393
INFO:root:current train perplexity3.9696619510650635

100%|██████████| 1/1 [02:18<00:00, 138.10s/it][A100%|██████████| 1/1 [02:18<00:00, 138.10s/it]
INFO:root:final mean train loss: 1748.1690483920452
INFO:root:final train perplexity: 3.969782590866089
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2100.0883118108654
INFO:root:eval perplexity: 5.465349197387695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/87
 44%|████▎     | 87/200 [3:31:25<4:34:24, 145.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1748.7728412334736
INFO:root:current train perplexity3.93000864982605
INFO:root:current mean train loss 1746.8659085048719
INFO:root:current train perplexity3.958223819732666
INFO:root:current mean train loss 1743.4743388882644
INFO:root:current train perplexity3.9449541568756104
INFO:root:current mean train loss 1741.336488431093
INFO:root:current train perplexity3.943782091140747
INFO:root:current mean train loss 1741.885850978197
INFO:root:current train perplexity3.9446020126342773
INFO:root:current mean train loss 1740.7163753311527
INFO:root:current train perplexity3.942232847213745
INFO:root:current mean train loss 1741.2441739332712
INFO:root:current train perplexity3.9438769817352295
INFO:root:current mean train loss 1740.4912059166131
INFO:root:current train perplexity3.943859100341797
INFO:root:current mean train loss 1741.089131626833
INFO:root:current train perplexity3.9512939453125
INFO:root:current mean train loss 1743.7036688244903
INFO:root:current train perplexity3.9542691707611084
INFO:root:current mean train loss 1743.5706145051238
INFO:root:current train perplexity3.9528400897979736
INFO:root:current mean train loss 1743.5014054665946
INFO:root:current train perplexity3.9535272121429443
INFO:root:current mean train loss 1743.6996133293903
INFO:root:current train perplexity3.95310640335083
INFO:root:current mean train loss 1743.973339772882
INFO:root:current train perplexity3.9547019004821777
INFO:root:current mean train loss 1744.575477362647
INFO:root:current train perplexity3.956451892852783
INFO:root:current mean train loss 1744.2674899373217
INFO:root:current train perplexity3.9560599327087402
INFO:root:current mean train loss 1744.2912696592857
INFO:root:current train perplexity3.9578981399536133
INFO:root:current mean train loss 1744.5158716122398
INFO:root:current train perplexity3.9578075408935547
INFO:root:current mean train loss 1745.2989610503403
INFO:root:current train perplexity3.959449291229248
INFO:root:current mean train loss 1745.342130747796
INFO:root:current train perplexity3.959965705871582

100%|██████████| 1/1 [02:18<00:00, 138.25s/it][A100%|██████████| 1/1 [02:18<00:00, 138.25s/it]
INFO:root:final mean train loss: 1744.991707621472
INFO:root:final train perplexity: 3.9598467350006104
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.05s/it][A100%|██████████| 1/1 [00:07<00:00,  7.05s/it]
INFO:root:eval mean loss: 2100.6217300739695
INFO:root:eval perplexity: 5.467707633972168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/88
 44%|████▍     | 88/200 [3:33:51<4:31:54, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.0347193667762
INFO:root:current train perplexity3.9167492389678955
INFO:root:current mean train loss 1730.69353903746
INFO:root:current train perplexity3.90130352973938
INFO:root:current mean train loss 1732.6128662109375
INFO:root:current train perplexity3.9057693481445312
INFO:root:current mean train loss 1732.8560689032831
INFO:root:current train perplexity3.9184346199035645
INFO:root:current mean train loss 1734.5440180614742
INFO:root:current train perplexity3.9205868244171143
INFO:root:current mean train loss 1734.2586528361344
INFO:root:current train perplexity3.9268760681152344
INFO:root:current mean train loss 1736.2887834068683
INFO:root:current train perplexity3.927827835083008
INFO:root:current mean train loss 1738.8227228896424
INFO:root:current train perplexity3.9291584491729736
INFO:root:current mean train loss 1739.0340171089385
INFO:root:current train perplexity3.929283857345581
INFO:root:current mean train loss 1738.5771555531564
INFO:root:current train perplexity3.9322378635406494
INFO:root:current mean train loss 1739.3973710848315
INFO:root:current train perplexity3.9379405975341797
INFO:root:current mean train loss 1739.5347770658996
INFO:root:current train perplexity3.936066150665283
INFO:root:current mean train loss 1739.8868124472128
INFO:root:current train perplexity3.9394614696502686
INFO:root:current mean train loss 1740.4471647310427
INFO:root:current train perplexity3.9403505325317383
INFO:root:current mean train loss 1740.221235335232
INFO:root:current train perplexity3.942448377609253
INFO:root:current mean train loss 1740.2923420968848
INFO:root:current train perplexity3.942540168762207
INFO:root:current mean train loss 1741.4276975738615
INFO:root:current train perplexity3.945261001586914
INFO:root:current mean train loss 1742.5015628400288
INFO:root:current train perplexity3.9481985569000244
INFO:root:current mean train loss 1742.064670854634
INFO:root:current train perplexity3.9480855464935303

100%|██████████| 1/1 [02:18<00:00, 138.07s/it][A100%|██████████| 1/1 [02:18<00:00, 138.07s/it]
INFO:root:final mean train loss: 1741.986474911011
INFO:root:final train perplexity: 3.9504730701446533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.29s/it][A100%|██████████| 1/1 [00:07<00:00,  7.29s/it]
INFO:root:eval mean loss: 2100.954824461159
INFO:root:eval perplexity: 5.469181060791016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/89
 44%|████▍     | 89/200 [3:36:17<4:29:28, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.3622639973958
INFO:root:current train perplexity3.9133920669555664
INFO:root:current mean train loss 1723.383386884417
INFO:root:current train perplexity3.8603053092956543
INFO:root:current mean train loss 1727.2075920824734
INFO:root:current train perplexity3.878213882446289
INFO:root:current mean train loss 1738.640903961964
INFO:root:current train perplexity3.9026989936828613
INFO:root:current mean train loss 1740.352002782729
INFO:root:current train perplexity3.915581703186035
INFO:root:current mean train loss 1740.9348406791687
INFO:root:current train perplexity3.9199278354644775
INFO:root:current mean train loss 1736.545364080691
INFO:root:current train perplexity3.9188013076782227
INFO:root:current mean train loss 1734.056227609013
INFO:root:current train perplexity3.9191133975982666
INFO:root:current mean train loss 1735.558897572785
INFO:root:current train perplexity3.92673659324646
INFO:root:current mean train loss 1735.1205013342071
INFO:root:current train perplexity3.9268975257873535
INFO:root:current mean train loss 1734.4911763187454
INFO:root:current train perplexity3.9280316829681396
INFO:root:current mean train loss 1735.612247274934
INFO:root:current train perplexity3.9283688068389893
INFO:root:current mean train loss 1735.3748627212574
INFO:root:current train perplexity3.9306583404541016
INFO:root:current mean train loss 1736.3755744375833
INFO:root:current train perplexity3.9317808151245117
INFO:root:current mean train loss 1736.113904828728
INFO:root:current train perplexity3.933248519897461
INFO:root:current mean train loss 1738.0702891677777
INFO:root:current train perplexity3.9372310638427734
INFO:root:current mean train loss 1739.5264375369543
INFO:root:current train perplexity3.9391815662384033
INFO:root:current mean train loss 1739.3842414071626
INFO:root:current train perplexity3.9394121170043945
INFO:root:current mean train loss 1739.0547098660838
INFO:root:current train perplexity3.938525676727295
INFO:root:current mean train loss 1739.3725931335193
INFO:root:current train perplexity3.9408371448516846

100%|██████████| 1/1 [02:18<00:00, 138.30s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 1739.2600043177063
INFO:root:final train perplexity: 3.9419875144958496
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2102.7985987020725
INFO:root:eval perplexity: 5.4773430824279785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/90
 45%|████▌     | 90/200 [3:38:42<4:27:05, 145.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1736.315884294181
INFO:root:current train perplexity3.847909927368164
INFO:root:current mean train loss 1731.5065255571706
INFO:root:current train perplexity3.91385817527771
INFO:root:current mean train loss 1733.5819321011872
INFO:root:current train perplexity3.9179913997650146
INFO:root:current mean train loss 1733.8232377350873
INFO:root:current train perplexity3.9205892086029053
INFO:root:current mean train loss 1735.4565799597538
INFO:root:current train perplexity3.9284169673919678
INFO:root:current mean train loss 1740.036827159514
INFO:root:current train perplexity3.9306461811065674
INFO:root:current mean train loss 1738.5788603329318
INFO:root:current train perplexity3.931488275527954
INFO:root:current mean train loss 1738.7984562207969
INFO:root:current train perplexity3.9370174407958984
INFO:root:current mean train loss 1739.1799575566383
INFO:root:current train perplexity3.9357874393463135
INFO:root:current mean train loss 1739.4902107230557
INFO:root:current train perplexity3.9363434314727783
INFO:root:current mean train loss 1738.165860694166
INFO:root:current train perplexity3.935323476791382
INFO:root:current mean train loss 1736.7849577370737
INFO:root:current train perplexity3.933396816253662
INFO:root:current mean train loss 1737.2461867181144
INFO:root:current train perplexity3.9324276447296143
INFO:root:current mean train loss 1736.5208313126059
INFO:root:current train perplexity3.9323513507843018
INFO:root:current mean train loss 1737.7004655073204
INFO:root:current train perplexity3.93426775932312
INFO:root:current mean train loss 1738.0883999831385
INFO:root:current train perplexity3.9346039295196533
INFO:root:current mean train loss 1736.957580903617
INFO:root:current train perplexity3.9347639083862305
INFO:root:current mean train loss 1737.358575435923
INFO:root:current train perplexity3.9342610836029053
INFO:root:current mean train loss 1737.8630091446614
INFO:root:current train perplexity3.9337241649627686
INFO:root:current mean train loss 1737.9292417440222
INFO:root:current train perplexity3.9341132640838623

100%|██████████| 1/1 [02:18<00:00, 138.41s/it][A100%|██████████| 1/1 [02:18<00:00, 138.41s/it]
INFO:root:final mean train loss: 1737.0584226484198
INFO:root:final train perplexity: 3.9351489543914795
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2103.339991359846
INFO:root:eval perplexity: 5.47974157333374
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/91
 46%|████▌     | 91/200 [3:41:08<4:24:43, 145.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1718.1329027258832
INFO:root:current train perplexity3.8982291221618652
INFO:root:current mean train loss 1715.384113468536
INFO:root:current train perplexity3.881744384765625
INFO:root:current mean train loss 1727.7602534100292
INFO:root:current train perplexity3.8993730545043945
INFO:root:current mean train loss 1729.8311185450912
INFO:root:current train perplexity3.909741163253784
INFO:root:current mean train loss 1731.859738473935
INFO:root:current train perplexity3.9172894954681396
INFO:root:current mean train loss 1727.9317673903245
INFO:root:current train perplexity3.9147391319274902
INFO:root:current mean train loss 1730.7319105402235
INFO:root:current train perplexity3.9182982444763184
INFO:root:current mean train loss 1730.0834299859669
INFO:root:current train perplexity3.917490243911743
INFO:root:current mean train loss 1729.7487868000148
INFO:root:current train perplexity3.918832778930664
INFO:root:current mean train loss 1730.8594938443528
INFO:root:current train perplexity3.9205400943756104
INFO:root:current mean train loss 1732.6073785785507
INFO:root:current train perplexity3.921565532684326
INFO:root:current mean train loss 1732.6692895806063
INFO:root:current train perplexity3.9211134910583496
INFO:root:current mean train loss 1732.660864081467
INFO:root:current train perplexity3.9213645458221436
INFO:root:current mean train loss 1732.348011940762
INFO:root:current train perplexity3.922687292098999
INFO:root:current mean train loss 1732.7925202302417
INFO:root:current train perplexity3.9232728481292725
INFO:root:current mean train loss 1733.7087717389381
INFO:root:current train perplexity3.92533016204834
INFO:root:current mean train loss 1733.3646548747438
INFO:root:current train perplexity3.925586700439453
INFO:root:current mean train loss 1733.7346504622171
INFO:root:current train perplexity3.9249160289764404
INFO:root:current mean train loss 1733.9706613327803
INFO:root:current train perplexity3.924506664276123
INFO:root:current mean train loss 1734.008122317715
INFO:root:current train perplexity3.9245216846466064

100%|██████████| 1/1 [02:18<00:00, 138.01s/it][A100%|██████████| 1/1 [02:18<00:00, 138.01s/it]
INFO:root:final mean train loss: 1733.976044732275
INFO:root:final train perplexity: 3.9255945682525635
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2104.1302953408963
INFO:root:eval perplexity: 5.483245372772217
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/92
 46%|████▌     | 92/200 [3:43:34<4:22:08, 145.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.2345745752727
INFO:root:current train perplexity3.8658499717712402
INFO:root:current mean train loss 1706.859323326064
INFO:root:current train perplexity3.8741023540496826
INFO:root:current mean train loss 1714.2239549295984
INFO:root:current train perplexity3.8819146156311035
INFO:root:current mean train loss 1717.0229095374914
INFO:root:current train perplexity3.8864035606384277
INFO:root:current mean train loss 1720.537983904647
INFO:root:current train perplexity3.889909505844116
INFO:root:current mean train loss 1718.8936036890818
INFO:root:current train perplexity3.8940160274505615
INFO:root:current mean train loss 1721.0337726023402
INFO:root:current train perplexity3.8987088203430176
INFO:root:current mean train loss 1721.0969403068173
INFO:root:current train perplexity3.8999345302581787
INFO:root:current mean train loss 1720.2183765751738
INFO:root:current train perplexity3.898542881011963
INFO:root:current mean train loss 1722.2241361782435
INFO:root:current train perplexity3.8978564739227295
INFO:root:current mean train loss 1725.2165935010362
INFO:root:current train perplexity3.899768114089966
INFO:root:current mean train loss 1725.7099013193251
INFO:root:current train perplexity3.902646780014038
INFO:root:current mean train loss 1726.9518422468825
INFO:root:current train perplexity3.9059388637542725
INFO:root:current mean train loss 1728.6973366460988
INFO:root:current train perplexity3.909602165222168
INFO:root:current mean train loss 1729.6877847751036
INFO:root:current train perplexity3.9121737480163574
INFO:root:current mean train loss 1729.168055519109
INFO:root:current train perplexity3.910334348678589
INFO:root:current mean train loss 1729.7810063732006
INFO:root:current train perplexity3.911536455154419
INFO:root:current mean train loss 1730.5515404677972
INFO:root:current train perplexity3.9122097492218018
INFO:root:current mean train loss 1730.515374765688
INFO:root:current train perplexity3.9124386310577393
INFO:root:current mean train loss 1731.128303671636
INFO:root:current train perplexity3.915332794189453

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1730.7025652494444
INFO:root:final train perplexity: 3.915472984313965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2105.0996349145335
INFO:root:eval perplexity: 5.487545490264893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/93
 46%|████▋     | 93/200 [3:45:59<4:19:45, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1709.9381103515625
INFO:root:current train perplexity3.883474826812744
INFO:root:current mean train loss 1716.9671176486545
INFO:root:current train perplexity3.8717527389526367
INFO:root:current mean train loss 1721.039200265067
INFO:root:current train perplexity3.879704713821411
INFO:root:current mean train loss 1723.809237189042
INFO:root:current train perplexity3.887707233428955
INFO:root:current mean train loss 1720.7645614624023
INFO:root:current train perplexity3.889434337615967
INFO:root:current mean train loss 1722.290076314992
INFO:root:current train perplexity3.8936638832092285
INFO:root:current mean train loss 1721.3176709343406
INFO:root:current train perplexity3.8948142528533936
INFO:root:current mean train loss 1723.20212496244
INFO:root:current train perplexity3.8988449573516846
INFO:root:current mean train loss 1724.2141885930841
INFO:root:current train perplexity3.899221897125244
INFO:root:current mean train loss 1724.6942613251356
INFO:root:current train perplexity3.902111291885376
INFO:root:current mean train loss 1726.015130050094
INFO:root:current train perplexity3.903979539871216
INFO:root:current mean train loss 1727.7629386255296
INFO:root:current train perplexity3.9058279991149902
INFO:root:current mean train loss 1727.8503065109253
INFO:root:current train perplexity3.907379627227783
INFO:root:current mean train loss 1728.1778479534646
INFO:root:current train perplexity3.907846450805664
INFO:root:current mean train loss 1727.6258339546823
INFO:root:current train perplexity3.9072980880737305
INFO:root:current mean train loss 1727.9986885939973
INFO:root:current train perplexity3.9068071842193604
INFO:root:current mean train loss 1728.1163227626255
INFO:root:current train perplexity3.9067225456237793
INFO:root:current mean train loss 1728.0880317602264
INFO:root:current train perplexity3.9063873291015625
INFO:root:current mean train loss 1728.0456612444939
INFO:root:current train perplexity3.906881332397461
INFO:root:current mean train loss 1728.8080237186316
INFO:root:current train perplexity3.908338785171509

100%|██████████| 1/1 [02:18<00:00, 138.25s/it][A100%|██████████| 1/1 [02:18<00:00, 138.25s/it]
INFO:root:final mean train loss: 1728.3530638171035
INFO:root:final train perplexity: 3.90822434425354
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2106.3721828595967
INFO:root:eval perplexity: 5.493196487426758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/94
 47%|████▋     | 94/200 [3:48:25<4:17:22, 145.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1720.2550501872583
INFO:root:current train perplexity3.840810775756836
INFO:root:current mean train loss 1718.5775103109138
INFO:root:current train perplexity3.866004705429077
INFO:root:current mean train loss 1713.6746846722433
INFO:root:current train perplexity3.856492280960083
INFO:root:current mean train loss 1719.0529179416917
INFO:root:current train perplexity3.8735463619232178
INFO:root:current mean train loss 1720.7678193182533
INFO:root:current train perplexity3.87601900100708
INFO:root:current mean train loss 1720.4135660398347
INFO:root:current train perplexity3.8786377906799316
INFO:root:current mean train loss 1719.6812518214222
INFO:root:current train perplexity3.8762786388397217
INFO:root:current mean train loss 1720.4649978312225
INFO:root:current train perplexity3.8796792030334473
INFO:root:current mean train loss 1721.34016287473
INFO:root:current train perplexity3.882429599761963
INFO:root:current mean train loss 1720.2481120118166
INFO:root:current train perplexity3.882728338241577
INFO:root:current mean train loss 1721.0185796134344
INFO:root:current train perplexity3.8803749084472656
INFO:root:current mean train loss 1721.6497886358147
INFO:root:current train perplexity3.8824894428253174
INFO:root:current mean train loss 1722.2411402553066
INFO:root:current train perplexity3.8855502605438232
INFO:root:current mean train loss 1723.1607708831984
INFO:root:current train perplexity3.887159824371338
INFO:root:current mean train loss 1723.990010130939
INFO:root:current train perplexity3.887749195098877
INFO:root:current mean train loss 1724.585378896466
INFO:root:current train perplexity3.8912832736968994
INFO:root:current mean train loss 1724.9319682079129
INFO:root:current train perplexity3.895012378692627
INFO:root:current mean train loss 1725.2940734965177
INFO:root:current train perplexity3.89707350730896
INFO:root:current mean train loss 1725.337089542596
INFO:root:current train perplexity3.89668345451355

100%|██████████| 1/1 [02:17<00:00, 137.98s/it][A100%|██████████| 1/1 [02:17<00:00, 137.98s/it]
INFO:root:final mean train loss: 1724.8961111771837
INFO:root:final train perplexity: 3.8975841999053955
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2106.2423879204066
INFO:root:eval perplexity: 5.492619514465332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/95
 48%|████▊     | 95/200 [3:50:50<4:14:48, 145.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1725.6521170479912
INFO:root:current train perplexity3.8052544593811035
INFO:root:current mean train loss 1720.7617262455456
INFO:root:current train perplexity3.8410189151763916
INFO:root:current mean train loss 1721.653347122335
INFO:root:current train perplexity3.855117082595825
INFO:root:current mean train loss 1721.233669791252
INFO:root:current train perplexity3.8571290969848633
INFO:root:current mean train loss 1719.819750504793
INFO:root:current train perplexity3.8630876541137695
INFO:root:current mean train loss 1718.970895730104
INFO:root:current train perplexity3.867549419403076
INFO:root:current mean train loss 1719.9661755888003
INFO:root:current train perplexity3.873095750808716
INFO:root:current mean train loss 1719.3022435292476
INFO:root:current train perplexity3.8775930404663086
INFO:root:current mean train loss 1719.348270950505
INFO:root:current train perplexity3.8800644874572754
INFO:root:current mean train loss 1718.790254622111
INFO:root:current train perplexity3.879929304122925
INFO:root:current mean train loss 1719.6896680120885
INFO:root:current train perplexity3.881702184677124
INFO:root:current mean train loss 1722.0148026142772
INFO:root:current train perplexity3.8857383728027344
INFO:root:current mean train loss 1722.6408732632633
INFO:root:current train perplexity3.886291027069092
INFO:root:current mean train loss 1723.5708641389008
INFO:root:current train perplexity3.887446641921997
INFO:root:current mean train loss 1722.2932706452495
INFO:root:current train perplexity3.885798215866089
INFO:root:current mean train loss 1723.0999513170048
INFO:root:current train perplexity3.887281656265259
INFO:root:current mean train loss 1723.2569643609143
INFO:root:current train perplexity3.888056516647339
INFO:root:current mean train loss 1723.08971014212
INFO:root:current train perplexity3.8904151916503906
INFO:root:current mean train loss 1723.8490053490216
INFO:root:current train perplexity3.8917250633239746
INFO:root:current mean train loss 1723.1398149607946
INFO:root:current train perplexity3.8908274173736572

100%|██████████| 1/1 [02:18<00:00, 138.48s/it][A100%|██████████| 1/1 [02:18<00:00, 138.48s/it]
INFO:root:final mean train loss: 1722.8724063905993
INFO:root:final train perplexity: 3.8913681507110596
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2109.0387755049037
INFO:root:eval perplexity: 5.505055904388428
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/96
 48%|████▊     | 96/200 [3:53:16<4:12:34, 145.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1728.9599018712197
INFO:root:current train perplexity3.8484067916870117
INFO:root:current mean train loss 1711.9794362774333
INFO:root:current train perplexity3.8456344604492188
INFO:root:current mean train loss 1714.2138698297147
INFO:root:current train perplexity3.8740971088409424
INFO:root:current mean train loss 1717.149296048905
INFO:root:current train perplexity3.8724958896636963
INFO:root:current mean train loss 1712.3831925890008
INFO:root:current train perplexity3.866253614425659
INFO:root:current mean train loss 1712.287790761829
INFO:root:current train perplexity3.864150285720825
INFO:root:current mean train loss 1712.6442838206344
INFO:root:current train perplexity3.861539602279663
INFO:root:current mean train loss 1711.3352565113073
INFO:root:current train perplexity3.8575565814971924
INFO:root:current mean train loss 1713.199037627623
INFO:root:current train perplexity3.861257553100586
INFO:root:current mean train loss 1713.980531161889
INFO:root:current train perplexity3.8642866611480713
INFO:root:current mean train loss 1713.964664966128
INFO:root:current train perplexity3.8636372089385986
INFO:root:current mean train loss 1713.701304198787
INFO:root:current train perplexity3.8650240898132324
INFO:root:current mean train loss 1713.337731070871
INFO:root:current train perplexity3.863684892654419
INFO:root:current mean train loss 1714.6160955622536
INFO:root:current train perplexity3.8677592277526855
INFO:root:current mean train loss 1714.9532947553612
INFO:root:current train perplexity3.869953155517578
INFO:root:current mean train loss 1716.5121768924007
INFO:root:current train perplexity3.8716955184936523
INFO:root:current mean train loss 1717.3925642788886
INFO:root:current train perplexity3.874483823776245
INFO:root:current mean train loss 1718.5084799728527
INFO:root:current train perplexity3.876227378845215
INFO:root:current mean train loss 1719.450398918581
INFO:root:current train perplexity3.878221273422241
INFO:root:current mean train loss 1720.1399289273895
INFO:root:current train perplexity3.881873369216919

100%|██████████| 1/1 [02:18<00:00, 138.34s/it][A100%|██████████| 1/1 [02:18<00:00, 138.34s/it]
INFO:root:final mean train loss: 1719.9809171106258
INFO:root:final train perplexity: 3.8825042247772217
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2109.8351825687055
INFO:root:eval perplexity: 5.508601188659668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/97
 48%|████▊     | 97/200 [3:55:42<4:10:09, 145.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.2124659220378
INFO:root:current train perplexity3.8247158527374268
INFO:root:current mean train loss 1707.0805655814506
INFO:root:current train perplexity3.825413942337036
INFO:root:current mean train loss 1707.8452778477822
INFO:root:current train perplexity3.8327507972717285
INFO:root:current mean train loss 1706.6189890894398
INFO:root:current train perplexity3.833189010620117
INFO:root:current mean train loss 1708.620537349156
INFO:root:current train perplexity3.836923599243164
INFO:root:current mean train loss 1707.5849232917285
INFO:root:current train perplexity3.839313268661499
INFO:root:current mean train loss 1709.5529794575255
INFO:root:current train perplexity3.8476476669311523
INFO:root:current mean train loss 1709.8683385186016
INFO:root:current train perplexity3.8494083881378174
INFO:root:current mean train loss 1710.12222966608
INFO:root:current train perplexity3.8538718223571777
INFO:root:current mean train loss 1711.0057302225491
INFO:root:current train perplexity3.8586535453796387
INFO:root:current mean train loss 1711.4053075659367
INFO:root:current train perplexity3.8611249923706055
INFO:root:current mean train loss 1712.97258383721
INFO:root:current train perplexity3.8627331256866455
INFO:root:current mean train loss 1713.8264019305889
INFO:root:current train perplexity3.8618500232696533
INFO:root:current mean train loss 1715.0301994527483
INFO:root:current train perplexity3.8657963275909424
INFO:root:current mean train loss 1714.9468996669707
INFO:root:current train perplexity3.865450382232666
INFO:root:current mean train loss 1715.322222569192
INFO:root:current train perplexity3.868321180343628
INFO:root:current mean train loss 1715.86292940899
INFO:root:current train perplexity3.8697192668914795
INFO:root:current mean train loss 1715.700002988907
INFO:root:current train perplexity3.8711328506469727
INFO:root:current mean train loss 1716.2893961456432
INFO:root:current train perplexity3.8724498748779297
INFO:root:current mean train loss 1717.6790747671891
INFO:root:current train perplexity3.8741836547851562

100%|██████████| 1/1 [02:18<00:00, 138.13s/it][A100%|██████████| 1/1 [02:18<00:00, 138.13s/it]
INFO:root:final mean train loss: 1717.5701977551373
INFO:root:final train perplexity: 3.875129461288452
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2109.9806903812055
INFO:root:eval perplexity: 5.509248733520508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/98
 49%|████▉     | 98/200 [3:58:08<4:07:38, 145.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1705.8348445012018
INFO:root:current train perplexity3.8064000606536865
INFO:root:current mean train loss 1696.3002522786458
INFO:root:current train perplexity3.8128767013549805
INFO:root:current mean train loss 1700.0271254053655
INFO:root:current train perplexity3.8195061683654785
INFO:root:current mean train loss 1703.2490853087543
INFO:root:current train perplexity3.8288328647613525
INFO:root:current mean train loss 1705.143397439936
INFO:root:current train perplexity3.8359079360961914
INFO:root:current mean train loss 1706.2288656319138
INFO:root:current train perplexity3.84169864654541
INFO:root:current mean train loss 1710.5089436237076
INFO:root:current train perplexity3.8483359813690186
INFO:root:current mean train loss 1711.1191468481925
INFO:root:current train perplexity3.853634834289551
INFO:root:current mean train loss 1711.7343253251445
INFO:root:current train perplexity3.853484869003296
INFO:root:current mean train loss 1711.7373718577962
INFO:root:current train perplexity3.8529131412506104
INFO:root:current mean train loss 1712.1050741132997
INFO:root:current train perplexity3.8519325256347656
INFO:root:current mean train loss 1711.2187231759656
INFO:root:current train perplexity3.8535244464874268
INFO:root:current mean train loss 1711.179280856287
INFO:root:current train perplexity3.8542208671569824
INFO:root:current mean train loss 1710.8518368675595
INFO:root:current train perplexity3.854382276535034
INFO:root:current mean train loss 1711.710904586844
INFO:root:current train perplexity3.8570172786712646
INFO:root:current mean train loss 1711.8687125599042
INFO:root:current train perplexity3.857490062713623
INFO:root:current mean train loss 1713.216908094618
INFO:root:current train perplexity3.8585736751556396
INFO:root:current mean train loss 1713.8236364780676
INFO:root:current train perplexity3.8620126247406006
INFO:root:current mean train loss 1714.066351203816
INFO:root:current train perplexity3.8647348880767822
INFO:root:current mean train loss 1714.4980075515864
INFO:root:current train perplexity3.8656375408172607

100%|██████████| 1/1 [02:18<00:00, 138.22s/it][A100%|██████████| 1/1 [02:18<00:00, 138.22s/it]
INFO:root:final mean train loss: 1714.110600443122
INFO:root:final train perplexity: 3.8645713329315186
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2111.5974289914393
INFO:root:eval perplexity: 5.516458988189697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/99
 50%|████▉     | 99/200 [4:00:34<4:05:16, 145.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1698.9914327481897
INFO:root:current train perplexity3.8056704998016357
INFO:root:current mean train loss 1697.7479053539234
INFO:root:current train perplexity3.804633855819702
INFO:root:current mean train loss 1698.7150705756872
INFO:root:current train perplexity3.814351797103882
INFO:root:current mean train loss 1700.99211687817
INFO:root:current train perplexity3.822526693344116
INFO:root:current mean train loss 1698.7243490258688
INFO:root:current train perplexity3.824998617172241
INFO:root:current mean train loss 1700.7138120251423
INFO:root:current train perplexity3.8282315731048584
INFO:root:current mean train loss 1701.8722158303358
INFO:root:current train perplexity3.8323988914489746
INFO:root:current mean train loss 1703.2727078879275
INFO:root:current train perplexity3.8318309783935547
INFO:root:current mean train loss 1704.6600531407225
INFO:root:current train perplexity3.836771249771118
INFO:root:current mean train loss 1706.333723950046
INFO:root:current train perplexity3.8386006355285645
INFO:root:current mean train loss 1705.7022458004203
INFO:root:current train perplexity3.840369701385498
INFO:root:current mean train loss 1707.052030456853
INFO:root:current train perplexity3.842808485031128
INFO:root:current mean train loss 1707.4084162243444
INFO:root:current train perplexity3.8431973457336426
INFO:root:current mean train loss 1708.76412941785
INFO:root:current train perplexity3.8462846279144287
INFO:root:current mean train loss 1709.9271830817465
INFO:root:current train perplexity3.8484933376312256
INFO:root:current mean train loss 1710.3108138555824
INFO:root:current train perplexity3.850938081741333
INFO:root:current mean train loss 1711.2477094761398
INFO:root:current train perplexity3.855435848236084
INFO:root:current mean train loss 1711.6938127888038
INFO:root:current train perplexity3.856104612350464
INFO:root:current mean train loss 1710.969199753213
INFO:root:current train perplexity3.8554956912994385
INFO:root:current mean train loss 1711.7178505475056
INFO:root:current train perplexity3.855858325958252

100%|██████████| 1/1 [02:18<00:00, 138.48s/it][A100%|██████████| 1/1 [02:18<00:00, 138.49s/it]
INFO:root:final mean train loss: 1711.2735861943218
INFO:root:final train perplexity: 3.8559341430664062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2112.2656427478114
INFO:root:eval perplexity: 5.519440650939941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/100
 50%|█████     | 100/200 [4:03:00<4:02:59, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.2488508128156
INFO:root:current train perplexity3.8177645206451416
INFO:root:current mean train loss 1705.8304283870525
INFO:root:current train perplexity3.8224122524261475
INFO:root:current mean train loss 1704.542696847565
INFO:root:current train perplexity3.827056407928467
INFO:root:current mean train loss 1703.5528787789788
INFO:root:current train perplexity3.8273684978485107
INFO:root:current mean train loss 1703.4788263049536
INFO:root:current train perplexity3.8309335708618164
INFO:root:current mean train loss 1701.5339991294084
INFO:root:current train perplexity3.834420680999756
INFO:root:current mean train loss 1702.706232815853
INFO:root:current train perplexity3.8340137004852295
INFO:root:current mean train loss 1704.3774171144105
INFO:root:current train perplexity3.832221746444702
INFO:root:current mean train loss 1703.4732031901765
INFO:root:current train perplexity3.8348753452301025
INFO:root:current mean train loss 1707.1371696403435
INFO:root:current train perplexity3.8434951305389404
INFO:root:current mean train loss 1707.0681629961898
INFO:root:current train perplexity3.843057632446289
INFO:root:current mean train loss 1706.8345807785786
INFO:root:current train perplexity3.8406224250793457
INFO:root:current mean train loss 1707.0539687041414
INFO:root:current train perplexity3.8417301177978516
INFO:root:current mean train loss 1707.9647811878742
INFO:root:current train perplexity3.8429136276245117
INFO:root:current mean train loss 1708.252435705835
INFO:root:current train perplexity3.84489107131958
INFO:root:current mean train loss 1708.0590159193735
INFO:root:current train perplexity3.847017765045166
INFO:root:current mean train loss 1708.2629449135982
INFO:root:current train perplexity3.8478667736053467
INFO:root:current mean train loss 1708.7002713095817
INFO:root:current train perplexity3.8483660221099854
INFO:root:current mean train loss 1709.1859751817362
INFO:root:current train perplexity3.8487472534179688

100%|██████████| 1/1 [02:18<00:00, 138.43s/it][A100%|██████████| 1/1 [02:18<00:00, 138.43s/it]
INFO:root:final mean train loss: 1709.1108822882686
INFO:root:final train perplexity: 3.849362850189209
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2112.884147048842
INFO:root:eval perplexity: 5.522202968597412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/101
 50%|█████     | 101/200 [4:05:25<4:00:36, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1699.5861740112305
INFO:root:current train perplexity3.863705635070801
INFO:root:current mean train loss 1699.3104847875134
INFO:root:current train perplexity3.8442790508270264
INFO:root:current mean train loss 1701.3928567391854
INFO:root:current train perplexity3.8385612964630127
INFO:root:current mean train loss 1703.914686758307
INFO:root:current train perplexity3.838379383087158
INFO:root:current mean train loss 1706.924138876108
INFO:root:current train perplexity3.8434553146362305
INFO:root:current mean train loss 1706.902582212936
INFO:root:current train perplexity3.8420653343200684
INFO:root:current mean train loss 1707.5349434196175
INFO:root:current train perplexity3.837977170944214
INFO:root:current mean train loss 1707.6014552622535
INFO:root:current train perplexity3.834505558013916
INFO:root:current mean train loss 1705.3117527681238
INFO:root:current train perplexity3.827282428741455
INFO:root:current mean train loss 1706.1419300595746
INFO:root:current train perplexity3.830517053604126
INFO:root:current mean train loss 1705.9034374567468
INFO:root:current train perplexity3.831845283508301
INFO:root:current mean train loss 1705.6661811199667
INFO:root:current train perplexity3.8330304622650146
INFO:root:current mean train loss 1706.6778731095164
INFO:root:current train perplexity3.8348681926727295
INFO:root:current mean train loss 1705.9754175806481
INFO:root:current train perplexity3.8338077068328857
INFO:root:current mean train loss 1706.0156923283291
INFO:root:current train perplexity3.8352837562561035
INFO:root:current mean train loss 1706.0443620103015
INFO:root:current train perplexity3.838016986846924
INFO:root:current mean train loss 1705.7250734839108
INFO:root:current train perplexity3.8397276401519775
INFO:root:current mean train loss 1706.381819647231
INFO:root:current train perplexity3.8408710956573486
INFO:root:current mean train loss 1706.9746532020065
INFO:root:current train perplexity3.8424506187438965
INFO:root:current mean train loss 1707.8136815845594
INFO:root:current train perplexity3.8443992137908936

100%|██████████| 1/1 [02:18<00:00, 138.79s/it][A100%|██████████| 1/1 [02:18<00:00, 138.79s/it]
INFO:root:final mean train loss: 1707.6344117257431
INFO:root:final train perplexity: 3.844883680343628
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2114.0461668190383
INFO:root:eval perplexity: 5.527394771575928
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/102
 51%|█████     | 102/200 [4:07:52<3:58:26, 145.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1681.5314090613163
INFO:root:current train perplexity3.771336793899536
INFO:root:current mean train loss 1688.9100461113721
INFO:root:current train perplexity3.7894935607910156
INFO:root:current mean train loss 1690.0719937173083
INFO:root:current train perplexity3.7910847663879395
INFO:root:current mean train loss 1692.980579089832
INFO:root:current train perplexity3.78444766998291
INFO:root:current mean train loss 1699.4423969083791
INFO:root:current train perplexity3.800962448120117
INFO:root:current mean train loss 1699.2658624989008
INFO:root:current train perplexity3.8072550296783447
INFO:root:current mean train loss 1701.916152929996
INFO:root:current train perplexity3.8116581439971924
INFO:root:current mean train loss 1699.32246713261
INFO:root:current train perplexity3.812241554260254
INFO:root:current mean train loss 1700.2037004743304
INFO:root:current train perplexity3.816850423812866
INFO:root:current mean train loss 1700.8988618022759
INFO:root:current train perplexity3.8184545040130615
INFO:root:current mean train loss 1700.7685616595702
INFO:root:current train perplexity3.8206050395965576
INFO:root:current mean train loss 1702.0504201028796
INFO:root:current train perplexity3.8221142292022705
INFO:root:current mean train loss 1702.3246351552996
INFO:root:current train perplexity3.825446367263794
INFO:root:current mean train loss 1702.8103597859915
INFO:root:current train perplexity3.826936960220337
INFO:root:current mean train loss 1703.9216565852887
INFO:root:current train perplexity3.8304176330566406
INFO:root:current mean train loss 1703.8697402267308
INFO:root:current train perplexity3.831794500350952
INFO:root:current mean train loss 1704.2588268871039
INFO:root:current train perplexity3.832343578338623
INFO:root:current mean train loss 1704.3854120411904
INFO:root:current train perplexity3.8317222595214844
INFO:root:current mean train loss 1704.507266879667
INFO:root:current train perplexity3.8341948986053467
INFO:root:current mean train loss 1704.9019718554991
INFO:root:current train perplexity3.834810972213745

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1704.2703470527315
INFO:root:final train perplexity: 3.834695816040039
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2115.933345713514
INFO:root:eval perplexity: 5.5358357429504395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/103
 52%|█████▏    | 103/200 [4:10:18<3:55:54, 145.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1697.605068359375
INFO:root:current train perplexity3.8321449756622314
INFO:root:current mean train loss 1706.74044921875
INFO:root:current train perplexity3.8050196170806885
INFO:root:current mean train loss 1699.7174809570313
INFO:root:current train perplexity3.8086771965026855
INFO:root:current mean train loss 1696.66693359375
INFO:root:current train perplexity3.816196918487549
INFO:root:current mean train loss 1694.1978439670138
INFO:root:current train perplexity3.8180994987487793
INFO:root:current mean train loss 1696.0011150568182
INFO:root:current train perplexity3.8140811920166016
INFO:root:current mean train loss 1697.087724233774
INFO:root:current train perplexity3.8151090145111084
INFO:root:current mean train loss 1699.3169332682291
INFO:root:current train perplexity3.8189480304718018
INFO:root:current mean train loss 1700.3346883616728
INFO:root:current train perplexity3.820488929748535
INFO:root:current mean train loss 1701.4670796926398
INFO:root:current train perplexity3.8222031593322754
INFO:root:current mean train loss 1700.593727794829
INFO:root:current train perplexity3.822641372680664
INFO:root:current mean train loss 1700.642737665591
INFO:root:current train perplexity3.8243470191955566
INFO:root:current mean train loss 1700.6152961914063
INFO:root:current train perplexity3.8252170085906982
INFO:root:current mean train loss 1700.820452383536
INFO:root:current train perplexity3.82409405708313
INFO:root:current mean train loss 1700.3175012627964
INFO:root:current train perplexity3.824598789215088
INFO:root:current mean train loss 1700.5338304876511
INFO:root:current train perplexity3.826317548751831
INFO:root:current mean train loss 1701.4800307765152
INFO:root:current train perplexity3.8287055492401123
INFO:root:current mean train loss 1701.4080424804688
INFO:root:current train perplexity3.8291232585906982
INFO:root:current mean train loss 1702.1326909575591
INFO:root:current train perplexity3.8298463821411133
INFO:root:current mean train loss 1702.6482542067308
INFO:root:current train perplexity3.829874277114868

100%|██████████| 1/1 [02:18<00:00, 138.30s/it][A100%|██████████| 1/1 [02:18<00:00, 138.30s/it]
INFO:root:final mean train loss: 1702.8629835227855
INFO:root:final train perplexity: 3.8304412364959717
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.13s/it][A100%|██████████| 1/1 [00:07<00:00,  7.13s/it]
INFO:root:eval mean loss: 2117.065498947252
INFO:root:eval perplexity: 5.540907859802246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/104
 52%|█████▏    | 104/200 [4:12:43<3:53:22, 145.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1707.5508996764227
INFO:root:current train perplexity3.792691230773926
INFO:root:current mean train loss 1697.8803857129492
INFO:root:current train perplexity3.786087989807129
INFO:root:current mean train loss 1699.9421994784352
INFO:root:current train perplexity3.8010470867156982
INFO:root:current mean train loss 1698.293505260665
INFO:root:current train perplexity3.804290533065796
INFO:root:current mean train loss 1699.2083024018837
INFO:root:current train perplexity3.8084230422973633
INFO:root:current mean train loss 1695.4558010740466
INFO:root:current train perplexity3.8064236640930176
INFO:root:current mean train loss 1695.4759614821496
INFO:root:current train perplexity3.806513547897339
INFO:root:current mean train loss 1695.7522397594626
INFO:root:current train perplexity3.807265043258667
INFO:root:current mean train loss 1696.1768780524617
INFO:root:current train perplexity3.8069331645965576
INFO:root:current mean train loss 1698.377209384291
INFO:root:current train perplexity3.8058061599731445
INFO:root:current mean train loss 1698.6833690582532
INFO:root:current train perplexity3.8076236248016357
INFO:root:current mean train loss 1698.8178665958721
INFO:root:current train perplexity3.8086671829223633
INFO:root:current mean train loss 1699.9825512676043
INFO:root:current train perplexity3.8125860691070557
INFO:root:current mean train loss 1699.1825316400534
INFO:root:current train perplexity3.814405918121338
INFO:root:current mean train loss 1699.7675076454127
INFO:root:current train perplexity3.817443609237671
INFO:root:current mean train loss 1699.1018018886855
INFO:root:current train perplexity3.816492795944214
INFO:root:current mean train loss 1700.411487453486
INFO:root:current train perplexity3.8164522647857666
INFO:root:current mean train loss 1699.5660249788882
INFO:root:current train perplexity3.8162178993225098
INFO:root:current mean train loss 1699.723512376766
INFO:root:current train perplexity3.817164659500122
INFO:root:current mean train loss 1699.126823793769
INFO:root:current train perplexity3.817394971847534

100%|██████████| 1/1 [02:18<00:00, 138.50s/it][A100%|██████████| 1/1 [02:18<00:00, 138.50s/it]
INFO:root:final mean train loss: 1698.7784587881267
INFO:root:final train perplexity: 3.818122386932373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2118.0454097233765
INFO:root:eval perplexity: 5.545300483703613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/105
 52%|█████▎    | 105/200 [4:15:09<3:50:59, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1702.4492565336682
INFO:root:current train perplexity3.804900646209717
INFO:root:current mean train loss 1692.684578937033
INFO:root:current train perplexity3.803258180618286
INFO:root:current mean train loss 1692.1853104712259
INFO:root:current train perplexity3.804292917251587
INFO:root:current mean train loss 1693.1383609771729
INFO:root:current train perplexity3.796459436416626
INFO:root:current mean train loss 1692.6823435381425
INFO:root:current train perplexity3.795469284057617
INFO:root:current mean train loss 1694.9134500581924
INFO:root:current train perplexity3.79656720161438
INFO:root:current mean train loss 1697.083477711817
INFO:root:current train perplexity3.7971854209899902
INFO:root:current mean train loss 1697.0788279942103
INFO:root:current train perplexity3.798610210418701
INFO:root:current mean train loss 1696.0285719099088
INFO:root:current train perplexity3.7980825901031494
INFO:root:current mean train loss 1695.8274054798653
INFO:root:current train perplexity3.799473285675049
INFO:root:current mean train loss 1697.0129025167205
INFO:root:current train perplexity3.8018674850463867
INFO:root:current mean train loss 1697.0481922046558
INFO:root:current train perplexity3.8043923377990723
INFO:root:current mean train loss 1696.7537523311246
INFO:root:current train perplexity3.8043265342712402
INFO:root:current mean train loss 1697.509087358596
INFO:root:current train perplexity3.8072218894958496
INFO:root:current mean train loss 1698.1361998689142
INFO:root:current train perplexity3.808206796646118
INFO:root:current mean train loss 1698.5634112887913
INFO:root:current train perplexity3.809553623199463
INFO:root:current mean train loss 1698.533607899718
INFO:root:current train perplexity3.810577869415283
INFO:root:current mean train loss 1698.094491111858
INFO:root:current train perplexity3.812199831008911
INFO:root:current mean train loss 1697.7954637401929
INFO:root:current train perplexity3.8133912086486816

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1697.2183100728753
INFO:root:final train perplexity: 3.813427448272705
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2119.467808933123
INFO:root:eval perplexity: 5.55168342590332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/106
 53%|█████▎    | 106/200 [4:17:35<3:48:28, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1675.4354248046875
INFO:root:current train perplexity3.690910816192627
INFO:root:current mean train loss 1675.956625154703
INFO:root:current train perplexity3.77162504196167
INFO:root:current mean train loss 1675.1009661166822
INFO:root:current train perplexity3.7796802520751953
INFO:root:current mean train loss 1677.3506832693106
INFO:root:current train perplexity3.777324914932251
INFO:root:current mean train loss 1683.2166032672226
INFO:root:current train perplexity3.791565179824829
INFO:root:current mean train loss 1684.2557548477264
INFO:root:current train perplexity3.7916557788848877
INFO:root:current mean train loss 1688.0914865198627
INFO:root:current train perplexity3.7924530506134033
INFO:root:current mean train loss 1686.6330456699693
INFO:root:current train perplexity3.7926831245422363
INFO:root:current mean train loss 1686.9200238288565
INFO:root:current train perplexity3.790881395339966
INFO:root:current mean train loss 1687.807298531146
INFO:root:current train perplexity3.793972969055176
INFO:root:current mean train loss 1687.8723557936205
INFO:root:current train perplexity3.7926154136657715
INFO:root:current mean train loss 1686.767955755776
INFO:root:current train perplexity3.7938592433929443
INFO:root:current mean train loss 1688.1296407046862
INFO:root:current train perplexity3.793853521347046
INFO:root:current mean train loss 1689.5559295020958
INFO:root:current train perplexity3.795820474624634
INFO:root:current mean train loss 1690.2233507699577
INFO:root:current train perplexity3.795840263366699
INFO:root:current mean train loss 1691.2394203831561
INFO:root:current train perplexity3.7969789505004883
INFO:root:current mean train loss 1692.5426013953681
INFO:root:current train perplexity3.801626682281494
INFO:root:current mean train loss 1694.0018652142812
INFO:root:current train perplexity3.804986000061035
INFO:root:current mean train loss 1694.5330845114258
INFO:root:current train perplexity3.805115222930908
INFO:root:current mean train loss 1695.0865551077147
INFO:root:current train perplexity3.8059308528900146

100%|██████████| 1/1 [02:18<00:00, 138.33s/it][A100%|██████████| 1/1 [02:18<00:00, 138.33s/it]
INFO:root:final mean train loss: 1695.1116399183095
INFO:root:final train perplexity: 3.8070974349975586
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.30s/it][A100%|██████████| 1/1 [00:07<00:00,  7.30s/it]
INFO:root:eval mean loss: 2119.262862401651
INFO:root:eval perplexity: 5.55076265335083
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/107
 54%|█████▎    | 107/200 [4:20:01<3:46:04, 145.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1757.5420600043403
INFO:root:current train perplexity3.8346810340881348
INFO:root:current mean train loss 1691.3932970984508
INFO:root:current train perplexity3.7861275672912598
INFO:root:current mean train loss 1693.065303137543
INFO:root:current train perplexity3.790440320968628
INFO:root:current mean train loss 1688.1771935037098
INFO:root:current train perplexity3.777787446975708
INFO:root:current mean train loss 1687.8906001770895
INFO:root:current train perplexity3.7773971557617188
INFO:root:current mean train loss 1688.7797472154773
INFO:root:current train perplexity3.779249668121338
INFO:root:current mean train loss 1687.9231944655137
INFO:root:current train perplexity3.7796790599823
INFO:root:current mean train loss 1686.5968774142038
INFO:root:current train perplexity3.7818422317504883
INFO:root:current mean train loss 1686.2160196840618
INFO:root:current train perplexity3.7881014347076416
INFO:root:current mean train loss 1686.6732970260587
INFO:root:current train perplexity3.7875068187713623
INFO:root:current mean train loss 1687.1580398049946
INFO:root:current train perplexity3.788705348968506
INFO:root:current mean train loss 1686.7475750808853
INFO:root:current train perplexity3.7866737842559814
INFO:root:current mean train loss 1687.4472734423107
INFO:root:current train perplexity3.787040948867798
INFO:root:current mean train loss 1688.913227364941
INFO:root:current train perplexity3.7892706394195557
INFO:root:current mean train loss 1690.4518980986645
INFO:root:current train perplexity3.7910780906677246
INFO:root:current mean train loss 1689.623513444139
INFO:root:current train perplexity3.7918002605438232
INFO:root:current mean train loss 1690.0276299890393
INFO:root:current train perplexity3.7940733432769775
INFO:root:current mean train loss 1691.3394828680925
INFO:root:current train perplexity3.7961349487304688
INFO:root:current mean train loss 1691.7594860987563
INFO:root:current train perplexity3.7982773780822754
INFO:root:current mean train loss 1692.5279327169821
INFO:root:current train perplexity3.799557685852051

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1692.7171855855336
INFO:root:final train perplexity: 3.799914598464966
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2120.3265186066324
INFO:root:eval perplexity: 5.555539608001709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/108
 54%|█████▍    | 108/200 [4:22:27<3:43:34, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1664.6960867745536
INFO:root:current train perplexity3.757443428039551
INFO:root:current mean train loss 1673.5771068431714
INFO:root:current train perplexity3.756420612335205
INFO:root:current mean train loss 1685.1694611245014
INFO:root:current train perplexity3.767885684967041
INFO:root:current mean train loss 1686.1668529909048
INFO:root:current train perplexity3.775656223297119
INFO:root:current mean train loss 1686.8562095905172
INFO:root:current train perplexity3.7799601554870605
INFO:root:current mean train loss 1690.271111547167
INFO:root:current train perplexity3.788480758666992
INFO:root:current mean train loss 1690.7340072511688
INFO:root:current train perplexity3.7866549491882324
INFO:root:current mean train loss 1690.4227437752445
INFO:root:current train perplexity3.783731460571289
INFO:root:current mean train loss 1691.0105860544536
INFO:root:current train perplexity3.7811458110809326
INFO:root:current mean train loss 1689.5232078511447
INFO:root:current train perplexity3.7801272869110107
INFO:root:current mean train loss 1691.0363405089447
INFO:root:current train perplexity3.7848997116088867
INFO:root:current mean train loss 1691.5140071112678
INFO:root:current train perplexity3.7876977920532227
INFO:root:current mean train loss 1690.3382901458122
INFO:root:current train perplexity3.7863614559173584
INFO:root:current mean train loss 1691.4210730849134
INFO:root:current train perplexity3.788254976272583
INFO:root:current mean train loss 1691.0861916784625
INFO:root:current train perplexity3.7875285148620605
INFO:root:current mean train loss 1691.2672952558785
INFO:root:current train perplexity3.79013991355896
INFO:root:current mean train loss 1691.4783140409977
INFO:root:current train perplexity3.787703275680542
INFO:root:current mean train loss 1691.3299307963346
INFO:root:current train perplexity3.7897820472717285
INFO:root:current mean train loss 1691.4149822515753
INFO:root:current train perplexity3.790454864501953
INFO:root:current mean train loss 1691.0882781588139
INFO:root:current train perplexity3.7911593914031982

100%|██████████| 1/1 [02:18<00:00, 138.68s/it][A100%|██████████| 1/1 [02:18<00:00, 138.68s/it]
INFO:root:final mean train loss: 1690.4486566602252
INFO:root:final train perplexity: 3.7931222915649414
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2121.472666638963
INFO:root:eval perplexity: 5.5606913566589355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/109
 55%|█████▍    | 109/200 [4:24:53<3:41:18, 145.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1675.0311772273137
INFO:root:current train perplexity3.716343402862549
INFO:root:current mean train loss 1682.6203797992907
INFO:root:current train perplexity3.7709169387817383
INFO:root:current mean train loss 1678.3833458310082
INFO:root:current train perplexity3.760866403579712
INFO:root:current mean train loss 1679.44434113936
INFO:root:current train perplexity3.7632899284362793
INFO:root:current mean train loss 1683.4099901587563
INFO:root:current train perplexity3.770953893661499
INFO:root:current mean train loss 1684.167741637299
INFO:root:current train perplexity3.7726922035217285
INFO:root:current mean train loss 1682.83047036013
INFO:root:current train perplexity3.7715423107147217
INFO:root:current mean train loss 1684.8056244545794
INFO:root:current train perplexity3.7721002101898193
INFO:root:current mean train loss 1684.561014811198
INFO:root:current train perplexity3.773134231567383
INFO:root:current mean train loss 1685.0219576539112
INFO:root:current train perplexity3.773789644241333
INFO:root:current mean train loss 1684.438604318597
INFO:root:current train perplexity3.7727279663085938
INFO:root:current mean train loss 1684.7905909220378
INFO:root:current train perplexity3.776416778564453
INFO:root:current mean train loss 1684.886067545833
INFO:root:current train perplexity3.776381254196167
INFO:root:current mean train loss 1685.7361510688736
INFO:root:current train perplexity3.779672145843506
INFO:root:current mean train loss 1685.5834922265087
INFO:root:current train perplexity3.778501272201538
INFO:root:current mean train loss 1685.8444281509242
INFO:root:current train perplexity3.7792062759399414
INFO:root:current mean train loss 1686.3264874696154
INFO:root:current train perplexity3.7812352180480957
INFO:root:current mean train loss 1686.2909072109553
INFO:root:current train perplexity3.781160831451416
INFO:root:current mean train loss 1687.2148745312288
INFO:root:current train perplexity3.7822723388671875
INFO:root:current mean train loss 1687.27104449663
INFO:root:current train perplexity3.7833290100097656

100%|██████████| 1/1 [02:18<00:00, 138.01s/it][A100%|██████████| 1/1 [02:18<00:00, 138.01s/it]
INFO:root:final mean train loss: 1687.3608263009014
INFO:root:final train perplexity: 3.7838969230651855
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2123.0437557139294
INFO:root:eval perplexity: 5.567761421203613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/110
 55%|█████▌    | 110/200 [4:27:18<3:38:41, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1670.7232170657835
INFO:root:current train perplexity3.7774858474731445
INFO:root:current mean train loss 1674.6685841577294
INFO:root:current train perplexity3.751795530319214
INFO:root:current mean train loss 1680.774578789353
INFO:root:current train perplexity3.759629964828491
INFO:root:current mean train loss 1678.698114493352
INFO:root:current train perplexity3.754502296447754
INFO:root:current mean train loss 1681.3346355901851
INFO:root:current train perplexity3.7633419036865234
INFO:root:current mean train loss 1682.5381166159793
INFO:root:current train perplexity3.7663655281066895
INFO:root:current mean train loss 1683.7203268783867
INFO:root:current train perplexity3.7684013843536377
INFO:root:current mean train loss 1682.650918749746
INFO:root:current train perplexity3.7693514823913574
INFO:root:current mean train loss 1683.3387070492304
INFO:root:current train perplexity3.7720940113067627
INFO:root:current mean train loss 1684.4782134096442
INFO:root:current train perplexity3.7738144397735596
INFO:root:current mean train loss 1683.469191805462
INFO:root:current train perplexity3.7708916664123535
INFO:root:current mean train loss 1682.6266843405822
INFO:root:current train perplexity3.769841432571411
INFO:root:current mean train loss 1683.322235708635
INFO:root:current train perplexity3.7731053829193115
INFO:root:current mean train loss 1683.075199770875
INFO:root:current train perplexity3.7725696563720703
INFO:root:current mean train loss 1683.6840007618384
INFO:root:current train perplexity3.7752187252044678
INFO:root:current mean train loss 1684.2614823117083
INFO:root:current train perplexity3.775649309158325
INFO:root:current mean train loss 1684.6046585805263
INFO:root:current train perplexity3.77423357963562
INFO:root:current mean train loss 1685.7199966491044
INFO:root:current train perplexity3.775782585144043
INFO:root:current mean train loss 1686.3289416105497
INFO:root:current train perplexity3.7771525382995605
INFO:root:current mean train loss 1685.7603027591733
INFO:root:current train perplexity3.7773618698120117

100%|██████████| 1/1 [02:18<00:00, 138.27s/it][A100%|██████████| 1/1 [02:18<00:00, 138.27s/it]
INFO:root:final mean train loss: 1685.1557327247424
INFO:root:final train perplexity: 3.7773213386535645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2124.354890431073
INFO:root:eval perplexity: 5.573668479919434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/111
 56%|█████▌    | 111/200 [4:29:44<3:36:15, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1690.1620128542877
INFO:root:current train perplexity3.7443020343780518
INFO:root:current mean train loss 1691.4827592090894
INFO:root:current train perplexity3.760986804962158
INFO:root:current mean train loss 1684.8216860044254
INFO:root:current train perplexity3.754681348800659
INFO:root:current mean train loss 1683.122679082841
INFO:root:current train perplexity3.754929780960083
INFO:root:current mean train loss 1684.6727784710165
INFO:root:current train perplexity3.7592697143554688
INFO:root:current mean train loss 1686.4169421928327
INFO:root:current train perplexity3.7641701698303223
INFO:root:current mean train loss 1685.548552488099
INFO:root:current train perplexity3.7643625736236572
INFO:root:current mean train loss 1684.50081100658
INFO:root:current train perplexity3.7636308670043945
INFO:root:current mean train loss 1683.3778709945507
INFO:root:current train perplexity3.7647907733917236
INFO:root:current mean train loss 1683.70053762935
INFO:root:current train perplexity3.7643442153930664
INFO:root:current mean train loss 1683.8831154277093
INFO:root:current train perplexity3.766408681869507
INFO:root:current mean train loss 1684.4630824791843
INFO:root:current train perplexity3.767456293106079
INFO:root:current mean train loss 1683.62977460087
INFO:root:current train perplexity3.767190456390381
INFO:root:current mean train loss 1684.4292196518759
INFO:root:current train perplexity3.768756151199341
INFO:root:current mean train loss 1684.2970694417427
INFO:root:current train perplexity3.7699060440063477
INFO:root:current mean train loss 1684.8057698157215
INFO:root:current train perplexity3.7718331813812256
INFO:root:current mean train loss 1684.214202120635
INFO:root:current train perplexity3.773000717163086
INFO:root:current mean train loss 1684.348662680768
INFO:root:current train perplexity3.7734732627868652
INFO:root:current mean train loss 1683.4766523375365
INFO:root:current train perplexity3.7721686363220215

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1683.7832051564274
INFO:root:final train perplexity: 3.773235321044922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2124.602079783771
INFO:root:eval perplexity: 5.574783802032471
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/112
 56%|█████▌    | 112/200 [4:32:10<3:33:46, 145.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1625.2417399088542
INFO:root:current train perplexity3.678941011428833
INFO:root:current mean train loss 1680.46433769152
INFO:root:current train perplexity3.7644124031066895
INFO:root:current mean train loss 1681.7786841181112
INFO:root:current train perplexity3.7733356952667236
INFO:root:current mean train loss 1679.0776673370463
INFO:root:current train perplexity3.7624478340148926
INFO:root:current mean train loss 1680.0688800669782
INFO:root:current train perplexity3.7523505687713623
INFO:root:current mean train loss 1679.3550309471298
INFO:root:current train perplexity3.752514362335205
INFO:root:current mean train loss 1679.5529752766117
INFO:root:current train perplexity3.7567689418792725
INFO:root:current mean train loss 1679.79803215016
INFO:root:current train perplexity3.7571921348571777
INFO:root:current mean train loss 1681.7299766683045
INFO:root:current train perplexity3.7555673122406006
INFO:root:current mean train loss 1679.8089817254117
INFO:root:current train perplexity3.7557010650634766
INFO:root:current mean train loss 1679.670369385009
INFO:root:current train perplexity3.757936716079712
INFO:root:current mean train loss 1679.975258682818
INFO:root:current train perplexity3.7559003829956055
INFO:root:current mean train loss 1679.928199419258
INFO:root:current train perplexity3.756685256958008
INFO:root:current mean train loss 1679.679071808447
INFO:root:current train perplexity3.7560386657714844
INFO:root:current mean train loss 1679.8674757529902
INFO:root:current train perplexity3.7574167251586914
INFO:root:current mean train loss 1680.413153510687
INFO:root:current train perplexity3.759291172027588
INFO:root:current mean train loss 1681.2204624873284
INFO:root:current train perplexity3.7596187591552734
INFO:root:current mean train loss 1680.6947927514175
INFO:root:current train perplexity3.761077404022217
INFO:root:current mean train loss 1681.2787163809016
INFO:root:current train perplexity3.7623708248138428
INFO:root:current mean train loss 1681.5984152284223
INFO:root:current train perplexity3.7638227939605713

100%|██████████| 1/1 [02:17<00:00, 138.00s/it][A100%|██████████| 1/1 [02:17<00:00, 138.00s/it]
INFO:root:final mean train loss: 1681.4519725959228
INFO:root:final train perplexity: 3.7663040161132812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2125.296458142869
INFO:root:eval perplexity: 5.577915191650391
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/113
 56%|█████▋    | 113/200 [4:34:35<3:31:13, 145.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1685.8050964355468
INFO:root:current train perplexity3.7043793201446533
INFO:root:current mean train loss 1670.9740091959636
INFO:root:current train perplexity3.6776938438415527
INFO:root:current mean train loss 1665.3227000843394
INFO:root:current train perplexity3.7074151039123535
INFO:root:current mean train loss 1671.2410362243652
INFO:root:current train perplexity3.7151689529418945
INFO:root:current mean train loss 1670.746480015346
INFO:root:current train perplexity3.724149227142334
INFO:root:current mean train loss 1672.4053067720854
INFO:root:current train perplexity3.7319180965423584
INFO:root:current mean train loss 1672.4074122275076
INFO:root:current train perplexity3.7333199977874756
INFO:root:current mean train loss 1672.4239778306749
INFO:root:current train perplexity3.7335104942321777
INFO:root:current mean train loss 1673.5166722739616
INFO:root:current train perplexity3.735522747039795
INFO:root:current mean train loss 1675.3313406239386
INFO:root:current train perplexity3.7410411834716797
INFO:root:current mean train loss 1675.912859509038
INFO:root:current train perplexity3.7455387115478516
INFO:root:current mean train loss 1676.7711625235422
INFO:root:current train perplexity3.7467899322509766
INFO:root:current mean train loss 1678.0112047539383
INFO:root:current train perplexity3.7484676837921143
INFO:root:current mean train loss 1677.4084542014382
INFO:root:current train perplexity3.748572587966919
INFO:root:current mean train loss 1676.799422916896
INFO:root:current train perplexity3.750359058380127
INFO:root:current mean train loss 1676.9773574026008
INFO:root:current train perplexity3.752429723739624
INFO:root:current mean train loss 1677.6598766185618
INFO:root:current train perplexity3.754624128341675
INFO:root:current mean train loss 1677.8282639614372
INFO:root:current train perplexity3.7552073001861572
INFO:root:current mean train loss 1678.7159975156678
INFO:root:current train perplexity3.7585034370422363
INFO:root:current mean train loss 1679.6036584854126
INFO:root:current train perplexity3.759420394897461

100%|██████████| 1/1 [02:17<00:00, 137.91s/it][A100%|██████████| 1/1 [02:17<00:00, 137.91s/it]
INFO:root:final mean train loss: 1679.2442717443978
INFO:root:final train perplexity: 3.7597527503967285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.25s/it][A100%|██████████| 1/1 [00:07<00:00,  7.25s/it]
INFO:root:eval mean loss: 2126.337783705258
INFO:root:eval perplexity: 5.582612991333008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/114
 57%|█████▋    | 114/200 [4:37:01<3:28:41, 145.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1682.9514292124156
INFO:root:current train perplexity3.696577548980713
INFO:root:current mean train loss 1669.8303008810446
INFO:root:current train perplexity3.731074094772339
INFO:root:current mean train loss 1672.8408821202531
INFO:root:current train perplexity3.7280685901641846
INFO:root:current mean train loss 1672.9018290262195
INFO:root:current train perplexity3.733020544052124
INFO:root:current mean train loss 1672.1918112887943
INFO:root:current train perplexity3.7356746196746826
INFO:root:current mean train loss 1672.6516920263762
INFO:root:current train perplexity3.7375874519348145
INFO:root:current mean train loss 1672.4564506015747
INFO:root:current train perplexity3.734485387802124
INFO:root:current mean train loss 1671.6107739224685
INFO:root:current train perplexity3.7342851161956787
INFO:root:current mean train loss 1671.8896568963746
INFO:root:current train perplexity3.735426664352417
INFO:root:current mean train loss 1671.9634004020895
INFO:root:current train perplexity3.734830379486084
INFO:root:current mean train loss 1672.3314097155255
INFO:root:current train perplexity3.7400317192077637
INFO:root:current mean train loss 1672.851585475415
INFO:root:current train perplexity3.7427170276641846
INFO:root:current mean train loss 1672.3903263866082
INFO:root:current train perplexity3.742278814315796
INFO:root:current mean train loss 1673.4873386517156
INFO:root:current train perplexity3.744211435317993
INFO:root:current mean train loss 1674.0430813910816
INFO:root:current train perplexity3.7451226711273193
INFO:root:current mean train loss 1675.7881739361378
INFO:root:current train perplexity3.74833083152771
INFO:root:current mean train loss 1675.9476795604335
INFO:root:current train perplexity3.7479429244995117
INFO:root:current mean train loss 1675.9603516749423
INFO:root:current train perplexity3.748157501220703
INFO:root:current mean train loss 1676.8939931305713
INFO:root:current train perplexity3.751166582107544
INFO:root:current mean train loss 1676.9366560952221
INFO:root:current train perplexity3.751155376434326

100%|██████████| 1/1 [02:18<00:00, 138.05s/it][A100%|██████████| 1/1 [02:18<00:00, 138.05s/it]
INFO:root:final mean train loss: 1676.4165613094606
INFO:root:final train perplexity: 3.7513768672943115
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2127.6305624238144
INFO:root:eval perplexity: 5.588453769683838
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/115
 57%|█████▊    | 115/200 [4:39:26<3:26:14, 145.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.2854184751156
INFO:root:current train perplexity3.717099189758301
INFO:root:current mean train loss 1666.85339038403
INFO:root:current train perplexity3.7295172214508057
INFO:root:current mean train loss 1666.5453395092582
INFO:root:current train perplexity3.7328073978424072
INFO:root:current mean train loss 1665.95253016585
INFO:root:current train perplexity3.7305502891540527
INFO:root:current mean train loss 1666.644900956343
INFO:root:current train perplexity3.7307162284851074
INFO:root:current mean train loss 1668.4541015625
INFO:root:current train perplexity3.732468843460083
INFO:root:current mean train loss 1668.4811610417264
INFO:root:current train perplexity3.730729579925537
INFO:root:current mean train loss 1667.7909830837098
INFO:root:current train perplexity3.7292282581329346
INFO:root:current mean train loss 1669.2239887317953
INFO:root:current train perplexity3.7302353382110596
INFO:root:current mean train loss 1670.8659598872346
INFO:root:current train perplexity3.732895612716675
INFO:root:current mean train loss 1672.417180852148
INFO:root:current train perplexity3.736111879348755
INFO:root:current mean train loss 1672.634775779896
INFO:root:current train perplexity3.737746477127075
INFO:root:current mean train loss 1673.9700899504398
INFO:root:current train perplexity3.740744113922119
INFO:root:current mean train loss 1673.2278718258112
INFO:root:current train perplexity3.738853931427002
INFO:root:current mean train loss 1673.187292379723
INFO:root:current train perplexity3.741650342941284
INFO:root:current mean train loss 1674.2442944304516
INFO:root:current train perplexity3.7434866428375244
INFO:root:current mean train loss 1674.2593045032875
INFO:root:current train perplexity3.7443654537200928
INFO:root:current mean train loss 1674.1720867091738
INFO:root:current train perplexity3.743591547012329
INFO:root:current mean train loss 1674.6049000761654
INFO:root:current train perplexity3.744635581970215
INFO:root:current mean train loss 1674.968765368115
INFO:root:current train perplexity3.7455227375030518

100%|██████████| 1/1 [02:18<00:00, 138.28s/it][A100%|██████████| 1/1 [02:18<00:00, 138.28s/it]
INFO:root:final mean train loss: 1674.645490391472
INFO:root:final train perplexity: 3.746141195297241
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2128.556123341229
INFO:root:eval perplexity: 5.592639923095703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/116
 58%|█████▊    | 116/200 [4:41:52<3:23:54, 145.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1662.9494835222272
INFO:root:current train perplexity3.7117502689361572
INFO:root:current mean train loss 1666.0437118797972
INFO:root:current train perplexity3.7169272899627686
INFO:root:current mean train loss 1666.029466692372
INFO:root:current train perplexity3.7243146896362305
INFO:root:current mean train loss 1669.1548762976963
INFO:root:current train perplexity3.7306649684906006
INFO:root:current mean train loss 1670.0822028222863
INFO:root:current train perplexity3.7287237644195557
INFO:root:current mean train loss 1671.2480859973593
INFO:root:current train perplexity3.72273588180542
INFO:root:current mean train loss 1672.1691525227623
INFO:root:current train perplexity3.72790789604187
INFO:root:current mean train loss 1672.1172908876965
INFO:root:current train perplexity3.725360870361328
INFO:root:current mean train loss 1671.5164960298418
INFO:root:current train perplexity3.7268733978271484
INFO:root:current mean train loss 1669.6316625750273
INFO:root:current train perplexity3.7271368503570557
INFO:root:current mean train loss 1670.716884523992
INFO:root:current train perplexity3.7316207885742188
INFO:root:current mean train loss 1669.9502224160706
INFO:root:current train perplexity3.7343673706054688
INFO:root:current mean train loss 1669.481780213507
INFO:root:current train perplexity3.736131191253662
INFO:root:current mean train loss 1669.652241356959
INFO:root:current train perplexity3.73457932472229
INFO:root:current mean train loss 1669.5562310961134
INFO:root:current train perplexity3.7340211868286133
INFO:root:current mean train loss 1670.0449667092269
INFO:root:current train perplexity3.7369658946990967
INFO:root:current mean train loss 1670.8599711794257
INFO:root:current train perplexity3.7387187480926514
INFO:root:current mean train loss 1671.628237792693
INFO:root:current train perplexity3.7379300594329834
INFO:root:current mean train loss 1672.6760017725314
INFO:root:current train perplexity3.7389540672302246
INFO:root:current mean train loss 1673.260606120287
INFO:root:current train perplexity3.7411375045776367

100%|██████████| 1/1 [02:18<00:00, 138.37s/it][A100%|██████████| 1/1 [02:18<00:00, 138.37s/it]
INFO:root:final mean train loss: 1672.789878395069
INFO:root:final train perplexity: 3.7406625747680664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2128.271375290891
INFO:root:eval perplexity: 5.591350555419922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/117
 58%|█████▊    | 117/200 [4:44:18<3:21:34, 145.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1657.614936135032
INFO:root:current train perplexity3.7274842262268066
INFO:root:current mean train loss 1657.5947837017952
INFO:root:current train perplexity3.725398540496826
INFO:root:current mean train loss 1663.90189743042
INFO:root:current train perplexity3.7450711727142334
INFO:root:current mean train loss 1663.6379721730025
INFO:root:current train perplexity3.734924793243408
INFO:root:current mean train loss 1662.4720739145748
INFO:root:current train perplexity3.7275309562683105
INFO:root:current mean train loss 1663.1676278665764
INFO:root:current train perplexity3.7289819717407227
INFO:root:current mean train loss 1664.6855800539947
INFO:root:current train perplexity3.734287738800049
INFO:root:current mean train loss 1667.8935523638265
INFO:root:current train perplexity3.735867500305176
INFO:root:current mean train loss 1667.270111496384
INFO:root:current train perplexity3.7361927032470703
INFO:root:current mean train loss 1667.7640803410457
INFO:root:current train perplexity3.7358176708221436
INFO:root:current mean train loss 1666.431302238913
INFO:root:current train perplexity3.733504295349121
INFO:root:current mean train loss 1667.763832991372
INFO:root:current train perplexity3.735913038253784
INFO:root:current mean train loss 1667.6019152528752
INFO:root:current train perplexity3.7343273162841797
INFO:root:current mean train loss 1667.9900499503276
INFO:root:current train perplexity3.7338225841522217
INFO:root:current mean train loss 1667.6738845661123
INFO:root:current train perplexity3.732992172241211
INFO:root:current mean train loss 1668.606979639164
INFO:root:current train perplexity3.7335431575775146
INFO:root:current mean train loss 1668.7120316491873
INFO:root:current train perplexity3.7314023971557617
INFO:root:current mean train loss 1669.3025395950215
INFO:root:current train perplexity3.7327282428741455
INFO:root:current mean train loss 1670.6863481553935
INFO:root:current train perplexity3.7336392402648926

100%|██████████| 1/1 [02:18<00:00, 138.04s/it][A100%|██████████| 1/1 [02:18<00:00, 138.04s/it]
INFO:root:final mean train loss: 1670.6692231563504
INFO:root:final train perplexity: 3.7344114780426025
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2129.8012751586048
INFO:root:eval perplexity: 5.598272800445557
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/118
 59%|█████▉    | 118/200 [4:46:43<3:19:02, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1708.698046875
INFO:root:current train perplexity3.752872943878174
INFO:root:current mean train loss 1674.5370965866816
INFO:root:current train perplexity3.733579635620117
INFO:root:current mean train loss 1671.576688738567
INFO:root:current train perplexity3.712639808654785
INFO:root:current mean train loss 1661.6796278656507
INFO:root:current train perplexity3.7081525325775146
INFO:root:current mean train loss 1662.4764250578703
INFO:root:current train perplexity3.7083451747894287
INFO:root:current mean train loss 1662.88854569539
INFO:root:current train perplexity3.7119576930999756
INFO:root:current mean train loss 1662.8639636331354
INFO:root:current train perplexity3.7154197692871094
INFO:root:current mean train loss 1665.3921720897051
INFO:root:current train perplexity3.71940016746521
INFO:root:current mean train loss 1663.5660000060657
INFO:root:current train perplexity3.720010757446289
INFO:root:current mean train loss 1664.3951608900206
INFO:root:current train perplexity3.7200305461883545
INFO:root:current mean train loss 1666.4164656454059
INFO:root:current train perplexity3.7215511798858643
INFO:root:current mean train loss 1666.21373020362
INFO:root:current train perplexity3.721313714981079
INFO:root:current mean train loss 1667.3745997511994
INFO:root:current train perplexity3.725243330001831
INFO:root:current mean train loss 1667.7280570896191
INFO:root:current train perplexity3.726912021636963
INFO:root:current mean train loss 1668.5928934226256
INFO:root:current train perplexity3.7279913425445557
INFO:root:current mean train loss 1668.573755126142
INFO:root:current train perplexity3.7284348011016846
INFO:root:current mean train loss 1669.4070449401286
INFO:root:current train perplexity3.7303874492645264
INFO:root:current mean train loss 1669.6287087896353
INFO:root:current train perplexity3.7302114963531494
INFO:root:current mean train loss 1669.0814521430273
INFO:root:current train perplexity3.7307374477386475
INFO:root:current mean train loss 1669.341601626579
INFO:root:current train perplexity3.7291340827941895

100%|██████████| 1/1 [02:18<00:00, 138.39s/it][A100%|██████████| 1/1 [02:18<00:00, 138.39s/it]
INFO:root:final mean train loss: 1668.7084574350731
INFO:root:final train perplexity: 3.7286415100097656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2131.303794049202
INFO:root:eval perplexity: 5.605079650878906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/119
 60%|█████▉    | 119/200 [4:49:09<3:16:42, 145.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1672.506924715909
INFO:root:current train perplexity3.6687850952148438
INFO:root:current mean train loss 1658.838306864754
INFO:root:current train perplexity3.7040140628814697
INFO:root:current mean train loss 1652.3005876970722
INFO:root:current train perplexity3.697981357574463
INFO:root:current mean train loss 1651.3940171899262
INFO:root:current train perplexity3.6956045627593994
INFO:root:current mean train loss 1653.898744122112
INFO:root:current train perplexity3.7019755840301514
INFO:root:current mean train loss 1654.3716491465368
INFO:root:current train perplexity3.70469331741333
INFO:root:current mean train loss 1656.0194615796447
INFO:root:current train perplexity3.7073357105255127
INFO:root:current mean train loss 1660.390893994276
INFO:root:current train perplexity3.713061571121216
INFO:root:current mean train loss 1660.37112359931
INFO:root:current train perplexity3.7107622623443604
INFO:root:current mean train loss 1661.0210830787776
INFO:root:current train perplexity3.711730718612671
INFO:root:current mean train loss 1662.6800808244022
INFO:root:current train perplexity3.714104413986206
INFO:root:current mean train loss 1663.1621927135557
INFO:root:current train perplexity3.712836265563965
INFO:root:current mean train loss 1662.9874073784013
INFO:root:current train perplexity3.712397575378418
INFO:root:current mean train loss 1664.0125690869954
INFO:root:current train perplexity3.7135751247406006
INFO:root:current mean train loss 1665.1100612377484
INFO:root:current train perplexity3.718126058578491
INFO:root:current mean train loss 1665.6333242007843
INFO:root:current train perplexity3.719501256942749
INFO:root:current mean train loss 1665.7335012414746
INFO:root:current train perplexity3.7202138900756836
INFO:root:current mean train loss 1665.7147308952162
INFO:root:current train perplexity3.720304250717163
INFO:root:current mean train loss 1667.2633121628662
INFO:root:current train perplexity3.7228121757507324
INFO:root:current mean train loss 1667.2684681204678
INFO:root:current train perplexity3.7225286960601807

100%|██████████| 1/1 [02:18<00:00, 138.27s/it][A100%|██████████| 1/1 [02:18<00:00, 138.27s/it]
INFO:root:final mean train loss: 1666.9954496028745
INFO:root:final train perplexity: 3.723607063293457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2132.366542951435
INFO:root:eval perplexity: 5.609899520874023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/120
 60%|██████    | 120/200 [4:51:35<3:14:20, 145.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1678.49461325621
INFO:root:current train perplexity3.715965986251831
INFO:root:current mean train loss 1670.8521394798224
INFO:root:current train perplexity3.715235471725464
INFO:root:current mean train loss 1660.3024187287526
INFO:root:current train perplexity3.712214469909668
INFO:root:current mean train loss 1659.3345475548488
INFO:root:current train perplexity3.7101736068725586
INFO:root:current mean train loss 1659.410071718394
INFO:root:current train perplexity3.707714319229126
INFO:root:current mean train loss 1660.5569087720316
INFO:root:current train perplexity3.7081847190856934
INFO:root:current mean train loss 1662.9075900989706
INFO:root:current train perplexity3.711549758911133
INFO:root:current mean train loss 1664.7889315098
INFO:root:current train perplexity3.7110114097595215
INFO:root:current mean train loss 1663.7222740346115
INFO:root:current train perplexity3.7105185985565186
INFO:root:current mean train loss 1663.4777210629659
INFO:root:current train perplexity3.7077736854553223
INFO:root:current mean train loss 1663.945213222412
INFO:root:current train perplexity3.710286855697632
INFO:root:current mean train loss 1665.7126511999973
INFO:root:current train perplexity3.7132134437561035
INFO:root:current mean train loss 1664.641065694518
INFO:root:current train perplexity3.711865186691284
INFO:root:current mean train loss 1664.5559574323772
INFO:root:current train perplexity3.7100107669830322
INFO:root:current mean train loss 1663.8070965860352
INFO:root:current train perplexity3.7109782695770264
INFO:root:current mean train loss 1664.2093406711947
INFO:root:current train perplexity3.712606191635132
INFO:root:current mean train loss 1663.7106966364304
INFO:root:current train perplexity3.713092565536499
INFO:root:current mean train loss 1664.4181192495687
INFO:root:current train perplexity3.7148399353027344
INFO:root:current mean train loss 1664.1235500250646
INFO:root:current train perplexity3.7147557735443115
INFO:root:current mean train loss 1664.9522218433713
INFO:root:current train perplexity3.7160956859588623

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.36s/it]
INFO:root:final mean train loss: 1664.7070324503889
INFO:root:final train perplexity: 3.716893196105957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2132.566145660184
INFO:root:eval perplexity: 5.610805511474609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/121
 60%|██████    | 121/200 [4:54:01<3:11:57, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1649.1504538399834
INFO:root:current train perplexity3.675779104232788
INFO:root:current mean train loss 1659.440637833033
INFO:root:current train perplexity3.684875726699829
INFO:root:current mean train loss 1658.0133752822876
INFO:root:current train perplexity3.6751976013183594
INFO:root:current mean train loss 1660.4123085964932
INFO:root:current train perplexity3.6857497692108154
INFO:root:current mean train loss 1656.5207490084465
INFO:root:current train perplexity3.6876492500305176
INFO:root:current mean train loss 1652.878471100073
INFO:root:current train perplexity3.682497262954712
INFO:root:current mean train loss 1652.526105927258
INFO:root:current train perplexity3.6813838481903076
INFO:root:current mean train loss 1654.36015650835
INFO:root:current train perplexity3.6841881275177
INFO:root:current mean train loss 1656.063338805582
INFO:root:current train perplexity3.6896886825561523
INFO:root:current mean train loss 1656.698444573949
INFO:root:current train perplexity3.695563554763794
INFO:root:current mean train loss 1658.3353427540171
INFO:root:current train perplexity3.6980950832366943
INFO:root:current mean train loss 1659.0919513636395
INFO:root:current train perplexity3.7016098499298096
INFO:root:current mean train loss 1660.5349826691256
INFO:root:current train perplexity3.703092336654663
INFO:root:current mean train loss 1660.9524230056807
INFO:root:current train perplexity3.7057361602783203
INFO:root:current mean train loss 1660.7802798931416
INFO:root:current train perplexity3.70639967918396
INFO:root:current mean train loss 1661.5950435844363
INFO:root:current train perplexity3.7065296173095703
INFO:root:current mean train loss 1662.275380083904
INFO:root:current train perplexity3.70703125
INFO:root:current mean train loss 1662.5655820668424
INFO:root:current train perplexity3.70700740814209
INFO:root:current mean train loss 1662.8923787084118
INFO:root:current train perplexity3.7100107669830322
INFO:root:current mean train loss 1662.5208427569617
INFO:root:current train perplexity3.7089874744415283

100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
INFO:root:final mean train loss: 1662.214853845578
INFO:root:final train perplexity: 3.709595203399658
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2133.7207767134864
INFO:root:eval perplexity: 5.616048336029053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/122
 61%|██████    | 122/200 [4:56:27<3:09:31, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1667.7312647153253
INFO:root:current train perplexity3.7257184982299805
INFO:root:current mean train loss 1660.4524264473446
INFO:root:current train perplexity3.6994388103485107
INFO:root:current mean train loss 1657.3929680345695
INFO:root:current train perplexity3.6879799365997314
INFO:root:current mean train loss 1660.5148487244471
INFO:root:current train perplexity3.6944029331207275
INFO:root:current mean train loss 1659.5735376854022
INFO:root:current train perplexity3.693693161010742
INFO:root:current mean train loss 1659.158419783827
INFO:root:current train perplexity3.6946020126342773
INFO:root:current mean train loss 1659.5724707684226
INFO:root:current train perplexity3.6936793327331543
INFO:root:current mean train loss 1658.7434355228716
INFO:root:current train perplexity3.691728115081787
INFO:root:current mean train loss 1657.4166592557542
INFO:root:current train perplexity3.691333293914795
INFO:root:current mean train loss 1657.1018978483507
INFO:root:current train perplexity3.6950111389160156
INFO:root:current mean train loss 1656.931302741656
INFO:root:current train perplexity3.6957790851593018
INFO:root:current mean train loss 1656.8844296974903
INFO:root:current train perplexity3.694969892501831
INFO:root:current mean train loss 1657.9232339791586
INFO:root:current train perplexity3.6966021060943604
INFO:root:current mean train loss 1657.481242958508
INFO:root:current train perplexity3.698186159133911
INFO:root:current mean train loss 1658.2860423163825
INFO:root:current train perplexity3.6993846893310547
INFO:root:current mean train loss 1659.0839021152854
INFO:root:current train perplexity3.7013564109802246
INFO:root:current mean train loss 1659.3405303499048
INFO:root:current train perplexity3.702643871307373
INFO:root:current mean train loss 1658.9726495715904
INFO:root:current train perplexity3.702335834503174
INFO:root:current mean train loss 1660.7560203540027
INFO:root:current train perplexity3.7051777839660645
INFO:root:current mean train loss 1661.137119422754
INFO:root:current train perplexity3.7052927017211914

100%|██████████| 1/1 [02:18<00:00, 138.52s/it][A100%|██████████| 1/1 [02:18<00:00, 138.52s/it]
INFO:root:final mean train loss: 1660.5934937015904
INFO:root:final train perplexity: 3.7048544883728027
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2134.928529563525
INFO:root:eval perplexity: 5.621534824371338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/123
 62%|██████▏   | 123/200 [4:58:53<3:07:12, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1648.2076212565105
INFO:root:current train perplexity3.6896588802337646
INFO:root:current mean train loss 1652.0331080386513
INFO:root:current train perplexity3.6789498329162598
INFO:root:current mean train loss 1653.293393049569
INFO:root:current train perplexity3.6801578998565674
INFO:root:current mean train loss 1650.9091127053284
INFO:root:current train perplexity3.679539680480957
INFO:root:current mean train loss 1653.6822684151787
INFO:root:current train perplexity3.687324047088623
INFO:root:current mean train loss 1654.7108007398701
INFO:root:current train perplexity3.689103841781616
INFO:root:current mean train loss 1655.3310566335485
INFO:root:current train perplexity3.6888251304626465
INFO:root:current mean train loss 1655.8206144308742
INFO:root:current train perplexity3.6901309490203857
INFO:root:current mean train loss 1655.2074036330318
INFO:root:current train perplexity3.690239429473877
INFO:root:current mean train loss 1654.5494959359216
INFO:root:current train perplexity3.6914877891540527
INFO:root:current mean train loss 1655.7720486982153
INFO:root:current train perplexity3.6926350593566895
INFO:root:current mean train loss 1655.5529517422203
INFO:root:current train perplexity3.692610502243042
INFO:root:current mean train loss 1655.9226395954458
INFO:root:current train perplexity3.6910595893859863
INFO:root:current mean train loss 1655.9330393400123
INFO:root:current train perplexity3.6911251544952393
INFO:root:current mean train loss 1657.107872633966
INFO:root:current train perplexity3.6930692195892334
INFO:root:current mean train loss 1657.2890436904236
INFO:root:current train perplexity3.6945950984954834
INFO:root:current mean train loss 1657.7511773645524
INFO:root:current train perplexity3.6940295696258545
INFO:root:current mean train loss 1658.609561788037
INFO:root:current train perplexity3.6956958770751953
INFO:root:current mean train loss 1658.9578853546627
INFO:root:current train perplexity3.697514057159424

100%|██████████| 1/1 [02:17<00:00, 137.84s/it][A100%|██████████| 1/1 [02:17<00:00, 137.84s/it]
INFO:root:final mean train loss: 1658.4704545829493
INFO:root:final train perplexity: 3.698655843734741
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2134.4745626246677
INFO:root:eval perplexity: 5.619472980499268
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/124
 62%|██████▏   | 124/200 [5:01:18<3:04:33, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.4987095424108
INFO:root:current train perplexity3.686306953430176
INFO:root:current mean train loss 1648.214844890844
INFO:root:current train perplexity3.6759140491485596
INFO:root:current mean train loss 1646.7030353638286
INFO:root:current train perplexity3.6776123046875
INFO:root:current mean train loss 1649.6289977033287
INFO:root:current train perplexity3.672548294067383
INFO:root:current mean train loss 1648.4121273706235
INFO:root:current train perplexity3.6703765392303467
INFO:root:current mean train loss 1647.0050532775517
INFO:root:current train perplexity3.668367862701416
INFO:root:current mean train loss 1650.513415869221
INFO:root:current train perplexity3.6735475063323975
INFO:root:current mean train loss 1649.9415084644625
INFO:root:current train perplexity3.673877239227295
INFO:root:current mean train loss 1650.9854461027048
INFO:root:current train perplexity3.6732020378112793
INFO:root:current mean train loss 1651.411696866171
INFO:root:current train perplexity3.6765148639678955
INFO:root:current mean train loss 1651.0720299698983
INFO:root:current train perplexity3.6788041591644287
INFO:root:current mean train loss 1652.7861643500876
INFO:root:current train perplexity3.683143138885498
INFO:root:current mean train loss 1653.2985340235346
INFO:root:current train perplexity3.6835620403289795
INFO:root:current mean train loss 1654.5737063722383
INFO:root:current train perplexity3.685072898864746
INFO:root:current mean train loss 1655.325114626366
INFO:root:current train perplexity3.686537742614746
INFO:root:current mean train loss 1656.0208884418287
INFO:root:current train perplexity3.6893210411071777
INFO:root:current mean train loss 1656.352479356672
INFO:root:current train perplexity3.6916122436523438
INFO:root:current mean train loss 1657.2376990024989
INFO:root:current train perplexity3.6940276622772217
INFO:root:current mean train loss 1657.6587292365627
INFO:root:current train perplexity3.6937427520751953
INFO:root:current mean train loss 1657.9425052668828
INFO:root:current train perplexity3.6959750652313232

100%|██████████| 1/1 [02:18<00:00, 138.42s/it][A100%|██████████| 1/1 [02:18<00:00, 138.42s/it]
INFO:root:final mean train loss: 1657.6964397266906
INFO:root:final train perplexity: 3.696398973464966
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2137.487824135638
INFO:root:eval perplexity: 5.633181571960449
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/125
 62%|██████▎   | 125/200 [5:03:44<3:02:15, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1619.1631113688152
INFO:root:current train perplexity3.636831283569336
INFO:root:current mean train loss 1653.153087000693
INFO:root:current train perplexity3.6717302799224854
INFO:root:current mean train loss 1643.4029639107841
INFO:root:current train perplexity3.665292739868164
INFO:root:current mean train loss 1649.071627393181
INFO:root:current train perplexity3.6653025150299072
INFO:root:current mean train loss 1652.459026696547
INFO:root:current train perplexity3.672928810119629
INFO:root:current mean train loss 1650.2758365077827
INFO:root:current train perplexity3.6729304790496826
INFO:root:current mean train loss 1648.8744665292593
INFO:root:current train perplexity3.6732842922210693
INFO:root:current mean train loss 1649.866777283052
INFO:root:current train perplexity3.6778974533081055
INFO:root:current mean train loss 1651.2331406676653
INFO:root:current train perplexity3.67889404296875
INFO:root:current mean train loss 1650.8993554053368
INFO:root:current train perplexity3.6774680614471436
INFO:root:current mean train loss 1651.2483308315277
INFO:root:current train perplexity3.6806609630584717
INFO:root:current mean train loss 1652.8939501127738
INFO:root:current train perplexity3.6837666034698486
INFO:root:current mean train loss 1653.2118551216874
INFO:root:current train perplexity3.6821465492248535
INFO:root:current mean train loss 1654.346200533864
INFO:root:current train perplexity3.6820991039276123
INFO:root:current mean train loss 1653.568603172731
INFO:root:current train perplexity3.6813015937805176
INFO:root:current mean train loss 1654.0735637034018
INFO:root:current train perplexity3.6834123134613037
INFO:root:current mean train loss 1654.2096213354853
INFO:root:current train perplexity3.6846859455108643
INFO:root:current mean train loss 1654.0851572129677
INFO:root:current train perplexity3.684927463531494
INFO:root:current mean train loss 1654.929393032141
INFO:root:current train perplexity3.686837911605835
INFO:root:current mean train loss 1654.9649484360789
INFO:root:current train perplexity3.6888091564178467

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1654.7121774401257
INFO:root:final train perplexity: 3.687709093093872
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2137.8635249577515
INFO:root:eval perplexity: 5.634894847869873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/126
 63%|██████▎   | 126/200 [5:06:10<2:59:48, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1646.8761552019816
INFO:root:current train perplexity3.642720937728882
INFO:root:current mean train loss 1642.1063388256316
INFO:root:current train perplexity3.667192220687866
INFO:root:current mean train loss 1642.7406340159816
INFO:root:current train perplexity3.666978120803833
INFO:root:current mean train loss 1645.8601865348928
INFO:root:current train perplexity3.6681785583496094
INFO:root:current mean train loss 1646.429376372945
INFO:root:current train perplexity3.668839454650879
INFO:root:current mean train loss 1647.0736611977964
INFO:root:current train perplexity3.667072057723999
INFO:root:current mean train loss 1648.5829682015406
INFO:root:current train perplexity3.669062852859497
INFO:root:current mean train loss 1648.1813419563407
INFO:root:current train perplexity3.669968843460083
INFO:root:current mean train loss 1650.1623937218992
INFO:root:current train perplexity3.6741623878479004
INFO:root:current mean train loss 1650.3028469754584
INFO:root:current train perplexity3.6717278957366943
INFO:root:current mean train loss 1651.0109920739899
INFO:root:current train perplexity3.673128843307495
INFO:root:current mean train loss 1650.8010527788808
INFO:root:current train perplexity3.6766748428344727
INFO:root:current mean train loss 1650.5000075740645
INFO:root:current train perplexity3.6768081188201904
INFO:root:current mean train loss 1650.6302443809425
INFO:root:current train perplexity3.6786036491394043
INFO:root:current mean train loss 1649.8442487008533
INFO:root:current train perplexity3.678544044494629
INFO:root:current mean train loss 1650.3246844549756
INFO:root:current train perplexity3.678882598876953
INFO:root:current mean train loss 1651.3340477533707
INFO:root:current train perplexity3.6797962188720703
INFO:root:current mean train loss 1651.9479420950604
INFO:root:current train perplexity3.682725191116333
INFO:root:current mean train loss 1652.9973889153525
INFO:root:current train perplexity3.683587074279785
INFO:root:current mean train loss 1653.101326220678
INFO:root:current train perplexity3.6824679374694824

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1653.3750803337155
INFO:root:final train perplexity: 3.6838228702545166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2138.977362450133
INFO:root:eval perplexity: 5.6399736404418945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/127
 64%|██████▎   | 127/200 [5:08:36<2:57:20, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1648.1614800814925
INFO:root:current train perplexity3.6767988204956055
INFO:root:current mean train loss 1651.5998983262461
INFO:root:current train perplexity3.66409969329834
INFO:root:current mean train loss 1649.021083624788
INFO:root:current train perplexity3.655332326889038
INFO:root:current mean train loss 1649.8687467948018
INFO:root:current train perplexity3.656247615814209
INFO:root:current mean train loss 1651.1218576223048
INFO:root:current train perplexity3.66036057472229
INFO:root:current mean train loss 1649.6317519321237
INFO:root:current train perplexity3.665768623352051
INFO:root:current mean train loss 1648.422802400444
INFO:root:current train perplexity3.6651153564453125
INFO:root:current mean train loss 1649.0402976969615
INFO:root:current train perplexity3.6707425117492676
INFO:root:current mean train loss 1649.587502077187
INFO:root:current train perplexity3.6683363914489746
INFO:root:current mean train loss 1649.9656551735386
INFO:root:current train perplexity3.6710076332092285
INFO:root:current mean train loss 1649.0368202368118
INFO:root:current train perplexity3.6704492568969727
INFO:root:current mean train loss 1647.8603510354262
INFO:root:current train perplexity3.671255350112915
INFO:root:current mean train loss 1648.0416297609363
INFO:root:current train perplexity3.6704466342926025
INFO:root:current mean train loss 1648.3440625467426
INFO:root:current train perplexity3.669955253601074
INFO:root:current mean train loss 1648.7958131222404
INFO:root:current train perplexity3.670790910720825
INFO:root:current mean train loss 1650.0799029329469
INFO:root:current train perplexity3.6718273162841797
INFO:root:current mean train loss 1649.9026968556807
INFO:root:current train perplexity3.6740546226501465
INFO:root:current mean train loss 1650.5078046536148
INFO:root:current train perplexity3.6742982864379883
INFO:root:current mean train loss 1651.2781760356397
INFO:root:current train perplexity3.6759872436523438
INFO:root:current mean train loss 1651.9159552132876
INFO:root:current train perplexity3.677027702331543

100%|██████████| 1/1 [02:18<00:00, 138.73s/it][A100%|██████████| 1/1 [02:18<00:00, 138.73s/it]
INFO:root:final mean train loss: 1651.0401067459638
INFO:root:final train perplexity: 3.6770451068878174
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2138.8689739687225
INFO:root:eval perplexity: 5.6394782066345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/128
 64%|██████▍   | 128/200 [5:11:02<2:55:03, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1657.3188509114584
INFO:root:current train perplexity3.6972296237945557
INFO:root:current mean train loss 1661.710952845982
INFO:root:current train perplexity3.6615095138549805
INFO:root:current mean train loss 1652.8690984552557
INFO:root:current train perplexity3.6627275943756104
INFO:root:current mean train loss 1655.3829505208334
INFO:root:current train perplexity3.6715292930603027
INFO:root:current mean train loss 1650.8904091282895
INFO:root:current train perplexity3.6625428199768066
INFO:root:current mean train loss 1649.872950280231
INFO:root:current train perplexity3.6652920246124268
INFO:root:current mean train loss 1648.6805060040508
INFO:root:current train perplexity3.6684536933898926
INFO:root:current mean train loss 1646.532823998236
INFO:root:current train perplexity3.6681594848632812
INFO:root:current mean train loss 1647.0950385044644
INFO:root:current train perplexity3.6681594848632812
INFO:root:current mean train loss 1648.8769397285657
INFO:root:current train perplexity3.669966697692871
INFO:root:current mean train loss 1648.0031505496004
INFO:root:current train perplexity3.6685192584991455
INFO:root:current mean train loss 1648.6057023977726
INFO:root:current train perplexity3.668586254119873
INFO:root:current mean train loss 1650.152984164369
INFO:root:current train perplexity3.6693975925445557
INFO:root:current mean train loss 1649.2104920099432
INFO:root:current train perplexity3.6685564517974854
INFO:root:current mean train loss 1649.0341897014036
INFO:root:current train perplexity3.6691529750823975
INFO:root:current mean train loss 1649.773734499008
INFO:root:current train perplexity3.669753074645996
INFO:root:current mean train loss 1650.105387418377
INFO:root:current train perplexity3.6689093112945557
INFO:root:current mean train loss 1649.5124544729313
INFO:root:current train perplexity3.669983386993408
INFO:root:current mean train loss 1649.5814996744791
INFO:root:current train perplexity3.671895742416382
INFO:root:current mean train loss 1649.4779513202136
INFO:root:current train perplexity3.670928478240967

100%|██████████| 1/1 [02:17<00:00, 137.99s/it][A100%|██████████| 1/1 [02:17<00:00, 137.99s/it]
INFO:root:final mean train loss: 1649.0515160418736
INFO:root:final train perplexity: 3.67128324508667
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2140.8736399116247
INFO:root:eval perplexity: 5.648629188537598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/129
 64%|██████▍   | 129/200 [5:13:27<2:52:27, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1632.9178480065386
INFO:root:current train perplexity3.642685890197754
INFO:root:current mean train loss 1632.4365558624268
INFO:root:current train perplexity3.6396751403808594
INFO:root:current mean train loss 1630.2396152444082
INFO:root:current train perplexity3.6493213176727295
INFO:root:current mean train loss 1632.2334657007334
INFO:root:current train perplexity3.6440982818603516
INFO:root:current mean train loss 1633.6043859962526
INFO:root:current train perplexity3.64670991897583
INFO:root:current mean train loss 1634.908554695748
INFO:root:current train perplexity3.65058970451355
INFO:root:current mean train loss 1636.8539580416816
INFO:root:current train perplexity3.6490843296051025
INFO:root:current mean train loss 1639.4307372738617
INFO:root:current train perplexity3.653289794921875
INFO:root:current mean train loss 1640.1329330649612
INFO:root:current train perplexity3.6552317142486572
INFO:root:current mean train loss 1642.3086809958181
INFO:root:current train perplexity3.6544859409332275
INFO:root:current mean train loss 1643.074726370228
INFO:root:current train perplexity3.658182382583618
INFO:root:current mean train loss 1644.298282188057
INFO:root:current train perplexity3.660524606704712
INFO:root:current mean train loss 1645.3520088313903
INFO:root:current train perplexity3.6610002517700195
INFO:root:current mean train loss 1645.5829522143836
INFO:root:current train perplexity3.6635515689849854
INFO:root:current mean train loss 1646.1800223751936
INFO:root:current train perplexity3.663351058959961
INFO:root:current mean train loss 1646.5142694981255
INFO:root:current train perplexity3.6655938625335693
INFO:root:current mean train loss 1648.1683659835346
INFO:root:current train perplexity3.6680378913879395
INFO:root:current mean train loss 1648.6719328335353
INFO:root:current train perplexity3.6678507328033447
INFO:root:current mean train loss 1648.4250082455528
INFO:root:current train perplexity3.668335199356079

100%|██████████| 1/1 [02:18<00:00, 138.15s/it][A100%|██████████| 1/1 [02:18<00:00, 138.15s/it]
INFO:root:final mean train loss: 1648.404271359042
INFO:root:final train perplexity: 3.66940975189209
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2141.6188579586383
INFO:root:eval perplexity: 5.652034282684326
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/130
 65%|██████▌   | 130/200 [5:15:53<2:49:59, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1669.625
INFO:root:current train perplexity3.643911838531494
INFO:root:current mean train loss 1651.3232892237672
INFO:root:current train perplexity3.6413910388946533
INFO:root:current mean train loss 1645.9471219441537
INFO:root:current train perplexity3.641634702682495
INFO:root:current mean train loss 1642.8807274284486
INFO:root:current train perplexity3.6493427753448486
INFO:root:current mean train loss 1640.8140851233
INFO:root:current train perplexity3.645700693130493
INFO:root:current mean train loss 1640.5839352111218
INFO:root:current train perplexity3.646207094192505
INFO:root:current mean train loss 1638.6140455424493
INFO:root:current train perplexity3.6425442695617676
INFO:root:current mean train loss 1642.4508392377027
INFO:root:current train perplexity3.6477396488189697
INFO:root:current mean train loss 1642.1494296042085
INFO:root:current train perplexity3.647552490234375
INFO:root:current mean train loss 1643.2725513151902
INFO:root:current train perplexity3.650644063949585
INFO:root:current mean train loss 1644.1408425246996
INFO:root:current train perplexity3.651857614517212
INFO:root:current mean train loss 1644.7132994339636
INFO:root:current train perplexity3.652736186981201
INFO:root:current mean train loss 1645.3579608421862
INFO:root:current train perplexity3.6549534797668457
INFO:root:current mean train loss 1645.7091990471615
INFO:root:current train perplexity3.656254529953003
INFO:root:current mean train loss 1645.424885085832
INFO:root:current train perplexity3.6564207077026367
INFO:root:current mean train loss 1645.5140642149706
INFO:root:current train perplexity3.6571691036224365
INFO:root:current mean train loss 1645.5598818231917
INFO:root:current train perplexity3.657470464706421
INFO:root:current mean train loss 1645.6099703516995
INFO:root:current train perplexity3.658639907836914
INFO:root:current mean train loss 1645.3552388475375
INFO:root:current train perplexity3.659726619720459
INFO:root:current mean train loss 1645.844935277796
INFO:root:current train perplexity3.6607911586761475

100%|██████████| 1/1 [02:18<00:00, 138.14s/it][A100%|██████████| 1/1 [02:18<00:00, 138.14s/it]
INFO:root:final mean train loss: 1645.9100592955158
INFO:root:final train perplexity: 3.662198543548584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.09s/it][A100%|██████████| 1/1 [00:07<00:00,  7.09s/it]
INFO:root:eval mean loss: 2141.570884758699
INFO:root:eval perplexity: 5.651815891265869
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/131
 66%|██████▌   | 131/200 [5:18:18<2:47:30, 145.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1647.5755145733174
INFO:root:current train perplexity3.6344735622406006
INFO:root:current mean train loss 1642.795201861669
INFO:root:current train perplexity3.636766195297241
INFO:root:current mean train loss 1635.4158319793971
INFO:root:current train perplexity3.6269383430480957
INFO:root:current mean train loss 1637.9447834038297
INFO:root:current train perplexity3.6249313354492188
INFO:root:current mean train loss 1636.7070424254512
INFO:root:current train perplexity3.6323788166046143
INFO:root:current mean train loss 1639.4946342439252
INFO:root:current train perplexity3.638082265853882
INFO:root:current mean train loss 1639.4906185259833
INFO:root:current train perplexity3.640190601348877
INFO:root:current mean train loss 1641.1008704319472
INFO:root:current train perplexity3.6406636238098145
INFO:root:current mean train loss 1640.5244903194991
INFO:root:current train perplexity3.6417431831359863
INFO:root:current mean train loss 1640.4874382266216
INFO:root:current train perplexity3.6419172286987305
INFO:root:current mean train loss 1638.9246659604198
INFO:root:current train perplexity3.6434059143066406
INFO:root:current mean train loss 1640.1528708422375
INFO:root:current train perplexity3.645016670227051
INFO:root:current mean train loss 1641.0543497654976
INFO:root:current train perplexity3.6477460861206055
INFO:root:current mean train loss 1641.195766719398
INFO:root:current train perplexity3.6494688987731934
INFO:root:current mean train loss 1642.282382446118
INFO:root:current train perplexity3.651345729827881
INFO:root:current mean train loss 1642.5979741447718
INFO:root:current train perplexity3.653785467147827
INFO:root:current mean train loss 1642.5062481081354
INFO:root:current train perplexity3.6528592109680176
INFO:root:current mean train loss 1642.2492296698472
INFO:root:current train perplexity3.654719352722168
INFO:root:current mean train loss 1643.4011654973945
INFO:root:current train perplexity3.6559033393859863
INFO:root:current mean train loss 1644.2489488389756
INFO:root:current train perplexity3.6568048000335693

100%|██████████| 1/1 [02:18<00:00, 138.19s/it][A100%|██████████| 1/1 [02:18<00:00, 138.19s/it]
INFO:root:final mean train loss: 1644.1486818560795
INFO:root:final train perplexity: 3.6571149826049805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.33s/it][A100%|██████████| 1/1 [00:07<00:00,  7.33s/it]
INFO:root:eval mean loss: 2143.450906523576
INFO:root:eval perplexity: 5.660414695739746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/132
 66%|██████▌   | 132/200 [5:20:44<2:45:07, 145.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1660.0614609829215
INFO:root:current train perplexity3.6259751319885254
INFO:root:current mean train loss 1640.5691908189467
INFO:root:current train perplexity3.617831230163574
INFO:root:current mean train loss 1642.6514279714827
INFO:root:current train perplexity3.6338727474212646
INFO:root:current mean train loss 1636.6999330214787
INFO:root:current train perplexity3.628556489944458
INFO:root:current mean train loss 1635.1067109463177
INFO:root:current train perplexity3.6333320140838623
INFO:root:current mean train loss 1636.2645140027912
INFO:root:current train perplexity3.636282444000244
INFO:root:current mean train loss 1639.0765506157052
INFO:root:current train perplexity3.6377861499786377
INFO:root:current mean train loss 1638.4753968353066
INFO:root:current train perplexity3.6399402618408203
INFO:root:current mean train loss 1637.8304993617012
INFO:root:current train perplexity3.640096426010132
INFO:root:current mean train loss 1638.6617698305358
INFO:root:current train perplexity3.644044876098633
INFO:root:current mean train loss 1638.1395872267872
INFO:root:current train perplexity3.642775774002075
INFO:root:current mean train loss 1639.376348006548
INFO:root:current train perplexity3.6443562507629395
INFO:root:current mean train loss 1640.3518660553789
INFO:root:current train perplexity3.6433510780334473
INFO:root:current mean train loss 1640.784965464009
INFO:root:current train perplexity3.6438188552856445
INFO:root:current mean train loss 1640.5838212762042
INFO:root:current train perplexity3.6448616981506348
INFO:root:current mean train loss 1641.042391230051
INFO:root:current train perplexity3.646667718887329
INFO:root:current mean train loss 1641.9140280260956
INFO:root:current train perplexity3.6478917598724365
INFO:root:current mean train loss 1642.980635502389
INFO:root:current train perplexity3.650799512863159
INFO:root:current mean train loss 1642.5160485038405
INFO:root:current train perplexity3.6514389514923096
INFO:root:current mean train loss 1642.7878556185265
INFO:root:current train perplexity3.6517062187194824

100%|██████████| 1/1 [02:17<00:00, 137.95s/it][A100%|██████████| 1/1 [02:17<00:00, 137.95s/it]
INFO:root:final mean train loss: 1642.1448608029086
INFO:root:final train perplexity: 3.6513400077819824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.25s/it][A100%|██████████| 1/1 [00:07<00:00,  7.25s/it]
INFO:root:eval mean loss: 2143.9114652593084
INFO:root:eval perplexity: 5.6625237464904785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/133
 66%|██████▋   | 133/200 [5:23:10<2:42:37, 145.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1651.8196207682292
INFO:root:current train perplexity3.640481948852539
INFO:root:current mean train loss 1638.2110794067382
INFO:root:current train perplexity3.6295149326324463
INFO:root:current mean train loss 1636.802683199369
INFO:root:current train perplexity3.6380128860473633
INFO:root:current mean train loss 1635.5374576144748
INFO:root:current train perplexity3.6323554515838623
INFO:root:current mean train loss 1634.5324423085087
INFO:root:current train perplexity3.628262758255005
INFO:root:current mean train loss 1634.4789879935129
INFO:root:current train perplexity3.6322922706604004
INFO:root:current mean train loss 1635.8242697975852
INFO:root:current train perplexity3.6345086097717285
INFO:root:current mean train loss 1635.1774929648952
INFO:root:current train perplexity3.6371145248413086
INFO:root:current mean train loss 1634.689638927371
INFO:root:current train perplexity3.636434316635132
INFO:root:current mean train loss 1636.3733487447103
INFO:root:current train perplexity3.6401143074035645
INFO:root:current mean train loss 1638.9451921570976
INFO:root:current train perplexity3.6414084434509277
INFO:root:current mean train loss 1640.5278611807987
INFO:root:current train perplexity3.6427721977233887
INFO:root:current mean train loss 1641.7971164279513
INFO:root:current train perplexity3.6434571743011475
INFO:root:current mean train loss 1642.9681459314684
INFO:root:current train perplexity3.646106719970703
INFO:root:current mean train loss 1642.9947250575235
INFO:root:current train perplexity3.648139715194702
INFO:root:current mean train loss 1642.2988556690705
INFO:root:current train perplexity3.647965669631958
INFO:root:current mean train loss 1641.7022891125048
INFO:root:current train perplexity3.6476287841796875
INFO:root:current mean train loss 1641.8351040233265
INFO:root:current train perplexity3.64785099029541
INFO:root:current mean train loss 1641.474207593036
INFO:root:current train perplexity3.647719621658325
INFO:root:current mean train loss 1642.0757349131059
INFO:root:current train perplexity3.64959979057312

100%|██████████| 1/1 [02:18<00:00, 138.09s/it][A100%|██████████| 1/1 [02:18<00:00, 138.09s/it]
INFO:root:final mean train loss: 1641.4977627829717
INFO:root:final train perplexity: 3.649477243423462
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2143.8306560110536
INFO:root:eval perplexity: 5.662153720855713
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/134
 67%|██████▋   | 134/200 [5:25:35<2:40:09, 145.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.5406747793222
INFO:root:current train perplexity3.631927251815796
INFO:root:current mean train loss 1629.4211853372176
INFO:root:current train perplexity3.6213202476501465
INFO:root:current mean train loss 1632.9100152301444
INFO:root:current train perplexity3.6266732215881348
INFO:root:current mean train loss 1630.2269092833017
INFO:root:current train perplexity3.6154401302337646
INFO:root:current mean train loss 1634.2671732712593
INFO:root:current train perplexity3.6221158504486084
INFO:root:current mean train loss 1636.152410391505
INFO:root:current train perplexity3.62776517868042
INFO:root:current mean train loss 1637.9288730367775
INFO:root:current train perplexity3.6272976398468018
INFO:root:current mean train loss 1636.7259801130651
INFO:root:current train perplexity3.6237401962280273
INFO:root:current mean train loss 1637.6531516132768
INFO:root:current train perplexity3.6291136741638184
INFO:root:current mean train loss 1639.1199261480856
INFO:root:current train perplexity3.6331260204315186
INFO:root:current mean train loss 1639.3838342863205
INFO:root:current train perplexity3.6344714164733887
INFO:root:current mean train loss 1641.0317707434488
INFO:root:current train perplexity3.6372458934783936
INFO:root:current mean train loss 1640.9764526653962
INFO:root:current train perplexity3.6374027729034424
INFO:root:current mean train loss 1639.9630884906046
INFO:root:current train perplexity3.636016845703125
INFO:root:current mean train loss 1640.1888668238512
INFO:root:current train perplexity3.637577533721924
INFO:root:current mean train loss 1641.2051556864794
INFO:root:current train perplexity3.639019250869751
INFO:root:current mean train loss 1639.8846699777785
INFO:root:current train perplexity3.6405105590820312
INFO:root:current mean train loss 1640.4030429236864
INFO:root:current train perplexity3.643237829208374
INFO:root:current mean train loss 1640.4555795432789
INFO:root:current train perplexity3.643723487854004
INFO:root:current mean train loss 1640.081547908368
INFO:root:current train perplexity3.6440343856811523

100%|██████████| 1/1 [02:18<00:00, 138.28s/it][A100%|██████████| 1/1 [02:18<00:00, 138.28s/it]
INFO:root:final mean train loss: 1639.5324386004181
INFO:root:final train perplexity: 3.643824577331543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2144.7325313919823
INFO:root:eval perplexity: 5.666285037994385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/135
 68%|██████▊   | 135/200 [5:28:01<2:37:45, 145.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.9635490255152
INFO:root:current train perplexity3.6290414333343506
INFO:root:current mean train loss 1634.4769425539625
INFO:root:current train perplexity3.6282758712768555
INFO:root:current mean train loss 1633.2544439406622
INFO:root:current train perplexity3.6253039836883545
INFO:root:current mean train loss 1635.0243378460104
INFO:root:current train perplexity3.6262564659118652
INFO:root:current mean train loss 1632.6136358469605
INFO:root:current train perplexity3.625201463699341
INFO:root:current mean train loss 1632.1782982823022
INFO:root:current train perplexity3.6254923343658447
INFO:root:current mean train loss 1632.667260249685
INFO:root:current train perplexity3.629725217819214
INFO:root:current mean train loss 1635.6916817537783
INFO:root:current train perplexity3.632733106613159
INFO:root:current mean train loss 1636.6809681459295
INFO:root:current train perplexity3.6315557956695557
INFO:root:current mean train loss 1637.7276451678822
INFO:root:current train perplexity3.6339268684387207
INFO:root:current mean train loss 1637.2055545785963
INFO:root:current train perplexity3.6366124153137207
INFO:root:current mean train loss 1637.1234828111915
INFO:root:current train perplexity3.638671636581421
INFO:root:current mean train loss 1637.176219533363
INFO:root:current train perplexity3.636631965637207
INFO:root:current mean train loss 1636.927368514336
INFO:root:current train perplexity3.6359241008758545
INFO:root:current mean train loss 1637.5244831866528
INFO:root:current train perplexity3.638361692428589
INFO:root:current mean train loss 1637.4748686786877
INFO:root:current train perplexity3.6384336948394775
INFO:root:current mean train loss 1637.4020680469212
INFO:root:current train perplexity3.6378800868988037
INFO:root:current mean train loss 1637.925544934395
INFO:root:current train perplexity3.6376936435699463
INFO:root:current mean train loss 1637.8715066235068
INFO:root:current train perplexity3.6375043392181396

100%|██████████| 1/1 [02:18<00:00, 138.33s/it][A100%|██████████| 1/1 [02:18<00:00, 138.33s/it]
INFO:root:final mean train loss: 1637.6397009160382
INFO:root:final train perplexity: 3.6383893489837646
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2145.3011812250666
INFO:root:eval perplexity: 5.668891906738281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/136
 68%|██████▊   | 136/200 [5:30:27<2:35:23, 145.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1653.0945379083807
INFO:root:current train perplexity3.6051828861236572
INFO:root:current mean train loss 1628.422119140625
INFO:root:current train perplexity3.6178834438323975
INFO:root:current mean train loss 1631.4346142809538
INFO:root:current train perplexity3.619903326034546
INFO:root:current mean train loss 1636.937852473121
INFO:root:current train perplexity3.6217100620269775
INFO:root:current mean train loss 1631.1853163967457
INFO:root:current train perplexity3.621623992919922
INFO:root:current mean train loss 1632.741577865093
INFO:root:current train perplexity3.6229753494262695
INFO:root:current mean train loss 1631.1389967298742
INFO:root:current train perplexity3.627974033355713
INFO:root:current mean train loss 1632.2305640369527
INFO:root:current train perplexity3.6242733001708984
INFO:root:current mean train loss 1633.872476109741
INFO:root:current train perplexity3.6278975009918213
INFO:root:current mean train loss 1635.5746309215492
INFO:root:current train perplexity3.6271755695343018
INFO:root:current mean train loss 1634.298253030145
INFO:root:current train perplexity3.623915672302246
INFO:root:current mean train loss 1634.7292552985768
INFO:root:current train perplexity3.623849868774414
INFO:root:current mean train loss 1635.0565873011392
INFO:root:current train perplexity3.625581741333008
INFO:root:current mean train loss 1634.6545086125213
INFO:root:current train perplexity3.6245148181915283
INFO:root:current mean train loss 1634.4740697913437
INFO:root:current train perplexity3.6248016357421875
INFO:root:current mean train loss 1634.5787556292914
INFO:root:current train perplexity3.6265201568603516
INFO:root:current mean train loss 1635.172053824294
INFO:root:current train perplexity3.6275792121887207
INFO:root:current mean train loss 1635.4916100382177
INFO:root:current train perplexity3.629362106323242
INFO:root:current mean train loss 1636.1726884425905
INFO:root:current train perplexity3.6317086219787598
INFO:root:current mean train loss 1636.340207597462
INFO:root:current train perplexity3.6339988708496094

100%|██████████| 1/1 [02:18<00:00, 138.11s/it][A100%|██████████| 1/1 [02:18<00:00, 138.11s/it]
INFO:root:final mean train loss: 1636.2086310297686
INFO:root:final train perplexity: 3.6342854499816895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2145.8545657759864
INFO:root:eval perplexity: 5.671429634094238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/137
 68%|██████▊   | 137/200 [5:32:52<2:32:54, 145.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1645.5391322544642
INFO:root:current train perplexity3.5713131427764893
INFO:root:current mean train loss 1634.5894594192505
INFO:root:current train perplexity3.6137242317199707
INFO:root:current mean train loss 1636.7920045016106
INFO:root:current train perplexity3.618626117706299
INFO:root:current mean train loss 1634.5168434701316
INFO:root:current train perplexity3.616485834121704
INFO:root:current mean train loss 1634.3469728844188
INFO:root:current train perplexity3.621128559112549
INFO:root:current mean train loss 1635.441595135313
INFO:root:current train perplexity3.6190619468688965
INFO:root:current mean train loss 1637.1381278068397
INFO:root:current train perplexity3.620150089263916
INFO:root:current mean train loss 1636.8778347811856
INFO:root:current train perplexity3.62025785446167
INFO:root:current mean train loss 1636.3252581167912
INFO:root:current train perplexity3.6213412284851074
INFO:root:current mean train loss 1635.8225091079187
INFO:root:current train perplexity3.623196840286255
INFO:root:current mean train loss 1635.178983102049
INFO:root:current train perplexity3.6216983795166016
INFO:root:current mean train loss 1635.5017186158093
INFO:root:current train perplexity3.6241228580474854
INFO:root:current mean train loss 1634.2995305263257
INFO:root:current train perplexity3.6254279613494873
INFO:root:current mean train loss 1634.7561930461102
INFO:root:current train perplexity3.6246750354766846
INFO:root:current mean train loss 1635.3862301268164
INFO:root:current train perplexity3.6265292167663574
INFO:root:current mean train loss 1635.507141592615
INFO:root:current train perplexity3.6265487670898438
INFO:root:current mean train loss 1635.425950558824
INFO:root:current train perplexity3.6270558834075928
INFO:root:current mean train loss 1634.9542711046006
INFO:root:current train perplexity3.627607583999634
INFO:root:current mean train loss 1634.7447483054398
INFO:root:current train perplexity3.627883195877075
INFO:root:current mean train loss 1634.4408690899734
INFO:root:current train perplexity3.6289687156677246

100%|██████████| 1/1 [02:17<00:00, 137.97s/it][A100%|██████████| 1/1 [02:17<00:00, 137.97s/it]
INFO:root:final mean train loss: 1634.7159230226948
INFO:root:final train perplexity: 3.630009889602661
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.08s/it][A100%|██████████| 1/1 [00:07<00:00,  7.08s/it]
INFO:root:eval mean loss: 2146.088037369099
INFO:root:eval perplexity: 5.672499656677246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/138
 69%|██████▉   | 138/200 [5:35:17<2:30:23, 145.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1640.9460530598958
INFO:root:current train perplexity3.5734915733337402
INFO:root:current mean train loss 1635.3697088833512
INFO:root:current train perplexity3.6089632511138916
INFO:root:current mean train loss 1632.434912607621
INFO:root:current train perplexity3.6044888496398926
INFO:root:current mean train loss 1629.8106891134512
INFO:root:current train perplexity3.605701208114624
INFO:root:current mean train loss 1627.2974576457163
INFO:root:current train perplexity3.60384464263916
INFO:root:current mean train loss 1628.4399808271216
INFO:root:current train perplexity3.6052658557891846
INFO:root:current mean train loss 1630.3558131964633
INFO:root:current train perplexity3.605862855911255
INFO:root:current mean train loss 1630.7999120110633
INFO:root:current train perplexity3.608590841293335
INFO:root:current mean train loss 1630.1039501664202
INFO:root:current train perplexity3.6129751205444336
INFO:root:current mean train loss 1628.4784974113343
INFO:root:current train perplexity3.6118783950805664
INFO:root:current mean train loss 1629.3441538249476
INFO:root:current train perplexity3.613083839416504
INFO:root:current mean train loss 1630.0106233155363
INFO:root:current train perplexity3.6155283451080322
INFO:root:current mean train loss 1629.3641168188378
INFO:root:current train perplexity3.6162595748901367
INFO:root:current mean train loss 1629.418518021027
INFO:root:current train perplexity3.6164233684539795
INFO:root:current mean train loss 1630.1591378710261
INFO:root:current train perplexity3.6171183586120605
INFO:root:current mean train loss 1630.4966893267092
INFO:root:current train perplexity3.619372844696045
INFO:root:current mean train loss 1630.497546572236
INFO:root:current train perplexity3.6200714111328125
INFO:root:current mean train loss 1631.417578964452
INFO:root:current train perplexity3.6208856105804443
INFO:root:current mean train loss 1632.266071135565
INFO:root:current train perplexity3.6221508979797363
INFO:root:current mean train loss 1632.9355732346562
INFO:root:current train perplexity3.623431444168091

100%|██████████| 1/1 [02:18<00:00, 138.44s/it][A100%|██████████| 1/1 [02:18<00:00, 138.44s/it]
INFO:root:final mean train loss: 1632.4997476413282
INFO:root:final train perplexity: 3.6236705780029297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2147.5921262916945
INFO:root:eval perplexity: 5.679404258728027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/139
 70%|██████▉   | 139/200 [5:37:43<2:28:05, 145.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.6029643397178
INFO:root:current train perplexity3.604520082473755
INFO:root:current mean train loss 1634.0327984845196
INFO:root:current train perplexity3.6028928756713867
INFO:root:current mean train loss 1633.8343575746958
INFO:root:current train perplexity3.6145591735839844
INFO:root:current mean train loss 1629.9614348859418
INFO:root:current train perplexity3.6171157360076904
INFO:root:current mean train loss 1629.8059972457556
INFO:root:current train perplexity3.613900899887085
INFO:root:current mean train loss 1633.4738180900383
INFO:root:current train perplexity3.61973237991333
INFO:root:current mean train loss 1633.4286406825315
INFO:root:current train perplexity3.618359088897705
INFO:root:current mean train loss 1633.2056037322118
INFO:root:current train perplexity3.619614601135254
INFO:root:current mean train loss 1631.685480458554
INFO:root:current train perplexity3.6185545921325684
INFO:root:current mean train loss 1630.5550883525127
INFO:root:current train perplexity3.616894483566284
INFO:root:current mean train loss 1630.8958395402983
INFO:root:current train perplexity3.618621826171875
INFO:root:current mean train loss 1632.0713506229158
INFO:root:current train perplexity3.618887424468994
INFO:root:current mean train loss 1631.7994880978542
INFO:root:current train perplexity3.6186766624450684
INFO:root:current mean train loss 1631.8498236702403
INFO:root:current train perplexity3.618467330932617
INFO:root:current mean train loss 1632.2922819166274
INFO:root:current train perplexity3.618896961212158
INFO:root:current mean train loss 1633.3369563416543
INFO:root:current train perplexity3.6209821701049805
INFO:root:current mean train loss 1632.6599443529774
INFO:root:current train perplexity3.6205954551696777
INFO:root:current mean train loss 1632.7047949800697
INFO:root:current train perplexity3.6205241680145264
INFO:root:current mean train loss 1632.5636134490803
INFO:root:current train perplexity3.620255708694458
INFO:root:current mean train loss 1632.5065298284594
INFO:root:current train perplexity3.6217031478881836

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1631.9977196613106
INFO:root:final train perplexity: 3.622236728668213
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2148.4073438712044
INFO:root:eval perplexity: 5.683150768280029
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/140
 70%|███████   | 140/200 [5:40:09<2:25:43, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1640.834265600277
INFO:root:current train perplexity3.592837333679199
INFO:root:current mean train loss 1632.9337069548708
INFO:root:current train perplexity3.5978009700775146
INFO:root:current mean train loss 1622.3243303196405
INFO:root:current train perplexity3.6035959720611572
INFO:root:current mean train loss 1626.0268683521604
INFO:root:current train perplexity3.6099207401275635
INFO:root:current mean train loss 1629.0495325140266
INFO:root:current train perplexity3.6145596504211426
INFO:root:current mean train loss 1627.6219950463487
INFO:root:current train perplexity3.6113274097442627
INFO:root:current mean train loss 1629.883415121042
INFO:root:current train perplexity3.6132612228393555
INFO:root:current mean train loss 1628.4373233976353
INFO:root:current train perplexity3.6113944053649902
INFO:root:current mean train loss 1628.6003994296163
INFO:root:current train perplexity3.6134955883026123
INFO:root:current mean train loss 1627.8524941745404
INFO:root:current train perplexity3.6134896278381348
INFO:root:current mean train loss 1629.4276910451301
INFO:root:current train perplexity3.6139557361602783
INFO:root:current mean train loss 1629.3949134056602
INFO:root:current train perplexity3.615135908126831
INFO:root:current mean train loss 1630.2066144548048
INFO:root:current train perplexity3.614936351776123
INFO:root:current mean train loss 1630.2048799267188
INFO:root:current train perplexity3.6141085624694824
INFO:root:current mean train loss 1631.1316434642283
INFO:root:current train perplexity3.6163642406463623
INFO:root:current mean train loss 1630.0759409541492
INFO:root:current train perplexity3.6155219078063965
INFO:root:current mean train loss 1629.902445463143
INFO:root:current train perplexity3.6148898601531982
INFO:root:current mean train loss 1630.340604785321
INFO:root:current train perplexity3.616518974304199
INFO:root:current mean train loss 1630.8689754825123
INFO:root:current train perplexity3.6160969734191895
INFO:root:current mean train loss 1630.3975068295224
INFO:root:current train perplexity3.6164724826812744

100%|██████████| 1/1 [02:18<00:00, 138.30s/it][A100%|██████████| 1/1 [02:18<00:00, 138.30s/it]
INFO:root:final mean train loss: 1629.9055898169106
INFO:root:final train perplexity: 3.6162643432617188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2150.3496089421265
INFO:root:eval perplexity: 5.692084312438965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/141
 70%|███████   | 141/200 [5:42:35<2:23:19, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1617.5590782165527
INFO:root:current train perplexity3.5813519954681396
INFO:root:current mean train loss 1618.0364959093988
INFO:root:current train perplexity3.574021339416504
INFO:root:current mean train loss 1616.391625481683
INFO:root:current train perplexity3.5776853561401367
INFO:root:current mean train loss 1619.3159691396386
INFO:root:current train perplexity3.581139326095581
INFO:root:current mean train loss 1622.1919235721712
INFO:root:current train perplexity3.5884640216827393
INFO:root:current mean train loss 1624.4954057731884
INFO:root:current train perplexity3.593998670578003
INFO:root:current mean train loss 1625.5641062067843
INFO:root:current train perplexity3.5955491065979004
INFO:root:current mean train loss 1625.874529507891
INFO:root:current train perplexity3.598233222961426
INFO:root:current mean train loss 1626.149503162929
INFO:root:current train perplexity3.6017513275146484
INFO:root:current mean train loss 1626.9382886771696
INFO:root:current train perplexity3.603443145751953
INFO:root:current mean train loss 1628.2660542508982
INFO:root:current train perplexity3.6037497520446777
INFO:root:current mean train loss 1628.6379913023882
INFO:root:current train perplexity3.605445384979248
INFO:root:current mean train loss 1627.7360489456742
INFO:root:current train perplexity3.606708288192749
INFO:root:current mean train loss 1627.961939508389
INFO:root:current train perplexity3.6072041988372803
INFO:root:current mean train loss 1628.8696038557246
INFO:root:current train perplexity3.6067686080932617
INFO:root:current mean train loss 1629.766034883963
INFO:root:current train perplexity3.6084823608398438
INFO:root:current mean train loss 1629.3125969508908
INFO:root:current train perplexity3.60965895652771
INFO:root:current mean train loss 1629.0519292954614
INFO:root:current train perplexity3.6103832721710205
INFO:root:current mean train loss 1628.6561969483453
INFO:root:current train perplexity3.6107499599456787

100%|██████████| 1/1 [02:18<00:00, 138.50s/it][A100%|██████████| 1/1 [02:18<00:00, 138.50s/it]
INFO:root:final mean train loss: 1628.4658831020706
INFO:root:final train perplexity: 3.6121609210968018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.10s/it][A100%|██████████| 1/1 [00:07<00:00,  7.10s/it]
INFO:root:eval mean loss: 2150.44995809785
INFO:root:eval perplexity: 5.69254732131958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/142
 71%|███████   | 142/200 [5:45:01<2:20:55, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1631.5853834885818
INFO:root:current train perplexity3.5868914127349854
INFO:root:current mean train loss 1618.4750954957135
INFO:root:current train perplexity3.584768533706665
INFO:root:current mean train loss 1627.3284631290346
INFO:root:current train perplexity3.598454236984253
INFO:root:current mean train loss 1624.1170946797624
INFO:root:current train perplexity3.587458372116089
INFO:root:current mean train loss 1622.4145073324946
INFO:root:current train perplexity3.5971341133117676
INFO:root:current mean train loss 1624.665895696272
INFO:root:current train perplexity3.597146511077881
INFO:root:current mean train loss 1626.6708179865925
INFO:root:current train perplexity3.6000587940216064
INFO:root:current mean train loss 1625.5936051592084
INFO:root:current train perplexity3.6006948947906494
INFO:root:current mean train loss 1625.617019184108
INFO:root:current train perplexity3.6004695892333984
INFO:root:current mean train loss 1624.5577212079854
INFO:root:current train perplexity3.5990796089172363
INFO:root:current mean train loss 1625.9797233137185
INFO:root:current train perplexity3.603933572769165
INFO:root:current mean train loss 1627.136645595554
INFO:root:current train perplexity3.603764772415161
INFO:root:current mean train loss 1626.4922458683275
INFO:root:current train perplexity3.60129976272583
INFO:root:current mean train loss 1624.712334940112
INFO:root:current train perplexity3.600109100341797
INFO:root:current mean train loss 1625.3088796174197
INFO:root:current train perplexity3.6015663146972656
INFO:root:current mean train loss 1625.392141398892
INFO:root:current train perplexity3.60296630859375
INFO:root:current mean train loss 1625.6116087429286
INFO:root:current train perplexity3.6062088012695312
INFO:root:current mean train loss 1625.9773363245904
INFO:root:current train perplexity3.6083645820617676
INFO:root:current mean train loss 1626.4902393574616
INFO:root:current train perplexity3.609314441680908
INFO:root:current mean train loss 1627.9499625302208
INFO:root:current train perplexity3.609734296798706

100%|██████████| 1/1 [02:18<00:00, 138.61s/it][A100%|██████████| 1/1 [02:18<00:00, 138.61s/it]
INFO:root:final mean train loss: 1627.7053323919822
INFO:root:final train perplexity: 3.609994649887085
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2150.4183340951904
INFO:root:eval perplexity: 5.69240140914917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/143
 72%|███████▏  | 143/200 [5:47:27<2:18:35, 145.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.7780802408854
INFO:root:current train perplexity3.604267120361328
INFO:root:current mean train loss 1623.2293644831732
INFO:root:current train perplexity3.5790798664093018
INFO:root:current mean train loss 1627.3341425356657
INFO:root:current train perplexity3.5843288898468018
INFO:root:current mean train loss 1625.8018599076704
INFO:root:current train perplexity3.5896286964416504
INFO:root:current mean train loss 1625.4693924305052
INFO:root:current train perplexity3.590987205505371
INFO:root:current mean train loss 1624.0521756154187
INFO:root:current train perplexity3.5911471843719482
INFO:root:current mean train loss 1624.4102376302083
INFO:root:current train perplexity3.5909245014190674
INFO:root:current mean train loss 1625.3663211352205
INFO:root:current train perplexity3.5944700241088867
INFO:root:current mean train loss 1624.5054608080761
INFO:root:current train perplexity3.5925168991088867
INFO:root:current mean train loss 1623.9677051831318
INFO:root:current train perplexity3.593977212905884
INFO:root:current mean train loss 1625.2758972760544
INFO:root:current train perplexity3.5979998111724854
INFO:root:current mean train loss 1625.208724786539
INFO:root:current train perplexity3.599001884460449
INFO:root:current mean train loss 1625.912119001683
INFO:root:current train perplexity3.601149559020996
INFO:root:current mean train loss 1625.4276043196369
INFO:root:current train perplexity3.6008975505828857
INFO:root:current mean train loss 1624.417733999399
INFO:root:current train perplexity3.6020309925079346
INFO:root:current mean train loss 1624.0338190614787
INFO:root:current train perplexity3.6022069454193115
INFO:root:current mean train loss 1624.6410139025354
INFO:root:current train perplexity3.6025912761688232
INFO:root:current mean train loss 1625.0053220539423
INFO:root:current train perplexity3.603583574295044
INFO:root:current mean train loss 1625.437339974492
INFO:root:current train perplexity3.6040396690368652
INFO:root:current mean train loss 1626.4912227017892
INFO:root:current train perplexity3.604832649230957

100%|██████████| 1/1 [02:18<00:00, 138.65s/it][A100%|██████████| 1/1 [02:18<00:00, 138.65s/it]
INFO:root:final mean train loss: 1625.8726783864015
INFO:root:final train perplexity: 3.604780912399292
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.25s/it][A100%|██████████| 1/1 [00:07<00:00,  7.25s/it]
INFO:root:eval mean loss: 2151.1452918086493
INFO:root:eval perplexity: 5.6957478523254395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/144
 72%|███████▏  | 144/200 [5:49:53<2:16:14, 145.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1628.8427344788897
INFO:root:current train perplexity3.632031202316284
INFO:root:current mean train loss 1619.8442980707907
INFO:root:current train perplexity3.567157030105591
INFO:root:current mean train loss 1617.4376630898912
INFO:root:current train perplexity3.5715250968933105
INFO:root:current mean train loss 1617.1718577624054
INFO:root:current train perplexity3.5759716033935547
INFO:root:current mean train loss 1618.8920974902126
INFO:root:current train perplexity3.5720889568328857
INFO:root:current mean train loss 1620.3257438478347
INFO:root:current train perplexity3.575380802154541
INFO:root:current mean train loss 1621.9231551345754
INFO:root:current train perplexity3.582528829574585
INFO:root:current mean train loss 1622.9556244182459
INFO:root:current train perplexity3.5888686180114746
INFO:root:current mean train loss 1622.405452435526
INFO:root:current train perplexity3.588622808456421
INFO:root:current mean train loss 1622.717996953785
INFO:root:current train perplexity3.5888991355895996
INFO:root:current mean train loss 1622.1126993465332
INFO:root:current train perplexity3.588810920715332
INFO:root:current mean train loss 1622.9120689332226
INFO:root:current train perplexity3.589378833770752
INFO:root:current mean train loss 1622.6987461313402
INFO:root:current train perplexity3.591757297515869
INFO:root:current mean train loss 1624.0603215841338
INFO:root:current train perplexity3.5931496620178223
INFO:root:current mean train loss 1623.891086370103
INFO:root:current train perplexity3.594785690307617
INFO:root:current mean train loss 1624.0064159903745
INFO:root:current train perplexity3.5959973335266113
INFO:root:current mean train loss 1623.9891682053308
INFO:root:current train perplexity3.597909927368164
INFO:root:current mean train loss 1624.4466719035088
INFO:root:current train perplexity3.598012685775757
INFO:root:current mean train loss 1624.908744279152
INFO:root:current train perplexity3.5996339321136475
INFO:root:current mean train loss 1624.7936167195326
INFO:root:current train perplexity3.599522829055786

100%|██████████| 1/1 [02:18<00:00, 138.03s/it][A100%|██████████| 1/1 [02:18<00:00, 138.03s/it]
INFO:root:final mean train loss: 1624.2669339470951
INFO:root:final train perplexity: 3.6002185344696045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2151.390478255901
INFO:root:eval perplexity: 5.696878433227539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/145
 72%|███████▎  | 145/200 [5:52:19<2:13:40, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1617.6863555908203
INFO:root:current train perplexity3.597682476043701
INFO:root:current mean train loss 1619.975381246427
INFO:root:current train perplexity3.586217164993286
INFO:root:current mean train loss 1612.68896484375
INFO:root:current train perplexity3.5705604553222656
INFO:root:current mean train loss 1615.4757442264765
INFO:root:current train perplexity3.5822622776031494
INFO:root:current mean train loss 1616.2455723203461
INFO:root:current train perplexity3.5869216918945312
INFO:root:current mean train loss 1616.7218762120456
INFO:root:current train perplexity3.585414409637451
INFO:root:current mean train loss 1618.3450843167593
INFO:root:current train perplexity3.5870730876922607
INFO:root:current mean train loss 1617.7394950826754
INFO:root:current train perplexity3.586199998855591
INFO:root:current mean train loss 1618.8766653272842
INFO:root:current train perplexity3.5888705253601074
INFO:root:current mean train loss 1618.465382302945
INFO:root:current train perplexity3.590977191925049
INFO:root:current mean train loss 1618.5434688482069
INFO:root:current train perplexity3.5915582180023193
INFO:root:current mean train loss 1618.8611337982911
INFO:root:current train perplexity3.5890607833862305
INFO:root:current mean train loss 1620.2958527577075
INFO:root:current train perplexity3.5921273231506348
INFO:root:current mean train loss 1621.7608015222634
INFO:root:current train perplexity3.593433141708374
INFO:root:current mean train loss 1622.3893181639291
INFO:root:current train perplexity3.5940046310424805
INFO:root:current mean train loss 1622.3271904284386
INFO:root:current train perplexity3.5937650203704834
INFO:root:current mean train loss 1623.2972700412456
INFO:root:current train perplexity3.5953428745269775
INFO:root:current mean train loss 1623.0419680364007
INFO:root:current train perplexity3.596449613571167
INFO:root:current mean train loss 1623.4118102241484
INFO:root:current train perplexity3.5959842205047607
INFO:root:current mean train loss 1623.2432039031671
INFO:root:current train perplexity3.5959184169769287

100%|██████████| 1/1 [02:18<00:00, 138.32s/it][A100%|██████████| 1/1 [02:18<00:00, 138.32s/it]
INFO:root:final mean train loss: 1622.9111695936456
INFO:root:final train perplexity: 3.596372127532959
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2153.253966419409
INFO:root:eval perplexity: 5.705470085144043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/146
 73%|███████▎  | 146/200 [5:54:45<2:11:15, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1612.758573555652
INFO:root:current train perplexity3.602307081222534
INFO:root:current mean train loss 1618.3024410015971
INFO:root:current train perplexity3.6045897006988525
INFO:root:current mean train loss 1618.9946675690892
INFO:root:current train perplexity3.5944125652313232
INFO:root:current mean train loss 1616.562432396756
INFO:root:current train perplexity3.5888538360595703
INFO:root:current mean train loss 1616.032193824308
INFO:root:current train perplexity3.584078550338745
INFO:root:current mean train loss 1618.5224437089878
INFO:root:current train perplexity3.5871620178222656
INFO:root:current mean train loss 1619.6140789194428
INFO:root:current train perplexity3.5924112796783447
INFO:root:current mean train loss 1620.243797546465
INFO:root:current train perplexity3.5900697708129883
INFO:root:current mean train loss 1620.234734144438
INFO:root:current train perplexity3.5914862155914307
INFO:root:current mean train loss 1620.2732313119186
INFO:root:current train perplexity3.5917201042175293
INFO:root:current mean train loss 1619.4897510623844
INFO:root:current train perplexity3.5886740684509277
INFO:root:current mean train loss 1619.684586540306
INFO:root:current train perplexity3.5909249782562256
INFO:root:current mean train loss 1619.8258119533994
INFO:root:current train perplexity3.591580390930176
INFO:root:current mean train loss 1621.1963332588477
INFO:root:current train perplexity3.5908219814300537
INFO:root:current mean train loss 1621.0282823973455
INFO:root:current train perplexity3.592682361602783
INFO:root:current mean train loss 1620.5366130638242
INFO:root:current train perplexity3.591932535171509
INFO:root:current mean train loss 1621.742075596162
INFO:root:current train perplexity3.5919063091278076
INFO:root:current mean train loss 1621.5579199300998
INFO:root:current train perplexity3.592367172241211
INFO:root:current mean train loss 1621.8430699495989
INFO:root:current train perplexity3.593240261077881
INFO:root:current mean train loss 1622.0734846643702
INFO:root:current train perplexity3.592766523361206

100%|██████████| 1/1 [02:17<00:00, 137.58s/it][A100%|██████████| 1/1 [02:17<00:00, 137.59s/it]
INFO:root:final mean train loss: 1621.6881399919334
INFO:root:final train perplexity: 3.592904567718506
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2154.11946571296
INFO:root:eval perplexity: 5.709465503692627
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/147
 74%|███████▎  | 147/200 [5:57:10<2:08:37, 145.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1606.6197248186384
INFO:root:current train perplexity3.5495684146881104
INFO:root:current mean train loss 1613.1480232007575
INFO:root:current train perplexity3.563528299331665
INFO:root:current mean train loss 1615.6087294200922
INFO:root:current train perplexity3.5693795680999756
INFO:root:current mean train loss 1609.2775593666574
INFO:root:current train perplexity3.564603567123413
INFO:root:current mean train loss 1611.2296934319309
INFO:root:current train perplexity3.5716235637664795
INFO:root:current mean train loss 1613.8630107764816
INFO:root:current train perplexity3.5718071460723877
INFO:root:current mean train loss 1614.608432365352
INFO:root:current train perplexity3.5745370388031006
INFO:root:current mean train loss 1616.0669020267956
INFO:root:current train perplexity3.5720489025115967
INFO:root:current mean train loss 1616.1874593552077
INFO:root:current train perplexity3.573499917984009
INFO:root:current mean train loss 1617.732239258791
INFO:root:current train perplexity3.5774660110473633
INFO:root:current mean train loss 1617.472747858322
INFO:root:current train perplexity3.5786049365997314
INFO:root:current mean train loss 1618.0557215313283
INFO:root:current train perplexity3.578989267349243
INFO:root:current mean train loss 1619.1366453949586
INFO:root:current train perplexity3.5778648853302
INFO:root:current mean train loss 1619.377058517609
INFO:root:current train perplexity3.5807154178619385
INFO:root:current mean train loss 1620.1087970810038
INFO:root:current train perplexity3.583573579788208
INFO:root:current mean train loss 1619.7367759962403
INFO:root:current train perplexity3.5836803913116455
INFO:root:current mean train loss 1619.9377392520612
INFO:root:current train perplexity3.58406925201416
INFO:root:current mean train loss 1620.6840607809675
INFO:root:current train perplexity3.5862090587615967
INFO:root:current mean train loss 1620.6552923461786
INFO:root:current train perplexity3.587068796157837

100%|██████████| 1/1 [02:18<00:00, 138.45s/it][A100%|██████████| 1/1 [02:18<00:00, 138.45s/it]
INFO:root:final mean train loss: 1619.8683602799085
INFO:root:final train perplexity: 3.5877513885498047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.20s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2154.3736130734706
INFO:root:eval perplexity: 5.710638046264648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/148
 74%|███████▍  | 148/200 [5:59:36<2:06:16, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1605.824267578125
INFO:root:current train perplexity3.553819417953491
INFO:root:current mean train loss 1615.1149690047555
INFO:root:current train perplexity3.602616310119629
INFO:root:current mean train loss 1611.5615251408067
INFO:root:current train perplexity3.5907504558563232
INFO:root:current mean train loss 1607.6865028986856
INFO:root:current train perplexity3.580472946166992
INFO:root:current mean train loss 1609.823401908415
INFO:root:current train perplexity3.5784552097320557
INFO:root:current mean train loss 1611.7110287564471
INFO:root:current train perplexity3.5788753032684326
INFO:root:current mean train loss 1609.4539084333715
INFO:root:current train perplexity3.572209358215332
INFO:root:current mean train loss 1609.6246913243008
INFO:root:current train perplexity3.571281671524048
INFO:root:current mean train loss 1611.5110092443922
INFO:root:current train perplexity3.5763914585113525
INFO:root:current mean train loss 1611.551280070654
INFO:root:current train perplexity3.575761556625366
INFO:root:current mean train loss 1611.8111170576124
INFO:root:current train perplexity3.5796549320220947
INFO:root:current mean train loss 1612.7006288536995
INFO:root:current train perplexity3.580078601837158
INFO:root:current mean train loss 1614.6683768566743
INFO:root:current train perplexity3.579348087310791
INFO:root:current mean train loss 1614.7569671978968
INFO:root:current train perplexity3.5806071758270264
INFO:root:current mean train loss 1615.248657830444
INFO:root:current train perplexity3.57961106300354
INFO:root:current mean train loss 1616.0087909157126
INFO:root:current train perplexity3.57924485206604
INFO:root:current mean train loss 1616.524172416191
INFO:root:current train perplexity3.5797765254974365
INFO:root:current mean train loss 1616.4747293811497
INFO:root:current train perplexity3.5800552368164062
INFO:root:current mean train loss 1617.346417051373
INFO:root:current train perplexity3.581583023071289
INFO:root:current mean train loss 1618.6059069919836
INFO:root:current train perplexity3.583003282546997

100%|██████████| 1/1 [02:18<00:00, 138.35s/it][A100%|██████████| 1/1 [02:18<00:00, 138.35s/it]
INFO:root:final mean train loss: 1618.49122287983
INFO:root:final train perplexity: 3.583857297897339
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2155.003929625166
INFO:root:eval perplexity: 5.713550567626953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/149
 74%|███████▍  | 149/200 [6:02:01<2:03:53, 145.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.267967224121
INFO:root:current train perplexity3.515738010406494
INFO:root:current mean train loss 1610.5619386615176
INFO:root:current train perplexity3.557668685913086
INFO:root:current mean train loss 1609.732748623552
INFO:root:current train perplexity3.567039728164673
INFO:root:current mean train loss 1611.5611355333442
INFO:root:current train perplexity3.5604441165924072
INFO:root:current mean train loss 1612.0960396660698
INFO:root:current train perplexity3.5633909702301025
INFO:root:current mean train loss 1610.915920859889
INFO:root:current train perplexity3.568324089050293
INFO:root:current mean train loss 1613.8817938309683
INFO:root:current train perplexity3.5703647136688232
INFO:root:current mean train loss 1614.1490496859524
INFO:root:current train perplexity3.56952166557312
INFO:root:current mean train loss 1611.4684021289531
INFO:root:current train perplexity3.5674827098846436
INFO:root:current mean train loss 1613.6197204589844
INFO:root:current train perplexity3.5688345432281494
INFO:root:current mean train loss 1615.4707077381222
INFO:root:current train perplexity3.5714757442474365
INFO:root:current mean train loss 1617.0839238790236
INFO:root:current train perplexity3.57133150100708
INFO:root:current mean train loss 1616.7980694461178
INFO:root:current train perplexity3.572892189025879
INFO:root:current mean train loss 1616.4428697190842
INFO:root:current train perplexity3.5727550983428955
INFO:root:current mean train loss 1617.3535329296603
INFO:root:current train perplexity3.575906276702881
INFO:root:current mean train loss 1617.2659316897082
INFO:root:current train perplexity3.5772037506103516
INFO:root:current mean train loss 1616.9620368059943
INFO:root:current train perplexity3.57814884185791
INFO:root:current mean train loss 1616.9965935193914
INFO:root:current train perplexity3.577666997909546
INFO:root:current mean train loss 1616.7360991765318
INFO:root:current train perplexity3.5778310298919678
INFO:root:current mean train loss 1617.1319158013068
INFO:root:current train perplexity3.5787503719329834

100%|██████████| 1/1 [02:18<00:00, 138.29s/it][A100%|██████████| 1/1 [02:18<00:00, 138.29s/it]
INFO:root:final mean train loss: 1617.0855033593652
INFO:root:final train perplexity: 3.579885721206665
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2156.6682328028037
INFO:root:eval perplexity: 5.721244812011719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/150
 75%|███████▌  | 150/200 [6:04:27<2:01:27, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1622.8689762037627
INFO:root:current train perplexity3.5745434761047363
INFO:root:current mean train loss 1613.9103520540582
INFO:root:current train perplexity3.555644989013672
INFO:root:current mean train loss 1613.218241618819
INFO:root:current train perplexity3.551823139190674
INFO:root:current mean train loss 1607.7603864697126
INFO:root:current train perplexity3.556270122528076
INFO:root:current mean train loss 1610.4385007590654
INFO:root:current train perplexity3.5611445903778076
INFO:root:current mean train loss 1609.672552501352
INFO:root:current train perplexity3.5563032627105713
INFO:root:current mean train loss 1611.3025566300919
INFO:root:current train perplexity3.559319257736206
INFO:root:current mean train loss 1613.0480218742177
INFO:root:current train perplexity3.559295892715454
INFO:root:current mean train loss 1612.6479508003442
INFO:root:current train perplexity3.5613622665405273
INFO:root:current mean train loss 1611.6854696967202
INFO:root:current train perplexity3.5624213218688965
INFO:root:current mean train loss 1612.4547481045936
INFO:root:current train perplexity3.56583309173584
INFO:root:current mean train loss 1612.8486676593777
INFO:root:current train perplexity3.5676393508911133
INFO:root:current mean train loss 1613.9554330964772
INFO:root:current train perplexity3.5700855255126953
INFO:root:current mean train loss 1614.1046371516518
INFO:root:current train perplexity3.5720672607421875
INFO:root:current mean train loss 1615.2470387208043
INFO:root:current train perplexity3.5731120109558105
INFO:root:current mean train loss 1615.1016118324826
INFO:root:current train perplexity3.573641777038574
INFO:root:current mean train loss 1614.603267412897
INFO:root:current train perplexity3.5735416412353516
INFO:root:current mean train loss 1614.4490056678594
INFO:root:current train perplexity3.5737013816833496
INFO:root:current mean train loss 1615.7609571474445
INFO:root:current train perplexity3.5760836601257324
INFO:root:current mean train loss 1616.0032125348737
INFO:root:current train perplexity3.5764293670654297

100%|██████████| 1/1 [02:18<00:00, 138.43s/it][A100%|██████████| 1/1 [02:18<00:00, 138.43s/it]
INFO:root:final mean train loss: 1616.0104702532562
INFO:root:final train perplexity: 3.5768516063690186
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2155.8597178357713
INFO:root:eval perplexity: 5.717506408691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/151
 76%|███████▌  | 151/200 [6:06:53<1:59:02, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.7757401899858
INFO:root:current train perplexity3.557807207107544
INFO:root:current mean train loss 1606.4548803122648
INFO:root:current train perplexity3.5638184547424316
INFO:root:current mean train loss 1606.3298752863604
INFO:root:current train perplexity3.557776689529419
INFO:root:current mean train loss 1606.61607185739
INFO:root:current train perplexity3.56058931350708
INFO:root:current mean train loss 1607.7040415751576
INFO:root:current train perplexity3.559231758117676
INFO:root:current mean train loss 1606.6921229278241
INFO:root:current train perplexity3.56203031539917
INFO:root:current mean train loss 1607.7868615685998
INFO:root:current train perplexity3.565460205078125
INFO:root:current mean train loss 1609.3873558741636
INFO:root:current train perplexity3.567406177520752
INFO:root:current mean train loss 1610.4004852083483
INFO:root:current train perplexity3.5673606395721436
INFO:root:current mean train loss 1611.3997575274166
INFO:root:current train perplexity3.5681021213531494
INFO:root:current mean train loss 1611.2896317415791
INFO:root:current train perplexity3.569312334060669
INFO:root:current mean train loss 1612.3977208865444
INFO:root:current train perplexity3.569481372833252
INFO:root:current mean train loss 1613.5444753444967
INFO:root:current train perplexity3.5709304809570312
INFO:root:current mean train loss 1612.6131160171979
INFO:root:current train perplexity3.5696654319763184
INFO:root:current mean train loss 1613.655858059372
INFO:root:current train perplexity3.570558547973633
INFO:root:current mean train loss 1614.6090374748064
INFO:root:current train perplexity3.5712850093841553
INFO:root:current mean train loss 1614.0786700666595
INFO:root:current train perplexity3.571005344390869
INFO:root:current mean train loss 1614.2252077683722
INFO:root:current train perplexity3.5716869831085205
INFO:root:current mean train loss 1614.7939340605742
INFO:root:current train perplexity3.5719804763793945
INFO:root:current mean train loss 1615.6773603406346
INFO:root:current train perplexity3.574375629425049

100%|██████████| 1/1 [02:18<00:00, 138.36s/it][A100%|██████████| 1/1 [02:18<00:00, 138.36s/it]
INFO:root:final mean train loss: 1615.14479755165
INFO:root:final train perplexity: 3.5744104385375977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2156.5200048135525
INFO:root:eval perplexity: 5.720560073852539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/152
 76%|███████▌  | 152/200 [6:09:19<1:56:36, 145.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1597.3230421686746
INFO:root:current train perplexity3.5669734477996826
INFO:root:current mean train loss 1604.341457346098
INFO:root:current train perplexity3.5611021518707275
INFO:root:current mean train loss 1609.0269853032519
INFO:root:current train perplexity3.5679585933685303
INFO:root:current mean train loss 1609.9970884796223
INFO:root:current train perplexity3.573103904724121
INFO:root:current mean train loss 1606.669455076103
INFO:root:current train perplexity3.5646281242370605
INFO:root:current mean train loss 1608.3319730415149
INFO:root:current train perplexity3.5646491050720215
INFO:root:current mean train loss 1610.4581284529993
INFO:root:current train perplexity3.564720392227173
INFO:root:current mean train loss 1611.3057788054757
INFO:root:current train perplexity3.565499782562256
INFO:root:current mean train loss 1611.572682986578
INFO:root:current train perplexity3.564073324203491
INFO:root:current mean train loss 1611.136139567968
INFO:root:current train perplexity3.5658798217773438
INFO:root:current mean train loss 1611.4835318920245
INFO:root:current train perplexity3.5656793117523193
INFO:root:current mean train loss 1611.221354132271
INFO:root:current train perplexity3.5625720024108887
INFO:root:current mean train loss 1613.1664124724462
INFO:root:current train perplexity3.563246965408325
INFO:root:current mean train loss 1613.5644940798989
INFO:root:current train perplexity3.5644540786743164
INFO:root:current mean train loss 1613.5042195336207
INFO:root:current train perplexity3.5656120777130127
INFO:root:current mean train loss 1612.6664057410524
INFO:root:current train perplexity3.565164804458618
INFO:root:current mean train loss 1612.6797868679905
INFO:root:current train perplexity3.5659258365631104
INFO:root:current mean train loss 1613.316335527267
INFO:root:current train perplexity3.5646746158599854
INFO:root:current mean train loss 1613.1643721164696
INFO:root:current train perplexity3.5675411224365234
INFO:root:current mean train loss 1613.2890983269904
INFO:root:current train perplexity3.569183349609375

100%|██████████| 1/1 [02:18<00:00, 138.37s/it][A100%|██████████| 1/1 [02:18<00:00, 138.37s/it]
INFO:root:final mean train loss: 1613.2890983269904
INFO:root:final train perplexity: 3.569183349609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2157.2646575278422
INFO:root:eval perplexity: 5.724006175994873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/153
 76%|███████▋  | 153/200 [6:11:45<1:54:12, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1608.0699243164063
INFO:root:current train perplexity3.542287826538086
INFO:root:current mean train loss 1606.2094323730469
INFO:root:current train perplexity3.5476038455963135
INFO:root:current mean train loss 1603.671201985677
INFO:root:current train perplexity3.54697847366333
INFO:root:current mean train loss 1605.9442193603516
INFO:root:current train perplexity3.5527689456939697
INFO:root:current mean train loss 1604.1792275390626
INFO:root:current train perplexity3.5578603744506836
INFO:root:current mean train loss 1604.4628963216146
INFO:root:current train perplexity3.5614192485809326
INFO:root:current mean train loss 1605.9377627999443
INFO:root:current train perplexity3.5612077713012695
INFO:root:current mean train loss 1607.1272389221192
INFO:root:current train perplexity3.5615835189819336
INFO:root:current mean train loss 1607.8250592719185
INFO:root:current train perplexity3.560886859893799
INFO:root:current mean train loss 1609.2760531005858
INFO:root:current train perplexity3.562779426574707
INFO:root:current mean train loss 1608.704120316939
INFO:root:current train perplexity3.563138723373413
INFO:root:current mean train loss 1609.4367229207357
INFO:root:current train perplexity3.563905954360962
INFO:root:current mean train loss 1610.19848435622
INFO:root:current train perplexity3.563636541366577
INFO:root:current mean train loss 1609.7522714669365
INFO:root:current train perplexity3.564391851425171
INFO:root:current mean train loss 1610.3257646484376
INFO:root:current train perplexity3.5658249855041504
INFO:root:current mean train loss 1611.1767682647705
INFO:root:current train perplexity3.5641536712646484
INFO:root:current mean train loss 1611.816515826057
INFO:root:current train perplexity3.5659377574920654
INFO:root:current mean train loss 1612.4346026611329
INFO:root:current train perplexity3.565948247909546
INFO:root:current mean train loss 1612.3942016601563
INFO:root:current train perplexity3.566620111465454

100%|██████████| 1/1 [02:18<00:00, 138.38s/it][A100%|██████████| 1/1 [02:18<00:00, 138.38s/it]
INFO:root:final mean train loss: 1612.4199110407212
INFO:root:final train perplexity: 3.566737413406372
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.18s/it][A100%|██████████| 1/1 [00:07<00:00,  7.18s/it]
INFO:root:eval mean loss: 2159.109683205895
INFO:root:eval perplexity: 5.7325544357299805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/154
 77%|███████▋  | 154/200 [6:14:10<1:51:47, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1605.5911793428309
INFO:root:current train perplexity3.578463315963745
INFO:root:current mean train loss 1620.7421488965679
INFO:root:current train perplexity3.5708675384521484
INFO:root:current mean train loss 1612.079352453557
INFO:root:current train perplexity3.5772151947021484
INFO:root:current mean train loss 1611.399740610213
INFO:root:current train perplexity3.575780153274536
INFO:root:current mean train loss 1610.1719411580111
INFO:root:current train perplexity3.567190170288086
INFO:root:current mean train loss 1610.3219669423204
INFO:root:current train perplexity3.56339693069458
INFO:root:current mean train loss 1610.5141162346788
INFO:root:current train perplexity3.563419818878174
INFO:root:current mean train loss 1612.6873689063589
INFO:root:current train perplexity3.5632927417755127
INFO:root:current mean train loss 1612.8856897435837
INFO:root:current train perplexity3.5636391639709473
INFO:root:current mean train loss 1612.9424180890899
INFO:root:current train perplexity3.5635647773742676
INFO:root:current mean train loss 1613.8089904485082
INFO:root:current train perplexity3.562939405441284
INFO:root:current mean train loss 1614.4174884464876
INFO:root:current train perplexity3.5646655559539795
INFO:root:current mean train loss 1614.5462227212472
INFO:root:current train perplexity3.5655946731567383
INFO:root:current mean train loss 1613.3897602194263
INFO:root:current train perplexity3.5645551681518555
INFO:root:current mean train loss 1613.3124498624404
INFO:root:current train perplexity3.564424991607666
INFO:root:current mean train loss 1612.9716532134507
INFO:root:current train perplexity3.5627543926239014
INFO:root:current mean train loss 1612.2942777030912
INFO:root:current train perplexity3.5637431144714355
INFO:root:current mean train loss 1612.4622066188983
INFO:root:current train perplexity3.5638201236724854
INFO:root:current mean train loss 1612.8169577498322
INFO:root:current train perplexity3.5641283988952637
INFO:root:current mean train loss 1612.1709059514785
INFO:root:current train perplexity3.564140796661377

100%|██████████| 1/1 [02:18<00:00, 138.31s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 1611.7593685117445
INFO:root:final train perplexity: 3.5648796558380127
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2159.038533961519
INFO:root:eval perplexity: 5.732224464416504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/155
 78%|███████▊  | 155/200 [6:16:36<1:49:20, 145.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1622.0146304859834
INFO:root:current train perplexity3.577829122543335
INFO:root:current mean train loss 1601.9637177880131
INFO:root:current train perplexity3.5375096797943115
INFO:root:current mean train loss 1595.3880203116653
INFO:root:current train perplexity3.5392141342163086
INFO:root:current mean train loss 1597.6998097311237
INFO:root:current train perplexity3.5419912338256836
INFO:root:current mean train loss 1600.9724796136952
INFO:root:current train perplexity3.5522165298461914
INFO:root:current mean train loss 1603.129540604152
INFO:root:current train perplexity3.552337408065796
INFO:root:current mean train loss 1603.5920267676706
INFO:root:current train perplexity3.553086519241333
INFO:root:current mean train loss 1603.9649052840812
INFO:root:current train perplexity3.5558359622955322
INFO:root:current mean train loss 1604.474578637871
INFO:root:current train perplexity3.5540709495544434
INFO:root:current mean train loss 1606.591850983254
INFO:root:current train perplexity3.5568888187408447
INFO:root:current mean train loss 1608.4865949324528
INFO:root:current train perplexity3.5573339462280273
INFO:root:current mean train loss 1609.0029289339795
INFO:root:current train perplexity3.558365821838379
INFO:root:current mean train loss 1609.2143617997872
INFO:root:current train perplexity3.557974338531494
INFO:root:current mean train loss 1609.1770838518728
INFO:root:current train perplexity3.5577778816223145
INFO:root:current mean train loss 1609.637657516671
INFO:root:current train perplexity3.559232234954834
INFO:root:current mean train loss 1610.1805930802843
INFO:root:current train perplexity3.5603694915771484
INFO:root:current mean train loss 1609.9643982755365
INFO:root:current train perplexity3.5610315799713135
INFO:root:current mean train loss 1610.896947594525
INFO:root:current train perplexity3.560563325881958
INFO:root:current mean train loss 1611.133840912431
INFO:root:current train perplexity3.562041997909546
INFO:root:current mean train loss 1611.3247239468878
INFO:root:current train perplexity3.5627241134643555

100%|██████████| 1/1 [02:18<00:00, 138.41s/it][A100%|██████████| 1/1 [02:18<00:00, 138.41s/it]
INFO:root:final mean train loss: 1610.9426658272562
INFO:root:final train perplexity: 3.562584400177002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2159.6799086983324
INFO:root:eval perplexity: 5.7351975440979
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/156
 78%|███████▊  | 156/200 [6:19:02<1:46:56, 145.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1618.288765701593
INFO:root:current train perplexity3.5340049266815186
INFO:root:current mean train loss 1602.7695732874586
INFO:root:current train perplexity3.5217020511627197
INFO:root:current mean train loss 1602.4286088069596
INFO:root:current train perplexity3.533165693283081
INFO:root:current mean train loss 1601.0890118634259
INFO:root:current train perplexity3.532137393951416
INFO:root:current mean train loss 1604.4901104100263
INFO:root:current train perplexity3.5368592739105225
INFO:root:current mean train loss 1604.0568610605008
INFO:root:current train perplexity3.542253017425537
INFO:root:current mean train loss 1606.00833453341
INFO:root:current train perplexity3.5456342697143555
INFO:root:current mean train loss 1606.477236406146
INFO:root:current train perplexity3.5469696521759033
INFO:root:current mean train loss 1607.8112190506574
INFO:root:current train perplexity3.5466251373291016
INFO:root:current mean train loss 1608.57858773762
INFO:root:current train perplexity3.547131061553955
INFO:root:current mean train loss 1607.8592741845564
INFO:root:current train perplexity3.548858165740967
INFO:root:current mean train loss 1608.1787986457089
INFO:root:current train perplexity3.549743175506592
INFO:root:current mean train loss 1607.8973742880696
INFO:root:current train perplexity3.549515962600708
INFO:root:current mean train loss 1608.9207196239186
INFO:root:current train perplexity3.5512313842773438
INFO:root:current mean train loss 1609.1418589112843
INFO:root:current train perplexity3.5545854568481445
INFO:root:current mean train loss 1609.9502889705734
INFO:root:current train perplexity3.55741548538208
INFO:root:current mean train loss 1610.104203832431
INFO:root:current train perplexity3.5578126907348633
INFO:root:current mean train loss 1609.7475830635842
INFO:root:current train perplexity3.5571584701538086
INFO:root:current mean train loss 1610.1560658723324
INFO:root:current train perplexity3.5581376552581787
INFO:root:current mean train loss 1610.3782243205608
INFO:root:current train perplexity3.5594582557678223

100%|██████████| 1/1 [02:17<00:00, 137.95s/it][A100%|██████████| 1/1 [02:17<00:00, 137.95s/it]
INFO:root:final mean train loss: 1609.8602163186893
INFO:root:final train perplexity: 3.559544801712036
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2159.882014714234
INFO:root:eval perplexity: 5.736136436462402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/157
 78%|███████▊  | 157/200 [6:21:27<1:44:24, 145.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.6745856789983
INFO:root:current train perplexity3.5371110439300537
INFO:root:current mean train loss 1599.5949133010138
INFO:root:current train perplexity3.5253446102142334
INFO:root:current mean train loss 1605.7622398262592
INFO:root:current train perplexity3.53402042388916
INFO:root:current mean train loss 1608.8925177532694
INFO:root:current train perplexity3.5457427501678467
INFO:root:current mean train loss 1609.9501261914897
INFO:root:current train perplexity3.5453662872314453
INFO:root:current mean train loss 1609.548351449026
INFO:root:current train perplexity3.550095796585083
INFO:root:current mean train loss 1609.56005987293
INFO:root:current train perplexity3.5485589504241943
INFO:root:current mean train loss 1608.9958078066509
INFO:root:current train perplexity3.549844264984131
INFO:root:current mean train loss 1609.8534378543977
INFO:root:current train perplexity3.548910140991211
INFO:root:current mean train loss 1608.688082103887
INFO:root:current train perplexity3.5497360229492188
INFO:root:current mean train loss 1609.2858620404304
INFO:root:current train perplexity3.5493242740631104
INFO:root:current mean train loss 1608.1730045684396
INFO:root:current train perplexity3.5499513149261475
INFO:root:current mean train loss 1608.9268469584854
INFO:root:current train perplexity3.5520613193511963
INFO:root:current mean train loss 1609.2547830503586
INFO:root:current train perplexity3.5533580780029297
INFO:root:current mean train loss 1609.8208079325082
INFO:root:current train perplexity3.5532562732696533
INFO:root:current mean train loss 1608.9975153864646
INFO:root:current train perplexity3.5543065071105957
INFO:root:current mean train loss 1608.408051854415
INFO:root:current train perplexity3.5538277626037598
INFO:root:current mean train loss 1608.54350204813
INFO:root:current train perplexity3.5545201301574707
INFO:root:current mean train loss 1608.8413899521756
INFO:root:current train perplexity3.5529675483703613
INFO:root:current mean train loss 1608.383369569856
INFO:root:current train perplexity3.554054021835327

100%|██████████| 1/1 [02:18<00:00, 138.07s/it][A100%|██████████| 1/1 [02:18<00:00, 138.07s/it]
INFO:root:final mean train loss: 1608.001435018704
INFO:root:final train perplexity: 3.554330348968506
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2160.844165125637
INFO:root:eval perplexity: 5.740601062774658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/158
 79%|███████▉  | 158/200 [6:23:53<1:41:56, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.16990033318
INFO:root:current train perplexity3.569808006286621
INFO:root:current mean train loss 1595.9051559860643
INFO:root:current train perplexity3.5608205795288086
INFO:root:current mean train loss 1595.4458757367051
INFO:root:current train perplexity3.5484812259674072
INFO:root:current mean train loss 1599.5871287160105
INFO:root:current train perplexity3.546381950378418
INFO:root:current mean train loss 1600.7381430714402
INFO:root:current train perplexity3.548161029815674
INFO:root:current mean train loss 1600.01401701055
INFO:root:current train perplexity3.5430829524993896
INFO:root:current mean train loss 1602.9824919095004
INFO:root:current train perplexity3.5434656143188477
INFO:root:current mean train loss 1603.5027480593153
INFO:root:current train perplexity3.543597936630249
INFO:root:current mean train loss 1604.5534298309499
INFO:root:current train perplexity3.5465433597564697
INFO:root:current mean train loss 1605.0482952292195
INFO:root:current train perplexity3.548567771911621
INFO:root:current mean train loss 1605.1494227255544
INFO:root:current train perplexity3.5481338500976562
INFO:root:current mean train loss 1606.099596292359
INFO:root:current train perplexity3.5461950302124023
INFO:root:current mean train loss 1606.882298569735
INFO:root:current train perplexity3.5473785400390625
INFO:root:current mean train loss 1606.375055085881
INFO:root:current train perplexity3.5492749214172363
INFO:root:current mean train loss 1606.5608173203389
INFO:root:current train perplexity3.548819065093994
INFO:root:current mean train loss 1607.9055557010302
INFO:root:current train perplexity3.5491585731506348
INFO:root:current mean train loss 1607.998032096161
INFO:root:current train perplexity3.550065279006958
INFO:root:current mean train loss 1607.5952733827905
INFO:root:current train perplexity3.549290895462036
INFO:root:current mean train loss 1607.542371868265
INFO:root:current train perplexity3.5503902435302734

100%|██████████| 1/1 [02:18<00:00, 138.03s/it][A100%|██████████| 1/1 [02:18<00:00, 138.03s/it]
INFO:root:final mean train loss: 1606.8501365303812
INFO:root:final train perplexity: 3.5511038303375244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.11s/it][A100%|██████████| 1/1 [00:07<00:00,  7.11s/it]
INFO:root:eval mean loss: 2161.2882997769834
INFO:root:eval perplexity: 5.742663383483887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/159
 80%|███████▉  | 159/200 [6:26:18<1:39:28, 145.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1570.8968505859375
INFO:root:current train perplexity3.577131748199463
INFO:root:current mean train loss 1605.2320460899202
INFO:root:current train perplexity3.5462381839752197
INFO:root:current mean train loss 1603.2616837001083
INFO:root:current train perplexity3.539576530456543
INFO:root:current mean train loss 1607.3309912271056
INFO:root:current train perplexity3.535108804702759
INFO:root:current mean train loss 1603.9789137199743
INFO:root:current train perplexity3.538553476333618
INFO:root:current mean train loss 1601.0842158708915
INFO:root:current train perplexity3.5368635654449463
INFO:root:current mean train loss 1601.6135091686565
INFO:root:current train perplexity3.537311315536499
INFO:root:current mean train loss 1601.9843296148838
INFO:root:current train perplexity3.5367164611816406
INFO:root:current mean train loss 1601.9784742245947
INFO:root:current train perplexity3.5358774662017822
INFO:root:current mean train loss 1602.1340731263424
INFO:root:current train perplexity3.538123607635498
INFO:root:current mean train loss 1603.5097753711327
INFO:root:current train perplexity3.540318012237549
INFO:root:current mean train loss 1604.3632315135478
INFO:root:current train perplexity3.5431408882141113
INFO:root:current mean train loss 1604.5105439501872
INFO:root:current train perplexity3.542243719100952
INFO:root:current mean train loss 1605.2408605713265
INFO:root:current train perplexity3.5434470176696777
INFO:root:current mean train loss 1604.9345213798927
INFO:root:current train perplexity3.54301381111145
INFO:root:current mean train loss 1605.5614662221205
INFO:root:current train perplexity3.5453643798828125
INFO:root:current mean train loss 1605.224939543954
INFO:root:current train perplexity3.542677879333496
INFO:root:current mean train loss 1604.6209764850405
INFO:root:current train perplexity3.5427513122558594
INFO:root:current mean train loss 1605.3223475923019
INFO:root:current train perplexity3.545757532119751
INFO:root:current mean train loss 1605.3677574181781
INFO:root:current train perplexity3.547346830368042

100%|██████████| 1/1 [02:17<00:00, 137.96s/it][A100%|██████████| 1/1 [02:17<00:00, 137.96s/it]
INFO:root:final mean train loss: 1605.639462715799
INFO:root:final train perplexity: 3.547715663909912
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2161.218503262134
INFO:root:eval perplexity: 5.742340564727783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/160
 80%|████████  | 160/200 [6:28:44<1:37:01, 145.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1561.2909449527137
INFO:root:current train perplexity3.4823920726776123
INFO:root:current mean train loss 1600.5810495584954
INFO:root:current train perplexity3.5194108486175537
INFO:root:current mean train loss 1605.0788061411959
INFO:root:current train perplexity3.532189130783081
INFO:root:current mean train loss 1602.2588610036246
INFO:root:current train perplexity3.534245729446411
INFO:root:current mean train loss 1603.623647612433
INFO:root:current train perplexity3.5323736667633057
INFO:root:current mean train loss 1600.933928208544
INFO:root:current train perplexity3.53464937210083
INFO:root:current mean train loss 1603.2941346299474
INFO:root:current train perplexity3.536907434463501
INFO:root:current mean train loss 1604.7738341011823
INFO:root:current train perplexity3.540302038192749
INFO:root:current mean train loss 1603.1957509992178
INFO:root:current train perplexity3.5375564098358154
INFO:root:current mean train loss 1603.5409195892698
INFO:root:current train perplexity3.537130355834961
INFO:root:current mean train loss 1602.7915926737687
INFO:root:current train perplexity3.536752700805664
INFO:root:current mean train loss 1601.8899518133169
INFO:root:current train perplexity3.5348925590515137
INFO:root:current mean train loss 1602.550925651469
INFO:root:current train perplexity3.5375263690948486
INFO:root:current mean train loss 1602.0404314644143
INFO:root:current train perplexity3.539555788040161
INFO:root:current mean train loss 1602.9953746620915
INFO:root:current train perplexity3.5402204990386963
INFO:root:current mean train loss 1603.0885262103204
INFO:root:current train perplexity3.540342330932617
INFO:root:current mean train loss 1604.416071872346
INFO:root:current train perplexity3.5412473678588867
INFO:root:current mean train loss 1604.2605148768134
INFO:root:current train perplexity3.5429768562316895
INFO:root:current mean train loss 1604.4644607619443
INFO:root:current train perplexity3.5433008670806885
INFO:root:current mean train loss 1604.952652494382
INFO:root:current train perplexity3.5437166690826416

100%|██████████| 1/1 [02:18<00:00, 138.07s/it][A100%|██████████| 1/1 [02:18<00:00, 138.07s/it]
INFO:root:final mean train loss: 1604.6776860824332
INFO:root:final train perplexity: 3.545025587081909
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2161.7137026955897
INFO:root:eval perplexity: 5.744640350341797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/161
 80%|████████  | 161/200 [6:31:09<1:34:36, 145.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1599.4055616590713
INFO:root:current train perplexity3.5200181007385254
INFO:root:current mean train loss 1611.9407061408547
INFO:root:current train perplexity3.4972054958343506
INFO:root:current mean train loss 1600.5546430167506
INFO:root:current train perplexity3.5085082054138184
INFO:root:current mean train loss 1599.6526761736188
INFO:root:current train perplexity3.5147900581359863
INFO:root:current mean train loss 1601.1868552111705
INFO:root:current train perplexity3.5207436084747314
INFO:root:current mean train loss 1598.9653101679105
INFO:root:current train perplexity3.5195229053497314
INFO:root:current mean train loss 1600.6020256378365
INFO:root:current train perplexity3.5214245319366455
INFO:root:current mean train loss 1602.0986425980277
INFO:root:current train perplexity3.523984432220459
INFO:root:current mean train loss 1601.8051676042912
INFO:root:current train perplexity3.524031162261963
INFO:root:current mean train loss 1601.2862456232056
INFO:root:current train perplexity3.5268375873565674
INFO:root:current mean train loss 1602.3563973563057
INFO:root:current train perplexity3.5287468433380127
INFO:root:current mean train loss 1603.116508161518
INFO:root:current train perplexity3.5330541133880615
INFO:root:current mean train loss 1602.9146295936362
INFO:root:current train perplexity3.533695697784424
INFO:root:current mean train loss 1602.596061204008
INFO:root:current train perplexity3.535914182662964
INFO:root:current mean train loss 1603.2052530697792
INFO:root:current train perplexity3.535891056060791
INFO:root:current mean train loss 1603.39422082901
INFO:root:current train perplexity3.538696050643921
INFO:root:current mean train loss 1603.616386432228
INFO:root:current train perplexity3.5390167236328125
INFO:root:current mean train loss 1603.4732215283652
INFO:root:current train perplexity3.5401968955993652
INFO:root:current mean train loss 1603.3619082249327
INFO:root:current train perplexity3.541071891784668
INFO:root:current mean train loss 1603.988263027727
INFO:root:current train perplexity3.5429675579071045

100%|██████████| 1/1 [02:18<00:00, 138.04s/it][A100%|██████████| 1/1 [02:18<00:00, 138.04s/it]
INFO:root:final mean train loss: 1604.024908622707
INFO:root:final train perplexity: 3.543200731277466
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.11s/it][A100%|██████████| 1/1 [00:07<00:00,  7.11s/it]
INFO:root:eval mean loss: 2162.8375244140625
INFO:root:eval perplexity: 5.749861717224121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/162
 81%|████████  | 162/200 [6:33:35<1:32:09, 145.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1600.0558368035083
INFO:root:current train perplexity3.5410823822021484
INFO:root:current mean train loss 1594.9292638442096
INFO:root:current train perplexity3.5252373218536377
INFO:root:current mean train loss 1600.4565304239748
INFO:root:current train perplexity3.5265071392059326
INFO:root:current mean train loss 1599.5179771877213
INFO:root:current train perplexity3.5281198024749756
INFO:root:current mean train loss 1598.409479608599
INFO:root:current train perplexity3.524017572402954
INFO:root:current mean train loss 1597.6696274052047
INFO:root:current train perplexity3.5250444412231445
INFO:root:current mean train loss 1599.8035172700518
INFO:root:current train perplexity3.5273170471191406
INFO:root:current mean train loss 1600.5235803206444
INFO:root:current train perplexity3.529083013534546
INFO:root:current mean train loss 1602.6693918064918
INFO:root:current train perplexity3.5308187007904053
INFO:root:current mean train loss 1602.2443016348454
INFO:root:current train perplexity3.5316531658172607
INFO:root:current mean train loss 1601.3742096150136
INFO:root:current train perplexity3.533261299133301
INFO:root:current mean train loss 1601.675453364694
INFO:root:current train perplexity3.535344362258911
INFO:root:current mean train loss 1603.1991299007382
INFO:root:current train perplexity3.5373353958129883
INFO:root:current mean train loss 1602.8602319281804
INFO:root:current train perplexity3.5365607738494873
INFO:root:current mean train loss 1602.9261751850966
INFO:root:current train perplexity3.537429094314575
INFO:root:current mean train loss 1602.4277962354868
INFO:root:current train perplexity3.5376410484313965
INFO:root:current mean train loss 1603.579749650257
INFO:root:current train perplexity3.539309024810791
INFO:root:current mean train loss 1603.8671165418434
INFO:root:current train perplexity3.5400853157043457
INFO:root:current mean train loss 1604.1213862444347
INFO:root:current train perplexity3.540597438812256
INFO:root:current mean train loss 1603.9660252631168
INFO:root:current train perplexity3.5412182807922363

100%|██████████| 1/1 [02:18<00:00, 138.26s/it][A100%|██████████| 1/1 [02:18<00:00, 138.26s/it]
INFO:root:final mean train loss: 1603.4255813083082
INFO:root:final train perplexity: 3.5415265560150146
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2162.9849212862923
INFO:root:eval perplexity: 5.750548362731934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/163
 82%|████████▏ | 163/200 [6:36:01<1:29:45, 145.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1592.831342424665
INFO:root:current train perplexity3.5268595218658447
INFO:root:current mean train loss 1596.3224264705882
INFO:root:current train perplexity3.5176236629486084
INFO:root:current mean train loss 1598.6662466543692
INFO:root:current train perplexity3.523935317993164
INFO:root:current mean train loss 1600.5182963603252
INFO:root:current train perplexity3.5248780250549316
INFO:root:current mean train loss 1597.7105988198139
INFO:root:current train perplexity3.51784348487854
INFO:root:current mean train loss 1598.896157569216
INFO:root:current train perplexity3.528496503829956
INFO:root:current mean train loss 1601.578566639459
INFO:root:current train perplexity3.532397508621216
INFO:root:current mean train loss 1600.3748877587257
INFO:root:current train perplexity3.529177665710449
INFO:root:current mean train loss 1600.8741690800107
INFO:root:current train perplexity3.5317835807800293
INFO:root:current mean train loss 1601.3142687610743
INFO:root:current train perplexity3.5316872596740723
INFO:root:current mean train loss 1601.7388670734156
INFO:root:current train perplexity3.5339291095733643
INFO:root:current mean train loss 1601.9536470853366
INFO:root:current train perplexity3.5350358486175537
INFO:root:current mean train loss 1602.2740626537893
INFO:root:current train perplexity3.534313678741455
INFO:root:current mean train loss 1602.4399012210595
INFO:root:current train perplexity3.535984516143799
INFO:root:current mean train loss 1601.5652452533748
INFO:root:current train perplexity3.53446364402771
INFO:root:current mean train loss 1601.322259249353
INFO:root:current train perplexity3.5344350337982178
INFO:root:current mean train loss 1601.6309448242187
INFO:root:current train perplexity3.5351016521453857
INFO:root:current mean train loss 1601.7877819341454
INFO:root:current train perplexity3.5359930992126465
INFO:root:current mean train loss 1602.453620592413
INFO:root:current train perplexity3.5381417274475098
INFO:root:current mean train loss 1602.6341353208281
INFO:root:current train perplexity3.538062572479248

100%|██████████| 1/1 [02:18<00:00, 138.21s/it][A100%|██████████| 1/1 [02:18<00:00, 138.21s/it]
INFO:root:final mean train loss: 1602.164514831143
INFO:root:final train perplexity: 3.538005828857422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2164.3649166458886
INFO:root:eval perplexity: 5.756970405578613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/164
 82%|████████▏ | 164/200 [6:38:26<1:27:22, 145.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1597.0612217694863
INFO:root:current train perplexity3.5118846893310547
INFO:root:current mean train loss 1599.4263073926304
INFO:root:current train perplexity3.5110726356506348
INFO:root:current mean train loss 1600.2938815126852
INFO:root:current train perplexity3.509458303451538
INFO:root:current mean train loss 1597.7822107911418
INFO:root:current train perplexity3.517303466796875
INFO:root:current mean train loss 1598.6363111805376
INFO:root:current train perplexity3.519650936126709
INFO:root:current mean train loss 1601.0106140864566
INFO:root:current train perplexity3.524651288986206
INFO:root:current mean train loss 1599.6817669597776
INFO:root:current train perplexity3.5227479934692383
INFO:root:current mean train loss 1599.220438199849
INFO:root:current train perplexity3.5257067680358887
INFO:root:current mean train loss 1599.305948939103
INFO:root:current train perplexity3.5240907669067383
INFO:root:current mean train loss 1598.777119026841
INFO:root:current train perplexity3.524348735809326
INFO:root:current mean train loss 1599.6548705043986
INFO:root:current train perplexity3.530005693435669
INFO:root:current mean train loss 1600.0279305513507
INFO:root:current train perplexity3.5296921730041504
INFO:root:current mean train loss 1600.5267782618857
INFO:root:current train perplexity3.5320639610290527
INFO:root:current mean train loss 1601.3298729729463
INFO:root:current train perplexity3.533338785171509
INFO:root:current mean train loss 1601.0458704442408
INFO:root:current train perplexity3.533810615539551
INFO:root:current mean train loss 1601.4144624783396
INFO:root:current train perplexity3.5343379974365234
INFO:root:current mean train loss 1601.6046777170088
INFO:root:current train perplexity3.53476619720459
INFO:root:current mean train loss 1601.66490360339
INFO:root:current train perplexity3.5337843894958496
INFO:root:current mean train loss 1602.1655963034495
INFO:root:current train perplexity3.5342020988464355

100%|██████████| 1/1 [02:18<00:00, 138.12s/it][A100%|██████████| 1/1 [02:18<00:00, 138.12s/it]
INFO:root:final mean train loss: 1601.1538315550342
INFO:root:final train perplexity: 3.535187244415283
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2164.390978224734
INFO:root:eval perplexity: 5.757091522216797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/165
 82%|████████▎ | 165/200 [6:40:52<1:24:55, 145.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1581.583984375
INFO:root:current train perplexity3.5112531185150146
INFO:root:current mean train loss 1602.2284099872295
INFO:root:current train perplexity3.51936936378479
INFO:root:current mean train loss 1598.7905548694087
INFO:root:current train perplexity3.515808343887329
INFO:root:current mean train loss 1595.513392799779
INFO:root:current train perplexity3.518185615539551
INFO:root:current mean train loss 1594.8341139991685
INFO:root:current train perplexity3.517932653427124
INFO:root:current mean train loss 1594.3900471036397
INFO:root:current train perplexity3.51916766166687
INFO:root:current mean train loss 1593.9731144178782
INFO:root:current train perplexity3.5216541290283203
INFO:root:current mean train loss 1594.0591083873403
INFO:root:current train perplexity3.5237412452697754
INFO:root:current mean train loss 1597.0665383410098
INFO:root:current train perplexity3.52594256401062
INFO:root:current mean train loss 1597.084825498868
INFO:root:current train perplexity3.523507595062256
INFO:root:current mean train loss 1598.1675199835424
INFO:root:current train perplexity3.522712230682373
INFO:root:current mean train loss 1598.6335168368576
INFO:root:current train perplexity3.5257277488708496
INFO:root:current mean train loss 1599.432256147315
INFO:root:current train perplexity3.5263848304748535
INFO:root:current mean train loss 1599.2016991925386
INFO:root:current train perplexity3.5246777534484863
INFO:root:current mean train loss 1599.6938530468194
INFO:root:current train perplexity3.5256707668304443
INFO:root:current mean train loss 1600.1035951654962
INFO:root:current train perplexity3.526740789413452
INFO:root:current mean train loss 1600.6800481553685
INFO:root:current train perplexity3.528951406478882
INFO:root:current mean train loss 1599.786337552496
INFO:root:current train perplexity3.528712034225464
INFO:root:current mean train loss 1600.193408703857
INFO:root:current train perplexity3.529802083969116
INFO:root:current mean train loss 1600.3445925672515
INFO:root:current train perplexity3.531071424484253

100%|██████████| 1/1 [02:17<00:00, 137.98s/it][A100%|██████████| 1/1 [02:17<00:00, 137.98s/it]
INFO:root:final mean train loss: 1599.8893813048116
INFO:root:final train perplexity: 3.531663179397583
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2165.05342350953
INFO:root:eval perplexity: 5.760176181793213
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/166
 83%|████████▎ | 166/200 [6:43:17<1:22:27, 145.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.1614815848213
INFO:root:current train perplexity3.579718828201294
INFO:root:current mean train loss 1599.2220912964876
INFO:root:current train perplexity3.519709587097168
INFO:root:current mean train loss 1599.4110797864819
INFO:root:current train perplexity3.525320291519165
INFO:root:current mean train loss 1598.4200154242114
INFO:root:current train perplexity3.5212340354919434
INFO:root:current mean train loss 1597.1850313381458
INFO:root:current train perplexity3.518038511276245
INFO:root:current mean train loss 1596.3865411505817
INFO:root:current train perplexity3.5137901306152344
INFO:root:current mean train loss 1596.688607675058
INFO:root:current train perplexity3.5130672454833984
INFO:root:current mean train loss 1597.1829133053593
INFO:root:current train perplexity3.517510414123535
INFO:root:current mean train loss 1598.119977721053
INFO:root:current train perplexity3.5196101665496826
INFO:root:current mean train loss 1597.6532820081347
INFO:root:current train perplexity3.521044969558716
INFO:root:current mean train loss 1597.4307325701288
INFO:root:current train perplexity3.520615577697754
INFO:root:current mean train loss 1598.8908420259881
INFO:root:current train perplexity3.52325701713562
INFO:root:current mean train loss 1598.440557956305
INFO:root:current train perplexity3.524736166000366
INFO:root:current mean train loss 1597.7218092428202
INFO:root:current train perplexity3.524352788925171
INFO:root:current mean train loss 1598.6319664264547
INFO:root:current train perplexity3.5258126258850098
INFO:root:current mean train loss 1599.4986125236276
INFO:root:current train perplexity3.5269577503204346
INFO:root:current mean train loss 1599.3414800645098
INFO:root:current train perplexity3.5271353721618652
INFO:root:current mean train loss 1599.9840409202952
INFO:root:current train perplexity3.5302557945251465
INFO:root:current mean train loss 1599.6853512005123
INFO:root:current train perplexity3.528844118118286
INFO:root:current mean train loss 1599.9387923185557
INFO:root:current train perplexity3.5294225215911865

100%|██████████| 1/1 [02:17<00:00, 137.88s/it][A100%|██████████| 1/1 [02:17<00:00, 137.88s/it]
INFO:root:final mean train loss: 1599.2611702646323
INFO:root:final train perplexity: 3.529914379119873
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.14s/it][A100%|██████████| 1/1 [00:07<00:00,  7.14s/it]
INFO:root:eval mean loss: 2165.789962011026
INFO:root:eval perplexity: 5.763607978820801
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/167
 84%|████████▎ | 167/200 [6:45:42<1:19:59, 145.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1590.1093139648438
INFO:root:current train perplexity3.533660650253296
INFO:root:current mean train loss 1593.5891184046648
INFO:root:current train perplexity3.509505033493042
INFO:root:current mean train loss 1595.9139445328913
INFO:root:current train perplexity3.521160840988159
INFO:root:current mean train loss 1595.0144693047337
INFO:root:current train perplexity3.5223100185394287
INFO:root:current mean train loss 1595.6950304562642
INFO:root:current train perplexity3.5191333293914795
INFO:root:current mean train loss 1593.6976139111146
INFO:root:current train perplexity3.516446828842163
INFO:root:current mean train loss 1593.0583511400373
INFO:root:current train perplexity3.5199193954467773
INFO:root:current mean train loss 1593.995336517086
INFO:root:current train perplexity3.5229389667510986
INFO:root:current mean train loss 1595.7453303007067
INFO:root:current train perplexity3.5242385864257812
INFO:root:current mean train loss 1595.6103771998685
INFO:root:current train perplexity3.5231473445892334
INFO:root:current mean train loss 1595.741174128244
INFO:root:current train perplexity3.523479461669922
INFO:root:current mean train loss 1596.8332429426625
INFO:root:current train perplexity3.521350622177124
INFO:root:current mean train loss 1597.471221085704
INFO:root:current train perplexity3.520739793777466
INFO:root:current mean train loss 1598.4111652003633
INFO:root:current train perplexity3.522029399871826
INFO:root:current mean train loss 1598.2854188115275
INFO:root:current train perplexity3.5239126682281494
INFO:root:current mean train loss 1597.983685199554
INFO:root:current train perplexity3.5232388973236084
INFO:root:current mean train loss 1597.4381476135627
INFO:root:current train perplexity3.5212364196777344
INFO:root:current mean train loss 1597.9105100291508
INFO:root:current train perplexity3.52360463142395
INFO:root:current mean train loss 1597.299598469698
INFO:root:current train perplexity3.524158477783203
INFO:root:current mean train loss 1598.1261356696257
INFO:root:current train perplexity3.5252845287323

100%|██████████| 1/1 [02:18<00:00, 138.20s/it][A100%|██████████| 1/1 [02:18<00:00, 138.20s/it]
INFO:root:final mean train loss: 1597.702120613102
INFO:root:final train perplexity: 3.5255770683288574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2165.8522866972794
INFO:root:eval perplexity: 5.763898849487305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/168
 84%|████████▍ | 168/200 [6:48:08<1:17:36, 145.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.43671875
INFO:root:current train perplexity3.496081590652466
INFO:root:current mean train loss 1581.8734776650706
INFO:root:current train perplexity3.517672300338745
INFO:root:current mean train loss 1586.9985648360907
INFO:root:current train perplexity3.5107202529907227
INFO:root:current mean train loss 1588.7013032295336
INFO:root:current train perplexity3.510894298553467
INFO:root:current mean train loss 1589.4296231112637
INFO:root:current train perplexity3.515310525894165
INFO:root:current mean train loss 1590.7879775038712
INFO:root:current train perplexity3.522402763366699
INFO:root:current mean train loss 1591.790970337846
INFO:root:current train perplexity3.5257954597473145
INFO:root:current mean train loss 1592.2600451740998
INFO:root:current train perplexity3.522855520248413
INFO:root:current mean train loss 1593.4962742141813
INFO:root:current train perplexity3.52311110496521
INFO:root:current mean train loss 1593.30512171241
INFO:root:current train perplexity3.5236945152282715
INFO:root:current mean train loss 1593.884818502851
INFO:root:current train perplexity3.5241949558258057
INFO:root:current mean train loss 1592.706661339962
INFO:root:current train perplexity3.523469924926758
INFO:root:current mean train loss 1593.0048364160546
INFO:root:current train perplexity3.522547721862793
INFO:root:current mean train loss 1593.4437706303333
INFO:root:current train perplexity3.5216197967529297
INFO:root:current mean train loss 1594.5004675586608
INFO:root:current train perplexity3.523848295211792
INFO:root:current mean train loss 1594.2346635726487
INFO:root:current train perplexity3.521996259689331
INFO:root:current mean train loss 1594.7551503345685
INFO:root:current train perplexity3.522588014602661
INFO:root:current mean train loss 1596.0621184868012
INFO:root:current train perplexity3.5226635932922363
INFO:root:current mean train loss 1597.503675533819
INFO:root:current train perplexity3.5249788761138916
INFO:root:current mean train loss 1597.9602711397058
INFO:root:current train perplexity3.525068521499634

100%|██████████| 1/1 [02:18<00:00, 138.10s/it][A100%|██████████| 1/1 [02:18<00:00, 138.10s/it]
INFO:root:final mean train loss: 1597.569955368927
INFO:root:final train perplexity: 3.525209665298462
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.24s/it][A100%|██████████| 1/1 [00:07<00:00,  7.24s/it]
INFO:root:eval mean loss: 2166.355091284353
INFO:root:eval perplexity: 5.766244411468506
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/169
 84%|████████▍ | 169/200 [6:50:34<1:15:12, 145.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1581.2340613471138
INFO:root:current train perplexity3.5251874923706055
INFO:root:current mean train loss 1587.4365944086119
INFO:root:current train perplexity3.524390697479248
INFO:root:current mean train loss 1592.2016588098863
INFO:root:current train perplexity3.534945249557495
INFO:root:current mean train loss 1591.9122258668306
INFO:root:current train perplexity3.532102346420288
INFO:root:current mean train loss 1593.4818601446636
INFO:root:current train perplexity3.527561664581299
INFO:root:current mean train loss 1594.658980789718
INFO:root:current train perplexity3.528287172317505
INFO:root:current mean train loss 1596.0833843776159
INFO:root:current train perplexity3.524923086166382
INFO:root:current mean train loss 1596.962753849326
INFO:root:current train perplexity3.525437355041504
INFO:root:current mean train loss 1597.268751371891
INFO:root:current train perplexity3.5242583751678467
INFO:root:current mean train loss 1597.7881052778582
INFO:root:current train perplexity3.5228705406188965
INFO:root:current mean train loss 1598.454708953402
INFO:root:current train perplexity3.5247230529785156
INFO:root:current mean train loss 1598.0104571137413
INFO:root:current train perplexity3.5233380794525146
INFO:root:current mean train loss 1598.6262021814503
INFO:root:current train perplexity3.5229995250701904
INFO:root:current mean train loss 1597.8765850456393
INFO:root:current train perplexity3.522583246231079
INFO:root:current mean train loss 1596.7177229342253
INFO:root:current train perplexity3.5216662883758545
INFO:root:current mean train loss 1595.971262148015
INFO:root:current train perplexity3.52004075050354
INFO:root:current mean train loss 1596.255569750042
INFO:root:current train perplexity3.5197830200195312
INFO:root:current mean train loss 1596.2851001748113
INFO:root:current train perplexity3.5210068225860596
INFO:root:current mean train loss 1596.426281790448
INFO:root:current train perplexity3.520470142364502
INFO:root:current mean train loss 1596.625872691311
INFO:root:current train perplexity3.521371603012085

100%|██████████| 1/1 [02:18<00:00, 138.37s/it][A100%|██████████| 1/1 [02:18<00:00, 138.37s/it]
INFO:root:final mean train loss: 1596.2273200377033
INFO:root:final train perplexity: 3.5214781761169434
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2166.735372773299
INFO:root:eval perplexity: 5.768017292022705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/170
 85%|████████▌ | 170/200 [6:53:00<1:12:49, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1600.8632318732443
INFO:root:current train perplexity3.503709316253662
INFO:root:current mean train loss 1601.6031771866733
INFO:root:current train perplexity3.5078859329223633
INFO:root:current mean train loss 1596.6973171564123
INFO:root:current train perplexity3.505310535430908
INFO:root:current mean train loss 1595.427284064207
INFO:root:current train perplexity3.5102343559265137
INFO:root:current mean train loss 1596.140850667817
INFO:root:current train perplexity3.5072412490844727
INFO:root:current mean train loss 1595.2019152811306
INFO:root:current train perplexity3.508335590362549
INFO:root:current mean train loss 1594.6133723155162
INFO:root:current train perplexity3.510305166244507
INFO:root:current mean train loss 1594.4903666565174
INFO:root:current train perplexity3.509523868560791
INFO:root:current mean train loss 1595.5499749543026
INFO:root:current train perplexity3.5138039588928223
INFO:root:current mean train loss 1596.6102633114651
INFO:root:current train perplexity3.5145530700683594
INFO:root:current mean train loss 1596.450425553475
INFO:root:current train perplexity3.5164380073547363
INFO:root:current mean train loss 1596.67887294503
INFO:root:current train perplexity3.5171570777893066
INFO:root:current mean train loss 1596.0948784638042
INFO:root:current train perplexity3.5174720287323
INFO:root:current mean train loss 1595.728467728441
INFO:root:current train perplexity3.5178182125091553
INFO:root:current mean train loss 1595.75647890848
INFO:root:current train perplexity3.5188207626342773
INFO:root:current mean train loss 1594.8886770220804
INFO:root:current train perplexity3.5188798904418945
INFO:root:current mean train loss 1595.1845003515393
INFO:root:current train perplexity3.520256519317627
INFO:root:current mean train loss 1595.3710463274917
INFO:root:current train perplexity3.5200562477111816
INFO:root:current mean train loss 1595.764952547024
INFO:root:current train perplexity3.5201363563537598

100%|██████████| 1/1 [02:17<00:00, 137.98s/it][A100%|██████████| 1/1 [02:17<00:00, 137.98s/it]
INFO:root:final mean train loss: 1595.6222183296793
INFO:root:final train perplexity: 3.519798517227173
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.23s/it][A100%|██████████| 1/1 [00:07<00:00,  7.23s/it]
INFO:root:eval mean loss: 2167.017784605635
INFO:root:eval perplexity: 5.769334316253662
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/171
 86%|████████▌ | 171/200 [6:55:25<1:10:22, 145.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1610.3539428710938
INFO:root:current train perplexity3.525625705718994
INFO:root:current mean train loss 1590.1081600549087
INFO:root:current train perplexity3.4934253692626953
INFO:root:current mean train loss 1596.7345634386377
INFO:root:current train perplexity3.5035197734832764
INFO:root:current mean train loss 1594.446235606873
INFO:root:current train perplexity3.507310152053833
INFO:root:current mean train loss 1594.507056926859
INFO:root:current train perplexity3.515537738800049
INFO:root:current mean train loss 1593.9506331734035
INFO:root:current train perplexity3.5154950618743896
INFO:root:current mean train loss 1593.6716824270318
INFO:root:current train perplexity3.5172924995422363
INFO:root:current mean train loss 1594.7574058294972
INFO:root:current train perplexity3.521249532699585
INFO:root:current mean train loss 1595.10404252651
INFO:root:current train perplexity3.5206458568573
INFO:root:current mean train loss 1596.2402393602115
INFO:root:current train perplexity3.5204293727874756
INFO:root:current mean train loss 1596.1574160991086
INFO:root:current train perplexity3.5184032917022705
INFO:root:current mean train loss 1595.455322707109
INFO:root:current train perplexity3.516544818878174
INFO:root:current mean train loss 1594.8566735617162
INFO:root:current train perplexity3.517458915710449
INFO:root:current mean train loss 1594.9796364099288
INFO:root:current train perplexity3.5151290893554688
INFO:root:current mean train loss 1595.0836455126744
INFO:root:current train perplexity3.5152745246887207
INFO:root:current mean train loss 1594.3969583903968
INFO:root:current train perplexity3.514310836791992
INFO:root:current mean train loss 1595.2500730445643
INFO:root:current train perplexity3.5154128074645996
INFO:root:current mean train loss 1595.38026125909
INFO:root:current train perplexity3.516248941421509
INFO:root:current mean train loss 1595.572075152054
INFO:root:current train perplexity3.517289400100708
INFO:root:current mean train loss 1595.462746395019
INFO:root:current train perplexity3.5189268589019775

100%|██████████| 1/1 [02:18<00:00, 138.24s/it][A100%|██████████| 1/1 [02:18<00:00, 138.24s/it]
INFO:root:final mean train loss: 1594.952876611845
INFO:root:final train perplexity: 3.5179407596588135
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.11s/it][A100%|██████████| 1/1 [00:07<00:00,  7.11s/it]
INFO:root:eval mean loss: 2168.1464185782356
INFO:root:eval perplexity: 5.774602890014648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/172
 86%|████████▌ | 172/200 [6:57:51<1:07:56, 145.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1559.9264924422555
INFO:root:current train perplexity3.4648449420928955
INFO:root:current mean train loss 1587.7602975736788
INFO:root:current train perplexity3.5164992809295654
INFO:root:current mean train loss 1586.2737971421313
INFO:root:current train perplexity3.5019915103912354
INFO:root:current mean train loss 1588.8687419123694
INFO:root:current train perplexity3.5006227493286133
INFO:root:current mean train loss 1590.0904465984227
INFO:root:current train perplexity3.5012733936309814
INFO:root:current mean train loss 1591.3888940756453
INFO:root:current train perplexity3.5000174045562744
INFO:root:current mean train loss 1591.7024915275758
INFO:root:current train perplexity3.5031497478485107
INFO:root:current mean train loss 1593.0010714497969
INFO:root:current train perplexity3.5085389614105225
INFO:root:current mean train loss 1593.8563001037078
INFO:root:current train perplexity3.5111324787139893
INFO:root:current mean train loss 1594.571410603797
INFO:root:current train perplexity3.5121796131134033
INFO:root:current mean train loss 1594.6908387364065
INFO:root:current train perplexity3.5104265213012695
INFO:root:current mean train loss 1594.0155769545163
INFO:root:current train perplexity3.5098838806152344
INFO:root:current mean train loss 1593.581217281563
INFO:root:current train perplexity3.511324167251587
INFO:root:current mean train loss 1592.8885031171758
INFO:root:current train perplexity3.5094473361968994
INFO:root:current mean train loss 1592.693963378563
INFO:root:current train perplexity3.5087897777557373
INFO:root:current mean train loss 1592.871043415032
INFO:root:current train perplexity3.5114388465881348
INFO:root:current mean train loss 1592.9246203410207
INFO:root:current train perplexity3.510758876800537
INFO:root:current mean train loss 1593.4875178677498
INFO:root:current train perplexity3.511728286743164
INFO:root:current mean train loss 1593.5131255383683
INFO:root:current train perplexity3.5133378505706787
INFO:root:current mean train loss 1593.9347335807495
INFO:root:current train perplexity3.5137360095977783

100%|██████████| 1/1 [02:17<00:00, 137.98s/it][A100%|██████████| 1/1 [02:17<00:00, 137.98s/it]
INFO:root:final mean train loss: 1594.1191491508387
INFO:root:final train perplexity: 3.5156285762786865
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2167.744783009198
INFO:root:eval perplexity: 5.772728443145752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/173
 86%|████████▋ | 173/200 [7:00:16<1:05:29, 145.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1590.5830169677733
INFO:root:current train perplexity3.5161895751953125
INFO:root:current mean train loss 1589.2347141810826
INFO:root:current train perplexity3.514488458633423
INFO:root:current mean train loss 1590.7479934692383
INFO:root:current train perplexity3.510340690612793
INFO:root:current mean train loss 1593.372728056066
INFO:root:current train perplexity3.5104787349700928
INFO:root:current mean train loss 1592.9247156316583
INFO:root:current train perplexity3.514068603515625
INFO:root:current mean train loss 1589.5332698115596
INFO:root:current train perplexity3.514378309249878
INFO:root:current mean train loss 1590.1272901535035
INFO:root:current train perplexity3.5149991512298584
INFO:root:current mean train loss 1590.391176130965
INFO:root:current train perplexity3.5133352279663086
INFO:root:current mean train loss 1590.7178966703868
INFO:root:current train perplexity3.5136008262634277
INFO:root:current mean train loss 1591.7986121644365
INFO:root:current train perplexity3.5154380798339844
INFO:root:current mean train loss 1591.5005128126877
INFO:root:current train perplexity3.5137529373168945
INFO:root:current mean train loss 1592.1549613015693
INFO:root:current train perplexity3.5136284828186035
INFO:root:current mean train loss 1592.9601225822203
INFO:root:current train perplexity3.513205051422119
INFO:root:current mean train loss 1593.1611503031716
INFO:root:current train perplexity3.513638973236084
INFO:root:current mean train loss 1593.611280822754
INFO:root:current train perplexity3.51300311088562
INFO:root:current mean train loss 1593.0866225998122
INFO:root:current train perplexity3.5124566555023193
INFO:root:current mean train loss 1593.6519488823124
INFO:root:current train perplexity3.513195037841797
INFO:root:current mean train loss 1593.7603861490886
INFO:root:current train perplexity3.512681484222412
INFO:root:current mean train loss 1593.7146427983823
INFO:root:current train perplexity3.5135111808776855
INFO:root:current mean train loss 1593.5571806917485
INFO:root:current train perplexity3.513231039047241

100%|██████████| 1/1 [02:18<00:00, 138.50s/it][A100%|██████████| 1/1 [02:18<00:00, 138.50s/it]
INFO:root:final mean train loss: 1593.2525524883877
INFO:root:final train perplexity: 3.5132265090942383
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.30s/it][A100%|██████████| 1/1 [00:07<00:00,  7.30s/it]
INFO:root:eval mean loss: 2168.279632784796
INFO:root:eval perplexity: 5.775225639343262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/174
 87%|████████▋ | 174/200 [7:02:42<1:03:08, 145.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.9311309279058
INFO:root:current train perplexity3.495154619216919
INFO:root:current mean train loss 1593.7987705886744
INFO:root:current train perplexity3.5151355266571045
INFO:root:current mean train loss 1588.8050266369771
INFO:root:current train perplexity3.517764091491699
INFO:root:current mean train loss 1589.0514121175813
INFO:root:current train perplexity3.5154953002929688
INFO:root:current mean train loss 1593.1303323624693
INFO:root:current train perplexity3.5160884857177734
INFO:root:current mean train loss 1592.9492417614592
INFO:root:current train perplexity3.5102579593658447
INFO:root:current mean train loss 1591.5805160545685
INFO:root:current train perplexity3.508620023727417
INFO:root:current mean train loss 1591.3850702363875
INFO:root:current train perplexity3.5087153911590576
INFO:root:current mean train loss 1592.0774397254686
INFO:root:current train perplexity3.509186029434204
INFO:root:current mean train loss 1592.2001400811048
INFO:root:current train perplexity3.5077359676361084
INFO:root:current mean train loss 1592.3911455022692
INFO:root:current train perplexity3.508824110031128
INFO:root:current mean train loss 1592.9057301724895
INFO:root:current train perplexity3.5102427005767822
INFO:root:current mean train loss 1592.5659939106629
INFO:root:current train perplexity3.510122776031494
INFO:root:current mean train loss 1592.6338638159486
INFO:root:current train perplexity3.510455846786499
INFO:root:current mean train loss 1592.436868619166
INFO:root:current train perplexity3.5105490684509277
INFO:root:current mean train loss 1591.8022007779884
INFO:root:current train perplexity3.508965492248535
INFO:root:current mean train loss 1592.162386887895
INFO:root:current train perplexity3.509495496749878
INFO:root:current mean train loss 1592.4091991409362
INFO:root:current train perplexity3.510310649871826
INFO:root:current mean train loss 1592.5448772407824
INFO:root:current train perplexity3.5096046924591064
INFO:root:current mean train loss 1592.2851231282136
INFO:root:current train perplexity3.5104358196258545

100%|██████████| 1/1 [02:18<00:00, 138.11s/it][A100%|██████████| 1/1 [02:18<00:00, 138.11s/it]
INFO:root:final mean train loss: 1592.3003897693382
INFO:root:final train perplexity: 3.5105888843536377
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2168.742175812417
INFO:root:eval perplexity: 5.777385234832764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/175
 88%|████████▊ | 175/200 [7:05:08<1:00:41, 145.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.9441792256123
INFO:root:current train perplexity3.508124351501465
INFO:root:current mean train loss 1588.86289258935
INFO:root:current train perplexity3.5035994052886963
INFO:root:current mean train loss 1586.6354387937672
INFO:root:current train perplexity3.495401620864868
INFO:root:current mean train loss 1589.2637482505431
INFO:root:current train perplexity3.5033493041992188
INFO:root:current mean train loss 1588.8822961477288
INFO:root:current train perplexity3.499579668045044
INFO:root:current mean train loss 1589.00109608082
INFO:root:current train perplexity3.5043976306915283
INFO:root:current mean train loss 1588.674202848259
INFO:root:current train perplexity3.504181385040283
INFO:root:current mean train loss 1589.8070497340318
INFO:root:current train perplexity3.5056517124176025
INFO:root:current mean train loss 1589.7939008979013
INFO:root:current train perplexity3.5064010620117188
INFO:root:current mean train loss 1590.5363918672597
INFO:root:current train perplexity3.507058620452881
INFO:root:current mean train loss 1592.0464951499214
INFO:root:current train perplexity3.5072813034057617
INFO:root:current mean train loss 1591.5685729460579
INFO:root:current train perplexity3.5051112174987793
INFO:root:current mean train loss 1590.488033564162
INFO:root:current train perplexity3.504503011703491
INFO:root:current mean train loss 1591.5566387592964
INFO:root:current train perplexity3.505276679992676
INFO:root:current mean train loss 1591.5731308004101
INFO:root:current train perplexity3.506350517272949
INFO:root:current mean train loss 1591.3917617894795
INFO:root:current train perplexity3.5062668323516846
INFO:root:current mean train loss 1591.3103850625607
INFO:root:current train perplexity3.506868839263916
INFO:root:current mean train loss 1591.308665175583
INFO:root:current train perplexity3.506018877029419
INFO:root:current mean train loss 1591.3479705452282
INFO:root:current train perplexity3.5059127807617188
INFO:root:current mean train loss 1591.5692149184515
INFO:root:current train perplexity3.507112979888916

100%|██████████| 1/1 [02:18<00:00, 138.27s/it][A100%|██████████| 1/1 [02:18<00:00, 138.27s/it]
INFO:root:final mean train loss: 1591.1103599344428
INFO:root:final train perplexity: 3.507295608520508
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2169.6870450500055
INFO:root:eval perplexity: 5.781803131103516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/176
 88%|████████▊ | 176/200 [7:07:33<58:16, 145.68s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.6271194625688
INFO:root:current train perplexity3.524388551712036
INFO:root:current mean train loss 1581.1118617831723
INFO:root:current train perplexity3.5314674377441406
INFO:root:current mean train loss 1587.5348764530981
INFO:root:current train perplexity3.5189545154571533
INFO:root:current mean train loss 1587.2059447929987
INFO:root:current train perplexity3.5156235694885254
INFO:root:current mean train loss 1587.8482173756524
INFO:root:current train perplexity3.513134717941284
INFO:root:current mean train loss 1587.9982317361332
INFO:root:current train perplexity3.5107932090759277
INFO:root:current mean train loss 1589.807510663045
INFO:root:current train perplexity3.50614595413208
INFO:root:current mean train loss 1589.1682025509147
INFO:root:current train perplexity3.506091356277466
INFO:root:current mean train loss 1589.4567534064604
INFO:root:current train perplexity3.5072333812713623
INFO:root:current mean train loss 1588.038566335299
INFO:root:current train perplexity3.505004644393921
INFO:root:current mean train loss 1589.6373779968205
INFO:root:current train perplexity3.5031630992889404
INFO:root:current mean train loss 1589.4245703862957
INFO:root:current train perplexity3.5045042037963867
INFO:root:current mean train loss 1589.721991056624
INFO:root:current train perplexity3.5055181980133057
INFO:root:current mean train loss 1589.4412924288674
INFO:root:current train perplexity3.5058417320251465
INFO:root:current mean train loss 1590.1183127410295
INFO:root:current train perplexity3.504674196243286
INFO:root:current mean train loss 1589.6669592722492
INFO:root:current train perplexity3.504112482070923
INFO:root:current mean train loss 1590.3432748570096
INFO:root:current train perplexity3.5055129528045654
INFO:root:current mean train loss 1590.8624176246904
INFO:root:current train perplexity3.5060155391693115
INFO:root:current mean train loss 1591.2485059135997
INFO:root:current train perplexity3.5063133239746094

100%|██████████| 1/1 [02:18<00:00, 138.29s/it][A100%|██████████| 1/1 [02:18<00:00, 138.29s/it]
INFO:root:final mean train loss: 1590.7106159496836
INFO:root:final train perplexity: 3.506190299987793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2169.2753979838485
INFO:root:eval perplexity: 5.779877662658691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/177
 88%|████████▊ | 177/200 [7:09:59<55:51, 145.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.2602996826172
INFO:root:current train perplexity3.5213053226470947
INFO:root:current mean train loss 1577.1202347366898
INFO:root:current train perplexity3.4949679374694824
INFO:root:current mean train loss 1586.173600416917
INFO:root:current train perplexity3.4989047050476074
INFO:root:current mean train loss 1591.656087900137
INFO:root:current train perplexity3.5011675357818604
INFO:root:current mean train loss 1589.6424632352941
INFO:root:current train perplexity3.502610683441162
INFO:root:current mean train loss 1589.3882953313391
INFO:root:current train perplexity3.5018093585968018
INFO:root:current mean train loss 1588.4653641550165
INFO:root:current train perplexity3.503568172454834
INFO:root:current mean train loss 1588.7644256764213
INFO:root:current train perplexity3.5046889781951904
INFO:root:current mean train loss 1589.4025644736714
INFO:root:current train perplexity3.505506992340088
INFO:root:current mean train loss 1590.3993629758054
INFO:root:current train perplexity3.5049655437469482
INFO:root:current mean train loss 1591.2239228505937
INFO:root:current train perplexity3.5071523189544678
INFO:root:current mean train loss 1590.7220431441435
INFO:root:current train perplexity3.506270170211792
INFO:root:current mean train loss 1590.5640939876732
INFO:root:current train perplexity3.505876064300537
INFO:root:current mean train loss 1589.6709564862265
INFO:root:current train perplexity3.5033037662506104
INFO:root:current mean train loss 1589.3171277479692
INFO:root:current train perplexity3.5046780109405518
INFO:root:current mean train loss 1590.346158217372
INFO:root:current train perplexity3.505030632019043
INFO:root:current mean train loss 1590.9382589159914
INFO:root:current train perplexity3.5054807662963867
INFO:root:current mean train loss 1591.208113373303
INFO:root:current train perplexity3.5055747032165527
INFO:root:current mean train loss 1590.7121615114465
INFO:root:current train perplexity3.5038952827453613
INFO:root:current mean train loss 1590.1448426956401
INFO:root:current train perplexity3.5033247470855713

100%|██████████| 1/1 [02:18<00:00, 138.20s/it][A100%|██████████| 1/1 [02:18<00:00, 138.20s/it]
INFO:root:final mean train loss: 1589.7098735861266
INFO:root:final train perplexity: 3.5034236907958984
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2170.1872697113254
INFO:root:eval perplexity: 5.784142017364502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/178
 89%|████████▉ | 178/200 [7:12:25<53:26, 145.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.15603515625
INFO:root:current train perplexity3.4554333686828613
INFO:root:current mean train loss 1582.580384765625
INFO:root:current train perplexity3.4701786041259766
INFO:root:current mean train loss 1582.6798562282986
INFO:root:current train perplexity3.4844658374786377
INFO:root:current mean train loss 1583.7156573016828
INFO:root:current train perplexity3.486788511276245
INFO:root:current mean train loss 1584.8472257008273
INFO:root:current train perplexity3.490246534347534
INFO:root:current mean train loss 1585.4095205543156
INFO:root:current train perplexity3.4951412677764893
INFO:root:current mean train loss 1588.9270515625
INFO:root:current train perplexity3.5000293254852295
INFO:root:current mean train loss 1588.402950228987
INFO:root:current train perplexity3.495694160461426
INFO:root:current mean train loss 1589.7248672762785
INFO:root:current train perplexity3.494943380355835
INFO:root:current mean train loss 1590.173857421875
INFO:root:current train perplexity3.4961941242218018
INFO:root:current mean train loss 1590.2458494902821
INFO:root:current train perplexity3.4960715770721436
INFO:root:current mean train loss 1590.3787504340278
INFO:root:current train perplexity3.4970901012420654
INFO:root:current mean train loss 1590.5816463050064
INFO:root:current train perplexity3.4986562728881836
INFO:root:current mean train loss 1591.1879093270932
INFO:root:current train perplexity3.498727560043335
INFO:root:current mean train loss 1590.100264100192
INFO:root:current train perplexity3.4983487129211426
INFO:root:current mean train loss 1590.16504522605
INFO:root:current train perplexity3.498077392578125
INFO:root:current mean train loss 1590.273686373197
INFO:root:current train perplexity3.49922513961792
INFO:root:current mean train loss 1590.5873903136323
INFO:root:current train perplexity3.501525640487671
INFO:root:current mean train loss 1589.9980465405608
INFO:root:current train perplexity3.501044273376465
INFO:root:current mean train loss 1589.5111754895495
INFO:root:current train perplexity3.5007338523864746

100%|██████████| 1/1 [02:17<00:00, 137.99s/it][A100%|██████████| 1/1 [02:17<00:00, 137.99s/it]
INFO:root:final mean train loss: 1588.8459764750871
INFO:root:final train perplexity: 3.501038074493408
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2169.967191655585
INFO:root:eval perplexity: 5.783112525939941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/179
 90%|████████▉ | 179/200 [7:14:50<50:58, 145.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.612534295945
INFO:root:current train perplexity3.496880292892456
INFO:root:current mean train loss 1578.6048506615866
INFO:root:current train perplexity3.4806346893310547
INFO:root:current mean train loss 1582.9261651157349
INFO:root:current train perplexity3.4863271713256836
INFO:root:current mean train loss 1582.3580907632036
INFO:root:current train perplexity3.4871737957000732
INFO:root:current mean train loss 1582.373208714826
INFO:root:current train perplexity3.4949212074279785
INFO:root:current mean train loss 1582.7783694108914
INFO:root:current train perplexity3.4959983825683594
INFO:root:current mean train loss 1584.4836906837154
INFO:root:current train perplexity3.4943692684173584
INFO:root:current mean train loss 1583.6964688776638
INFO:root:current train perplexity3.493889808654785
INFO:root:current mean train loss 1584.617043973148
INFO:root:current train perplexity3.493682384490967
INFO:root:current mean train loss 1583.9581871599655
INFO:root:current train perplexity3.494915246963501
INFO:root:current mean train loss 1585.6781883312965
INFO:root:current train perplexity3.495837450027466
INFO:root:current mean train loss 1585.6716932841232
INFO:root:current train perplexity3.495619535446167
INFO:root:current mean train loss 1585.6034903656841
INFO:root:current train perplexity3.4967288970947266
INFO:root:current mean train loss 1586.376280191936
INFO:root:current train perplexity3.4948582649230957
INFO:root:current mean train loss 1587.7951058270035
INFO:root:current train perplexity3.497274160385132
INFO:root:current mean train loss 1587.8332955722833
INFO:root:current train perplexity3.4992175102233887
INFO:root:current mean train loss 1588.210403721167
INFO:root:current train perplexity3.4997849464416504
INFO:root:current mean train loss 1588.7909967048024
INFO:root:current train perplexity3.5006589889526367
INFO:root:current mean train loss 1588.9796987527357
INFO:root:current train perplexity3.5009708404541016
INFO:root:current mean train loss 1589.6139093275297
INFO:root:current train perplexity3.5016119480133057

100%|██████████| 1/1 [02:18<00:00, 138.32s/it][A100%|██████████| 1/1 [02:18<00:00, 138.32s/it]
INFO:root:final mean train loss: 1589.065026695417
INFO:root:final train perplexity: 3.501642942428589
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.20s/it]
INFO:root:eval mean loss: 2170.4514779165283
INFO:root:eval perplexity: 5.7853779792785645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/180
 90%|█████████ | 180/200 [7:17:16<48:33, 145.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1586.1660901085804
INFO:root:current train perplexity3.5184335708618164
INFO:root:current mean train loss 1583.6383294639347
INFO:root:current train perplexity3.507054328918457
INFO:root:current mean train loss 1588.272442084942
INFO:root:current train perplexity3.509329080581665
INFO:root:current mean train loss 1589.649487168676
INFO:root:current train perplexity3.510740280151367
INFO:root:current mean train loss 1587.360482143161
INFO:root:current train perplexity3.5127272605895996
INFO:root:current mean train loss 1587.28785620947
INFO:root:current train perplexity3.508038282394409
INFO:root:current mean train loss 1589.3079380156962
INFO:root:current train perplexity3.506404399871826
INFO:root:current mean train loss 1586.8563227596962
INFO:root:current train perplexity3.5040283203125
INFO:root:current mean train loss 1585.5401759119889
INFO:root:current train perplexity3.500190019607544
INFO:root:current mean train loss 1586.584565577343
INFO:root:current train perplexity3.4986941814422607
INFO:root:current mean train loss 1586.756910286274
INFO:root:current train perplexity3.500178813934326
INFO:root:current mean train loss 1587.2103660129287
INFO:root:current train perplexity3.49847674369812
INFO:root:current mean train loss 1588.397679481173
INFO:root:current train perplexity3.4977433681488037
INFO:root:current mean train loss 1587.727329144678
INFO:root:current train perplexity3.4972941875457764
INFO:root:current mean train loss 1587.306395647704
INFO:root:current train perplexity3.4973764419555664
INFO:root:current mean train loss 1587.6513867625981
INFO:root:current train perplexity3.4980478286743164
INFO:root:current mean train loss 1587.9257981735514
INFO:root:current train perplexity3.498445510864258
INFO:root:current mean train loss 1587.4106768705142
INFO:root:current train perplexity3.4990293979644775
INFO:root:current mean train loss 1588.3489235749228
INFO:root:current train perplexity3.5008962154388428
INFO:root:current mean train loss 1589.0185881493467
INFO:root:current train perplexity3.50020432472229

100%|██████████| 1/1 [02:18<00:00, 138.28s/it][A100%|██████████| 1/1 [02:18<00:00, 138.28s/it]
INFO:root:final mean train loss: 1588.3132454722563
INFO:root:final train perplexity: 3.4995675086975098
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.22s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2170.449288009752
INFO:root:eval perplexity: 5.785367488861084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/181
 90%|█████████ | 181/200 [7:19:42<46:08, 145.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1595.0929822419819
INFO:root:current train perplexity3.4925334453582764
INFO:root:current mean train loss 1587.8276505903764
INFO:root:current train perplexity3.5012993812561035
INFO:root:current mean train loss 1582.5278032828069
INFO:root:current train perplexity3.4827992916107178
INFO:root:current mean train loss 1583.3796208158453
INFO:root:current train perplexity3.4874660968780518
INFO:root:current mean train loss 1585.6700680516346
INFO:root:current train perplexity3.4879188537597656
INFO:root:current mean train loss 1586.1552891201443
INFO:root:current train perplexity3.491849899291992
INFO:root:current mean train loss 1586.2702643941846
INFO:root:current train perplexity3.492185354232788
INFO:root:current mean train loss 1587.8442953837286
INFO:root:current train perplexity3.4941887855529785
INFO:root:current mean train loss 1587.1869398143194
INFO:root:current train perplexity3.4945333003997803
INFO:root:current mean train loss 1586.1020515316823
INFO:root:current train perplexity3.494713306427002
INFO:root:current mean train loss 1586.0512894981412
INFO:root:current train perplexity3.4960715770721436
INFO:root:current mean train loss 1587.1050109863281
INFO:root:current train perplexity3.494572877883911
INFO:root:current mean train loss 1588.1629359326018
INFO:root:current train perplexity3.495297908782959
INFO:root:current mean train loss 1589.0081047235533
INFO:root:current train perplexity3.4970946311950684
INFO:root:current mean train loss 1588.7039230057217
INFO:root:current train perplexity3.4975268840789795
INFO:root:current mean train loss 1588.9604321010222
INFO:root:current train perplexity3.4974117279052734
INFO:root:current mean train loss 1588.247824657504
INFO:root:current train perplexity3.4965627193450928
INFO:root:current mean train loss 1588.1217471973316
INFO:root:current train perplexity3.498380422592163
INFO:root:current mean train loss 1587.6669718858275
INFO:root:current train perplexity3.4969003200531006
INFO:root:current mean train loss 1588.0548600416917
INFO:root:current train perplexity3.4977028369903564

100%|██████████| 1/1 [02:18<00:00, 138.32s/it][A100%|██████████| 1/1 [02:18<00:00, 138.32s/it]
INFO:root:final mean train loss: 1587.5841550148923
INFO:root:final train perplexity: 3.497555732727051
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.16s/it][A100%|██████████| 1/1 [00:07<00:00,  7.16s/it]
INFO:root:eval mean loss: 2170.975630523465
INFO:root:eval perplexity: 5.787830829620361
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/182
 91%|█████████ | 182/200 [7:22:08<43:43, 145.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1582.0073071551578
INFO:root:current train perplexity3.4827780723571777
INFO:root:current mean train loss 1584.0779219711383
INFO:root:current train perplexity3.495340347290039
INFO:root:current mean train loss 1589.1817210330898
INFO:root:current train perplexity3.492795467376709
INFO:root:current mean train loss 1586.500662844903
INFO:root:current train perplexity3.491968870162964
INFO:root:current mean train loss 1586.1599826774057
INFO:root:current train perplexity3.4924964904785156
INFO:root:current mean train loss 1585.8790649619914
INFO:root:current train perplexity3.48873233795166
INFO:root:current mean train loss 1584.9344831194196
INFO:root:current train perplexity3.4852566719055176
INFO:root:current mean train loss 1585.3716117406702
INFO:root:current train perplexity3.4861016273498535
INFO:root:current mean train loss 1586.3433203617108
INFO:root:current train perplexity3.485649824142456
INFO:root:current mean train loss 1586.095168990551
INFO:root:current train perplexity3.4871926307678223
INFO:root:current mean train loss 1587.054779974125
INFO:root:current train perplexity3.488708257675171
INFO:root:current mean train loss 1588.2579383562318
INFO:root:current train perplexity3.4912221431732178
INFO:root:current mean train loss 1587.8670275718412
INFO:root:current train perplexity3.4901986122131348
INFO:root:current mean train loss 1588.6460031918803
INFO:root:current train perplexity3.4907944202423096
INFO:root:current mean train loss 1588.515946896062
INFO:root:current train perplexity3.4913928508758545
INFO:root:current mean train loss 1587.4735006271333
INFO:root:current train perplexity3.4922075271606445
INFO:root:current mean train loss 1587.322467441187
INFO:root:current train perplexity3.4938759803771973
INFO:root:current mean train loss 1587.3227923042737
INFO:root:current train perplexity3.4946882724761963
INFO:root:current mean train loss 1587.2653642222167
INFO:root:current train perplexity3.4952316284179688

100%|██████████| 1/1 [02:18<00:00, 138.44s/it][A100%|██████████| 1/1 [02:18<00:00, 138.44s/it]
INFO:root:final mean train loss: 1586.8542091727438
INFO:root:final train perplexity: 3.4955427646636963
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2171.1804376696864
INFO:root:eval perplexity: 5.788789749145508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/183
 92%|█████████▏| 183/200 [7:24:34<41:18, 145.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1614.8177001953125
INFO:root:current train perplexity3.4996774196624756
INFO:root:current mean train loss 1574.4117631392046
INFO:root:current train perplexity3.4770028591156006
INFO:root:current mean train loss 1583.9196329752604
INFO:root:current train perplexity3.48629093170166
INFO:root:current mean train loss 1586.4885364163306
INFO:root:current train perplexity3.4855406284332275
INFO:root:current mean train loss 1585.5964790158155
INFO:root:current train perplexity3.490675449371338
INFO:root:current mean train loss 1583.5383406096814
INFO:root:current train perplexity3.491752862930298
INFO:root:current mean train loss 1582.9094040167136
INFO:root:current train perplexity3.4896414279937744
INFO:root:current mean train loss 1584.0272209919674
INFO:root:current train perplexity3.486969232559204
INFO:root:current mean train loss 1581.6010122793691
INFO:root:current train perplexity3.485670566558838
INFO:root:current mean train loss 1584.638311968793
INFO:root:current train perplexity3.4898087978363037
INFO:root:current mean train loss 1585.4587583636294
INFO:root:current train perplexity3.4900155067443848
INFO:root:current mean train loss 1586.4437568183419
INFO:root:current train perplexity3.4928414821624756
INFO:root:current mean train loss 1586.0235154837617
INFO:root:current train perplexity3.494112968444824
INFO:root:current mean train loss 1585.5439972156787
INFO:root:current train perplexity3.492929458618164
INFO:root:current mean train loss 1585.6457398326684
INFO:root:current train perplexity3.4923408031463623
INFO:root:current mean train loss 1586.1741088867188
INFO:root:current train perplexity3.493896007537842
INFO:root:current mean train loss 1585.6703672420904
INFO:root:current train perplexity3.4929697513580322
INFO:root:current mean train loss 1585.389441060741
INFO:root:current train perplexity3.4927265644073486
INFO:root:current mean train loss 1585.853930057083
INFO:root:current train perplexity3.493931770324707
INFO:root:current mean train loss 1587.0026684186846
INFO:root:current train perplexity3.4944028854370117

100%|██████████| 1/1 [02:18<00:00, 138.51s/it][A100%|██████████| 1/1 [02:18<00:00, 138.51s/it]
INFO:root:final mean train loss: 1586.2995031436644
INFO:root:final train perplexity: 3.494014263153076
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.15s/it][A100%|██████████| 1/1 [00:07<00:00,  7.15s/it]
INFO:root:eval mean loss: 2171.1970413965537
INFO:root:eval perplexity: 5.788867950439453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/184
 92%|█████████▏| 184/200 [7:27:00<38:53, 145.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1581.0703034577546
INFO:root:current train perplexity3.5010037422180176
INFO:root:current mean train loss 1585.2585968257874
INFO:root:current train perplexity3.4815754890441895
INFO:root:current mean train loss 1592.568458859616
INFO:root:current train perplexity3.4950037002563477
INFO:root:current mean train loss 1589.3136471622945
INFO:root:current train perplexity3.4869518280029297
INFO:root:current mean train loss 1589.651426078564
INFO:root:current train perplexity3.4871485233306885
INFO:root:current mean train loss 1587.6773758079341
INFO:root:current train perplexity3.488615036010742
INFO:root:current mean train loss 1587.9606489701705
INFO:root:current train perplexity3.491071939468384
INFO:root:current mean train loss 1587.536032234622
INFO:root:current train perplexity3.4916763305664062
INFO:root:current mean train loss 1586.073050004251
INFO:root:current train perplexity3.4912538528442383
INFO:root:current mean train loss 1585.424339187441
INFO:root:current train perplexity3.490427017211914
INFO:root:current mean train loss 1585.222251884661
INFO:root:current train perplexity3.48799204826355
INFO:root:current mean train loss 1585.7877970630338
INFO:root:current train perplexity3.488788366317749
INFO:root:current mean train loss 1585.802415420296
INFO:root:current train perplexity3.492532968521118
INFO:root:current mean train loss 1584.7983032318552
INFO:root:current train perplexity3.492673397064209
INFO:root:current mean train loss 1585.2545723758049
INFO:root:current train perplexity3.4916131496429443
INFO:root:current mean train loss 1585.342694855236
INFO:root:current train perplexity3.492903709411621
INFO:root:current mean train loss 1584.250747502473
INFO:root:current train perplexity3.492701768875122
INFO:root:current mean train loss 1584.7910561972985
INFO:root:current train perplexity3.4920647144317627
INFO:root:current mean train loss 1585.2468932537545
INFO:root:current train perplexity3.49084734916687
INFO:root:current mean train loss 1585.3533976722642
INFO:root:current train perplexity3.4914677143096924

100%|██████████| 1/1 [02:18<00:00, 138.17s/it][A100%|██████████| 1/1 [02:18<00:00, 138.18s/it]
INFO:root:final mean train loss: 1585.9087309575236
INFO:root:final train perplexity: 3.4929373264312744
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2171.796880194481
INFO:root:eval perplexity: 5.791676998138428
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/185
 92%|█████████▎| 185/200 [7:29:25<36:26, 145.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.5889615145597
INFO:root:current train perplexity3.496720552444458
INFO:root:current mean train loss 1586.4498443603516
INFO:root:current train perplexity3.5115411281585693
INFO:root:current mean train loss 1586.327305027696
INFO:root:current train perplexity3.499945640563965
INFO:root:current mean train loss 1586.077701302462
INFO:root:current train perplexity3.4999189376831055
INFO:root:current mean train loss 1585.8349383929828
INFO:root:current train perplexity3.4971654415130615
INFO:root:current mean train loss 1585.0049166959875
INFO:root:current train perplexity3.4952356815338135
INFO:root:current mean train loss 1584.2323189492552
INFO:root:current train perplexity3.491549015045166
INFO:root:current mean train loss 1584.192134405977
INFO:root:current train perplexity3.4906580448150635
INFO:root:current mean train loss 1583.944771716945
INFO:root:current train perplexity3.491075277328491
INFO:root:current mean train loss 1583.1062218617585
INFO:root:current train perplexity3.4873714447021484
INFO:root:current mean train loss 1582.841384712307
INFO:root:current train perplexity3.4841761589050293
INFO:root:current mean train loss 1583.4017926196118
INFO:root:current train perplexity3.48738169670105
INFO:root:current mean train loss 1582.539465018024
INFO:root:current train perplexity3.486849308013916
INFO:root:current mean train loss 1582.5255501156762
INFO:root:current train perplexity3.4877851009368896
INFO:root:current mean train loss 1583.9173101282515
INFO:root:current train perplexity3.488530158996582
INFO:root:current mean train loss 1584.934570233439
INFO:root:current train perplexity3.48987078666687
INFO:root:current mean train loss 1585.6703124405983
INFO:root:current train perplexity3.4901936054229736
INFO:root:current mean train loss 1586.0447867857206
INFO:root:current train perplexity3.49117112159729
INFO:root:current mean train loss 1585.9620079983858
INFO:root:current train perplexity3.490114212036133
INFO:root:current mean train loss 1586.5582800971138
INFO:root:current train perplexity3.491863250732422

100%|██████████| 1/1 [02:18<00:00, 138.31s/it][A100%|██████████| 1/1 [02:18<00:00, 138.31s/it]
INFO:root:final mean train loss: 1585.5966293019474
INFO:root:final train perplexity: 3.492077589035034
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2171.842055300449
INFO:root:eval perplexity: 5.7918877601623535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/186
 93%|█████████▎| 186/200 [7:31:51<34:01, 145.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1585.6333908331198
INFO:root:current train perplexity3.495082378387451
INFO:root:current mean train loss 1586.6220255786588
INFO:root:current train perplexity3.479496717453003
INFO:root:current mean train loss 1577.8815899260655
INFO:root:current train perplexity3.4827983379364014
INFO:root:current mean train loss 1579.7551688830938
INFO:root:current train perplexity3.4824986457824707
INFO:root:current mean train loss 1582.5089000114392
INFO:root:current train perplexity3.487410068511963
INFO:root:current mean train loss 1582.1964202717663
INFO:root:current train perplexity3.4843270778656006
INFO:root:current mean train loss 1581.8386998717615
INFO:root:current train perplexity3.487577438354492
INFO:root:current mean train loss 1582.8763122959613
INFO:root:current train perplexity3.4890174865722656
INFO:root:current mean train loss 1583.6258030270035
INFO:root:current train perplexity3.490253210067749
INFO:root:current mean train loss 1583.4749594538566
INFO:root:current train perplexity3.487827777862549
INFO:root:current mean train loss 1585.648808543127
INFO:root:current train perplexity3.4881205558776855
INFO:root:current mean train loss 1586.0040324208655
INFO:root:current train perplexity3.488394260406494
INFO:root:current mean train loss 1586.963254319024
INFO:root:current train perplexity3.4884486198425293
INFO:root:current mean train loss 1586.6702363338652
INFO:root:current train perplexity3.490877151489258
INFO:root:current mean train loss 1585.8488473755133
INFO:root:current train perplexity3.4895386695861816
INFO:root:current mean train loss 1585.5636342138453
INFO:root:current train perplexity3.489600419998169
INFO:root:current mean train loss 1586.0453644441884
INFO:root:current train perplexity3.4899706840515137
INFO:root:current mean train loss 1585.2994163638823
INFO:root:current train perplexity3.4906179904937744
INFO:root:current mean train loss 1585.3595364266732
INFO:root:current train perplexity3.4908061027526855
INFO:root:current mean train loss 1586.1428182194393
INFO:root:current train perplexity3.491713047027588

100%|██████████| 1/1 [02:18<00:00, 138.23s/it][A100%|██████████| 1/1 [02:18<00:00, 138.23s/it]
INFO:root:final mean train loss: 1585.6508609311963
INFO:root:final train perplexity: 3.492227077484131
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2171.8866529532356
INFO:root:eval perplexity: 5.792096138000488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/187
 94%|█████████▎| 187/200 [7:34:17<31:34, 145.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1596.0971507537058
INFO:root:current train perplexity3.4887871742248535
INFO:root:current mean train loss 1593.840253851387
INFO:root:current train perplexity3.4907054901123047
INFO:root:current mean train loss 1590.3163030610667
INFO:root:current train perplexity3.483959913253784
INFO:root:current mean train loss 1590.560181955812
INFO:root:current train perplexity3.4919803142547607
INFO:root:current mean train loss 1587.0041411970449
INFO:root:current train perplexity3.485745429992676
INFO:root:current mean train loss 1585.9999596618864
INFO:root:current train perplexity3.4859440326690674
INFO:root:current mean train loss 1584.2857244711006
INFO:root:current train perplexity3.481722831726074
INFO:root:current mean train loss 1585.0946904748755
INFO:root:current train perplexity3.479196548461914
INFO:root:current mean train loss 1586.675111392636
INFO:root:current train perplexity3.4813284873962402
INFO:root:current mean train loss 1585.7397631935792
INFO:root:current train perplexity3.483915090560913
INFO:root:current mean train loss 1586.6497249001695
INFO:root:current train perplexity3.485745906829834
INFO:root:current mean train loss 1586.7605930710486
INFO:root:current train perplexity3.4872167110443115
INFO:root:current mean train loss 1585.985637634946
INFO:root:current train perplexity3.484703779220581
INFO:root:current mean train loss 1586.2826802955483
INFO:root:current train perplexity3.485379695892334
INFO:root:current mean train loss 1585.7962173234787
INFO:root:current train perplexity3.485520124435425
INFO:root:current mean train loss 1584.7183242237008
INFO:root:current train perplexity3.485599994659424
INFO:root:current mean train loss 1585.276719649159
INFO:root:current train perplexity3.4878761768341064
INFO:root:current mean train loss 1584.5938559361598
INFO:root:current train perplexity3.4870338439941406
INFO:root:current mean train loss 1585.0469231651232
INFO:root:current train perplexity3.4885895252227783
INFO:root:current mean train loss 1584.6071910646012
INFO:root:current train perplexity3.488222360610962

100%|██████████| 1/1 [02:18<00:00, 138.66s/it][A100%|██████████| 1/1 [02:18<00:00, 138.66s/it]
INFO:root:final mean train loss: 1584.2017094337514
INFO:root:final train perplexity: 3.4882383346557617
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2172.1212599734044
INFO:root:eval perplexity: 5.793196201324463
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/188
 94%|█████████▍| 188/200 [7:36:43<29:10, 145.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1589.9748997738486
INFO:root:current train perplexity3.4976961612701416
INFO:root:current mean train loss 1583.6993464543268
INFO:root:current train perplexity3.49230694770813
INFO:root:current mean train loss 1583.818819931806
INFO:root:current train perplexity3.4907195568084717
INFO:root:current mean train loss 1584.7675076641615
INFO:root:current train perplexity3.487293243408203
INFO:root:current mean train loss 1583.7428249782986
INFO:root:current train perplexity3.483703136444092
INFO:root:current mean train loss 1583.8433374228598
INFO:root:current train perplexity3.4823367595672607
INFO:root:current mean train loss 1584.4529414554295
INFO:root:current train perplexity3.4803948402404785
INFO:root:current mean train loss 1585.6901815546385
INFO:root:current train perplexity3.4805543422698975
INFO:root:current mean train loss 1584.7923128437064
INFO:root:current train perplexity3.482637405395508
INFO:root:current mean train loss 1584.107648226484
INFO:root:current train perplexity3.4803006649017334
INFO:root:current mean train loss 1584.2079578695775
INFO:root:current train perplexity3.4820876121520996
INFO:root:current mean train loss 1584.4214830470385
INFO:root:current train perplexity3.4819211959838867
INFO:root:current mean train loss 1584.1314279681467
INFO:root:current train perplexity3.4819083213806152
INFO:root:current mean train loss 1584.6461834432403
INFO:root:current train perplexity3.483605146408081
INFO:root:current mean train loss 1584.3783216189381
INFO:root:current train perplexity3.484231472015381
INFO:root:current mean train loss 1583.3842589758033
INFO:root:current train perplexity3.484769105911255
INFO:root:current mean train loss 1583.6908730295906
INFO:root:current train perplexity3.484342336654663
INFO:root:current mean train loss 1583.8646171548573
INFO:root:current train perplexity3.485830545425415
INFO:root:current mean train loss 1583.9108107916597
INFO:root:current train perplexity3.485130786895752

100%|██████████| 1/1 [02:18<00:00, 138.84s/it][A100%|██████████| 1/1 [02:18<00:00, 138.84s/it]
INFO:root:final mean train loss: 1583.4575396916268
INFO:root:final train perplexity: 3.4861912727355957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.12s/it][A100%|██████████| 1/1 [00:07<00:00,  7.12s/it]
INFO:root:eval mean loss: 2172.3303529996397
INFO:root:eval perplexity: 5.7941765785217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/189
 94%|█████████▍| 189/200 [7:39:09<26:45, 145.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1580.6517639160156
INFO:root:current train perplexity3.481532335281372
INFO:root:current mean train loss 1576.5994382585798
INFO:root:current train perplexity3.464444875717163
INFO:root:current mean train loss 1579.0929467543117
INFO:root:current train perplexity3.4762306213378906
INFO:root:current mean train loss 1578.2452247815254
INFO:root:current train perplexity3.480698347091675
INFO:root:current mean train loss 1578.5268344323613
INFO:root:current train perplexity3.4744582176208496
INFO:root:current mean train loss 1578.9474585056305
INFO:root:current train perplexity3.4714651107788086
INFO:root:current mean train loss 1578.1998983146318
INFO:root:current train perplexity3.4715576171875
INFO:root:current mean train loss 1577.9590366663558
INFO:root:current train perplexity3.473525285720825
INFO:root:current mean train loss 1578.9772979285328
INFO:root:current train perplexity3.4756662845611572
INFO:root:current mean train loss 1579.7412817436352
INFO:root:current train perplexity3.479970932006836
INFO:root:current mean train loss 1580.5025035270119
INFO:root:current train perplexity3.480717420578003
INFO:root:current mean train loss 1580.7548610769588
INFO:root:current train perplexity3.4805455207824707
INFO:root:current mean train loss 1580.5276126672725
INFO:root:current train perplexity3.4812419414520264
INFO:root:current mean train loss 1581.2468498043897
INFO:root:current train perplexity3.4808907508850098
INFO:root:current mean train loss 1581.055789763799
INFO:root:current train perplexity3.4811758995056152
INFO:root:current mean train loss 1581.7760197069279
INFO:root:current train perplexity3.4826560020446777
INFO:root:current mean train loss 1582.2792518180297
INFO:root:current train perplexity3.4826953411102295
INFO:root:current mean train loss 1582.6133613942939
INFO:root:current train perplexity3.4841060638427734
INFO:root:current mean train loss 1583.2368058295176
INFO:root:current train perplexity3.4849531650543213
INFO:root:current mean train loss 1582.7707027291654
INFO:root:current train perplexity3.483454942703247

100%|██████████| 1/1 [02:18<00:00, 138.92s/it][A100%|██████████| 1/1 [02:18<00:00, 138.92s/it]
INFO:root:final mean train loss: 1582.4895222058394
INFO:root:final train perplexity: 3.4835309982299805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2172.6769634273883
INFO:root:eval perplexity: 5.795799732208252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/190
 95%|█████████▌| 190/200 [7:41:36<24:21, 146.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1604.8137712149785
INFO:root:current train perplexity3.475621461868286
INFO:root:current mean train loss 1583.6006001317223
INFO:root:current train perplexity3.491373300552368
INFO:root:current mean train loss 1579.9374173759893
INFO:root:current train perplexity3.4820258617401123
INFO:root:current mean train loss 1580.0711985301102
INFO:root:current train perplexity3.483851432800293
INFO:root:current mean train loss 1582.6863888471555
INFO:root:current train perplexity3.4851484298706055
INFO:root:current mean train loss 1581.8246027292062
INFO:root:current train perplexity3.484893560409546
INFO:root:current mean train loss 1584.0126937599364
INFO:root:current train perplexity3.4842164516448975
INFO:root:current mean train loss 1583.6309436018412
INFO:root:current train perplexity3.4820806980133057
INFO:root:current mean train loss 1583.6671359035736
INFO:root:current train perplexity3.482445001602173
INFO:root:current mean train loss 1584.6996821967084
INFO:root:current train perplexity3.4844977855682373
INFO:root:current mean train loss 1582.4914132017204
INFO:root:current train perplexity3.482046127319336
INFO:root:current mean train loss 1583.0734389704662
INFO:root:current train perplexity3.4822022914886475
INFO:root:current mean train loss 1583.6556833513973
INFO:root:current train perplexity3.4834377765655518
INFO:root:current mean train loss 1583.534832658043
INFO:root:current train perplexity3.4836766719818115
INFO:root:current mean train loss 1583.613250924625
INFO:root:current train perplexity3.484062433242798
INFO:root:current mean train loss 1582.9486505522145
INFO:root:current train perplexity3.4825799465179443
INFO:root:current mean train loss 1582.4233142157286
INFO:root:current train perplexity3.48184871673584
INFO:root:current mean train loss 1582.0734889403873
INFO:root:current train perplexity3.4811198711395264
INFO:root:current mean train loss 1582.6934424015
INFO:root:current train perplexity3.48264479637146
INFO:root:current mean train loss 1583.2781161658809
INFO:root:current train perplexity3.4833834171295166

100%|██████████| 1/1 [02:18<00:00, 138.56s/it][A100%|██████████| 1/1 [02:18<00:00, 138.56s/it]
INFO:root:final mean train loss: 1582.5473050162218
INFO:root:final train perplexity: 3.4836902618408203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.11s/it][A100%|██████████| 1/1 [00:07<00:00,  7.11s/it]
INFO:root:eval mean loss: 2172.7757066226177
INFO:root:eval perplexity: 5.796263217926025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/191
 96%|█████████▌| 191/200 [7:44:02<21:54, 146.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.1417289402175
INFO:root:current train perplexity3.4970924854278564
INFO:root:current mean train loss 1586.892551369863
INFO:root:current train perplexity3.4826693534851074
INFO:root:current mean train loss 1586.6038907679115
INFO:root:current train perplexity3.488243579864502
INFO:root:current mean train loss 1583.4299238789288
INFO:root:current train perplexity3.4872844219207764
INFO:root:current mean train loss 1581.6653154997548
INFO:root:current train perplexity3.483696937561035
INFO:root:current mean train loss 1581.9759821070857
INFO:root:current train perplexity3.483699083328247
INFO:root:current mean train loss 1582.5784460487012
INFO:root:current train perplexity3.4850378036499023
INFO:root:current mean train loss 1582.9490485715482
INFO:root:current train perplexity3.485097646713257
INFO:root:current mean train loss 1582.4390267446531
INFO:root:current train perplexity3.4828834533691406
INFO:root:current mean train loss 1583.2002283463266
INFO:root:current train perplexity3.483043909072876
INFO:root:current mean train loss 1583.0106572284296
INFO:root:current train perplexity3.4830322265625
INFO:root:current mean train loss 1582.402480732916
INFO:root:current train perplexity3.4815502166748047
INFO:root:current mean train loss 1581.5350346695363
INFO:root:current train perplexity3.479931116104126
INFO:root:current mean train loss 1582.1103823974972
INFO:root:current train perplexity3.4801137447357178
INFO:root:current mean train loss 1582.2573813706183
INFO:root:current train perplexity3.480501890182495
INFO:root:current mean train loss 1581.6635867732
INFO:root:current train perplexity3.478367567062378
INFO:root:current mean train loss 1581.4453122033528
INFO:root:current train perplexity3.479783773422241
INFO:root:current mean train loss 1581.3105568727403
INFO:root:current train perplexity3.4802310466766357
INFO:root:current mean train loss 1582.2207277903449
INFO:root:current train perplexity3.4815261363983154
INFO:root:current mean train loss 1582.4602264059288
INFO:root:current train perplexity3.481863498687744

100%|██████████| 1/1 [02:18<00:00, 138.61s/it][A100%|██████████| 1/1 [02:18<00:00, 138.61s/it]
INFO:root:final mean train loss: 1582.1655015200001
INFO:root:final train perplexity: 3.4826407432556152
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.26s/it][A100%|██████████| 1/1 [00:07<00:00,  7.26s/it]
INFO:root:eval mean loss: 2172.7214961491577
INFO:root:eval perplexity: 5.7960100173950195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/192
 96%|█████████▌| 192/200 [7:46:28<19:28, 146.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1596.576652405754
INFO:root:current train perplexity3.5261354446411133
INFO:root:current mean train loss 1590.4147956707727
INFO:root:current train perplexity3.4911868572235107
INFO:root:current mean train loss 1586.7590229919201
INFO:root:current train perplexity3.470884084701538
INFO:root:current mean train loss 1584.0576837713068
INFO:root:current train perplexity3.479285717010498
INFO:root:current mean train loss 1583.8736624995781
INFO:root:current train perplexity3.4799187183380127
INFO:root:current mean train loss 1582.8941934426343
INFO:root:current train perplexity3.4772768020629883
INFO:root:current mean train loss 1581.8721844657334
INFO:root:current train perplexity3.477081060409546
INFO:root:current mean train loss 1583.1941057157703
INFO:root:current train perplexity3.477808713912964
INFO:root:current mean train loss 1583.2765052415266
INFO:root:current train perplexity3.480104923248291
INFO:root:current mean train loss 1583.6376176083447
INFO:root:current train perplexity3.480677604675293
INFO:root:current mean train loss 1584.1350419196115
INFO:root:current train perplexity3.482607126235962
INFO:root:current mean train loss 1584.236799822321
INFO:root:current train perplexity3.482895612716675
INFO:root:current mean train loss 1584.1301324622366
INFO:root:current train perplexity3.481738328933716
INFO:root:current mean train loss 1583.656899757973
INFO:root:current train perplexity3.4813222885131836
INFO:root:current mean train loss 1583.7015900680324
INFO:root:current train perplexity3.4810879230499268
INFO:root:current mean train loss 1583.6374920181793
INFO:root:current train perplexity3.4821524620056152
INFO:root:current mean train loss 1583.657476355208
INFO:root:current train perplexity3.4844374656677246
INFO:root:current mean train loss 1583.2791280676138
INFO:root:current train perplexity3.482614040374756
INFO:root:current mean train loss 1582.4281157873934
INFO:root:current train perplexity3.4830424785614014
INFO:root:current mean train loss 1582.9720820904506
INFO:root:current train perplexity3.4840471744537354

100%|██████████| 1/1 [02:18<00:00, 138.60s/it][A100%|██████████| 1/1 [02:18<00:00, 138.60s/it]
INFO:root:final mean train loss: 1582.791541949343
INFO:root:final train perplexity: 3.484360694885254
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.27s/it][A100%|██████████| 1/1 [00:07<00:00,  7.27s/it]
INFO:root:eval mean loss: 2172.947153510777
INFO:root:eval perplexity: 5.797067165374756
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/193
 96%|█████████▋| 193/200 [7:48:54<17:02, 146.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.7183700561523
INFO:root:current train perplexity3.470446825027466
INFO:root:current mean train loss 1577.574484592014
INFO:root:current train perplexity3.464507579803467
INFO:root:current mean train loss 1576.6929338727678
INFO:root:current train perplexity3.4677822589874268
INFO:root:current mean train loss 1579.094570440995
INFO:root:current train perplexity3.46169376373291
INFO:root:current mean train loss 1583.1237030029297
INFO:root:current train perplexity3.4775328636169434
INFO:root:current mean train loss 1579.958619847791
INFO:root:current train perplexity3.4710798263549805
INFO:root:current mean train loss 1580.5296731387868
INFO:root:current train perplexity3.472890615463257
INFO:root:current mean train loss 1579.978295741937
INFO:root:current train perplexity3.475720167160034
INFO:root:current mean train loss 1581.395495744185
INFO:root:current train perplexity3.475795030593872
INFO:root:current mean train loss 1580.5771357322226
INFO:root:current train perplexity3.474440574645996
INFO:root:current mean train loss 1581.7876324688946
INFO:root:current train perplexity3.476301908493042
INFO:root:current mean train loss 1581.4097484523968
INFO:root:current train perplexity3.4777674674987793
INFO:root:current mean train loss 1581.6639384269715
INFO:root:current train perplexity3.477850914001465
INFO:root:current mean train loss 1581.9671966995018
INFO:root:current train perplexity3.477771282196045
INFO:root:current mean train loss 1581.937861262141
INFO:root:current train perplexity3.4777960777282715
INFO:root:current mean train loss 1582.6289321319966
INFO:root:current train perplexity3.4799559116363525
INFO:root:current mean train loss 1582.9293189639136
INFO:root:current train perplexity3.480480909347534
INFO:root:current mean train loss 1582.2589223111613
INFO:root:current train perplexity3.4815049171447754
INFO:root:current mean train loss 1582.289430204351
INFO:root:current train perplexity3.481848955154419
INFO:root:current mean train loss 1582.4104225235756
INFO:root:current train perplexity3.4820570945739746

100%|██████████| 1/1 [02:18<00:00, 138.34s/it][A100%|██████████| 1/1 [02:18<00:00, 138.34s/it]
INFO:root:final mean train loss: 1581.927675186596
INFO:root:final train perplexity: 3.481987714767456
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2173.0454789831283
INFO:root:eval perplexity: 5.7975287437438965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/194
 97%|█████████▋| 194/200 [7:51:20<14:36, 146.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1602.1504447386437
INFO:root:current train perplexity3.497199296951294
INFO:root:current mean train loss 1588.0895165767768
INFO:root:current train perplexity3.483837366104126
INFO:root:current mean train loss 1585.4325740313288
INFO:root:current train perplexity3.481210231781006
INFO:root:current mean train loss 1582.612419378247
INFO:root:current train perplexity3.478346347808838
INFO:root:current mean train loss 1580.5230079205703
INFO:root:current train perplexity3.4788601398468018
INFO:root:current mean train loss 1582.9873147066714
INFO:root:current train perplexity3.4812488555908203
INFO:root:current mean train loss 1582.8197215886164
INFO:root:current train perplexity3.4824039936065674
INFO:root:current mean train loss 1580.9477715199087
INFO:root:current train perplexity3.479309320449829
INFO:root:current mean train loss 1581.0557548327324
INFO:root:current train perplexity3.47983455657959
INFO:root:current mean train loss 1579.9955987346807
INFO:root:current train perplexity3.482332944869995
INFO:root:current mean train loss 1580.6973002319892
INFO:root:current train perplexity3.485276222229004
INFO:root:current mean train loss 1580.5863563327264
INFO:root:current train perplexity3.4838597774505615
INFO:root:current mean train loss 1581.6278819699608
INFO:root:current train perplexity3.482793092727661
INFO:root:current mean train loss 1581.435911862613
INFO:root:current train perplexity3.4829325675964355
INFO:root:current mean train loss 1581.501874354177
INFO:root:current train perplexity3.4825692176818848
INFO:root:current mean train loss 1581.827237181165
INFO:root:current train perplexity3.480987787246704
INFO:root:current mean train loss 1581.4472395133184
INFO:root:current train perplexity3.4810612201690674
INFO:root:current mean train loss 1581.3866580884592
INFO:root:current train perplexity3.4804985523223877
INFO:root:current mean train loss 1581.3630404555302
INFO:root:current train perplexity3.480233907699585

100%|██████████| 1/1 [02:18<00:00, 138.65s/it][A100%|██████████| 1/1 [02:18<00:00, 138.65s/it]
INFO:root:final mean train loss: 1581.342304239355
INFO:root:final train perplexity: 3.4803802967071533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.31s/it][A100%|██████████| 1/1 [00:07<00:00,  7.31s/it]
INFO:root:eval mean loss: 2173.175124148105
INFO:root:eval perplexity: 5.7981367111206055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/195
 98%|█████████▊| 195/200 [7:53:46<12:10, 146.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.0973074776787
INFO:root:current train perplexity3.501997709274292
INFO:root:current mean train loss 1557.9039360180236
INFO:root:current train perplexity3.440048933029175
INFO:root:current mean train loss 1571.040121773693
INFO:root:current train perplexity3.449733018875122
INFO:root:current mean train loss 1579.4297532002638
INFO:root:current train perplexity3.463472366333008
INFO:root:current mean train loss 1579.1675597849676
INFO:root:current train perplexity3.4660260677337646
INFO:root:current mean train loss 1578.422198225088
INFO:root:current train perplexity3.469090461730957
INFO:root:current mean train loss 1579.5512818575683
INFO:root:current train perplexity3.469456911087036
INFO:root:current mean train loss 1579.8470723982953
INFO:root:current train perplexity3.4745092391967773
INFO:root:current mean train loss 1580.25880435878
INFO:root:current train perplexity3.4776103496551514
INFO:root:current mean train loss 1581.5470959018564
INFO:root:current train perplexity3.4779469966888428
INFO:root:current mean train loss 1581.0614531327046
INFO:root:current train perplexity3.48061203956604
INFO:root:current mean train loss 1578.629293828721
INFO:root:current train perplexity3.4783966541290283
INFO:root:current mean train loss 1579.6123913634551
INFO:root:current train perplexity3.4758312702178955
INFO:root:current mean train loss 1579.9089107426335
INFO:root:current train perplexity3.4772701263427734
INFO:root:current mean train loss 1580.2485815153432
INFO:root:current train perplexity3.475998640060425
INFO:root:current mean train loss 1580.3457968949956
INFO:root:current train perplexity3.476602792739868
INFO:root:current mean train loss 1580.599268349573
INFO:root:current train perplexity3.478229522705078
INFO:root:current mean train loss 1580.7638331872859
INFO:root:current train perplexity3.478482484817505
INFO:root:current mean train loss 1581.3279885288898
INFO:root:current train perplexity3.4801745414733887
INFO:root:current mean train loss 1581.3492085966072
INFO:root:current train perplexity3.4796485900878906

100%|██████████| 1/1 [02:18<00:00, 138.48s/it][A100%|██████████| 1/1 [02:18<00:00, 138.48s/it]
INFO:root:final mean train loss: 1581.2088029623874
INFO:root:final train perplexity: 3.4800140857696533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.22s/it]
INFO:root:eval mean loss: 2173.19837897551
INFO:root:eval perplexity: 5.798244953155518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/196
 98%|█████████▊| 196/200 [7:56:12<09:44, 146.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1578.5642641129032
INFO:root:current train perplexity3.4667463302612305
INFO:root:current mean train loss 1573.551668356393
INFO:root:current train perplexity3.4737839698791504
INFO:root:current mean train loss 1569.3483860296603
INFO:root:current train perplexity3.4837634563446045
INFO:root:current mean train loss 1572.8111321486736
INFO:root:current train perplexity3.47859263420105
INFO:root:current mean train loss 1576.191248210013
INFO:root:current train perplexity3.4708235263824463
INFO:root:current mean train loss 1575.9743656941503
INFO:root:current train perplexity3.4675395488739014
INFO:root:current mean train loss 1576.4279903164002
INFO:root:current train perplexity3.47217059135437
INFO:root:current mean train loss 1578.0822313050403
INFO:root:current train perplexity3.473241090774536
INFO:root:current mean train loss 1578.337892534644
INFO:root:current train perplexity3.4744627475738525
INFO:root:current mean train loss 1579.2573800747684
INFO:root:current train perplexity3.4765007495880127
INFO:root:current mean train loss 1580.5489142017384
INFO:root:current train perplexity3.477386236190796
INFO:root:current mean train loss 1581.2256249007032
INFO:root:current train perplexity3.4758145809173584
INFO:root:current mean train loss 1581.919396209097
INFO:root:current train perplexity3.4762916564941406
INFO:root:current mean train loss 1581.766056143906
INFO:root:current train perplexity3.4770615100860596
INFO:root:current mean train loss 1580.7259889998527
INFO:root:current train perplexity3.477313280105591
INFO:root:current mean train loss 1580.7145347071971
INFO:root:current train perplexity3.4766623973846436
INFO:root:current mean train loss 1580.3950295603254
INFO:root:current train perplexity3.477048635482788
INFO:root:current mean train loss 1580.847428681601
INFO:root:current train perplexity3.47784686088562
INFO:root:current mean train loss 1580.4348601878244
INFO:root:current train perplexity3.4778497219085693
INFO:root:current mean train loss 1580.789326932997
INFO:root:current train perplexity3.477278709411621

100%|██████████| 1/1 [02:18<00:00, 138.44s/it][A100%|██████████| 1/1 [02:18<00:00, 138.44s/it]
INFO:root:final mean train loss: 1580.1844503659524
INFO:root:final train perplexity: 3.477203845977783
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.21s/it][A100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2173.3820740178967
INFO:root:eval perplexity: 5.799106121063232
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/197
 98%|█████████▊| 197/200 [7:58:38<07:18, 146.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.1513366699219
INFO:root:current train perplexity3.4695441722869873
INFO:root:current mean train loss 1578.878494675095
INFO:root:current train perplexity3.4770407676696777
INFO:root:current mean train loss 1579.1577689878402
INFO:root:current train perplexity3.483175039291382
INFO:root:current mean train loss 1577.8233905660695
INFO:root:current train perplexity3.4752423763275146
INFO:root:current mean train loss 1580.7492427825928
INFO:root:current train perplexity3.4764344692230225
INFO:root:current mean train loss 1580.497359672602
INFO:root:current train perplexity3.476736307144165
INFO:root:current mean train loss 1580.160685032974
INFO:root:current train perplexity3.4776854515075684
INFO:root:current mean train loss 1580.6604411895262
INFO:root:current train perplexity3.4802298545837402
INFO:root:current mean train loss 1582.8036858900539
INFO:root:current train perplexity3.4804530143737793
INFO:root:current mean train loss 1582.4148630391696
INFO:root:current train perplexity3.4804956912994385
INFO:root:current mean train loss 1581.6689294713144
INFO:root:current train perplexity3.478483200073242
INFO:root:current mean train loss 1582.2921835869447
INFO:root:current train perplexity3.480997085571289
INFO:root:current mean train loss 1581.329239087227
INFO:root:current train perplexity3.4812164306640625
INFO:root:current mean train loss 1581.3055860932573
INFO:root:current train perplexity3.480137825012207
INFO:root:current mean train loss 1581.366997397407
INFO:root:current train perplexity3.478818655014038
INFO:root:current mean train loss 1581.6683104364754
INFO:root:current train perplexity3.480682134628296
INFO:root:current mean train loss 1581.2851163253044
INFO:root:current train perplexity3.479515552520752
INFO:root:current mean train loss 1581.3300086398965
INFO:root:current train perplexity3.478316068649292
INFO:root:current mean train loss 1581.315251800405
INFO:root:current train perplexity3.4786581993103027
INFO:root:current mean train loss 1581.2747644193364
INFO:root:current train perplexity3.479039430618286

100%|██████████| 1/1 [02:18<00:00, 138.72s/it][A100%|██████████| 1/1 [02:18<00:00, 138.72s/it]
INFO:root:final mean train loss: 1580.6724640523553
INFO:root:final train perplexity: 3.4785425662994385
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.17s/it][A100%|██████████| 1/1 [00:07<00:00,  7.17s/it]
INFO:root:eval mean loss: 2173.3560176335327
INFO:root:eval perplexity: 5.798985004425049
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/198
 99%|█████████▉| 198/200 [8:01:04<04:52, 146.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.938097205529
INFO:root:current train perplexity3.456395387649536
INFO:root:current mean train loss 1578.2706809303977
INFO:root:current train perplexity3.4590420722961426
INFO:root:current mean train loss 1577.7100581331074
INFO:root:current train perplexity3.4583473205566406
INFO:root:current mean train loss 1577.8886802359802
INFO:root:current train perplexity3.466657876968384
INFO:root:current mean train loss 1576.6417089318716
INFO:root:current train perplexity3.4610719680786133
INFO:root:current mean train loss 1576.1865346722898
INFO:root:current train perplexity3.46586012840271
INFO:root:current mean train loss 1576.2322529957708
INFO:root:current train perplexity3.4643568992614746
INFO:root:current mean train loss 1576.1629273258782
INFO:root:current train perplexity3.467439889907837
INFO:root:current mean train loss 1577.5885997617866
INFO:root:current train perplexity3.469494104385376
INFO:root:current mean train loss 1579.4636004037807
INFO:root:current train perplexity3.4741923809051514
INFO:root:current mean train loss 1578.788047310556
INFO:root:current train perplexity3.4747884273529053
INFO:root:current mean train loss 1580.235957408463
INFO:root:current train perplexity3.477189302444458
INFO:root:current mean train loss 1579.9641450060215
INFO:root:current train perplexity3.476813793182373
INFO:root:current mean train loss 1580.3736191298935
INFO:root:current train perplexity3.475891590118408
INFO:root:current mean train loss 1580.4585586704086
INFO:root:current train perplexity3.477698802947998
INFO:root:current mean train loss 1580.0472224908897
INFO:root:current train perplexity3.4780428409576416
INFO:root:current mean train loss 1580.5162949570665
INFO:root:current train perplexity3.4765841960906982
INFO:root:current mean train loss 1579.9967458060376
INFO:root:current train perplexity3.4753401279449463
INFO:root:current mean train loss 1580.149162067485
INFO:root:current train perplexity3.4745850563049316
INFO:root:current mean train loss 1579.4883515103172
INFO:root:current train perplexity3.4746155738830566

100%|██████████| 1/1 [02:19<00:00, 139.07s/it][A100%|██████████| 1/1 [02:19<00:00, 139.07s/it]
INFO:root:final mean train loss: 1579.3307378771806
INFO:root:final train perplexity: 3.474863290786743
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.28s/it][A100%|██████████| 1/1 [00:07<00:00,  7.28s/it]
INFO:root:eval mean loss: 2173.469984987949
INFO:root:eval perplexity: 5.799518585205078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/199
100%|█████████▉| 199/200 [8:03:31<02:26, 146.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1589.7141574766579
INFO:root:current train perplexity3.481928825378418
INFO:root:current mean train loss 1579.1879346239698
INFO:root:current train perplexity3.4731812477111816
INFO:root:current mean train loss 1584.9115522668717
INFO:root:current train perplexity3.474320888519287
INFO:root:current mean train loss 1584.1926739278265
INFO:root:current train perplexity3.4718730449676514
INFO:root:current mean train loss 1583.14661885495
INFO:root:current train perplexity3.474268674850464
INFO:root:current mean train loss 1582.6225483163525
INFO:root:current train perplexity3.474984645843506
INFO:root:current mean train loss 1581.8220055543782
INFO:root:current train perplexity3.47965407371521
INFO:root:current mean train loss 1581.335461082361
INFO:root:current train perplexity3.479724168777466
INFO:root:current mean train loss 1581.6164932769982
INFO:root:current train perplexity3.4771804809570312
INFO:root:current mean train loss 1580.6208672610903
INFO:root:current train perplexity3.4796299934387207
INFO:root:current mean train loss 1580.0662811335706
INFO:root:current train perplexity3.4797539710998535
INFO:root:current mean train loss 1579.85688639736
INFO:root:current train perplexity3.477642774581909
INFO:root:current mean train loss 1580.6690212969847
INFO:root:current train perplexity3.4795491695404053
INFO:root:current mean train loss 1580.9070290594473
INFO:root:current train perplexity3.4789459705352783
INFO:root:current mean train loss 1580.535665123408
INFO:root:current train perplexity3.479234218597412
INFO:root:current mean train loss 1580.1007221284617
INFO:root:current train perplexity3.478374481201172
INFO:root:current mean train loss 1579.7896387415465
INFO:root:current train perplexity3.4780642986297607
INFO:root:current mean train loss 1579.880728330944
INFO:root:current train perplexity3.477323055267334
INFO:root:current mean train loss 1580.10990609018
INFO:root:current train perplexity3.4772679805755615
INFO:root:current mean train loss 1580.250648598618
INFO:root:current train perplexity3.4762277603149414

100%|██████████| 1/1 [02:18<00:00, 138.63s/it][A100%|██████████| 1/1 [02:18<00:00, 138.63s/it]
INFO:root:final mean train loss: 1579.8123622322948
INFO:root:final train perplexity: 3.4761838912963867
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:07<00:00,  7.19s/it][A100%|██████████| 1/1 [00:07<00:00,  7.19s/it]
INFO:root:eval mean loss: 2173.482518405779
INFO:root:eval perplexity: 5.799577713012695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqa_l6_baseline/200
100%|██████████| 200/200 [8:05:57<00:00, 146.20s/it]100%|██████████| 200/200 [8:05:57<00:00, 145.79s/it]
INFO:root:evaluating final model
INFO:root:start evaluating on validation
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:07<00:00,  7.21s/it]100%|██████████| 1/1 [00:07<00:00,  7.21s/it]
INFO:root:eval mean loss: 2173.482518405779
INFO:root:eval perplexity: 5.799577713012695
INFO:root:evalaution complete
INFO:root:save model final: multiqa_l6_baseline/final
