INFO:root:Output: small_multiqa_multiqa
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'cls.predictions.decoder.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 97890.09327651515
INFO:root:current train perplexity15263.99609375
INFO:root:current mean train loss 81517.75883322864
INFO:root:current train perplexity3073.3115234375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.44s/it]
INFO:root:final mean train loss: 75142.59844380041
INFO:root:final train perplexity: 1654.8775634765625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.99s/it]
INFO:root:eval mean loss: 44215.04241071428
INFO:root:eval perplexity: 97.13153076171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/1

  0%|          | 1/200 [08:55<29:36:25, 535.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42948.03990502451
INFO:root:current train perplexity69.83186340332031
INFO:root:current mean train loss 39137.55660182119
INFO:root:current train perplexity47.37162399291992


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it]
INFO:root:final mean train loss: 36539.76310483871
INFO:root:final train perplexity: 36.744873046875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.39s/it]
INFO:root:eval mean loss: 31779.748093377977
INFO:root:eval perplexity: 26.817825317382812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/2

  1%|          | 2/200 [17:47<29:19:58, 533.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31231.842447916668
INFO:root:current train perplexity22.098970413208008
INFO:root:current mean train loss 29715.778386680824
INFO:root:current train perplexity18.69947052001953
INFO:root:current mean train loss 28810.673751154558
INFO:root:current train perplexity17.104122161865234


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.40s/it]
INFO:root:final mean train loss: 28427.527958039314
INFO:root:final train perplexity: 16.508419036865234
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.30s/it]
INFO:root:eval mean loss: 28545.91671316964
INFO:root:eval perplexity: 19.1898136138916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/3

  2%|â–         | 3/200 [26:40<29:10:57, 533.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26411.014630681817
INFO:root:current train perplexity13.456194877624512
INFO:root:current mean train loss 25920.703918850806
INFO:root:current train perplexity12.861058235168457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.82s/it]
INFO:root:final mean train loss: 25550.24406186996
INFO:root:final train perplexity: 12.42956256866455
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.14s/it]
INFO:root:eval mean loss: 27130.937546502977
INFO:root:eval perplexity: 16.575654983520508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/4

  2%|â–         | 4/200 [35:27<28:54:06, 530.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24875.43331473214
INFO:root:current train perplexity11.3126859664917
INFO:root:current mean train loss 24400.82452905958
INFO:root:current train perplexity11.04273796081543
INFO:root:current mean train loss 24115.859922252417
INFO:root:current train perplexity10.781543731689453


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.79s/it]
INFO:root:final mean train loss: 24002.296418220765
INFO:root:final train perplexity: 10.669614791870117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it]
INFO:root:eval mean loss: 26321.69689360119
INFO:root:eval perplexity: 15.243941307067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/5

  2%|â–Ž         | 5/200 [44:29<28:58:25, 534.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23371.463320974577
INFO:root:current train perplexity10.019562721252441
INFO:root:current mean train loss 23136.171960986634
INFO:root:current train perplexity9.780916213989258


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.68s/it]
INFO:root:final mean train loss: 22986.980476625504
INFO:root:final train perplexity: 9.652888298034668
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it]
INFO:root:eval mean loss: 25760.345865885418
INFO:root:eval perplexity: 14.383543968200684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/6

  3%|â–Ž         | 6/200 [53:20<28:45:00, 533.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22346.43927556818
INFO:root:current train perplexity9.115103721618652
INFO:root:current mean train loss 22409.483723958332
INFO:root:current train perplexity9.110406875610352
INFO:root:current mean train loss 22285.107301540284
INFO:root:current train perplexity8.996334075927734


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.34s/it]
INFO:root:final mean train loss: 22233.081251575102
INFO:root:final train perplexity: 8.961149215698242
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.35s/it]
INFO:root:eval mean loss: 25313.69835844494
INFO:root:eval perplexity: 13.733783721923828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/7

  4%|â–Ž         | 7/200 [1:02:08<28:29:51, 531.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21873.011842757936
INFO:root:current train perplexity8.634847640991211
INFO:root:current mean train loss 21781.454778565952
INFO:root:current train perplexity8.54178237915039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.34s/it]
INFO:root:final mean train loss: 21664.437255859375
INFO:root:final train perplexity: 8.472384452819824
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.02s/it]
INFO:root:eval mean loss: 25005.021484375
INFO:root:eval perplexity: 13.301965713500977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/8

  4%|â–         | 8/200 [1:11:09<28:30:28, 534.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21436.242708333335
INFO:root:current train perplexity8.207365036010742
INFO:root:current mean train loss 21351.17058423913
INFO:root:current train perplexity8.182992935180664
INFO:root:current mean train loss 21237.308739098837
INFO:root:current train perplexity8.110036849975586


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.22s/it]
INFO:root:final mean train loss: 21201.44515498992
INFO:root:final train perplexity: 8.094183921813965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.59s/it]
INFO:root:eval mean loss: 24707.3662109375
INFO:root:eval perplexity: 12.898435592651367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/9

  4%|â–         | 9/200 [1:20:12<28:30:32, 537.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20974.98245102612
INFO:root:current train perplexity7.876243591308594
INFO:root:current mean train loss 20904.938271893712
INFO:root:current train perplexity7.839421272277832


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.04s/it]
INFO:root:final mean train loss: 20809.78953109249
INFO:root:final train perplexity: 7.787469863891602
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.92s/it]
INFO:root:eval mean loss: 24465.709984188987
INFO:root:eval perplexity: 12.579841613769531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/10

  5%|â–Œ         | 10/200 [1:29:25<28:36:34, 542.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20769.378289473683
INFO:root:current train perplexity7.62421178817749
INFO:root:current mean train loss 20592.608554359245
INFO:root:current train perplexity7.575190544128418
INFO:root:current mean train loss 20498.688882348742
INFO:root:current train perplexity7.540170669555664


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.25s/it]
INFO:root:final mean train loss: 20470.906198809225
INFO:root:final train perplexity: 7.531478404998779
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.02s/it]
INFO:root:eval mean loss: 24267.431966145832
INFO:root:eval perplexity: 12.324325561523438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/11

  6%|â–Œ         | 11/200 [1:38:38<28:38:13, 545.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20290.490289392605
INFO:root:current train perplexity7.35089635848999
INFO:root:current mean train loss 20231.878415113304
INFO:root:current train perplexity7.341670513153076


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.36s/it]
INFO:root:final mean train loss: 20183.540031186996
INFO:root:final train perplexity: 7.321005344390869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.32s/it]
INFO:root:eval mean loss: 24086.983351934523
INFO:root:eval perplexity: 12.096294403076172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/12

  6%|â–Œ         | 12/200 [1:47:55<28:40:27, 549.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20054.452275815216
INFO:root:current train perplexity7.199102401733398
INFO:root:current mean train loss 19961.271404979674
INFO:root:current train perplexity7.167207717895508
INFO:root:current mean train loss 19934.8878047926
INFO:root:current train perplexity7.1403703689575195


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.05s/it]
INFO:root:final mean train loss: 19928.7199155746
INFO:root:final train perplexity: 7.13929557800293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it]
INFO:root:eval mean loss: 23942.247163318454
INFO:root:eval perplexity: 11.916447639465332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/13

  6%|â–‹         | 13/200 [1:56:58<28:25:26, 547.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19803.894479166665
INFO:root:current train perplexity7.007542133331299
INFO:root:current mean train loss 19733.457488839285
INFO:root:current train perplexity6.993601322174072


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.89s/it]
INFO:root:final mean train loss: 19697.39369250882
INFO:root:final train perplexity: 6.978248119354248
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.13s/it]
INFO:root:eval mean loss: 23808.09972563244
INFO:root:eval perplexity: 11.752144813537598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/14

  7%|â–‹         | 14/200 [2:06:08<28:18:54, 548.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19512.357060185186
INFO:root:current train perplexity6.879868984222412
INFO:root:current mean train loss 19474.111758735235
INFO:root:current train perplexity6.855226993560791
INFO:root:current mean train loss 19514.310426417953
INFO:root:current train perplexity6.845883369445801


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it]
INFO:root:final mean train loss: 19494.603306924142
INFO:root:final train perplexity: 6.840059280395508
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.80s/it]
INFO:root:eval mean loss: 23699.911458333332
INFO:root:eval perplexity: 11.621286392211914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/15

  8%|â–Š         | 15/200 [2:15:02<27:56:19, 543.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19324.348694620254
INFO:root:current train perplexity6.733754634857178
INFO:root:current mean train loss 19326.3992885824
INFO:root:current train perplexity6.725090503692627


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.76s/it]
INFO:root:final mean train loss: 19314.013762443297
INFO:root:final train perplexity: 6.719302654266357
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.97s/it]
INFO:root:eval mean loss: 23590.600725446428
INFO:root:eval perplexity: 11.490555763244629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/16

  8%|â–Š         | 16/200 [2:23:58<27:40:23, 541.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19107.05412046371
INFO:root:current train perplexity6.6465935707092285
INFO:root:current mean train loss 19141.172456464694
INFO:root:current train perplexity6.602563381195068
INFO:root:current mean train loss 19139.565180262445
INFO:root:current train perplexity6.6005024909973145


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.51s/it]
INFO:root:final mean train loss: 19136.61593923261
INFO:root:final train perplexity: 6.602756977081299
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.52s/it]
INFO:root:eval mean loss: 23477.699869791668
INFO:root:eval perplexity: 11.357073783874512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/17

  8%|â–Š         | 17/200 [2:32:53<27:25:19, 539.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18982.427428463856
INFO:root:current train perplexity6.500640869140625
INFO:root:current mean train loss 18997.03989497951
INFO:root:current train perplexity6.495722770690918


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.57s/it]
INFO:root:final mean train loss: 18980.733922158517
INFO:root:final train perplexity: 6.502015113830566
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.20s/it]
INFO:root:eval mean loss: 23418.931012834822
INFO:root:eval perplexity: 11.288206100463867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/18

  9%|â–‰         | 18/200 [2:41:47<27:11:32, 537.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18904.20853794643
INFO:root:current train perplexity6.44322395324707
INFO:root:current mean train loss 18900.329224537036
INFO:root:current train perplexity6.433026313781738
INFO:root:current mean train loss 18832.16408743351
INFO:root:current train perplexity6.403328895568848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.37s/it]
INFO:root:final mean train loss: 18832.80056467364
INFO:root:final train perplexity: 6.407833099365234
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.06s/it]
INFO:root:eval mean loss: 23335.18331473214
INFO:root:eval perplexity: 11.190786361694336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/19

 10%|â–‰         | 19/200 [2:50:58<27:14:40, 541.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18693.383530890806
INFO:root:current train perplexity6.305465221405029
INFO:root:current mean train loss 18728.540482954544
INFO:root:current train perplexity6.3227009773254395


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.96s/it]
INFO:root:final mean train loss: 18697.200967111894
INFO:root:final train perplexity: 6.322701930999756
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.31s/it]
INFO:root:eval mean loss: 23244.860537574405
INFO:root:eval perplexity: 11.086663246154785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/20

 10%|â–ˆ         | 20/200 [3:00:07<27:12:07, 544.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18715.24939903846
INFO:root:current train perplexity6.267210483551025
INFO:root:current mean train loss 18605.635524392987
INFO:root:current train perplexity6.25152063369751
INFO:root:current mean train loss 18592.643280923116
INFO:root:current train perplexity6.247016906738281


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.48s/it]
INFO:root:final mean train loss: 18570.232551820816
INFO:root:final train perplexity: 6.244016647338867
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.60s/it]
INFO:root:eval mean loss: 23185.954171316964
INFO:root:eval perplexity: 11.019277572631836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/21

 10%|â–ˆ         | 21/200 [3:09:39<27:27:47, 552.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18441.138178228022
INFO:root:current train perplexity6.167754173278809
INFO:root:current mean train loss 18462.402221040575
INFO:root:current train perplexity6.164127349853516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.71s/it]
INFO:root:final mean train loss: 18448.24779485887
INFO:root:final train perplexity: 6.169339179992676
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.15s/it]
INFO:root:eval mean loss: 23120.418317522322
INFO:root:eval perplexity: 10.944791793823242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/22

 11%|â–ˆ         | 22/200 [3:18:48<27:15:54, 551.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18375.265852107557
INFO:root:current train perplexity6.117154598236084
INFO:root:current mean train loss 18349.44828999126
INFO:root:current train perplexity6.103032112121582
INFO:root:current mean train loss 18358.850742669754
INFO:root:current train perplexity6.103307723999023


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.27s/it]
INFO:root:final mean train loss: 18336.29066122732
INFO:root:final train perplexity: 6.101589202880859
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:32<00:00, 92.15s/it]
INFO:root:eval mean loss: 23085.712727864582
INFO:root:eval perplexity: 10.90555191040039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/23

 12%|â–ˆâ–        | 23/200 [3:28:12<27:17:27, 555.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18290.05008223684
INFO:root:current train perplexity6.04367733001709
INFO:root:current mean train loss 18256.889282852564
INFO:root:current train perplexity6.038973331451416


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.11s/it]
INFO:root:final mean train loss: 18235.242730909777
INFO:root:final train perplexity: 6.041079044342041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.48s/it]
INFO:root:eval mean loss: 22999.197591145832
INFO:root:eval perplexity: 10.808338165283203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/24

 12%|â–ˆâ–        | 24/200 [3:37:20<27:02:04, 552.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18070.593334441488
INFO:root:current train perplexity5.9624505043029785
INFO:root:current mean train loss 18132.052986819726
INFO:root:current train perplexity5.979396820068359
INFO:root:current mean train loss 18143.431087107794
INFO:root:current train perplexity5.979393482208252


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.96s/it]
INFO:root:final mean train loss: 18130.12878811744
INFO:root:final train perplexity: 5.978771686553955
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 90.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:31<00:00, 91.01s/it]
INFO:root:eval mean loss: 23010.330031622023
INFO:root:eval perplexity: 10.820796966552734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/25

 12%|â–ˆâ–Ž        | 25/200 [3:46:52<27:09:39, 558.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18091.05074179293
INFO:root:current train perplexity5.943976879119873
INFO:root:current mean train loss 18070.778158369976
INFO:root:current train perplexity5.925686359405518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.47s/it]
INFO:root:final mean train loss: 18041.32180097026
INFO:root:final train perplexity: 5.926630973815918
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.34s/it]
INFO:root:eval mean loss: 22935.487351190477
INFO:root:eval perplexity: 10.737305641174316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/26

 13%|â–ˆâ–Ž        | 26/200 [3:56:02<26:53:01, 556.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17980.749042585783
INFO:root:current train perplexity5.9012131690979
INFO:root:current mean train loss 17986.776451262416
INFO:root:current train perplexity5.887249946594238


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.37s/it]
INFO:root:final mean train loss: 17952.586063508064
INFO:root:final train perplexity: 5.874985218048096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.67s/it]
INFO:root:eval mean loss: 22888.051688058036
INFO:root:eval perplexity: 10.684720039367676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/27

 14%|â–ˆâ–Ž        | 27/200 [4:05:10<26:36:24, 553.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18105.979817708332
INFO:root:current train perplexity5.864461898803711
INFO:root:current mean train loss 17894.257091929612
INFO:root:current train perplexity5.8373517990112305
INFO:root:current mean train loss 17892.145291333127
INFO:root:current train perplexity5.825695991516113


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.36s/it]
INFO:root:final mean train loss: 17865.726105720765
INFO:root:final train perplexity: 5.824869632720947
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it]
INFO:root:eval mean loss: 22853.225120907737
INFO:root:eval perplexity: 10.64627742767334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/28

 14%|â–ˆâ–        | 28/200 [4:14:26<26:29:26, 554.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17856.926633522726
INFO:root:current train perplexity5.803706645965576
INFO:root:current mean train loss 17823.918308971774
INFO:root:current train perplexity5.786777973175049


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.49s/it]
INFO:root:final mean train loss: 17783.143290858116
INFO:root:final train perplexity: 5.777616024017334
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.11s/it]
INFO:root:eval mean loss: 22818.672781808036
INFO:root:eval perplexity: 10.6082763671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/29

 14%|â–ˆâ–        | 29/200 [4:23:40<26:19:54, 554.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17803.85323660714
INFO:root:current train perplexity5.731008052825928
INFO:root:current mean train loss 17813.187007155375
INFO:root:current train perplexity5.741975784301758
INFO:root:current mean train loss 17752.063915307972
INFO:root:current train perplexity5.73563814163208


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.82s/it]
INFO:root:final mean train loss: 17705.66306624874
INFO:root:final train perplexity: 5.733630180358887
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.04s/it]
INFO:root:eval mean loss: 22770.83912295387
INFO:root:eval perplexity: 10.55588436126709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/30

 15%|â–ˆâ–Œ        | 30/200 [4:32:43<26:01:01, 550.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17650.677039194914
INFO:root:current train perplexity5.6942973136901855
INFO:root:current mean train loss 17650.802009630504
INFO:root:current train perplexity5.694138050079346


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.71s/it]
INFO:root:final mean train loss: 17630.048682428176
INFO:root:final train perplexity: 5.691028594970703
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it]
INFO:root:eval mean loss: 22734.491164434523
INFO:root:eval perplexity: 10.516250610351562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/31

 16%|â–ˆâ–Œ        | 31/200 [4:41:44<25:43:18, 547.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17499.517933238636
INFO:root:current train perplexity5.627148628234863
INFO:root:current mean train loss 17568.830676379504
INFO:root:current train perplexity5.636404037475586
INFO:root:current mean train loss 17573.180381738744
INFO:root:current train perplexity5.65477180480957


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.26s/it]
INFO:root:final mean train loss: 17559.697257749496
INFO:root:final train perplexity: 5.651674747467041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.42s/it]
INFO:root:eval mean loss: 22719.299641927082
INFO:root:eval perplexity: 10.499730110168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/32

 16%|â–ˆâ–Œ        | 32/200 [4:50:36<25:20:14, 542.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17490.718843005954
INFO:root:current train perplexity5.595323085784912
INFO:root:current mean train loss 17513.055622124233
INFO:root:current train perplexity5.616525173187256


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.67s/it]
INFO:root:final mean train loss: 17489.882095829133
INFO:root:final train perplexity: 5.6128926277160645
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.08s/it]
INFO:root:eval mean loss: 22699.71447172619
INFO:root:eval perplexity: 10.478470802307129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/33

 16%|â–ˆâ–‹        | 33/200 [4:59:36<25:08:56, 542.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17267.203776041668
INFO:root:current train perplexity5.527695178985596
INFO:root:current mean train loss 17419.19050611413
INFO:root:current train perplexity5.574011325836182
INFO:root:current mean train loss 17408.71703306686
INFO:root:current train perplexity5.565860748291016


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.36s/it]
INFO:root:final mean train loss: 17423.337835496473
INFO:root:final train perplexity: 5.576173305511475
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it]
INFO:root:eval mean loss: 22638.92264229911
INFO:root:eval perplexity: 10.412749290466309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/34

 17%|â–ˆâ–‹        | 34/200 [5:08:29<24:52:48, 539.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17350.675431436568
INFO:root:current train perplexity5.534625053405762
INFO:root:current mean train loss 17376.78011555015
INFO:root:current train perplexity5.539488792419434


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.37s/it]
INFO:root:final mean train loss: 17362.25386687248
INFO:root:final train perplexity: 5.5426788330078125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it]
INFO:root:eval mean loss: 22622.323428199405
INFO:root:eval perplexity: 10.394874572753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/35

 18%|â–ˆâ–Š        | 35/200 [5:17:29<24:43:26, 539.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17490.688939144737
INFO:root:current train perplexity5.56348991394043
INFO:root:current mean train loss 17353.12801995798
INFO:root:current train perplexity5.5258660316467285
INFO:root:current mean train loss 17322.53194563356
INFO:root:current train perplexity5.510751724243164


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.73s/it]
INFO:root:final mean train loss: 17305.523405997985
INFO:root:final train perplexity: 5.511751651763916
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it]
INFO:root:eval mean loss: 22625.534807477678
INFO:root:eval perplexity: 10.398332595825195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/36

 18%|â–ˆâ–Š        | 36/200 [5:26:37<24:41:28, 542.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17223.464967539614
INFO:root:current train perplexity5.466442108154297
INFO:root:current mean train loss 17243.195072642542
INFO:root:current train perplexity5.481117248535156


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.77s/it]
INFO:root:final mean train loss: 17248.325187436996
INFO:root:final train perplexity: 5.480743408203125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.96s/it]
INFO:root:eval mean loss: 22574.151994977678
INFO:root:eval perplexity: 10.343181610107422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/37

 18%|â–ˆâ–Š        | 37/200 [5:35:28<24:23:57, 538.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17118.363111413044
INFO:root:current train perplexity5.412288665771484
INFO:root:current mean train loss 17138.868783346035
INFO:root:current train perplexity5.438127517700195
INFO:root:current mean train loss 17212.040573325394
INFO:root:current train perplexity5.454166412353516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.34s/it]
INFO:root:final mean train loss: 17194.44779722152
INFO:root:final train perplexity: 5.451696395874023
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it]
INFO:root:eval mean loss: 22574.267647879464
INFO:root:eval perplexity: 10.343305587768555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/38

 19%|â–ˆâ–‰        | 38/200 [5:44:25<24:13:41, 538.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17153.611236979166
INFO:root:current train perplexity5.411815166473389
INFO:root:current mean train loss 17140.9187109375
INFO:root:current train perplexity5.415350914001465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.37s/it]
INFO:root:final mean train loss: 17138.176186838456
INFO:root:final train perplexity: 5.421523094177246
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.48s/it]
INFO:root:eval mean loss: 22548.426362537204
INFO:root:eval perplexity: 10.315680503845215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/39

 20%|â–ˆâ–‰        | 39/200 [5:53:36<24:14:27, 542.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16995.56268084491
INFO:root:current train perplexity5.351011753082275
INFO:root:current mean train loss 17090.412532295766
INFO:root:current train perplexity5.3932623863220215
INFO:root:current mean train loss 17091.88435693144
INFO:root:current train perplexity5.390510082244873


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.73s/it]
INFO:root:final mean train loss: 17083.985481508316
INFO:root:final train perplexity: 5.3926215171813965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it]
INFO:root:eval mean loss: 22554.312523251487
INFO:root:eval perplexity: 10.321964263916016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/40

 20%|â–ˆâ–ˆ        | 40/200 [6:02:35<24:03:06, 541.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17070.3447018394
INFO:root:current train perplexity5.371645927429199
INFO:root:current mean train loss 17088.369364306913
INFO:root:current train perplexity5.3797125816345215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.37s/it]
INFO:root:final mean train loss: 17035.951884608116
INFO:root:final train perplexity: 5.367133140563965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.58s/it]
INFO:root:eval mean loss: 22525.780668712796
INFO:root:eval perplexity: 10.29153060913086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/41

 20%|â–ˆâ–ˆ        | 41/200 [6:11:40<23:56:48, 542.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16903.93564138105
INFO:root:current train perplexity5.3062849044799805
INFO:root:current mean train loss 16962.178889849714
INFO:root:current train perplexity5.324796676635742
INFO:root:current mean train loss 17004.459593141233
INFO:root:current train perplexity5.339636325836182


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.10s/it]
INFO:root:final mean train loss: 16984.448588709678
INFO:root:final train perplexity: 5.339938640594482
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.26s/it]
INFO:root:eval mean loss: 22492.016531808036
INFO:root:eval perplexity: 10.255629539489746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/42

 21%|â–ˆâ–ˆ        | 42/200 [6:20:57<23:59:56, 546.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16931.08198418675
INFO:root:current train perplexity5.314149379730225
INFO:root:current mean train loss 16964.65223702186
INFO:root:current train perplexity5.3179521560668945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.41s/it]
INFO:root:final mean train loss: 16937.48250063004
INFO:root:final train perplexity: 5.3152594566345215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.62s/it]
INFO:root:eval mean loss: 22500.58421688988
INFO:root:eval perplexity: 10.264726638793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/43

 22%|â–ˆâ–ˆâ–       | 43/200 [6:30:14<23:58:30, 549.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17024.700167410716
INFO:root:current train perplexity5.329261302947998
INFO:root:current mean train loss 16942.474985532408
INFO:root:current train perplexity5.295804500579834
INFO:root:current mean train loss 16910.767777593086
INFO:root:current train perplexity5.293965816497803


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.22s/it]
INFO:root:final mean train loss: 16894.478933026712
INFO:root:final train perplexity: 5.29276180267334
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.97s/it]
INFO:root:eval mean loss: 22492.483468191964
INFO:root:eval perplexity: 10.256125450134277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:39:16<23:43:02, 547.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16839.978313577587
INFO:root:current train perplexity5.258185863494873
INFO:root:current mean train loss 16860.95051909258
INFO:root:current train perplexity5.2659125328063965


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.19s/it]
INFO:root:final mean train loss: 16852.377846994706
INFO:root:final train perplexity: 5.270829200744629
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.14s/it]
INFO:root:eval mean loss: 22473.270414806546
INFO:root:eval perplexity: 10.235750198364258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:48:27<23:37:15, 548.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16839.1519931891
INFO:root:current train perplexity5.255686283111572
INFO:root:current mean train loss 16809.702745616007
INFO:root:current train perplexity5.247186660766602
INFO:root:current mean train loss 16819.06064902589
INFO:root:current train perplexity5.246185302734375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.75s/it]
INFO:root:final mean train loss: 16803.079322076614
INFO:root:final train perplexity: 5.245262622833252
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.76s/it]
INFO:root:eval mean loss: 22449.265485491072
INFO:root:eval perplexity: 10.210352897644043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:57:31<23:24:20, 547.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16808.042754120877
INFO:root:current train perplexity5.224926471710205
INFO:root:current mean train loss 16795.147368905433
INFO:root:current train perplexity5.229092597961426


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.87s/it]
INFO:root:final mean train loss: 16761.17068186114
INFO:root:final train perplexity: 5.223625659942627
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.47s/it]
INFO:root:eval mean loss: 22445.509254092263
INFO:root:eval perplexity: 10.206385612487793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [7:06:42<23:17:55, 548.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16732.64294149709
INFO:root:current train perplexity5.204396724700928
INFO:root:current mean train loss 16739.395371230334
INFO:root:current train perplexity5.204799652099609
INFO:root:current mean train loss 16737.345445923354
INFO:root:current train perplexity5.206277847290039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.26s/it]
INFO:root:final mean train loss: 16726.716694493447
INFO:root:final train perplexity: 5.205904960632324
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.58s/it]
INFO:root:eval mean loss: 22439.549339657737
INFO:root:eval perplexity: 10.200089454650879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/48

 24%|â–ˆâ–ˆâ–       | 48/200 [7:15:41<23:01:51, 545.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16665.30894325658
INFO:root:current train perplexity5.166323184967041
INFO:root:current mean train loss 16679.455759214743
INFO:root:current train perplexity5.173228740692139


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.02s/it]
INFO:root:final mean train loss: 16682.992289881553
INFO:root:final train perplexity: 5.183501243591309
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.46s/it]
INFO:root:eval mean loss: 22412.157319568454
INFO:root:eval perplexity: 10.171215057373047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/49

 24%|â–ˆâ–ˆâ–       | 49/200 [7:24:37<22:45:38, 542.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16680.320748836435
INFO:root:current train perplexity5.147745132446289
INFO:root:current mean train loss 16658.972011851616
INFO:root:current train perplexity5.164080619812012
INFO:root:current mean train loss 16657.53856038082
INFO:root:current train perplexity5.164674282073975


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.91s/it]
INFO:root:final mean train loss: 16646.765676190775
INFO:root:final train perplexity: 5.165013790130615
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.42s/it]
INFO:root:eval mean loss: 22421.686569940477
INFO:root:eval perplexity: 10.181252479553223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [7:33:58<22:50:17, 548.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16626.407246291037
INFO:root:current train perplexity5.13364839553833
INFO:root:current mean train loss 16598.2747428549
INFO:root:current train perplexity5.144180774688721


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.54s/it]
INFO:root:final mean train loss: 16609.18587764617
INFO:root:final train perplexity: 5.145904541015625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.74s/it]
INFO:root:eval mean loss: 22410.197591145832
INFO:root:eval perplexity: 10.16915225982666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:43:11<22:45:00, 549.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16593.07586550245
INFO:root:current train perplexity5.127112865447998
INFO:root:current mean train loss 16590.915226614237
INFO:root:current train perplexity5.124802589416504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.69s/it]
INFO:root:final mean train loss: 16570.99888955393
INFO:root:final train perplexity: 5.126558303833008
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.31s/it]
INFO:root:eval mean loss: 22406.79813058036
INFO:root:eval perplexity: 10.165574073791504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:52:08<22:26:43, 545.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16668.686848958332
INFO:root:current train perplexity5.240872859954834
INFO:root:current mean train loss 16513.624800895024
INFO:root:current train perplexity5.096631050109863
INFO:root:current mean train loss 16549.505633274322
INFO:root:current train perplexity5.110012054443359


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.47s/it]
INFO:root:final mean train loss: 16539.384363974295
INFO:root:final train perplexity: 5.110598087310791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it]
INFO:root:eval mean loss: 22378.831194196428
INFO:root:eval perplexity: 10.13619613647461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [8:01:17<22:19:49, 546.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16504.258096590907
INFO:root:current train perplexity5.10396671295166
INFO:root:current mean train loss 16547.96214717742
INFO:root:current train perplexity5.106838226318359


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.63s/it]
INFO:root:final mean train loss: 16502.535912298386
INFO:root:final train perplexity: 5.092057704925537
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.55s/it]
INFO:root:eval mean loss: 22378.914039248513
INFO:root:eval perplexity: 10.13627815246582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [8:10:26<22:11:52, 547.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16357.486607142857
INFO:root:current train perplexity5.083932399749756
INFO:root:current mean train loss 16467.307827102803
INFO:root:current train perplexity5.077375888824463
INFO:root:current mean train loss 16471.2677857035
INFO:root:current train perplexity5.079245567321777


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.81s/it]
INFO:root:final mean train loss: 16469.580582157258
INFO:root:final train perplexity: 5.075533390045166
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.91s/it]
INFO:root:eval mean loss: 22365.531226748513
INFO:root:eval perplexity: 10.122251510620117
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [8:19:29<21:59:58, 546.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16466.820097325213
INFO:root:current train perplexity5.053247451782227
INFO:root:current mean train loss 16411.97589917453
INFO:root:current train perplexity5.0491042137146


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.63s/it]
INFO:root:final mean train loss: 16435.246747416833
INFO:root:final train perplexity: 5.058373928070068
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.21s/it]
INFO:root:eval mean loss: 22370.38927641369
INFO:root:eval perplexity: 10.127340316772461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [8:28:26<21:44:19, 543.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16546.150656960228
INFO:root:current train perplexity5.083930492401123
INFO:root:current mean train loss 16390.395824535473
INFO:root:current train perplexity5.034547805786133
INFO:root:current mean train loss 16427.39870131072
INFO:root:current train perplexity5.044223308563232


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.64s/it]
INFO:root:final mean train loss: 16398.20428269909
INFO:root:final train perplexity: 5.039927005767822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.69s/it]
INFO:root:eval mean loss: 22357.839913504464
INFO:root:eval perplexity: 10.11419677734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [8:37:46<21:46:32, 548.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16348.956240699405
INFO:root:current train perplexity5.005504131317139
INFO:root:current mean train loss 16371.907699865798
INFO:root:current train perplexity5.022890090942383


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.05s/it]
INFO:root:final mean train loss: 16371.04685531124
INFO:root:final train perplexity: 5.026444435119629
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.79s/it]
INFO:root:eval mean loss: 22351.078055245536
INFO:root:eval perplexity: 10.107121467590332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:46:45<21:30:58, 545.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16335.155989583332
INFO:root:current train perplexity5.015064239501953
INFO:root:current mean train loss 16286.50269191576
INFO:root:current train perplexity4.985682964324951
INFO:root:current mean train loss 16322.630677688954
INFO:root:current train perplexity5.0041937828063965


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.00s/it]
INFO:root:final mean train loss: 16339.442430065525
INFO:root:final train perplexity: 5.010800838470459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22348.024018787204
INFO:root:eval perplexity: 10.103926658630371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:55:42<21:15:50, 542.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16315.14211170709
INFO:root:current train perplexity4.977792739868164
INFO:root:current mean train loss 16312.959697791915
INFO:root:current train perplexity4.99647331237793


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.16s/it]
INFO:root:final mean train loss: 16308.245542464718
INFO:root:final train perplexity: 4.995406150817871
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.89s/it]
INFO:root:eval mean loss: 22348.427827380954
INFO:root:eval perplexity: 10.104348182678223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [9:04:34<20:59:26, 539.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16278.545590049342
INFO:root:current train perplexity4.989103317260742
INFO:root:current mean train loss 16260.27862394958
INFO:root:current train perplexity4.977865219116211
INFO:root:current mean train loss 16276.876578553081
INFO:root:current train perplexity4.977215766906738


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.56s/it]
INFO:root:final mean train loss: 16278.360942225303
INFO:root:final train perplexity: 4.980702877044678
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.53s/it]
INFO:root:eval mean loss: 22329.88504464286
INFO:root:eval perplexity: 10.084976196289062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [9:13:28<20:46:37, 538.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16275.299460827464
INFO:root:current train perplexity4.969049453735352
INFO:root:current mean train loss 16279.49259868421
INFO:root:current train perplexity4.970050811767578


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.56s/it]
INFO:root:final mean train loss: 16246.275276430191
INFO:root:final train perplexity: 4.964964866638184
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.45s/it]
INFO:root:eval mean loss: 22338.81568545387
INFO:root:eval perplexity: 10.094303131103516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [9:22:24<20:36:09, 537.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16154.88862941576
INFO:root:current train perplexity4.923593521118164
INFO:root:current mean train loss 16237.206173780487
INFO:root:current train perplexity4.946883678436279
INFO:root:current mean train loss 16236.97987317825
INFO:root:current train perplexity4.954931259155273


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.10s/it]
INFO:root:final mean train loss: 16220.595931514617
INFO:root:final train perplexity: 4.952406883239746
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it]
INFO:root:eval mean loss: 22325.257952008928
INFO:root:eval perplexity: 10.08014965057373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [9:31:22<20:27:26, 537.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16184.540924479166
INFO:root:current train perplexity4.941648960113525
INFO:root:current mean train loss 16217.03717075893
INFO:root:current train perplexity4.936387062072754


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.99s/it]
INFO:root:final mean train loss: 16195.29919827369
INFO:root:final train perplexity: 4.940064907073975
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.97s/it]
INFO:root:eval mean loss: 22321.770182291668
INFO:root:eval perplexity: 10.076509475708008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [9:40:42<20:33:40, 544.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16272.475622106482
INFO:root:current train perplexity4.936357498168945
INFO:root:current mean train loss 16149.86204324557
INFO:root:current train perplexity4.914816856384277
INFO:root:current mean train loss 16176.59711849532
INFO:root:current train perplexity4.925933837890625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.38s/it]
INFO:root:final mean train loss: 16169.539172757057
INFO:root:final train perplexity: 4.927529811859131
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.68s/it]
INFO:root:eval mean loss: 22322.05859375
INFO:root:eval perplexity: 10.076809883117676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [9:50:02<20:35:22, 549.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16157.04295638845
INFO:root:current train perplexity4.907163619995117
INFO:root:current mean train loss 16150.343111688198
INFO:root:current train perplexity4.910501956939697


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.65s/it]
INFO:root:final mean train loss: 16142.0027327999
INFO:root:final train perplexity: 4.914165019989014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.65s/it]
INFO:root:eval mean loss: 22314.72265625
INFO:root:eval perplexity: 10.069162368774414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:59:19<20:31:38, 551.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16064.054624495968
INFO:root:current train perplexity4.867913246154785
INFO:root:current mean train loss 16124.815094227099
INFO:root:current train perplexity4.89296817779541
INFO:root:current mean train loss 16124.770435944263
INFO:root:current train perplexity4.900254726409912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.07s/it]
INFO:root:final mean train loss: 16114.53224625126
INFO:root:final train perplexity: 4.9008684158325195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.20s/it]
INFO:root:eval mean loss: 22312.404157366072
INFO:root:eval perplexity: 10.066747665405273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [10:08:33<20:23:40, 552.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16089.243493505272
INFO:root:current train perplexity4.878981590270996
INFO:root:current mean train loss 16103.031954405738
INFO:root:current train perplexity4.883782386779785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.76s/it]
INFO:root:final mean train loss: 16088.689319241432
INFO:root:final train perplexity: 4.888391494750977
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.04s/it]
INFO:root:eval mean loss: 22315.982956659227
INFO:root:eval perplexity: 10.070476531982422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [10:17:44<20:13:46, 551.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16141.152790178572
INFO:root:current train perplexity4.893075466156006
INFO:root:current mean train loss 16084.424905960648
INFO:root:current train perplexity4.8748345375061035
INFO:root:current mean train loss 16082.172776761969
INFO:root:current train perplexity4.876491546630859


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.29s/it]
INFO:root:final mean train loss: 16065.376701108871
INFO:root:final train perplexity: 4.877164363861084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.93s/it]
INFO:root:eval mean loss: 22311.191987537204
INFO:root:eval perplexity: 10.065485954284668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [10:26:45<19:57:58, 548.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16036.607657596982
INFO:root:current train perplexity4.867600440979004
INFO:root:current mean train loss 16020.50237090575
INFO:root:current train perplexity4.861777305603027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.31s/it]
INFO:root:final mean train loss: 16039.629930065525
INFO:root:final train perplexity: 4.864795207977295
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.88s/it]
INFO:root:eval mean loss: 22317.820638020832
INFO:root:eval perplexity: 10.072389602661133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [10:35:44<19:42:10, 545.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15926.031475360576
INFO:root:current train perplexity4.84809684753418
INFO:root:current mean train loss 15980.549938174461
INFO:root:current train perplexity4.83898401260376
INFO:root:current mean train loss 16021.501131831852
INFO:root:current train perplexity4.849775314331055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.98s/it]
INFO:root:final mean train loss: 16015.408986737652
INFO:root:final train perplexity: 4.853187084197998
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.59s/it]
INFO:root:eval mean loss: 22290.13453311012
INFO:root:eval perplexity: 10.043570518493652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [10:44:34<19:23:24, 541.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15967.663686899039
INFO:root:current train perplexity4.826293468475342
INFO:root:current mean train loss 15984.732795116164
INFO:root:current train perplexity4.834925174713135


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.29s/it]
INFO:root:final mean train loss: 15991.216966198337
INFO:root:final train perplexity: 4.841619968414307
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.35s/it]
INFO:root:eval mean loss: 22299.840425037204
INFO:root:eval perplexity: 10.053664207458496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [10:53:23<19:06:20, 537.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15978.740347928779
INFO:root:current train perplexity4.810744285583496
INFO:root:current mean train loss 15977.975852272728
INFO:root:current train perplexity4.828559875488281
INFO:root:current mean train loss 15985.160996174125
INFO:root:current train perplexity4.832139492034912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.26s/it]
INFO:root:final mean train loss: 15971.006564232612
INFO:root:final train perplexity: 4.8319783210754395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it]
INFO:root:eval mean loss: 22299.61681547619
INFO:root:eval perplexity: 10.053434371948242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [11:02:14<18:53:39, 535.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15937.957175164474
INFO:root:current train perplexity4.812640190124512
INFO:root:current mean train loss 15947.9130859375
INFO:root:current train perplexity4.818214416503906


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.83s/it]
INFO:root:final mean train loss: 15943.521409557712
INFO:root:final train perplexity: 4.818897724151611
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.30s/it]
INFO:root:eval mean loss: 22299.498558407737
INFO:root:eval perplexity: 10.053311347961426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [11:11:23<18:52:42, 539.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15912.98618267952
INFO:root:current train perplexity4.793335914611816
INFO:root:current mean train loss 15915.386353369473
INFO:root:current train perplexity4.794060230255127
INFO:root:current mean train loss 15935.07646444838
INFO:root:current train perplexity4.808486461639404


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.67s/it]
INFO:root:final mean train loss: 15922.281163369456
INFO:root:final train perplexity: 4.808812618255615
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.47s/it]
INFO:root:eval mean loss: 22289.511555989582
INFO:root:eval perplexity: 10.042923927307129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [11:20:27<18:46:38, 540.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15902.938890861742
INFO:root:current train perplexity4.793315410614014
INFO:root:current mean train loss 15908.98678450848
INFO:root:current train perplexity4.796584129333496


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.11s/it]
INFO:root:final mean train loss: 15899.34328140751
INFO:root:final train perplexity: 4.797945022583008
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.39s/it]
INFO:root:eval mean loss: 22299.816476004464
INFO:root:eval perplexity: 10.053640365600586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [11:29:44<18:47:49, 545.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15868.130208333334
INFO:root:current train perplexity4.78608512878418
INFO:root:current mean train loss 15891.212825951987
INFO:root:current train perplexity4.784929275512695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.23s/it]
INFO:root:final mean train loss: 15872.755804246472
INFO:root:final train perplexity: 4.785379409790039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.99s/it]
INFO:root:eval mean loss: 22302.74962797619
INFO:root:eval perplexity: 10.056694030761719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [11:38:52<18:40:07, 546.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15413.632486979166
INFO:root:current train perplexity4.633621692657471
INFO:root:current mean train loss 15864.542314547936
INFO:root:current train perplexity4.770744323730469
INFO:root:current mean train loss 15856.270223983991
INFO:root:current train perplexity4.772773742675781


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.04s/it]
INFO:root:final mean train loss: 15856.574974798386
INFO:root:final train perplexity: 4.777748107910156
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it]
INFO:root:eval mean loss: 22292.229910714286
INFO:root:eval perplexity: 10.04575252532959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [11:48:04<18:34:19, 548.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15817.266956676136
INFO:root:current train perplexity4.775085926055908
INFO:root:current mean train loss 15831.43947202621
INFO:root:current train perplexity4.7656025886535645


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.92s/it]
INFO:root:final mean train loss: 15833.396511939263
INFO:root:final train perplexity: 4.766838073730469
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.47s/it]
INFO:root:eval mean loss: 22292.589401971727
INFO:root:eval perplexity: 10.046123504638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [11:57:26<18:33:46, 552.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15780.72572544643
INFO:root:current train perplexity4.738160133361816
INFO:root:current mean train loss 15846.258862076518
INFO:root:current train perplexity4.760433673858643
INFO:root:current mean train loss 15846.700638775665
INFO:root:current train perplexity4.765304088592529


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.10s/it]
INFO:root:final mean train loss: 15817.52019673009
INFO:root:final train perplexity: 4.759379863739014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.04s/it]
INFO:root:eval mean loss: 22289.198893229168
INFO:root:eval perplexity: 10.042598724365234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [12:06:28<18:18:37, 549.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15746.720074152543
INFO:root:current train perplexity4.7271528244018555
INFO:root:current mean train loss 15785.239853577044
INFO:root:current train perplexity4.74049186706543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.66s/it]
INFO:root:final mean train loss: 15790.816902406754
INFO:root:final train perplexity: 4.746860980987549
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.74s/it]
INFO:root:eval mean loss: 22300.77490234375
INFO:root:eval perplexity: 10.05463695526123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [12:15:29<18:04:13, 546.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15890.68155184659
INFO:root:current train perplexity4.824070453643799
INFO:root:current mean train loss 15807.860976210586
INFO:root:current train perplexity4.739324569702148
INFO:root:current mean train loss 15805.879706938686
INFO:root:current train perplexity4.740069389343262


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.68s/it]
INFO:root:final mean train loss: 15774.103657384072
INFO:root:final train perplexity: 4.73904275894165
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it]
INFO:root:eval mean loss: 22287.301455543155
INFO:root:eval perplexity: 10.040628433227539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [12:24:35<17:55:04, 546.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15758.738746279761
INFO:root:current train perplexity4.731410026550293
INFO:root:current mean train loss 15750.339490270322
INFO:root:current train perplexity4.732831954956055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.27s/it]
INFO:root:final mean train loss: 15749.751275831653
INFO:root:final train perplexity: 4.727672576904297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it]
INFO:root:eval mean loss: 22291.94893973214
INFO:root:eval perplexity: 10.045459747314453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [12:33:30<17:39:08, 543.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15739.556575520834
INFO:root:current train perplexity4.690448760986328
INFO:root:current mean train loss 15716.419989809783
INFO:root:current train perplexity4.714503288269043
INFO:root:current mean train loss 15746.01202761628
INFO:root:current train perplexity4.721012115478516


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.79s/it]
INFO:root:final mean train loss: 15732.99911794355
INFO:root:final train perplexity: 4.7198686599731445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.11s/it]
INFO:root:eval mean loss: 22308.570289248513
INFO:root:eval perplexity: 10.062752723693848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [12:42:27<17:26:23, 541.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15683.206623134329
INFO:root:current train perplexity4.70339822769165
INFO:root:current mean train loss 15720.66259473241
INFO:root:current train perplexity4.709335803985596


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.95s/it]
INFO:root:final mean train loss: 15714.164873676915
INFO:root:final train perplexity: 4.711108207702637
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.44s/it]
INFO:root:eval mean loss: 22287.640694754464
INFO:root:eval perplexity: 10.040980339050293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [12:51:16<17:09:59, 537.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15491.115902549342
INFO:root:current train perplexity4.695689678192139
INFO:root:current mean train loss 15717.456030068277
INFO:root:current train perplexity4.70200252532959
INFO:root:current mean train loss 15713.614824129567
INFO:root:current train perplexity4.703793048858643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.39s/it]
INFO:root:final mean train loss: 15699.415291078629
INFO:root:final train perplexity: 4.704259395599365
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.52s/it]
INFO:root:eval mean loss: 22293.934105282737
INFO:root:eval perplexity: 10.04752254486084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [13:00:32<17:11:39, 542.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15626.342154489437
INFO:root:current train perplexity4.694859981536865
INFO:root:current mean train loss 15675.021821317618
INFO:root:current train perplexity4.692344665527344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.72s/it]
INFO:root:final mean train loss: 15682.389317666331
INFO:root:final train perplexity: 4.696366310119629
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.79s/it]
INFO:root:eval mean loss: 22300.142554873513
INFO:root:eval perplexity: 10.053980827331543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [13:09:37<17:04:11, 543.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15688.09273097826
INFO:root:current train perplexity4.675093173980713
INFO:root:current mean train loss 15667.102253239329
INFO:root:current train perplexity4.683794021606445
INFO:root:current mean train loss 15671.766111091649
INFO:root:current train perplexity4.686310291290283


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.58s/it]
INFO:root:final mean train loss: 15660.582007623489
INFO:root:final train perplexity: 4.686275959014893
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.29s/it]
INFO:root:eval mean loss: 22288.51160249256
INFO:root:eval perplexity: 10.041885375976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [13:18:31<16:49:41, 540.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15653.327005208334
INFO:root:current train perplexity4.669981002807617
INFO:root:current mean train loss 15642.364921875
INFO:root:current train perplexity4.675594329833984


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.66s/it]
INFO:root:final mean train loss: 15647.2525359123
INFO:root:final train perplexity: 4.680119037628174
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it]
INFO:root:eval mean loss: 22278.955868675595
INFO:root:eval perplexity: 10.031961441040039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [13:27:36<16:42:45, 542.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15595.655635127316
INFO:root:current train perplexity4.658854007720947
INFO:root:current mean train loss 15585.36276605561
INFO:root:current train perplexity4.652322292327881
INFO:root:current mean train loss 15624.438782007159
INFO:root:current train perplexity4.665891647338867


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.00s/it]
INFO:root:final mean train loss: 15626.482941658267
INFO:root:final train perplexity: 4.670540809631348
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.63s/it]
INFO:root:eval mean loss: 22294.862327938987
INFO:root:eval perplexity: 10.048487663269043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [13:36:46<16:37:50, 544.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15662.330362440665
INFO:root:current train perplexity4.662554740905762
INFO:root:current mean train loss 15605.644727653631
INFO:root:current train perplexity4.657647132873535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.18s/it]
INFO:root:final mean train loss: 15604.637183404739
INFO:root:final train perplexity: 4.660488128662109
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.84s/it]
INFO:root:eval mean loss: 22272.743698846727
INFO:root:eval perplexity: 10.02550983428955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/91
##########################best##############
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [13:45:37<16:21:29, 540.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15608.281281502017
INFO:root:current train perplexity4.662115097045898
INFO:root:current mean train loss 15589.54126908397
INFO:root:current train perplexity4.657114505767822
INFO:root:current mean train loss 15605.83952245671
INFO:root:current train perplexity4.657721042633057


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.76s/it]
INFO:root:final mean train loss: 15596.38697470388
INFO:root:final train perplexity: 4.6566972732543945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:24<00:00, 84.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:24<00:00, 84.05s/it]
INFO:root:eval mean loss: 22299.273949032737
INFO:root:eval perplexity: 10.05307674407959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [13:54:34<16:11:07, 539.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15604.650708301959
INFO:root:current train perplexity4.653457164764404
INFO:root:current mean train loss 15584.641019894125
INFO:root:current train perplexity4.645826816558838


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.31s/it]
INFO:root:final mean train loss: 15573.321588331653
INFO:root:final train perplexity: 4.646115303039551
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it]
INFO:root:eval mean loss: 22294.45877511161
INFO:root:eval perplexity: 10.04806900024414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [14:03:44<16:07:46, 542.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15553.935100446428
INFO:root:current train perplexity4.612207889556885
INFO:root:current mean train loss 15540.420768229167
INFO:root:current train perplexity4.631534099578857
INFO:root:current mean train loss 15567.157646276595
INFO:root:current train perplexity4.637717247009277


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:01<00:00, 481.24s/it]
INFO:root:final mean train loss: 15557.70129000756
INFO:root:final train perplexity: 4.638962268829346
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.63s/it]
INFO:root:eval mean loss: 22309.061988467263
INFO:root:eval perplexity: 10.063265800476074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [14:13:03<16:07:13, 547.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15566.884496228447
INFO:root:current train perplexity4.622762680053711
INFO:root:current mean train loss 15562.028753760027
INFO:root:current train perplexity4.633648872375488


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.88s/it]
INFO:root:final mean train loss: 15539.92178049395
INFO:root:final train perplexity: 4.630834579467773
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it]
INFO:root:eval mean loss: 22297.83045014881
INFO:root:eval perplexity: 10.051573753356934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [14:22:05<15:55:21, 545.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15478.30158253205
INFO:root:current train perplexity4.582763671875
INFO:root:current mean train loss 15530.459181092627
INFO:root:current train perplexity4.609523296356201
INFO:root:current mean train loss 15547.619974176256
INFO:root:current train perplexity4.624244689941406


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.92s/it]
INFO:root:final mean train loss: 15525.334602602066
INFO:root:final train perplexity: 4.6241774559021
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it]
INFO:root:eval mean loss: 22302.881766183036
INFO:root:eval perplexity: 10.056831359863281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [14:31:04<15:42:36, 543.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15555.89467075893
INFO:root:current train perplexity4.624034881591797
INFO:root:current mean train loss 15512.190716009489
INFO:root:current train perplexity4.6178669929504395


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.96s/it]
INFO:root:final mean train loss: 15512.515069776966
INFO:root:final train perplexity: 4.61833381652832
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.83s/it]
INFO:root:eval mean loss: 22299.717122395832
INFO:root:eval perplexity: 10.053537368774414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [14:39:55<15:26:47, 539.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15483.588730922966
INFO:root:current train perplexity4.6131768226623535
INFO:root:current mean train loss 15505.896634615385
INFO:root:current train perplexity4.613151550292969
INFO:root:current mean train loss 15502.709450552984
INFO:root:current train perplexity4.610100269317627


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.41s/it]
INFO:root:final mean train loss: 15493.444536762852
INFO:root:final train perplexity: 4.609655380249023
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.61s/it]
INFO:root:eval mean loss: 22296.25462704613
INFO:root:eval perplexity: 10.049935340881348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [14:48:54<15:17:17, 539.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15527.905222039473
INFO:root:current train perplexity4.604300022125244
INFO:root:current mean train loss 15495.09430088141
INFO:root:current train perplexity4.603593349456787


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.28s/it]
INFO:root:final mean train loss: 15482.897799584174
INFO:root:final train perplexity: 4.604862689971924
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.62s/it]
INFO:root:eval mean loss: 22296.028250558036
INFO:root:eval perplexity: 10.049699783325195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [14:57:57<15:10:22, 540.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15404.272481715425
INFO:root:current train perplexity4.554741382598877
INFO:root:current mean train loss 15464.281442655187
INFO:root:current train perplexity4.587675094604492
INFO:root:current mean train loss 15479.482825151821
INFO:root:current train perplexity4.597771167755127


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.97s/it]
INFO:root:final mean train loss: 15468.898563508064
INFO:root:final train perplexity: 4.598508834838867
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.37s/it]
INFO:root:eval mean loss: 22304.26781063988
INFO:root:eval perplexity: 10.058274269104004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [15:06:50<14:57:02, 538.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15401.159534801136
INFO:root:current train perplexity4.572306156158447
INFO:root:current mean train loss 15470.08124607412
INFO:root:current train perplexity4.590019226074219


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.41s/it]
INFO:root:final mean train loss: 15451.839107390373
INFO:root:final train perplexity: 4.59077787399292
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.79s/it]
INFO:root:eval mean loss: 22292.075148809523
INFO:root:eval perplexity: 10.045589447021484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [15:16:05<14:56:25, 543.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15479.07732077206
INFO:root:current train perplexity4.574819564819336
INFO:root:current mean train loss 15459.661598458195
INFO:root:current train perplexity4.584323883056641


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.93s/it]
INFO:root:final mean train loss: 15437.3705109627
INFO:root:final train perplexity: 4.584230422973633
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it]
INFO:root:eval mean loss: 22301.04015531994
INFO:root:eval perplexity: 10.054915428161621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [15:25:04<14:45:31, 542.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15477.423502604166
INFO:root:current train perplexity4.5096564292907715
INFO:root:current mean train loss 15457.068169751214
INFO:root:current train perplexity4.571573734283447
INFO:root:current mean train loss 15422.681784944582
INFO:root:current train perplexity4.5726094245910645


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.71s/it]
INFO:root:final mean train loss: 15424.076408140121
INFO:root:final train perplexity: 4.578224182128906
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it]
INFO:root:eval mean loss: 22289.228678385418
INFO:root:eval perplexity: 10.042632102966309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [15:34:11<14:38:43, 543.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15383.777468039772
INFO:root:current train perplexity4.5447611808776855
INFO:root:current mean train loss 15409.929296875
INFO:root:current train perplexity4.571085453033447


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.40s/it]
INFO:root:final mean train loss: 15412.846549741684
INFO:root:final train perplexity: 4.573156356811523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.91s/it]
INFO:root:eval mean loss: 22302.256417410714
INFO:root:eval perplexity: 10.056180953979492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [15:43:15<14:30:05, 543.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15197.27915736607
INFO:root:current train perplexity4.500993251800537
INFO:root:current mean train loss 15320.643417786216
INFO:root:current train perplexity4.54910945892334
INFO:root:current mean train loss 15404.919596354166
INFO:root:current train perplexity4.567060470581055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.41s/it]
INFO:root:final mean train loss: 15396.339701990928
INFO:root:final train perplexity: 4.56571626663208
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it]
INFO:root:eval mean loss: 22305.538016183036
INFO:root:eval perplexity: 10.059597969055176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [15:52:19<14:21:01, 543.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15384.711384401484
INFO:root:current train perplexity4.557994365692139
INFO:root:current mean train loss 15421.092951552673
INFO:root:current train perplexity4.562806129455566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.01s/it]
INFO:root:final mean train loss: 15379.462756741432
INFO:root:final train perplexity: 4.558122634887695
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.30s/it]
INFO:root:eval mean loss: 22307.092261904763
INFO:root:eval perplexity: 10.061217308044434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [16:01:29<14:14:35, 545.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15111.25790127841
INFO:root:current train perplexity4.5155253410339355
INFO:root:current mean train loss 15341.290100647522
INFO:root:current train perplexity4.53997802734375
INFO:root:current mean train loss 15377.641684871149
INFO:root:current train perplexity4.551232814788818


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.28s/it]
INFO:root:final mean train loss: 15367.0406218498
INFO:root:final train perplexity: 4.552541255950928
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.66s/it]
INFO:root:eval mean loss: 22315.258719308036
INFO:root:eval perplexity: 10.069722175598145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [16:10:24<14:00:48, 542.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15323.136098710318
INFO:root:current train perplexity4.538340091705322
INFO:root:current mean train loss 15360.972949817868
INFO:root:current train perplexity4.545765399932861


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.32s/it]
INFO:root:final mean train loss: 15356.875752110634
INFO:root:final train perplexity: 4.54797887802124
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.83s/it]
INFO:root:eval mean loss: 22315.698056175595
INFO:root:eval perplexity: 10.070180892944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [16:19:13<13:45:42, 538.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15080.629166666668
INFO:root:current train perplexity4.514883995056152
INFO:root:current mean train loss 15309.328549592392
INFO:root:current train perplexity4.526274681091309
INFO:root:current mean train loss 15337.472043059593
INFO:root:current train perplexity4.533506870269775


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.39s/it]
INFO:root:final mean train loss: 15336.400965536794
INFO:root:final train perplexity: 4.538803577423096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.43s/it]
INFO:root:eval mean loss: 22317.555199032737
INFO:root:eval perplexity: 10.072115898132324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [16:28:02<13:32:13, 535.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15252.381165461753
INFO:root:current train perplexity4.514420032501221
INFO:root:current mean train loss 15319.170196715942
INFO:root:current train perplexity4.53322172164917


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.99s/it]
INFO:root:final mean train loss: 15328.282151745212
INFO:root:final train perplexity: 4.535171031951904
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.64s/it]
INFO:root:eval mean loss: 22314.422944568454
INFO:root:eval perplexity: 10.06885051727295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [16:36:51<13:20:30, 533.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15297.968133223685
INFO:root:current train perplexity4.505646228790283
INFO:root:current mean train loss 15359.480083048844
INFO:root:current train perplexity4.52838659286499
INFO:root:current mean train loss 15318.330952126142
INFO:root:current train perplexity4.525777816772461


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.90s/it]
INFO:root:final mean train loss: 15312.008072391633
INFO:root:final train perplexity: 4.527896881103516
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.60s/it]
INFO:root:eval mean loss: 22310.538992745536
INFO:root:eval perplexity: 10.064804077148438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [16:45:56<13:16:19, 536.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15323.728763204226
INFO:root:current train perplexity4.524158477783203
INFO:root:current mean train loss 15314.621619152047
INFO:root:current train perplexity4.524972438812256


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.13s/it]
INFO:root:final mean train loss: 15299.886742376511
INFO:root:final train perplexity: 4.522487163543701
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.86s/it]
INFO:root:eval mean loss: 22320.187965029763
INFO:root:eval perplexity: 10.074858665466309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [16:54:59<13:10:28, 538.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15231.970023777174
INFO:root:current train perplexity4.501591205596924
INFO:root:current mean train loss 15271.628628366361
INFO:root:current train perplexity4.5148396492004395
INFO:root:current mean train loss 15292.181605591368
INFO:root:current train perplexity4.517945289611816


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.85s/it]
INFO:root:final mean train loss: 15287.301234091481
INFO:root:final train perplexity: 4.516876220703125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.46s/it]
INFO:root:eval mean loss: 22310.817964099704
INFO:root:eval perplexity: 10.065096855163574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [17:04:04<13:03:44, 540.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15260.266796875
INFO:root:current train perplexity4.506511688232422
INFO:root:current mean train loss 15288.528978794642
INFO:root:current train perplexity4.5146331787109375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.47s/it]
INFO:root:final mean train loss: 15274.48960827243
INFO:root:final train perplexity: 4.511172771453857
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it]
INFO:root:eval mean loss: 22317.54761904762
INFO:root:eval perplexity: 10.07210922241211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [17:13:06<12:55:27, 541.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15234.474428530093
INFO:root:current train perplexity4.5245866775512695
INFO:root:current mean train loss 15256.321765809547
INFO:root:current train perplexity4.507270336151123
INFO:root:current mean train loss 15270.51786205947
INFO:root:current train perplexity4.503793716430664


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.09s/it]
INFO:root:final mean train loss: 15263.97044717112
INFO:root:final train perplexity: 4.506494522094727
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it]
INFO:root:eval mean loss: 22304.205543154763
INFO:root:eval perplexity: 10.058209419250488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [17:21:59<12:43:01, 538.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15229.331784018987
INFO:root:current train perplexity4.492641448974609
INFO:root:current mean train loss 15267.057458973464
INFO:root:current train perplexity4.49730110168457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.98s/it]
INFO:root:final mean train loss: 15251.24265609249
INFO:root:final train perplexity: 4.500840187072754
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.51s/it]
INFO:root:eval mean loss: 22317.17517671131
INFO:root:eval perplexity: 10.071720123291016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [17:30:50<12:30:59, 536.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15295.237556703629
INFO:root:current train perplexity4.503688812255859
INFO:root:current mean train loss 15268.278581226145
INFO:root:current train perplexity4.495673179626465
INFO:root:current mean train loss 15252.99349803842
INFO:root:current train perplexity4.495988845825195


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.49s/it]
INFO:root:final mean train loss: 15236.689799647178
INFO:root:final train perplexity: 4.494384288787842
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 75.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 75.00s/it]
INFO:root:eval mean loss: 22330.288876488095
INFO:root:eval perplexity: 10.085399627685547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [17:39:55<12:25:45, 539.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15252.322583301959
INFO:root:current train perplexity4.490157604217529
INFO:root:current mean train loss 15261.942756360997
INFO:root:current train perplexity4.4942545890808105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.57s/it]
INFO:root:final mean train loss: 15232.717233965473
INFO:root:final train perplexity: 4.492623805999756
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it]
INFO:root:eval mean loss: 22330.75732421875
INFO:root:eval perplexity: 10.085887908935547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [17:49:11<12:23:26, 543.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15178.926088169643
INFO:root:current train perplexity4.462443828582764
INFO:root:current mean train loss 15232.950202546297
INFO:root:current train perplexity4.477748394012451
INFO:root:current mean train loss 15233.422107712766
INFO:root:current train perplexity4.488394737243652


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.40s/it]
INFO:root:final mean train loss: 15223.233410250756
INFO:root:final train perplexity: 4.4884233474731445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.28s/it]
INFO:root:eval mean loss: 22333.93336123512
INFO:root:eval perplexity: 10.089204788208008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [17:58:25<12:18:42, 547.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15153.371991738506
INFO:root:current train perplexity4.4733052253723145
INFO:root:current mean train loss 15211.323007185829
INFO:root:current train perplexity4.47982120513916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.87s/it]
INFO:root:final mean train loss: 15207.476771200856
INFO:root:final train perplexity: 4.481453895568848
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.43s/it]
INFO:root:eval mean loss: 22339.683500744046
INFO:root:eval perplexity: 10.095208168029785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [18:07:33<12:09:35, 547.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15167.768504607371
INFO:root:current train perplexity4.462471961975098
INFO:root:current mean train loss 15178.548884330035
INFO:root:current train perplexity4.47302770614624
INFO:root:current mean train loss 15206.52604439069
INFO:root:current train perplexity4.476478576660156


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.33s/it]
INFO:root:final mean train loss: 15196.316803962955
INFO:root:final train perplexity: 4.476522922515869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it]
INFO:root:eval mean loss: 22327.638834635418
INFO:root:eval perplexity: 10.082632064819336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [18:16:28<11:55:47, 543.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15160.376963856455
INFO:root:current train perplexity4.462102890014648
INFO:root:current mean train loss 15180.86042825589
INFO:root:current train perplexity4.462441444396973


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.92s/it]
INFO:root:final mean train loss: 15189.111265120968
INFO:root:final train perplexity: 4.473343372344971
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.01s/it]
INFO:root:eval mean loss: 22327.84640066964
INFO:root:eval perplexity: 10.08284854888916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [18:25:37<11:48:46, 545.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15169.247365552326
INFO:root:current train perplexity4.485569953918457
INFO:root:current mean train loss 15203.521347792832
INFO:root:current train perplexity4.473050594329834
INFO:root:current mean train loss 15190.938163097993
INFO:root:current train perplexity4.469050407409668


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.82s/it]
INFO:root:final mean train loss: 15178.03771578881
INFO:root:final train perplexity: 4.468459606170654
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.82s/it]
INFO:root:eval mean loss: 22344.153134300595
INFO:root:eval perplexity: 10.09988021850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [18:34:31<11:35:18, 541.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15154.54021381579
INFO:root:current train perplexity4.463262557983398
INFO:root:current mean train loss 15188.403265224359
INFO:root:current train perplexity4.46323823928833


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.55s/it]
INFO:root:final mean train loss: 15169.768279044858
INFO:root:final train perplexity: 4.464816570281982
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it]
INFO:root:eval mean loss: 22338.870698474704
INFO:root:eval perplexity: 10.0943603515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [18:43:48<11:32:11, 546.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15154.530959109043
INFO:root:current train perplexity4.447507858276367
INFO:root:current mean train loss 15173.658143335459
INFO:root:current train perplexity4.457058429718018
INFO:root:current mean train loss 15170.889233299595
INFO:root:current train perplexity4.459572792053223


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.09s/it]
INFO:root:final mean train loss: 15157.141810263356
INFO:root:final train perplexity: 4.459259986877441
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.33s/it]
INFO:root:eval mean loss: 22336.486793154763
INFO:root:eval perplexity: 10.091870307922363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [18:52:46<11:19:58, 543.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15145.042554450758
INFO:root:current train perplexity4.453442096710205
INFO:root:current mean train loss 15156.77806022299
INFO:root:current train perplexity4.452579498291016


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.90s/it]
INFO:root:final mean train loss: 15148.246755292339
INFO:root:final train perplexity: 4.455349445343018
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it]
INFO:root:eval mean loss: 22335.734793526786
INFO:root:eval perplexity: 10.091085433959961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [19:01:47<11:09:45, 543.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15128.399031096813
INFO:root:current train perplexity4.439608097076416
INFO:root:current mean train loss 15119.766588627897
INFO:root:current train perplexity4.442032337188721


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.66s/it]
INFO:root:final mean train loss: 15133.14777201991
INFO:root:final train perplexity: 4.448719024658203
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.04s/it]
INFO:root:eval mean loss: 22339.875534784227
INFO:root:eval perplexity: 10.095410346984863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [19:10:50<11:00:37, 542.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14966.421549479166
INFO:root:current train perplexity4.4571075439453125
INFO:root:current mean train loss 15128.14955628034
INFO:root:current train perplexity4.448938369750977
INFO:root:current mean train loss 15125.475629233375
INFO:root:current train perplexity4.4418792724609375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.97s/it]
INFO:root:final mean train loss: 15127.096750567036
INFO:root:final train perplexity: 4.446064472198486
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.59s/it]
INFO:root:eval mean loss: 22337.79415457589
INFO:root:eval perplexity: 10.093236923217773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [19:19:48<10:49:53, 541.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15145.007333096592
INFO:root:current train perplexity4.446614742279053
INFO:root:current mean train loss 15125.625756048386
INFO:root:current train perplexity4.444121837615967


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.10s/it]
INFO:root:final mean train loss: 15116.558050340222
INFO:root:final train perplexity: 4.441445827484131
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.05s/it]
INFO:root:eval mean loss: 22337.145042782737
INFO:root:eval perplexity: 10.092557907104492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [19:29:02<10:45:14, 545.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15167.815848214286
INFO:root:current train perplexity4.406924247741699
INFO:root:current mean train loss 15129.147643472546
INFO:root:current train perplexity4.445130825042725
INFO:root:current mean train loss 15115.190137190519
INFO:root:current train perplexity4.436913967132568


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.64s/it]
INFO:root:final mean train loss: 15103.94054782006
INFO:root:final train perplexity: 4.435921669006348
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.99s/it]
INFO:root:eval mean loss: 22371.393647693454
INFO:root:eval perplexity: 10.128393173217773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [19:38:13<10:37:55, 546.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15133.56398967161
INFO:root:current train perplexity4.441342830657959
INFO:root:current mean train loss 15083.750884433963
INFO:root:current train perplexity4.430531978607178


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.59s/it]
INFO:root:final mean train loss: 15093.576037991432
INFO:root:final train perplexity: 4.431388854980469
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it]
INFO:root:eval mean loss: 22344.175688244046
INFO:root:eval perplexity: 10.099905014038086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [19:47:19<10:28:52, 546.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14996.32333096591
INFO:root:current train perplexity4.445781230926514
INFO:root:current mean train loss 15096.177716779279
INFO:root:current train perplexity4.426130771636963
INFO:root:current mean train loss 15093.120191239632
INFO:root:current train perplexity4.42845344543457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.61s/it]
INFO:root:final mean train loss: 15091.047237273186
INFO:root:final train perplexity: 4.43028450012207
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.58s/it]
INFO:root:eval mean loss: 22350.652808779763
INFO:root:eval perplexity: 10.10667610168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [19:56:21<10:18:04, 545.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14986.24916294643
INFO:root:current train perplexity4.415174961090088
INFO:root:current mean train loss 15048.8820875671
INFO:root:current train perplexity4.417086601257324


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.91s/it]
INFO:root:final mean train loss: 15079.31876890121
INFO:root:final train perplexity: 4.425162315368652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.82s/it]
INFO:root:eval mean loss: 22357.43970889137
INFO:root:eval perplexity: 10.113777160644531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [20:05:21<10:07:00, 543.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15038.824153645834
INFO:root:current train perplexity4.431277751922607
INFO:root:current mean train loss 15084.032880434783
INFO:root:current train perplexity4.427154541015625
INFO:root:current mean train loss 15080.23488372093
INFO:root:current train perplexity4.419961929321289


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.34s/it]
INFO:root:final mean train loss: 15072.400032289566
INFO:root:final train perplexity: 4.422143459320068
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.09s/it]
INFO:root:eval mean loss: 22358.443219866072
INFO:root:eval perplexity: 10.114828109741211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [20:14:21<9:56:50, 542.59s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15029.04541744403
INFO:root:current train perplexity4.404351711273193
INFO:root:current mean train loss 15100.646308944612
INFO:root:current train perplexity4.425621032714844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.23s/it]
INFO:root:final mean train loss: 15066.283841040826
INFO:root:final train perplexity: 4.419476509094238
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.38s/it]
INFO:root:eval mean loss: 22360.107863653273
INFO:root:eval perplexity: 10.116572380065918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [20:23:44<9:54:33, 548.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15114.653063322368
INFO:root:current train perplexity4.420708179473877
INFO:root:current mean train loss 15094.01249015231
INFO:root:current train perplexity4.426665306091309
INFO:root:current mean train loss 15088.785116117295
INFO:root:current train perplexity4.41782808303833


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.76s/it]
INFO:root:final mean train loss: 15055.148079164566
INFO:root:final train perplexity: 4.414624214172363
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it]
INFO:root:eval mean loss: 22370.44796316964
INFO:root:eval perplexity: 10.127403259277344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [20:32:53<9:45:24, 548.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15037.33535981514
INFO:root:current train perplexity4.403181552886963
INFO:root:current mean train loss 15074.402635005483
INFO:root:current train perplexity4.411630630493164


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.21s/it]
INFO:root:final mean train loss: 15046.658002299648
INFO:root:final train perplexity: 4.410930156707764
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.85s/it]
INFO:root:eval mean loss: 22362.694684709822
INFO:root:eval perplexity: 10.119280815124512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [20:42:04<9:36:45, 549.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14946.272376019022
INFO:root:current train perplexity4.379756927490234
INFO:root:current mean train loss 15037.495506224594
INFO:root:current train perplexity4.408942222595215
INFO:root:current mean train loss 15044.13883390555
INFO:root:current train perplexity4.407716751098633


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.34s/it]
INFO:root:final mean train loss: 15038.666196761593
INFO:root:final train perplexity: 4.407453536987305
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.34s/it]
INFO:root:eval mean loss: 22364.921712239582
INFO:root:eval perplexity: 10.121611595153809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [20:51:12<9:27:21, 549.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14987.929479166667
INFO:root:current train perplexity4.3889055252075195
INFO:root:current mean train loss 15028.527287946428
INFO:root:current train perplexity4.394031524658203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.73s/it]
INFO:root:final mean train loss: 15029.314291677167
INFO:root:final train perplexity: 4.403390884399414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.36s/it]
INFO:root:eval mean loss: 22353.116722470237
INFO:root:eval perplexity: 10.109254837036133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [21:00:04<9:13:02, 543.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15029.854383680555
INFO:root:current train perplexity4.400936603546143
INFO:root:current mean train loss 14995.1943359375
INFO:root:current train perplexity4.391469955444336
INFO:root:current mean train loss 15032.659627099394
INFO:root:current train perplexity4.39842414855957


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.74s/it]
INFO:root:final mean train loss: 15021.334618353074
INFO:root:final train perplexity: 4.39992618560791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it]
INFO:root:eval mean loss: 22362.04615420387
INFO:root:eval perplexity: 10.118600845336914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [21:09:21<9:07:51, 547.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14972.827247329906
INFO:root:current train perplexity4.392702579498291
INFO:root:current mean train loss 15003.66235487954
INFO:root:current train perplexity4.392810821533203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.66s/it]
INFO:root:final mean train loss: 15014.76591639365
INFO:root:final train perplexity: 4.3970770835876465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it]
INFO:root:eval mean loss: 22358.0751953125
INFO:root:eval perplexity: 10.114444732666016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [21:18:40<9:02:03, 551.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14992.359627016129
INFO:root:current train perplexity4.396886825561523
INFO:root:current mean train loss 15022.515341722328
INFO:root:current train perplexity4.399517059326172
INFO:root:current mean train loss 15015.533896442099
INFO:root:current train perplexity4.394782543182373


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.62s/it]
INFO:root:final mean train loss: 15004.643779139366
INFO:root:final train perplexity: 4.392688751220703
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it]
INFO:root:eval mean loss: 22374.96216982887
INFO:root:eval perplexity: 10.132134437561035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [21:27:55<8:53:53, 552.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15063.154614551959
INFO:root:current train perplexity4.382622241973877
INFO:root:current mean train loss 15019.250325520834
INFO:root:current train perplexity4.385371208190918


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.28s/it]
INFO:root:final mean train loss: 14998.370203818044
INFO:root:final train perplexity: 4.3899712562561035
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.35s/it]
INFO:root:eval mean loss: 22362.073660714286
INFO:root:eval perplexity: 10.118629455566406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [21:37:01<8:42:45, 550.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14943.926590401787
INFO:root:current train perplexity4.358327865600586
INFO:root:current mean train loss 14984.916116898148
INFO:root:current train perplexity4.376633167266846
INFO:root:current mean train loss 14995.287242353723
INFO:root:current train perplexity4.387711048126221


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.48s/it]
INFO:root:final mean train loss: 14993.65226499496
INFO:root:final train perplexity: 4.387928485870361
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.93s/it]
INFO:root:eval mean loss: 22370.64122953869
INFO:root:eval perplexity: 10.127603530883789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [21:46:14<8:34:28, 551.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14963.10983521911
INFO:root:current train perplexity4.372175216674805
INFO:root:current mean train loss 15003.015076662767
INFO:root:current train perplexity4.385034561157227


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:18<00:00, 498.11s/it]
INFO:root:final mean train loss: 14982.22621204007
INFO:root:final train perplexity: 4.382986545562744
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.57s/it]
INFO:root:eval mean loss: 22366.629441034227
INFO:root:eval perplexity: 10.12340259552002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [21:55:52<8:32:34, 559.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14999.310897435897
INFO:root:current train perplexity4.3780059814453125
INFO:root:current mean train loss 14984.11301427608
INFO:root:current train perplexity4.381003379821777
INFO:root:current mean train loss 14987.201094240325
INFO:root:current train perplexity4.380408763885498


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.64s/it]
INFO:root:final mean train loss: 14977.249909431705
INFO:root:final train perplexity: 4.380836486816406
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it]
INFO:root:eval mean loss: 22370.142252604168
INFO:root:eval perplexity: 10.127081871032715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [22:05:18<8:25:13, 561.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15016.586742359203
INFO:root:current train perplexity4.370611667633057
INFO:root:current mean train loss 14970.293260184882
INFO:root:current train perplexity4.366734027862549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.11s/it]
INFO:root:final mean train loss: 14967.672898815525
INFO:root:final train perplexity: 4.376699924468994
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.54s/it]
INFO:root:eval mean loss: 22375.693894159227
INFO:root:eval perplexity: 10.132905006408691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [22:14:23<8:11:22, 556.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14965.48828125
INFO:root:current train perplexity4.375785827636719
INFO:root:current mean train loss 14960.72312745848
INFO:root:current train perplexity4.368128776550293
INFO:root:current mean train loss 14981.09924366641
INFO:root:current train perplexity4.375797748565674


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.37s/it]
INFO:root:final mean train loss: 14966.038743542087
INFO:root:final train perplexity: 4.375994682312012
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.57s/it]
INFO:root:eval mean loss: 22375.434547061013
INFO:root:eval perplexity: 10.1326322555542
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [22:23:29<7:59:37, 553.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14973.426120476974
INFO:root:current train perplexity4.370762825012207
INFO:root:current mean train loss 14959.841561498397
INFO:root:current train perplexity4.3700480461120605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.86s/it]
INFO:root:final mean train loss: 14958.620696037045
INFO:root:final train perplexity: 4.372794151306152
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.26s/it]
INFO:root:eval mean loss: 22374.530598958332
INFO:root:eval perplexity: 10.131682395935059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [22:32:37<7:49:00, 551.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14928.461934840425
INFO:root:current train perplexity4.353789329528809
INFO:root:current mean train loss 14927.703018707483
INFO:root:current train perplexity4.35821008682251
INFO:root:current mean train loss 14960.64477637905
INFO:root:current train perplexity4.369062423706055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.62s/it]
INFO:root:final mean train loss: 14949.767278855847
INFO:root:final train perplexity: 4.368977069854736
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it]
INFO:root:eval mean loss: 22375.62669735863
INFO:root:eval perplexity: 10.132832527160645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [22:41:41<7:37:45, 549.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14929.16703164457
INFO:root:current train perplexity4.355865001678467
INFO:root:current mean train loss 14921.345492108983
INFO:root:current train perplexity4.361086368560791


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.06s/it]
INFO:root:final mean train loss: 14940.525233114919
INFO:root:final train perplexity: 4.364995956420898
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.15s/it]
INFO:root:eval mean loss: 22389.522065662204
INFO:root:eval perplexity: 10.147415161132812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [22:50:42<7:26:32, 546.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14921.945063572304
INFO:root:current train perplexity4.360385417938232
INFO:root:current mean train loss 14922.29437862169
INFO:root:current train perplexity4.357609748840332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.87s/it]
INFO:root:final mean train loss: 14935.774437689011
INFO:root:final train perplexity: 4.362951755523682
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.13s/it]
INFO:root:eval mean loss: 22385.10423642113
INFO:root:eval perplexity: 10.142775535583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [22:59:52<7:18:09, 547.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15013.771809895834
INFO:root:current train perplexity4.375577926635742
INFO:root:current mean train loss 14915.212217460557
INFO:root:current train perplexity4.350767612457275
INFO:root:current mean train loss 14939.665154518165
INFO:root:current train perplexity4.359550476074219


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.59s/it]
INFO:root:final mean train loss: 14928.239836662045
INFO:root:final train perplexity: 4.359710693359375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.36s/it]
INFO:root:eval mean loss: 22385.289504278273
INFO:root:eval perplexity: 10.142969131469727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [23:08:59<7:09:03, 547.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14883.055859375
INFO:root:current train perplexity4.335280895233154
INFO:root:current mean train loss 14912.472914566531
INFO:root:current train perplexity4.350213527679443


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.30s/it]
INFO:root:final mean train loss: 14923.64814610635
INFO:root:final train perplexity: 4.357736587524414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.27s/it]
INFO:root:eval mean loss: 22390.07296316964
INFO:root:eval perplexity: 10.147993087768555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [23:18:07<6:59:56, 547.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15012.393973214286
INFO:root:current train perplexity4.402821063995361
INFO:root:current mean train loss 14890.565831264603
INFO:root:current train perplexity4.351437568664551
INFO:root:current mean train loss 14935.616555329107
INFO:root:current train perplexity4.35732889175415


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.92s/it]
INFO:root:final mean train loss: 14918.470364478326
INFO:root:final train perplexity: 4.3555121421813965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.95s/it]
INFO:root:eval mean loss: 22395.218726748513
INFO:root:eval perplexity: 10.153400421142578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [23:27:07<6:49:00, 545.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14957.078869835805
INFO:root:current train perplexity4.357487678527832
INFO:root:current mean train loss 14907.321061812107
INFO:root:current train perplexity4.3501057624816895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.36s/it]
INFO:root:final mean train loss: 14915.206554782006
INFO:root:final train perplexity: 4.354109764099121
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.71s/it]
INFO:root:eval mean loss: 22390.527157738095
INFO:root:eval perplexity: 10.148469924926758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [23:36:21<6:41:53, 548.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14877.215110085228
INFO:root:current train perplexity4.29955530166626
INFO:root:current mean train loss 14908.680206573761
INFO:root:current train perplexity4.341558933258057
INFO:root:current mean train loss 14912.911850192535
INFO:root:current train perplexity4.346184730529785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.65s/it]
INFO:root:final mean train loss: 14906.730708952873
INFO:root:final train perplexity: 4.350471019744873
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.34s/it]
INFO:root:eval mean loss: 22394.746907552082
INFO:root:eval perplexity: 10.15290355682373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [23:45:22<6:31:14, 545.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14838.127914186507
INFO:root:current train perplexity4.337767124176025
INFO:root:current mean train loss 14914.298115174463
INFO:root:current train perplexity4.3508806228637695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.15s/it]
INFO:root:final mean train loss: 14901.753043882309
INFO:root:final train perplexity: 4.348335266113281
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it]
INFO:root:eval mean loss: 22391.583519345237
INFO:root:eval perplexity: 10.149580955505371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [23:54:25<6:21:24, 544.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14959.268033854167
INFO:root:current train perplexity4.340693473815918
INFO:root:current mean train loss 14913.63355129076
INFO:root:current train perplexity4.3528594970703125
INFO:root:current mean train loss 14912.835519622093
INFO:root:current train perplexity4.348511695861816


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:08<00:00, 488.07s/it]
INFO:root:final mean train loss: 14896.87083779612
INFO:root:final train perplexity: 4.346242427825928
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.27s/it]
INFO:root:eval mean loss: 22393.256905691964
INFO:root:eval perplexity: 10.151337623596191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [24:03:52<6:16:59, 551.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14862.382681319963
INFO:root:current train perplexity4.324831962585449
INFO:root:current mean train loss 14873.833563342067
INFO:root:current train perplexity4.333364486694336


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.14s/it]
INFO:root:final mean train loss: 14886.44046906502
INFO:root:final train perplexity: 4.34177303314209
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.68s/it]
INFO:root:eval mean loss: 22395.818777901786
INFO:root:eval perplexity: 10.154029846191406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [24:13:04<6:07:50, 551.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14855.816252055922
INFO:root:current train perplexity4.316170692443848
INFO:root:current mean train loss 14902.801142331933
INFO:root:current train perplexity4.3410725593566895
INFO:root:current mean train loss 14887.520316067352
INFO:root:current train perplexity4.342124938964844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.62s/it]
INFO:root:final mean train loss: 14885.694343813004
INFO:root:final train perplexity: 4.34145450592041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it]
INFO:root:eval mean loss: 22396.023390997023
INFO:root:eval perplexity: 10.154245376586914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [24:22:21<5:59:38, 553.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14893.474334286971
INFO:root:current train perplexity4.329061508178711
INFO:root:current mean train loss 14884.025236430922
INFO:root:current train perplexity4.331544876098633


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.47s/it]
INFO:root:final mean train loss: 14879.20929545741
INFO:root:final train perplexity: 4.338677406311035
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.10s/it]
INFO:root:eval mean loss: 22389.253603980655
INFO:root:eval perplexity: 10.147136688232422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [24:31:37<5:50:54, 554.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14877.369140625
INFO:root:current train perplexity4.350614070892334
INFO:root:current mean train loss 14922.582412347561
INFO:root:current train perplexity4.341602325439453
INFO:root:current mean train loss 14885.549318595851
INFO:root:current train perplexity4.334217071533203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.30s/it]
INFO:root:final mean train loss: 14873.03857421875
INFO:root:final train perplexity: 4.336038112640381
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.81s/it]
INFO:root:eval mean loss: 22392.46117001488
INFO:root:eval perplexity: 10.150500297546387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [24:40:46<5:40:43, 552.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14853.679401041667
INFO:root:current train perplexity4.342623233795166
INFO:root:current mean train loss 14865.186925223214
INFO:root:current train perplexity4.3341851234436035


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.74s/it]
INFO:root:final mean train loss: 14869.018763388356
INFO:root:final train perplexity: 4.334319114685059
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.80s/it]
INFO:root:eval mean loss: 22397.888625372023
INFO:root:eval perplexity: 10.156206130981445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [24:49:52<5:30:24, 550.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14919.412109375
INFO:root:current train perplexity4.3306884765625
INFO:root:current mean train loss 14905.484828678642
INFO:root:current train perplexity4.347561836242676
INFO:root:current mean train loss 14883.406697411894
INFO:root:current train perplexity4.3359055519104


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.42s/it]
INFO:root:final mean train loss: 14865.13019389491
INFO:root:final train perplexity: 4.3326568603515625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.43s/it]
INFO:root:eval mean loss: 22400.436616443454
INFO:root:eval perplexity: 10.15888500213623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [24:58:58<5:20:25, 549.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14896.823959157437
INFO:root:current train perplexity4.339117527008057
INFO:root:current mean train loss 14885.072827557611
INFO:root:current train perplexity4.333618640899658


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it]
INFO:root:final mean train loss: 14857.144932900706
INFO:root:final train perplexity: 4.329246520996094
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.57s/it]
INFO:root:eval mean loss: 22403.54471261161
INFO:root:eval perplexity: 10.162152290344238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [25:07:52<5:08:36, 544.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14872.791204637097
INFO:root:current train perplexity4.32328462600708
INFO:root:current mean train loss 14884.642361939408
INFO:root:current train perplexity4.327284336090088
INFO:root:current mean train loss 14873.986091382576
INFO:root:current train perplexity4.331277847290039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.41s/it]
INFO:root:final mean train loss: 14859.930620747227
INFO:root:final train perplexity: 4.330435276031494
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.62s/it]
INFO:root:eval mean loss: 22400.889090401786
INFO:root:eval perplexity: 10.159361839294434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [25:16:58<4:59:44, 544.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14848.13827183735
INFO:root:current train perplexity4.321047782897949
INFO:root:current mean train loss 14878.343259050547
INFO:root:current train perplexity4.328285217285156


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.86s/it]
INFO:root:final mean train loss: 14855.46279611895
INFO:root:final train perplexity: 4.328527450561523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.32s/it]
INFO:root:eval mean loss: 22399.21898251488
INFO:root:eval perplexity: 10.15760326385498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [25:25:58<4:49:52, 543.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15004.464369419642
INFO:root:current train perplexity4.363953590393066
INFO:root:current mean train loss 14844.922649016204
INFO:root:current train perplexity4.319355010986328
INFO:root:current mean train loss 14868.202414394947
INFO:root:current train perplexity4.327901363372803


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.99s/it]
INFO:root:final mean train loss: 14850.155005670364
INFO:root:final train perplexity: 4.3262619972229
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.46s/it]
INFO:root:eval mean loss: 22402.9501953125
INFO:root:eval perplexity: 10.161527633666992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [25:35:09<4:42:00, 545.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14837.126167385057
INFO:root:current train perplexity4.32184362411499
INFO:root:current mean train loss 14837.09822547627
INFO:root:current train perplexity4.319779396057129


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.87s/it]
INFO:root:final mean train loss: 14838.830637285786
INFO:root:final train perplexity: 4.321433067321777
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.54s/it]
INFO:root:eval mean loss: 22411.90550595238
INFO:root:eval perplexity: 10.170950889587402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [25:44:14<4:32:49, 545.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14757.712364783654
INFO:root:current train perplexity4.287820339202881
INFO:root:current mean train loss 14812.717604822392
INFO:root:current train perplexity4.317438125610352
INFO:root:current mean train loss 14848.20525382453
INFO:root:current train perplexity4.323276996612549

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.06s/it]
INFO:root:final mean train loss: 14839.694528887348
INFO:root:final train perplexity: 4.32180118560791
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.89s/it]
INFO:root:eval mean loss: 22407.15015811012
INFO:root:eval perplexity: 10.165945053100586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [25:53:11<4:22:21, 542.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14880.120922046703
INFO:root:current train perplexity4.338221549987793
INFO:root:current mean train loss 14834.335170566099
INFO:root:current train perplexity4.320090293884277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.43s/it]
INFO:root:final mean train loss: 14835.644098097278
INFO:root:final train perplexity: 4.320075035095215
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.30s/it]
INFO:root:eval mean loss: 22403.34900483631
INFO:root:eval perplexity: 10.161946296691895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [26:02:03<4:11:55, 539.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14797.303597383721
INFO:root:current train perplexity4.310769081115723
INFO:root:current mean train loss 14855.207393192744
INFO:root:current train perplexity4.322162628173828
INFO:root:current mean train loss 14845.704583815586
INFO:root:current train perplexity4.318168640136719

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.88s/it]
INFO:root:final mean train loss: 14830.902564264114
INFO:root:final train perplexity: 4.318054676055908
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.57s/it]
INFO:root:eval mean loss: 22410.556803385418
INFO:root:eval perplexity: 10.169529914855957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [26:11:18<4:04:52, 544.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.525812088816
INFO:root:current train perplexity4.319655418395996
INFO:root:current mean train loss 14851.085892427885
INFO:root:current train perplexity4.31816291809082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.78s/it]
INFO:root:final mean train loss: 14830.894862021169
INFO:root:final train perplexity: 4.318051338195801
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.89s/it]
INFO:root:eval mean loss: 22408.341471354168
INFO:root:eval perplexity: 10.167195320129395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [26:20:44<3:58:42, 550.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14760.853016954787
INFO:root:current train perplexity4.296539306640625
INFO:root:current mean train loss 14816.697378560799
INFO:root:current train perplexity4.306057453155518
INFO:root:current mean train loss 14838.264379586286
INFO:root:current train perplexity4.316463470458984

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.08s/it]
INFO:root:final mean train loss: 14826.104590631301
INFO:root:final train perplexity: 4.316012382507324
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it]
INFO:root:eval mean loss: 22404.23707217262
INFO:root:eval perplexity: 10.162881851196289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [26:29:51<3:49:04, 549.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14820.02025134154
INFO:root:current train perplexity4.3094048500061035
INFO:root:current mean train loss 14824.393206265704
INFO:root:current train perplexity4.310573101043701

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.01s/it]
INFO:root:final mean train loss: 14822.093249905494
INFO:root:final train perplexity: 4.314304351806641
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.27s/it]
INFO:root:eval mean loss: 22405.25646391369
INFO:root:eval perplexity: 10.163954734802246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [26:39:14<3:41:26, 553.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14832.276290594362
INFO:root:current train perplexity4.299404144287109
INFO:root:current mean train loss 14820.782090749171
INFO:root:current train perplexity4.306535720825195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.90s/it]
INFO:root:final mean train loss: 14819.343297158519
INFO:root:final train perplexity: 4.313134670257568
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.13s/it]
INFO:root:eval mean loss: 22412.584495907737
INFO:root:eval perplexity: 10.171664237976074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [26:48:18<3:31:05, 550.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14791.78125
INFO:root:current train perplexity4.272772312164307
INFO:root:current mean train loss 14810.75727207221
INFO:root:current train perplexity4.3087568283081055
INFO:root:current mean train loss 14835.165443157328
INFO:root:current train perplexity4.310657978057861

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.62s/it]
INFO:root:final mean train loss: 14814.954680412045
INFO:root:final train perplexity: 4.311268329620361
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it]
INFO:root:eval mean loss: 22416.512137276786
INFO:root:eval perplexity: 10.175799369812012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [26:57:30<3:22:06, 551.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14867.565926846592
INFO:root:current train perplexity4.300283432006836
INFO:root:current mean train loss 14846.320394405242
INFO:root:current train perplexity4.310946464538574

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.53s/it]
INFO:root:final mean train loss: 14812.414030997983
INFO:root:final train perplexity: 4.310187816619873
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it]
INFO:root:eval mean loss: 22415.633835565477
INFO:root:eval perplexity: 10.174875259399414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [27:06:32<3:11:56, 548.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14729.389090401786
INFO:root:current train perplexity4.2889790534973145
INFO:root:current mean train loss 14792.865663332359
INFO:root:current train perplexity4.308192253112793
INFO:root:current mean train loss 14821.890186254528
INFO:root:current train perplexity4.309408187866211

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.08s/it]
INFO:root:final mean train loss: 14812.814055412045
INFO:root:final train perplexity: 4.310357570648193
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it]
INFO:root:eval mean loss: 22408.793108258928
INFO:root:eval perplexity: 10.167673110961914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [27:15:33<3:02:00, 546.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14781.887860831568
INFO:root:current train perplexity4.30546760559082
INFO:root:current mean train loss 14791.158669909591
INFO:root:current train perplexity4.298967361450195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.49s/it]
INFO:root:final mean train loss: 14805.241963048134
INFO:root:final train perplexity: 4.3071393966674805
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.92s/it]
INFO:root:eval mean loss: 22416.507789248513
INFO:root:eval perplexity: 10.17579460144043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [27:24:20<2:51:10, 540.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14877.122247869318
INFO:root:current train perplexity4.28166389465332
INFO:root:current mean train loss 14827.556684614301
INFO:root:current train perplexity4.307920932769775
INFO:root:current mean train loss 14812.88897271179
INFO:root:current train perplexity4.305198669433594

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it]
INFO:root:final mean train loss: 14797.201593214466
INFO:root:final train perplexity: 4.303725242614746
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.94s/it]
INFO:root:eval mean loss: 22417.684384300595
INFO:root:eval perplexity: 10.177034378051758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [27:33:12<2:41:23, 537.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14751.412155877977
INFO:root:current train perplexity4.294750213623047
INFO:root:current mean train loss 14794.245207055215
INFO:root:current train perplexity4.297769069671631

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.26s/it]
INFO:root:final mean train loss: 14801.31404359879
INFO:root:final train perplexity: 4.305471420288086
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.12s/it]
INFO:root:eval mean loss: 22415.826078869046
INFO:root:eval perplexity: 10.175079345703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [27:42:17<2:32:57, 539.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14778.555403645832
INFO:root:current train perplexity4.319026947021484
INFO:root:current mean train loss 14815.013739809783
INFO:root:current train perplexity4.309788703918457
INFO:root:current mean train loss 14804.891855922966
INFO:root:current train perplexity4.302313327789307

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.53s/it]
INFO:root:final mean train loss: 14793.164428710938
INFO:root:final train perplexity: 4.3020124435424805
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.75s/it]
INFO:root:eval mean loss: 22412.321730840773
INFO:root:eval perplexity: 10.171387672424316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [27:51:18<2:24:03, 540.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14830.697076142724
INFO:root:current train perplexity4.29742431640625
INFO:root:current mean train loss 14797.592369947604
INFO:root:current train perplexity4.296517848968506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.22s/it]
INFO:root:final mean train loss: 14790.25148453251
INFO:root:final train perplexity: 4.300776481628418
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it]
INFO:root:eval mean loss: 22416.32996186756
INFO:root:eval perplexity: 10.175607681274414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [28:00:28<2:15:49, 543.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14736.79240337171
INFO:root:current train perplexity4.2813825607299805
INFO:root:current mean train loss 14794.77404477416
INFO:root:current train perplexity4.304804801940918
INFO:root:current mean train loss 14817.723449985731
INFO:root:current train perplexity4.306014537811279

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.48s/it]
INFO:root:final mean train loss: 14795.238395444809
INFO:root:final train perplexity: 4.302892208099365
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it]
INFO:root:eval mean loss: 22414.627418154763
INFO:root:eval perplexity: 10.173815727233887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [28:09:41<2:07:28, 546.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14814.820463798416
INFO:root:current train perplexity4.299198627471924
INFO:root:current mean train loss 14821.76080500731
INFO:root:current train perplexity4.302396774291992

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.59s/it]
INFO:root:final mean train loss: 14790.160132623489
INFO:root:final train perplexity: 4.300737380981445
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.50s/it]
INFO:root:eval mean loss: 22415.876999627977
INFO:root:eval perplexity: 10.175132751464844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [28:18:48<1:58:22, 546.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14849.81840183424
INFO:root:current train perplexity4.290532112121582
INFO:root:current mean train loss 14783.62449186992
INFO:root:current train perplexity4.294711589813232
INFO:root:current mean train loss 14803.447839300728
INFO:root:current train perplexity4.301553726196289

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.31s/it]
INFO:root:final mean train loss: 14787.747019121723
INFO:root:final train perplexity: 4.2997145652771
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.04s/it]
INFO:root:eval mean loss: 22414.034621465773
INFO:root:eval perplexity: 10.173192024230957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [28:27:49<1:48:58, 544.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14811.735677083334
INFO:root:current train perplexity4.303934574127197
INFO:root:current mean train loss 14779.423883928572
INFO:root:current train perplexity4.295161247253418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.11s/it]
INFO:root:final mean train loss: 14785.511513986896
INFO:root:final train perplexity: 4.298766613006592
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.54s/it]
INFO:root:eval mean loss: 22414.566941034227
INFO:root:eval perplexity: 10.173752784729004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [28:36:46<1:39:27, 542.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14738.129195601852
INFO:root:current train perplexity4.285374164581299
INFO:root:current mean train loss 14773.156872846948
INFO:root:current train perplexity4.2975263595581055
INFO:root:current mean train loss 14790.562370938876
INFO:root:current train perplexity4.297824382781982

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.09s/it]
INFO:root:final mean train loss: 14788.498755670364
INFO:root:final train perplexity: 4.300033092498779
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it]
INFO:root:eval mean loss: 22417.06470889137
INFO:root:eval perplexity: 10.176379203796387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [28:45:51<1:30:32, 543.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14852.51405508307
INFO:root:current train perplexity4.304431438446045
INFO:root:current mean train loss 14787.902758379889
INFO:root:current train perplexity4.294729709625244

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.82s/it]
INFO:root:final mean train loss: 14781.508474042339
INFO:root:final train perplexity: 4.297069072723389
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22416.367001488095
INFO:root:eval perplexity: 10.175649642944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [28:54:49<1:21:13, 541.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14902.137978830646
INFO:root:current train perplexity4.306004524230957
INFO:root:current mean train loss 14837.004621898855
INFO:root:current train perplexity4.303008079528809
INFO:root:current mean train loss 14792.660870704816
INFO:root:current train perplexity4.2981133460998535

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.17s/it]
INFO:root:final mean train loss: 14786.762203093498
INFO:root:final train perplexity: 4.2992963790893555
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.12s/it]
INFO:root:eval mean loss: 22415.934151785714
INFO:root:eval perplexity: 10.175188064575195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [29:03:36<1:11:37, 537.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14776.345397213856
INFO:root:current train perplexity4.2960710525512695
INFO:root:current mean train loss 14781.809965206625
INFO:root:current train perplexity4.2957563400268555

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.26s/it]
INFO:root:final mean train loss: 14782.318627142136
INFO:root:final train perplexity: 4.297412395477295
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.96s/it]
INFO:root:eval mean loss: 22418.228608630954
INFO:root:eval perplexity: 10.177607536315918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [29:12:25<1:02:23, 534.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14785.030636160714
INFO:root:current train perplexity4.316092014312744
INFO:root:current mean train loss 14788.455092592592
INFO:root:current train perplexity4.2943949699401855
INFO:root:current mean train loss 14793.433543882978
INFO:root:current train perplexity4.294643878936768

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.83s/it]
INFO:root:final mean train loss: 14777.284033990676
INFO:root:final train perplexity: 4.295279026031494
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.69s/it]
INFO:root:eval mean loss: 22415.784202938987
INFO:root:eval perplexity: 10.175033569335938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [29:21:12<53:15, 532.54s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14741.488169001437
INFO:root:current train perplexity4.286689758300781
INFO:root:current mean train loss 14776.252203793449
INFO:root:current train perplexity4.293943405151367

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.97s/it]
INFO:root:final mean train loss: 14777.691213300152
INFO:root:final train perplexity: 4.295451641082764
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.06s/it]
INFO:root:eval mean loss: 22417.168805803572
INFO:root:eval perplexity: 10.176490783691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [29:29:58<44:12, 530.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14846.1220703125
INFO:root:current train perplexity4.310125350952148
INFO:root:current mean train loss 14780.799165355216
INFO:root:current train perplexity4.2937517166137695
INFO:root:current mean train loss 14785.704469305701
INFO:root:current train perplexity4.294067859649658

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.41s/it]
INFO:root:final mean train loss: 14774.742750598538
INFO:root:final train perplexity: 4.2942023277282715
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.56s/it]
INFO:root:eval mean loss: 22418.919363839286
INFO:root:eval perplexity: 10.178333282470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [29:38:42<35:14, 528.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14806.832063444368
INFO:root:current train perplexity4.296634674072266
INFO:root:current mean train loss 14793.516381708116
INFO:root:current train perplexity4.296616077423096

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.48s/it]
INFO:root:final mean train loss: 14769.662432270665
INFO:root:final train perplexity: 4.292051315307617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.96s/it]
INFO:root:eval mean loss: 22416.677873883928
INFO:root:eval perplexity: 10.175971984863281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [29:47:29<26:24, 528.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14791.796148255815
INFO:root:current train perplexity4.291382789611816
INFO:root:current mean train loss 14784.338901333042
INFO:root:current train perplexity4.288336277008057
INFO:root:current mean train loss 14790.013760288066
INFO:root:current train perplexity4.295758247375488

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.84s/it]
INFO:root:final mean train loss: 14777.842718308972
INFO:root:final train perplexity: 4.295515537261963
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.35s/it]
INFO:root:eval mean loss: 22418.629045758928
INFO:root:eval perplexity: 10.178030014038086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [29:56:27<17:42, 531.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14811.7591796875
INFO:root:current train perplexity4.306147575378418
INFO:root:current mean train loss 14794.90749198718
INFO:root:current train perplexity4.297558307647705

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.60s/it]
INFO:root:final mean train loss: 14777.465198147682
INFO:root:final train perplexity: 4.295355319976807
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22417.917387462796
INFO:root:eval perplexity: 10.177282333374023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [30:05:43<08:58, 538.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14754.217420212766
INFO:root:current train perplexity4.280872821807861
INFO:root:current mean train loss 14770.701052295919
INFO:root:current train perplexity4.291936874389648
INFO:root:current mean train loss 14783.788244085274
INFO:root:current train perplexity4.293222904205322

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.48s/it]
INFO:root:final mean train loss: 14772.24529438634
INFO:root:final train perplexity: 4.293145179748535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.82s/it]
INFO:root:eval mean loss: 22418.11462983631
INFO:root:eval perplexity: 10.177486419677734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_multiqa/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [30:14:55<00:00, 542.62s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [30:14:55<00:00, 544.48s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.88s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.88s/it]
INFO:root:eval mean loss: 22418.11462983631
INFO:root:eval perplexity: 10.177486419677734
INFO:root:evalaution complete
INFO:root:save model final: small_multiqa_multiqa/final
