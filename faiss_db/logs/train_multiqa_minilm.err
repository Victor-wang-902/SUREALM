INFO:root:Output: small_multiqa_minilm
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at nreimers/MiniLM-L6-H384-uncased and are newly initialized: ['encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'cls.predictions.decoder.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 95213.8616635101
INFO:root:current train perplexity11729.7939453125
INFO:root:current mean train loss 79430.42101130653
INFO:root:current train perplexity2502.092041015625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.99s/it]
INFO:root:final mean train loss: 73338.04167716733
INFO:root:final train perplexity: 1385.054443359375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.91s/it]
INFO:root:eval mean loss: 43763.777064732145
INFO:root:eval perplexity: 92.69941711425781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/1

  0%|          | 1/200 [09:07<30:14:28, 547.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42623.91253063725
INFO:root:current train perplexity67.62958526611328
INFO:root:current mean train loss 39051.1572071606
INFO:root:current train perplexity46.96989440917969


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.23s/it]
INFO:root:final mean train loss: 36568.66324344758
INFO:root:final train perplexity: 36.84974670410156
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.63s/it]
INFO:root:eval mean loss: 31967.344401041668
INFO:root:eval perplexity: 27.343597412109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/2

  1%|          | 2/200 [18:13<30:03:57, 546.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31505.680338541668
INFO:root:current train perplexity22.706979751586914
INFO:root:current mean train loss 29965.196298543688
INFO:root:current train perplexity19.164793014526367
INFO:root:current mean train loss 29055.969827586207
INFO:root:current train perplexity17.52263832092285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.57s/it]
INFO:root:final mean train loss: 28669.084157636087
INFO:root:final train perplexity: 16.906455993652344
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.58s/it]
INFO:root:eval mean loss: 28685.642531622023
INFO:root:eval perplexity: 19.469329833984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/3

  2%|â–         | 3/200 [26:58<29:22:52, 536.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26597.382848011363
INFO:root:current train perplexity13.705296516418457
INFO:root:current mean train loss 26096.965284778227
INFO:root:current train perplexity13.086387634277344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.06s/it]
INFO:root:final mean train loss: 25714.474412487398
INFO:root:final train perplexity: 12.632537841796875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.51s/it]
INFO:root:eval mean loss: 27183.486281622023
INFO:root:eval perplexity: 16.666051864624023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/4

  2%|â–         | 4/200 [35:45<29:00:45, 532.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 25016.132254464286
INFO:root:current train perplexity11.468982696533203
INFO:root:current mean train loss 24524.18092873832
INFO:root:current train perplexity11.177635192871094
INFO:root:current mean train loss 24232.455247961956
INFO:root:current train perplexity10.906204223632812


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.03s/it]
INFO:root:final mean train loss: 24114.618825604837
INFO:root:final train perplexity: 10.78847599029541
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.89s/it]
INFO:root:eval mean loss: 26334.47326078869
INFO:root:eval perplexity: 15.264118194580078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/5

  2%|â–Ž         | 5/200 [44:47<29:02:30, 536.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23452.883904925846
INFO:root:current train perplexity10.100326538085938
INFO:root:current mean train loss 23210.023830581762
INFO:root:current train perplexity9.852375030517578


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.56s/it]
INFO:root:final mean train loss: 23056.36320249496
INFO:root:final train perplexity: 9.719175338745117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.75s/it]
INFO:root:eval mean loss: 25739.220796130954
INFO:root:eval perplexity: 14.352131843566895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/6

  3%|â–Ž         | 6/200 [54:13<29:26:10, 546.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22420.203125
INFO:root:current train perplexity9.181840896606445
INFO:root:current mean train loss 22460.990463119368
INFO:root:current train perplexity9.156789779663086
INFO:root:current mean train loss 22332.230718675946
INFO:root:current train perplexity9.038222312927246


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.68s/it]
INFO:root:final mean train loss: 22281.608941847277
INFO:root:final train perplexity: 9.004144668579102
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it]
INFO:root:eval mean loss: 25271.801246279763
INFO:root:eval perplexity: 13.67436408996582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/7

  4%|â–Ž         | 7/200 [1:03:18<29:15:46, 545.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21903.315569196428
INFO:root:current train perplexity8.660676002502441
INFO:root:current mean train loss 21806.06200872316
INFO:root:current train perplexity8.562506675720215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.52s/it]
INFO:root:final mean train loss: 21682.411164314515
INFO:root:final train perplexity: 8.48741626739502
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.97s/it]
INFO:root:eval mean loss: 24943.302827380954
INFO:root:eval perplexity: 13.217270851135254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/8

  4%|â–         | 8/200 [1:12:26<29:08:45, 546.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21464.377994791666
INFO:root:current train perplexity8.230072021484375
INFO:root:current mean train loss 21357.42214673913
INFO:root:current train perplexity8.188031196594238
INFO:root:current mean train loss 21240.1214934593
INFO:root:current train perplexity8.112286567687988


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.08s/it]
INFO:root:final mean train loss: 21200.930648311492
INFO:root:final train perplexity: 8.093772888183594
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.06s/it]
INFO:root:eval mean loss: 24622.398856026786
INFO:root:eval perplexity: 12.785506248474121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/9

  4%|â–         | 9/200 [1:21:34<29:01:10, 546.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20970.713123833953
INFO:root:current train perplexity7.8729376792907715
INFO:root:current mean train loss 20896.034267402694
INFO:root:current train perplexity7.832547664642334


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.84s/it]
INFO:root:final mean train loss: 20800.30275800151
INFO:root:final train perplexity: 7.780185699462891
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.39s/it]
INFO:root:eval mean loss: 24375.03294735863
INFO:root:eval perplexity: 12.462333679199219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/10

  5%|â–Œ         | 10/200 [1:30:54<29:05:25, 551.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20772.164987664473
INFO:root:current train perplexity7.626293182373047
INFO:root:current mean train loss 20577.61247702206
INFO:root:current train perplexity7.564032077789307
INFO:root:current mean train loss 20485.65304830194
INFO:root:current train perplexity7.530490875244141


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.80s/it]
INFO:root:final mean train loss: 20458.466548796623
INFO:root:final train perplexity: 7.522243976593018
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.32s/it]
INFO:root:eval mean loss: 24161.301920572918
INFO:root:eval perplexity: 12.189690589904785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/11

  6%|â–Œ         | 11/200 [1:39:57<28:47:41, 548.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20279.33868838028
INFO:root:current train perplexity7.342843055725098
INFO:root:current mean train loss 20212.639402869154
INFO:root:current train perplexity7.327766418457031


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.95s/it]
INFO:root:final mean train loss: 20161.535002677672
INFO:root:final train perplexity: 7.305133819580078
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.21s/it]
INFO:root:eval mean loss: 23980.08375186012
INFO:root:eval perplexity: 11.963201522827148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/12

  6%|â–Œ         | 12/200 [1:49:00<28:34:07, 547.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20020.357421875
INFO:root:current train perplexity7.174983024597168
INFO:root:current mean train loss 19934.78245680894
INFO:root:current train perplexity7.148501396179199
INFO:root:current mean train loss 19909.038142867154
INFO:root:current train perplexity7.122191429138184


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.45s/it]
INFO:root:final mean train loss: 19905.587748865928
INFO:root:final train perplexity: 7.123026371002197
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.37s/it]
INFO:root:eval mean loss: 23827.885323660714
INFO:root:eval perplexity: 11.776235580444336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/13

  6%|â–‹         | 13/200 [1:58:02<28:19:56, 545.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19787.487838541667
INFO:root:current train perplexity6.996248245239258
INFO:root:current mean train loss 19716.188816964284
INFO:root:current train perplexity6.98170804977417


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.99s/it]
INFO:root:final mean train loss: 19677.02941500756
INFO:root:final train perplexity: 6.964245319366455
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.15s/it]
INFO:root:eval mean loss: 23676.28004092262
INFO:root:eval perplexity: 11.592901229858398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/14

  7%|â–‹         | 14/200 [2:07:02<28:05:24, 543.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19493.499565972223
INFO:root:current train perplexity6.867058753967285
INFO:root:current mean train loss 19459.649037278545
INFO:root:current train perplexity6.845433235168457
INFO:root:current mean train loss 19498.692473155286
INFO:root:current train perplexity6.83535099029541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.22s/it]
INFO:root:final mean train loss: 19476.05134434854
INFO:root:final train perplexity: 6.827554225921631
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.31s/it]
INFO:root:eval mean loss: 23548.922549293155
INFO:root:eval perplexity: 11.4410982131958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/15

  8%|â–Š         | 15/200 [2:15:55<27:46:48, 540.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19298.115926621835
INFO:root:current train perplexity6.716342449188232
INFO:root:current mean train loss 19298.551763268155
INFO:root:current train perplexity6.7066473960876465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.89s/it]
INFO:root:final mean train loss: 19284.458566973288
INFO:root:final train perplexity: 6.69974422454834
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.87s/it]
INFO:root:eval mean loss: 23452.881184895832
INFO:root:eval perplexity: 11.327940940856934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/16

  8%|â–Š         | 16/200 [2:24:47<27:29:15, 537.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19078.587386592742
INFO:root:current train perplexity6.627864837646484
INFO:root:current mean train loss 19116.3118588979
INFO:root:current train perplexity6.586398124694824
INFO:root:current mean train loss 19114.45072375541
INFO:root:current train perplexity6.584178924560547


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.25s/it]
INFO:root:final mean train loss: 19110.421166204636
INFO:root:final train perplexity: 6.585717678070068
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.02s/it]
INFO:root:eval mean loss: 23346.853980654763
INFO:root:eval perplexity: 11.204314231872559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/17

  8%|â–Š         | 17/200 [2:33:53<27:28:40, 540.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18962.68526449548
INFO:root:current train perplexity6.487997055053711
INFO:root:current mean train loss 18973.667883367485
INFO:root:current train perplexity6.480785846710205


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.30s/it]
INFO:root:final mean train loss: 18956.57240344632
INFO:root:final train perplexity: 6.486538410186768
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.39s/it]
INFO:root:eval mean loss: 23274.504929315477
INFO:root:eval perplexity: 11.120731353759766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/18

  9%|â–‰         | 18/200 [2:42:48<27:14:33, 538.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18888.688448660716
INFO:root:current train perplexity6.433376789093018
INFO:root:current mean train loss 18878.68431712963
INFO:root:current train perplexity6.419327735900879
INFO:root:current mean train loss 18808.50078125
INFO:root:current train perplexity6.388406753540039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.79s/it]
INFO:root:final mean train loss: 18808.523591072328
INFO:root:final train perplexity: 6.392508506774902
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:38<00:00, 98.74s/it]
INFO:root:eval mean loss: 23188.16003999256
INFO:root:eval perplexity: 11.0217924118042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/19

 10%|â–‰         | 19/200 [2:52:04<27:20:53, 543.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18662.89603538075
INFO:root:current train perplexity6.286555767059326
INFO:root:current mean train loss 18698.23446900067
INFO:root:current train perplexity6.303861141204834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.36s/it]
INFO:root:final mean train loss: 18669.237997731856
INFO:root:final train perplexity: 6.305287837982178
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.61s/it]
INFO:root:eval mean loss: 23122.516043526786
INFO:root:eval perplexity: 10.94716739654541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/20

 10%|â–ˆ         | 20/200 [3:00:56<27:00:28, 540.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18699.55388621795
INFO:root:current train perplexity6.257570266723633
INFO:root:current mean train loss 18582.43542041367
INFO:root:current train perplexity6.23724889755249
INFO:root:current mean train loss 18570.4019760068
INFO:root:current train perplexity6.233339786529541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.24s/it]
INFO:root:final mean train loss: 18547.811995967742
INFO:root:final train perplexity: 6.230223655700684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.24s/it]
INFO:root:eval mean loss: 23064.13060360863
INFO:root:eval perplexity: 10.881218910217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/21

 10%|â–ˆ         | 21/200 [3:10:07<27:01:33, 543.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18416.97267771291
INFO:root:current train perplexity6.1530680656433105
INFO:root:current mean train loss 18435.05612933573
INFO:root:current train perplexity6.147546291351318


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.96s/it]
INFO:root:final mean train loss: 18424.922957881805
INFO:root:final train perplexity: 6.155163764953613
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.98s/it]
INFO:root:eval mean loss: 23004.300734747023
INFO:root:eval perplexity: 10.814047813415527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/22

 11%|â–ˆ         | 22/200 [3:19:19<26:59:42, 545.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18373.02956940407
INFO:root:current train perplexity6.11580753326416
INFO:root:current mean train loss 18342.266744973775
INFO:root:current train perplexity6.0987138748168945
INFO:root:current mean train loss 18341.13360018004
INFO:root:current train perplexity6.092662334442139


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it]
INFO:root:final mean train loss: 18318.247448336693
INFO:root:final train perplexity: 6.0907392501831055
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:20<00:00, 80.25s/it]
INFO:root:eval mean loss: 22961.753255208332
INFO:root:eval perplexity: 10.766534805297852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/23

 12%|â–ˆâ–        | 23/200 [3:28:18<26:44:36, 543.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18271.55699013158
INFO:root:current train perplexity6.032693862915039
INFO:root:current mean train loss 18235.298908253204
INFO:root:current train perplexity6.026143550872803


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.10s/it]
INFO:root:final mean train loss: 18211.030556955644
INFO:root:final train perplexity: 6.026670932769775
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it]
INFO:root:eval mean loss: 22901.750883556546
INFO:root:eval perplexity: 10.699877738952637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/24

 12%|â–ˆâ–        | 24/200 [3:37:16<26:30:22, 542.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18048.048121675532
INFO:root:current train perplexity5.949184417724609
INFO:root:current mean train loss 18112.21073820153
INFO:root:current train perplexity5.967707633972168
INFO:root:current mean train loss 18123.38021887652
INFO:root:current train perplexity5.967587471008301


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.94s/it]
INFO:root:final mean train loss: 18110.24541645665
INFO:root:final train perplexity: 5.967056751251221
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.19s/it]
INFO:root:eval mean loss: 22870.745582217263
INFO:root:eval perplexity: 10.665600776672363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/25

 12%|â–ˆâ–Ž        | 25/200 [3:46:17<26:20:24, 541.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18063.885120738636
INFO:root:current train perplexity5.928088665008545
INFO:root:current mean train loss 18046.817662531408
INFO:root:current train perplexity5.911724090576172


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.02s/it]
INFO:root:final mean train loss: 18016.125248078377
INFO:root:final train perplexity: 5.911920070648193
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.44s/it]
INFO:root:eval mean loss: 22808.379929315477
INFO:root:eval perplexity: 10.596979141235352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/26

 13%|â–ˆâ–Ž        | 26/200 [3:55:10<26:03:33, 539.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17961.266199448528
INFO:root:current train perplexity5.8898725509643555
INFO:root:current mean train loss 17955.953331953642
INFO:root:current train perplexity5.869393348693848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.74s/it]
INFO:root:final mean train loss: 17926.588835685485
INFO:root:final train perplexity: 5.859940528869629
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.01s/it]
INFO:root:eval mean loss: 22773.012276785714
INFO:root:eval perplexity: 10.558260917663574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/27

 14%|â–ˆâ–Ž        | 27/200 [4:04:03<25:49:31, 537.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17989.143229166668
INFO:root:current train perplexity5.797901153564453
INFO:root:current mean train loss 17867.921059617718
INFO:root:current train perplexity5.822214126586914
INFO:root:current mean train loss 17864.825171259236
INFO:root:current train perplexity5.810041904449463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.01s/it]
INFO:root:final mean train loss: 17838.399681829636
INFO:root:final train perplexity: 5.8091912269592285
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.68s/it]
INFO:root:eval mean loss: 22721.493489583332
INFO:root:eval perplexity: 10.502116203308105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/28

 14%|â–ˆâ–        | 28/200 [4:12:56<25:36:42, 536.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17831.31580255682
INFO:root:current train perplexity5.789088249206543
INFO:root:current mean train loss 17797.710987903225
INFO:root:current train perplexity5.7718610763549805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.36s/it]
INFO:root:final mean train loss: 17761.555837323587
INFO:root:final train perplexity: 5.765327453613281
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.47s/it]
INFO:root:eval mean loss: 22686.064964657737
INFO:root:eval perplexity: 10.463677406311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/29

 14%|â–ˆâ–        | 29/200 [4:21:48<25:24:33, 534.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17796.48409598214
INFO:root:current train perplexity5.726868629455566
INFO:root:current mean train loss 17787.984320239488
INFO:root:current train perplexity5.7277936935424805
INFO:root:current mean train loss 17725.436575332125
INFO:root:current train perplexity5.720629692077637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.42s/it]
INFO:root:final mean train loss: 17680.569068170364
INFO:root:final train perplexity: 5.719459056854248
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.61s/it]
INFO:root:eval mean loss: 22642.910249255954
INFO:root:eval perplexity: 10.417047500610352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/30

 15%|â–ˆâ–Œ        | 30/200 [4:30:37<25:10:25, 533.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17639.83620233051
INFO:root:current train perplexity5.688216686248779
INFO:root:current mean train loss 17631.26799577437
INFO:root:current train perplexity5.6831865310668945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.98s/it]
INFO:root:final mean train loss: 17610.14499984249
INFO:root:final train perplexity: 5.679867744445801
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.96s/it]
INFO:root:eval mean loss: 22617.862397693454
INFO:root:eval perplexity: 10.390077590942383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/31

 16%|â–ˆâ–Œ        | 31/200 [4:39:30<25:01:46, 533.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17452.262428977272
INFO:root:current train perplexity5.600958347320557
INFO:root:current mean train loss 17552.53960796734
INFO:root:current train perplexity5.627374172210693
INFO:root:current mean train loss 17555.248842935427
INFO:root:current train perplexity5.644784927368164


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.17s/it]
INFO:root:final mean train loss: 17541.774051789314
INFO:root:final train perplexity: 5.641693592071533
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.83s/it]
INFO:root:eval mean loss: 22593.002790178572
INFO:root:eval perplexity: 10.363378524780273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/32

 16%|â–ˆâ–Œ        | 32/200 [4:48:15<24:45:39, 530.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17463.189639136905
INFO:root:current train perplexity5.580179214477539
INFO:root:current mean train loss 17485.286773868866
INFO:root:current train perplexity5.601177215576172


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.45s/it]
INFO:root:final mean train loss: 17464.65699029738
INFO:root:final train perplexity: 5.598944664001465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.55s/it]
INFO:root:eval mean loss: 22575.726422991072
INFO:root:eval perplexity: 10.344868659973145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/33

 16%|â–ˆâ–‹        | 33/200 [4:57:00<24:32:25, 529.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17248.636848958333
INFO:root:current train perplexity5.517542839050293
INFO:root:current mean train loss 17394.08406929348
INFO:root:current train perplexity5.560225486755371
INFO:root:current mean train loss 17383.518041424417
INFO:root:current train perplexity5.552048683166504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.33s/it]
INFO:root:final mean train loss: 17401.87888656124
INFO:root:final train perplexity: 5.564383506774902
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.29s/it]
INFO:root:eval mean loss: 22521.006742931546
INFO:root:eval perplexity: 10.286447525024414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/34

 17%|â–ˆâ–‹        | 34/200 [5:05:50<24:24:05, 529.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17321.328722597947
INFO:root:current train perplexity5.518630504608154
INFO:root:current mean train loss 17349.27423863211
INFO:root:current train perplexity5.524496555328369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.88s/it]
INFO:root:final mean train loss: 17333.72882276966
INFO:root:final train perplexity: 5.527105808258057
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.84s/it]
INFO:root:eval mean loss: 22492.214936755954
INFO:root:eval perplexity: 10.255839347839355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/35

 18%|â–ˆâ–Š        | 35/200 [5:14:40<24:15:33, 529.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17473.851870888157
INFO:root:current train perplexity5.554306983947754
INFO:root:current mean train loss 17325.75460379464
INFO:root:current train perplexity5.510984897613525
INFO:root:current mean train loss 17293.92101437643
INFO:root:current train perplexity5.4952392578125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.87s/it]
INFO:root:final mean train loss: 17277.21758048765
INFO:root:final train perplexity: 5.496385097503662
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.07s/it]
INFO:root:eval mean loss: 22499.79480561756
INFO:root:eval perplexity: 10.26388931274414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/36

 18%|â–ˆâ–Š        | 36/200 [5:23:44<24:19:03, 533.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17190.45861300616
INFO:root:current train perplexity5.448676109313965
INFO:root:current mean train loss 17212.302009091738
INFO:root:current train perplexity5.464436054229736


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.03s/it]
INFO:root:final mean train loss: 17222.030883789062
INFO:root:final train perplexity: 5.46654748916626
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.51s/it]
INFO:root:eval mean loss: 22452.633254278273
INFO:root:eval perplexity: 10.213912010192871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/37

 18%|â–ˆâ–Š        | 37/200 [5:32:24<23:59:08, 529.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17093.790591032608
INFO:root:current train perplexity5.399186134338379
INFO:root:current mean train loss 17112.69004858994
INFO:root:current train perplexity5.424078941345215
INFO:root:current mean train loss 17181.71898647702
INFO:root:current train perplexity5.437891960144043


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:20<00:00, 440.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:20<00:00, 440.39s/it]
INFO:root:final mean train loss: 17166.1193296371
INFO:root:final train perplexity: 5.4364848136901855
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.73s/it]
INFO:root:eval mean loss: 22442.924758184523
INFO:root:eval perplexity: 10.203655242919922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/38

 19%|â–ˆâ–‰        | 38/200 [5:41:05<23:43:11, 527.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17124.06546875
INFO:root:current train perplexity5.396098613739014
INFO:root:current mean train loss 17109.27303013393
INFO:root:current train perplexity5.398488998413086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.77s/it]
INFO:root:final mean train loss: 17108.37662235383
INFO:root:final train perplexity: 5.405609130859375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.36s/it]
INFO:root:eval mean loss: 22433.583844866072
INFO:root:eval perplexity: 10.193795204162598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/39

 20%|â–ˆâ–‰        | 39/200 [5:49:58<23:39:05, 528.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16971.178168402777
INFO:root:current train perplexity5.338149547576904
INFO:root:current mean train loss 17064.213075172243
INFO:root:current train perplexity5.379348278045654
INFO:root:current mean train loss 17070.423952884084
INFO:root:current train perplexity5.379120826721191


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.61s/it]
INFO:root:final mean train loss: 17061.049989761847
INFO:root:final train perplexity: 5.380436897277832
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.38s/it]
INFO:root:eval mean loss: 22421.750511532737
INFO:root:eval perplexity: 10.181320190429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/40

 20%|â–ˆâ–ˆ        | 40/200 [5:58:48<23:31:27, 529.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17047.5546875
INFO:root:current train perplexity5.359602928161621
INFO:root:current mean train loss 17061.580623690643
INFO:root:current train perplexity5.365540504455566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.07s/it]
INFO:root:final mean train loss: 17007.174032888106
INFO:root:final train perplexity: 5.351921081542969
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.76s/it]
INFO:root:eval mean loss: 22397.802153087796
INFO:root:eval perplexity: 10.156116485595703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/41

 20%|â–ˆâ–ˆ        | 41/200 [6:07:33<23:18:53, 527.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16877.20158140121
INFO:root:current train perplexity5.292297840118408
INFO:root:current mean train loss 16937.161997554867
INFO:root:current train perplexity5.311678886413574
INFO:root:current mean train loss 16979.028366815477
INFO:root:current train perplexity5.326275825500488


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.98s/it]
INFO:root:final mean train loss: 16958.970462922127
INFO:root:final train perplexity: 5.326536655426025
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.91s/it]
INFO:root:eval mean loss: 22374.693289620536
INFO:root:eval perplexity: 10.131854057312012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/42

 21%|â–ˆâ–ˆ        | 42/200 [6:16:17<23:07:06, 526.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16893.563264777862
INFO:root:current train perplexity5.294515609741211
INFO:root:current mean train loss 16933.464555584018
INFO:root:current train perplexity5.30164098739624


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.42s/it]
INFO:root:final mean train loss: 16909.435602003527
INFO:root:final train perplexity: 5.3005757331848145
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.41s/it]
INFO:root:eval mean loss: 22372.85763113839
INFO:root:eval perplexity: 10.129931449890137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/43

 22%|â–ˆâ–ˆâ–       | 43/200 [6:25:00<22:55:44, 525.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17015.58565848214
INFO:root:current train perplexity5.324490070343018
INFO:root:current mean train loss 16920.282241030094
INFO:root:current train perplexity5.28425407409668
INFO:root:current mean train loss 16882.802019614363
INFO:root:current train perplexity5.279396057128906


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.91s/it]
INFO:root:final mean train loss: 16866.476302608367
INFO:root:final train perplexity: 5.278164386749268
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.14s/it]
INFO:root:eval mean loss: 22346.520368303572
INFO:root:eval perplexity: 10.102354049682617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:33:45<22:45:53, 525.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16806.90688981681
INFO:root:current train perplexity5.2410736083984375
INFO:root:current mean train loss 16825.594287892713
INFO:root:current train perplexity5.24760103225708


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.63s/it]
INFO:root:final mean train loss: 16816.139475176413
INFO:root:final train perplexity: 5.252023696899414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.93s/it]
INFO:root:eval mean loss: 22340.32384672619
INFO:root:eval perplexity: 10.095879554748535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:42:28<22:35:27, 524.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16800.836263020832
INFO:root:current train perplexity5.235880374908447
INFO:root:current mean train loss 16778.013917772034
INFO:root:current train perplexity5.230815887451172
INFO:root:current mean train loss 16788.011240683838
INFO:root:current train perplexity5.230157852172852


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.87s/it]
INFO:root:final mean train loss: 16772.06771358367
INFO:root:final train perplexity: 5.229243278503418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.54s/it]
INFO:root:eval mean loss: 22322.453194754464
INFO:root:eval perplexity: 10.07722282409668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:51:10<22:24:58, 524.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16774.816695999314
INFO:root:current train perplexity5.207876682281494
INFO:root:current mean train loss 16767.0703125
INFO:root:current train perplexity5.2146525382995605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:18<00:00, 438.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:18<00:00, 438.90s/it]
INFO:root:final mean train loss: 16733.425375661544
INFO:root:final train perplexity: 5.209350109100342
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.01s/it]
INFO:root:eval mean loss: 22313.714262462796
INFO:root:eval perplexity: 10.068113327026367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [6:59:46<22:09:30, 521.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16696.64612100291
INFO:root:current train perplexity5.18596076965332
INFO:root:current mean train loss 16700.936878551136
INFO:root:current train perplexity5.185111045837402
INFO:root:current mean train loss 16703.095691068673
INFO:root:current train perplexity5.188730716705322


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.37s/it]
INFO:root:final mean train loss: 16692.7098624937
INFO:root:final train perplexity: 5.188471794128418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.40s/it]
INFO:root:eval mean loss: 22302.992722284227
INFO:root:eval perplexity: 10.0569486618042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/48

 24%|â–ˆâ–ˆâ–       | 48/200 [7:08:49<22:17:08, 527.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16629.037345805922
INFO:root:current train perplexity5.147890567779541
INFO:root:current mean train loss 16649.175310496794
INFO:root:current train perplexity5.1578168869018555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.23s/it]
INFO:root:final mean train loss: 16653.394172914566
INFO:root:final train perplexity: 5.168391227722168
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.66s/it]
INFO:root:eval mean loss: 22280.064918154763
INFO:root:eval perplexity: 10.033109664916992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/49

 24%|â–ˆâ–ˆâ–       | 49/200 [7:17:46<22:15:24, 530.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16654.8486328125
INFO:root:current train perplexity5.134881019592285
INFO:root:current mean train loss 16624.14944063563
INFO:root:current train perplexity5.146389007568359
INFO:root:current mean train loss 16623.614186646002
INFO:root:current train perplexity5.147434234619141


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.79s/it]
INFO:root:final mean train loss: 16612.84449423513
INFO:root:final train perplexity: 5.147761344909668
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.59s/it]
INFO:root:eval mean loss: 22286.436128162204
INFO:root:eval perplexity: 10.039728164672852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [7:26:32<22:03:08, 529.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16604.965455334597
INFO:root:current train perplexity5.122829914093018
INFO:root:current mean train loss 16576.304314541456
INFO:root:current train perplexity5.133039951324463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.10s/it]
INFO:root:final mean train loss: 16584.399248676917
INFO:root:final train perplexity: 5.1333394050598145
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it]
INFO:root:eval mean loss: 22279.95958891369
INFO:root:eval perplexity: 10.033003807067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:35:49<22:15:16, 537.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16548.318876378675
INFO:root:current train perplexity5.104556560516357
INFO:root:current mean train loss 16559.048472423427
INFO:root:current train perplexity5.108743190765381


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.09s/it]
INFO:root:final mean train loss: 16536.62511813256
INFO:root:final train perplexity: 5.1092071533203125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.77s/it]
INFO:root:eval mean loss: 22280.321149553572
INFO:root:eval perplexity: 10.033379554748535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:44:46<22:05:52, 537.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16617.532877604168
INFO:root:current train perplexity5.214298248291016
INFO:root:current mean train loss 16476.502607327064
INFO:root:current train perplexity5.078005790710449
INFO:root:current mean train loss 16515.671220751232
INFO:root:current train perplexity5.09299898147583


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.31s/it]
INFO:root:final mean train loss: 16505.855220671623
INFO:root:final train perplexity: 5.093725204467773
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.62s/it]
INFO:root:eval mean loss: 22260.671340215773
INFO:root:eval perplexity: 10.012995719909668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [7:53:51<22:01:56, 539.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16476.84021661932
INFO:root:current train perplexity5.090163707733154
INFO:root:current mean train loss 16522.3347719254
INFO:root:current train perplexity5.093958854675293


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.23s/it]
INFO:root:final mean train loss: 16474.013293850807
INFO:root:final train perplexity: 5.077752113342285
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.24s/it]
INFO:root:eval mean loss: 22249.734723772322
INFO:root:eval perplexity: 10.001664161682129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [8:02:43<21:47:47, 537.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16320.59765625
INFO:root:current train perplexity5.0653228759765625
INFO:root:current mean train loss 16437.117552570093
INFO:root:current train perplexity5.0622735023498535
INFO:root:current mean train loss 16439.14566821407
INFO:root:current train perplexity5.063172817230225


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.54s/it]
INFO:root:final mean train loss: 16438.365124117943
INFO:root:final train perplexity: 5.059930801391602
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.30s/it]
INFO:root:eval mean loss: 22230.488606770832
INFO:root:eval perplexity: 9.98176383972168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [8:11:37<21:36:22, 536.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16432.694915254237
INFO:root:current train perplexity5.03631067276001
INFO:root:current mean train loss 16376.371468406054
INFO:root:current train perplexity5.031399726867676


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.39s/it]
INFO:root:final mean train loss: 16399.833543346773
INFO:root:final train perplexity: 5.040736198425293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it]
INFO:root:eval mean loss: 22248.664411272322
INFO:root:eval perplexity: 10.000557899475098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [8:20:29<21:24:27, 535.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16517.819779829544
INFO:root:current train perplexity5.06979513168335
INFO:root:current mean train loss 16355.511146889077
INFO:root:current train perplexity5.017257213592529
INFO:root:current mean train loss 16395.298874407585
INFO:root:current train perplexity5.028299331665039


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.45s/it]
INFO:root:final mean train loss: 16370.145342426915
INFO:root:final train perplexity: 5.025998115539551
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.88s/it]
INFO:root:eval mean loss: 22247.10730561756
INFO:root:eval perplexity: 9.998946189880371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [8:29:18<21:10:50, 533.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16312.30941530258
INFO:root:current train perplexity4.987466812133789
INFO:root:current mean train loss 16341.426044861963
INFO:root:current train perplexity5.007819175720215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.09s/it]
INFO:root:final mean train loss: 16340.100613501763
INFO:root:final train perplexity: 5.011125564575195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.16s/it]
INFO:root:eval mean loss: 22230.67306082589
INFO:root:eval perplexity: 9.981953620910645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:38:07<20:59:05, 532.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16307.509895833333
INFO:root:current train perplexity5.001396656036377
INFO:root:current mean train loss 16251.344318953805
INFO:root:current train perplexity4.968421459197998
INFO:root:current mean train loss 16288.00136264535
INFO:root:current train perplexity4.987127304077148


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.73s/it]
INFO:root:final mean train loss: 16306.780383694557
INFO:root:final train perplexity: 4.994683742523193
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.39s/it]
INFO:root:eval mean loss: 22222.44507998512
INFO:root:eval perplexity: 9.973456382751465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:46:51<20:44:09, 529.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16277.766834771455
INFO:root:current train perplexity4.959523677825928
INFO:root:current mean train loss 16280.587259075599
INFO:root:current train perplexity4.9805474281311035


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.49s/it]
INFO:root:final mean train loss: 16278.245613344254
INFO:root:final train perplexity: 4.98064661026001
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.39s/it]
INFO:root:eval mean loss: 22207.199544270832
INFO:root:eval perplexity: 9.957735061645508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [8:55:55<20:45:46, 533.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16237.839226973685
INFO:root:current train perplexity4.969091892242432
INFO:root:current mean train loss 16222.027130383403
INFO:root:current train perplexity4.959105014801025
INFO:root:current mean train loss 16245.702259917238
INFO:root:current train perplexity4.961939811706543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:05<00:00, 485.05s/it]
INFO:root:final mean train loss: 16246.427072832661
INFO:root:final train perplexity: 4.96504020690918
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.14s/it]
INFO:root:eval mean loss: 22210.651460193454
INFO:root:eval perplexity: 9.961292266845703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [9:05:21<20:59:00, 543.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16242.331852442781
INFO:root:current train perplexity4.952938079833984
INFO:root:current mean train loss 16247.347713358919
INFO:root:current train perplexity4.954339504241943


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.35s/it]
INFO:root:final mean train loss: 16215.719498172883
INFO:root:final train perplexity: 4.9500250816345215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it]
INFO:root:eval mean loss: 22213.49781436012
INFO:root:eval perplexity: 9.964225769042969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [9:14:13<20:41:51, 539.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16118.322647758152
INFO:root:current train perplexity4.905862331390381
INFO:root:current mean train loss 16204.291539634147
INFO:root:current train perplexity4.930878639221191
INFO:root:current mean train loss 16204.708296839966
INFO:root:current train perplexity4.939196586608887


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.65s/it]
INFO:root:final mean train loss: 16188.796725365424
INFO:root:final train perplexity: 4.9368977546691895
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it]
INFO:root:eval mean loss: 22211.202985491072
INFO:root:eval perplexity: 9.961859703063965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [9:23:08<20:29:58, 538.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16155.386575520834
INFO:root:current train perplexity4.927446365356445
INFO:root:current mean train loss 16187.269118303571
INFO:root:current train perplexity4.92194128036499


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.14s/it]
INFO:root:final mean train loss: 16169.38537203881
INFO:root:final train perplexity: 4.927454948425293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.94s/it]
INFO:root:eval mean loss: 22198.340285528273
INFO:root:eval perplexity: 9.9486083984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [9:31:59<20:15:46, 536.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16244.006401909723
INFO:root:current train perplexity4.922587871551514
INFO:root:current mean train loss 16120.143216350885
INFO:root:current train perplexity4.900437355041504
INFO:root:current mean train loss 16141.115346227974
INFO:root:current train perplexity4.908736228942871


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.52s/it]
INFO:root:final mean train loss: 16135.283403950352
INFO:root:final train perplexity: 4.910909652709961
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.42s/it]
INFO:root:eval mean loss: 22209.541015625
INFO:root:eval perplexity: 9.9601469039917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [9:40:53<20:05:19, 535.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16125.67207278481
INFO:root:current train perplexity4.892031192779541
INFO:root:current mean train loss 16117.648502967877
INFO:root:current train perplexity4.894708156585693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.89s/it]
INFO:root:final mean train loss: 16108.15865202873
INFO:root:final train perplexity: 4.8977885246276855
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.33s/it]
INFO:root:eval mean loss: 22191.239350818454
INFO:root:eval perplexity: 9.941300392150879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:50:01<20:04:39, 539.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16033.777060231854
INFO:root:current train perplexity4.853414058685303
INFO:root:current mean train loss 16092.653424677957
INFO:root:current train perplexity4.877497673034668
INFO:root:current mean train loss 16091.081612723214
INFO:root:current train perplexity4.884011268615723


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.65s/it]
INFO:root:final mean train loss: 16081.800076392388
INFO:root:final train perplexity: 4.885071277618408
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.42s/it]
INFO:root:eval mean loss: 22187.07091703869
INFO:root:eval perplexity: 9.93701171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [9:58:59<19:54:44, 538.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16061.009083207831
INFO:root:current train perplexity4.865429878234863
INFO:root:current mean train loss 16069.9502700222
INFO:root:current train perplexity4.86789608001709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.43s/it]
INFO:root:final mean train loss: 16057.141058152722
INFO:root:final train perplexity: 4.873204231262207
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:24<00:00, 84.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:24<00:00, 84.49s/it]
INFO:root:eval mean loss: 22188.286318824405
INFO:root:eval perplexity: 9.938261985778809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [10:07:57<19:44:39, 538.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16115.8744140625
INFO:root:current train perplexity4.880922317504883
INFO:root:current mean train loss 16048.289214409722
INFO:root:current train perplexity4.857517719268799
INFO:root:current mean train loss 16047.59644281915
INFO:root:current train perplexity4.859908103942871


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.88s/it]
INFO:root:final mean train loss: 16030.496585969002
INFO:root:final train perplexity: 4.860414028167725
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.65s/it]
INFO:root:eval mean loss: 22186.683640252977
INFO:root:eval perplexity: 9.936613082885742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [10:16:57<19:37:01, 539.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16002.933964170259
INFO:root:current train perplexity4.851452350616455
INFO:root:current mean train loss 15987.263703208557
INFO:root:current train perplexity4.845851898193359


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.72s/it]
INFO:root:final mean train loss: 16007.582113942792
INFO:root:final train perplexity: 4.849442005157471
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it]
INFO:root:eval mean loss: 22206.084030877977
INFO:root:eval perplexity: 9.956583023071289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [10:25:52<19:25:07, 537.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15890.434645432691
INFO:root:current train perplexity4.831020355224609
INFO:root:current mean train loss 15943.472698403777
INFO:root:current train perplexity4.821314811706543
INFO:root:current mean train loss 15983.242502124738
INFO:root:current train perplexity4.8315253257751465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.74s/it]
INFO:root:final mean train loss: 15977.322356193295
INFO:root:final train perplexity: 4.834989547729492
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.53s/it]
INFO:root:eval mean loss: 22166.125511532737
INFO:root:eval perplexity: 9.915492057800293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [10:35:04<19:25:20, 542.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15929.899585765797
INFO:root:current train perplexity4.808361053466797
INFO:root:current mean train loss 15947.497571375981
INFO:root:current train perplexity4.8172101974487305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.60s/it]
INFO:root:final mean train loss: 15955.948624149445
INFO:root:final train perplexity: 4.824808120727539
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.34s/it]
INFO:root:eval mean loss: 22183.019368489582
INFO:root:eval perplexity: 9.932845115661621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [10:43:59<19:11:52, 539.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15937.008289425872
INFO:root:current train perplexity4.7910475730896
INFO:root:current mean train loss 15938.462658435315
INFO:root:current train perplexity4.809793949127197
INFO:root:current mean train loss 15947.275868859311
INFO:root:current train perplexity4.814133167266846


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.38s/it]
INFO:root:final mean train loss: 15932.930971207157
INFO:root:final train perplexity: 4.81386661529541
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.49s/it]
INFO:root:eval mean loss: 22180.152878534227
INFO:root:eval perplexity: 9.929899215698242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [10:52:56<19:01:08, 539.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15901.140378289474
INFO:root:current train perplexity4.795203685760498
INFO:root:current mean train loss 15910.705819310897
INFO:root:current train perplexity4.800572395324707


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.56s/it]
INFO:root:final mean train loss: 15908.102275233116
INFO:root:final train perplexity: 4.8020920753479
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.58s/it]
INFO:root:eval mean loss: 22178.102469308036
INFO:root:eval perplexity: 9.927790641784668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [11:01:53<18:50:58, 538.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15883.073075964096
INFO:root:current train perplexity4.77923583984375
INFO:root:current mean train loss 15880.324364902212
INFO:root:current train perplexity4.7775349617004395
INFO:root:current mean train loss 15898.431822494938
INFO:root:current train perplexity4.791152477264404


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.61s/it]
INFO:root:final mean train loss: 15885.719521799396
INFO:root:final train perplexity: 4.791502475738525
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.02s/it]
INFO:root:eval mean loss: 22171.177571614582
INFO:root:eval perplexity: 9.920680046081543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [11:10:46<18:38:29, 536.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15861.754764441288
INFO:root:current train perplexity4.773900985717773
INFO:root:current mean train loss 15873.132773241206
INFO:root:current train perplexity4.779665470123291


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.91s/it]
INFO:root:final mean train loss: 15862.874196698589
INFO:root:final train perplexity: 4.7807183265686035
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.73s/it]
INFO:root:eval mean loss: 22175.545503162204
INFO:root:eval perplexity: 9.925165176391602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [11:19:46<18:31:03, 537.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15836.598326439951
INFO:root:current train perplexity4.7712178230285645
INFO:root:current mean train loss 15856.903598406458
INFO:root:current train perplexity4.768784523010254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.53s/it]
INFO:root:final mean train loss: 15839.508974136845
INFO:root:final train perplexity: 4.769712448120117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.01s/it]
INFO:root:eval mean loss: 22190.209077380954
INFO:root:eval perplexity: 9.940240859985352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [11:28:34<18:16:24, 534.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15443.994791666666
INFO:root:current train perplexity4.647638320922852
INFO:root:current mean train loss 15837.365433479976
INFO:root:current train perplexity4.757992267608643
INFO:root:current mean train loss 15827.872046259237
INFO:root:current train perplexity4.759432792663574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.65s/it]
INFO:root:final mean train loss: 15826.912550403225
INFO:root:final train perplexity: 4.763791561126709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it]
INFO:root:eval mean loss: 22177.863420758928
INFO:root:eval perplexity: 9.927546501159668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [11:37:39<18:13:24, 537.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15781.931516335228
INFO:root:current train perplexity4.7584381103515625
INFO:root:current mean train loss 15801.263558467743
INFO:root:current train perplexity4.75144100189209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.96s/it]
INFO:root:final mean train loss: 15804.357122605847
INFO:root:final train perplexity: 4.753204822540283
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it]
INFO:root:eval mean loss: 22183.627511160714
INFO:root:eval perplexity: 9.933469772338867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [11:46:28<17:59:29, 535.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15733.06138392857
INFO:root:current train perplexity4.715949535369873
INFO:root:current mean train loss 15807.556585864486
INFO:root:current train perplexity4.742325782775879
INFO:root:current mean train loss 15809.17139379529
INFO:root:current train perplexity4.747714996337891


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.33s/it]
INFO:root:final mean train loss: 15783.427935200352
INFO:root:final train perplexity: 4.74340295791626
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.67s/it]
INFO:root:eval mean loss: 22174.973656063987
INFO:root:eval perplexity: 9.924576759338379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [11:55:26<17:52:01, 536.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15716.954548463984
INFO:root:current train perplexity4.713293075561523
INFO:root:current mean train loss 15750.600297268082
INFO:root:current train perplexity4.724331378936768


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.91s/it]
INFO:root:final mean train loss: 15761.179636309223
INFO:root:final train perplexity: 4.733005046844482
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.22s/it]
INFO:root:eval mean loss: 22179.01232328869
INFO:root:eval perplexity: 9.928727149963379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [12:04:18<17:40:42, 534.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15844.633433948864
INFO:root:current train perplexity4.802122592926025
INFO:root:current mean train loss 15771.63254856419
INFO:root:current train perplexity4.7224555015563965
INFO:root:current mean train loss 15776.562111226303
INFO:root:current train perplexity4.726407527923584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.42s/it]
INFO:root:final mean train loss: 15744.64607484879
INFO:root:final train perplexity: 4.725293159484863
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.72s/it]
INFO:root:eval mean loss: 22165.87325613839
INFO:root:eval perplexity: 9.915231704711914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [12:13:09<17:29:28, 533.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15711.887741815477
INFO:root:current train perplexity4.709598541259766
INFO:root:current mean train loss 15710.901205425613
INFO:root:current train perplexity4.714444637298584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.94s/it]
INFO:root:final mean train loss: 15714.19244187878
INFO:root:final train perplexity: 4.711121082305908
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.17s/it]
INFO:root:eval mean loss: 22161.863699776786
INFO:root:eval perplexity: 9.911121368408203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [12:22:07<17:23:01, 534.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15719.0265625
INFO:root:current train perplexity4.681002140045166
INFO:root:current mean train loss 15684.359222146739
INFO:root:current train perplexity4.69961404800415
INFO:root:current mean train loss 15709.319381359011
INFO:root:current train perplexity4.703968524932861


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.75s/it]
INFO:root:final mean train loss: 15696.928659746723
INFO:root:final train perplexity: 4.70310640335083
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.93s/it]
INFO:root:eval mean loss: 22177.47240048363
INFO:root:eval perplexity: 9.927144050598145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [12:30:54<17:09:46, 532.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15638.846329874068
INFO:root:current train perplexity4.6828460693359375
INFO:root:current mean train loss 15689.609187874252
INFO:root:current train perplexity4.694942474365234


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.43s/it]
INFO:root:final mean train loss: 15682.829349640877
INFO:root:final train perplexity: 4.696569919586182
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it]
INFO:root:eval mean loss: 22160.296037946428
INFO:root:eval perplexity: 9.909512519836426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/85
###################best#################
eval test multi:
INFO:root:eval mean loss: 2796.212177998311
INFO:root:eval perplexity: 9.919435501098633
eval test only:
INFO:root:eval mean loss: 2795.8150158214858
INFO:root:eval perplexity: 9.916203498840332


 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [12:39:51<17:03:29, 533.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15432.832236842105
INFO:root:current train perplexity4.668444633483887
INFO:root:current mean train loss 15682.96662454044
INFO:root:current train perplexity4.68605899810791
INFO:root:current mean train loss 15675.371120505137
INFO:root:current train perplexity4.686100959777832


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.42s/it]
INFO:root:final mean train loss: 15662.517255229335
INFO:root:final train perplexity: 4.687170028686523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.30s/it]
INFO:root:eval mean loss: 22168.585309709822
INFO:root:eval perplexity: 9.91801929473877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [12:48:42<16:52:46, 533.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15589.239615426937
INFO:root:current train perplexity4.677651882171631
INFO:root:current mean train loss 15636.655981588085
INFO:root:current train perplexity4.674623012542725


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.64s/it]
INFO:root:final mean train loss: 15645.38972719254
INFO:root:final train perplexity: 4.679259300231934
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.56s/it]
INFO:root:eval mean loss: 22185.232259114582
INFO:root:eval perplexity: 9.935118675231934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [12:57:30<16:41:03, 531.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15662.63816236413
INFO:root:current train perplexity4.66340970993042
INFO:root:current mean train loss 15631.153558498476
INFO:root:current train perplexity4.6672282218933105
INFO:root:current mean train loss 15634.812640134529
INFO:root:current train perplexity4.669272422790527


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.62s/it]
INFO:root:final mean train loss: 15623.787361391129
INFO:root:final train perplexity: 4.669300079345703
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.51s/it]
INFO:root:eval mean loss: 22174.112234933036
INFO:root:eval perplexity: 9.923691749572754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [13:06:14<16:27:52, 529.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15611.843020833334
INFO:root:current train perplexity4.650946140289307
INFO:root:current mean train loss 15602.941277901786
INFO:root:current train perplexity4.657454013824463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.53s/it]
INFO:root:final mean train loss: 15609.78017893145
INFO:root:final train perplexity: 4.662852764129639
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.54s/it]
INFO:root:eval mean loss: 22167.0771484375
INFO:root:eval perplexity: 9.916470527648926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [13:14:59<16:17:06, 528.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15563.255859375
INFO:root:current train perplexity4.643984794616699
INFO:root:current mean train loss 15545.443528543306
INFO:root:current train perplexity4.634039402008057
INFO:root:current mean train loss 15586.08807561261
INFO:root:current train perplexity4.648284435272217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.06s/it]
INFO:root:final mean train loss: 15587.060286983367
INFO:root:final train perplexity: 4.6524152755737305
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.26s/it]
INFO:root:eval mean loss: 22176.864560081845
INFO:root:eval perplexity: 9.926521301269531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [13:24:06<16:18:13, 533.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15627.973076542721
INFO:root:current train perplexity4.646834850311279
INFO:root:current mean train loss 15570.86193915852
INFO:root:current train perplexity4.641702175140381


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.01s/it]
INFO:root:final mean train loss: 15572.646519814769
INFO:root:final train perplexity: 4.645806312561035
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.17s/it]
INFO:root:eval mean loss: 22163.071196056546
INFO:root:eval perplexity: 9.912360191345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [13:33:05<16:12:28, 535.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15567.918724798386
INFO:root:current train perplexity4.643591403961182
INFO:root:current mean train loss 15548.703661736641
INFO:root:current train perplexity4.6383843421936035
INFO:root:current mean train loss 15563.098586309523
INFO:root:current train perplexity4.638136386871338


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.11s/it]
INFO:root:final mean train loss: 15554.587099136845
INFO:root:final train perplexity: 4.637537479400635
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.95s/it]
INFO:root:eval mean loss: 22169.62327938988
INFO:root:eval perplexity: 9.919083595275879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [13:42:02<16:04:36, 535.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15568.0314264872
INFO:root:current train perplexity4.63669490814209
INFO:root:current mean train loss 15546.789761569331
INFO:root:current train perplexity4.6285271644592285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.47s/it]
INFO:root:final mean train loss: 15536.440272177419
INFO:root:final train perplexity: 4.629245281219482
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.15s/it]
INFO:root:eval mean loss: 22177.455124627977
INFO:root:eval perplexity: 9.927124977111816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [13:50:53<15:52:51, 534.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15506.924162946429
INFO:root:current train perplexity4.590947151184082
INFO:root:current mean train loss 15511.159288194445
INFO:root:current train perplexity4.618185520172119
INFO:root:current mean train loss 15534.64340093085
INFO:root:current train perplexity4.6228790283203125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.20s/it]
INFO:root:final mean train loss: 15525.394767515121
INFO:root:final train perplexity: 4.624204635620117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.43s/it]
INFO:root:eval mean loss: 22192.898949032737
INFO:root:eval perplexity: 9.943007469177246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [13:59:44<15:42:30, 533.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15526.369488595546
INFO:root:current train perplexity4.604379177093506
INFO:root:current mean train loss 15519.408526905081
INFO:root:current train perplexity4.614231109619141


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.77s/it]
INFO:root:final mean train loss: 15497.73099640877
INFO:root:final train perplexity: 4.6116042137146
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it]
INFO:root:eval mean loss: 22171.052176339286
INFO:root:eval perplexity: 9.920552253723145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [14:08:47<15:38:31, 536.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15452.06780849359
INFO:root:current train perplexity4.5709547996521
INFO:root:current mean train loss 15494.332761915468
INFO:root:current train perplexity4.593166351318359
INFO:root:current mean train loss 15510.983063382584
INFO:root:current train perplexity4.607587814331055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.76s/it]
INFO:root:final mean train loss: 15488.917000063004
INFO:root:final train perplexity: 4.6075968742370605
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.81s/it]
INFO:root:eval mean loss: 22184.06129092262
INFO:root:eval perplexity: 9.933917999267578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [14:17:35<15:24:55, 533.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15517.379056490385
INFO:root:current train perplexity4.606535911560059
INFO:root:current mean train loss 15473.746196007853
INFO:root:current train perplexity4.600390434265137


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.45s/it]
INFO:root:final mean train loss: 15473.86925875756
INFO:root:final train perplexity: 4.60076379776001
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.50s/it]
INFO:root:eval mean loss: 22178.613211495536
INFO:root:eval perplexity: 9.928316116333008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [14:26:18<15:10:39, 530.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15447.366120094477
INFO:root:current train perplexity4.596705436706543
INFO:root:current mean train loss 15472.585759943182
INFO:root:current train perplexity4.598023891448975
INFO:root:current mean train loss 15467.732972447273
INFO:root:current train perplexity4.594232559204102


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.39s/it]
INFO:root:final mean train loss: 15458.813267861644
INFO:root:final train perplexity: 4.593936920166016
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it]
INFO:root:eval mean loss: 22186.446126302082
INFO:root:eval perplexity: 9.936369895935059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [14:35:32<15:14:08, 537.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15483.40590049342
INFO:root:current train perplexity4.584196090698242
INFO:root:current mean train loss 15451.559500200321
INFO:root:current train perplexity4.583887100219727


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.31s/it]
INFO:root:final mean train loss: 15440.12898500504
INFO:root:final train perplexity: 4.58547830581665
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.94s/it]
INFO:root:eval mean loss: 22182.180245535714
INFO:root:eval perplexity: 9.931981086730957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [14:44:55<15:17:28, 545.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15364.96135305851
INFO:root:current train perplexity4.537150859832764
INFO:root:current mean train loss 15422.031263286564
INFO:root:current train perplexity4.5686211585998535
INFO:root:current mean train loss 15439.337961791498
INFO:root:current train perplexity4.579617023468018


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:03<00:00, 483.69s/it]
INFO:root:final mean train loss: 15428.303604618195
INFO:root:final train perplexity: 4.580132961273193
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it]
INFO:root:eval mean loss: 22193.700148809523
INFO:root:eval perplexity: 9.943832397460938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [14:54:18<15:17:37, 550.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15368.691988241793
INFO:root:current train perplexity4.557679176330566
INFO:root:current mean train loss 15433.184256242148
INFO:root:current train perplexity4.573366165161133


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.78s/it]
INFO:root:final mean train loss: 15414.581842237903
INFO:root:final train perplexity: 4.573938369750977
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.79s/it]
INFO:root:eval mean loss: 22180.809709821428
INFO:root:eval perplexity: 9.930574417114258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [15:03:28<15:07:55, 550.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15434.750555300245
INFO:root:current train perplexity4.554942607879639
INFO:root:current mean train loss 15421.776496533526
INFO:root:current train perplexity4.567249298095703


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.02s/it]
INFO:root:final mean train loss: 15398.194387128277
INFO:root:final train perplexity: 4.566551208496094
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.11s/it]
INFO:root:eval mean loss: 22170.886300223214
INFO:root:eval perplexity: 9.920379638671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [15:12:27<14:53:33, 547.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15477.5849609375
INFO:root:current train perplexity4.509726524353027
INFO:root:current mean train loss 15417.35434996966
INFO:root:current train perplexity4.553756237030029
INFO:root:current mean train loss 15383.210321736453
INFO:root:current train perplexity4.554854869842529


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.76s/it]
INFO:root:final mean train loss: 15383.898642263104
INFO:root:final train perplexity: 4.560117244720459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.47s/it]
INFO:root:eval mean loss: 22169.746721540178
INFO:root:eval perplexity: 9.919208526611328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [15:21:25<14:40:06, 544.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15331.424254261363
INFO:root:current train perplexity4.521405220031738
INFO:root:current mean train loss 15355.867389112904
INFO:root:current train perplexity4.546779155731201


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.74s/it]
INFO:root:final mean train loss: 15365.582397460938
INFO:root:final train perplexity: 4.551886558532715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.66s/it]
INFO:root:eval mean loss: 22183.190104166668
INFO:root:eval perplexity: 9.93302059173584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [15:30:46<14:38:37, 549.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15194.24888392857
INFO:root:current train perplexity4.499643325805664
INFO:root:current mean train loss 15272.595703125
INFO:root:current train perplexity4.527547836303711
INFO:root:current mean train loss 15364.666591183575
INFO:root:current train perplexity4.548971176147461


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.29s/it]
INFO:root:final mean train loss: 15354.608741021926
INFO:root:final train perplexity: 4.546962261199951
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.38s/it]
INFO:root:eval mean loss: 22194.57445126488
INFO:root:eval perplexity: 9.94472885131836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [15:39:44<14:24:21, 545.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15356.350105932202
INFO:root:current train perplexity4.545265197753906
INFO:root:current mean train loss 15380.206742580582
INFO:root:current train perplexity4.544480323791504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.16s/it]
INFO:root:final mean train loss: 15338.89136923513
INFO:root:final train perplexity: 4.539918422698975
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.48s/it]
INFO:root:eval mean loss: 22178.17371186756
INFO:root:eval perplexity: 9.927863121032715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [15:48:49<14:14:43, 545.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15081.098366477272
INFO:root:current train perplexity4.501958847045898
INFO:root:current mean train loss 15302.533599028717
INFO:root:current train perplexity4.5226593017578125
INFO:root:current mean train loss 15339.902362263034
INFO:root:current train perplexity4.534337043762207


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.38s/it]
INFO:root:final mean train loss: 15329.94467064642
INFO:root:final train perplexity: 4.535914421081543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.93s/it]
INFO:root:eval mean loss: 22197.36742001488
INFO:root:eval perplexity: 9.94760513305664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [15:57:47<14:02:13, 543.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15272.485553075398
INFO:root:current train perplexity4.515707015991211
INFO:root:current mean train loss 15324.390223590874
INFO:root:current train perplexity4.529403209686279


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.44s/it]
INFO:root:final mean train loss: 15315.494892735634
INFO:root:final train perplexity: 4.529454708099365
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 22198.044596354168
INFO:root:eval perplexity: 9.948302268981934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [16:06:54<13:54:50, 544.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15046.703059895834
INFO:root:current train perplexity4.499599933624268
INFO:root:current mean train loss 15277.88484205163
INFO:root:current train perplexity4.5122599601745605
INFO:root:current mean train loss 15306.489984556685
INFO:root:current train perplexity4.519686698913574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.12s/it]
INFO:root:final mean train loss: 15302.54556372858
INFO:root:final train perplexity: 4.523672580718994
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it]
INFO:root:eval mean loss: 22204.972842261905
INFO:root:eval perplexity: 9.955439567565918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [16:15:56<13:44:51, 543.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15225.835092117537
INFO:root:current train perplexity4.50259256362915
INFO:root:current mean train loss 15289.580019648203
INFO:root:current train perplexity4.520007133483887


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.36s/it]
INFO:root:final mean train loss: 15290.679793819305
INFO:root:final train perplexity: 4.518381595611572
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it]
INFO:root:eval mean loss: 22208.517113095237
INFO:root:eval perplexity: 9.95909309387207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [16:24:58<13:34:51, 543.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15265.562397203947
INFO:root:current train perplexity4.4913010597229
INFO:root:current mean train loss 15327.787864364496
INFO:root:current train perplexity4.514296054840088
INFO:root:current mean train loss 15284.625075806222
INFO:root:current train perplexity4.510767459869385


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.66s/it]
INFO:root:final mean train loss: 15276.183054277973
INFO:root:final train perplexity: 4.511926174163818
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.19s/it]
INFO:root:eval mean loss: 22203.58275204613
INFO:root:eval perplexity: 9.95400619506836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [16:34:15<13:31:51, 547.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15288.26357559419
INFO:root:current train perplexity4.5083818435668945
INFO:root:current mean train loss 15273.043522706506
INFO:root:current train perplexity4.506465911865234


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:04<00:00, 484.96s/it]
INFO:root:final mean train loss: 15260.542795488911
INFO:root:final train perplexity: 4.504971027374268
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.66s/it]
INFO:root:eval mean loss: 22209.982747395832
INFO:root:eval perplexity: 9.960600852966309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [16:43:38<13:29:26, 551.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15187.485266644022
INFO:root:current train perplexity4.481855392456055
INFO:root:current mean train loss 15235.597179878048
INFO:root:current train perplexity4.498812198638916
INFO:root:current mean train loss 15253.527654673486
INFO:root:current train perplexity4.500757217407227


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.94s/it]
INFO:root:final mean train loss: 15248.63013876638
INFO:root:final train perplexity: 4.499680519104004
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.99s/it]
INFO:root:eval mean loss: 22210.80115327381
INFO:root:eval perplexity: 9.961443901062012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [16:52:38<13:15:04, 548.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15221.114322916666
INFO:root:current train perplexity4.489138126373291
INFO:root:current mean train loss 15246.999949776786
INFO:root:current train perplexity4.496184825897217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.75s/it]
INFO:root:final mean train loss: 15232.526957850303
INFO:root:final train perplexity: 4.492539405822754
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.95s/it]
INFO:root:eval mean loss: 22218.05984933036
INFO:root:eval perplexity: 9.9689302444458
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [17:01:44<13:05:06, 547.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15210.421043113425
INFO:root:current train perplexity4.513815879821777
INFO:root:current mean train loss 15216.140386626477
INFO:root:current train perplexity4.489432334899902
INFO:root:current mean train loss 15233.966568867016
INFO:root:current train perplexity4.487600326538086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.48s/it]
INFO:root:final mean train loss: 15228.327817855343
INFO:root:final train perplexity: 4.490678787231445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.47s/it]
INFO:root:eval mean loss: 22205.628557477678
INFO:root:eval perplexity: 9.956113815307617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [17:10:55<12:57:12, 548.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15188.673432555379
INFO:root:current train perplexity4.4746575355529785
INFO:root:current mean train loss 15227.816706311103
INFO:root:current train perplexity4.479956150054932


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:56<00:00, 476.25s/it]
INFO:root:final mean train loss: 15213.712103074597
INFO:root:final train perplexity: 4.484210014343262
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.69s/it]
INFO:root:eval mean loss: 22218.805850074405
INFO:root:eval perplexity: 9.96970272064209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [17:20:09<12:50:36, 550.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15248.625031502017
INFO:root:current train perplexity4.483080863952637
INFO:root:current mean train loss 15235.98228769084
INFO:root:current train perplexity4.481401443481445
INFO:root:current mean train loss 15219.795272761094
INFO:root:current train perplexity4.481302261352539


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.13s/it]
INFO:root:final mean train loss: 15201.188366305443
INFO:root:final train perplexity: 4.47867488861084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.45s/it]
INFO:root:eval mean loss: 22224.43529110863
INFO:root:eval perplexity: 9.97551155090332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [17:29:19<12:41:04, 550.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15199.046498493975
INFO:root:current train perplexity4.466663360595703
INFO:root:current mean train loss 15217.47322724556
INFO:root:current train perplexity4.474618434906006


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.90s/it]
INFO:root:final mean train loss: 15189.179947391633
INFO:root:final train perplexity: 4.4733734130859375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.24s/it]
INFO:root:eval mean loss: 22224.426246279763
INFO:root:eval perplexity: 9.975502014160156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [17:38:26<12:30:37, 549.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15141.572488839285
INFO:root:current train perplexity4.446049213409424
INFO:root:current mean train loss 15195.830794270832
INFO:root:current train perplexity4.46142053604126
INFO:root:current mean train loss 15191.847544049202
INFO:root:current train perplexity4.470040321350098


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.55s/it]
INFO:root:final mean train loss: 15180.507190335182
INFO:root:final train perplexity: 4.46954870223999
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.41s/it]
INFO:root:eval mean loss: 22231.136207217263
INFO:root:eval perplexity: 9.98243236541748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [17:47:16<12:13:40, 543.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15120.643868983478
INFO:root:current train perplexity4.458854675292969
INFO:root:current mean train loss 15173.165415065174
INFO:root:current train perplexity4.463001728057861


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.98s/it]
INFO:root:final mean train loss: 15166.170358965473
INFO:root:final train perplexity: 4.463232040405273
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.17s/it]
INFO:root:eval mean loss: 22224.837425595237
INFO:root:eval perplexity: 9.97592544555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [17:56:18<12:04:09, 543.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15120.026317107371
INFO:root:current train perplexity4.441511631011963
INFO:root:current mean train loss 15136.343504102968
INFO:root:current train perplexity4.454432487487793
INFO:root:current mean train loss 15165.705825869509
INFO:root:current train perplexity4.458503723144531


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.30s/it]
INFO:root:final mean train loss: 15155.897858650455
INFO:root:final train perplexity: 4.458712100982666
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.55s/it]
INFO:root:eval mean loss: 22230.69215029762
INFO:root:eval perplexity: 9.981972694396973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [18:05:11<11:50:59, 539.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15120.725403502747
INFO:root:current train perplexity4.444681167602539
INFO:root:current mean train loss 15144.070706192735
INFO:root:current train perplexity4.446295261383057


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.27s/it]
INFO:root:final mean train loss: 15147.506060200352
INFO:root:final train perplexity: 4.455024242401123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.71s/it]
INFO:root:eval mean loss: 22224.92859468006
INFO:root:eval perplexity: 9.976020812988281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [18:14:13<11:42:37, 540.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15106.791583393895
INFO:root:current train perplexity4.457937240600586
INFO:root:current mean train loss 15155.442225743007
INFO:root:current train perplexity4.451910018920898
INFO:root:current mean train loss 15142.70923755787
INFO:root:current train perplexity4.4478583335876465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.85s/it]
INFO:root:final mean train loss: 15131.020161290322
INFO:root:final train perplexity: 4.4477858543396
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.98s/it]
INFO:root:eval mean loss: 22236.540666852678
INFO:root:eval perplexity: 9.988017082214355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [18:23:04<11:30:13, 537.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15098.590090460526
INFO:root:current train perplexity4.438680171966553
INFO:root:current mean train loss 15138.851021634615
INFO:root:current train perplexity4.441509246826172


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.78s/it]
INFO:root:final mean train loss: 15123.348428049396
INFO:root:final train perplexity: 4.444421768188477
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.35s/it]
INFO:root:eval mean loss: 22243.13732328869
INFO:root:eval perplexity: 9.994837760925293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [18:32:07<11:23:14, 539.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15106.143430019947
INFO:root:current train perplexity4.426365852355957
INFO:root:current mean train loss 15125.393993144133
INFO:root:current train perplexity4.435921669006348
INFO:root:current mean train loss 15125.360422728998
INFO:root:current train perplexity4.439608097076416


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.94s/it]
INFO:root:final mean train loss: 15111.648969096523
INFO:root:final train perplexity: 4.439295291900635
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it]
INFO:root:eval mean loss: 22236.005812872023
INFO:root:eval perplexity: 9.987464904785156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [18:41:20<11:19:22, 543.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15095.680496369949
INFO:root:current train perplexity4.431814193725586
INFO:root:current mean train loss 15111.148849717336
INFO:root:current train perplexity4.4326043128967285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.93s/it]
INFO:root:final mean train loss: 15103.81956826487
INFO:root:final train perplexity: 4.435868740081787
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.86s/it]
INFO:root:eval mean loss: 22234.912202380954
INFO:root:eval perplexity: 9.986336708068848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [18:50:16<11:07:23, 541.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15090.946844362745
INFO:root:current train perplexity4.4232563972473145
INFO:root:current mean train loss 15074.396549048013
INFO:root:current train perplexity4.422201156616211


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.33s/it]
INFO:root:final mean train loss: 15089.541566910282
INFO:root:final train perplexity: 4.429625988006592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.26s/it]
INFO:root:eval mean loss: 22232.523484002977
INFO:root:eval perplexity: 9.983865737915039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [18:59:14<10:57:22, 540.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14922.564453125
INFO:root:current train perplexity4.437630653381348
INFO:root:current mean train loss 15081.123501972088
INFO:root:current train perplexity4.428342819213867
INFO:root:current mean train loss 15076.524380387931
INFO:root:current train perplexity4.420495986938477


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.92s/it]
INFO:root:final mean train loss: 15078.91109343498
INFO:root:final train perplexity: 4.424983978271484
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.39s/it]
INFO:root:eval mean loss: 22239.063825334822
INFO:root:eval perplexity: 9.990625381469727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [19:08:16<10:48:42, 540.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15108.367080965909
INFO:root:current train perplexity4.430591583251953
INFO:root:current mean train loss 15085.449930695564
INFO:root:current train perplexity4.426549911499023


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.81s/it]
INFO:root:final mean train loss: 15075.470809444305
INFO:root:final train perplexity: 4.423482418060303
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.69s/it]
INFO:root:eval mean loss: 22238.592378162204
INFO:root:eval perplexity: 9.99013900756836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [19:17:20<10:41:01, 541.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15114.234095982143
INFO:root:current train perplexity4.383894920349121
INFO:root:current mean train loss 15086.135961229556
INFO:root:current train perplexity4.4263176918029785
INFO:root:current mean train loss 15074.391710069445
INFO:root:current train perplexity4.419107437133789


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.58s/it]
INFO:root:final mean train loss: 15061.940831338206
INFO:root:final train perplexity: 4.417583465576172
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.02s/it]
INFO:root:eval mean loss: 22264.476376488095
INFO:root:eval perplexity: 10.016937255859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [19:26:05<10:26:15, 536.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15091.96577065678
INFO:root:current train perplexity4.423177719116211
INFO:root:current mean train loss 15046.027933372641
INFO:root:current train perplexity4.414069652557373


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.08s/it]
INFO:root:final mean train loss: 15052.47546780494
INFO:root:final train perplexity: 4.413461208343506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.42s/it]
INFO:root:eval mean loss: 22245.109328497023
INFO:root:eval perplexity: 9.996877670288086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [19:35:16<10:21:59, 540.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14968.580610795454
INFO:root:current train perplexity4.433526515960693
INFO:root:current mean train loss 15047.651463963964
INFO:root:current train perplexity4.405018329620361
INFO:root:current mean train loss 15046.010112744372
INFO:root:current train perplexity4.407933235168457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.93s/it]
INFO:root:final mean train loss: 15043.563913652973
INFO:root:final train perplexity: 4.409584045410156
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.96s/it]
INFO:root:eval mean loss: 22239.224307105655
INFO:root:eval perplexity: 9.990789413452148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [19:44:23<10:15:16, 542.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14948.426277281746
INFO:root:current train perplexity4.39865779876709
INFO:root:current mean train loss 15002.674523101994
INFO:root:current train perplexity4.3969855308532715


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.12s/it]
INFO:root:final mean train loss: 15035.810861895161
INFO:root:final train perplexity: 4.406213760375977
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.07s/it]
INFO:root:eval mean loss: 22255.54952566964
INFO:root:eval perplexity: 10.007684707641602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [19:53:34<10:08:57, 545.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14984.251692708332
INFO:root:current train perplexity4.407402515411377
INFO:root:current mean train loss 15042.119989809782
INFO:root:current train perplexity4.408890724182129
INFO:root:current mean train loss 15037.68719113372
INFO:root:current train perplexity4.4014668464660645


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.14s/it]
INFO:root:final mean train loss: 15030.050871818295
INFO:root:final train perplexity: 4.403709888458252
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.56s/it]
INFO:root:eval mean loss: 22256.363909040178
INFO:root:eval perplexity: 10.008529663085938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [20:02:35<9:58:16, 543.88s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14984.530958488805
INFO:root:current train perplexity4.385053634643555
INFO:root:current mean train loss 15062.69613702283
INFO:root:current train perplexity4.409108638763428


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.01s/it]
INFO:root:final mean train loss: 15022.790928994456
INFO:root:final train perplexity: 4.4005584716796875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.19s/it]
INFO:root:eval mean loss: 22256.34593563988
INFO:root:eval perplexity: 10.00851058959961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [20:11:45<9:51:11, 545.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15063.650699013158
INFO:root:current train perplexity4.398592472076416
INFO:root:current mean train loss 15045.14382549895
INFO:root:current train perplexity4.405395984649658
INFO:root:current mean train loss 15047.220551512557
INFO:root:current train perplexity4.399785041809082


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.40s/it]
INFO:root:final mean train loss: 15013.179195280998
INFO:root:final train perplexity: 4.396388053894043
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.76s/it]
INFO:root:eval mean loss: 22266.665992373513
INFO:root:eval perplexity: 10.019204139709473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [20:20:42<9:39:28, 543.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14993.216906910211
INFO:root:current train perplexity4.384074687957764
INFO:root:current mean train loss 15031.810878106726
INFO:root:current train perplexity4.3931684494018555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.04s/it]
INFO:root:final mean train loss: 15006.007544732864
INFO:root:final train perplexity: 4.393280029296875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.26s/it]
INFO:root:eval mean loss: 22258.884788876487
INFO:root:eval perplexity: 10.01114273071289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [20:29:39<9:28:29, 541.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14875.444887907608
INFO:root:current train perplexity4.349209308624268
INFO:root:current mean train loss 14986.95699949187
INFO:root:current train perplexity4.387012958526611
INFO:root:current mean train loss 14997.174231011772
INFO:root:current train perplexity4.387353420257568


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.93s/it]
INFO:root:final mean train loss: 14993.40828188004
INFO:root:final train perplexity: 4.387824058532715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.22s/it]
INFO:root:eval mean loss: 22250.52367001488
INFO:root:eval perplexity: 10.002482414245605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [20:38:47<9:21:15, 543.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14934.498932291666
INFO:root:current train perplexity4.3658246994018555
INFO:root:current mean train loss 14982.825200892858
INFO:root:current train perplexity4.37429666519165


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.65s/it]
INFO:root:final mean train loss: 14984.506489415322
INFO:root:final train perplexity: 4.383972644805908
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.24s/it]
INFO:root:eval mean loss: 22248.819242931546
INFO:root:eval perplexity: 10.000718116760254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [20:47:49<9:12:04, 543.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14982.477358217593
INFO:root:current train perplexity4.380427837371826
INFO:root:current mean train loss 14947.692498154527
INFO:root:current train perplexity4.3709330558776855
INFO:root:current mean train loss 14988.124587004406
INFO:root:current train perplexity4.379164695739746


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.79s/it]
INFO:root:final mean train loss: 14977.215028824345
INFO:root:final train perplexity: 4.380821704864502
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.78s/it]
INFO:root:eval mean loss: 22261.840983072918
INFO:root:eval perplexity: 10.014205932617188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [20:56:42<8:59:46, 539.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14921.97814477848
INFO:root:current train perplexity4.370680809020996
INFO:root:current mean train loss 14953.310595975909
INFO:root:current train perplexity4.3710455894470215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.27s/it]
INFO:root:final mean train loss: 14964.85517735635
INFO:root:final train perplexity: 4.375483989715576
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.48s/it]
INFO:root:eval mean loss: 22261.709658668155
INFO:root:eval perplexity: 10.014069557189941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [21:05:39<8:50:07, 539.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14941.960622479839
INFO:root:current train perplexity4.375051975250244
INFO:root:current mean train loss 14975.13671875
INFO:root:current train perplexity4.3790082931518555
INFO:root:current mean train loss 14974.505838237283
INFO:root:current train perplexity4.377041339874268


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.67s/it]
INFO:root:final mean train loss: 14962.55160424017
INFO:root:final train perplexity: 4.374489784240723
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.44s/it]
INFO:root:eval mean loss: 22273.76297433036
INFO:root:eval perplexity: 10.026566505432129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [21:14:40<8:41:40, 539.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15023.021425545934
INFO:root:current train perplexity4.3654022216796875
INFO:root:current mean train loss 14978.164868297472
INFO:root:current train perplexity4.367674350738525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.96s/it]
INFO:root:final mean train loss: 14956.263510427167
INFO:root:final train perplexity: 4.3717780113220215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.39s/it]
INFO:root:eval mean loss: 22259.964308965773
INFO:root:eval perplexity: 10.012262344360352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [21:23:34<8:31:06, 538.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14905.032533482143
INFO:root:current train perplexity4.34166145324707
INFO:root:current mean train loss 14938.617100694444
INFO:root:current train perplexity4.356714725494385
INFO:root:current mean train loss 14947.373125831116
INFO:root:current train perplexity4.367027282714844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.18s/it]
INFO:root:final mean train loss: 14946.044598979335
INFO:root:final train perplexity: 4.367373466491699
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.86s/it]
INFO:root:eval mean loss: 22266.83426339286
INFO:root:eval perplexity: 10.019381523132324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [21:32:37<8:23:30, 539.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14926.685142780172
INFO:root:current train perplexity4.35650110244751
INFO:root:current mean train loss 14960.45727669619
INFO:root:current train perplexity4.366684913635254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.22s/it]
INFO:root:final mean train loss: 14941.228381741432
INFO:root:final train perplexity: 4.365299701690674
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.85s/it]
INFO:root:eval mean loss: 22269.289155505954
INFO:root:eval perplexity: 10.021925926208496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [21:41:29<8:12:29, 537.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14932.327423878205
INFO:root:current train perplexity4.349231719970703
INFO:root:current mean train loss 14933.558151135341
INFO:root:current train perplexity4.359221935272217
INFO:root:current mean train loss 14942.326527360094
INFO:root:current train perplexity4.361077785491943


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.04s/it]
INFO:root:final mean train loss: 14932.930124590473
INFO:root:final train perplexity: 4.361728191375732
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.84s/it]
INFO:root:eval mean loss: 22268.619954427082
INFO:root:eval perplexity: 10.021234512329102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [21:50:27<8:03:39, 537.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14972.332278073489
INFO:root:current train perplexity4.35165548324585
INFO:root:current mean train loss 14931.487918234621
INFO:root:current train perplexity4.350081920623779


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:02<00:00, 482.11s/it]
INFO:root:final mean train loss: 14926.287270822833
INFO:root:final train perplexity: 4.3588714599609375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.86s/it]
INFO:root:eval mean loss: 22274.364327566964
INFO:root:eval perplexity: 10.027193069458008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [21:59:49<8:01:07, 544.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14910.50531431686
INFO:root:current train perplexity4.3521199226379395
INFO:root:current mean train loss 14902.93711756993
INFO:root:current train perplexity4.343324184417725
INFO:root:current mean train loss 14928.305744438014
INFO:root:current train perplexity4.353095531463623


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.73s/it]
INFO:root:final mean train loss: 14913.042185137348
INFO:root:final train perplexity: 4.353179931640625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.63s/it]
INFO:root:eval mean loss: 22269.432570684523
INFO:root:eval perplexity: 10.022075653076172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [22:08:40<7:48:31, 540.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14929.979625822369
INFO:root:current train perplexity4.352097034454346
INFO:root:current mean train loss 14908.761633613782
INFO:root:current train perplexity4.34809684753418


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.89s/it]
INFO:root:final mean train loss: 14908.374271515877
INFO:root:final train perplexity: 4.351175785064697
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.27s/it]
INFO:root:eval mean loss: 22266.172712053572
INFO:root:eval perplexity: 10.018695831298828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [22:17:31<7:37:02, 537.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14885.189723238032
INFO:root:current train perplexity4.335263729095459
INFO:root:current mean train loss 14882.62155213648
INFO:root:current train perplexity4.338878154754639
INFO:root:current mean train loss 14914.978420736335
INFO:root:current train perplexity4.349442481994629


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.51s/it]
INFO:root:final mean train loss: 14904.220041582661
INFO:root:final train perplexity: 4.34939432144165
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 22270.945475260418
INFO:root:eval perplexity: 10.023646354675293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [22:26:22<7:26:24, 535.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14876.042179608587
INFO:root:current train perplexity4.333115577697754
INFO:root:current mean train loss 14877.882410097362
INFO:root:current train perplexity4.342418670654297


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.53s/it]
INFO:root:final mean train loss: 14897.02415810862
INFO:root:final train perplexity: 4.346307277679443
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.96s/it]
INFO:root:eval mean loss: 22282.404971168155
INFO:root:eval perplexity: 10.035539627075195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [22:35:13<7:16:22, 534.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14887.546875
INFO:root:current train perplexity4.345608234405518
INFO:root:current mean train loss 14880.008071192053
INFO:root:current train perplexity4.33947229385376


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.12s/it]
INFO:root:final mean train loss: 14890.83041283392
INFO:root:final train perplexity: 4.343653202056885
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.39s/it]
INFO:root:eval mean loss: 22276.294782366072
INFO:root:eval perplexity: 10.029196739196777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [22:43:57<7:05:02, 531.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14986.736653645834
INFO:root:current train perplexity4.363963603973389
INFO:root:current mean train loss 14869.283790958738
INFO:root:current train perplexity4.331114292144775
INFO:root:current mean train loss 14892.41203721521
INFO:root:current train perplexity4.339296340942383


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.03s/it]
INFO:root:final mean train loss: 14882.38875063004
INFO:root:final train perplexity: 4.340038776397705
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.33s/it]
INFO:root:eval mean loss: 22276.068684895832
INFO:root:eval perplexity: 10.028962135314941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [22:52:56<6:58:01, 533.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14843.793465909092
INFO:root:current train perplexity4.318538665771484
INFO:root:current mean train loss 14870.402217741936
INFO:root:current train perplexity4.332207202911377


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.45s/it]
INFO:root:final mean train loss: 14879.93177844632
INFO:root:final train perplexity: 4.338986873626709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.31s/it]
INFO:root:eval mean loss: 22286.861467633928
INFO:root:eval perplexity: 10.040170669555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [23:01:54<6:50:06, 534.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15006.07924107143
INFO:root:current train perplexity4.400076866149902
INFO:root:current mean train loss 14853.019722911798
INFO:root:current train perplexity4.33533239364624
INFO:root:current mean train loss 14892.784967542271
INFO:root:current train perplexity4.338975429534912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.16s/it]
INFO:root:final mean train loss: 14876.210886309223
INFO:root:final train perplexity: 4.337395191192627
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.16s/it]
INFO:root:eval mean loss: 22285.604957217263
INFO:root:eval perplexity: 10.038864135742188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [23:10:46<6:40:33, 534.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14903.31574417373
INFO:root:current train perplexity4.334493637084961
INFO:root:current mean train loss 14857.453628636007
INFO:root:current train perplexity4.328765392303467


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.58s/it]
INFO:root:final mean train loss: 14868.586583291331
INFO:root:final train perplexity: 4.334134101867676
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it]
INFO:root:eval mean loss: 22284.022437686013
INFO:root:eval perplexity: 10.037221908569336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [23:19:57<6:35:18, 539.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14829.771839488636
INFO:root:current train perplexity4.279603481292725
INFO:root:current mean train loss 14854.527563696509
INFO:root:current train perplexity4.3184661865234375
INFO:root:current mean train loss 14864.273863299763
INFO:root:current train perplexity4.3254075050354


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.52s/it]
INFO:root:final mean train loss: 14859.109544323337
INFO:root:final train perplexity: 4.330084800720215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.25s/it]
INFO:root:eval mean loss: 22290.602725074405
INFO:root:eval perplexity: 10.044061660766602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [23:29:05<6:28:20, 541.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14790.275390625
INFO:root:current train perplexity4.317288398742676
INFO:root:current mean train loss 14867.774204371166
INFO:root:current train perplexity4.33096981048584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.19s/it]
INFO:root:final mean train loss: 14856.680738879788
INFO:root:final train perplexity: 4.329047679901123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.20s/it]
INFO:root:eval mean loss: 22289.163527715773
INFO:root:eval perplexity: 10.042562484741211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [23:38:08<6:19:35, 542.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14905.162760416666
INFO:root:current train perplexity4.31770658493042
INFO:root:current mean train loss 14865.609935461956
INFO:root:current train perplexity4.332292556762695
INFO:root:current mean train loss 14863.790715843023
INFO:root:current train perplexity4.327541351318359


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.32s/it]
INFO:root:final mean train loss: 14847.895972467239
INFO:root:final train perplexity: 4.325298309326172
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.92s/it]
INFO:root:eval mean loss: 22287.488257998513
INFO:root:eval perplexity: 10.04081916809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [23:47:07<6:09:53, 541.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14810.514750466418
INFO:root:current train perplexity4.302785873413086
INFO:root:current mean train loss 14828.307751684131
INFO:root:current train perplexity4.313960075378418


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.14s/it]
INFO:root:final mean train loss: 14842.155903477822
INFO:root:final train perplexity: 4.322849750518799
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.73s/it]
INFO:root:eval mean loss: 22289.18870907738
INFO:root:eval perplexity: 10.042591094970703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [23:55:53<5:57:44, 536.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14822.755910773027
INFO:root:current train perplexity4.302146911621094
INFO:root:current mean train loss 14855.57630317752
INFO:root:current train perplexity4.3209228515625
INFO:root:current mean train loss 14840.884333083619
INFO:root:current train perplexity4.322198390960693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.62s/it]
INFO:root:final mean train loss: 14840.91224719632
INFO:root:final train perplexity: 4.322319507598877
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it]
INFO:root:eval mean loss: 22301.506998697918
INFO:root:eval perplexity: 10.055401802062988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [24:04:58<5:50:24, 539.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14837.136374889964
INFO:root:current train perplexity4.305131435394287
INFO:root:current mean train loss 14837.810758177997
INFO:root:current train perplexity4.311873912811279


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.18s/it]
INFO:root:final mean train loss: 14832.395637758316
INFO:root:final train perplexity: 4.318691253662109
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.62s/it]
INFO:root:eval mean loss: 22288.792154947918
INFO:root:eval perplexity: 10.042177200317383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [24:13:47<5:39:25, 535.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14826.545601222826
INFO:root:current train perplexity4.328815460205078
INFO:root:current mean train loss 14876.414213351118
INFO:root:current train perplexity4.321925640106201
INFO:root:current mean train loss 14838.461769548767
INFO:root:current train perplexity4.314157009124756


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.14s/it]
INFO:root:final mean train loss: 14828.946966355847
INFO:root:final train perplexity: 4.317221641540527
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.30s/it]
INFO:root:eval mean loss: 22297.3095703125
INFO:root:eval perplexity: 10.051032066345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [24:22:36<5:29:14, 533.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14817.4036328125
INFO:root:current train perplexity4.327076435089111
INFO:root:current mean train loss 14817.924838169643
INFO:root:current train perplexity4.314023017883301


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.86s/it]
INFO:root:final mean train loss: 14824.34154092112
INFO:root:final train perplexity: 4.3152618408203125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it]
INFO:root:eval mean loss: 22299.874000186013
INFO:root:eval perplexity: 10.053702354431152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [24:31:17<5:17:59, 529.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14858.018156828704
INFO:root:current train perplexity4.3046464920043945
INFO:root:current mean train loss 14853.783180056595
INFO:root:current train perplexity4.325456142425537
INFO:root:current mean train loss 14836.64101218337
INFO:root:current train perplexity4.3159661293029785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.05s/it]
INFO:root:final mean train loss: 14818.619239068801
INFO:root:final train perplexity: 4.312827110290527
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it]
INFO:root:eval mean loss: 22299.61023530506
INFO:root:eval perplexity: 10.053426742553711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [24:40:18<5:11:05, 533.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14849.56798852848
INFO:root:current train perplexity4.318963050842285
INFO:root:current mean train loss 14839.84474292947
INFO:root:current train perplexity4.314352989196777


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.80s/it]
INFO:root:final mean train loss: 14811.766558247227
INFO:root:final train perplexity: 4.30991268157959
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it]
INFO:root:eval mean loss: 22306.134091331845
INFO:root:eval perplexity: 10.060219764709473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [24:49:03<5:00:53, 530.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14829.058310231854
INFO:root:current train perplexity4.304713249206543
INFO:root:current mean train loss 14837.576574427481
INFO:root:current train perplexity4.307285785675049
INFO:root:current mean train loss 14826.938417376894
INFO:root:current train perplexity4.311241149902344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.44s/it]
INFO:root:final mean train loss: 14811.53888530116
INFO:root:final train perplexity: 4.309815406799316
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.34s/it]
INFO:root:eval mean loss: 22303.727864583332
INFO:root:eval perplexity: 10.057710647583008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [24:57:59<4:52:53, 532.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14804.477868505272
INFO:root:current train perplexity4.302492618560791
INFO:root:current mean train loss 14833.329032189207
INFO:root:current train perplexity4.309140682220459


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.71s/it]
INFO:root:final mean train loss: 14808.521295362903
INFO:root:final train perplexity: 4.30853271484375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it]
INFO:root:eval mean loss: 22297.299665178572
INFO:root:eval perplexity: 10.05102252960205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [25:07:06<4:46:12, 536.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14938.080412946429
INFO:root:current train perplexity4.335598945617676
INFO:root:current mean train loss 14790.772432002315
INFO:root:current train perplexity4.296363830566406
INFO:root:current mean train loss 14812.639149767287
INFO:root:current train perplexity4.3042707443237305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.10s/it]
INFO:root:final mean train loss: 14795.295827557964
INFO:root:final train perplexity: 4.302916526794434
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.11s/it]
INFO:root:eval mean loss: 22303.962937127977
INFO:root:eval perplexity: 10.057957649230957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [25:16:01<4:37:02, 536.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14797.723935883621
INFO:root:current train perplexity4.305077075958252
INFO:root:current mean train loss 14792.999237550133
INFO:root:current train perplexity4.301034450531006


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.98s/it]
INFO:root:final mean train loss: 14791.384619928176
INFO:root:final train perplexity: 4.3012566566467285
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it]
INFO:root:eval mean loss: 22310.148018973214
INFO:root:eval perplexity: 10.064398765563965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [25:25:16<4:30:56, 541.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14718.077549078525
INFO:root:current train perplexity4.271088123321533
INFO:root:current mean train loss 14767.064579586331
INFO:root:current train perplexity4.2980194091796875
INFO:root:current mean train loss 14797.827732740585
INFO:root:current train perplexity4.301856517791748


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.29s/it]
INFO:root:final mean train loss: 14789.62837859123
INFO:root:final train perplexity: 4.300512313842773
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.69s/it]
INFO:root:eval mean loss: 22312.232421875
INFO:root:eval perplexity: 10.066568374633789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/171

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [25:34:14<4:21:19, 540.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14828.089822287087
INFO:root:current train perplexity4.316016674041748
INFO:root:current mean train loss 14784.738705620091
INFO:root:current train perplexity4.299006938934326


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.33s/it]
INFO:root:final mean train loss: 14786.236962103074
INFO:root:final train perplexity: 4.299073219299316
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.81s/it]
INFO:root:eval mean loss: 22305.27111235119
INFO:root:eval perplexity: 10.059317588806152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/172

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [25:43:22<4:13:21, 542.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14751.151844113372
INFO:root:current train perplexity4.291170120239258
INFO:root:current mean train loss 14797.993054796765
INFO:root:current train perplexity4.2978644371032715
INFO:root:current mean train loss 14793.592725212191
INFO:root:current train perplexity4.296052932739258


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.74s/it]
INFO:root:final mean train loss: 14779.170733051915
INFO:root:final train perplexity: 4.296078681945801
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.32s/it]
INFO:root:eval mean loss: 22314.95654296875
INFO:root:eval perplexity: 10.06940746307373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/173

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [25:52:24<4:04:13, 542.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14801.188846628289
INFO:root:current train perplexity4.2928080558776855
INFO:root:current mean train loss 14794.86647636218
INFO:root:current train perplexity4.29431676864624


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.61s/it]
INFO:root:final mean train loss: 14777.485816217239
INFO:root:final train perplexity: 4.295364856719971
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.63s/it]
INFO:root:eval mean loss: 22304.433058965773
INFO:root:eval perplexity: 10.05844497680664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/174

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [26:01:31<3:55:45, 544.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14707.59142287234
INFO:root:current train perplexity4.2739973068237305
INFO:root:current mean train loss 14772.856206154336
INFO:root:current train perplexity4.287495136260986
INFO:root:current mean train loss 14792.662998956224
INFO:root:current train perplexity4.297107696533203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:50<00:00, 470.31s/it]
INFO:root:final mean train loss: 14780.457216324345
INFO:root:final train perplexity: 4.296624183654785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.24s/it]
INFO:root:eval mean loss: 22307.44482421875
INFO:root:eval perplexity: 10.06158447265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/175

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [26:10:40<3:47:13, 545.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14770.606435448231
INFO:root:current train perplexity4.288466453552246
INFO:root:current mean train loss 14773.53372330402
INFO:root:current train perplexity4.28902006149292


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.19s/it]
INFO:root:final mean train loss: 14770.762648059475
INFO:root:final train perplexity: 4.292517185211182
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.31s/it]
INFO:root:eval mean loss: 22307.944800967263
INFO:root:eval perplexity: 10.062102317810059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/176

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [26:19:56<3:39:27, 548.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14781.208065257353
INFO:root:current train perplexity4.277867794036865
INFO:root:current mean train loss 14768.973251241721
INFO:root:current train perplexity4.284609794616699


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.45s/it]
INFO:root:final mean train loss: 14769.104622133316
INFO:root:final train perplexity: 4.291815280914307
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.17s/it]
INFO:root:eval mean loss: 22315.776181175595
INFO:root:eval perplexity: 10.07026195526123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/177

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [26:28:53<3:28:56, 545.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14784.8896484375
INFO:root:current train perplexity4.269882678985596
INFO:root:current mean train loss 14760.889828580097
INFO:root:current train perplexity4.2876176834106445
INFO:root:current mean train loss 14790.413971097598
INFO:root:current train perplexity4.29170036315918


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.32s/it]
INFO:root:final mean train loss: 14766.027883222027
INFO:root:final train perplexity: 4.290513038635254
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it]
INFO:root:eval mean loss: 22317.669782366072
INFO:root:eval perplexity: 10.072234153747559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/178

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [26:38:02<3:20:16, 546.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14811.9763671875
INFO:root:current train perplexity4.276893138885498
INFO:root:current mean train loss 14792.071799395162
INFO:root:current train perplexity4.287990570068359


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.04s/it]
INFO:root:final mean train loss: 14762.338579731602
INFO:root:final train perplexity: 4.288951396942139
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.89s/it]
INFO:root:eval mean loss: 22316.51990327381
INFO:root:eval perplexity: 10.071035385131836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/179

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [26:47:09<3:11:19, 546.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14692.709542410714
INFO:root:current train perplexity4.27345609664917
INFO:root:current mean train loss 14743.296619450935
INFO:root:current train perplexity4.287158489227295
INFO:root:current mean train loss 14768.073794157608
INFO:root:current train perplexity4.286612033843994


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.62s/it]
INFO:root:final mean train loss: 14758.450455204133
INFO:root:final train perplexity: 4.287306785583496
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.56s/it]
INFO:root:eval mean loss: 22311.827659970237
INFO:root:eval perplexity: 10.066145896911621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/180

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [26:56:11<3:01:45, 545.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14728.67485434322
INFO:root:current train perplexity4.282900333404541
INFO:root:current mean train loss 14743.69514052673
INFO:root:current train perplexity4.278896331787109


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.59s/it]
INFO:root:final mean train loss: 14752.527879284275
INFO:root:final train perplexity: 4.284803867340088
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it]
INFO:root:eval mean loss: 22317.289132254464
INFO:root:eval perplexity: 10.071837425231934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/181

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [27:05:07<2:51:43, 542.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14849.269176136364
INFO:root:current train perplexity4.270020961761475
INFO:root:current mean train loss 14779.128712697073
INFO:root:current train perplexity4.287421703338623
INFO:root:current mean train loss 14768.7169634923
INFO:root:current train perplexity4.286498546600342


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.07s/it]
INFO:root:final mean train loss: 14752.026123046875
INFO:root:final train perplexity: 4.284591197967529
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.24s/it]
INFO:root:eval mean loss: 22319.03136625744
INFO:root:eval perplexity: 10.073653221130371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/182

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [27:14:17<2:43:22, 544.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14704.628487723214
INFO:root:current train perplexity4.274945259094238
INFO:root:current mean train loss 14743.094774491949
INFO:root:current train perplexity4.276157379150391


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.58s/it]
INFO:root:final mean train loss: 14748.483272429436
INFO:root:final train perplexity: 4.283095359802246
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.93s/it]
INFO:root:eval mean loss: 22320.271298363095
INFO:root:eval perplexity: 10.074945449829102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/183

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [27:23:06<2:33:00, 540.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14717.244791666666
INFO:root:current train perplexity4.292891502380371
INFO:root:current mean train loss 14759.8787109375
INFO:root:current train perplexity4.286420822143555
INFO:root:current mean train loss 14755.510124454942
INFO:root:current train perplexity4.2814249992370605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.44s/it]
INFO:root:final mean train loss: 14744.37132607737
INFO:root:final train perplexity: 4.28135871887207
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.69s/it]
INFO:root:eval mean loss: 22314.602074032737
INFO:root:eval perplexity: 10.069038391113281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/184

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [27:31:56<2:23:10, 536.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14778.61177996735
INFO:root:current train perplexity4.27547550201416
INFO:root:current mean train loss 14754.421740503369
INFO:root:current train perplexity4.278283596038818


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.28s/it]
INFO:root:final mean train loss: 14747.911743164062
INFO:root:final train perplexity: 4.282854080200195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.42s/it]
INFO:root:eval mean loss: 22320.546572730655
INFO:root:eval perplexity: 10.075233459472656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/185

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [27:40:47<2:13:49, 535.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14670.139956825658
INFO:root:current train perplexity4.25331449508667
INFO:root:current mean train loss 14731.694721638656
INFO:root:current train perplexity4.278095245361328
INFO:root:current mean train loss 14764.250191745148
INFO:root:current train perplexity4.283386707305908


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.18s/it]
INFO:root:final mean train loss: 14742.553687310989
INFO:root:final train perplexity: 4.280591011047363
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.48s/it]
INFO:root:eval mean loss: 22318.501534598214
INFO:root:eval perplexity: 10.073102951049805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/186

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [27:49:45<2:05:03, 535.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14759.765171104753
INFO:root:current train perplexity4.275960445404053
INFO:root:current mean train loss 14772.13180738304
INFO:root:current train perplexity4.28142786026001


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.09s/it]
INFO:root:final mean train loss: 14739.975361485634
INFO:root:final train perplexity: 4.279501438140869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.13s/it]
INFO:root:eval mean loss: 22323.432593936013
INFO:root:eval perplexity: 10.07824420928955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/187

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [27:58:32<1:55:36, 533.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14808.649159307066
INFO:root:current train perplexity4.273243427276611
INFO:root:current mean train loss 14735.851038490853
INFO:root:current train perplexity4.274533271789551
INFO:root:current mean train loss 14753.849767026346
INFO:root:current train perplexity4.28057861328125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.03s/it]
INFO:root:final mean train loss: 14740.9517546623
INFO:root:final train perplexity: 4.279914379119873
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it]
INFO:root:eval mean loss: 22321.535598028273
INFO:root:eval perplexity: 10.076266288757324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/188

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [28:07:19<1:46:18, 531.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14764.770377604167
INFO:root:current train perplexity4.28406286239624
INFO:root:current mean train loss 14727.640424107143
INFO:root:current train perplexity4.273283958435059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:57<00:00, 477.81s/it]
INFO:root:final mean train loss: 14733.285455519153
INFO:root:final train perplexity: 4.276679515838623
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22318.368210565477
INFO:root:eval perplexity: 10.07296371459961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/189

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [28:16:36<1:38:50, 539.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14684.738245081018
INFO:root:current train perplexity4.262842178344727
INFO:root:current mean train loss 14720.245232529527
INFO:root:current train perplexity4.275143146514893
INFO:root:current mean train loss 14733.154223740363
INFO:root:current train perplexity4.2735700607299805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.92s/it]
INFO:root:final mean train loss: 14732.388341103831
INFO:root:final train perplexity: 4.276300430297852
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it]
INFO:root:eval mean loss: 22323.918619791668
INFO:root:eval perplexity: 10.078750610351562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/190

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [28:25:33<1:29:45, 538.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14791.201011174842
INFO:root:current train perplexity4.278570652008057
INFO:root:current mean train loss 14736.231117973115
INFO:root:current train perplexity4.272915363311768


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.71s/it]
INFO:root:final mean train loss: 14729.578223443801
INFO:root:final train perplexity: 4.275115966796875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.41s/it]
INFO:root:eval mean loss: 22321.45777529762
INFO:root:eval perplexity: 10.07618522644043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/191

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [28:34:20<1:20:15, 535.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14841.265026461693
INFO:root:current train perplexity4.280399799346924
INFO:root:current mean train loss 14780.795078423187
INFO:root:current train perplexity4.279284477233887
INFO:root:current mean train loss 14741.284801136364
INFO:root:current train perplexity4.276401996612549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.38s/it]
INFO:root:final mean train loss: 14735.079440209174
INFO:root:final train perplexity: 4.277435779571533
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.15s/it]
INFO:root:eval mean loss: 22322.404227120536
INFO:root:eval perplexity: 10.077169418334961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/192

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [28:43:08<1:11:02, 532.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14733.337219973644
INFO:root:current train perplexity4.2778825759887695
INFO:root:current mean train loss 14728.998068220628
INFO:root:current train perplexity4.273443698883057


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.20s/it]
INFO:root:final mean train loss: 14729.537928427419
INFO:root:final train perplexity: 4.27509880065918
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it]
INFO:root:eval mean loss: 22323.90897042411
INFO:root:eval perplexity: 10.078741073608398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/193

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [28:51:51<1:01:49, 529.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14737.402818080358
INFO:root:current train perplexity4.295807838439941
INFO:root:current mean train loss 14745.392947048611
INFO:root:current train perplexity4.276210308074951
INFO:root:current mean train loss 14744.18459524601
INFO:root:current train perplexity4.273858547210693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:55<00:00, 475.09s/it]
INFO:root:final mean train loss: 14728.680183656754
INFO:root:final train perplexity: 4.2747368812561035
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.29s/it]
INFO:root:eval mean loss: 22321.241117931546
INFO:root:eval perplexity: 10.075959205627441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/194

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [29:01:07<53:47, 537.86s/it]  

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14686.75314295977
INFO:root:current train perplexity4.263586044311523
INFO:root:current mean train loss 14724.752485795454
INFO:root:current train perplexity4.272191047668457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.60s/it]
INFO:root:final mean train loss: 14724.694268995716
INFO:root:final train perplexity: 4.273056507110596
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.21s/it]
INFO:root:eval mean loss: 22324.33421688988
INFO:root:eval perplexity: 10.079183578491211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/195

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [29:10:22<45:14, 542.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14798.522360777244
INFO:root:current train perplexity4.28998327255249
INFO:root:current mean train loss 14730.065513995054
INFO:root:current train perplexity4.272329807281494
INFO:root:current mean train loss 14733.620333747385
INFO:root:current train perplexity4.272082328796387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.54s/it]
INFO:root:final mean train loss: 14723.3234390751
INFO:root:final train perplexity: 4.27247953414917
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.16s/it]
INFO:root:eval mean loss: 22323.992257254464
INFO:root:eval perplexity: 10.078829765319824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/196

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [29:19:36<36:24, 546.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14769.106423849587
INFO:root:current train perplexity4.280704975128174
INFO:root:current mean train loss 14753.328748772905
INFO:root:current train perplexity4.279634475708008


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.31s/it]
INFO:root:final mean train loss: 14727.718245967742
INFO:root:final train perplexity: 4.274332046508789
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.18s/it]
INFO:root:eval mean loss: 22321.79334077381
INFO:root:eval perplexity: 10.076533317565918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/197

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [29:28:43<27:19, 546.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14734.600994731105
INFO:root:current train perplexity4.267280101776123
INFO:root:current mean train loss 14735.049927611451
INFO:root:current train perplexity4.267572402954102
INFO:root:current mean train loss 14738.448736496914
INFO:root:current train perplexity4.273983478546143


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.23s/it]
INFO:root:final mean train loss: 14726.61275359123
INFO:root:final train perplexity: 4.273865222930908
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.36s/it]
INFO:root:eval mean loss: 22323.124813988095
INFO:root:eval perplexity: 10.077924728393555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/198

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [29:37:50<18:13, 546.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14751.123036595394
INFO:root:current train perplexity4.2804856300354
INFO:root:current mean train loss 14736.672556089743
INFO:root:current train perplexity4.272965431213379


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.42s/it]
INFO:root:final mean train loss: 14720.816496818295
INFO:root:final train perplexity: 4.271422863006592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.83s/it]
INFO:root:eval mean loss: 22322.24488467262
INFO:root:eval perplexity: 10.077004432678223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [29:46:53<09:05, 545.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14717.549098238032
INFO:root:current train perplexity4.265430927276611
INFO:root:current mean train loss 14722.238726349915
INFO:root:current train perplexity4.271473407745361
INFO:root:current mean train loss 14735.499102511387
INFO:root:current train perplexity4.272839069366455


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.25s/it]
INFO:root:final mean train loss: 14723.339249149445
INFO:root:final train perplexity: 4.272486686706543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.64s/it]
INFO:root:eval mean loss: 22322.544363839286
INFO:root:eval perplexity: 10.077319145202637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_multiqa_minilm/200

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [29:55:54<00:00, 544.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [29:55:54<00:00, 538.77s/it]
INFO:root:evaluating final model
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.36s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.36s/it]
INFO:root:eval mean loss: 22322.544363839286
INFO:root:eval perplexity: 10.077319145202637
INFO:root:evalaution complete
INFO:root:save model final: small_multiqa_minilm/final
