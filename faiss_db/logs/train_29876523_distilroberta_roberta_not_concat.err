INFO:root:Output: distilroberta_roberta_not_concat
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10735.548260929609
INFO:root:current train perplexity4774.1630859375
INFO:root:current mean train loss 9271.022617972676
INFO:root:current train perplexity1515.0865478515625
INFO:root:current mean train loss 8627.306761470527
INFO:root:current train perplexity914.8511352539062
INFO:root:current mean train loss 8249.86273055686
INFO:root:current train perplexity673.4010009765625
INFO:root:current mean train loss 7909.369715994489
INFO:root:current train perplexity520.4974975585938
INFO:root:current mean train loss 7613.750701038189
INFO:root:current train perplexity408.81072998046875
INFO:root:current mean train loss 7308.224044952611
INFO:root:current train perplexity320.7684631347656
INFO:root:current mean train loss 7005.5434014197435
INFO:root:current train perplexity252.9145050048828
INFO:root:current mean train loss 6728.073364122028
INFO:root:current train perplexity203.6495361328125
INFO:root:current mean train loss 6488.10228587963
INFO:root:current train perplexity167.46490478515625
INFO:root:current mean train loss 6264.43354110093
INFO:root:current train perplexity140.63816833496094
INFO:root:current mean train loss 6065.003305977638
INFO:root:current train perplexity119.96424865722656
INFO:root:current mean train loss 5886.337614721661
INFO:root:current train perplexity104.06832885742188
INFO:root:current mean train loss 5720.930068806123
INFO:root:current train perplexity91.34862518310547
INFO:root:current mean train loss 5573.253709829991
INFO:root:current train perplexity81.18123626708984
INFO:root:current mean train loss 5436.634418728502
INFO:root:current train perplexity72.85830688476562
INFO:root:current mean train loss 5311.241260225454
INFO:root:current train perplexity65.93350982666016
INFO:root:current mean train loss 5195.187612231404
INFO:root:current train perplexity60.17931365966797
INFO:root:current mean train loss 5088.64858817552
INFO:root:current train perplexity55.352928161621094

100%|██████████| 1/1 [07:45<00:00, 465.64s/it][A100%|██████████| 1/1 [07:45<00:00, 465.64s/it]
INFO:root:final mean train loss: 5002.551128316274
INFO:root:final train perplexity: 51.83173370361328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.78s/it][A100%|██████████| 1/1 [00:38<00:00, 38.78s/it]
INFO:root:eval mean loss: 2790.171689730164
INFO:root:eval perplexity: 9.56248664855957
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.90s/it][A100%|██████████| 1/1 [00:37<00:00, 37.90s/it]
INFO:root:eval mean loss: 3109.559980243656
INFO:root:eval perplexity: 12.890082359313965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/1
  0%|          | 1/200 [09:03<30:04:02, 543.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3043.413528442383
INFO:root:current train perplexity11.306675910949707
INFO:root:current mean train loss 3021.711756212958
INFO:root:current train perplexity10.94603443145752
INFO:root:current mean train loss 2999.7906392415366
INFO:root:current train perplexity10.729999542236328
INFO:root:current mean train loss 2996.0919707093058
INFO:root:current train perplexity10.620664596557617
INFO:root:current mean train loss 2977.2842325063853
INFO:root:current train perplexity10.452269554138184
INFO:root:current mean train loss 2953.736333802689
INFO:root:current train perplexity10.300507545471191
INFO:root:current mean train loss 2931.735063429003
INFO:root:current train perplexity10.138776779174805
INFO:root:current mean train loss 2909.617930833188
INFO:root:current train perplexity9.969441413879395
INFO:root:current mean train loss 2891.072297339346
INFO:root:current train perplexity9.827982902526855
INFO:root:current mean train loss 2874.6766591967453
INFO:root:current train perplexity9.691495895385742
INFO:root:current mean train loss 2857.9804891751505
INFO:root:current train perplexity9.566102981567383
INFO:root:current mean train loss 2846.0365834663417
INFO:root:current train perplexity9.450911521911621
INFO:root:current mean train loss 2832.077303434673
INFO:root:current train perplexity9.34185791015625
INFO:root:current mean train loss 2817.473811465556
INFO:root:current train perplexity9.236917495727539
INFO:root:current mean train loss 2803.526044942565
INFO:root:current train perplexity9.14005184173584
INFO:root:current mean train loss 2791.527378374165
INFO:root:current train perplexity9.050224304199219
INFO:root:current mean train loss 2780.136755310663
INFO:root:current train perplexity8.962557792663574
INFO:root:current mean train loss 2768.696869109894
INFO:root:current train perplexity8.875977516174316
INFO:root:current mean train loss 2756.5373630607705
INFO:root:current train perplexity8.798563003540039
INFO:root:current mean train loss 2745.896170661942
INFO:root:current train perplexity8.720462799072266

100%|██████████| 1/1 [08:01<00:00, 481.42s/it][A100%|██████████| 1/1 [08:01<00:00, 481.42s/it]
INFO:root:final mean train loss: 2736.9609337449374
INFO:root:final train perplexity: 8.67116928100586
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.71s/it][A100%|██████████| 1/1 [00:45<00:00, 45.71s/it]
INFO:root:eval mean loss: 2321.5186525168992
INFO:root:eval perplexity: 6.5443830490112305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.45s/it][A100%|██████████| 1/1 [00:41<00:00, 41.45s/it]
INFO:root:eval mean loss: 2665.707143364223
INFO:root:eval perplexity: 8.94911003112793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/2
  1%|          | 2/200 [18:36<30:51:09, 560.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2483.5598958333335
INFO:root:current train perplexity7.247480392456055
INFO:root:current mean train loss 2482.3457416735196
INFO:root:current train perplexity7.11596155166626
INFO:root:current mean train loss 2474.3660883432804
INFO:root:current train perplexity7.058298110961914
INFO:root:current mean train loss 2470.650999143675
INFO:root:current train perplexity7.016079902648926
INFO:root:current mean train loss 2468.567564931257
INFO:root:current train perplexity6.99694299697876
INFO:root:current mean train loss 2461.0808478779463
INFO:root:current train perplexity6.972113132476807
INFO:root:current mean train loss 2457.0048857051615
INFO:root:current train perplexity6.9541544914245605
INFO:root:current mean train loss 2453.4692744193917
INFO:root:current train perplexity6.938042640686035
INFO:root:current mean train loss 2450.1826211441607
INFO:root:current train perplexity6.904253959655762
INFO:root:current mean train loss 2444.630599403177
INFO:root:current train perplexity6.869670391082764
INFO:root:current mean train loss 2438.319484005362
INFO:root:current train perplexity6.838932514190674
INFO:root:current mean train loss 2433.0628754766453
INFO:root:current train perplexity6.804856300354004
INFO:root:current mean train loss 2428.884723647861
INFO:root:current train perplexity6.780694484710693
INFO:root:current mean train loss 2423.1872308592283
INFO:root:current train perplexity6.762179374694824
INFO:root:current mean train loss 2417.3443236367652
INFO:root:current train perplexity6.735923767089844
INFO:root:current mean train loss 2412.604431431043
INFO:root:current train perplexity6.709878921508789
INFO:root:current mean train loss 2408.704600159551
INFO:root:current train perplexity6.685693264007568
INFO:root:current mean train loss 2402.5725692863484
INFO:root:current train perplexity6.657268524169922
INFO:root:current mean train loss 2397.7507382823155
INFO:root:current train perplexity6.631186485290527
INFO:root:current mean train loss 2393.5802507158764
INFO:root:current train perplexity6.60980224609375

100%|██████████| 1/1 [08:14<00:00, 494.36s/it][A100%|██████████| 1/1 [08:14<00:00, 494.36s/it]
INFO:root:final mean train loss: 2390.688058642504
INFO:root:final train perplexity: 6.5977325439453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.05s/it][A100%|██████████| 1/1 [00:43<00:00, 43.05s/it]
INFO:root:eval mean loss: 2148.6498564591643
INFO:root:eval perplexity: 5.6900506019592285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.05s/it][A100%|██████████| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 2507.3551705001937
INFO:root:eval perplexity: 7.856714248657227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/3
  2%|▏         | 3/200 [28:16<31:10:26, 569.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2265.3979174804685
INFO:root:current train perplexity6.0283942222595215
INFO:root:current mean train loss 2270.3535384114584
INFO:root:current train perplexity5.9861345291137695
INFO:root:current mean train loss 2258.70903125
INFO:root:current train perplexity5.932566165924072
INFO:root:current mean train loss 2255.834552873884
INFO:root:current train perplexity5.938554763793945
INFO:root:current mean train loss 2264.715413140191
INFO:root:current train perplexity5.953171730041504
INFO:root:current mean train loss 2262.149464222301
INFO:root:current train perplexity5.934976577758789
INFO:root:current mean train loss 2256.1429820838343
INFO:root:current train perplexity5.916594982147217
INFO:root:current mean train loss 2251.453556152344
INFO:root:current train perplexity5.898919582366943
INFO:root:current mean train loss 2244.227793255974
INFO:root:current train perplexity5.874505043029785
INFO:root:current mean train loss 2243.1216394685443
INFO:root:current train perplexity5.870061874389648
INFO:root:current mean train loss 2241.2910872395832
INFO:root:current train perplexity5.8616180419921875
INFO:root:current mean train loss 2235.526768745754
INFO:root:current train perplexity5.848295211791992
INFO:root:current mean train loss 2230.8343538085937
INFO:root:current train perplexity5.8328094482421875
INFO:root:current mean train loss 2228.033216869213
INFO:root:current train perplexity5.815868377685547
INFO:root:current mean train loss 2226.3910139412715
INFO:root:current train perplexity5.811069011688232
INFO:root:current mean train loss 2224.71369172127
INFO:root:current train perplexity5.796962738037109
INFO:root:current mean train loss 2222.7926023910986
INFO:root:current train perplexity5.785216331481934
INFO:root:current mean train loss 2222.8167758789064
INFO:root:current train perplexity5.7796220779418945
INFO:root:current mean train loss 2221.154652858425
INFO:root:current train perplexity5.769266128540039
INFO:root:current mean train loss 2218.935101161859
INFO:root:current train perplexity5.758845329284668

100%|██████████| 1/1 [07:50<00:00, 470.01s/it][A100%|██████████| 1/1 [07:50<00:00, 470.01s/it]
INFO:root:final mean train loss: 2217.819100014922
INFO:root:final train perplexity: 5.756317138671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.81s/it][A100%|██████████| 1/1 [00:41<00:00, 41.81s/it]
INFO:root:eval mean loss: 2039.1156815332724
INFO:root:eval perplexity: 5.207409381866455
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.67s/it][A100%|██████████| 1/1 [00:39<00:00, 39.67s/it]
INFO:root:eval mean loss: 2413.887590989999
INFO:root:eval perplexity: 7.275598049163818
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/4
  2%|▏         | 4/200 [37:31<30:41:06, 563.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2119.0072112581624
INFO:root:current train perplexity5.303112506866455
INFO:root:current mean train loss 2148.771012905829
INFO:root:current train perplexity5.372742652893066
INFO:root:current mean train loss 2152.399970465385
INFO:root:current train perplexity5.402615547180176
INFO:root:current mean train loss 2143.735808910401
INFO:root:current train perplexity5.383525848388672
INFO:root:current mean train loss 2143.1303763216006
INFO:root:current train perplexity5.3943400382995605
INFO:root:current mean train loss 2140.5322495986966
INFO:root:current train perplexity5.3958330154418945
INFO:root:current mean train loss 2140.149025706873
INFO:root:current train perplexity5.40114164352417
INFO:root:current mean train loss 2141.754869921111
INFO:root:current train perplexity5.414080619812012
INFO:root:current mean train loss 2142.55646871283
INFO:root:current train perplexity5.422661304473877
INFO:root:current mean train loss 2142.352049518889
INFO:root:current train perplexity5.420295238494873
INFO:root:current mean train loss 2141.8658076592887
INFO:root:current train perplexity5.416384220123291
INFO:root:current mean train loss 2142.6903790602237
INFO:root:current train perplexity5.417069911956787
INFO:root:current mean train loss 2140.7560783693334
INFO:root:current train perplexity5.413397312164307
INFO:root:current mean train loss 2141.4230188175807
INFO:root:current train perplexity5.4138898849487305
INFO:root:current mean train loss 2141.262196796316
INFO:root:current train perplexity5.415709972381592
INFO:root:current mean train loss 2141.729222962867
INFO:root:current train perplexity5.418903350830078
INFO:root:current mean train loss 2141.969864742872
INFO:root:current train perplexity5.417967319488525
INFO:root:current mean train loss 2143.9764501704426
INFO:root:current train perplexity5.4231743812561035
INFO:root:current mean train loss 2145.406360105199
INFO:root:current train perplexity5.432366371154785
INFO:root:current mean train loss 2146.7551477429342
INFO:root:current train perplexity5.440831661224365

100%|██████████| 1/1 [08:03<00:00, 483.54s/it][A100%|██████████| 1/1 [08:03<00:00, 483.54s/it]
INFO:root:final mean train loss: 2146.422246012493
INFO:root:final train perplexity: 5.440938949584961
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.87s/it][A100%|██████████| 1/1 [00:41<00:00, 41.87s/it]
INFO:root:eval mean loss: 2036.1021776131704
INFO:root:eval perplexity: 5.194725513458252
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.67s/it][A100%|██████████| 1/1 [00:38<00:00, 38.67s/it]
INFO:root:eval mean loss: 2408.649559075105
INFO:root:eval perplexity: 7.244335651397705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/5
  2%|▎         | 5/200 [46:57<30:35:11, 564.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2111.0863196963355
INFO:root:current train perplexity5.359612464904785
INFO:root:current mean train loss 2138.9967346191406
INFO:root:current train perplexity5.41417121887207
INFO:root:current mean train loss 2132.316911724252
INFO:root:current train perplexity5.38710880279541
INFO:root:current mean train loss 2127.0995597839355
INFO:root:current train perplexity5.377472877502441
INFO:root:current mean train loss 2130.918811136041
INFO:root:current train perplexity5.38591194152832
INFO:root:current mean train loss 2129.083612520401
INFO:root:current train perplexity5.385931015014648
INFO:root:current mean train loss 2129.7017227975944
INFO:root:current train perplexity5.387786865234375
INFO:root:current mean train loss 2128.8001908282845
INFO:root:current train perplexity5.372781276702881
INFO:root:current mean train loss 2129.0201882755055
INFO:root:current train perplexity5.374448299407959
INFO:root:current mean train loss 2125.86675318276
INFO:root:current train perplexity5.359775543212891
INFO:root:current mean train loss 2124.437681641526
INFO:root:current train perplexity5.3472981452941895
INFO:root:current mean train loss 2125.944872881915
INFO:root:current train perplexity5.351978302001953
INFO:root:current mean train loss 2121.6509730258836
INFO:root:current train perplexity5.3408966064453125
INFO:root:current mean train loss 2119.1489599150727
INFO:root:current train perplexity5.333665370941162
INFO:root:current mean train loss 2119.801490804256
INFO:root:current train perplexity5.329712390899658
INFO:root:current mean train loss 2118.5570198598534
INFO:root:current train perplexity5.321751594543457
INFO:root:current mean train loss 2117.2500336345756
INFO:root:current train perplexity5.3165154457092285
INFO:root:current mean train loss 2116.400068685078
INFO:root:current train perplexity5.313270092010498
INFO:root:current mean train loss 2114.7917315894124
INFO:root:current train perplexity5.304998397827148

100%|██████████| 1/1 [07:56<00:00, 476.69s/it][A100%|██████████| 1/1 [07:56<00:00, 476.69s/it]
INFO:root:final mean train loss: 2111.7619481166084
INFO:root:final train perplexity: 5.294126510620117
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.01s/it][A100%|██████████| 1/1 [00:45<00:00, 45.03s/it]
INFO:root:eval mean loss: 1978.817413546515
INFO:root:eval perplexity: 4.95941686630249
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.06s/it][A100%|██████████| 1/1 [00:42<00:00, 42.06s/it]
INFO:root:eval mean loss: 2366.209778264905
INFO:root:eval perplexity: 6.99593448638916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/6
  3%|▎         | 6/200 [56:24<30:27:45, 565.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1961.9608154296875
INFO:root:current train perplexity5.040195941925049
INFO:root:current mean train loss 2041.0915624033107
INFO:root:current train perplexity5.00858211517334
INFO:root:current mean train loss 2045.5293782552083
INFO:root:current train perplexity5.029863357543945
INFO:root:current mean train loss 2051.7617698492004
INFO:root:current train perplexity5.029261112213135
INFO:root:current mean train loss 2051.3253377786004
INFO:root:current train perplexity5.0403947830200195
INFO:root:current mean train loss 2052.8261309412424
INFO:root:current train perplexity5.0354485511779785
INFO:root:current mean train loss 2052.4389053319337
INFO:root:current train perplexity5.032259941101074
INFO:root:current mean train loss 2052.065364385978
INFO:root:current train perplexity5.0257887840271
INFO:root:current mean train loss 2052.74885092365
INFO:root:current train perplexity5.027182579040527
INFO:root:current mean train loss 2051.331570065379
INFO:root:current train perplexity5.02629280090332
INFO:root:current mean train loss 2050.408734575971
INFO:root:current train perplexity5.022007942199707
INFO:root:current mean train loss 2048.4361439218965
INFO:root:current train perplexity5.0187087059021
INFO:root:current mean train loss 2044.080439152269
INFO:root:current train perplexity5.012350082397461
INFO:root:current mean train loss 2043.433676224869
INFO:root:current train perplexity5.011656284332275
INFO:root:current mean train loss 2043.8461946300913
INFO:root:current train perplexity5.0099873542785645
INFO:root:current mean train loss 2042.3213668914734
INFO:root:current train perplexity5.006928443908691
INFO:root:current mean train loss 2043.4391932776389
INFO:root:current train perplexity5.010654449462891
INFO:root:current mean train loss 2044.9171336340526
INFO:root:current train perplexity5.01615047454834
INFO:root:current mean train loss 2046.1147394513898
INFO:root:current train perplexity5.0187225341796875
INFO:root:current mean train loss 2045.8898680484779
INFO:root:current train perplexity5.020421981811523

100%|██████████| 1/1 [07:57<00:00, 477.52s/it][A100%|██████████| 1/1 [07:57<00:00, 477.52s/it]
INFO:root:final mean train loss: 2045.8082492999579
INFO:root:final train perplexity: 5.025611877441406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.68s/it][A100%|██████████| 1/1 [00:42<00:00, 42.68s/it]
INFO:root:eval mean loss: 1950.2215623787954
INFO:root:eval perplexity: 4.845973014831543
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.63s/it][A100%|██████████| 1/1 [00:40<00:00, 40.63s/it]
INFO:root:eval mean loss: 2335.879252548759
INFO:root:eval perplexity: 6.8236403465271
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/7
  4%|▎         | 7/200 [1:05:47<30:16:31, 564.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1968.0520901150173
INFO:root:current train perplexity4.742634296417236
INFO:root:current mean train loss 1994.7882493875795
INFO:root:current train perplexity4.86294412612915
INFO:root:current mean train loss 1999.1703452014049
INFO:root:current train perplexity4.875013828277588
INFO:root:current mean train loss 2003.8632294276974
INFO:root:current train perplexity4.881445407867432
INFO:root:current mean train loss 2012.3097563383112
INFO:root:current train perplexity4.883699417114258
INFO:root:current mean train loss 2009.9286814803781
INFO:root:current train perplexity4.875298500061035
INFO:root:current mean train loss 2011.4820226774248
INFO:root:current train perplexity4.876026153564453
INFO:root:current mean train loss 2009.7340217101541
INFO:root:current train perplexity4.870865345001221
INFO:root:current mean train loss 2010.517915683737
INFO:root:current train perplexity4.872391700744629
INFO:root:current mean train loss 2011.870855593214
INFO:root:current train perplexity4.874358654022217
INFO:root:current mean train loss 2013.2153199201482
INFO:root:current train perplexity4.87858247756958
INFO:root:current mean train loss 2013.812670112296
INFO:root:current train perplexity4.883376598358154
INFO:root:current mean train loss 2012.6624068336926
INFO:root:current train perplexity4.882269382476807
INFO:root:current mean train loss 2012.3226981503105
INFO:root:current train perplexity4.882355690002441
INFO:root:current mean train loss 2011.3472294343376
INFO:root:current train perplexity4.880796432495117
INFO:root:current mean train loss 2011.2346427827013
INFO:root:current train perplexity4.882948875427246
INFO:root:current mean train loss 2010.764810644652
INFO:root:current train perplexity4.879609107971191
INFO:root:current mean train loss 2009.7837045511906
INFO:root:current train perplexity4.878882884979248
INFO:root:current mean train loss 2008.6530764404565
INFO:root:current train perplexity4.875221252441406
INFO:root:current mean train loss 2008.726436356434
INFO:root:current train perplexity4.876198768615723

100%|██████████| 1/1 [07:54<00:00, 474.85s/it][A100%|██████████| 1/1 [07:54<00:00, 474.85s/it]
INFO:root:final mean train loss: 2007.5116922798868
INFO:root:final train perplexity: 4.875992774963379
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.12s/it][A100%|██████████| 1/1 [00:43<00:00, 43.12s/it]
INFO:root:eval mean loss: 1937.4868657538232
INFO:root:eval perplexity: 4.796289920806885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.15s/it][A100%|██████████| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 2330.206994455757
INFO:root:eval perplexity: 6.791894912719727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/8
  4%|▍         | 8/200 [1:15:07<30:01:53, 563.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1962.5812604631697
INFO:root:current train perplexity4.6689629554748535
INFO:root:current mean train loss 1967.017447012442
INFO:root:current train perplexity4.688153266906738
INFO:root:current mean train loss 1959.175811377992
INFO:root:current train perplexity4.678311824798584
INFO:root:current mean train loss 1956.8518398000233
INFO:root:current train perplexity4.680936813354492
INFO:root:current mean train loss 1959.0494019957794
INFO:root:current train perplexity4.695733547210693
INFO:root:current mean train loss 1958.1132397232768
INFO:root:current train perplexity4.702595233917236
INFO:root:current mean train loss 1962.0247399037278
INFO:root:current train perplexity4.712253570556641
INFO:root:current mean train loss 1959.6528928172831
INFO:root:current train perplexity4.7057390213012695
INFO:root:current mean train loss 1962.8197378192833
INFO:root:current train perplexity4.722723484039307
INFO:root:current mean train loss 1965.6244207208806
INFO:root:current train perplexity4.727573394775391
INFO:root:current mean train loss 1965.477794643531
INFO:root:current train perplexity4.728024005889893
INFO:root:current mean train loss 1970.0605946276157
INFO:root:current train perplexity4.736071586608887
INFO:root:current mean train loss 1968.8645260706605
INFO:root:current train perplexity4.7338547706604
INFO:root:current mean train loss 1969.4739307189257
INFO:root:current train perplexity4.733187675476074
INFO:root:current mean train loss 1969.8866254321374
INFO:root:current train perplexity4.734372615814209
INFO:root:current mean train loss 1969.6861696324054
INFO:root:current train perplexity4.734111309051514
INFO:root:current mean train loss 1968.334666251553
INFO:root:current train perplexity4.73178243637085
INFO:root:current mean train loss 1968.3201377318985
INFO:root:current train perplexity4.730854034423828
INFO:root:current mean train loss 1968.820965759111
INFO:root:current train perplexity4.729125499725342
INFO:root:current mean train loss 1968.9559098433463
INFO:root:current train perplexity4.728760242462158

100%|██████████| 1/1 [07:55<00:00, 475.14s/it][A100%|██████████| 1/1 [07:55<00:00, 475.14s/it]
INFO:root:final mean train loss: 1968.0836955429747
INFO:root:final train perplexity: 4.7266058921813965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.16s/it][A100%|██████████| 1/1 [00:43<00:00, 43.16s/it]
INFO:root:eval mean loss: 1911.9636044333167
INFO:root:eval perplexity: 4.698245525360107
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.05s/it][A100%|██████████| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 2313.736258865248
INFO:root:eval perplexity: 6.700546741485596
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/9
  4%|▍         | 9/200 [1:24:28<29:50:15, 562.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1937.4143606332632
INFO:root:current train perplexity4.595849514007568
INFO:root:current mean train loss 1945.3464524118524
INFO:root:current train perplexity4.602596282958984
INFO:root:current mean train loss 1941.3336249457466
INFO:root:current train perplexity4.602396488189697
INFO:root:current mean train loss 1941.2826163552024
INFO:root:current train perplexity4.608185291290283
INFO:root:current mean train loss 1940.4475070649544
INFO:root:current train perplexity4.615532875061035
INFO:root:current mean train loss 1940.6561858688576
INFO:root:current train perplexity4.61706018447876
INFO:root:current mean train loss 1939.607408956516
INFO:root:current train perplexity4.618216514587402
INFO:root:current mean train loss 1940.3921898375165
INFO:root:current train perplexity4.618259429931641
INFO:root:current mean train loss 1940.4213882947752
INFO:root:current train perplexity4.622178077697754
INFO:root:current mean train loss 1939.6610813942277
INFO:root:current train perplexity4.619876861572266
INFO:root:current mean train loss 1939.1699041214279
INFO:root:current train perplexity4.6196699142456055
INFO:root:current mean train loss 1940.996701558431
INFO:root:current train perplexity4.62420654296875
INFO:root:current mean train loss 1941.2955879967053
INFO:root:current train perplexity4.629230976104736
INFO:root:current mean train loss 1940.1720920540172
INFO:root:current train perplexity4.627172470092773
INFO:root:current mean train loss 1940.766738597355
INFO:root:current train perplexity4.629597187042236
INFO:root:current mean train loss 1940.0909627540825
INFO:root:current train perplexity4.6274495124816895
INFO:root:current mean train loss 1941.2575305264452
INFO:root:current train perplexity4.6287617683410645
INFO:root:current mean train loss 1939.906583672789
INFO:root:current train perplexity4.624714374542236
INFO:root:current mean train loss 1941.0209002566903
INFO:root:current train perplexity4.627627849578857
INFO:root:current mean train loss 1941.694200359407
INFO:root:current train perplexity4.628384113311768

100%|██████████| 1/1 [08:14<00:00, 494.58s/it][A100%|██████████| 1/1 [08:14<00:00, 494.58s/it]
INFO:root:final mean train loss: 1940.719963069883
INFO:root:final train perplexity: 4.625627517700195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.32s/it][A100%|██████████| 1/1 [00:42<00:00, 42.32s/it]
INFO:root:eval mean loss: 1900.3731498988807
INFO:root:eval perplexity: 4.654385089874268
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.41s/it][A100%|██████████| 1/1 [00:40<00:00, 40.41s/it]
INFO:root:eval mean loss: 2305.78505582336
INFO:root:eval perplexity: 6.656888484954834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/10
  5%|▌         | 10/200 [1:34:08<29:58:03, 567.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1935.3061275758605
INFO:root:current train perplexity4.564702033996582
INFO:root:current mean train loss 1918.816849025749
INFO:root:current train perplexity4.534576892852783
INFO:root:current mean train loss 1913.0175291153578
INFO:root:current train perplexity4.516146659851074
INFO:root:current mean train loss 1907.7113265370936
INFO:root:current train perplexity4.504571914672852
INFO:root:current mean train loss 1915.1198436354778
INFO:root:current train perplexity4.513669967651367
INFO:root:current mean train loss 1912.5893069838808
INFO:root:current train perplexity4.510082721710205
INFO:root:current mean train loss 1913.8020734801244
INFO:root:current train perplexity4.517578125
INFO:root:current mean train loss 1912.1551166033403
INFO:root:current train perplexity4.5164971351623535
INFO:root:current mean train loss 1912.0870886694026
INFO:root:current train perplexity4.512691974639893
INFO:root:current mean train loss 1911.7941413304632
INFO:root:current train perplexity4.508696556091309
INFO:root:current mean train loss 1909.4137121616288
INFO:root:current train perplexity4.501798629760742
INFO:root:current mean train loss 1908.688921403844
INFO:root:current train perplexity4.505856037139893
INFO:root:current mean train loss 1907.8562354169744
INFO:root:current train perplexity4.5076422691345215
INFO:root:current mean train loss 1906.1598638697556
INFO:root:current train perplexity4.505080699920654
INFO:root:current mean train loss 1906.2950215089718
INFO:root:current train perplexity4.503654956817627
INFO:root:current mean train loss 1961.41525402762
INFO:root:current train perplexity4.698352336883545
INFO:root:current mean train loss 1961.7589542121498
INFO:root:current train perplexity4.69836950302124
INFO:root:current mean train loss 1957.9053202920745
INFO:root:current train perplexity4.68659782409668
INFO:root:current mean train loss 1961.6075177547318
INFO:root:current train perplexity4.702545166015625
INFO:root:current mean train loss 1959.7690756406926
INFO:root:current train perplexity4.695029258728027

100%|██████████| 1/1 [07:55<00:00, 475.24s/it][A100%|██████████| 1/1 [07:55<00:00, 475.24s/it]
INFO:root:final mean train loss: 1960.18082014383
INFO:root:final train perplexity: 4.69721794128418
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.60s/it][A100%|██████████| 1/1 [00:42<00:00, 42.60s/it]
INFO:root:eval mean loss: 2007.901286240165
INFO:root:eval perplexity: 5.077521324157715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.25s/it][A100%|██████████| 1/1 [00:39<00:00, 39.25s/it]
INFO:root:eval mean loss: 2405.6490032655975
INFO:root:eval perplexity: 7.22648811340332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/11
  6%|▌         | 11/200 [1:43:27<29:40:44, 565.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1920.3058457485465
INFO:root:current train perplexity4.574521541595459
INFO:root:current mean train loss 1929.7335474157846
INFO:root:current train perplexity4.593870162963867
INFO:root:current mean train loss 1911.2202135632922
INFO:root:current train perplexity4.526254177093506
INFO:root:current mean train loss 1916.767090792483
INFO:root:current train perplexity4.55297327041626
INFO:root:current mean train loss 1913.632555298354
INFO:root:current train perplexity4.532553195953369
INFO:root:current mean train loss 1913.3570973262852
INFO:root:current train perplexity4.528679847717285
INFO:root:current mean train loss 1912.9146817488156
INFO:root:current train perplexity4.521202087402344
INFO:root:current mean train loss 1907.86830445889
INFO:root:current train perplexity4.499666690826416
INFO:root:current mean train loss 1904.52344411329
INFO:root:current train perplexity4.490004539489746
INFO:root:current mean train loss 1901.0471260736244
INFO:root:current train perplexity4.481617450714111
INFO:root:current mean train loss 1900.1720832838757
INFO:root:current train perplexity4.477128505706787
INFO:root:current mean train loss 1898.3632258757773
INFO:root:current train perplexity4.468044757843018
INFO:root:current mean train loss 1897.5615259054846
INFO:root:current train perplexity4.4677019119262695
INFO:root:current mean train loss 1894.7559076394525
INFO:root:current train perplexity4.463253021240234
INFO:root:current mean train loss 1892.8627031000274
INFO:root:current train perplexity4.4578537940979
INFO:root:current mean train loss 1891.6373632750926
INFO:root:current train perplexity4.451672554016113
INFO:root:current mean train loss 1892.4658720077568
INFO:root:current train perplexity4.450593948364258
INFO:root:current mean train loss 1891.5764376137317
INFO:root:current train perplexity4.449155330657959
INFO:root:current mean train loss 1890.7915439706844
INFO:root:current train perplexity4.446415901184082

100%|██████████| 1/1 [07:43<00:00, 463.47s/it][A100%|██████████| 1/1 [07:43<00:00, 463.47s/it]
INFO:root:final mean train loss: 1889.507313538367
INFO:root:final train perplexity: 4.44240140914917
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 43.00s/it][A100%|██████████| 1/1 [00:42<00:00, 43.00s/it]
INFO:root:eval mean loss: 1877.4646762279754
INFO:root:eval perplexity: 4.5688982009887695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.12s/it][A100%|██████████| 1/1 [00:40<00:00, 40.12s/it]
INFO:root:eval mean loss: 2289.5572198096743
INFO:root:eval perplexity: 6.568665027618408
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/12
  6%|▌         | 12/200 [1:52:36<29:15:53, 560.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1718.8258870442708
INFO:root:current train perplexity3.7452218532562256
INFO:root:current mean train loss 1842.9025167817051
INFO:root:current train perplexity4.269340991973877
INFO:root:current mean train loss 1836.8001221905788
INFO:root:current train perplexity4.271379470825195
INFO:root:current mean train loss 1840.9044950881807
INFO:root:current train perplexity4.26719331741333
INFO:root:current mean train loss 1835.5296855008337
INFO:root:current train perplexity4.258281707763672
INFO:root:current mean train loss 1840.3231897676442
INFO:root:current train perplexity4.271585941314697
INFO:root:current mean train loss 1842.9929735680323
INFO:root:current train perplexity4.278771877288818
INFO:root:current mean train loss 1847.1265113798008
INFO:root:current train perplexity4.288626670837402
INFO:root:current mean train loss 1847.1746154253094
INFO:root:current train perplexity4.287355422973633
INFO:root:current mean train loss 1848.621328157444
INFO:root:current train perplexity4.294064998626709
INFO:root:current mean train loss 1848.1745163678886
INFO:root:current train perplexity4.294053554534912
INFO:root:current mean train loss 1847.1844867557584
INFO:root:current train perplexity4.292767524719238
INFO:root:current mean train loss 1845.9119142451489
INFO:root:current train perplexity4.291779518127441
INFO:root:current mean train loss 1846.1705666086075
INFO:root:current train perplexity4.293203353881836
INFO:root:current mean train loss 1847.0335112155035
INFO:root:current train perplexity4.297268390655518
INFO:root:current mean train loss 1848.1925182025273
INFO:root:current train perplexity4.300112724304199
INFO:root:current mean train loss 1850.865709101343
INFO:root:current train perplexity4.304441928863525
INFO:root:current mean train loss 1850.7539962795433
INFO:root:current train perplexity4.30633020401001
INFO:root:current mean train loss 1851.5329269603828
INFO:root:current train perplexity4.307492256164551
INFO:root:current mean train loss 1851.7560004967486
INFO:root:current train perplexity4.308827877044678

100%|██████████| 1/1 [07:58<00:00, 478.91s/it][A100%|██████████| 1/1 [07:58<00:00, 478.91s/it]
INFO:root:final mean train loss: 1850.8487931413597
INFO:root:final train perplexity: 4.308913707733154
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.72s/it][A100%|██████████| 1/1 [00:42<00:00, 42.72s/it]
INFO:root:eval mean loss: 1873.8183779885583
INFO:root:eval perplexity: 4.555436134338379
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.61s/it][A100%|██████████| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 2288.9824448172926
INFO:root:eval perplexity: 6.565561771392822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/13
  6%|▋         | 13/200 [2:02:01<29:10:35, 561.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2478.7978576660157
INFO:root:current train perplexity7.051170825958252
INFO:root:current mean train loss 2604.658123779297
INFO:root:current train perplexity7.773837089538574
INFO:root:current mean train loss 2299.3730751731177
INFO:root:current train perplexity6.1045098304748535
INFO:root:current mean train loss 2181.933620452881
INFO:root:current train perplexity5.567433834075928
INFO:root:current mean train loss 2124.117688278925
INFO:root:current train perplexity5.3181071281433105
INFO:root:current mean train loss 2082.9926809457634
INFO:root:current train perplexity5.149379253387451
INFO:root:current mean train loss 2054.0522065193422
INFO:root:current train perplexity5.041682243347168
INFO:root:current mean train loss 2031.8637885199653
INFO:root:current train perplexity4.955831527709961
INFO:root:current mean train loss 2013.908055300829
INFO:root:current train perplexity4.890883922576904
INFO:root:current mean train loss 2003.1776091202446
INFO:root:current train perplexity4.847224235534668
INFO:root:current mean train loss 1991.5746261297488
INFO:root:current train perplexity4.805606365203857
INFO:root:current mean train loss 1982.1492148263114
INFO:root:current train perplexity4.776519775390625
INFO:root:current mean train loss 1976.1862178614883
INFO:root:current train perplexity4.754458904266357
INFO:root:current mean train loss 1969.7515606504498
INFO:root:current train perplexity4.7321248054504395
INFO:root:current mean train loss 1966.2801052899429
INFO:root:current train perplexity4.713077068328857
INFO:root:current mean train loss 1960.795471914191
INFO:root:current train perplexity4.697366237640381
INFO:root:current mean train loss 1958.664744511357
INFO:root:current train perplexity4.690829277038574
INFO:root:current mean train loss 1954.8459499625274
INFO:root:current train perplexity4.674896240234375
INFO:root:current mean train loss 1951.626739837311
INFO:root:current train perplexity4.665085792541504
INFO:root:current mean train loss 1949.2994576772055
INFO:root:current train perplexity4.655807971954346

100%|██████████| 1/1 [07:55<00:00, 475.97s/it][A100%|██████████| 1/1 [07:55<00:00, 475.97s/it]
INFO:root:final mean train loss: 1946.5993555389266
INFO:root:final train perplexity: 4.647139549255371
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.47s/it][A100%|██████████| 1/1 [00:41<00:00, 41.47s/it]
INFO:root:eval mean loss: 1890.9730415073693
INFO:root:eval perplexity: 4.619114875793457
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.73s/it][A100%|██████████| 1/1 [00:39<00:00, 39.73s/it]
INFO:root:eval mean loss: 2316.9948384169993
INFO:root:eval perplexity: 6.71851921081543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/14
  7%|▋         | 14/200 [2:11:21<28:59:19, 561.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1854.8427239495354
INFO:root:current train perplexity4.382190227508545
INFO:root:current mean train loss 1870.5173758625115
INFO:root:current train perplexity4.3653364181518555
INFO:root:current mean train loss 1876.7931809566192
INFO:root:current train perplexity4.381354331970215
INFO:root:current mean train loss 1873.7833769936944
INFO:root:current train perplexity4.3816962242126465
INFO:root:current mean train loss 1872.9176162265803
INFO:root:current train perplexity4.3801727294921875
INFO:root:current mean train loss 1876.4439681353294
INFO:root:current train perplexity4.391122341156006
INFO:root:current mean train loss 1876.2138646962692
INFO:root:current train perplexity4.390014171600342
INFO:root:current mean train loss 1875.2322201360032
INFO:root:current train perplexity4.393802165985107
INFO:root:current mean train loss 1874.4016372881197
INFO:root:current train perplexity4.391504764556885
INFO:root:current mean train loss 1871.0811026297358
INFO:root:current train perplexity4.382064342498779
INFO:root:current mean train loss 1872.49266295033
INFO:root:current train perplexity4.384328842163086
INFO:root:current mean train loss 1874.086681624306
INFO:root:current train perplexity4.39094352722168
INFO:root:current mean train loss 1873.2516633930377
INFO:root:current train perplexity4.3875837326049805
INFO:root:current mean train loss 1872.4967086666452
INFO:root:current train perplexity4.385075092315674
INFO:root:current mean train loss 1873.363000241932
INFO:root:current train perplexity4.3875813484191895
INFO:root:current mean train loss 1872.9794738412136
INFO:root:current train perplexity4.385308742523193
INFO:root:current mean train loss 1873.466459447899
INFO:root:current train perplexity4.387311935424805
INFO:root:current mean train loss 1875.6410900197225
INFO:root:current train perplexity4.390025615692139
INFO:root:current mean train loss 1874.578869914645
INFO:root:current train perplexity4.386688232421875
INFO:root:current mean train loss 1873.2993745109625
INFO:root:current train perplexity4.384138107299805

100%|██████████| 1/1 [08:00<00:00, 480.06s/it][A100%|██████████| 1/1 [08:00<00:00, 480.06s/it]
INFO:root:final mean train loss: 1872.2599166277619
INFO:root:final train perplexity: 4.382343292236328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.84s/it][A100%|██████████| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 1871.910320309037
INFO:root:eval perplexity: 4.5484089851379395
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.22s/it][A100%|██████████| 1/1 [00:41<00:00, 41.22s/it]
INFO:root:eval mean loss: 2297.3805684840427
INFO:root:eval perplexity: 6.611049652099609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/15
  8%|▊         | 15/200 [2:20:48<28:56:00, 563.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1830.7739777741608
INFO:root:current train perplexity4.250425338745117
INFO:root:current mean train loss 1845.3031758890525
INFO:root:current train perplexity4.247410774230957
INFO:root:current mean train loss 1839.6609746978038
INFO:root:current train perplexity4.249353885650635
INFO:root:current mean train loss 1849.527298921919
INFO:root:current train perplexity4.270355224609375
INFO:root:current mean train loss 1841.076442903359
INFO:root:current train perplexity4.2489237785339355
INFO:root:current mean train loss 1843.8763394682846
INFO:root:current train perplexity4.258390426635742
INFO:root:current mean train loss 1839.3765971799144
INFO:root:current train perplexity4.2563347816467285
INFO:root:current mean train loss 1839.7256872850007
INFO:root:current train perplexity4.258199691772461
INFO:root:current mean train loss 1840.3872005989735
INFO:root:current train perplexity4.26394510269165
INFO:root:current mean train loss 1840.3758175128162
INFO:root:current train perplexity4.263422012329102
INFO:root:current mean train loss 1840.35844951246
INFO:root:current train perplexity4.264525890350342
INFO:root:current mean train loss 1838.2227001487693
INFO:root:current train perplexity4.259246826171875
INFO:root:current mean train loss 1839.3865475205903
INFO:root:current train perplexity4.260474681854248
INFO:root:current mean train loss 1838.8355712890625
INFO:root:current train perplexity4.262531757354736
INFO:root:current mean train loss 1838.7239036507588
INFO:root:current train perplexity4.264235496520996
INFO:root:current mean train loss 1837.22192500641
INFO:root:current train perplexity4.261856555938721
INFO:root:current mean train loss 1836.4633523371372
INFO:root:current train perplexity4.261776924133301
INFO:root:current mean train loss 1835.8452737910445
INFO:root:current train perplexity4.25893497467041
INFO:root:current mean train loss 1835.9966437248222
INFO:root:current train perplexity4.258261203765869
INFO:root:current mean train loss 1836.17500397322
INFO:root:current train perplexity4.257898807525635

100%|██████████| 1/1 [08:04<00:00, 484.12s/it][A100%|██████████| 1/1 [08:04<00:00, 484.12s/it]
INFO:root:final mean train loss: 1835.8414920070109
INFO:root:final train perplexity: 4.258181095123291
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.24s/it][A100%|██████████| 1/1 [00:43<00:00, 43.24s/it]
INFO:root:eval mean loss: 1851.410021193484
INFO:root:eval perplexity: 4.473576068878174
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.02s/it][A100%|██████████| 1/1 [00:40<00:00, 40.02s/it]
INFO:root:eval mean loss: 2277.794470387993
INFO:root:eval perplexity: 6.505448341369629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/16
  8%|▊         | 16/200 [2:30:18<28:52:56, 565.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1794.4037862428477
INFO:root:current train perplexity4.0989298820495605
INFO:root:current mean train loss 1797.5763053670962
INFO:root:current train perplexity4.122250080108643
INFO:root:current mean train loss 1808.0993760450299
INFO:root:current train perplexity4.135228633880615
INFO:root:current mean train loss 1805.7148486854574
INFO:root:current train perplexity4.135251045227051
INFO:root:current mean train loss 1802.269687271928
INFO:root:current train perplexity4.129512310028076
INFO:root:current mean train loss 1800.8461794343805
INFO:root:current train perplexity4.127994060516357
INFO:root:current mean train loss 1802.4443761424764
INFO:root:current train perplexity4.141750812530518
INFO:root:current mean train loss 1815.6562348005837
INFO:root:current train perplexity4.1834282875061035
INFO:root:current mean train loss 1815.362013176306
INFO:root:current train perplexity4.18496036529541
INFO:root:current mean train loss 1815.6195040701837
INFO:root:current train perplexity4.1838483810424805
INFO:root:current mean train loss 1814.7033764352095
INFO:root:current train perplexity4.180322170257568
INFO:root:current mean train loss 1819.116741542018
INFO:root:current train perplexity4.198610305786133
INFO:root:current mean train loss 1823.0435253176324
INFO:root:current train perplexity4.214697360992432
INFO:root:current mean train loss 1823.9400884960794
INFO:root:current train perplexity4.21794319152832
INFO:root:current mean train loss 1824.8083873673575
INFO:root:current train perplexity4.219253063201904
INFO:root:current mean train loss 1828.0433971227772
INFO:root:current train perplexity4.230405807495117
INFO:root:current mean train loss 2123.6885646635155
INFO:root:current train perplexity5.339921951293945
INFO:root:current mean train loss 2445.9600818360477
INFO:root:current train perplexity6.888719081878662
INFO:root:current mean train loss 2719.108754013771
INFO:root:current train perplexity8.539165496826172
INFO:root:current mean train loss 2922.0883688730737
INFO:root:current train perplexity10.027080535888672

100%|██████████| 1/1 [08:13<00:00, 493.28s/it][A100%|██████████| 1/1 [08:13<00:00, 493.28s/it]
INFO:root:final mean train loss: 2941.76175999413
INFO:root:final train perplexity: 10.192296028137207
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.11s/it][A100%|██████████| 1/1 [00:43<00:00, 43.11s/it]
INFO:root:eval mean loss: 6335.105297332115
INFO:root:eval perplexity: 168.41995239257812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6457.7377522786455
INFO:root:eval perplexity: 202.16639709472656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/17
  8%|▊         | 17/200 [2:39:57<28:56:07, 569.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6589.24209317294
INFO:root:current train perplexity185.0126495361328
INFO:root:current mean train loss 6621.472328997673
INFO:root:current train perplexity182.50926208496094
INFO:root:current mean train loss 6723.148076375325
INFO:root:current train perplexity197.8139190673828
INFO:root:current mean train loss 6816.886649534875
INFO:root:current train perplexity211.8612823486328
INFO:root:current mean train loss 6855.541156706263
INFO:root:current train perplexity219.4456787109375
INFO:root:current mean train loss 6872.885499707696
INFO:root:current train perplexity223.6096954345703
INFO:root:current mean train loss 6894.205479821493
INFO:root:current train perplexity226.6553192138672
INFO:root:current mean train loss 6903.218113623295
INFO:root:current train perplexity228.51449584960938
INFO:root:current mean train loss 6905.135311092342
INFO:root:current train perplexity229.68951416015625
INFO:root:current mean train loss 6904.783834727669
INFO:root:current train perplexity230.53375244140625
INFO:root:current mean train loss 6907.8565207088695
INFO:root:current train perplexity232.1569061279297
INFO:root:current mean train loss 6910.600673893887
INFO:root:current train perplexity232.94442749023438
INFO:root:current mean train loss 6909.604701829993
INFO:root:current train perplexity233.30638122558594
INFO:root:current mean train loss 6908.141564624797
INFO:root:current train perplexity233.5681915283203
INFO:root:current mean train loss 6911.54148257676
INFO:root:current train perplexity233.79783630371094
INFO:root:current mean train loss 6910.027989461981
INFO:root:current train perplexity233.94259643554688
INFO:root:current mean train loss 6913.537040240391
INFO:root:current train perplexity234.34519958496094
INFO:root:current mean train loss 6917.622011871679
INFO:root:current train perplexity234.79013061523438
INFO:root:current mean train loss 6923.425373142049
INFO:root:current train perplexity235.42230224609375

100%|██████████| 1/1 [07:57<00:00, 477.04s/it][A100%|██████████| 1/1 [07:57<00:00, 477.04s/it]
INFO:root:final mean train loss: 6921.547810441487
INFO:root:final train perplexity: 235.678955078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.13s/it][A100%|██████████| 1/1 [00:43<00:00, 43.13s/it]
INFO:root:eval mean loss: 6691.215390902039
INFO:root:eval perplexity: 224.6693878173828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.05s/it][A100%|██████████| 1/1 [00:39<00:00, 39.05s/it]
INFO:root:eval mean loss: 6788.73497755984
INFO:root:eval perplexity: 265.3934631347656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/18
  9%|▉         | 18/200 [2:49:19<28:39:54, 567.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6859.14521484375
INFO:root:current train perplexity238.8716583251953
INFO:root:current mean train loss 6983.573921130952
INFO:root:current train perplexity240.00733947753906
INFO:root:current mean train loss 6961.266673018293
INFO:root:current train perplexity240.9622039794922
INFO:root:current mean train loss 6969.759005186987
INFO:root:current train perplexity239.9497833251953
INFO:root:current mean train loss 6966.192299623843
INFO:root:current train perplexity239.3399200439453
INFO:root:current mean train loss 6939.558493193069
INFO:root:current train perplexity238.2897491455078
INFO:root:current mean train loss 6935.151974916064
INFO:root:current train perplexity237.6527099609375
INFO:root:current mean train loss 6935.9863329731825
INFO:root:current train perplexity237.75437927246094
INFO:root:current mean train loss 6938.780291634317
INFO:root:current train perplexity237.83590698242188
INFO:root:current mean train loss 6933.095100461844
INFO:root:current train perplexity237.84339904785156
INFO:root:current mean train loss 6932.066139517257
INFO:root:current train perplexity238.19671630859375
INFO:root:current mean train loss 6928.725444092902
INFO:root:current train perplexity238.4238739013672
INFO:root:current mean train loss 6934.806093182702
INFO:root:current train perplexity238.87974548339844
INFO:root:current mean train loss 6939.412802696959
INFO:root:current train perplexity239.17333984375
INFO:root:current mean train loss 6938.415663923488
INFO:root:current train perplexity239.28878784179688
INFO:root:current mean train loss 6945.194533521076
INFO:root:current train perplexity239.86306762695312
INFO:root:current mean train loss 6942.225094918224
INFO:root:current train perplexity239.49996948242188
INFO:root:current mean train loss 6945.211044893237
INFO:root:current train perplexity239.3583984375
INFO:root:current mean train loss 6943.091293174342
INFO:root:current train perplexity239.03265380859375
INFO:root:current mean train loss 6940.326693989911
INFO:root:current train perplexity238.8596954345703

100%|██████████| 1/1 [07:53<00:00, 473.66s/it][A100%|██████████| 1/1 [07:53<00:00, 473.66s/it]
INFO:root:final mean train loss: 6937.4812012949915
INFO:root:final train perplexity: 238.66114807128906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.21s/it][A100%|██████████| 1/1 [00:42<00:00, 42.21s/it]
INFO:root:eval mean loss: 6649.193819952349
INFO:root:eval perplexity: 217.15821838378906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.39s/it][A100%|██████████| 1/1 [00:38<00:00, 38.39s/it]
INFO:root:eval mean loss: 6747.725099387744
INFO:root:eval perplexity: 256.5948181152344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/19
 10%|▉         | 19/200 [2:58:36<28:21:10, 563.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6881.120294744318
INFO:root:current train perplexity231.98361206054688
INFO:root:current mean train loss 6870.01813444544
INFO:root:current train perplexity233.7471160888672
INFO:root:current mean train loss 6896.580698374155
INFO:root:current train perplexity235.87942504882812
INFO:root:current mean train loss 6915.550888914499
INFO:root:current train perplexity237.04144287109375
INFO:root:current mean train loss 6930.213959752666
INFO:root:current train perplexity238.1410675048828
INFO:root:current mean train loss 6914.785319945821
INFO:root:current train perplexity238.267822265625
INFO:root:current mean train loss 6922.8498527306065
INFO:root:current train perplexity239.45361328125
INFO:root:current mean train loss 6918.742992961175
INFO:root:current train perplexity239.12442016601562
INFO:root:current mean train loss 6925.465651611922
INFO:root:current train perplexity239.31207275390625
INFO:root:current mean train loss 6937.242349024708
INFO:root:current train perplexity239.6973876953125
INFO:root:current mean train loss 6938.504532129097
INFO:root:current train perplexity239.62106323242188
INFO:root:current mean train loss 6940.642449309269
INFO:root:current train perplexity239.78518676757812
INFO:root:current mean train loss 6944.916821968341
INFO:root:current train perplexity240.14974975585938
INFO:root:current mean train loss 6947.292408076069
INFO:root:current train perplexity240.23599243164062
INFO:root:current mean train loss 6944.870444768591
INFO:root:current train perplexity240.07212829589844
INFO:root:current mean train loss 6946.910306391672
INFO:root:current train perplexity240.10691833496094
INFO:root:current mean train loss 6949.386975534159
INFO:root:current train perplexity240.44009399414062
INFO:root:current mean train loss 6948.244177487115
INFO:root:current train perplexity240.30026245117188
INFO:root:current mean train loss 6947.640271518678
INFO:root:current train perplexity240.37635803222656
INFO:root:current mean train loss 6947.874390791656
INFO:root:current train perplexity240.36398315429688

100%|██████████| 1/1 [07:52<00:00, 472.05s/it][A100%|██████████| 1/1 [07:52<00:00, 472.05s/it]
INFO:root:final mean train loss: 6946.667606294125
INFO:root:final train perplexity: 240.39767456054688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.36s/it][A100%|██████████| 1/1 [00:43<00:00, 43.36s/it]
INFO:root:eval mean loss: 6705.837284602172
INFO:root:eval perplexity: 227.34352111816406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.53s/it][A100%|██████████| 1/1 [00:41<00:00, 41.53s/it]
INFO:root:eval mean loss: 6805.193239036182
INFO:root:eval perplexity: 269.00897216796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/20
 10%|█         | 20/200 [3:07:55<28:07:45, 562.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6914.336901542468
INFO:root:current train perplexity242.1824188232422
INFO:root:current mean train loss 6901.215209082734
INFO:root:current train perplexity241.55996704101562
INFO:root:current mean train loss 6945.055206426517
INFO:root:current train perplexity241.60696411132812
INFO:root:current mean train loss 6959.660838979536
INFO:root:current train perplexity242.85093688964844
INFO:root:current mean train loss 6942.984505134183
INFO:root:current train perplexity242.31434631347656
INFO:root:current mean train loss 6950.255454436747
INFO:root:current train perplexity242.7288055419922
INFO:root:current mean train loss 6941.414095357737
INFO:root:current train perplexity242.3891143798828
INFO:root:current mean train loss 6950.061719014293
INFO:root:current train perplexity242.96885681152344
INFO:root:current mean train loss 6951.5976126014975
INFO:root:current train perplexity242.39649963378906
INFO:root:current mean train loss 6953.011875270401
INFO:root:current train perplexity242.01907348632812
INFO:root:current mean train loss 6952.633125488751
INFO:root:current train perplexity241.8905487060547
INFO:root:current mean train loss 6954.6494234937445
INFO:root:current train perplexity241.9486846923828
INFO:root:current mean train loss 6961.547978460452
INFO:root:current train perplexity241.86863708496094
INFO:root:current mean train loss 6956.1855884463685
INFO:root:current train perplexity241.52081298828125
INFO:root:current mean train loss 6949.283778611449
INFO:root:current train perplexity241.26988220214844
INFO:root:current mean train loss 6952.500764307688
INFO:root:current train perplexity241.04977416992188
INFO:root:current mean train loss 6950.517141382894
INFO:root:current train perplexity240.83229064941406
INFO:root:current mean train loss 6950.280668218085
INFO:root:current train perplexity240.90213012695312
INFO:root:current mean train loss 6949.619970092442
INFO:root:current train perplexity240.85720825195312
INFO:root:current mean train loss 6950.046226056843
INFO:root:current train perplexity240.79727172851562

100%|██████████| 1/1 [07:55<00:00, 475.83s/it][A100%|██████████| 1/1 [07:55<00:00, 475.83s/it]
INFO:root:final mean train loss: 6948.390520350716
INFO:root:final train perplexity: 240.72482299804688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.04s/it][A100%|██████████| 1/1 [00:40<00:00, 40.04s/it]
INFO:root:eval mean loss: 6685.720999210439
INFO:root:eval perplexity: 223.67271423339844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.53s/it][A100%|██████████| 1/1 [00:37<00:00, 37.53s/it]
INFO:root:eval mean loss: 6785.087044790282
INFO:root:eval perplexity: 264.5987854003906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/21
 10%|█         | 21/200 [3:17:11<27:52:22, 560.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6969.111432756697
INFO:root:current train perplexity240.2384033203125
INFO:root:current mean train loss 6940.239768003806
INFO:root:current train perplexity239.34767150878906
INFO:root:current mean train loss 6953.20164680481
INFO:root:current train perplexity239.50750732421875
INFO:root:current mean train loss 6925.011330593838
INFO:root:current train perplexity239.1468963623047
INFO:root:current mean train loss 6925.179145679139
INFO:root:current train perplexity239.2860565185547
INFO:root:current mean train loss 6932.76573916648
INFO:root:current train perplexity239.80816650390625
INFO:root:current mean train loss 6928.594402778439
INFO:root:current train perplexity239.47689819335938
INFO:root:current mean train loss 6933.452734245825
INFO:root:current train perplexity239.68162536621094
INFO:root:current mean train loss 6944.748892810857
INFO:root:current train perplexity239.7100830078125
INFO:root:current mean train loss 6940.972708346953
INFO:root:current train perplexity239.40919494628906
INFO:root:current mean train loss 6940.583209875858
INFO:root:current train perplexity239.18417358398438
INFO:root:current mean train loss 6944.181572620431
INFO:root:current train perplexity239.19696044921875
INFO:root:current mean train loss 6942.402009028538
INFO:root:current train perplexity239.1676483154297
INFO:root:current mean train loss 6939.704485417819
INFO:root:current train perplexity239.26368713378906
INFO:root:current mean train loss 6939.7405506175955
INFO:root:current train perplexity239.38784790039062
INFO:root:current mean train loss 6941.4554041688425
INFO:root:current train perplexity239.56427001953125
INFO:root:current mean train loss 6938.350064337542
INFO:root:current train perplexity239.45692443847656
INFO:root:current mean train loss 6937.63828508729
INFO:root:current train perplexity239.3829345703125
INFO:root:current mean train loss 6940.632299752071
INFO:root:current train perplexity239.52658081054688
INFO:root:current mean train loss 6943.478497401825
INFO:root:current train perplexity239.51583862304688

100%|██████████| 1/1 [07:36<00:00, 456.44s/it][A100%|██████████| 1/1 [07:36<00:00, 456.44s/it]
INFO:root:final mean train loss: 6942.200634593261
INFO:root:final train perplexity: 239.5517120361328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.81s/it][A100%|██████████| 1/1 [00:39<00:00, 39.81s/it]
INFO:root:eval mean loss: 6675.926896332004
INFO:root:eval perplexity: 221.90699768066406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.46s/it][A100%|██████████| 1/1 [00:38<00:00, 38.46s/it]
INFO:root:eval mean loss: 6776.789608786292
INFO:root:eval perplexity: 262.800048828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/22
 11%|█         | 22/200 [3:26:08<27:22:05, 553.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6934.98752541738
INFO:root:current train perplexity238.8413543701172
INFO:root:current mean train loss 6884.642256367413
INFO:root:current train perplexity238.99378967285156
INFO:root:current mean train loss 6907.134159297733
INFO:root:current train perplexity238.86880493164062
INFO:root:current mean train loss 6910.128526621146
INFO:root:current train perplexity238.3599853515625
INFO:root:current mean train loss 6923.412356096393
INFO:root:current train perplexity238.86083984375
INFO:root:current mean train loss 6923.268399596423
INFO:root:current train perplexity238.89808654785156
INFO:root:current mean train loss 6930.751226870125
INFO:root:current train perplexity238.7032470703125
INFO:root:current mean train loss 6933.8360853108825
INFO:root:current train perplexity238.6478271484375
INFO:root:current mean train loss 6928.668164509952
INFO:root:current train perplexity238.45103454589844
INFO:root:current mean train loss 6928.83709221239
INFO:root:current train perplexity238.09371948242188
INFO:root:current mean train loss 6931.606008453227
INFO:root:current train perplexity238.075439453125
INFO:root:current mean train loss 6930.209891004635
INFO:root:current train perplexity237.68399047851562
INFO:root:current mean train loss 6930.325808253142
INFO:root:current train perplexity237.74700927734375
INFO:root:current mean train loss 6925.958271690641
INFO:root:current train perplexity237.60557556152344
INFO:root:current mean train loss 6926.501769480864
INFO:root:current train perplexity237.73556518554688
INFO:root:current mean train loss 6927.943110422958
INFO:root:current train perplexity237.59561157226562
INFO:root:current mean train loss 6931.901832411835
INFO:root:current train perplexity237.82559204101562
INFO:root:current mean train loss 6933.537484467533
INFO:root:current train perplexity237.85984802246094
INFO:root:current mean train loss 6933.110811949413
INFO:root:current train perplexity237.8734588623047
INFO:root:current mean train loss 6935.713468247117
INFO:root:current train perplexity238.02821350097656

100%|██████████| 1/1 [07:44<00:00, 464.43s/it][A100%|██████████| 1/1 [07:44<00:00, 464.43s/it]
INFO:root:final mean train loss: 6934.248133672348
INFO:root:final train perplexity: 238.05307006835938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.88s/it][A100%|██████████| 1/1 [00:39<00:00, 39.88s/it]
INFO:root:eval mean loss: 6673.264456241689
INFO:root:eval perplexity: 221.42945861816406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.74s/it][A100%|██████████| 1/1 [00:36<00:00, 36.74s/it]
INFO:root:eval mean loss: 6779.136582827738
INFO:root:eval perplexity: 263.30755615234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/23
 12%|█▏        | 23/200 [3:35:12<27:04:05, 550.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6891.437868923611
INFO:root:current train perplexity235.13275146484375
INFO:root:current mean train loss 6852.065072471217
INFO:root:current train perplexity236.0574188232422
INFO:root:current mean train loss 6871.03818359375
INFO:root:current train perplexity237.31796264648438
INFO:root:current mean train loss 6880.848061899039
INFO:root:current train perplexity236.2877197265625
INFO:root:current mean train loss 6897.948854033801
INFO:root:current train perplexity236.8244171142578
INFO:root:current mean train loss 6909.969215936176
INFO:root:current train perplexity236.73770141601562
INFO:root:current mean train loss 6914.709631878397
INFO:root:current train perplexity236.236572265625
INFO:root:current mean train loss 6913.350680503362
INFO:root:current train perplexity235.82183837890625
INFO:root:current mean train loss 6892.615515273877
INFO:root:current train perplexity232.77854919433594
INFO:root:current mean train loss 6874.838783341224
INFO:root:current train perplexity228.23092651367188
INFO:root:current mean train loss 6851.11808656465
INFO:root:current train perplexity223.90065002441406
INFO:root:current mean train loss 6834.207684069722
INFO:root:current train perplexity220.38388061523438
INFO:root:current mean train loss 6819.369485828488
INFO:root:current train perplexity217.50694274902344
INFO:root:current mean train loss 6800.175461583858
INFO:root:current train perplexity214.85916137695312
INFO:root:current mean train loss 6788.500159592597
INFO:root:current train perplexity212.47132873535156
INFO:root:current mean train loss 6776.130687094635
INFO:root:current train perplexity210.40802001953125
INFO:root:current mean train loss 6767.4033766526445
INFO:root:current train perplexity208.68223571777344
INFO:root:current mean train loss 6761.72621442912
INFO:root:current train perplexity207.2345733642578
INFO:root:current mean train loss 6752.488857886905
INFO:root:current train perplexity205.84445190429688

100%|██████████| 1/1 [07:39<00:00, 459.39s/it][A100%|██████████| 1/1 [07:39<00:00, 459.39s/it]
INFO:root:final mean train loss: 6742.685963502269
INFO:root:final train perplexity: 204.65249633789062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.02s/it][A100%|██████████| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 6338.07410447141
INFO:root:eval perplexity: 168.824951171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.63s/it][A100%|██████████| 1/1 [00:37<00:00, 37.63s/it]
INFO:root:eval mean loss: 6468.392164297983
INFO:root:eval perplexity: 203.94505310058594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/24
 12%|█▏        | 24/200 [3:44:11<26:45:18, 547.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6192.793108258928
INFO:root:current train perplexity160.33230590820312
INFO:root:current mean train loss 6531.098057827103
INFO:root:current train perplexity178.54380798339844
INFO:root:current mean train loss 6545.160120867301
INFO:root:current train perplexity179.1646728515625
INFO:root:current mean train loss 6586.705628435464
INFO:root:current train perplexity181.75048828125
INFO:root:current mean train loss 6603.477611044994
INFO:root:current train perplexity182.74325561523438
INFO:root:current mean train loss 6604.945518598989
INFO:root:current train perplexity182.66441345214844
INFO:root:current mean train loss 6600.796047254685
INFO:root:current train perplexity182.4124298095703
INFO:root:current mean train loss 6601.127426212208
INFO:root:current train perplexity182.07334899902344
INFO:root:current mean train loss 6596.697222665931
INFO:root:current train perplexity181.98463439941406
INFO:root:current mean train loss 6600.296693576867
INFO:root:current train perplexity182.1405487060547
INFO:root:current mean train loss 6598.112137886358
INFO:root:current train perplexity182.2354278564453
INFO:root:current mean train loss 6599.616513962991
INFO:root:current train perplexity182.44940185546875
INFO:root:current mean train loss 6607.481169819931
INFO:root:current train perplexity182.9373321533203
INFO:root:current mean train loss 6607.211776208039
INFO:root:current train perplexity182.98635864257812
INFO:root:current mean train loss 6610.497611690432
INFO:root:current train perplexity183.13519287109375
INFO:root:current mean train loss 6611.318775078281
INFO:root:current train perplexity183.4040069580078
INFO:root:current mean train loss 6606.526940241911
INFO:root:current train perplexity183.4209747314453
INFO:root:current mean train loss 6607.777246208168
INFO:root:current train perplexity183.6820526123047
INFO:root:current mean train loss 6605.991532224941
INFO:root:current train perplexity183.66690063476562
INFO:root:current mean train loss 6607.8206248770975
INFO:root:current train perplexity183.8185577392578

100%|██████████| 1/1 [07:37<00:00, 457.87s/it][A100%|██████████| 1/1 [07:37<00:00, 457.87s/it]
INFO:root:final mean train loss: 6607.012602851779
INFO:root:final train perplexity: 183.87213134765625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.75s/it][A100%|██████████| 1/1 [00:40<00:00, 40.75s/it]
INFO:root:eval mean loss: 6350.665343805408
INFO:root:eval perplexity: 170.55398559570312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.41s/it][A100%|██████████| 1/1 [00:38<00:00, 38.41s/it]
INFO:root:eval mean loss: 6478.678764613807
INFO:root:eval perplexity: 205.67706298828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/25
 12%|█▎        | 25/200 [3:53:11<26:29:31, 544.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6902.632466634114
INFO:root:current train perplexity193.5108642578125
INFO:root:current mean train loss 6689.191819713962
INFO:root:current train perplexity187.6239776611328
INFO:root:current mean train loss 6678.173012869699
INFO:root:current train perplexity188.35580444335938
INFO:root:current mean train loss 6665.2457757643715
INFO:root:current train perplexity187.54696655273438
INFO:root:current mean train loss 6657.6111841741595
INFO:root:current train perplexity187.41233825683594
INFO:root:current mean train loss 6641.517783128578
INFO:root:current train perplexity186.53700256347656
INFO:root:current mean train loss 6633.447411952875
INFO:root:current train perplexity186.1630859375
INFO:root:current mean train loss 6626.763721782199
INFO:root:current train perplexity185.06170654296875
INFO:root:current mean train loss 6628.758941946678
INFO:root:current train perplexity184.93405151367188
INFO:root:current mean train loss 6636.060829063515
INFO:root:current train perplexity186.46726989746094
INFO:root:current mean train loss 6632.969519138336
INFO:root:current train perplexity186.44522094726562
INFO:root:current mean train loss 6625.755225565085
INFO:root:current train perplexity185.93260192871094
INFO:root:current mean train loss 6626.06311912786
INFO:root:current train perplexity185.68911743164062
INFO:root:current mean train loss 6618.7876673580295
INFO:root:current train perplexity185.3952178955078
INFO:root:current mean train loss 6618.940501009481
INFO:root:current train perplexity185.20533752441406
INFO:root:current mean train loss 6616.16622964854
INFO:root:current train perplexity185.0159149169922
INFO:root:current mean train loss 6616.624753754714
INFO:root:current train perplexity184.91941833496094
INFO:root:current mean train loss 6613.748394959487
INFO:root:current train perplexity184.7864990234375
INFO:root:current mean train loss 6614.6886955395075
INFO:root:current train perplexity184.8041229248047
INFO:root:current mean train loss 6615.749652061541
INFO:root:current train perplexity184.83038330078125

100%|██████████| 1/1 [07:37<00:00, 457.39s/it][A100%|██████████| 1/1 [07:37<00:00, 457.39s/it]
INFO:root:final mean train loss: 6613.619296490876
INFO:root:final train perplexity: 184.83311462402344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.32s/it][A100%|██████████| 1/1 [00:41<00:00, 41.33s/it]
INFO:root:eval mean loss: 6332.514440658245
INFO:root:eval perplexity: 168.06719970703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.00s/it][A100%|██████████| 1/1 [00:38<00:00, 38.00s/it]
INFO:root:eval mean loss: 6457.627796362478
INFO:root:eval perplexity: 202.1482696533203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/26
 13%|█▎        | 26/200 [4:02:10<26:15:19, 543.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6588.423851943598
INFO:root:current train perplexity186.04222106933594
INFO:root:current mean train loss 6581.723837128768
INFO:root:current train perplexity182.3433837890625
INFO:root:current mean train loss 6583.584152538252
INFO:root:current train perplexity182.57872009277344
INFO:root:current mean train loss 6601.226436491936
INFO:root:current train perplexity183.87625122070312
INFO:root:current mean train loss 6606.706616044856
INFO:root:current train perplexity184.1974334716797
INFO:root:current mean train loss 6612.896662177969
INFO:root:current train perplexity184.6775360107422
INFO:root:current mean train loss 6615.2439852281595
INFO:root:current train perplexity185.08587646484375
INFO:root:current mean train loss 6619.180223225582
INFO:root:current train perplexity184.99818420410156
INFO:root:current mean train loss 6619.262829430179
INFO:root:current train perplexity184.8932342529297
INFO:root:current mean train loss 6616.707213382539
INFO:root:current train perplexity184.97850036621094
INFO:root:current mean train loss 6610.729960299592
INFO:root:current train perplexity184.73794555664062
INFO:root:current mean train loss 6617.495649546724
INFO:root:current train perplexity184.98477172851562
INFO:root:current mean train loss 6617.678916322522
INFO:root:current train perplexity185.16171264648438
INFO:root:current mean train loss 6616.856646305229
INFO:root:current train perplexity185.11985778808594
INFO:root:current mean train loss 6617.568498980742
INFO:root:current train perplexity185.18431091308594
INFO:root:current mean train loss 6620.27391532487
INFO:root:current train perplexity185.29225158691406
INFO:root:current mean train loss 6620.446156652289
INFO:root:current train perplexity185.305419921875
INFO:root:current mean train loss 6620.560958590609
INFO:root:current train perplexity185.2694549560547
INFO:root:current mean train loss 6619.236394166299
INFO:root:current train perplexity185.17645263671875
INFO:root:current mean train loss 6617.9065309944135
INFO:root:current train perplexity185.2232666015625

100%|██████████| 1/1 [07:35<00:00, 455.06s/it][A100%|██████████| 1/1 [07:35<00:00, 455.06s/it]
INFO:root:final mean train loss: 6615.625883116844
INFO:root:final train perplexity: 185.12612915039062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.67s/it][A100%|██████████| 1/1 [00:40<00:00, 40.67s/it]
INFO:root:eval mean loss: 6331.6912105912015
INFO:root:eval perplexity: 167.9552764892578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.16s/it][A100%|██████████| 1/1 [00:38<00:00, 38.16s/it]
INFO:root:eval mean loss: 6456.868838479333
INFO:root:eval perplexity: 202.0221405029297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/27
 14%|█▎        | 27/200 [4:11:06<26:00:15, 541.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6540.6741732893315
INFO:root:current train perplexity181.66912841796875
INFO:root:current mean train loss 6573.462229282041
INFO:root:current train perplexity183.50022888183594
INFO:root:current mean train loss 6590.590169270833
INFO:root:current train perplexity184.52243041992188
INFO:root:current mean train loss 6595.171248963426
INFO:root:current train perplexity184.5091552734375
INFO:root:current mean train loss 6600.557351724549
INFO:root:current train perplexity184.8180389404297
INFO:root:current mean train loss 6610.885465669803
INFO:root:current train perplexity185.26504516601562
INFO:root:current mean train loss 6602.842425407247
INFO:root:current train perplexity185.15040588378906
INFO:root:current mean train loss 6612.166650132956
INFO:root:current train perplexity185.26194763183594
INFO:root:current mean train loss 6601.098497368517
INFO:root:current train perplexity184.77345275878906
INFO:root:current mean train loss 6603.883625962291
INFO:root:current train perplexity184.72262573242188
INFO:root:current mean train loss 6607.089744063091
INFO:root:current train perplexity184.87948608398438
INFO:root:current mean train loss 6614.264782103438
INFO:root:current train perplexity185.11094665527344
INFO:root:current mean train loss 6614.573130791062
INFO:root:current train perplexity185.11280822753906
INFO:root:current mean train loss 6615.704530516499
INFO:root:current train perplexity185.1829833984375
INFO:root:current mean train loss 6615.099238308042
INFO:root:current train perplexity185.28138732910156
INFO:root:current mean train loss 6613.806996963755
INFO:root:current train perplexity185.3013458251953
INFO:root:current mean train loss 6610.741779028291
INFO:root:current train perplexity185.23324584960938
INFO:root:current mean train loss 6615.051833637745
INFO:root:current train perplexity185.38380432128906
INFO:root:current mean train loss 6617.19507913415
INFO:root:current train perplexity185.47665405273438
INFO:root:current mean train loss 6618.686441392285
INFO:root:current train perplexity185.51849365234375

100%|██████████| 1/1 [07:29<00:00, 449.70s/it][A100%|██████████| 1/1 [07:29<00:00, 449.70s/it]
INFO:root:final mean train loss: 6618.230026268202
INFO:root:final train perplexity: 185.50698852539062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.58s/it][A100%|██████████| 1/1 [00:40<00:00, 40.58s/it]
INFO:root:eval mean loss: 6340.995118918994
INFO:root:eval perplexity: 169.22454833984375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.43s/it][A100%|██████████| 1/1 [00:38<00:00, 38.43s/it]
INFO:root:eval mean loss: 6465.463432582557
INFO:root:eval perplexity: 203.45452880859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/28
 14%|█▍        | 28/200 [4:19:57<25:42:42, 538.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6647.206412760416
INFO:root:current train perplexity187.63819885253906
INFO:root:current mean train loss 6616.384553571429
INFO:root:current train perplexity185.18978881835938
INFO:root:current mean train loss 6635.209337713069
INFO:root:current train perplexity186.95159912109375
INFO:root:current mean train loss 6639.2198203125
INFO:root:current train perplexity187.59034729003906
INFO:root:current mean train loss 6641.496714638158
INFO:root:current train perplexity187.48974609375
INFO:root:current mean train loss 6649.421028362772
INFO:root:current train perplexity187.62110900878906
INFO:root:current mean train loss 6644.598997395833
INFO:root:current train perplexity187.03514099121094
INFO:root:current mean train loss 6639.249750504032
INFO:root:current train perplexity186.5503387451172
INFO:root:current mean train loss 6629.408308035714
INFO:root:current train perplexity186.38104248046875
INFO:root:current mean train loss 6634.86028896234
INFO:root:current train perplexity186.3562469482422
INFO:root:current mean train loss 6636.045500090843
INFO:root:current train perplexity186.90577697753906
INFO:root:current mean train loss 6635.055129238697
INFO:root:current train perplexity187.287353515625
INFO:root:current mean train loss 6631.432757352941
INFO:root:current train perplexity187.28529357910156
INFO:root:current mean train loss 6636.500475142046
INFO:root:current train perplexity187.57550048828125
INFO:root:current mean train loss 6634.030622020657
INFO:root:current train perplexity187.6419677734375
INFO:root:current mean train loss 6633.722527591766
INFO:root:current train perplexity187.80551147460938
INFO:root:current mean train loss 6638.007393306903
INFO:root:current train perplexity188.14512634277344
INFO:root:current mean train loss 6640.875097381162
INFO:root:current train perplexity188.51681518554688
INFO:root:current mean train loss 6645.0052369791665
INFO:root:current train perplexity189.2030487060547
INFO:root:current mean train loss 6651.320040298655
INFO:root:current train perplexity190.15589904785156

100%|██████████| 1/1 [07:38<00:00, 458.94s/it][A100%|██████████| 1/1 [07:38<00:00, 458.94s/it]
INFO:root:final mean train loss: 6650.338664537238
INFO:root:final train perplexity: 190.26791381835938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.27s/it][A100%|██████████| 1/1 [00:40<00:00, 40.29s/it]
INFO:root:eval mean loss: 6641.600561696587
INFO:root:eval perplexity: 215.82777404785156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 36.97s/it][A100%|██████████| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 6765.695347129876
INFO:root:eval perplexity: 260.4138488769531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/29
 14%|█▍        | 29/200 [4:28:56<25:34:11, 538.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6933.6707232931385
INFO:root:current train perplexity242.35629272460938
INFO:root:current mean train loss 6950.551060994466
INFO:root:current train perplexity247.2133026123047
INFO:root:current mean train loss 6997.706452670162
INFO:root:current train perplexity256.7442626953125
INFO:root:current mean train loss 7017.585501534598
INFO:root:current train perplexity258.9662780761719
INFO:root:current mean train loss 6966.979374086954
INFO:root:current train perplexity246.97364807128906
INFO:root:current mean train loss 6927.463274156725
INFO:root:current train perplexity238.89967346191406
INFO:root:current mean train loss 6894.524354085757
INFO:root:current train perplexity232.75057983398438
INFO:root:current mean train loss 6864.743649261166
INFO:root:current train perplexity227.3291015625
INFO:root:current mean train loss 6842.510392398578
INFO:root:current train perplexity223.37982177734375
INFO:root:current mean train loss 6830.680031561083
INFO:root:current train perplexity220.3457489013672
INFO:root:current mean train loss 6817.095000214629
INFO:root:current train perplexity217.88343811035156
INFO:root:current mean train loss 6804.367891247641
INFO:root:current train perplexity215.6442413330078
INFO:root:current mean train loss 6793.121826927729
INFO:root:current train perplexity213.73960876464844
INFO:root:current mean train loss 6784.822588690396
INFO:root:current train perplexity212.53709411621094
INFO:root:current mean train loss 6779.190557648605
INFO:root:current train perplexity210.9959716796875
INFO:root:current mean train loss 6772.790069733433
INFO:root:current train perplexity209.5884246826172
INFO:root:current mean train loss 6764.496279885583
INFO:root:current train perplexity208.21080017089844
INFO:root:current mean train loss 6763.11186517988
INFO:root:current train perplexity207.5695343017578
INFO:root:current mean train loss 6757.643291449194
INFO:root:current train perplexity206.67178344726562

100%|██████████| 1/1 [07:36<00:00, 456.64s/it][A100%|██████████| 1/1 [07:36<00:00, 456.64s/it]
INFO:root:final mean train loss: 6749.477750084704
INFO:root:final train perplexity: 205.75230407714844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.64s/it][A100%|██████████| 1/1 [00:39<00:00, 39.64s/it]
INFO:root:eval mean loss: 6335.782635195035
INFO:root:eval perplexity: 168.5123291015625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.60s/it][A100%|██████████| 1/1 [00:37<00:00, 37.60s/it]
INFO:root:eval mean loss: 6461.469936938996
INFO:root:eval perplexity: 202.78768920898438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/30
 15%|█▌        | 30/200 [4:37:52<25:23:27, 537.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6516.92626953125
INFO:root:current train perplexity181.6317138671875
INFO:root:current mean train loss 6635.334405461583
INFO:root:current train perplexity185.22662353515625
INFO:root:current mean train loss 6614.253413296202
INFO:root:current train perplexity186.0675048828125
INFO:root:current mean train loss 6623.901959761833
INFO:root:current train perplexity186.13990783691406
INFO:root:current mean train loss 6614.586561879202
INFO:root:current train perplexity185.91619873046875
INFO:root:current mean train loss 6639.898967030943
INFO:root:current train perplexity188.11561584472656
INFO:root:current mean train loss 6632.664306239737
INFO:root:current train perplexity188.59999084472656
INFO:root:current mean train loss 6631.163361413523
INFO:root:current train perplexity188.68310546875
INFO:root:current mean train loss 6634.865830693758
INFO:root:current train perplexity189.01828002929688
INFO:root:current mean train loss 6645.388734185919
INFO:root:current train perplexity189.45050048828125
INFO:root:current mean train loss 6640.377309294475
INFO:root:current train perplexity189.3706512451172
INFO:root:current mean train loss 6640.058329135905
INFO:root:current train perplexity189.34230041503906
INFO:root:current mean train loss 6639.3025364777195
INFO:root:current train perplexity189.4060516357422
INFO:root:current mean train loss 6642.8357603162
INFO:root:current train perplexity189.56082153320312
INFO:root:current mean train loss 6642.477015087163
INFO:root:current train perplexity189.3256072998047
INFO:root:current mean train loss 6643.286927523402
INFO:root:current train perplexity189.18653869628906
INFO:root:current mean train loss 6642.999008264062
INFO:root:current train perplexity189.15036010742188
INFO:root:current mean train loss 6645.310538017938
INFO:root:current train perplexity189.50244140625
INFO:root:current mean train loss 6648.1514144231105
INFO:root:current train perplexity189.7866668701172
INFO:root:current mean train loss 6648.073318153729
INFO:root:current train perplexity189.76449584960938

100%|██████████| 1/1 [07:28<00:00, 448.45s/it][A100%|██████████| 1/1 [07:28<00:00, 448.45s/it]
INFO:root:final mean train loss: 6647.344695413948
INFO:root:final train perplexity: 189.8188018798828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.56s/it][A100%|██████████| 1/1 [00:38<00:00, 38.56s/it]
INFO:root:eval mean loss: 6390.272327612478
INFO:root:eval perplexity: 176.1088104248047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.58s/it][A100%|██████████| 1/1 [00:36<00:00, 36.58s/it]
INFO:root:eval mean loss: 6504.454602829954
INFO:root:eval perplexity: 210.0821990966797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/31
 16%|█▌        | 31/200 [4:46:39<25:04:51, 534.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6489.635667067308
INFO:root:current train perplexity188.90005493164062
INFO:root:current mean train loss 6664.86957077753
INFO:root:current train perplexity191.49420166015625
INFO:root:current mean train loss 6679.999801230641
INFO:root:current train perplexity191.90902709960938
INFO:root:current mean train loss 6667.674479665931
INFO:root:current train perplexity192.34373474121094
INFO:root:current mean train loss 6681.690669243325
INFO:root:current train perplexity193.2200469970703
INFO:root:current mean train loss 6689.215203927044
INFO:root:current train perplexity193.60223388671875
INFO:root:current mean train loss 6682.9151857028755
INFO:root:current train perplexity193.1337432861328
INFO:root:current mean train loss 6674.153807921186
INFO:root:current train perplexity192.62164306640625
INFO:root:current mean train loss 6671.655028114596
INFO:root:current train perplexity192.50302124023438
INFO:root:current mean train loss 6677.730137077315
INFO:root:current train perplexity192.77821350097656
INFO:root:current mean train loss 6680.327898943866
INFO:root:current train perplexity193.06155395507812
INFO:root:current mean train loss 6678.682785007077
INFO:root:current train perplexity193.31478881835938
INFO:root:current mean train loss 6687.794752609477
INFO:root:current train perplexity194.06964111328125
INFO:root:current mean train loss 6682.394396475537
INFO:root:current train perplexity193.96066284179688
INFO:root:current mean train loss 6683.818110783003
INFO:root:current train perplexity194.26795959472656
INFO:root:current mean train loss 6683.827214672244
INFO:root:current train perplexity194.3280029296875
INFO:root:current mean train loss 6676.687074780904
INFO:root:current train perplexity193.86080932617188
INFO:root:current mean train loss 6672.465885944742
INFO:root:current train perplexity193.25588989257812
INFO:root:current mean train loss 6668.027276096574
INFO:root:current train perplexity192.80828857421875
INFO:root:current mean train loss 6665.567745600905
INFO:root:current train perplexity192.48245239257812

100%|██████████| 1/1 [07:32<00:00, 452.59s/it][A100%|██████████| 1/1 [07:32<00:00, 452.59s/it]
INFO:root:final mean train loss: 6663.6246045488215
INFO:root:final train perplexity: 192.2733917236328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.82s/it][A100%|██████████| 1/1 [00:40<00:00, 40.82s/it]
INFO:root:eval mean loss: 6330.628916638962
INFO:root:eval perplexity: 167.81094360351562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.11s/it][A100%|██████████| 1/1 [00:39<00:00, 39.13s/it]
INFO:root:eval mean loss: 6456.955705791501
INFO:root:eval perplexity: 202.03648376464844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/32
 16%|█▌        | 32/200 [4:55:34<24:56:26, 534.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6640.368981649709
INFO:root:current train perplexity185.36480712890625
INFO:root:current mean train loss 6637.423090581294
INFO:root:current train perplexity184.11270141601562
INFO:root:current mean train loss 6633.015723460006
INFO:root:current train perplexity184.01290893554688
INFO:root:current mean train loss 6639.835503314049
INFO:root:current train perplexity184.87155151367188
INFO:root:current mean train loss 6622.215236138544
INFO:root:current train perplexity184.3741455078125
INFO:root:current mean train loss 6629.892618590297
INFO:root:current train perplexity184.87013244628906
INFO:root:current mean train loss 6630.386212243633
INFO:root:current train perplexity185.20091247558594
INFO:root:current mean train loss 6633.01295555392
INFO:root:current train perplexity185.67477416992188
INFO:root:current mean train loss 6624.816467647168
INFO:root:current train perplexity186.24246215820312
INFO:root:current mean train loss 6632.564598625563
INFO:root:current train perplexity187.0436248779297
INFO:root:current mean train loss 6632.066497539399
INFO:root:current train perplexity187.08482360839844
INFO:root:current mean train loss 6630.640475482557
INFO:root:current train perplexity187.20172119140625
INFO:root:current mean train loss 6628.562513748869
INFO:root:current train perplexity187.2515411376953
INFO:root:current mean train loss 6626.976737743159
INFO:root:current train perplexity187.32522583007812
INFO:root:current mean train loss 6626.9540785527115
INFO:root:current train perplexity187.36203002929688
INFO:root:current mean train loss 6627.798406930999
INFO:root:current train perplexity187.18984985351562
INFO:root:current mean train loss 6627.32963561083
INFO:root:current train perplexity187.13238525390625
INFO:root:current mean train loss 6627.679745208513
INFO:root:current train perplexity186.9575653076172
INFO:root:current mean train loss 6630.16776395271
INFO:root:current train perplexity186.96871948242188
INFO:root:current mean train loss 6630.594260395892
INFO:root:current train perplexity186.94491577148438

100%|██████████| 1/1 [07:35<00:00, 455.94s/it][A100%|██████████| 1/1 [07:35<00:00, 455.94s/it]
INFO:root:final mean train loss: 6627.884717363212
INFO:root:final train perplexity: 186.92584228515625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.94s/it][A100%|██████████| 1/1 [00:39<00:00, 39.94s/it]
INFO:root:eval mean loss: 6329.743820298648
INFO:root:eval perplexity: 167.6907958984375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.20s/it][A100%|██████████| 1/1 [00:37<00:00, 37.20s/it]
INFO:root:eval mean loss: 6455.794529691656
INFO:root:eval perplexity: 201.8437042236328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/33
 16%|█▋        | 33/200 [5:04:29<24:48:24, 534.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6534.735302734375
INFO:root:current train perplexity185.10009765625
INFO:root:current mean train loss 6595.430624389648
INFO:root:current train perplexity184.67074584960938
INFO:root:current mean train loss 6594.56562875601
INFO:root:current train perplexity184.62294006347656
INFO:root:current mean train loss 6601.758550347222
INFO:root:current train perplexity184.80853271484375
INFO:root:current mean train loss 6601.244828464674
INFO:root:current train perplexity184.8079071044922
INFO:root:current mean train loss 6598.35703125
INFO:root:current train perplexity184.86695861816406
INFO:root:current mean train loss 6587.959793738163
INFO:root:current train perplexity184.0452880859375
INFO:root:current mean train loss 6592.3417184930095
INFO:root:current train perplexity184.02044677734375
INFO:root:current mean train loss 6600.520284111555
INFO:root:current train perplexity184.35401916503906
INFO:root:current mean train loss 6604.996460469564
INFO:root:current train perplexity184.3291473388672
INFO:root:current mean train loss 6608.519512824292
INFO:root:current train perplexity184.08863830566406
INFO:root:current mean train loss 6604.112265119881
INFO:root:current train perplexity183.77203369140625
INFO:root:current mean train loss 6605.032264152405
INFO:root:current train perplexity183.64947509765625
INFO:root:current mean train loss 6610.5595358455885
INFO:root:current train perplexity183.65744018554688
INFO:root:current mean train loss 6610.8114568840965
INFO:root:current train perplexity183.55657958984375
INFO:root:current mean train loss 6609.191176194411
INFO:root:current train perplexity183.40042114257812
INFO:root:current mean train loss 6605.8239610786895
INFO:root:current train perplexity182.9114227294922
INFO:root:current mean train loss 6601.070136052912
INFO:root:current train perplexity182.48159790039062
INFO:root:current mean train loss 6598.557595923639
INFO:root:current train perplexity182.12908935546875
INFO:root:current mean train loss 6593.52222128109
INFO:root:current train perplexity181.69061279296875

100%|██████████| 1/1 [07:33<00:00, 453.08s/it][A100%|██████████| 1/1 [07:33<00:00, 453.08s/it]
INFO:root:final mean train loss: 6591.07749217962
INFO:root:final train perplexity: 181.57403564453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.58s/it][A100%|██████████| 1/1 [00:40<00:00, 40.58s/it]
INFO:root:eval mean loss: 6334.885898021941
INFO:root:eval perplexity: 168.38999938964844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.30s/it][A100%|██████████| 1/1 [00:37<00:00, 37.30s/it]
INFO:root:eval mean loss: 6475.922158964982
INFO:root:eval perplexity: 205.2115478515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/34
 17%|█▋        | 34/200 [5:13:23<24:38:27, 534.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6559.082703429383
INFO:root:current train perplexity174.41067504882812
INFO:root:current mean train loss 6543.485533633475
INFO:root:current train perplexity174.95066833496094
INFO:root:current mean train loss 6551.533522182423
INFO:root:current train perplexity176.20086669921875
INFO:root:current mean train loss 6553.184580673906
INFO:root:current train perplexity176.31097412109375
INFO:root:current mean train loss 6556.434284714033
INFO:root:current train perplexity176.27264404296875
INFO:root:current mean train loss 6554.585110722216
INFO:root:current train perplexity176.13526916503906
INFO:root:current mean train loss 6550.074355786097
INFO:root:current train perplexity175.88372802734375
INFO:root:current mean train loss 6545.320906355574
INFO:root:current train perplexity175.47100830078125
INFO:root:current mean train loss 6546.1498878679095
INFO:root:current train perplexity175.29119873046875
INFO:root:current mean train loss 6550.723215499456
INFO:root:current train perplexity175.1924285888672
INFO:root:current mean train loss 6550.5563305188025
INFO:root:current train perplexity175.13388061523438
INFO:root:current mean train loss 6551.681982048508
INFO:root:current train perplexity175.2362060546875
INFO:root:current mean train loss 6547.399744044269
INFO:root:current train perplexity175.19886779785156
INFO:root:current mean train loss 6545.7082010683325
INFO:root:current train perplexity174.96534729003906
INFO:root:current mean train loss 6547.5677051376315
INFO:root:current train perplexity175.20254516601562
INFO:root:current mean train loss 6549.657329977806
INFO:root:current train perplexity175.33099365234375
INFO:root:current mean train loss 6551.265490191283
INFO:root:current train perplexity175.49903869628906
INFO:root:current mean train loss 6547.086881638646
INFO:root:current train perplexity175.32321166992188
INFO:root:current mean train loss 6547.02048231886
INFO:root:current train perplexity175.34202575683594
INFO:root:current mean train loss 6549.327880982865
INFO:root:current train perplexity175.40298461914062

100%|██████████| 1/1 [07:29<00:00, 449.17s/it][A100%|██████████| 1/1 [07:29<00:00, 449.17s/it]
INFO:root:final mean train loss: 6547.00781520857
INFO:root:final train perplexity: 175.36752319335938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.18s/it][A100%|██████████| 1/1 [00:39<00:00, 39.19s/it]
INFO:root:eval mean loss: 6330.15955369016
INFO:root:eval perplexity: 167.7472686767578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.65s/it][A100%|██████████| 1/1 [00:36<00:00, 36.65s/it]
INFO:root:eval mean loss: 6471.159835923648
INFO:root:eval perplexity: 204.40965270996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/35
 18%|█▊        | 35/200 [5:22:10<24:23:43, 532.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6532.212890625
INFO:root:current train perplexity174.6765899658203
INFO:root:current mean train loss 6549.656446319265
INFO:root:current train perplexity176.1685333251953
INFO:root:current mean train loss 6551.157273065476
INFO:root:current train perplexity176.5871124267578
INFO:root:current mean train loss 6548.020516487548
INFO:root:current train perplexity175.96116638183594
INFO:root:current mean train loss 6547.398970260311
INFO:root:current train perplexity175.6986541748047
INFO:root:current mean train loss 6548.391713357533
INFO:root:current train perplexity175.29672241210938
INFO:root:current mean train loss 6548.034438603206
INFO:root:current train perplexity175.2377166748047
INFO:root:current mean train loss 6543.687160539987
INFO:root:current train perplexity174.987548828125
INFO:root:current mean train loss 6548.160651631537
INFO:root:current train perplexity175.0929718017578
INFO:root:current mean train loss 6549.234115631288
INFO:root:current train perplexity175.18800354003906
INFO:root:current mean train loss 6551.240024155193
INFO:root:current train perplexity175.2049560546875
INFO:root:current mean train loss 6555.414974449068
INFO:root:current train perplexity175.38392639160156
INFO:root:current mean train loss 6558.0665545456195
INFO:root:current train perplexity175.34454345703125
INFO:root:current mean train loss 6555.697358797749
INFO:root:current train perplexity175.27381896972656
INFO:root:current mean train loss 6552.31326510469
INFO:root:current train perplexity175.2545928955078
INFO:root:current mean train loss 6553.43342894742
INFO:root:current train perplexity175.3007354736328
INFO:root:current mean train loss 6555.995911581316
INFO:root:current train perplexity175.32931518554688
INFO:root:current mean train loss 6551.91293297537
INFO:root:current train perplexity175.23805236816406
INFO:root:current mean train loss 6549.233201217248
INFO:root:current train perplexity175.1680450439453

100%|██████████| 1/1 [07:29<00:00, 449.66s/it][A100%|██████████| 1/1 [07:29<00:00, 449.66s/it]
INFO:root:final mean train loss: 6544.831874029839
INFO:root:final train perplexity: 175.06675720214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.92s/it][A100%|██████████| 1/1 [00:38<00:00, 38.92s/it]
INFO:root:eval mean loss: 6324.431154075244
INFO:root:eval perplexity: 166.97142028808594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.07s/it][A100%|██████████| 1/1 [00:37<00:00, 37.09s/it]
INFO:root:eval mean loss: 6465.492024739583
INFO:root:eval perplexity: 203.45938110351562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/36
 18%|█▊        | 36/200 [5:30:58<24:11:22, 530.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6574.862615411932
INFO:root:current train perplexity170.13827514648438
INFO:root:current mean train loss 6631.45654296875
INFO:root:current train perplexity175.86695861816406
INFO:root:current mean train loss 6588.477515921209
INFO:root:current train perplexity174.88877868652344
INFO:root:current mean train loss 6577.555850896804
INFO:root:current train perplexity175.7957000732422
INFO:root:current mean train loss 6570.476177577555
INFO:root:current train perplexity175.52682495117188
INFO:root:current mean train loss 6565.9299865842095
INFO:root:current train perplexity175.7523651123047
INFO:root:current mean train loss 6553.518063209646
INFO:root:current train perplexity175.12669372558594
INFO:root:current mean train loss 6551.9236873406735
INFO:root:current train perplexity174.8904571533203
INFO:root:current mean train loss 6548.643174177328
INFO:root:current train perplexity174.57359313964844
INFO:root:current mean train loss 6553.4348442002265
INFO:root:current train perplexity174.76515197753906
INFO:root:current mean train loss 6558.891260103703
INFO:root:current train perplexity174.70066833496094
INFO:root:current mean train loss 6552.046793693041
INFO:root:current train perplexity174.4134979248047
INFO:root:current mean train loss 6552.500293130032
INFO:root:current train perplexity174.359619140625
INFO:root:current mean train loss 6560.05958409313
INFO:root:current train perplexity174.61329650878906
INFO:root:current mean train loss 6553.402047874402
INFO:root:current train perplexity174.40650939941406
INFO:root:current mean train loss 6545.915391943457
INFO:root:current train perplexity174.29095458984375
INFO:root:current mean train loss 6544.272106926016
INFO:root:current train perplexity174.23818969726562
INFO:root:current mean train loss 6545.818940404003
INFO:root:current train perplexity174.32054138183594
INFO:root:current mean train loss 6542.317360973306
INFO:root:current train perplexity174.18186950683594
INFO:root:current mean train loss 6540.528947336146
INFO:root:current train perplexity174.1641845703125

100%|██████████| 1/1 [07:34<00:00, 454.81s/it][A100%|██████████| 1/1 [07:34<00:00, 454.81s/it]
INFO:root:final mean train loss: 6537.185686489457
INFO:root:final train perplexity: 174.013427734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.03s/it][A100%|██████████| 1/1 [00:39<00:00, 39.03s/it]
INFO:root:eval mean loss: 6310.495148354388
INFO:root:eval perplexity: 165.0989532470703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.68s/it][A100%|██████████| 1/1 [00:36<00:00, 36.68s/it]
INFO:root:eval mean loss: 6451.330264260583
INFO:root:eval perplexity: 201.10435485839844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/37
 18%|█▊        | 37/200 [5:39:51<24:04:11, 531.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6545.014125279018
INFO:root:current train perplexity172.14593505859375
INFO:root:current mean train loss 6545.507732391357
INFO:root:current train perplexity173.1731719970703
INFO:root:current mean train loss 6519.387494003564
INFO:root:current train perplexity173.13906860351562
INFO:root:current mean train loss 6523.933705399676
INFO:root:current train perplexity173.97518920898438
INFO:root:current mean train loss 6517.640468704366
INFO:root:current train perplexity173.78846740722656
INFO:root:current mean train loss 6523.26581920277
INFO:root:current train perplexity173.74769592285156
INFO:root:current mean train loss 6528.733791084047
INFO:root:current train perplexity173.5825653076172
INFO:root:current mean train loss 6531.081791133671
INFO:root:current train perplexity173.68299865722656
INFO:root:current mean train loss 6535.419756166025
INFO:root:current train perplexity173.65972900390625
INFO:root:current mean train loss 6538.888863399111
INFO:root:current train perplexity173.77088928222656
INFO:root:current mean train loss 6535.538464497963
INFO:root:current train perplexity173.58761596679688
INFO:root:current mean train loss 6526.407602296654
INFO:root:current train perplexity173.2069549560547
INFO:root:current mean train loss 6530.203945694218
INFO:root:current train perplexity173.4329071044922
INFO:root:current mean train loss 6527.4121534968
INFO:root:current train perplexity173.34494018554688
INFO:root:current mean train loss 6529.363163282891
INFO:root:current train perplexity173.36386108398438
INFO:root:current mean train loss 6524.900263122239
INFO:root:current train perplexity173.25189208984375
INFO:root:current mean train loss 6528.560889091773
INFO:root:current train perplexity173.29237365722656
INFO:root:current mean train loss 6528.475790800871
INFO:root:current train perplexity173.24388122558594
INFO:root:current mean train loss 6531.71244801995
INFO:root:current train perplexity173.331298828125
INFO:root:current mean train loss 6533.345475699397
INFO:root:current train perplexity173.4122314453125

100%|██████████| 1/1 [07:30<00:00, 450.82s/it][A100%|██████████| 1/1 [07:30<00:00, 450.82s/it]
INFO:root:final mean train loss: 6532.666541703109
INFO:root:final train perplexity: 173.3938751220703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.93s/it][A100%|██████████| 1/1 [00:39<00:00, 39.93s/it]
INFO:root:eval mean loss: 6309.253338320035
INFO:root:eval perplexity: 164.9331512451172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.13s/it][A100%|██████████| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 6450.123397502493
INFO:root:eval perplexity: 200.90480041503906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/38
 19%|█▉        | 38/200 [5:48:41<23:54:15, 531.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6460.689051649306
INFO:root:current train perplexity169.0892791748047
INFO:root:current mean train loss 6497.545965786638
INFO:root:current train perplexity171.60716247558594
INFO:root:current mean train loss 6485.042576132016
INFO:root:current train perplexity171.5273895263672
INFO:root:current mean train loss 6499.720376188859
INFO:root:current train perplexity172.4318389892578
INFO:root:current mean train loss 6504.881944566362
INFO:root:current train perplexity172.62796020507812
INFO:root:current mean train loss 6507.453405425746
INFO:root:current train perplexity172.35012817382812
INFO:root:current mean train loss 6505.093023255814
INFO:root:current train perplexity172.50181579589844
INFO:root:current mean train loss 6509.261581113675
INFO:root:current train perplexity172.63059997558594
INFO:root:current mean train loss 6514.962583210059
INFO:root:current train perplexity172.7290802001953
INFO:root:current mean train loss 6508.936220134755
INFO:root:current train perplexity172.5626220703125
INFO:root:current mean train loss 6508.566570723684
INFO:root:current train perplexity172.56089782714844
INFO:root:current mean train loss 6514.783903776611
INFO:root:current train perplexity172.83346557617188
INFO:root:current mean train loss 6522.956488061622
INFO:root:current train perplexity173.0670928955078
INFO:root:current mean train loss 6526.795198507202
INFO:root:current train perplexity173.1707000732422
INFO:root:current mean train loss 6525.86260137327
INFO:root:current train perplexity173.07089233398438
INFO:root:current mean train loss 6526.81171875
INFO:root:current train perplexity173.12570190429688
INFO:root:current mean train loss 6530.017258144947
INFO:root:current train perplexity173.21414184570312
INFO:root:current mean train loss 6532.055461194932
INFO:root:current train perplexity173.31195068359375
INFO:root:current mean train loss 6532.320010797765
INFO:root:current train perplexity173.34385681152344
INFO:root:current mean train loss 6533.451280828245
INFO:root:current train perplexity173.37973022460938

100%|██████████| 1/1 [07:30<00:00, 450.95s/it][A100%|██████████| 1/1 [07:30<00:00, 450.95s/it]
INFO:root:final mean train loss: 6532.980249725203
INFO:root:final train perplexity: 173.43687438964844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.35s/it][A100%|██████████| 1/1 [00:39<00:00, 39.35s/it]
INFO:root:eval mean loss: 6309.72104076629
INFO:root:eval perplexity: 164.99569702148438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.35s/it][A100%|██████████| 1/1 [00:37<00:00, 37.35s/it]
INFO:root:eval mean loss: 6450.6272085203345
INFO:root:eval perplexity: 200.98806762695312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/39
 20%|█▉        | 39/200 [5:57:31<23:44:33, 530.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6519.084811302923
INFO:root:current train perplexity173.6959228515625
INFO:root:current mean train loss 6490.661416136189
INFO:root:current train perplexity173.218017578125
INFO:root:current mean train loss 6473.26365510198
INFO:root:current train perplexity172.10679626464844
INFO:root:current mean train loss 6469.569851195614
INFO:root:current train perplexity171.612548828125
INFO:root:current mean train loss 6485.527119690206
INFO:root:current train perplexity171.96446228027344
INFO:root:current mean train loss 6493.9991033696615
INFO:root:current train perplexity172.23806762695312
INFO:root:current mean train loss 6505.458094847527
INFO:root:current train perplexity172.6851806640625
INFO:root:current mean train loss 6504.8898522084155
INFO:root:current train perplexity172.80577087402344
INFO:root:current mean train loss 6510.440192910745
INFO:root:current train perplexity173.0963897705078
INFO:root:current mean train loss 6537.028067035635
INFO:root:current train perplexity176.04701232910156
INFO:root:current mean train loss 6570.460508069974
INFO:root:current train perplexity179.57650756835938
INFO:root:current mean train loss 6585.232896289399
INFO:root:current train perplexity182.2432403564453
INFO:root:current mean train loss 6605.189722414818
INFO:root:current train perplexity184.5791015625
INFO:root:current mean train loss 6613.659234896981
INFO:root:current train perplexity186.04541015625
INFO:root:current mean train loss 6626.154196346507
INFO:root:current train perplexity187.12095642089844
INFO:root:current mean train loss 6626.117718294854
INFO:root:current train perplexity186.97396850585938
INFO:root:current mean train loss 6624.452364080363
INFO:root:current train perplexity186.2222137451172
INFO:root:current mean train loss 6618.401534012309
INFO:root:current train perplexity185.44790649414062
INFO:root:current mean train loss 6613.202238384046
INFO:root:current train perplexity184.84219360351562
INFO:root:current mean train loss 6611.142742129762
INFO:root:current train perplexity184.3390655517578

100%|██████████| 1/1 [07:34<00:00, 454.27s/it][A100%|██████████| 1/1 [07:34<00:00, 454.27s/it]
INFO:root:final mean train loss: 6609.649842139632
INFO:root:final train perplexity: 184.25514221191406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.46s/it][A100%|██████████| 1/1 [00:40<00:00, 40.46s/it]
INFO:root:eval mean loss: 6309.437165821698
INFO:root:eval perplexity: 164.95777893066406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.47s/it][A100%|██████████| 1/1 [00:38<00:00, 38.47s/it]
INFO:root:eval mean loss: 6449.6403990400595
INFO:root:eval perplexity: 200.82501220703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/40
 20%|██        | 40/200 [6:06:27<23:39:38, 532.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6539.186424545095
INFO:root:current train perplexity172.54525756835938
INFO:root:current mean train loss 6523.20855610597
INFO:root:current train perplexity170.7702178955078
INFO:root:current mean train loss 6528.991392949149
INFO:root:current train perplexity171.82913208007812
INFO:root:current mean train loss 6537.100655507916
INFO:root:current train perplexity172.1527557373047
INFO:root:current mean train loss 6537.211271855428
INFO:root:current train perplexity172.44186401367188
INFO:root:current mean train loss 6530.073657943383
INFO:root:current train perplexity172.06411743164062
INFO:root:current mean train loss 6518.865888772551
INFO:root:current train perplexity172.02786254882812
INFO:root:current mean train loss 6516.550306131659
INFO:root:current train perplexity171.98660278320312
INFO:root:current mean train loss 6518.510092256826
INFO:root:current train perplexity172.19931030273438
INFO:root:current mean train loss 6524.525858956045
INFO:root:current train perplexity172.3729705810547
INFO:root:current mean train loss 6524.90001185632
INFO:root:current train perplexity172.56837463378906
INFO:root:current mean train loss 6522.988088256732
INFO:root:current train perplexity172.5099639892578
INFO:root:current mean train loss 6529.063222686791
INFO:root:current train perplexity172.9652862548828
INFO:root:current mean train loss 6532.837947986539
INFO:root:current train perplexity173.6488800048828
INFO:root:current mean train loss 6545.4050357346605
INFO:root:current train perplexity175.1704559326172
INFO:root:current mean train loss 6556.403496266921
INFO:root:current train perplexity176.65945434570312
INFO:root:current mean train loss 6567.159405652081
INFO:root:current train perplexity178.0928192138672
INFO:root:current mean train loss 6577.083386305948
INFO:root:current train perplexity179.13998413085938
INFO:root:current mean train loss 6580.720131427954
INFO:root:current train perplexity179.74588012695312
INFO:root:current mean train loss 6585.015205310053
INFO:root:current train perplexity180.40765380859375

100%|██████████| 1/1 [07:38<00:00, 458.98s/it][A100%|██████████| 1/1 [07:38<00:00, 458.98s/it]
INFO:root:final mean train loss: 6582.996724477393
INFO:root:final train perplexity: 180.41978454589844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.37s/it][A100%|██████████| 1/1 [00:40<00:00, 40.37s/it]
INFO:root:eval mean loss: 6415.042322902815
INFO:root:eval perplexity: 179.67449951171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.23s/it][A100%|██████████| 1/1 [00:38<00:00, 38.23s/it]
INFO:root:eval mean loss: 6538.323240456006
INFO:root:eval perplexity: 216.01380920410156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/41
 20%|██        | 41/200 [6:15:27<23:36:47, 534.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6644.274083455403
INFO:root:current train perplexity189.87872314453125
INFO:root:current mean train loss 6632.100346779337
INFO:root:current train perplexity190.717529296875
INFO:root:current mean train loss 6632.909699311128
INFO:root:current train perplexity190.43191528320312
INFO:root:current mean train loss 6612.483578460385
INFO:root:current train perplexity187.7769317626953
INFO:root:current mean train loss 6595.806018460182
INFO:root:current train perplexity184.91554260253906
INFO:root:current mean train loss 6593.113350068163
INFO:root:current train perplexity182.9205780029297
INFO:root:current mean train loss 6573.539553587464
INFO:root:current train perplexity181.3955078125
INFO:root:current mean train loss 6570.96910578282
INFO:root:current train perplexity180.39027404785156
INFO:root:current mean train loss 6568.176718575613
INFO:root:current train perplexity179.76121520996094
INFO:root:current mean train loss 6572.063143688033
INFO:root:current train perplexity179.45692443847656
INFO:root:current mean train loss 6568.988109282333
INFO:root:current train perplexity179.15357971191406
INFO:root:current mean train loss 6564.22572882917
INFO:root:current train perplexity178.6887664794922
INFO:root:current mean train loss 6559.756871352961
INFO:root:current train perplexity178.09027099609375
INFO:root:current mean train loss 6560.656445872135
INFO:root:current train perplexity177.85324096679688
INFO:root:current mean train loss 6557.508857931045
INFO:root:current train perplexity177.36001586914062
INFO:root:current mean train loss 6557.055552394169
INFO:root:current train perplexity177.1368865966797
INFO:root:current mean train loss 6557.019271274783
INFO:root:current train perplexity176.98558044433594
INFO:root:current mean train loss 6560.043277596047
INFO:root:current train perplexity177.1368865966797
INFO:root:current mean train loss 6557.463698246308
INFO:root:current train perplexity176.99014282226562

100%|██████████| 1/1 [07:48<00:00, 468.60s/it][A100%|██████████| 1/1 [07:48<00:00, 468.60s/it]
INFO:root:final mean train loss: 6558.237935045543
INFO:root:final train perplexity: 176.9287109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.97s/it][A100%|██████████| 1/1 [00:41<00:00, 41.97s/it]
INFO:root:eval mean loss: 6304.69032926086
INFO:root:eval perplexity: 164.3253936767578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.06s/it][A100%|██████████| 1/1 [00:40<00:00, 40.06s/it]
INFO:root:eval mean loss: 6445.468008054909
INFO:root:eval perplexity: 200.13739013671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/42
 21%|██        | 42/200 [6:24:40<23:42:35, 540.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6394.880784254808
INFO:root:current train perplexity171.140869140625
INFO:root:current mean train loss 6524.299895430033
INFO:root:current train perplexity171.77285766601562
INFO:root:current mean train loss 6548.585657827171
INFO:root:current train perplexity173.18267822265625
INFO:root:current mean train loss 6538.973642172524
INFO:root:current train perplexity172.803466796875
INFO:root:current mean train loss 6568.658839191132
INFO:root:current train perplexity175.83912658691406
INFO:root:current mean train loss 6571.125015229045
INFO:root:current train perplexity177.9527587890625
INFO:root:current mean train loss 6569.543029287316
INFO:root:current train perplexity178.5919189453125
INFO:root:current mean train loss 6569.7902602614395
INFO:root:current train perplexity178.80145263671875
INFO:root:current mean train loss 6559.310914437269
INFO:root:current train perplexity177.9687957763672
INFO:root:current mean train loss 6564.696667172953
INFO:root:current train perplexity177.76524353027344
INFO:root:current mean train loss 6565.134552574346
INFO:root:current train perplexity177.8396759033203
INFO:root:current mean train loss 6569.166869788157
INFO:root:current train perplexity178.94558715820312
INFO:root:current mean train loss 6578.415169888061
INFO:root:current train perplexity179.9533233642578
INFO:root:current mean train loss 6584.109294673458
INFO:root:current train perplexity180.35552978515625
INFO:root:current mean train loss 6590.347494180711
INFO:root:current train perplexity180.8513946533203
INFO:root:current mean train loss 6589.040685478458
INFO:root:current train perplexity180.98658752441406
INFO:root:current mean train loss 6588.1921073407475
INFO:root:current train perplexity180.982177734375
INFO:root:current mean train loss 6585.398571185877
INFO:root:current train perplexity180.66807556152344
INFO:root:current mean train loss 6586.952580161076
INFO:root:current train perplexity180.51280212402344
INFO:root:current mean train loss 6588.914761357325
INFO:root:current train perplexity180.80569458007812

100%|██████████| 1/1 [07:37<00:00, 457.58s/it][A100%|██████████| 1/1 [07:37<00:00, 457.58s/it]
INFO:root:final mean train loss: 6587.070881915225
INFO:root:final train perplexity: 181.00091552734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.57s/it][A100%|██████████| 1/1 [00:41<00:00, 41.57s/it]
INFO:root:eval mean loss: 6400.708548038564
INFO:root:eval perplexity: 177.60247802734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.38s/it][A100%|██████████| 1/1 [00:38<00:00, 38.38s/it]
INFO:root:eval mean loss: 6523.803554237312
INFO:root:eval perplexity: 213.45065307617188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/43
 22%|██▏       | 43/200 [6:33:40<23:33:29, 540.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6837.485400390625
INFO:root:current train perplexity193.81356811523438
INFO:root:current mean train loss 6634.5568997896635
INFO:root:current train perplexity182.21856689453125
INFO:root:current mean train loss 6600.783067255435
INFO:root:current train perplexity179.97984313964844
INFO:root:current mean train loss 6602.396445904356
INFO:root:current train perplexity182.5045623779297
INFO:root:current mean train loss 6605.804826035611
INFO:root:current train perplexity182.78953552246094
INFO:root:current mean train loss 6593.39216354658
INFO:root:current train perplexity182.33250427246094
INFO:root:current mean train loss 6592.534408327133
INFO:root:current train perplexity181.71063232421875
INFO:root:current mean train loss 6595.8819155340325
INFO:root:current train perplexity181.76930236816406
INFO:root:current mean train loss 6595.390947971574
INFO:root:current train perplexity181.4647979736328
INFO:root:current mean train loss 6595.464616410451
INFO:root:current train perplexity180.8505401611328
INFO:root:current mean train loss 6587.194111233313
INFO:root:current train perplexity180.2598419189453
INFO:root:current mean train loss 6581.882612002212
INFO:root:current train perplexity179.95606994628906
INFO:root:current mean train loss 6577.848651867378
INFO:root:current train perplexity179.43099975585938
INFO:root:current mean train loss 6575.934503127937
INFO:root:current train perplexity178.9183807373047
INFO:root:current mean train loss 6571.09053929469
INFO:root:current train perplexity178.47427368164062
INFO:root:current mean train loss 6572.331952422896
INFO:root:current train perplexity178.22093200683594
INFO:root:current mean train loss 6568.0368640361385
INFO:root:current train perplexity177.78482055664062
INFO:root:current mean train loss 6564.814056290643
INFO:root:current train perplexity177.4524688720703
INFO:root:current mean train loss 6564.61299601904
INFO:root:current train perplexity177.2299041748047
INFO:root:current mean train loss 6561.984515918475
INFO:root:current train perplexity177.07928466796875

100%|██████████| 1/1 [07:51<00:00, 471.76s/it][A100%|██████████| 1/1 [07:51<00:00, 471.76s/it]
INFO:root:final mean train loss: 6558.858966744673
INFO:root:final train perplexity: 177.0155487060547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.96s/it][A100%|██████████| 1/1 [00:41<00:00, 41.96s/it]
INFO:root:eval mean loss: 6302.439809812721
INFO:root:eval perplexity: 164.02635192871094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.48s/it][A100%|██████████| 1/1 [00:41<00:00, 41.48s/it]
INFO:root:eval mean loss: 6443.016744410738
INFO:root:eval perplexity: 199.73458862304688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/44
 22%|██▏       | 44/200 [6:42:58<23:38:20, 545.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6466.883280003324
INFO:root:current train perplexity170.84906005859375
INFO:root:current mean train loss 6514.48796569409
INFO:root:current train perplexity172.45782470703125
INFO:root:current mean train loss 6497.373728887273
INFO:root:current train perplexity171.53598022460938
INFO:root:current mean train loss 6505.8649402805295
INFO:root:current train perplexity171.91461181640625
INFO:root:current mean train loss 6509.209662725461
INFO:root:current train perplexity172.47886657714844
INFO:root:current mean train loss 6523.6020364988
INFO:root:current train perplexity172.88497924804688
INFO:root:current mean train loss 6519.016997017485
INFO:root:current train perplexity172.64212036132812
INFO:root:current mean train loss 6525.685016759748
INFO:root:current train perplexity173.13528442382812
INFO:root:current mean train loss 6528.963726525605
INFO:root:current train perplexity173.25436401367188
INFO:root:current mean train loss 6536.218349887804
INFO:root:current train perplexity173.1253662109375
INFO:root:current mean train loss 6536.6010485688275
INFO:root:current train perplexity173.04258728027344
INFO:root:current mean train loss 6540.495948585304
INFO:root:current train perplexity173.05982971191406
INFO:root:current mean train loss 6540.655869790622
INFO:root:current train perplexity173.01585388183594
INFO:root:current mean train loss 6538.286923777376
INFO:root:current train perplexity173.10316467285156
INFO:root:current mean train loss 6534.714709109904
INFO:root:current train perplexity172.99349975585938
INFO:root:current mean train loss 6534.150029543067
INFO:root:current train perplexity172.97361755371094
INFO:root:current mean train loss 6534.322088634164
INFO:root:current train perplexity172.94467163085938
INFO:root:current mean train loss 6533.661264735081
INFO:root:current train perplexity172.80470275878906
INFO:root:current mean train loss 6530.9929145024025
INFO:root:current train perplexity172.69390869140625
INFO:root:current mean train loss 6528.87256712049
INFO:root:current train perplexity172.5989990234375

100%|██████████| 1/1 [08:31<00:00, 511.24s/it][A100%|██████████| 1/1 [08:31<00:00, 511.24s/it]
INFO:root:final mean train loss: 6526.900273664035
INFO:root:final train perplexity: 172.6066436767578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.72s/it][A100%|██████████| 1/1 [00:47<00:00, 47.72s/it]
INFO:root:eval mean loss: 6301.593814065271
INFO:root:eval perplexity: 163.91415405273438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.67s/it][A100%|██████████| 1/1 [00:44<00:00, 44.67s/it]
INFO:root:eval mean loss: 6441.676413245235
INFO:root:eval perplexity: 199.51461791992188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/45
 22%|██▎       | 45/200 [6:53:05<24:16:28, 563.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6497.756439208984
INFO:root:current train perplexity170.43325805664062
INFO:root:current mean train loss 6541.80317204173
INFO:root:current train perplexity172.65554809570312
INFO:root:current mean train loss 6548.761125044389
INFO:root:current train perplexity172.80099487304688
INFO:root:current mean train loss 6545.335952255752
INFO:root:current train perplexity173.13172912597656
INFO:root:current mean train loss 6537.405438653354
INFO:root:current train perplexity172.98475646972656
INFO:root:current mean train loss 6532.0828710244905
INFO:root:current train perplexity172.68270874023438
INFO:root:current mean train loss 6522.318848391614
INFO:root:current train perplexity172.3015594482422
INFO:root:current mean train loss 6517.572082839087
INFO:root:current train perplexity172.11843872070312
INFO:root:current mean train loss 6521.682035658095
INFO:root:current train perplexity172.279052734375
INFO:root:current mean train loss 6522.575455661631
INFO:root:current train perplexity172.2786407470703
INFO:root:current mean train loss 6520.43769366042
INFO:root:current train perplexity172.17803955078125
INFO:root:current mean train loss 6523.27417034136
INFO:root:current train perplexity172.216552734375
INFO:root:current mean train loss 6522.857529265971
INFO:root:current train perplexity172.33665466308594
INFO:root:current mean train loss 6521.116912215336
INFO:root:current train perplexity172.23330688476562
INFO:root:current mean train loss 6523.011520302361
INFO:root:current train perplexity172.34658813476562
INFO:root:current mean train loss 6520.873867025156
INFO:root:current train perplexity172.21047973632812
INFO:root:current mean train loss 6523.181257101206
INFO:root:current train perplexity172.2745361328125
INFO:root:current mean train loss 6521.486340857958
INFO:root:current train perplexity172.24176025390625
INFO:root:current mean train loss 6524.533024210787
INFO:root:current train perplexity172.3269500732422
INFO:root:current mean train loss 6526.723618392789
INFO:root:current train perplexity172.363525390625

100%|██████████| 1/1 [08:53<00:00, 533.57s/it][A100%|██████████| 1/1 [08:53<00:00, 533.57s/it]
INFO:root:final mean train loss: 6525.126986120304
INFO:root:final train perplexity: 172.36524963378906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.26s/it][A100%|██████████| 1/1 [00:47<00:00, 47.26s/it]
INFO:root:eval mean loss: 6304.571798121676
INFO:root:eval perplexity: 164.3095703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.92s/it][A100%|██████████| 1/1 [00:46<00:00, 46.92s/it]
INFO:root:eval mean loss: 6444.402489195479
INFO:root:eval perplexity: 199.962158203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/46
 23%|██▎       | 46/200 [7:03:36<24:58:38, 583.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6534.678162374614
INFO:root:current train perplexity174.25430297851562
INFO:root:current mean train loss 6466.738853159531
INFO:root:current train perplexity171.6898193359375
INFO:root:current mean train loss 6510.61611015347
INFO:root:current train perplexity172.86585998535156
INFO:root:current mean train loss 6523.452030532317
INFO:root:current train perplexity173.1285858154297
INFO:root:current mean train loss 6516.30109898811
INFO:root:current train perplexity172.9898681640625
INFO:root:current mean train loss 6515.002594361822
INFO:root:current train perplexity172.7805633544922
INFO:root:current mean train loss 6522.192230090171
INFO:root:current train perplexity172.76144409179688
INFO:root:current mean train loss 6521.853621283811
INFO:root:current train perplexity172.6385040283203
INFO:root:current mean train loss 6530.984803978079
INFO:root:current train perplexity172.75881958007812
INFO:root:current mean train loss 6539.560956015864
INFO:root:current train perplexity172.87286376953125
INFO:root:current mean train loss 6541.838010775613
INFO:root:current train perplexity172.90501403808594
INFO:root:current mean train loss 6537.346727647386
INFO:root:current train perplexity172.72091674804688
INFO:root:current mean train loss 6531.320400931889
INFO:root:current train perplexity172.4482879638672
INFO:root:current mean train loss 6530.120476061844
INFO:root:current train perplexity172.43077087402344
INFO:root:current mean train loss 6530.037525782305
INFO:root:current train perplexity172.468505859375
INFO:root:current mean train loss 6528.962899890299
INFO:root:current train perplexity172.58639526367188
INFO:root:current mean train loss 6528.134653503309
INFO:root:current train perplexity172.54270935058594
INFO:root:current mean train loss 6528.430628695694
INFO:root:current train perplexity172.55357360839844
INFO:root:current mean train loss 6528.160808589597
INFO:root:current train perplexity172.48980712890625
INFO:root:current mean train loss 6527.905037800511
INFO:root:current train perplexity172.46768188476562

100%|██████████| 1/1 [08:23<00:00, 503.09s/it][A100%|██████████| 1/1 [08:23<00:00, 503.09s/it]
INFO:root:final mean train loss: 6525.8285469212915
INFO:root:final train perplexity: 172.46078491210938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.23s/it][A100%|██████████| 1/1 [00:43<00:00, 43.23s/it]
INFO:root:eval mean loss: 6303.380296639517
INFO:root:eval perplexity: 164.15122985839844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6443.169511510971
INFO:root:eval perplexity: 199.75955200195312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/47
 24%|██▎       | 47/200 [7:13:24<24:52:43, 585.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6557.138537348533
INFO:root:current train perplexity173.93951416015625
INFO:root:current mean train loss 6527.79007112137
INFO:root:current train perplexity173.30270385742188
INFO:root:current mean train loss 6534.726318359375
INFO:root:current train perplexity172.91136169433594
INFO:root:current mean train loss 6549.802684074671
INFO:root:current train perplexity173.12100219726562
INFO:root:current mean train loss 6553.932587772967
INFO:root:current train perplexity172.875
INFO:root:current mean train loss 6552.007418935515
INFO:root:current train perplexity172.7402801513672
INFO:root:current mean train loss 6541.44582596481
INFO:root:current train perplexity172.610107421875
INFO:root:current mean train loss 6543.3267831443845
INFO:root:current train perplexity172.5813751220703
INFO:root:current mean train loss 6540.361317250139
INFO:root:current train perplexity172.72364807128906
INFO:root:current mean train loss 6535.6641769867865
INFO:root:current train perplexity172.64056396484375
INFO:root:current mean train loss 6532.737586182975
INFO:root:current train perplexity172.40980529785156
INFO:root:current mean train loss 6526.984423909641
INFO:root:current train perplexity172.32333374023438
INFO:root:current mean train loss 6521.762848793817
INFO:root:current train perplexity172.15652465820312
INFO:root:current mean train loss 6524.631020738331
INFO:root:current train perplexity172.33599853515625
INFO:root:current mean train loss 6521.121804332861
INFO:root:current train perplexity172.1707305908203
INFO:root:current mean train loss 6523.289030721996
INFO:root:current train perplexity172.24192810058594
INFO:root:current mean train loss 6522.9739255511995
INFO:root:current train perplexity172.2439727783203
INFO:root:current mean train loss 6526.878705288863
INFO:root:current train perplexity172.39352416992188
INFO:root:current mean train loss 6526.960583251695
INFO:root:current train perplexity172.48060607910156

100%|██████████| 1/1 [08:03<00:00, 483.31s/it][A100%|██████████| 1/1 [08:03<00:00, 483.31s/it]
INFO:root:final mean train loss: 6525.784506070151
INFO:root:final train perplexity: 172.45469665527344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.43s/it][A100%|██████████| 1/1 [00:41<00:00, 41.43s/it]
INFO:root:eval mean loss: 6301.125197390293
INFO:root:eval perplexity: 163.85194396972656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.34s/it][A100%|██████████| 1/1 [00:45<00:00, 45.34s/it]
INFO:root:eval mean loss: 6439.821207682292
INFO:root:eval perplexity: 199.2104034423828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/48
 24%|██▍       | 48/200 [7:22:57<24:33:30, 581.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6616.457845052083
INFO:root:current train perplexity177.42506408691406
INFO:root:current mean train loss 6517.469688349185
INFO:root:current train perplexity174.64602661132812
INFO:root:current mean train loss 6512.247935592297
INFO:root:current train perplexity174.10372924804688
INFO:root:current mean train loss 6529.410728236607
INFO:root:current train perplexity173.99484252929688
INFO:root:current mean train loss 6525.155169898343
INFO:root:current train perplexity173.50247192382812
INFO:root:current mean train loss 6527.897980506675
INFO:root:current train perplexity173.59017944335938
INFO:root:current mean train loss 6533.83802242124
INFO:root:current train perplexity173.46971130371094
INFO:root:current mean train loss 6522.156629015516
INFO:root:current train perplexity173.14576721191406
INFO:root:current mean train loss 6529.544797258435
INFO:root:current train perplexity173.2646942138672
INFO:root:current mean train loss 6531.901864540642
INFO:root:current train perplexity173.2986602783203
INFO:root:current mean train loss 6527.135510795105
INFO:root:current train perplexity173.0079345703125
INFO:root:current mean train loss 6522.109461270319
INFO:root:current train perplexity172.82794189453125
INFO:root:current mean train loss 6525.678952064043
INFO:root:current train perplexity172.71524047851562
INFO:root:current mean train loss 6526.2352527923
INFO:root:current train perplexity172.8225860595703
INFO:root:current mean train loss 6528.528231283127
INFO:root:current train perplexity172.87664794921875
INFO:root:current mean train loss 6528.228190748762
INFO:root:current train perplexity172.7805633544922
INFO:root:current mean train loss 6530.736346870162
INFO:root:current train perplexity172.8528289794922
INFO:root:current mean train loss 6531.259866128371
INFO:root:current train perplexity172.8457489013672
INFO:root:current mean train loss 6530.925526751894
INFO:root:current train perplexity172.777099609375
INFO:root:current mean train loss 6530.737816171671
INFO:root:current train perplexity172.80140686035156

100%|██████████| 1/1 [07:53<00:00, 473.07s/it][A100%|██████████| 1/1 [07:53<00:00, 473.07s/it]
INFO:root:final mean train loss: 6528.2487860682995
INFO:root:final train perplexity: 172.79052734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.14s/it][A100%|██████████| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 6296.776573235262
INFO:root:eval perplexity: 163.27627563476562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.02s/it][A100%|██████████| 1/1 [00:38<00:00, 38.02s/it]
INFO:root:eval mean loss: 6434.622108405363
INFO:root:eval perplexity: 198.36073303222656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/49
 24%|██▍       | 49/200 [7:32:11<24:02:47, 573.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6593.617645263672
INFO:root:current train perplexity174.11294555664062
INFO:root:current mean train loss 6527.45585493608
INFO:root:current train perplexity172.12754821777344
INFO:root:current mean train loss 6566.700563628098
INFO:root:current train perplexity173.6297607421875
INFO:root:current mean train loss 6568.884806805347
INFO:root:current train perplexity173.4609375
INFO:root:current mean train loss 6546.538020381221
INFO:root:current train perplexity172.8972625732422
INFO:root:current mean train loss 6541.658900669643
INFO:root:current train perplexity172.88275146484375
INFO:root:current mean train loss 6539.571010927611
INFO:root:current train perplexity172.8326416015625
INFO:root:current mean train loss 6529.515912498932
INFO:root:current train perplexity172.6534881591797
INFO:root:current mean train loss 6530.739148066594
INFO:root:current train perplexity172.53431701660156
INFO:root:current mean train loss 6531.089533073196
INFO:root:current train perplexity172.7098846435547
INFO:root:current mean train loss 6531.6968942095145
INFO:root:current train perplexity172.4986114501953
INFO:root:current mean train loss 6533.106697648658
INFO:root:current train perplexity172.8390655517578
INFO:root:current mean train loss 6533.745463978161
INFO:root:current train perplexity172.84352111816406
INFO:root:current mean train loss 6528.350497592319
INFO:root:current train perplexity172.79580688476562
INFO:root:current mean train loss 6530.936532984899
INFO:root:current train perplexity172.97535705566406
INFO:root:current mean train loss 6530.4128440279255
INFO:root:current train perplexity173.03607177734375
INFO:root:current mean train loss 6532.844996433632
INFO:root:current train perplexity173.1737518310547
INFO:root:current mean train loss 6535.098879490383
INFO:root:current train perplexity173.2998046875
INFO:root:current mean train loss 6535.7981498085255
INFO:root:current train perplexity173.34864807128906
INFO:root:current mean train loss 6537.531540643601
INFO:root:current train perplexity173.5386199951172

100%|██████████| 1/1 [07:38<00:00, 458.20s/it][A100%|██████████| 1/1 [07:38<00:00, 458.20s/it]
INFO:root:final mean train loss: 6533.421974232145
INFO:root:final train perplexity: 173.49734497070312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.59s/it][A100%|██████████| 1/1 [00:40<00:00, 40.63s/it]
INFO:root:eval mean loss: 6292.142069065824
INFO:root:eval perplexity: 162.6651611328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.68s/it][A100%|██████████| 1/1 [00:37<00:00, 37.70s/it]
INFO:root:eval mean loss: 6426.440653050199
INFO:root:eval perplexity: 197.03097534179688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/50
 25%|██▌       | 50/200 [7:41:10<23:27:23, 562.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6487.165308115434
INFO:root:current train perplexity173.84283447265625
INFO:root:current mean train loss 6539.304176279363
INFO:root:current train perplexity176.2748260498047
INFO:root:current mean train loss 6568.909311072414
INFO:root:current train perplexity177.77947998046875
INFO:root:current mean train loss 6674.099316965885
INFO:root:current train perplexity191.87059020996094
INFO:root:current mean train loss 6726.623289384396
INFO:root:current train perplexity199.56979370117188
INFO:root:current mean train loss 6759.012604593579
INFO:root:current train perplexity205.13504028320312
INFO:root:current mean train loss 6776.313685718413
INFO:root:current train perplexity208.85675048828125
INFO:root:current mean train loss 6783.80422920665
INFO:root:current train perplexity211.22479248046875
INFO:root:current mean train loss 6799.856391250736
INFO:root:current train perplexity213.32296752929688
INFO:root:current mean train loss 6812.947909291854
INFO:root:current train perplexity215.21926879882812
INFO:root:current mean train loss 6821.273599019155
INFO:root:current train perplexity216.5467987060547
INFO:root:current mean train loss 6820.691533313615
INFO:root:current train perplexity217.44419860839844
INFO:root:current mean train loss 6831.629278813676
INFO:root:current train perplexity218.68344116210938
INFO:root:current mean train loss 6834.528779275158
INFO:root:current train perplexity219.4031982421875
INFO:root:current mean train loss 6843.609884173891
INFO:root:current train perplexity220.336181640625
INFO:root:current mean train loss 6845.620665045997
INFO:root:current train perplexity220.85275268554688
INFO:root:current mean train loss 6845.8519877103545
INFO:root:current train perplexity221.5067596435547
INFO:root:current mean train loss 6847.3013192248245
INFO:root:current train perplexity221.8241729736328
INFO:root:current mean train loss 6849.802320827981
INFO:root:current train perplexity222.25540161132812
INFO:root:current mean train loss 6851.997329359608
INFO:root:current train perplexity222.57643127441406

100%|██████████| 1/1 [07:41<00:00, 461.70s/it][A100%|██████████| 1/1 [07:41<00:00, 461.70s/it]
INFO:root:final mean train loss: 6850.04102682863
INFO:root:final train perplexity: 222.74716186523438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.57s/it][A100%|██████████| 1/1 [00:40<00:00, 40.57s/it]
INFO:root:eval mean loss: 6654.883929313497
INFO:root:eval perplexity: 218.16018676757812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.87s/it][A100%|██████████| 1/1 [00:37<00:00, 37.87s/it]
INFO:root:eval mean loss: 6745.00165184508
INFO:root:eval perplexity: 256.0208740234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/51
 26%|██▌       | 51/200 [7:50:12<23:02:43, 556.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6996.843676017992
INFO:root:current train perplexity232.13674926757812
INFO:root:current mean train loss 6930.2226033038405
INFO:root:current train perplexity230.30422973632812
INFO:root:current mean train loss 6910.957592957002
INFO:root:current train perplexity229.53765869140625
INFO:root:current mean train loss 6900.97989775444
INFO:root:current train perplexity229.49935913085938
INFO:root:current mean train loss 6888.799780587781
INFO:root:current train perplexity229.3531951904297
INFO:root:current mean train loss 6891.277482642723
INFO:root:current train perplexity229.21205139160156
INFO:root:current mean train loss 6892.470690661365
INFO:root:current train perplexity229.42593383789062
INFO:root:current mean train loss 6897.459992172201
INFO:root:current train perplexity229.8573760986328
INFO:root:current mean train loss 6894.713064286229
INFO:root:current train perplexity229.55418395996094
INFO:root:current mean train loss 6894.353011674269
INFO:root:current train perplexity229.67144775390625
INFO:root:current mean train loss 6895.561091954444
INFO:root:current train perplexity229.8140869140625
INFO:root:current mean train loss 6895.024894805961
INFO:root:current train perplexity229.95635986328125
INFO:root:current mean train loss 6895.156026300849
INFO:root:current train perplexity229.9505615234375
INFO:root:current mean train loss 6896.282540406524
INFO:root:current train perplexity229.93466186523438
INFO:root:current mean train loss 6892.858949002921
INFO:root:current train perplexity229.82186889648438
INFO:root:current mean train loss 6891.774700608138
INFO:root:current train perplexity229.82843017578125
INFO:root:current mean train loss 6889.860622667035
INFO:root:current train perplexity229.81396484375
INFO:root:current mean train loss 6890.4441822090885
INFO:root:current train perplexity229.7654266357422
INFO:root:current mean train loss 6891.915809950261
INFO:root:current train perplexity229.79457092285156
INFO:root:current mean train loss 6891.643329174084
INFO:root:current train perplexity229.97149658203125

100%|██████████| 1/1 [07:41<00:00, 461.50s/it][A100%|██████████| 1/1 [07:41<00:00, 461.50s/it]
INFO:root:final mean train loss: 6890.299608193079
INFO:root:final train perplexity: 229.9379425048828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.79s/it][A100%|██████████| 1/1 [00:40<00:00, 40.79s/it]
INFO:root:eval mean loss: 6647.75193927305
INFO:root:eval perplexity: 216.90476989746094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.59s/it][A100%|██████████| 1/1 [00:38<00:00, 38.59s/it]
INFO:root:eval mean loss: 6738.121228806516
INFO:root:eval perplexity: 254.57684326171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/52
 26%|██▌       | 52/200 [7:59:16<22:43:36, 552.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6883.954560429217
INFO:root:current train perplexity227.15562438964844
INFO:root:current mean train loss 6903.3452868852455
INFO:root:current train perplexity229.45448303222656
INFO:root:current mean train loss 6900.291088090768
INFO:root:current train perplexity229.37802124023438
INFO:root:current mean train loss 6905.788394459856
INFO:root:current train perplexity230.66140747070312
INFO:root:current mean train loss 6895.328351449275
INFO:root:current train perplexity230.44406127929688
INFO:root:current mean train loss 6891.97156745819
INFO:root:current train perplexity230.04299926757812
INFO:root:current mean train loss 6890.699192298453
INFO:root:current train perplexity229.90187072753906
INFO:root:current mean train loss 6887.077322422773
INFO:root:current train perplexity229.79763793945312
INFO:root:current mean train loss 6885.385275472466
INFO:root:current train perplexity229.60345458984375
INFO:root:current mean train loss 6878.401025440297
INFO:root:current train perplexity229.3924560546875
INFO:root:current mean train loss 6875.580730068386
INFO:root:current train perplexity229.3027801513672
INFO:root:current mean train loss 6872.756972144442
INFO:root:current train perplexity229.078857421875
INFO:root:current mean train loss 6874.391531155617
INFO:root:current train perplexity229.05450439453125
INFO:root:current mean train loss 6881.203240450447
INFO:root:current train perplexity229.26058959960938
INFO:root:current mean train loss 6880.531517352916
INFO:root:current train perplexity229.16571044921875
INFO:root:current mean train loss 6889.650365948753
INFO:root:current train perplexity229.52593994140625
INFO:root:current mean train loss 6890.969191571041
INFO:root:current train perplexity229.7032012939453
INFO:root:current mean train loss 6893.581015526413
INFO:root:current train perplexity229.7954559326172
INFO:root:current mean train loss 6893.098159830556
INFO:root:current train perplexity229.70408630371094
INFO:root:current mean train loss 6889.169654465299
INFO:root:current train perplexity229.73300170898438

100%|██████████| 1/1 [07:44<00:00, 464.18s/it][A100%|██████████| 1/1 [07:44<00:00, 464.18s/it]
INFO:root:final mean train loss: 6889.169654465299
INFO:root:final train perplexity: 229.73300170898438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.21s/it][A100%|██████████| 1/1 [00:41<00:00, 41.21s/it]
INFO:root:eval mean loss: 6644.12215688719
INFO:root:eval perplexity: 216.26869201660156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.69s/it][A100%|██████████| 1/1 [00:38<00:00, 38.69s/it]
INFO:root:eval mean loss: 6734.56330081588
INFO:root:eval perplexity: 253.8333282470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/53
 26%|██▋       | 53/200 [8:08:22<22:29:35, 550.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6945.885859375
INFO:root:current train perplexity230.42977905273438
INFO:root:current mean train loss 6893.001540527343
INFO:root:current train perplexity227.80731201171875
INFO:root:current mean train loss 6886.226604817708
INFO:root:current train perplexity229.1896514892578
INFO:root:current mean train loss 6910.525008544922
INFO:root:current train perplexity230.09423828125
INFO:root:current mean train loss 6907.24283984375
INFO:root:current train perplexity229.7954559326172
INFO:root:current mean train loss 6918.747693684896
INFO:root:current train perplexity230.26194763183594
INFO:root:current mean train loss 6915.215814034598
INFO:root:current train perplexity230.36550903320312
INFO:root:current mean train loss 6914.226729736328
INFO:root:current train perplexity230.1946563720703
INFO:root:current mean train loss 6909.066076931424
INFO:root:current train perplexity229.8061981201172
INFO:root:current mean train loss 6904.403185546875
INFO:root:current train perplexity229.7430877685547
INFO:root:current mean train loss 6905.655153586647
INFO:root:current train perplexity229.9716033935547
INFO:root:current mean train loss 6903.018664550781
INFO:root:current train perplexity229.91085815429688
INFO:root:current mean train loss 6901.360322265625
INFO:root:current train perplexity229.86712646484375
INFO:root:current mean train loss 6898.043333565848
INFO:root:current train perplexity229.84487915039062
INFO:root:current mean train loss 6895.207818033854
INFO:root:current train perplexity229.75656127929688
INFO:root:current mean train loss 6890.810027770996
INFO:root:current train perplexity229.47987365722656
INFO:root:current mean train loss 6888.161781652114
INFO:root:current train perplexity229.3455352783203
INFO:root:current mean train loss 6887.340600585938
INFO:root:current train perplexity229.34149169921875
INFO:root:current mean train loss 6889.13869217722
INFO:root:current train perplexity229.36358642578125

100%|██████████| 1/1 [07:45<00:00, 465.39s/it][A100%|██████████| 1/1 [07:45<00:00, 465.39s/it]
INFO:root:final mean train loss: 6886.455221063611
INFO:root:final train perplexity: 229.24134826660156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.22s/it][A100%|██████████| 1/1 [00:41<00:00, 41.22s/it]
INFO:root:eval mean loss: 6630.479130305297
INFO:root:eval perplexity: 213.89407348632812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.08s/it][A100%|██████████| 1/1 [00:39<00:00, 39.09s/it]
INFO:root:eval mean loss: 6721.633808108932
INFO:root:eval perplexity: 251.1493377685547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/54
 27%|██▋       | 54/200 [8:17:31<22:18:35, 550.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6660.139820772059
INFO:root:current train perplexity222.49960327148438
INFO:root:current mean train loss 6880.413824619391
INFO:root:current train perplexity227.09140014648438
INFO:root:current mean train loss 6897.194947976671
INFO:root:current train perplexity228.51710510253906
INFO:root:current mean train loss 6888.829748496648
INFO:root:current train perplexity228.13560485839844
INFO:root:current mean train loss 6876.395745512965
INFO:root:current train perplexity227.89793395996094
INFO:root:current mean train loss 6870.6950131089825
INFO:root:current train perplexity227.61264038085938
INFO:root:current mean train loss 6871.516475733134
INFO:root:current train perplexity227.81100463867188
INFO:root:current mean train loss 6868.706841930352
INFO:root:current train perplexity227.67027282714844
INFO:root:current mean train loss 6869.676986713012
INFO:root:current train perplexity227.6279296875
INFO:root:current mean train loss 6871.104927221068
INFO:root:current train perplexity227.52159118652344
INFO:root:current mean train loss 6874.221936551285
INFO:root:current train perplexity227.308837890625
INFO:root:current mean train loss 6877.8768219841095
INFO:root:current train perplexity227.39306640625
INFO:root:current mean train loss 6872.51254004147
INFO:root:current train perplexity227.096923828125
INFO:root:current mean train loss 6870.256832600726
INFO:root:current train perplexity227.03250122070312
INFO:root:current mean train loss 6869.092857172393
INFO:root:current train perplexity226.93130493164062
INFO:root:current mean train loss 6873.241950923389
INFO:root:current train perplexity227.218994140625
INFO:root:current mean train loss 6873.790376662028
INFO:root:current train perplexity227.18270874023438
INFO:root:current mean train loss 6875.603961249181
INFO:root:current train perplexity227.23828125
INFO:root:current mean train loss 6875.0863970272085
INFO:root:current train perplexity227.13633728027344
INFO:root:current mean train loss 6876.544836037347
INFO:root:current train perplexity227.2617950439453

100%|██████████| 1/1 [07:42<00:00, 462.54s/it][A100%|██████████| 1/1 [07:42<00:00, 462.54s/it]
INFO:root:final mean train loss: 6875.188750128042
INFO:root:final train perplexity: 227.21217346191406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.54s/it][A100%|██████████| 1/1 [00:40<00:00, 40.54s/it]
INFO:root:eval mean loss: 6560.859222628546
INFO:root:eval perplexity: 202.17709350585938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.96s/it][A100%|██████████| 1/1 [00:37<00:00, 37.96s/it]
INFO:root:eval mean loss: 6658.809057790337
INFO:root:eval perplexity: 238.5068817138672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/55
 28%|██▊       | 55/200 [8:26:34<22:04:49, 548.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6914.937930836397
INFO:root:current train perplexity227.7763671875
INFO:root:current mean train loss 6868.425704728312
INFO:root:current train perplexity226.10122680664062
INFO:root:current mean train loss 6856.5680588942305
INFO:root:current train perplexity225.26625061035156
INFO:root:current mean train loss 6852.44465755988
INFO:root:current train perplexity225.28709411621094
INFO:root:current mean train loss 6860.833676105271
INFO:root:current train perplexity225.5430145263672
INFO:root:current mean train loss 6855.212278901861
INFO:root:current train perplexity225.25465393066406
INFO:root:current mean train loss 6852.967283615931
INFO:root:current train perplexity225.3577880859375
INFO:root:current mean train loss 6873.988599896756
INFO:root:current train perplexity225.72850036621094
INFO:root:current mean train loss 6874.735033067296
INFO:root:current train perplexity225.69825744628906
INFO:root:current mean train loss 6878.256095151064
INFO:root:current train perplexity225.399169921875
INFO:root:current mean train loss 6877.13505415483
INFO:root:current train perplexity225.03199768066406
INFO:root:current mean train loss 6870.072494264633
INFO:root:current train perplexity224.7624969482422
INFO:root:current mean train loss 6867.865499882876
INFO:root:current train perplexity224.89801025390625
INFO:root:current mean train loss 6873.015257507965
INFO:root:current train perplexity225.3141632080078
INFO:root:current mean train loss 6870.274427342116
INFO:root:current train perplexity225.15274047851562
INFO:root:current mean train loss 6867.382983430268
INFO:root:current train perplexity224.7984161376953
INFO:root:current mean train loss 6861.91954146984
INFO:root:current train perplexity223.82080078125
INFO:root:current mean train loss 6853.893762221111
INFO:root:current train perplexity222.79623413085938
INFO:root:current mean train loss 6849.3858657221235
INFO:root:current train perplexity221.97251892089844
INFO:root:current mean train loss 6843.238558212012
INFO:root:current train perplexity221.1326446533203

100%|██████████| 1/1 [07:43<00:00, 464.00s/it][A100%|██████████| 1/1 [07:43<00:00, 464.00s/it]
INFO:root:final mean train loss: 6838.43813011181
INFO:root:final train perplexity: 220.7168426513672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.68s/it][A100%|██████████| 1/1 [00:41<00:00, 41.70s/it]
INFO:root:eval mean loss: 6464.101533064605
INFO:root:eval perplexity: 186.9509735107422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.43s/it][A100%|██████████| 1/1 [00:38<00:00, 38.43s/it]
INFO:root:eval mean loss: 6566.69327020307
INFO:root:eval perplexity: 221.1114501953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/56
 28%|██▊       | 56/200 [8:35:41<21:54:33, 547.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6680.38126148897
INFO:root:current train perplexity201.9631805419922
INFO:root:current mean train loss 6715.903536967095
INFO:root:current train perplexity203.80642700195312
INFO:root:current mean train loss 6724.145089563621
INFO:root:current train perplexity204.61444091796875
INFO:root:current mean train loss 6731.654779591791
INFO:root:current train perplexity204.826171875
INFO:root:current mean train loss 6736.281743694568
INFO:root:current train perplexity205.0871124267578
INFO:root:current mean train loss 6740.504038289757
INFO:root:current train perplexity205.27066040039062
INFO:root:current mean train loss 6734.660933299731
INFO:root:current train perplexity205.07537841796875
INFO:root:current mean train loss 6741.742864982107
INFO:root:current train perplexity205.0444793701172
INFO:root:current mean train loss 6750.515606639248
INFO:root:current train perplexity205.36964416503906
INFO:root:current mean train loss 6747.385770940129
INFO:root:current train perplexity205.38755798339844
INFO:root:current mean train loss 6746.9943287828555
INFO:root:current train perplexity205.26654052734375
INFO:root:current mean train loss 6745.140875716089
INFO:root:current train perplexity205.20205688476562
INFO:root:current mean train loss 6746.647451179681
INFO:root:current train perplexity205.03842163085938
INFO:root:current mean train loss 6746.10469060823
INFO:root:current train perplexity204.9954071044922
INFO:root:current mean train loss 6742.6880767843295
INFO:root:current train perplexity204.85586547851562
INFO:root:current mean train loss 6741.831685580875
INFO:root:current train perplexity204.5730743408203
INFO:root:current mean train loss 6744.234832819125
INFO:root:current train perplexity204.69680786132812
INFO:root:current mean train loss 6742.10104242888
INFO:root:current train perplexity204.59161376953125
INFO:root:current mean train loss 6741.858877749781
INFO:root:current train perplexity204.3169708251953
INFO:root:current mean train loss 6740.697494123607
INFO:root:current train perplexity204.2240447998047

100%|██████████| 1/1 [07:39<00:00, 459.98s/it][A100%|██████████| 1/1 [07:39<00:00, 459.98s/it]
INFO:root:final mean train loss: 6739.9589235552985
INFO:root:final train perplexity: 204.21246337890625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.95s/it][A100%|██████████| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 6453.45056931516
INFO:root:eval perplexity: 185.3465118408203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.41s/it][A100%|██████████| 1/1 [00:39<00:00, 39.43s/it]
INFO:root:eval mean loss: 6556.722787843529
INFO:root:eval perplexity: 219.3064422607422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/57
 28%|██▊       | 57/200 [8:44:45<21:42:43, 546.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6676.373405905331
INFO:root:current train perplexity201.7959747314453
INFO:root:current mean train loss 6661.918276832217
INFO:root:current train perplexity199.83099365234375
INFO:root:current mean train loss 6671.397380771921
INFO:root:current train perplexity199.19293212890625
INFO:root:current mean train loss 6698.559303615404
INFO:root:current train perplexity200.2762908935547
INFO:root:current mean train loss 6702.279514932225
INFO:root:current train perplexity200.2935791015625
INFO:root:current mean train loss 6703.749552981954
INFO:root:current train perplexity200.40133666992188
INFO:root:current mean train loss 6713.346452358954
INFO:root:current train perplexity200.6923370361328
INFO:root:current mean train loss 6708.1356321970625
INFO:root:current train perplexity200.589111328125
INFO:root:current mean train loss 6712.233292680731
INFO:root:current train perplexity200.64678955078125
INFO:root:current mean train loss 6715.880608172456
INFO:root:current train perplexity200.7559814453125
INFO:root:current mean train loss 6718.882124425767
INFO:root:current train perplexity200.73263549804688
INFO:root:current mean train loss 6715.574653102927
INFO:root:current train perplexity200.7100372314453
INFO:root:current mean train loss 6721.728929970919
INFO:root:current train perplexity200.9673614501953
INFO:root:current mean train loss 6719.501950983416
INFO:root:current train perplexity200.92214965820312
INFO:root:current mean train loss 6719.103596450847
INFO:root:current train perplexity201.07395935058594
INFO:root:current mean train loss 6722.87874245157
INFO:root:current train perplexity201.36448669433594
INFO:root:current mean train loss 6723.618978450052
INFO:root:current train perplexity201.5147247314453
INFO:root:current mean train loss 6725.491632107696
INFO:root:current train perplexity201.5445098876953
INFO:root:current mean train loss 6727.492227231665
INFO:root:current train perplexity201.564208984375
INFO:root:current mean train loss 6724.17967484637
INFO:root:current train perplexity201.42930603027344

100%|██████████| 1/1 [07:41<00:00, 461.56s/it][A100%|██████████| 1/1 [07:41<00:00, 461.56s/it]
INFO:root:final mean train loss: 6722.906614671978
INFO:root:final train perplexity: 201.48272705078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.91s/it][A100%|██████████| 1/1 [00:40<00:00, 40.91s/it]
INFO:root:eval mean loss: 6362.450235136857
INFO:root:eval perplexity: 172.1883087158203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.14s/it][A100%|██████████| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 6478.6887094484155
INFO:root:eval perplexity: 205.6788330078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/58
 29%|██▉       | 58/200 [8:53:48<21:31:14, 545.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6698.78067555147
INFO:root:current train perplexity202.8716278076172
INFO:root:current mean train loss 6651.973960092905
INFO:root:current train perplexity200.5588836669922
INFO:root:current mean train loss 6671.1765350877195
INFO:root:current train perplexity201.32598876953125
INFO:root:current mean train loss 6686.830239194399
INFO:root:current train perplexity201.22128295898438
INFO:root:current mean train loss 6687.890363240979
INFO:root:current train perplexity201.64390563964844
INFO:root:current mean train loss 6700.278054887821
INFO:root:current train perplexity201.70718383789062
INFO:root:current mean train loss 6706.314743242473
INFO:root:current train perplexity201.3997344970703
INFO:root:current mean train loss 6699.5607117087975
INFO:root:current train perplexity200.92587280273438
INFO:root:current mean train loss 6704.376235323976
INFO:root:current train perplexity201.0034942626953
INFO:root:current mean train loss 6706.21300810993
INFO:root:current train perplexity201.17532348632812
INFO:root:current mean train loss 6703.810194952477
INFO:root:current train perplexity201.02755737304688
INFO:root:current mean train loss 6708.480174133043
INFO:root:current train perplexity201.047119140625
INFO:root:current mean train loss 6710.287192211819
INFO:root:current train perplexity201.11903381347656
INFO:root:current mean train loss 6709.055079887748
INFO:root:current train perplexity201.0113525390625
INFO:root:current mean train loss 6710.125455729167
INFO:root:current train perplexity200.8590087890625
INFO:root:current mean train loss 6713.069790023659
INFO:root:current train perplexity200.92770385742188
INFO:root:current mean train loss 6720.352551233309
INFO:root:current train perplexity201.12266540527344
INFO:root:current mean train loss 6720.615445006128
INFO:root:current train perplexity200.96420288085938
INFO:root:current mean train loss 6719.859332777271
INFO:root:current train perplexity200.94302368164062

100%|██████████| 1/1 [07:39<00:00, 459.65s/it][A100%|██████████| 1/1 [07:39<00:00, 459.65s/it]
INFO:root:final mean train loss: 6719.268071577156
INFO:root:final train perplexity: 200.90489196777344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.23s/it][A100%|██████████| 1/1 [00:41<00:00, 41.23s/it]
INFO:root:eval mean loss: 6356.605794270833
INFO:root:eval perplexity: 171.37582397460938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.08s/it][A100%|██████████| 1/1 [00:39<00:00, 39.08s/it]
INFO:root:eval mean loss: 6480.2243245442705
INFO:root:eval perplexity: 205.93870544433594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/59
 30%|██▉       | 59/200 [9:02:50<21:19:47, 544.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6789.619873046875
INFO:root:current train perplexity213.82717895507812
INFO:root:current mean train loss 6724.804883769914
INFO:root:current train perplexity199.5494384765625
INFO:root:current mean train loss 6717.411195660582
INFO:root:current train perplexity200.2418212890625
INFO:root:current mean train loss 6731.309297069019
INFO:root:current train perplexity200.63780212402344
INFO:root:current mean train loss 6731.695917385728
INFO:root:current train perplexity200.24278259277344
INFO:root:current mean train loss 6731.122865958043
INFO:root:current train perplexity200.4044952392578
INFO:root:current mean train loss 6736.3998244783015
INFO:root:current train perplexity200.27342224121094
INFO:root:current mean train loss 6739.144175820201
INFO:root:current train perplexity200.29481506347656
INFO:root:current mean train loss 6743.815392548901
INFO:root:current train perplexity200.6422882080078
INFO:root:current mean train loss 6736.090232426206
INFO:root:current train perplexity200.49366760253906
INFO:root:current mean train loss 6727.315456002058
INFO:root:current train perplexity200.14120483398438
INFO:root:current mean train loss 6721.769366864933
INFO:root:current train perplexity199.80726623535156
INFO:root:current mean train loss 6717.387906548981
INFO:root:current train perplexity199.6331787109375
INFO:root:current mean train loss 6718.540354832709
INFO:root:current train perplexity199.79554748535156
INFO:root:current mean train loss 6718.102836140893
INFO:root:current train perplexity199.73126220703125
INFO:root:current mean train loss 6712.7359103877125
INFO:root:current train perplexity199.40713500976562
INFO:root:current mean train loss 6709.046633297733
INFO:root:current train perplexity199.11477661132812
INFO:root:current mean train loss 6707.701100153312
INFO:root:current train perplexity198.7908935546875
INFO:root:current mean train loss 6709.103332993723
INFO:root:current train perplexity198.7338409423828
INFO:root:current mean train loss 6703.826597259875
INFO:root:current train perplexity198.31646728515625

100%|██████████| 1/1 [07:39<00:00, 459.37s/it][A100%|██████████| 1/1 [07:39<00:00, 459.37s/it]
INFO:root:final mean train loss: 6701.757625854923
INFO:root:final train perplexity: 198.14764404296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.14s/it][A100%|██████████| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 6372.879082862367
INFO:root:eval perplexity: 173.64747619628906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.25s/it][A100%|██████████| 1/1 [00:38<00:00, 38.25s/it]
INFO:root:eval mean loss: 6493.33925504211
INFO:root:eval perplexity: 208.1711883544922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/60
 30%|███       | 60/200 [9:11:51<21:07:34, 543.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6665.622481496711
INFO:root:current train perplexity189.6240234375
INFO:root:current mean train loss 6686.376600249475
INFO:root:current train perplexity193.60186767578125
INFO:root:current mean train loss 6676.238214362157
INFO:root:current train perplexity192.82078552246094
INFO:root:current mean train loss 6676.962541634012
INFO:root:current train perplexity192.50062561035156
INFO:root:current mean train loss 6688.453708839872
INFO:root:current train perplexity193.16635131835938
INFO:root:current mean train loss 6686.810179017642
INFO:root:current train perplexity193.17869567871094
INFO:root:current mean train loss 6696.040089547153
INFO:root:current train perplexity193.8814239501953
INFO:root:current mean train loss 6702.17185394754
INFO:root:current train perplexity193.7353057861328
INFO:root:current mean train loss 6697.7900796035565
INFO:root:current train perplexity193.5484161376953
INFO:root:current mean train loss 6686.997007085657
INFO:root:current train perplexity192.9777069091797
INFO:root:current mean train loss 6678.354934946946
INFO:root:current train perplexity192.36610412597656
INFO:root:current mean train loss 6668.974442687389
INFO:root:current train perplexity191.826416015625
INFO:root:current mean train loss 6665.418118559013
INFO:root:current train perplexity191.5242462158203
INFO:root:current mean train loss 6665.16358236294
INFO:root:current train perplexity191.20632934570312
INFO:root:current mean train loss 6661.999802829348
INFO:root:current train perplexity190.75936889648438
INFO:root:current mean train loss 6661.170913545609
INFO:root:current train perplexity190.7193603515625
INFO:root:current mean train loss 6655.703542406578
INFO:root:current train perplexity190.4645233154297
INFO:root:current mean train loss 6654.481297322663
INFO:root:current train perplexity190.23025512695312
INFO:root:current mean train loss 6650.89245464541
INFO:root:current train perplexity190.0064239501953
INFO:root:current mean train loss 6650.020390258598
INFO:root:current train perplexity189.94320678710938

100%|██████████| 1/1 [07:33<00:00, 453.17s/it][A100%|██████████| 1/1 [07:33<00:00, 453.17s/it]
INFO:root:final mean train loss: 6647.563851453172
INFO:root:final train perplexity: 189.85174560546875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 6330.571370442708
INFO:root:eval perplexity: 167.8031768798828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.52s/it][A100%|██████████| 1/1 [00:37<00:00, 37.53s/it]
INFO:root:eval mean loss: 6455.622064252272
INFO:root:eval perplexity: 201.81512451171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/61
 30%|███       | 61/200 [9:20:43<20:50:50, 539.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6591.0679931640625
INFO:root:current train perplexity185.3306884765625
INFO:root:current mean train loss 6612.206970214844
INFO:root:current train perplexity188.08108520507812
INFO:root:current mean train loss 6629.345138291181
INFO:root:current train perplexity188.24554443359375
INFO:root:current mean train loss 6633.161722819011
INFO:root:current train perplexity188.0265655517578
INFO:root:current mean train loss 6630.706596724484
INFO:root:current train perplexity187.6298828125
INFO:root:current mean train loss 6641.436485176656
INFO:root:current train perplexity187.92132568359375
INFO:root:current mean train loss 6643.541543060878
INFO:root:current train perplexity187.97250366210938
INFO:root:current mean train loss 6642.011960236922
INFO:root:current train perplexity188.0746307373047
INFO:root:current mean train loss 6638.907667534203
INFO:root:current train perplexity187.99545288085938
INFO:root:current mean train loss 6639.777723002638
INFO:root:current train perplexity188.15481567382812
INFO:root:current mean train loss 6637.1799283414275
INFO:root:current train perplexity188.16342163085938
INFO:root:current mean train loss 6637.298512633418
INFO:root:current train perplexity188.1009063720703
INFO:root:current mean train loss 6638.586886409031
INFO:root:current train perplexity187.94390869140625
INFO:root:current mean train loss 6635.205041942482
INFO:root:current train perplexity187.8485870361328
INFO:root:current mean train loss 6633.2276964958
INFO:root:current train perplexity187.6018829345703
INFO:root:current mean train loss 6638.300366083781
INFO:root:current train perplexity187.75338745117188
INFO:root:current mean train loss 6639.338366669373
INFO:root:current train perplexity187.70191955566406
INFO:root:current mean train loss 6635.4874886367725
INFO:root:current train perplexity187.52139282226562
INFO:root:current mean train loss 6636.59452789905
INFO:root:current train perplexity187.55831909179688
INFO:root:current mean train loss 6635.864153144774
INFO:root:current train perplexity187.4711456298828

100%|██████████| 1/1 [07:33<00:00, 453.70s/it][A100%|██████████| 1/1 [07:33<00:00, 453.70s/it]
INFO:root:final mean train loss: 6631.13696190569
INFO:root:final train perplexity: 187.40625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.77s/it][A100%|██████████| 1/1 [00:39<00:00, 39.77s/it]
INFO:root:eval mean loss: 6331.43711214539
INFO:root:eval perplexity: 167.92076110839844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.17s/it][A100%|██████████| 1/1 [00:37<00:00, 37.17s/it]
INFO:root:eval mean loss: 6458.260060844692
INFO:root:eval perplexity: 202.2533721923828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/62
 31%|███       | 62/200 [9:29:36<20:37:07, 537.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6558.237359964623
INFO:root:current train perplexity184.85400390625
INFO:root:current mean train loss 6618.881893382353
INFO:root:current train perplexity185.86656188964844
INFO:root:current mean train loss 6633.534679548542
INFO:root:current train perplexity186.05401611328125
INFO:root:current mean train loss 6628.19188069892
INFO:root:current train perplexity186.4639892578125
INFO:root:current mean train loss 6629.775653628587
INFO:root:current train perplexity186.26039123535156
INFO:root:current mean train loss 6639.683284711235
INFO:root:current train perplexity186.7266387939453
INFO:root:current mean train loss 6633.667976975258
INFO:root:current train perplexity186.60443115234375
INFO:root:current mean train loss 6635.826723704142
INFO:root:current train perplexity186.6617431640625
INFO:root:current mean train loss 6636.58867370677
INFO:root:current train perplexity186.86959838867188
INFO:root:current mean train loss 6639.732901958454
INFO:root:current train perplexity187.02052307128906
INFO:root:current mean train loss 6644.001183374881
INFO:root:current train perplexity187.2791290283203
INFO:root:current mean train loss 6648.761790742899
INFO:root:current train perplexity187.6303253173828
INFO:root:current mean train loss 6645.401028157422
INFO:root:current train perplexity187.67239379882812
INFO:root:current mean train loss 6641.192590683897
INFO:root:current train perplexity187.532470703125
INFO:root:current mean train loss 6640.176835776196
INFO:root:current train perplexity187.41859436035156
INFO:root:current mean train loss 6637.627232951344
INFO:root:current train perplexity187.28118896484375
INFO:root:current mean train loss 6638.001147889255
INFO:root:current train perplexity187.28860473632812
INFO:root:current mean train loss 6634.137035450389
INFO:root:current train perplexity187.07688903808594
INFO:root:current mean train loss 6629.611690976204
INFO:root:current train perplexity186.94107055664062
INFO:root:current mean train loss 6630.3959963437665
INFO:root:current train perplexity187.05404663085938

100%|██████████| 1/1 [07:29<00:00, 449.51s/it][A100%|██████████| 1/1 [07:29<00:00, 449.51s/it]
INFO:root:final mean train loss: 6628.835901796127
INFO:root:final train perplexity: 187.06626892089844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.96s/it][A100%|██████████| 1/1 [00:39<00:00, 39.97s/it]
INFO:root:eval mean loss: 6331.03911271332
INFO:root:eval perplexity: 167.86656188964844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.88s/it][A100%|██████████| 1/1 [00:37<00:00, 37.89s/it]
INFO:root:eval mean loss: 6458.155390313331
INFO:root:eval perplexity: 202.2359161376953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/63
 32%|███▏      | 63/200 [9:38:26<20:22:35, 535.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6633.247049386161
INFO:root:current train perplexity188.43682861328125
INFO:root:current mean train loss 6636.999813304228
INFO:root:current train perplexity186.86630249023438
INFO:root:current mean train loss 6645.032420066551
INFO:root:current train perplexity187.64231872558594
INFO:root:current mean train loss 6640.710756703969
INFO:root:current train perplexity187.951171875
INFO:root:current mean train loss 6640.590540849402
INFO:root:current train perplexity188.1837921142578
INFO:root:current mean train loss 6643.243116091009
INFO:root:current train perplexity187.93450927734375
INFO:root:current mean train loss 6641.05635130014
INFO:root:current train perplexity187.91165161132812
INFO:root:current mean train loss 6644.904894226867
INFO:root:current train perplexity187.7478485107422
INFO:root:current mean train loss 6638.2487927667025
INFO:root:current train perplexity187.68161010742188
INFO:root:current mean train loss 6635.144051526257
INFO:root:current train perplexity187.4835662841797
INFO:root:current mean train loss 6636.332049047167
INFO:root:current train perplexity187.52210998535156
INFO:root:current mean train loss 6633.860914963942
INFO:root:current train perplexity187.49366760253906
INFO:root:current mean train loss 6634.011338505782
INFO:root:current train perplexity187.3907012939453
INFO:root:current mean train loss 6639.762931612112
INFO:root:current train perplexity187.47909545898438
INFO:root:current mean train loss 6633.228462146577
INFO:root:current train perplexity187.28387451171875
INFO:root:current mean train loss 6631.177684924861
INFO:root:current train perplexity187.23484802246094
INFO:root:current mean train loss 6632.238832101422
INFO:root:current train perplexity187.09437561035156
INFO:root:current mean train loss 6630.057958432645
INFO:root:current train perplexity187.09384155273438
INFO:root:current mean train loss 6630.435841149315
INFO:root:current train perplexity187.12721252441406
INFO:root:current mean train loss 6632.501171379283
INFO:root:current train perplexity187.23753356933594

100%|██████████| 1/1 [07:42<00:00, 462.23s/it][A100%|██████████| 1/1 [07:42<00:00, 462.23s/it]
INFO:root:final mean train loss: 6629.723355799688
INFO:root:final train perplexity: 187.197265625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.35s/it][A100%|██████████| 1/1 [00:41<00:00, 41.35s/it]
INFO:root:eval mean loss: 6332.570056238918
INFO:root:eval perplexity: 168.07473754882812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.74s/it][A100%|██████████| 1/1 [00:38<00:00, 38.74s/it]
INFO:root:eval mean loss: 6460.0217769974515
INFO:root:eval perplexity: 202.54647827148438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/64
 32%|███▏      | 64/200 [9:47:30<20:20:00, 538.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6565.133008934986
INFO:root:current train perplexity186.3434600830078
INFO:root:current mean train loss 6582.27669096758
INFO:root:current train perplexity185.94039916992188
INFO:root:current mean train loss 6599.106860436629
INFO:root:current train perplexity186.18101501464844
INFO:root:current mean train loss 6613.105173510175
INFO:root:current train perplexity186.7417755126953
INFO:root:current mean train loss 6611.631451929864
INFO:root:current train perplexity186.81649780273438
INFO:root:current mean train loss 6612.122878846359
INFO:root:current train perplexity186.91192626953125
INFO:root:current mean train loss 6624.7784063125455
INFO:root:current train perplexity187.1072235107422
INFO:root:current mean train loss 6623.753274648587
INFO:root:current train perplexity187.06385803222656
INFO:root:current mean train loss 6623.558654303481
INFO:root:current train perplexity187.05075073242188
INFO:root:current mean train loss 6624.6709142683
INFO:root:current train perplexity187.1243438720703
INFO:root:current mean train loss 6629.79421528217
INFO:root:current train perplexity187.2432403564453
INFO:root:current mean train loss 6634.315880535225
INFO:root:current train perplexity187.2635955810547
INFO:root:current mean train loss 6632.854148835106
INFO:root:current train perplexity187.07180786132812
INFO:root:current mean train loss 6632.287109727041
INFO:root:current train perplexity187.05271911621094
INFO:root:current mean train loss 6627.343228882082
INFO:root:current train perplexity186.91015625
INFO:root:current mean train loss 6626.519646320691
INFO:root:current train perplexity186.80990600585938
INFO:root:current mean train loss 6627.958502750815
INFO:root:current train perplexity186.85801696777344
INFO:root:current mean train loss 6629.908461064284
INFO:root:current train perplexity186.93153381347656
INFO:root:current mean train loss 6627.8539229141825
INFO:root:current train perplexity186.85720825195312

100%|██████████| 1/1 [07:44<00:00, 464.19s/it][A100%|██████████| 1/1 [07:44<00:00, 464.19s/it]
INFO:root:final mean train loss: 6627.979831497416
INFO:root:final train perplexity: 186.93992614746094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.82s/it][A100%|██████████| 1/1 [00:40<00:00, 40.82s/it]
INFO:root:eval mean loss: 6331.4811942459
INFO:root:eval perplexity: 167.92669677734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.11s/it][A100%|██████████| 1/1 [00:38<00:00, 38.11s/it]
INFO:root:eval mean loss: 6459.259337080286
INFO:root:eval perplexity: 202.4195098876953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/65
 32%|███▎      | 65/200 [9:56:36<20:15:54, 540.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6726.18212890625
INFO:root:current train perplexity188.572021484375
INFO:root:current mean train loss 6600.811622032752
INFO:root:current train perplexity186.3541259765625
INFO:root:current mean train loss 6618.640969669118
INFO:root:current train perplexity185.27996826171875
INFO:root:current mean train loss 6613.2592130962175
INFO:root:current train perplexity185.69577026367188
INFO:root:current mean train loss 6623.938604675897
INFO:root:current train perplexity185.6977996826172
INFO:root:current mean train loss 6611.61493501209
INFO:root:current train perplexity185.3762969970703
INFO:root:current mean train loss 6615.523139195727
INFO:root:current train perplexity185.8101043701172
INFO:root:current mean train loss 6616.935007962314
INFO:root:current train perplexity186.00379943847656
INFO:root:current mean train loss 6625.3132233121505
INFO:root:current train perplexity186.61288452148438
INFO:root:current mean train loss 6629.007655320969
INFO:root:current train perplexity186.74168395996094
INFO:root:current mean train loss 6631.0175377591195
INFO:root:current train perplexity187.0836639404297
INFO:root:current mean train loss 6632.201136492301
INFO:root:current train perplexity187.28172302246094
INFO:root:current mean train loss 6633.405934077165
INFO:root:current train perplexity187.42930603027344
INFO:root:current mean train loss 6634.182367055694
INFO:root:current train perplexity187.41725158691406
INFO:root:current mean train loss 6635.720921182225
INFO:root:current train perplexity187.56744384765625
INFO:root:current mean train loss 6638.724701252389
INFO:root:current train perplexity187.78579711914062
INFO:root:current mean train loss 6638.583774937656
INFO:root:current train perplexity187.7711181640625
INFO:root:current mean train loss 6633.275408104551
INFO:root:current train perplexity187.60475158691406
INFO:root:current mean train loss 6634.429049811183
INFO:root:current train perplexity187.5951690673828
INFO:root:current mean train loss 6634.661869081128
INFO:root:current train perplexity187.51754760742188

100%|██████████| 1/1 [07:41<00:00, 461.09s/it][A100%|██████████| 1/1 [07:41<00:00, 461.09s/it]
INFO:root:final mean train loss: 6632.182820207112
INFO:root:final train perplexity: 187.56109619140625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.11s/it][A100%|██████████| 1/1 [00:41<00:00, 41.11s/it]
INFO:root:eval mean loss: 6337.907571129765
INFO:root:eval perplexity: 168.80224609375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.78s/it][A100%|██████████| 1/1 [00:38<00:00, 38.78s/it]
INFO:root:eval mean loss: 6464.9181427651265
INFO:root:eval perplexity: 203.3633575439453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/66
 33%|███▎      | 66/200 [10:05:39<20:08:54, 541.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6734.369512648809
INFO:root:current train perplexity190.58407592773438
INFO:root:current mean train loss 6619.455622901602
INFO:root:current train perplexity188.4364776611328
INFO:root:current mean train loss 6647.748479920814
INFO:root:current train perplexity189.11717224121094
INFO:root:current mean train loss 6635.94591638678
INFO:root:current train perplexity188.83062744140625
INFO:root:current mean train loss 6644.337118189578
INFO:root:current train perplexity188.5098114013672
INFO:root:current mean train loss 6630.433776504019
INFO:root:current train perplexity188.31260681152344
INFO:root:current mean train loss 6627.276480412138
INFO:root:current train perplexity187.92581176757812
INFO:root:current mean train loss 6636.200611130374
INFO:root:current train perplexity188.39622497558594
INFO:root:current mean train loss 6635.061103551309
INFO:root:current train perplexity188.56015014648438
INFO:root:current mean train loss 6640.83434965815
INFO:root:current train perplexity188.77049255371094
INFO:root:current mean train loss 6639.320149420758
INFO:root:current train perplexity188.68292236328125
INFO:root:current mean train loss 6637.371420867948
INFO:root:current train perplexity188.70883178710938
INFO:root:current mean train loss 6639.010886952293
INFO:root:current train perplexity188.7371826171875
INFO:root:current mean train loss 6637.39991010598
INFO:root:current train perplexity188.50396728515625
INFO:root:current mean train loss 6640.956798276961
INFO:root:current train perplexity188.71180725097656
INFO:root:current mean train loss 6643.130563388601
INFO:root:current train perplexity188.64442443847656
INFO:root:current mean train loss 6638.8758578809375
INFO:root:current train perplexity188.5723876953125
INFO:root:current mean train loss 6638.425868068165
INFO:root:current train perplexity188.5346221923828
INFO:root:current mean train loss 6642.305447674269
INFO:root:current train perplexity188.63973999023438
INFO:root:current mean train loss 6643.123779932327
INFO:root:current train perplexity188.69137573242188

100%|██████████| 1/1 [07:41<00:00, 461.33s/it][A100%|██████████| 1/1 [07:41<00:00, 461.33s/it]
INFO:root:final mean train loss: 6638.461385891405
INFO:root:final train perplexity: 188.49273681640625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.19s/it][A100%|██████████| 1/1 [00:41<00:00, 41.19s/it]
INFO:root:eval mean loss: 6339.48905695922
INFO:root:eval perplexity: 169.0184326171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.90s/it][A100%|██████████| 1/1 [00:38<00:00, 38.90s/it]
INFO:root:eval mean loss: 6464.507573553857
INFO:root:eval perplexity: 203.29481506347656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/67
 34%|███▎      | 67/200 [10:14:43<20:01:35, 542.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6636.8060109991775
INFO:root:current train perplexity186.1568603515625
INFO:root:current mean train loss 6647.572113479393
INFO:root:current train perplexity186.44451904296875
INFO:root:current mean train loss 6643.6539891347165
INFO:root:current train perplexity187.53712463378906
INFO:root:current mean train loss 6633.855015139608
INFO:root:current train perplexity187.27252197265625
INFO:root:current mean train loss 6648.160101624929
INFO:root:current train perplexity188.15858459472656
INFO:root:current mean train loss 6641.7401644908805
INFO:root:current train perplexity187.67506408691406
INFO:root:current mean train loss 6635.3887017229135
INFO:root:current train perplexity187.43556213378906
INFO:root:current mean train loss 6628.711077103447
INFO:root:current train perplexity187.33148193359375
INFO:root:current mean train loss 6629.144797532257
INFO:root:current train perplexity187.39767456054688
INFO:root:current mean train loss 6633.967568859109
INFO:root:current train perplexity187.5862274169922
INFO:root:current mean train loss 6636.342056068612
INFO:root:current train perplexity187.8607635498047
INFO:root:current mean train loss 6640.536984086665
INFO:root:current train perplexity188.00082397460938
INFO:root:current mean train loss 6640.649333996997
INFO:root:current train perplexity187.86569213867188
INFO:root:current mean train loss 6641.308136853046
INFO:root:current train perplexity187.72430419921875
INFO:root:current mean train loss 6641.544297092316
INFO:root:current train perplexity187.79440307617188
INFO:root:current mean train loss 6640.424715793644
INFO:root:current train perplexity187.8193817138672
INFO:root:current mean train loss 6637.486635760073
INFO:root:current train perplexity187.80523681640625
INFO:root:current mean train loss 6634.435657005179
INFO:root:current train perplexity187.70388793945312
INFO:root:current mean train loss 6634.353688569012
INFO:root:current train perplexity187.66836547851562
INFO:root:current mean train loss 6634.507945782137
INFO:root:current train perplexity187.64053344726562

100%|██████████| 1/1 [07:42<00:00, 462.59s/it][A100%|██████████| 1/1 [07:42<00:00, 462.60s/it]
INFO:root:final mean train loss: 6632.881802326612
INFO:root:final train perplexity: 187.6644287109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.62s/it][A100%|██████████| 1/1 [00:40<00:00, 40.62s/it]
INFO:root:eval mean loss: 6335.0965428994905
INFO:root:eval perplexity: 168.4187469482422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.34s/it][A100%|██████████| 1/1 [00:38<00:00, 38.34s/it]
INFO:root:eval mean loss: 6459.848223314218
INFO:root:eval perplexity: 202.51759338378906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/68
 34%|███▍      | 68/200 [10:23:47<19:53:51, 542.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6680.197327769886
INFO:root:current train perplexity189.86767578125
INFO:root:current mean train loss 6635.072996471774
INFO:root:current train perplexity186.00389099121094
INFO:root:current mean train loss 6622.782877604167
INFO:root:current train perplexity185.92771911621094
INFO:root:current mean train loss 6629.009105413732
INFO:root:current train perplexity186.97183227539062
INFO:root:current mean train loss 6635.604516869849
INFO:root:current train perplexity186.992431640625
INFO:root:current mean train loss 6627.693560846003
INFO:root:current train perplexity186.5807647705078
INFO:root:current mean train loss 6627.972384154342
INFO:root:current train perplexity186.80064392089844
INFO:root:current mean train loss 6630.78325227649
INFO:root:current train perplexity187.0862579345703
INFO:root:current mean train loss 6641.8346165707235
INFO:root:current train perplexity187.11766052246094
INFO:root:current mean train loss 6652.078913919339
INFO:root:current train perplexity187.2900390625
INFO:root:current mean train loss 6649.690609263922
INFO:root:current train perplexity187.0204315185547
INFO:root:current mean train loss 6647.2200453192645
INFO:root:current train perplexity187.08982849121094
INFO:root:current mean train loss 6646.349203965388
INFO:root:current train perplexity187.2042236328125
INFO:root:current mean train loss 6641.6406001354935
INFO:root:current train perplexity187.0836639404297
INFO:root:current mean train loss 6633.888617509665
INFO:root:current train perplexity187.06118774414062
INFO:root:current mean train loss 6631.153265361234
INFO:root:current train perplexity186.9630126953125
INFO:root:current mean train loss 6632.939346617731
INFO:root:current train perplexity187.16995239257812
INFO:root:current mean train loss 6631.801727764423
INFO:root:current train perplexity187.21896362304688
INFO:root:current mean train loss 6631.672462253622
INFO:root:current train perplexity187.25770568847656
INFO:root:current mean train loss 6630.446618995764
INFO:root:current train perplexity187.2886962890625

100%|██████████| 1/1 [07:40<00:00, 460.69s/it][A100%|██████████| 1/1 [07:40<00:00, 460.69s/it]
INFO:root:final mean train loss: 6630.785177795441
INFO:root:final train perplexity: 187.35433959960938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.25s/it][A100%|██████████| 1/1 [00:41<00:00, 41.25s/it]
INFO:root:eval mean loss: 6333.865902731604
INFO:root:eval perplexity: 168.2511444091797
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.85s/it][A100%|██████████| 1/1 [00:37<00:00, 37.85s/it]
INFO:root:eval mean loss: 6460.110350696753
INFO:root:eval perplexity: 202.5611572265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/69
 34%|███▍      | 69/200 [10:32:49<19:44:36, 542.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6611.664930555556
INFO:root:current train perplexity189.2732391357422
INFO:root:current mean train loss 6620.604321856831
INFO:root:current train perplexity187.87420654296875
INFO:root:current mean train loss 6623.709361356848
INFO:root:current train perplexity187.06394958496094
INFO:root:current mean train loss 6631.156298565608
INFO:root:current train perplexity187.19235229492188
INFO:root:current mean train loss 6637.456827454648
INFO:root:current train perplexity187.51388549804688
INFO:root:current mean train loss 6638.693789608828
INFO:root:current train perplexity187.60447692871094
INFO:root:current mean train loss 6637.871250697544
INFO:root:current train perplexity187.93460083007812
INFO:root:current mean train loss 6642.852243057804
INFO:root:current train perplexity187.88827514648438
INFO:root:current mean train loss 6648.300918999068
INFO:root:current train perplexity187.90591430664062
INFO:root:current mean train loss 6641.997764053659
INFO:root:current train perplexity187.95001220703125
INFO:root:current mean train loss 6642.612114749738
INFO:root:current train perplexity187.9311065673828
INFO:root:current mean train loss 6644.765922051648
INFO:root:current train perplexity188.03408813476562
INFO:root:current mean train loss 6645.202739979486
INFO:root:current train perplexity188.06431579589844
INFO:root:current mean train loss 6641.227771102861
INFO:root:current train perplexity187.99832153320312
INFO:root:current mean train loss 6641.253893976626
INFO:root:current train perplexity188.0117645263672
INFO:root:current mean train loss 6640.064837351403
INFO:root:current train perplexity187.99420166015625
INFO:root:current mean train loss 6641.052765622664
INFO:root:current train perplexity188.0846710205078
INFO:root:current mean train loss 6641.6897945361
INFO:root:current train perplexity188.0165252685547
INFO:root:current mean train loss 6640.126918434078
INFO:root:current train perplexity187.90126037597656
INFO:root:current mean train loss 6635.7113131200085
INFO:root:current train perplexity187.8504638671875

100%|██████████| 1/1 [07:39<00:00, 459.59s/it][A100%|██████████| 1/1 [07:39<00:00, 459.59s/it]
INFO:root:final mean train loss: 6634.136382764238
INFO:root:final train perplexity: 187.85037231445312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.13s/it][A100%|██████████| 1/1 [00:41<00:00, 41.13s/it]
INFO:root:eval mean loss: 6336.857475551307
INFO:root:eval perplexity: 168.65887451171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.39s/it][A100%|██████████| 1/1 [00:38<00:00, 38.39s/it]
INFO:root:eval mean loss: 6462.18500838043
INFO:root:eval perplexity: 202.90704345703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/70
 35%|███▌      | 70/200 [10:41:51<19:34:50, 542.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6629.488517161166
INFO:root:current train perplexity190.03868103027344
INFO:root:current mean train loss 6627.987756799768
INFO:root:current train perplexity187.6029510498047
INFO:root:current mean train loss 6648.763315379001
INFO:root:current train perplexity188.31207275390625
INFO:root:current mean train loss 6643.9635433402955
INFO:root:current train perplexity188.30479431152344
INFO:root:current mean train loss 6643.005770505815
INFO:root:current train perplexity188.38714599609375
INFO:root:current mean train loss 6639.953722709306
INFO:root:current train perplexity188.26824951171875
INFO:root:current mean train loss 6629.300007370283
INFO:root:current train perplexity187.83236694335938
INFO:root:current mean train loss 6631.33893588106
INFO:root:current train perplexity187.87286376953125
INFO:root:current mean train loss 6630.446331903824
INFO:root:current train perplexity188.03292846679688
INFO:root:current mean train loss 6631.868475101112
INFO:root:current train perplexity188.078125
INFO:root:current mean train loss 6626.805410281795
INFO:root:current train perplexity188.07785034179688
INFO:root:current mean train loss 6634.294968690864
INFO:root:current train perplexity188.2597198486328
INFO:root:current mean train loss 6633.165284907753
INFO:root:current train perplexity188.0861053466797
INFO:root:current mean train loss 6634.021113857766
INFO:root:current train perplexity188.0548095703125
INFO:root:current mean train loss 6633.855065073561
INFO:root:current train perplexity188.1610107421875
INFO:root:current mean train loss 6633.497341648147
INFO:root:current train perplexity188.1320343017578
INFO:root:current mean train loss 6634.429884951802
INFO:root:current train perplexity188.0046844482422
INFO:root:current mean train loss 6635.039298589034
INFO:root:current train perplexity187.97518920898438
INFO:root:current mean train loss 6638.6999295882415
INFO:root:current train perplexity187.986572265625

100%|██████████| 1/1 [07:44<00:00, 464.33s/it][A100%|██████████| 1/1 [07:44<00:00, 464.33s/it]
INFO:root:final mean train loss: 6634.951922764396
INFO:root:final train perplexity: 187.971435546875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.83s/it][A100%|██████████| 1/1 [00:40<00:00, 40.83s/it]
INFO:root:eval mean loss: 6340.324393630874
INFO:root:eval perplexity: 169.13267517089844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.50s/it][A100%|██████████| 1/1 [00:38<00:00, 38.50s/it]
INFO:root:eval mean loss: 6465.990810962434
INFO:root:eval perplexity: 203.54283142089844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/71
 36%|███▌      | 71/200 [10:50:57<19:28:19, 543.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6679.566080729167
INFO:root:current train perplexity182.03176879882812
INFO:root:current mean train loss 6687.199172685731
INFO:root:current train perplexity189.2296600341797
INFO:root:current mean train loss 6648.585107895934
INFO:root:current train perplexity187.88441467285156
INFO:root:current mean train loss 6630.756060431985
INFO:root:current train perplexity187.58407592773438
INFO:root:current mean train loss 6618.35140735645
INFO:root:current train perplexity187.2964630126953
INFO:root:current mean train loss 6625.033611312685
INFO:root:current train perplexity187.45567321777344
INFO:root:current mean train loss 6621.659017732828
INFO:root:current train perplexity187.27182006835938
INFO:root:current mean train loss 6629.148099299531
INFO:root:current train perplexity187.39633178710938
INFO:root:current mean train loss 6631.826314239881
INFO:root:current train perplexity187.76144409179688
INFO:root:current mean train loss 6633.9486172909765
INFO:root:current train perplexity187.9348602294922
INFO:root:current mean train loss 6631.998397311444
INFO:root:current train perplexity187.87152099609375
INFO:root:current mean train loss 6633.735170995564
INFO:root:current train perplexity188.07606506347656
INFO:root:current mean train loss 6634.535659511686
INFO:root:current train perplexity188.01329040527344
INFO:root:current mean train loss 6626.870858208509
INFO:root:current train perplexity187.84365844726562
INFO:root:current mean train loss 6632.4582227812725
INFO:root:current train perplexity187.86166381835938
INFO:root:current mean train loss 6636.970852916459
INFO:root:current train perplexity187.9658660888672
INFO:root:current mean train loss 6635.742979816898
INFO:root:current train perplexity187.81643676757812
INFO:root:current mean train loss 6636.121727428012
INFO:root:current train perplexity187.91845703125
INFO:root:current mean train loss 6634.031947274277
INFO:root:current train perplexity187.9586181640625
INFO:root:current mean train loss 6638.829573704338
INFO:root:current train perplexity188.0667266845703

100%|██████████| 1/1 [07:47<00:00, 467.18s/it][A100%|██████████| 1/1 [07:47<00:00, 467.18s/it]
INFO:root:final mean train loss: 6634.839300189305
INFO:root:final train perplexity: 187.9546661376953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.06s/it][A100%|██████████| 1/1 [00:41<00:00, 41.06s/it]
INFO:root:eval mean loss: 6338.777077099956
INFO:root:eval perplexity: 168.9210968017578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.27s/it][A100%|██████████| 1/1 [00:38<00:00, 38.28s/it]
INFO:root:eval mean loss: 6465.093954316268
INFO:root:eval perplexity: 203.39283752441406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/72
 36%|███▌      | 72/200 [11:00:06<19:22:47, 545.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6647.281398607337
INFO:root:current train perplexity189.1452178955078
INFO:root:current mean train loss 6573.917829808181
INFO:root:current train perplexity186.68844604492188
INFO:root:current mean train loss 6588.173832504204
INFO:root:current train perplexity187.18869018554688
INFO:root:current mean train loss 6600.513792811533
INFO:root:current train perplexity187.38034057617188
INFO:root:current mean train loss 6595.715652934767
INFO:root:current train perplexity186.83404541015625
INFO:root:current mean train loss 6598.099733545949
INFO:root:current train perplexity186.90533447265625
INFO:root:current mean train loss 6599.243151522372
INFO:root:current train perplexity186.92315673828125
INFO:root:current mean train loss 6607.313748054979
INFO:root:current train perplexity187.0905303955078
INFO:root:current mean train loss 6612.305356142732
INFO:root:current train perplexity187.09776306152344
INFO:root:current mean train loss 6620.2306650147275
INFO:root:current train perplexity187.2110137939453
INFO:root:current mean train loss 6620.894733626588
INFO:root:current train perplexity187.22128295898438
INFO:root:current mean train loss 6626.554214436776
INFO:root:current train perplexity187.37489318847656
INFO:root:current mean train loss 6625.9580405509
INFO:root:current train perplexity187.3052978515625
INFO:root:current mean train loss 6630.391125460601
INFO:root:current train perplexity187.43690490722656
INFO:root:current mean train loss 6630.084520008894
INFO:root:current train perplexity187.43699645996094
INFO:root:current mean train loss 6626.820778659512
INFO:root:current train perplexity187.39437866210938
INFO:root:current mean train loss 6628.691439945317
INFO:root:current train perplexity187.506103515625
INFO:root:current mean train loss 6628.757976016123
INFO:root:current train perplexity187.41000366210938
INFO:root:current mean train loss 6632.551541661667
INFO:root:current train perplexity187.6212921142578
INFO:root:current mean train loss 6632.9887385534485
INFO:root:current train perplexity187.6514434814453

100%|██████████| 1/1 [07:42<00:00, 462.70s/it][A100%|██████████| 1/1 [07:42<00:00, 462.71s/it]
INFO:root:final mean train loss: 6633.154077234619
INFO:root:final train perplexity: 187.7047882080078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.92s/it][A100%|██████████| 1/1 [00:39<00:00, 39.92s/it]
INFO:root:eval mean loss: 6337.191366425643
INFO:root:eval perplexity: 168.70440673828125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.80s/it][A100%|██████████| 1/1 [00:38<00:00, 38.80s/it]
INFO:root:eval mean loss: 6463.254654255319
INFO:root:eval perplexity: 203.08554077148438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/73
 36%|███▋      | 73/200 [11:09:10<19:13:00, 544.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6611.722631835937
INFO:root:current train perplexity187.5807647705078
INFO:root:current mean train loss 6611.528107561384
INFO:root:current train perplexity187.00071716308594
INFO:root:current mean train loss 6647.207684326172
INFO:root:current train perplexity187.52621459960938
INFO:root:current mean train loss 6658.930249023438
INFO:root:current train perplexity187.68751525878906
INFO:root:current mean train loss 6646.708024458451
INFO:root:current train perplexity187.1474609375
INFO:root:current mean train loss 6639.8675473813655
INFO:root:current train perplexity187.33621215820312
INFO:root:current mean train loss 6636.253730010986
INFO:root:current train perplexity187.35398864746094
INFO:root:current mean train loss 6633.7297158731
INFO:root:current train perplexity187.22824096679688
INFO:root:current mean train loss 6629.1063267299105
INFO:root:current train perplexity187.07215881347656
INFO:root:current mean train loss 6628.385076254986
INFO:root:current train perplexity187.22315979003906
INFO:root:current mean train loss 6628.319293682392
INFO:root:current train perplexity187.21127319335938
INFO:root:current mean train loss 6627.654441217791
INFO:root:current train perplexity187.38258361816406
INFO:root:current mean train loss 6629.594719868322
INFO:root:current train perplexity187.46702575683594
INFO:root:current mean train loss 6629.579238572761
INFO:root:current train perplexity187.49859619140625
INFO:root:current mean train loss 6629.921551852756
INFO:root:current train perplexity187.5618133544922
INFO:root:current mean train loss 6629.883083908279
INFO:root:current train perplexity187.69691467285156
INFO:root:current mean train loss 6630.188969012004
INFO:root:current train perplexity187.69520568847656
INFO:root:current mean train loss 6635.320568707346
INFO:root:current train perplexity187.78042602539062
INFO:root:current mean train loss 6637.055880073879
INFO:root:current train perplexity187.8192901611328
INFO:root:current mean train loss 6635.868647813305
INFO:root:current train perplexity187.70263671875

100%|██████████| 1/1 [07:43<00:00, 463.48s/it][A100%|██████████| 1/1 [07:43<00:00, 463.48s/it]
INFO:root:final mean train loss: 6632.5651018274475
INFO:root:final train perplexity: 187.61753845214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.54s/it][A100%|██████████| 1/1 [00:41<00:00, 41.54s/it]
INFO:root:eval mean loss: 6338.2144108765515
INFO:root:eval perplexity: 168.8441925048828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.04s/it][A100%|██████████| 1/1 [00:38<00:00, 38.04s/it]
INFO:root:eval mean loss: 6464.487504675033
INFO:root:eval perplexity: 203.2914276123047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/74
 37%|███▋      | 74/200 [11:18:16<19:04:32, 545.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6491.175866913377
INFO:root:current train perplexity185.63954162597656
INFO:root:current mean train loss 6596.771814042596
INFO:root:current train perplexity187.13015747070312
INFO:root:current mean train loss 6622.8377823291585
INFO:root:current train perplexity188.41275024414062
INFO:root:current mean train loss 6629.443074886204
INFO:root:current train perplexity188.90988159179688
INFO:root:current mean train loss 6642.682620392848
INFO:root:current train perplexity189.29129028320312
INFO:root:current mean train loss 6641.826492720489
INFO:root:current train perplexity188.67015075683594
INFO:root:current mean train loss 6645.786788313356
INFO:root:current train perplexity188.5715789794922
INFO:root:current mean train loss 6648.0860394133915
INFO:root:current train perplexity188.66502380371094
INFO:root:current mean train loss 6645.828087965833
INFO:root:current train perplexity188.46522521972656
INFO:root:current mean train loss 6641.554523719142
INFO:root:current train perplexity188.1584014892578
INFO:root:current mean train loss 6642.439886434188
INFO:root:current train perplexity188.11911010742188
INFO:root:current mean train loss 6641.814950268745
INFO:root:current train perplexity187.97743225097656
INFO:root:current mean train loss 6645.159413145759
INFO:root:current train perplexity188.22265625
INFO:root:current mean train loss 6644.497619763841
INFO:root:current train perplexity188.2678985595703
INFO:root:current mean train loss 6645.929918403076
INFO:root:current train perplexity188.27767944335938
INFO:root:current mean train loss 6645.324333529022
INFO:root:current train perplexity188.2723846435547
INFO:root:current mean train loss 6640.931658010995
INFO:root:current train perplexity188.07516479492188
INFO:root:current mean train loss 6640.710275249448
INFO:root:current train perplexity188.13462829589844
INFO:root:current mean train loss 6640.527410536988
INFO:root:current train perplexity188.11131286621094
INFO:root:current mean train loss 6637.82344303901
INFO:root:current train perplexity188.0131072998047

100%|██████████| 1/1 [07:49<00:00, 469.39s/it][A100%|██████████| 1/1 [07:49<00:00, 469.39s/it]
INFO:root:final mean train loss: 6635.344109993539
INFO:root:final train perplexity: 188.02960205078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.12s/it][A100%|██████████| 1/1 [00:41<00:00, 41.12s/it]
INFO:root:eval mean loss: 6339.586557374779
INFO:root:eval perplexity: 169.03179931640625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.59s/it][A100%|██████████| 1/1 [00:38<00:00, 38.60s/it]
INFO:root:eval mean loss: 6463.696878636137
INFO:root:eval perplexity: 203.15943908691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/75
 38%|███▊      | 75/200 [11:27:27<18:59:29, 546.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6646.4368203652875
INFO:root:current train perplexity189.88560485839844
INFO:root:current mean train loss 6676.774374775503
INFO:root:current train perplexity190.43246459960938
INFO:root:current mean train loss 6672.361119625342
INFO:root:current train perplexity190.4016876220703
INFO:root:current mean train loss 6658.83870268633
INFO:root:current train perplexity190.02110290527344
INFO:root:current mean train loss 6661.744278662315
INFO:root:current train perplexity190.15335083007812
INFO:root:current mean train loss 6646.650662837544
INFO:root:current train perplexity189.26792907714844
INFO:root:current mean train loss 6653.271957442739
INFO:root:current train perplexity189.44326782226562
INFO:root:current mean train loss 6658.460431554829
INFO:root:current train perplexity189.50172424316406
INFO:root:current mean train loss 6657.210748668121
INFO:root:current train perplexity189.39151000976562
INFO:root:current mean train loss 6647.680446491594
INFO:root:current train perplexity189.1073455810547
INFO:root:current mean train loss 6644.380894836767
INFO:root:current train perplexity189.09796142578125
INFO:root:current mean train loss 6639.408868169054
INFO:root:current train perplexity188.80487060546875
INFO:root:current mean train loss 6637.033666493941
INFO:root:current train perplexity188.7010040283203
INFO:root:current mean train loss 6639.4631198399975
INFO:root:current train perplexity188.75267028808594
INFO:root:current mean train loss 6641.114019303342
INFO:root:current train perplexity188.8677215576172
INFO:root:current mean train loss 6643.31075068744
INFO:root:current train perplexity189.07046508789062
INFO:root:current mean train loss 6641.478469538717
INFO:root:current train perplexity188.99024963378906
INFO:root:current mean train loss 6641.6026684267545
INFO:root:current train perplexity188.96456909179688
INFO:root:current mean train loss 6642.814228265492
INFO:root:current train perplexity188.98736572265625
INFO:root:current mean train loss 6644.822417007029
INFO:root:current train perplexity189.07876586914062

100%|██████████| 1/1 [07:42<00:00, 462.70s/it][A100%|██████████| 1/1 [07:42<00:00, 462.70s/it]
INFO:root:final mean train loss: 6642.249697255776
INFO:root:final train perplexity: 189.05703735351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.59s/it][A100%|██████████| 1/1 [00:40<00:00, 40.59s/it]
INFO:root:eval mean loss: 6344.7617152870125
INFO:root:eval perplexity: 169.7412109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.98s/it][A100%|██████████| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 6467.629991896609
INFO:root:eval perplexity: 203.81739807128906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/76
 38%|███▊      | 76/200 [11:36:31<18:48:24, 546.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6631.007925180288
INFO:root:current train perplexity190.59652709960938
INFO:root:current mean train loss 6649.313466336715
INFO:root:current train perplexity189.19735717773438
INFO:root:current mean train loss 6647.420814540378
INFO:root:current train perplexity189.66787719726562
INFO:root:current mean train loss 6629.302518332401
INFO:root:current train perplexity189.2958984375
INFO:root:current mean train loss 6627.125143202647
INFO:root:current train perplexity188.96466064453125
INFO:root:current mean train loss 6635.165808250053
INFO:root:current train perplexity189.12863159179688
INFO:root:current mean train loss 6630.946689015014
INFO:root:current train perplexity188.80406188964844
INFO:root:current mean train loss 6630.295805843079
INFO:root:current train perplexity188.841064453125
INFO:root:current mean train loss 6634.480326814149
INFO:root:current train perplexity188.91744995117188
INFO:root:current mean train loss 6629.827503685513
INFO:root:current train perplexity188.85484313964844
INFO:root:current mean train loss 6633.2686034708695
INFO:root:current train perplexity188.95465087890625
INFO:root:current mean train loss 6637.540877053159
INFO:root:current train perplexity189.01602172851562
INFO:root:current mean train loss 6644.363505155887
INFO:root:current train perplexity189.1806640625
INFO:root:current mean train loss 6647.386455829327
INFO:root:current train perplexity189.2552032470703
INFO:root:current mean train loss 6644.002862552922
INFO:root:current train perplexity189.16136169433594
INFO:root:current mean train loss 6643.529739427836
INFO:root:current train perplexity189.10806274414062
INFO:root:current mean train loss 6643.793987759184
INFO:root:current train perplexity189.02395629882812
INFO:root:current mean train loss 6643.311174470443
INFO:root:current train perplexity189.08705139160156
INFO:root:current mean train loss 6646.064840186657
INFO:root:current train perplexity189.2474365234375

100%|██████████| 1/1 [07:41<00:00, 461.83s/it][A100%|██████████| 1/1 [07:41<00:00, 461.83s/it]
INFO:root:final mean train loss: 6642.658773648276
INFO:root:final train perplexity: 189.1181640625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.93s/it][A100%|██████████| 1/1 [00:40<00:00, 40.93s/it]
INFO:root:eval mean loss: 6346.1077508588205
INFO:root:eval perplexity: 169.92608642578125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.51s/it][A100%|██████████| 1/1 [00:38<00:00, 38.51s/it]
INFO:root:eval mean loss: 6468.272865241301
INFO:root:eval perplexity: 203.92510986328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/77
 38%|███▊      | 77/200 [11:45:35<18:37:52, 545.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6596.511962890625
INFO:root:current train perplexity188.0834197998047
INFO:root:current mean train loss 6659.466091579861
INFO:root:current train perplexity190.46942138671875
INFO:root:current mean train loss 6643.464775672326
INFO:root:current train perplexity190.0606231689453
INFO:root:current mean train loss 6656.360854111708
INFO:root:current train perplexity190.1276092529297
INFO:root:current mean train loss 6647.012183095894
INFO:root:current train perplexity189.27053833007812
INFO:root:current mean train loss 6643.911741241695
INFO:root:current train perplexity189.87020874023438
INFO:root:current mean train loss 6653.528764423571
INFO:root:current train perplexity189.9832305908203
INFO:root:current mean train loss 6648.39119259247
INFO:root:current train perplexity189.85952758789062
INFO:root:current mean train loss 6637.910174379254
INFO:root:current train perplexity189.5894775390625
INFO:root:current mean train loss 6640.507580189978
INFO:root:current train perplexity189.493408203125
INFO:root:current mean train loss 6647.690723237537
INFO:root:current train perplexity189.6285400390625
INFO:root:current mean train loss 6649.858472032237
INFO:root:current train perplexity189.6106414794922
INFO:root:current mean train loss 6650.311548094086
INFO:root:current train perplexity189.38148498535156
INFO:root:current mean train loss 6645.703729752006
INFO:root:current train perplexity189.09597778320312
INFO:root:current mean train loss 6645.03483581543
INFO:root:current train perplexity189.04107666015625
INFO:root:current mean train loss 6644.5599588652185
INFO:root:current train perplexity189.13575744628906
INFO:root:current mean train loss 6645.892073142588
INFO:root:current train perplexity189.2025909423828
INFO:root:current mean train loss 6643.452830830558
INFO:root:current train perplexity189.1334991455078
INFO:root:current mean train loss 6644.883171149059
INFO:root:current train perplexity189.27090454101562
INFO:root:current mean train loss 6646.448380124394
INFO:root:current train perplexity189.3428497314453

100%|██████████| 1/1 [07:44<00:00, 464.81s/it][A100%|██████████| 1/1 [07:44<00:00, 464.81s/it]
INFO:root:final mean train loss: 6643.951058238189
INFO:root:final train perplexity: 189.31106567382812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.81s/it][A100%|██████████| 1/1 [00:38<00:00, 38.82s/it]
INFO:root:eval mean loss: 6360.651971478835
INFO:root:eval perplexity: 171.93789672851562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.58s/it][A100%|██████████| 1/1 [00:37<00:00, 37.58s/it]
INFO:root:eval mean loss: 6485.131017806682
INFO:root:eval perplexity: 206.77114868164062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/78
 39%|███▉      | 78/200 [11:54:38<18:27:48, 544.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6489.16220703125
INFO:root:current train perplexity186.13121032714844
INFO:root:current mean train loss 6639.03936328125
INFO:root:current train perplexity189.2395782470703
INFO:root:current mean train loss 6623.558719618056
INFO:root:current train perplexity189.5367889404297
INFO:root:current mean train loss 6619.578637319712
INFO:root:current train perplexity189.06199645996094
INFO:root:current mean train loss 6628.079813878677
INFO:root:current train perplexity189.04910278320312
INFO:root:current mean train loss 6625.0946716889885
INFO:root:current train perplexity189.02322387695312
INFO:root:current mean train loss 6631.88548984375
INFO:root:current train perplexity189.072998046875
INFO:root:current mean train loss 6632.776773976293
INFO:root:current train perplexity189.0831756591797
INFO:root:current mean train loss 6636.8694223484845
INFO:root:current train perplexity189.1315155029297
INFO:root:current mean train loss 6634.123069573479
INFO:root:current train perplexity188.8448486328125
INFO:root:current mean train loss 6630.81120617378
INFO:root:current train perplexity188.61563110351562
INFO:root:current mean train loss 6633.968103732639
INFO:root:current train perplexity188.71351623535156
INFO:root:current mean train loss 6639.017018494898
INFO:root:current train perplexity188.9687042236328
INFO:root:current mean train loss 6640.553392909787
INFO:root:current train perplexity188.9127655029297
INFO:root:current mean train loss 6641.3673320997805
INFO:root:current train perplexity188.76589965820312
INFO:root:current mean train loss 6644.720559362193
INFO:root:current train perplexity188.9169921875
INFO:root:current mean train loss 6641.579997295673
INFO:root:current train perplexity188.7830047607422
INFO:root:current mean train loss 6639.943492980073
INFO:root:current train perplexity188.6851806640625
INFO:root:current mean train loss 6642.086965164812
INFO:root:current train perplexity188.75437927246094
INFO:root:current mean train loss 6642.712678064124
INFO:root:current train perplexity188.812255859375

100%|██████████| 1/1 [07:29<00:00, 449.42s/it][A100%|██████████| 1/1 [07:29<00:00, 449.42s/it]
INFO:root:final mean train loss: 6640.801864308537
INFO:root:final train perplexity: 188.84115600585938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.03s/it][A100%|██████████| 1/1 [00:40<00:00, 40.03s/it]
INFO:root:eval mean loss: 6349.370034075798
INFO:root:eval perplexity: 170.375244140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.55s/it][A100%|██████████| 1/1 [00:37<00:00, 37.55s/it]
INFO:root:eval mean loss: 6472.742069758422
INFO:root:eval perplexity: 204.67572021484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/79
 40%|███▉      | 79/200 [12:03:28<18:09:27, 540.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6641.4863048735115
INFO:root:current train perplexity188.8426971435547
INFO:root:current mean train loss 6602.6873968419895
INFO:root:current train perplexity187.88656616210938
INFO:root:current mean train loss 6612.929782331483
INFO:root:current train perplexity187.96408081054688
INFO:root:current mean train loss 6626.20704552723
INFO:root:current train perplexity189.3227081298828
INFO:root:current mean train loss 6636.846231175764
INFO:root:current train perplexity189.7081298828125
INFO:root:current mean train loss 6634.126599076049
INFO:root:current train perplexity189.1763458251953
INFO:root:current mean train loss 6631.334701585621
INFO:root:current train perplexity188.80532836914062
INFO:root:current mean train loss 6638.838446028471
INFO:root:current train perplexity189.04368591308594
INFO:root:current mean train loss 6636.3938168052255
INFO:root:current train perplexity188.84854125976562
INFO:root:current mean train loss 6636.959257542961
INFO:root:current train perplexity188.76345825195312
INFO:root:current mean train loss 6643.779240642994
INFO:root:current train perplexity188.96636962890625
INFO:root:current mean train loss 6639.479383157974
INFO:root:current train perplexity188.75347900390625
INFO:root:current mean train loss 6639.218064755058
INFO:root:current train perplexity188.93222045898438
INFO:root:current mean train loss 6634.607579420292
INFO:root:current train perplexity188.88645935058594
INFO:root:current mean train loss 6635.647875062305
INFO:root:current train perplexity188.95924377441406
INFO:root:current mean train loss 6640.539019434987
INFO:root:current train perplexity189.11013793945312
INFO:root:current mean train loss 6644.285383737915
INFO:root:current train perplexity189.27261352539062
INFO:root:current mean train loss 6643.907093980966
INFO:root:current train perplexity189.19871520996094
INFO:root:current mean train loss 6641.9733046408455
INFO:root:current train perplexity189.19248962402344
INFO:root:current mean train loss 6642.960595803698
INFO:root:current train perplexity189.2100830078125

100%|██████████| 1/1 [07:39<00:00, 459.58s/it][A100%|██████████| 1/1 [07:39<00:00, 459.58s/it]
INFO:root:final mean train loss: 6643.795928478001
INFO:root:final train perplexity: 189.28787231445312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.70s/it][A100%|██████████| 1/1 [00:40<00:00, 40.70s/it]
INFO:root:eval mean loss: 6351.144991827349
INFO:root:eval perplexity: 170.62020874023438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.27s/it][A100%|██████████| 1/1 [00:38<00:00, 38.28s/it]
INFO:root:eval mean loss: 6474.384548322529
INFO:root:eval perplexity: 204.95230102539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/80
 40%|████      | 80/200 [12:12:29<18:00:50, 540.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6618.364456435382
INFO:root:current train perplexity189.30511474609375
INFO:root:current mean train loss 6670.511113772603
INFO:root:current train perplexity190.50775146484375
INFO:root:current mean train loss 6681.5572250542955
INFO:root:current train perplexity189.66616821289062
INFO:root:current mean train loss 6679.357511642584
INFO:root:current train perplexity190.09361267089844
INFO:root:current mean train loss 6679.85285181781
INFO:root:current train perplexity190.2753448486328
INFO:root:current mean train loss 6674.091644014144
INFO:root:current train perplexity190.0882568359375
INFO:root:current mean train loss 6676.850100620021
INFO:root:current train perplexity190.2152862548828
INFO:root:current mean train loss 6675.018635102726
INFO:root:current train perplexity190.2389678955078
INFO:root:current mean train loss 6667.807208486431
INFO:root:current train perplexity189.99908447265625
INFO:root:current mean train loss 6661.953503303408
INFO:root:current train perplexity189.7603302001953
INFO:root:current mean train loss 6651.38666342068
INFO:root:current train perplexity189.32858276367188
INFO:root:current mean train loss 6652.076936947261
INFO:root:current train perplexity189.39141845703125
INFO:root:current mean train loss 6651.900572518492
INFO:root:current train perplexity189.34934997558594
INFO:root:current mean train loss 6651.618497128518
INFO:root:current train perplexity189.363525390625
INFO:root:current mean train loss 6646.103114357544
INFO:root:current train perplexity189.32469177246094
INFO:root:current mean train loss 6648.013063324346
INFO:root:current train perplexity189.43983459472656
INFO:root:current mean train loss 6644.856122146248
INFO:root:current train perplexity189.38330078125
INFO:root:current mean train loss 6644.291645199687
INFO:root:current train perplexity189.495849609375
INFO:root:current mean train loss 6642.273226322955
INFO:root:current train perplexity189.33273315429688
INFO:root:current mean train loss 6646.607971471813
INFO:root:current train perplexity189.41192626953125

100%|██████████| 1/1 [07:44<00:00, 464.28s/it][A100%|██████████| 1/1 [07:44<00:00, 464.28s/it]
INFO:root:final mean train loss: 6644.490130833767
INFO:root:final train perplexity: 189.39169311523438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.61s/it][A100%|██████████| 1/1 [00:40<00:00, 40.61s/it]
INFO:root:eval mean loss: 6349.147339732935
INFO:root:eval perplexity: 170.34454345703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.02s/it][A100%|██████████| 1/1 [00:38<00:00, 38.02s/it]
INFO:root:eval mean loss: 6471.76635309314
INFO:root:eval perplexity: 204.51153564453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/81
 40%|████      | 81/200 [12:21:34<17:54:46, 541.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6588.94429738898
INFO:root:current train perplexity187.20726013183594
INFO:root:current mean train loss 6617.333243630149
INFO:root:current train perplexity188.1575927734375
INFO:root:current mean train loss 6646.666440217391
INFO:root:current train perplexity188.58721923828125
INFO:root:current mean train loss 6635.8508703353555
INFO:root:current train perplexity188.3057861328125
INFO:root:current mean train loss 6631.730676987592
INFO:root:current train perplexity188.5329132080078
INFO:root:current mean train loss 6644.637886047363
INFO:root:current train perplexity188.90843200683594
INFO:root:current mean train loss 6642.650914299417
INFO:root:current train perplexity189.18409729003906
INFO:root:current mean train loss 6637.622776306782
INFO:root:current train perplexity189.21856689453125
INFO:root:current mean train loss 6642.084160512985
INFO:root:current train perplexity189.23318481445312
INFO:root:current mean train loss 6643.834445640689
INFO:root:current train perplexity189.1988067626953
INFO:root:current mean train loss 6643.106734378631
INFO:root:current train perplexity189.12799072265625
INFO:root:current mean train loss 6643.260862181787
INFO:root:current train perplexity189.2043914794922
INFO:root:current mean train loss 6645.442778106019
INFO:root:current train perplexity189.4674835205078
INFO:root:current mean train loss 6643.358322143555
INFO:root:current train perplexity189.4067840576172
INFO:root:current mean train loss 6640.422649435234
INFO:root:current train perplexity189.14251708984375
INFO:root:current mean train loss 6642.168645093889
INFO:root:current train perplexity189.1722869873047
INFO:root:current mean train loss 6643.761307381787
INFO:root:current train perplexity189.3684844970703
INFO:root:current mean train loss 6644.025779655388
INFO:root:current train perplexity189.38284301757812
INFO:root:current mean train loss 6641.901224294959
INFO:root:current train perplexity189.21160888671875
INFO:root:current mean train loss 6646.352360652043
INFO:root:current train perplexity189.3484344482422

100%|██████████| 1/1 [07:42<00:00, 462.17s/it][A100%|██████████| 1/1 [07:42<00:00, 462.17s/it]
INFO:root:final mean train loss: 6644.3264801594805
INFO:root:final train perplexity: 189.36721801757812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.74s/it][A100%|██████████| 1/1 [00:40<00:00, 40.74s/it]
INFO:root:eval mean loss: 6343.581795766844
INFO:root:eval perplexity: 169.57908630371094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.27s/it][A100%|██████████| 1/1 [00:39<00:00, 39.27s/it]
INFO:root:eval mean loss: 6469.003037905863
INFO:root:eval perplexity: 204.04757690429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/82
 41%|████      | 82/200 [12:30:39<17:47:17, 542.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6602.531039986559
INFO:root:current train perplexity185.3805389404297
INFO:root:current mean train loss 6617.434319846989
INFO:root:current train perplexity185.4029998779297
INFO:root:current mean train loss 6625.125773250853
INFO:root:current train perplexity185.67628479003906
INFO:root:current mean train loss 6628.3020932729005
INFO:root:current train perplexity185.81976318359375
INFO:root:current mean train loss 6622.248672825812
INFO:root:current train perplexity186.0355682373047
INFO:root:current mean train loss 6623.770646145131
INFO:root:current train perplexity186.25888061523438
INFO:root:current mean train loss 6622.799729296311
INFO:root:current train perplexity186.22470092773438
INFO:root:current mean train loss 6627.668008773053
INFO:root:current train perplexity186.24246215820312
INFO:root:current mean train loss 6632.321824367476
INFO:root:current train perplexity186.35989379882812
INFO:root:current mean train loss 6626.48901440946
INFO:root:current train perplexity186.3463897705078
INFO:root:current mean train loss 6618.758056863992
INFO:root:current train perplexity186.33502197265625
INFO:root:current mean train loss 6617.381413551708
INFO:root:current train perplexity186.15969848632812
INFO:root:current mean train loss 6624.158621921524
INFO:root:current train perplexity186.3463897705078
INFO:root:current mean train loss 6624.966792668701
INFO:root:current train perplexity186.31680297851562
INFO:root:current mean train loss 6624.350238940577
INFO:root:current train perplexity186.4476318359375
INFO:root:current mean train loss 6623.014961084628
INFO:root:current train perplexity186.49964904785156
INFO:root:current mean train loss 6620.790463893145
INFO:root:current train perplexity186.4245147705078
INFO:root:current mean train loss 6624.162699234001
INFO:root:current train perplexity186.5076446533203
INFO:root:current mean train loss 6627.4225136605255
INFO:root:current train perplexity186.535400390625

100%|██████████| 1/1 [07:41<00:00, 461.78s/it][A100%|██████████| 1/1 [07:41<00:00, 461.78s/it]
INFO:root:final mean train loss: 6625.502397453551
INFO:root:final train perplexity: 186.57479858398438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.02s/it][A100%|██████████| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 6337.693182762633
INFO:root:eval perplexity: 168.77304077148438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.80s/it][A100%|██████████| 1/1 [00:38<00:00, 38.82s/it]
INFO:root:eval mean loss: 6466.608117069758
INFO:root:eval perplexity: 203.64613342285156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/83
 42%|████▏     | 83/200 [12:39:42<17:38:59, 543.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6658.02265625
INFO:root:current train perplexity194.888671875
INFO:root:current mean train loss 6573.278169389205
INFO:root:current train perplexity183.61216735839844
INFO:root:current mean train loss 6592.800223214285
INFO:root:current train perplexity185.82003784179688
INFO:root:current mean train loss 6596.477419354838
INFO:root:current train perplexity185.6676025390625
INFO:root:current mean train loss 6595.808909346418
INFO:root:current train perplexity185.31787109375
INFO:root:current mean train loss 6609.570608340992
INFO:root:current train perplexity185.7930145263672
INFO:root:current mean train loss 6613.490789094519
INFO:root:current train perplexity185.96603393554688
INFO:root:current mean train loss 6618.5748817121475
INFO:root:current train perplexity186.76707458496094
INFO:root:current mean train loss 6613.3022472993825
INFO:root:current train perplexity186.65061950683594
INFO:root:current mean train loss 6619.640017599588
INFO:root:current train perplexity186.4361572265625
INFO:root:current mean train loss 6629.3333757155015
INFO:root:current train perplexity186.5942840576172
INFO:root:current mean train loss 6633.587906021256
INFO:root:current train perplexity186.60728454589844
INFO:root:current mean train loss 6632.525547197831
INFO:root:current train perplexity186.557373046875
INFO:root:current mean train loss 6628.897017384304
INFO:root:current train perplexity186.4507293701172
INFO:root:current mean train loss 6633.192537261746
INFO:root:current train perplexity186.64999389648438
INFO:root:current mean train loss 6629.360177592094
INFO:root:current train perplexity186.5296173095703
INFO:root:current mean train loss 6629.615855189732
INFO:root:current train perplexity186.5441131591797
INFO:root:current mean train loss 6631.485115702668
INFO:root:current train perplexity186.76101684570312
INFO:root:current mean train loss 6631.839949769078
INFO:root:current train perplexity186.84063720703125
INFO:root:current mean train loss 6630.676992238629
INFO:root:current train perplexity186.85374450683594

100%|██████████| 1/1 [07:41<00:00, 461.00s/it][A100%|██████████| 1/1 [07:41<00:00, 461.00s/it]
INFO:root:final mean train loss: 6627.666504521834
INFO:root:final train perplexity: 186.8937530517578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.40s/it][A100%|██████████| 1/1 [00:41<00:00, 41.40s/it]
INFO:root:eval mean loss: 6343.386888436392
INFO:root:eval perplexity: 169.55239868164062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.13s/it][A100%|██████████| 1/1 [00:38<00:00, 38.13s/it]
INFO:root:eval mean loss: 6472.413802775931
INFO:root:eval perplexity: 204.6204833984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/84
 42%|████▏     | 84/200 [12:48:46<17:29:59, 543.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6666.927264178241
INFO:root:current train perplexity188.79928588867188
INFO:root:current mean train loss 6680.931986651082
INFO:root:current train perplexity187.11810302734375
INFO:root:current mean train loss 6649.673531284417
INFO:root:current train perplexity187.0696563720703
INFO:root:current mean train loss 6646.841992486143
INFO:root:current train perplexity186.72824096679688
INFO:root:current mean train loss 6651.713510410568
INFO:root:current train perplexity186.62899780273438
INFO:root:current mean train loss 6651.629413988378
INFO:root:current train perplexity187.15798950195312
INFO:root:current mean train loss 6643.5538152910685
INFO:root:current train perplexity186.68649291992188
INFO:root:current mean train loss 6634.273482499785
INFO:root:current train perplexity186.4922637939453
INFO:root:current mean train loss 6631.357479736624
INFO:root:current train perplexity186.31324768066406
INFO:root:current mean train loss 6633.488024204423
INFO:root:current train perplexity186.3966064453125
INFO:root:current mean train loss 6626.219403260407
INFO:root:current train perplexity186.23126220703125
INFO:root:current mean train loss 6630.156710552767
INFO:root:current train perplexity186.48025512695312
INFO:root:current mean train loss 6628.631756348054
INFO:root:current train perplexity186.49742126464844
INFO:root:current mean train loss 6627.961752528612
INFO:root:current train perplexity186.7578887939453
INFO:root:current mean train loss 6625.164220926222
INFO:root:current train perplexity186.56021118164062
INFO:root:current mean train loss 6627.171300701948
INFO:root:current train perplexity186.74899291992188
INFO:root:current mean train loss 6629.379348614206
INFO:root:current train perplexity186.77659606933594
INFO:root:current mean train loss 6630.920742933918
INFO:root:current train perplexity186.79788208007812
INFO:root:current mean train loss 6629.21217303597
INFO:root:current train perplexity186.72691345214844
INFO:root:current mean train loss 6627.083231808673
INFO:root:current train perplexity186.67161560058594

100%|██████████| 1/1 [07:34<00:00, 454.63s/it][A100%|██████████| 1/1 [07:34<00:00, 454.63s/it]
INFO:root:final mean train loss: 6625.985212317393
INFO:root:final train perplexity: 186.64590454101562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 40.98s/it][A100%|██████████| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 6341.270339857602
INFO:root:eval perplexity: 169.26223754882812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.44s/it][A100%|██████████| 1/1 [00:38<00:00, 38.44s/it]
INFO:root:eval mean loss: 6470.45494999446
INFO:root:eval perplexity: 204.29115295410156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/85
 42%|████▎     | 85/200 [12:57:42<17:17:08, 541.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6630.00410600142
INFO:root:current train perplexity187.76368713378906
INFO:root:current mean train loss 6594.6927490234375
INFO:root:current train perplexity184.4762420654297
INFO:root:current mean train loss 6606.029318887679
INFO:root:current train perplexity185.2677764892578
INFO:root:current mean train loss 6616.400203261264
INFO:root:current train perplexity185.86807250976562
INFO:root:current mean train loss 6618.8072575749575
INFO:root:current train perplexity186.0223388671875
INFO:root:current mean train loss 6634.0852804744945
INFO:root:current train perplexity186.46389770507812
INFO:root:current mean train loss 6625.895443365441
INFO:root:current train perplexity186.45758056640625
INFO:root:current mean train loss 6624.00290671728
INFO:root:current train perplexity186.40211486816406
INFO:root:current mean train loss 6623.041980616854
INFO:root:current train perplexity186.3333282470703
INFO:root:current mean train loss 6620.343174303992
INFO:root:current train perplexity186.29583740234375
INFO:root:current mean train loss 6626.731994395055
INFO:root:current train perplexity186.49349975585938
INFO:root:current mean train loss 6628.258045116505
INFO:root:current train perplexity186.51840209960938
INFO:root:current mean train loss 6628.0472576963175
INFO:root:current train perplexity186.59677124023438
INFO:root:current mean train loss 6626.760127476284
INFO:root:current train perplexity186.48052978515625
INFO:root:current mean train loss 6626.414894336479
INFO:root:current train perplexity186.32923889160156
INFO:root:current mean train loss 6625.587218605792
INFO:root:current train perplexity186.2143096923828
INFO:root:current mean train loss 6630.228417018324
INFO:root:current train perplexity186.31103515625
INFO:root:current mean train loss 6629.205741112385
INFO:root:current train perplexity186.2548828125
INFO:root:current mean train loss 6627.083969546502
INFO:root:current train perplexity186.1715087890625
INFO:root:current mean train loss 6624.38581929776
INFO:root:current train perplexity186.12730407714844

100%|██████████| 1/1 [07:32<00:00, 452.23s/it][A100%|██████████| 1/1 [07:32<00:00, 452.23s/it]
INFO:root:final mean train loss: 6622.398624883777
INFO:root:final train perplexity: 186.11834716796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.11s/it][A100%|██████████| 1/1 [00:40<00:00, 40.12s/it]
INFO:root:eval mean loss: 6342.47083125554
INFO:root:eval perplexity: 169.42672729492188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.22s/it][A100%|██████████| 1/1 [00:37<00:00, 37.22s/it]
INFO:root:eval mean loss: 6471.852001433677
INFO:root:eval perplexity: 204.52586364746094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/86
 43%|████▎     | 86/200 [13:06:34<17:02:54, 538.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6643.7168689164955
INFO:root:current train perplexity184.52841186523438
INFO:root:current mean train loss 6645.124657293284
INFO:root:current train perplexity184.9812469482422
INFO:root:current mean train loss 6641.54397524545
INFO:root:current train perplexity185.38893127441406
INFO:root:current mean train loss 6635.534607102666
INFO:root:current train perplexity185.7640380859375
INFO:root:current mean train loss 6613.126804840022
INFO:root:current train perplexity185.43569946289062
INFO:root:current mean train loss 6600.715419939283
INFO:root:current train perplexity185.1333770751953
INFO:root:current mean train loss 6600.812932878687
INFO:root:current train perplexity185.1885528564453
INFO:root:current mean train loss 6599.310507735504
INFO:root:current train perplexity185.304443359375
INFO:root:current mean train loss 6602.525404802736
INFO:root:current train perplexity185.4770050048828
INFO:root:current mean train loss 6611.063797679825
INFO:root:current train perplexity185.82809448242188
INFO:root:current mean train loss 6616.489718941447
INFO:root:current train perplexity186.14328002929688
INFO:root:current mean train loss 6615.082008959814
INFO:root:current train perplexity186.12606811523438
INFO:root:current mean train loss 6612.815992314508
INFO:root:current train perplexity186.15304565429688
INFO:root:current mean train loss 6616.0767433900855
INFO:root:current train perplexity186.3526153564453
INFO:root:current mean train loss 6619.275371909223
INFO:root:current train perplexity186.4202423095703
INFO:root:current mean train loss 6618.673686113669
INFO:root:current train perplexity186.47874450683594
INFO:root:current mean train loss 6620.625815761871
INFO:root:current train perplexity186.47402954101562
INFO:root:current mean train loss 6624.337233206009
INFO:root:current train perplexity186.53993225097656
INFO:root:current mean train loss 6624.193092538873
INFO:root:current train perplexity186.52037048339844
INFO:root:current mean train loss 6625.788140467635
INFO:root:current train perplexity186.5515899658203

100%|██████████| 1/1 [07:46<00:00, 466.43s/it][A100%|██████████| 1/1 [07:46<00:00, 466.43s/it]
INFO:root:final mean train loss: 6625.23728117219
INFO:root:final train perplexity: 186.53575134277344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.02s/it][A100%|██████████| 1/1 [00:41<00:00, 41.02s/it]
INFO:root:eval mean loss: 6344.904440588985
INFO:root:eval perplexity: 169.7607879638672
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.26s/it][A100%|██████████| 1/1 [00:38<00:00, 38.26s/it]
INFO:root:eval mean loss: 6473.49658462849
INFO:root:eval perplexity: 204.80282592773438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/87
 44%|████▎     | 87/200 [13:15:42<16:59:28, 541.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6624.0357509515225
INFO:root:current train perplexity186.37420654296875
INFO:root:current mean train loss 6650.339755969101
INFO:root:current train perplexity186.89830017089844
INFO:root:current mean train loss 6630.669089337905
INFO:root:current train perplexity187.43565368652344
INFO:root:current mean train loss 6650.643159412202
INFO:root:current train perplexity187.87733459472656
INFO:root:current mean train loss 6648.6511659502485
INFO:root:current train perplexity187.88844299316406
INFO:root:current mean train loss 6644.3686447407545
INFO:root:current train perplexity187.67337036132812
INFO:root:current mean train loss 6639.619899693492
INFO:root:current train perplexity187.43154907226562
INFO:root:current mean train loss 6641.227806424727
INFO:root:current train perplexity187.1107940673828
INFO:root:current mean train loss 6645.974953062713
INFO:root:current train perplexity187.06761169433594
INFO:root:current mean train loss 6641.637431700537
INFO:root:current train perplexity186.97032165527344
INFO:root:current mean train loss 6636.481538167469
INFO:root:current train perplexity186.86781311035156
INFO:root:current mean train loss 6639.968638913943
INFO:root:current train perplexity186.80911254882812
INFO:root:current mean train loss 6641.625311766432
INFO:root:current train perplexity186.77169799804688
INFO:root:current mean train loss 6642.692414703148
INFO:root:current train perplexity186.85659790039062
INFO:root:current mean train loss 6638.520807785014
INFO:root:current train perplexity186.75352478027344
INFO:root:current mean train loss 6636.383381852028
INFO:root:current train perplexity186.71096801757812
INFO:root:current mean train loss 6635.548332859989
INFO:root:current train perplexity186.8037567138672
INFO:root:current mean train loss 6629.986186693704
INFO:root:current train perplexity186.68612670898438
INFO:root:current mean train loss 6631.707665911625
INFO:root:current train perplexity186.86773681640625
INFO:root:current mean train loss 6628.744357858316
INFO:root:current train perplexity186.7974395751953

100%|██████████| 1/1 [07:42<00:00, 462.97s/it][A100%|██████████| 1/1 [07:42<00:00, 462.97s/it]
INFO:root:final mean train loss: 6626.963399220523
INFO:root:final train perplexity: 186.7900390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.56s/it][A100%|██████████| 1/1 [00:41<00:00, 41.56s/it]
INFO:root:eval mean loss: 6345.268836920988
INFO:root:eval perplexity: 169.81082153320312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.23s/it][A100%|██████████| 1/1 [00:38<00:00, 38.23s/it]
INFO:root:eval mean loss: 6471.383697293329
INFO:root:eval perplexity: 204.44717407226562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/88
 44%|████▍     | 88/200 [13:24:47<16:52:35, 542.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6640.5515779194075
INFO:root:current train perplexity188.79893493652344
INFO:root:current mean train loss 6621.458148036859
INFO:root:current train perplexity188.04261779785156
INFO:root:current mean train loss 6622.511867717161
INFO:root:current train perplexity187.68258666992188
INFO:root:current mean train loss 6625.873448625395
INFO:root:current train perplexity187.47874450683594
INFO:root:current mean train loss 6627.284520004735
INFO:root:current train perplexity187.25270080566406
INFO:root:current mean train loss 6631.544578847164
INFO:root:current train perplexity187.26904296875
INFO:root:current mean train loss 6645.292272510117
INFO:root:current train perplexity187.99195861816406
INFO:root:current mean train loss 6647.614121462264
INFO:root:current train perplexity187.93666076660156
INFO:root:current mean train loss 6652.864660439944
INFO:root:current train perplexity187.9563751220703
INFO:root:current mean train loss 6647.203068074748
INFO:root:current train perplexity187.74623107910156
INFO:root:current mean train loss 6644.879043147118
INFO:root:current train perplexity187.64581298828125
INFO:root:current mean train loss 6638.628654958813
INFO:root:current train perplexity187.394287109375
INFO:root:current mean train loss 6635.157681286197
INFO:root:current train perplexity187.4108123779297
INFO:root:current mean train loss 6627.271698238687
INFO:root:current train perplexity187.1696014404297
INFO:root:current mean train loss 6628.74774018865
INFO:root:current train perplexity187.01124572753906
INFO:root:current mean train loss 6625.157596982759
INFO:root:current train perplexity186.83200073242188
INFO:root:current mean train loss 6630.699527850756
INFO:root:current train perplexity187.0700225830078
INFO:root:current mean train loss 6631.957678664694
INFO:root:current train perplexity187.0784912109375
INFO:root:current mean train loss 6628.941256802441
INFO:root:current train perplexity186.97023010253906

100%|██████████| 1/1 [07:45<00:00, 465.01s/it][A100%|██████████| 1/1 [07:45<00:00, 465.01s/it]
INFO:root:final mean train loss: 6628.777395828409
INFO:root:final train perplexity: 187.05752563476562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.51s/it][A100%|██████████| 1/1 [00:40<00:00, 40.51s/it]
INFO:root:eval mean loss: 6345.424579593307
INFO:root:eval perplexity: 169.8321990966797
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.79s/it][A100%|██████████| 1/1 [00:37<00:00, 37.79s/it]
INFO:root:eval mean loss: 6473.490399732657
INFO:root:eval perplexity: 204.8017578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/89
 44%|████▍     | 89/200 [13:33:53<16:45:23, 543.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6758.2825113932295
INFO:root:current train perplexity192.35903930664062
INFO:root:current mean train loss 6670.97505405971
INFO:root:current train perplexity190.1670379638672
INFO:root:current mean train loss 6655.47035533977
INFO:root:current train perplexity188.5054931640625
INFO:root:current mean train loss 6657.542176857973
INFO:root:current train perplexity188.28477478027344
INFO:root:current mean train loss 6634.931230563562
INFO:root:current train perplexity187.54115295410156
INFO:root:current mean train loss 6630.279794692993
INFO:root:current train perplexity187.6320343017578
INFO:root:current mean train loss 6634.0053176381225
INFO:root:current train perplexity187.29940795898438
INFO:root:current mean train loss 6633.2141888221995
INFO:root:current train perplexity187.0259552001953
INFO:root:current mean train loss 6641.866732893319
INFO:root:current train perplexity187.4297637939453
INFO:root:current mean train loss 6641.769763076514
INFO:root:current train perplexity187.4594268798828
INFO:root:current mean train loss 6638.9117421990795
INFO:root:current train perplexity187.32325744628906
INFO:root:current mean train loss 6640.618513148465
INFO:root:current train perplexity187.43789672851562
INFO:root:current mean train loss 6633.463955013665
INFO:root:current train perplexity187.26046752929688
INFO:root:current mean train loss 6634.305935371212
INFO:root:current train perplexity187.09945678710938
INFO:root:current mean train loss 6632.50243552751
INFO:root:current train perplexity187.05654907226562
INFO:root:current mean train loss 6632.739617241754
INFO:root:current train perplexity186.97869873046875
INFO:root:current mean train loss 6633.248412177226
INFO:root:current train perplexity187.04718017578125
INFO:root:current mean train loss 6631.448913003796
INFO:root:current train perplexity187.0682373046875
INFO:root:current mean train loss 6632.587743224424
INFO:root:current train perplexity187.143798828125
INFO:root:current mean train loss 6628.839064083339
INFO:root:current train perplexity186.8313751220703

100%|██████████| 1/1 [07:38<00:00, 458.43s/it][A100%|██████████| 1/1 [07:38<00:00, 458.43s/it]
INFO:root:final mean train loss: 6627.355713998676
INFO:root:final train perplexity: 186.8478546142578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.02s/it][A100%|██████████| 1/1 [00:40<00:00, 40.02s/it]
INFO:root:eval mean loss: 6343.919492464539
INFO:root:eval perplexity: 169.62550354003906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.22s/it][A100%|██████████| 1/1 [00:38<00:00, 38.22s/it]
INFO:root:eval mean loss: 6472.001009460882
INFO:root:eval perplexity: 204.55113220214844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/90
 45%|████▌     | 90/200 [13:42:52<16:33:56, 542.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6651.512173356681
INFO:root:current train perplexity191.26751708984375
INFO:root:current mean train loss 6631.159020712209
INFO:root:current train perplexity188.30722045898438
INFO:root:current mean train loss 6618.8943542747
INFO:root:current train perplexity186.39535522460938
INFO:root:current mean train loss 6611.199491831307
INFO:root:current train perplexity186.4482421875
INFO:root:current mean train loss 6610.752087430798
INFO:root:current train perplexity186.35687255859375
INFO:root:current mean train loss 6601.540823635397
INFO:root:current train perplexity186.0916290283203
INFO:root:current mean train loss 6604.704201702852
INFO:root:current train perplexity186.38575744628906
INFO:root:current mean train loss 6612.726486813057
INFO:root:current train perplexity186.9154052734375
INFO:root:current mean train loss 6619.141798877601
INFO:root:current train perplexity187.01641845703125
INFO:root:current mean train loss 6617.345412994483
INFO:root:current train perplexity187.00973510742188
INFO:root:current mean train loss 6616.943543014304
INFO:root:current train perplexity186.8892059326172
INFO:root:current mean train loss 6622.690081533022
INFO:root:current train perplexity186.883056640625
INFO:root:current mean train loss 6624.393982181906
INFO:root:current train perplexity187.0442352294922
INFO:root:current mean train loss 6625.100937176683
INFO:root:current train perplexity186.9817352294922
INFO:root:current mean train loss 6624.281883159661
INFO:root:current train perplexity186.82327270507812
INFO:root:current mean train loss 6627.989080575028
INFO:root:current train perplexity186.96060180664062
INFO:root:current mean train loss 6631.489859996067
INFO:root:current train perplexity186.93252563476562
INFO:root:current mean train loss 6630.741123673818
INFO:root:current train perplexity186.93055725097656
INFO:root:current mean train loss 6629.859554134346
INFO:root:current train perplexity186.90122985839844
INFO:root:current mean train loss 6630.402720149284
INFO:root:current train perplexity186.9181671142578

100%|██████████| 1/1 [07:42<00:00, 462.78s/it][A100%|██████████| 1/1 [07:42<00:00, 462.78s/it]
INFO:root:final mean train loss: 6627.629837136165
INFO:root:final train perplexity: 186.88839721679688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.68s/it][A100%|██████████| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 6343.952245401152
INFO:root:eval perplexity: 169.62994384765625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.42s/it][A100%|██████████| 1/1 [00:38<00:00, 38.44s/it]
INFO:root:eval mean loss: 6472.960377361757
INFO:root:eval perplexity: 204.71250915527344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/91
 46%|████▌     | 91/200 [13:51:57<16:26:09, 542.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6633.835565981658
INFO:root:current train perplexity187.06617736816406
INFO:root:current mean train loss 6660.991759417808
INFO:root:current train perplexity188.38014221191406
INFO:root:current mean train loss 6659.008022897612
INFO:root:current train perplexity188.7350311279297
INFO:root:current mean train loss 6659.5160864681175
INFO:root:current train perplexity188.6844482421875
INFO:root:current mean train loss 6667.054618527536
INFO:root:current train perplexity188.9317626953125
INFO:root:current mean train loss 6669.395744798821
INFO:root:current train perplexity188.79803466796875
INFO:root:current mean train loss 6657.334595860342
INFO:root:current train perplexity188.4624481201172
INFO:root:current mean train loss 6648.207572548383
INFO:root:current train perplexity187.93504333496094
INFO:root:current mean train loss 6640.69834261414
INFO:root:current train perplexity187.592041015625
INFO:root:current mean train loss 6638.341709128898
INFO:root:current train perplexity187.36622619628906
INFO:root:current mean train loss 6641.081018743278
INFO:root:current train perplexity187.40036010742188
INFO:root:current mean train loss 6638.125129526615
INFO:root:current train perplexity187.45523071289062
INFO:root:current mean train loss 6629.696762060468
INFO:root:current train perplexity187.04290771484375
INFO:root:current mean train loss 6633.766848242478
INFO:root:current train perplexity186.9499053955078
INFO:root:current mean train loss 6633.241453052062
INFO:root:current train perplexity187.04281616210938
INFO:root:current mean train loss 6632.078648654795
INFO:root:current train perplexity186.96524047851562
INFO:root:current mean train loss 6630.201247816677
INFO:root:current train perplexity186.87110900878906
INFO:root:current mean train loss 6629.258322314845
INFO:root:current train perplexity186.7830047607422
INFO:root:current mean train loss 6628.485796199976
INFO:root:current train perplexity186.8502655029297
INFO:root:current mean train loss 6628.6592409108425
INFO:root:current train perplexity186.8755645751953

100%|██████████| 1/1 [07:47<00:00, 467.03s/it][A100%|██████████| 1/1 [07:47<00:00, 467.03s/it]
INFO:root:final mean train loss: 6627.47579289685
INFO:root:final train perplexity: 186.86550903320312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6345.444237242354
INFO:root:eval perplexity: 169.8349609375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.76s/it][A100%|██████████| 1/1 [00:37<00:00, 37.80s/it]
INFO:root:eval mean loss: 6474.258011621786
INFO:root:eval perplexity: 204.93089294433594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/92
 46%|████▌     | 92/200 [14:01:04<16:19:31, 544.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6637.050440228175
INFO:root:current train perplexity184.81777954101562
INFO:root:current mean train loss 6644.899417058091
INFO:root:current train perplexity186.69485473632812
INFO:root:current mean train loss 6629.784580709363
INFO:root:current train perplexity186.16627502441406
INFO:root:current mean train loss 6629.810973280389
INFO:root:current train perplexity186.8756561279297
INFO:root:current mean train loss 6618.12618853773
INFO:root:current train perplexity186.3311004638672
INFO:root:current mean train loss 6635.961032034025
INFO:root:current train perplexity186.94081115722656
INFO:root:current mean train loss 6637.041555459323
INFO:root:current train perplexity187.0296173095703
INFO:root:current mean train loss 6636.419981390277
INFO:root:current train perplexity186.94554138183594
INFO:root:current mean train loss 6630.298067130468
INFO:root:current train perplexity186.99171447753906
INFO:root:current mean train loss 6632.165422386098
INFO:root:current train perplexity187.0905303955078
INFO:root:current mean train loss 6635.433862006114
INFO:root:current train perplexity187.0525360107422
INFO:root:current mean train loss 6637.324327490192
INFO:root:current train perplexity187.07510375976562
INFO:root:current mean train loss 6634.909311519571
INFO:root:current train perplexity187.15692138671875
INFO:root:current mean train loss 6635.938659981429
INFO:root:current train perplexity187.33969116210938
INFO:root:current mean train loss 6638.675246243271
INFO:root:current train perplexity187.4948272705078
INFO:root:current mean train loss 6637.01591803123
INFO:root:current train perplexity187.4329833984375
INFO:root:current mean train loss 6632.7752088775
INFO:root:current train perplexity187.21600341796875
INFO:root:current mean train loss 6632.302302039759
INFO:root:current train perplexity187.1267547607422
INFO:root:current mean train loss 6632.625645275597
INFO:root:current train perplexity187.19735717773438
INFO:root:current mean train loss 6630.152880038525
INFO:root:current train perplexity187.09616088867188

100%|██████████| 1/1 [07:30<00:00, 450.35s/it][A100%|██████████| 1/1 [07:30<00:00, 450.36s/it]
INFO:root:final mean train loss: 6629.070216592008
INFO:root:final train perplexity: 187.10079956054688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.68s/it][A100%|██████████| 1/1 [00:39<00:00, 39.68s/it]
INFO:root:eval mean loss: 6344.865634350066
INFO:root:eval perplexity: 169.7554473876953
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.84s/it][A100%|██████████| 1/1 [00:36<00:00, 36.84s/it]
INFO:root:eval mean loss: 6472.938315533577
INFO:root:eval perplexity: 204.70880126953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/93
 46%|████▋     | 93/200 [14:09:53<16:02:32, 539.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6688.357818603516
INFO:root:current train perplexity187.98765563964844
INFO:root:current mean train loss 6719.928342013889
INFO:root:current train perplexity189.16018676757812
INFO:root:current mean train loss 6676.357659040179
INFO:root:current train perplexity187.78875732421875
INFO:root:current mean train loss 6655.522061317845
INFO:root:current train perplexity186.87806701660156
INFO:root:current mean train loss 6650.128500366211
INFO:root:current train perplexity187.06805419921875
INFO:root:current mean train loss 6650.700403252963
INFO:root:current train perplexity187.4111785888672
INFO:root:current mean train loss 6646.679617130055
INFO:root:current train perplexity187.3665008544922
INFO:root:current mean train loss 6640.248481946114
INFO:root:current train perplexity187.177001953125
INFO:root:current mean train loss 6642.487905051492
INFO:root:current train perplexity187.19754028320312
INFO:root:current mean train loss 6637.1460523955675
INFO:root:current train perplexity187.01588439941406
INFO:root:current mean train loss 6636.850998263889
INFO:root:current train perplexity187.08616638183594
INFO:root:current mean train loss 6639.438891187765
INFO:root:current train perplexity187.29833984375
INFO:root:current mean train loss 6635.996998596192
INFO:root:current train perplexity187.08018493652344
INFO:root:current mean train loss 6634.062451525702
INFO:root:current train perplexity186.9738006591797
INFO:root:current mean train loss 6631.374262629329
INFO:root:current train perplexity187.05369567871094
INFO:root:current mean train loss 6629.948084268691
INFO:root:current train perplexity186.9835968017578
INFO:root:current mean train loss 6628.700383068266
INFO:root:current train perplexity186.8629150390625
INFO:root:current mean train loss 6630.331970626316
INFO:root:current train perplexity186.93215942382812
INFO:root:current mean train loss 6629.722684300199
INFO:root:current train perplexity186.96292114257812
INFO:root:current mean train loss 6628.829091205019
INFO:root:current train perplexity186.8262176513672

100%|██████████| 1/1 [07:28<00:00, 448.30s/it][A100%|██████████| 1/1 [07:28<00:00, 448.30s/it]
INFO:root:final mean train loss: 6627.32419215677
INFO:root:final train perplexity: 186.84323120117188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.79s/it][A100%|██████████| 1/1 [00:39<00:00, 39.79s/it]
INFO:root:eval mean loss: 6343.55691420102
INFO:root:eval perplexity: 169.57568359375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.77s/it][A100%|██████████| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 6471.251332384475
INFO:root:eval perplexity: 204.425048828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/94
 47%|████▋     | 94/200 [14:18:41<15:47:00, 536.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6630.794574540915
INFO:root:current train perplexity186.8985595703125
INFO:root:current mean train loss 6622.69091796875
INFO:root:current train perplexity187.21609497070312
INFO:root:current mean train loss 6625.437776199495
INFO:root:current train perplexity188.23593139648438
INFO:root:current mean train loss 6614.60768138972
INFO:root:current train perplexity187.26083374023438
INFO:root:current mean train loss 6616.169587839538
INFO:root:current train perplexity187.04620361328125
INFO:root:current mean train loss 6615.145282074434
INFO:root:current train perplexity186.82139587402344
INFO:root:current mean train loss 6609.217961184093
INFO:root:current train perplexity186.7327880859375
INFO:root:current mean train loss 6608.39009995981
INFO:root:current train perplexity186.5650177001953
INFO:root:current mean train loss 6606.445171513552
INFO:root:current train perplexity186.42930603027344
INFO:root:current mean train loss 6611.480435446966
INFO:root:current train perplexity186.55967712402344
INFO:root:current mean train loss 6614.020329325006
INFO:root:current train perplexity186.50462341308594
INFO:root:current mean train loss 6619.196750013053
INFO:root:current train perplexity186.6678009033203
INFO:root:current mean train loss 6620.555713003566
INFO:root:current train perplexity186.71737670898438
INFO:root:current mean train loss 6621.134311596837
INFO:root:current train perplexity186.6617431640625
INFO:root:current mean train loss 6620.102818592915
INFO:root:current train perplexity186.7025146484375
INFO:root:current mean train loss 6620.977169717635
INFO:root:current train perplexity186.66912841796875
INFO:root:current mean train loss 6622.249639759502
INFO:root:current train perplexity186.69699096679688
INFO:root:current mean train loss 6627.183729338394
INFO:root:current train perplexity186.79156494140625
INFO:root:current mean train loss 6628.252224163564
INFO:root:current train perplexity186.74855041503906

100%|██████████| 1/1 [07:42<00:00, 462.37s/it][A100%|██████████| 1/1 [07:42<00:00, 462.37s/it]
INFO:root:final mean train loss: 6627.219580669086
INFO:root:final train perplexity: 186.82772827148438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.73s/it][A100%|██████████| 1/1 [00:40<00:00, 40.73s/it]
INFO:root:eval mean loss: 6344.535814217642
INFO:root:eval perplexity: 169.7101287841797
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.26s/it][A100%|██████████| 1/1 [00:38<00:00, 38.26s/it]
INFO:root:eval mean loss: 6473.604278348016
INFO:root:eval perplexity: 204.82080078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/95
 48%|████▊     | 95/200 [14:27:45<15:42:13, 538.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6671.113385881697
INFO:root:current train perplexity191.8451690673828
INFO:root:current mean train loss 6609.530076411733
INFO:root:current train perplexity187.0511932373047
INFO:root:current mean train loss 6617.226923006718
INFO:root:current train perplexity187.53176879882812
INFO:root:current mean train loss 6616.642920232882
INFO:root:current train perplexity187.42529296875
INFO:root:current mean train loss 6617.553950360431
INFO:root:current train perplexity187.1363067626953
INFO:root:current mean train loss 6619.617821125668
INFO:root:current train perplexity186.71542358398438
INFO:root:current mean train loss 6611.302609521325
INFO:root:current train perplexity186.3584747314453
INFO:root:current mean train loss 6602.957635788691
INFO:root:current train perplexity186.49110412597656
INFO:root:current mean train loss 6613.312057907517
INFO:root:current train perplexity186.70216369628906
INFO:root:current mean train loss 6612.826168669652
INFO:root:current train perplexity186.88839721679688
INFO:root:current mean train loss 6620.9863724266515
INFO:root:current train perplexity186.96978759765625
INFO:root:current mean train loss 6625.073095790788
INFO:root:current train perplexity186.84483337402344
INFO:root:current mean train loss 6626.682751122967
INFO:root:current train perplexity186.94937133789062
INFO:root:current mean train loss 6629.875873629542
INFO:root:current train perplexity187.04852294921875
INFO:root:current mean train loss 6628.62833474684
INFO:root:current train perplexity187.0218505859375
INFO:root:current mean train loss 6629.901101438656
INFO:root:current train perplexity187.1917266845703
INFO:root:current mean train loss 6629.745751287562
INFO:root:current train perplexity187.24636840820312
INFO:root:current mean train loss 6632.35133174865
INFO:root:current train perplexity187.35496520996094
INFO:root:current mean train loss 6634.812000951799
INFO:root:current train perplexity187.44271850585938
INFO:root:current mean train loss 6632.770768790409
INFO:root:current train perplexity187.49331665039062

100%|██████████| 1/1 [07:39<00:00, 459.40s/it][A100%|██████████| 1/1 [07:39<00:00, 459.40s/it]
INFO:root:final mean train loss: 6632.372040887583
INFO:root:final train perplexity: 187.58900451660156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.50s/it][A100%|██████████| 1/1 [00:40<00:00, 40.50s/it]
INFO:root:eval mean loss: 6340.93305525543
INFO:root:eval perplexity: 169.21615600585938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.33s/it][A100%|██████████| 1/1 [00:38<00:00, 38.33s/it]
INFO:root:eval mean loss: 6464.523389883921
INFO:root:eval perplexity: 203.29742431640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/96
 48%|████▊     | 96/200 [14:36:45<15:34:25, 539.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6625.887553553427
INFO:root:current train perplexity188.66888427734375
INFO:root:current mean train loss 6687.732529967796
INFO:root:current train perplexity190.70335388183594
INFO:root:current mean train loss 6652.895338710769
INFO:root:current train perplexity188.9401397705078
INFO:root:current mean train loss 6653.60597030778
INFO:root:current train perplexity189.24436950683594
INFO:root:current mean train loss 6641.611919500435
INFO:root:current train perplexity188.23162841796875
INFO:root:current mean train loss 6645.316388778543
INFO:root:current train perplexity188.382568359375
INFO:root:current mean train loss 6655.102881865343
INFO:root:current train perplexity188.73854064941406
INFO:root:current mean train loss 6648.6841114216395
INFO:root:current train perplexity188.53578186035156
INFO:root:current mean train loss 6646.463663883875
INFO:root:current train perplexity188.7159423828125
INFO:root:current mean train loss 6648.087059340594
INFO:root:current train perplexity188.65440368652344
INFO:root:current mean train loss 6650.143473228359
INFO:root:current train perplexity188.89149475097656
INFO:root:current mean train loss 6650.81580269811
INFO:root:current train perplexity189.13368225097656
INFO:root:current mean train loss 6650.635449060088
INFO:root:current train perplexity189.056396484375
INFO:root:current mean train loss 6647.594026240256
INFO:root:current train perplexity188.9635772705078
INFO:root:current mean train loss 6647.69724003374
INFO:root:current train perplexity189.02801513671875
INFO:root:current mean train loss 6644.036050847587
INFO:root:current train perplexity188.83782958984375
INFO:root:current mean train loss 6644.635361382013
INFO:root:current train perplexity188.91465759277344
INFO:root:current mean train loss 6643.940917686669
INFO:root:current train perplexity188.82188415527344
INFO:root:current mean train loss 6641.124539986261
INFO:root:current train perplexity188.6954345703125
INFO:root:current mean train loss 6641.737817496602
INFO:root:current train perplexity188.72926330566406

100%|██████████| 1/1 [07:44<00:00, 464.42s/it][A100%|██████████| 1/1 [07:44<00:00, 464.42s/it]
INFO:root:final mean train loss: 6639.559708080205
INFO:root:final train perplexity: 188.65611267089844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.40s/it][A100%|██████████| 1/1 [00:40<00:00, 40.40s/it]
INFO:root:eval mean loss: 6339.981010707557
INFO:root:eval perplexity: 169.0857391357422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.86s/it][A100%|██████████| 1/1 [00:38<00:00, 38.86s/it]
INFO:root:eval mean loss: 6466.716607276429
INFO:root:eval perplexity: 203.6643829345703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/97
 48%|████▊     | 97/200 [14:45:52<15:29:04, 541.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6675.657725016276
INFO:root:current train perplexity187.5601043701172
INFO:root:current mean train loss 6620.2462092219175
INFO:root:current train perplexity188.0742645263672
INFO:root:current mean train loss 6603.128845214844
INFO:root:current train perplexity186.95213317871094
INFO:root:current mean train loss 6610.685239594558
INFO:root:current train perplexity186.561279296875
INFO:root:current mean train loss 6622.594746180943
INFO:root:current train perplexity186.6537322998047
INFO:root:current mean train loss 6633.880009337933
INFO:root:current train perplexity187.31915283203125
INFO:root:current mean train loss 6638.076957796827
INFO:root:current train perplexity187.38507080078125
INFO:root:current mean train loss 6645.533976672167
INFO:root:current train perplexity187.24600219726562
INFO:root:current mean train loss 6642.019005541531
INFO:root:current train perplexity187.18780517578125
INFO:root:current mean train loss 6643.774879680907
INFO:root:current train perplexity187.48365783691406
INFO:root:current mean train loss 6648.004471407592
INFO:root:current train perplexity187.7822265625
INFO:root:current mean train loss 6647.1397288252665
INFO:root:current train perplexity187.64134216308594
INFO:root:current mean train loss 6646.631284273588
INFO:root:current train perplexity187.44378662109375
INFO:root:current mean train loss 6645.023702287533
INFO:root:current train perplexity187.4071502685547
INFO:root:current mean train loss 6641.4535856299635
INFO:root:current train perplexity187.4698028564453
INFO:root:current mean train loss 6641.863030169978
INFO:root:current train perplexity187.6662139892578
INFO:root:current mean train loss 6640.242902440932
INFO:root:current train perplexity187.75132751464844
INFO:root:current mean train loss 6636.477146314538
INFO:root:current train perplexity187.78553771972656
INFO:root:current mean train loss 6637.380632144548
INFO:root:current train perplexity187.81239318847656
INFO:root:current mean train loss 6635.267514959253
INFO:root:current train perplexity187.75241088867188

100%|██████████| 1/1 [07:42<00:00, 462.40s/it][A100%|██████████| 1/1 [07:42<00:00, 462.40s/it]
INFO:root:final mean train loss: 6633.02692330611
INFO:root:final train perplexity: 187.68617248535156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.24s/it][A100%|██████████| 1/1 [00:41<00:00, 41.24s/it]
INFO:root:eval mean loss: 6344.974765209441
INFO:root:eval perplexity: 169.7703399658203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.50s/it][A100%|██████████| 1/1 [00:37<00:00, 37.50s/it]
INFO:root:eval mean loss: 6467.203290357657
INFO:root:eval perplexity: 203.74578857421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/98
 49%|████▉     | 98/200 [14:54:55<15:21:21, 541.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6681.0220703125
INFO:root:current train perplexity191.0074005126953
INFO:root:current mean train loss 6700.022333688447
INFO:root:current train perplexity191.5299072265625
INFO:root:current mean train loss 6680.13039504717
INFO:root:current train perplexity190.4995880126953
INFO:root:current mean train loss 6666.228412617723
INFO:root:current train perplexity190.26998901367188
INFO:root:current mean train loss 6650.480253486223
INFO:root:current train perplexity189.52305603027344
INFO:root:current mean train loss 6645.332529037611
INFO:root:current train perplexity189.5048828125
INFO:root:current mean train loss 6647.370259633459
INFO:root:current train perplexity189.4765167236328
INFO:root:current mean train loss 6654.351988230188
INFO:root:current train perplexity189.68289184570312
INFO:root:current mean train loss 6656.819959131141
INFO:root:current train perplexity189.6492462158203
INFO:root:current mean train loss 6653.142818976684
INFO:root:current train perplexity189.4994659423828
INFO:root:current mean train loss 6646.782354936913
INFO:root:current train perplexity189.2188262939453
INFO:root:current mean train loss 6651.046414800161
INFO:root:current train perplexity189.50262451171875
INFO:root:current mean train loss 6643.198615828805
INFO:root:current train perplexity189.37850952148438
INFO:root:current mean train loss 6642.6359643286405
INFO:root:current train perplexity189.2801055908203
INFO:root:current mean train loss 6642.728775597269
INFO:root:current train perplexity189.24996948242188
INFO:root:current mean train loss 6644.190559167831
INFO:root:current train perplexity189.19744873046875
INFO:root:current mean train loss 6644.245974392361
INFO:root:current train perplexity189.1332244873047
INFO:root:current mean train loss 6644.096507889961
INFO:root:current train perplexity189.23922729492188
INFO:root:current mean train loss 6645.038510860003
INFO:root:current train perplexity189.27748107910156
INFO:root:current mean train loss 6644.592971234892
INFO:root:current train perplexity189.20169067382812

100%|██████████| 1/1 [07:39<00:00, 459.49s/it][A100%|██████████| 1/1 [07:39<00:00, 459.49s/it]
INFO:root:final mean train loss: 6643.850697727559
INFO:root:final train perplexity: 189.29598999023438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.85s/it][A100%|██████████| 1/1 [00:39<00:00, 39.87s/it]
INFO:root:eval mean loss: 6344.734795752992
INFO:root:eval perplexity: 169.73739624023438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.38s/it][A100%|██████████| 1/1 [00:37<00:00, 37.38s/it]
INFO:root:eval mean loss: 6467.591898167387
INFO:root:eval perplexity: 203.8109893798828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/99
 50%|████▉     | 99/200 [15:03:54<15:10:51, 541.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6565.0626845941315
INFO:root:current train perplexity186.70883178710938
INFO:root:current mean train loss 6620.855903373969
INFO:root:current train perplexity188.7948760986328
INFO:root:current mean train loss 6637.903900362921
INFO:root:current train perplexity188.8963623046875
INFO:root:current mean train loss 6638.553838759817
INFO:root:current train perplexity188.66943359375
INFO:root:current mean train loss 6641.593330604902
INFO:root:current train perplexity189.11131286621094
INFO:root:current mean train loss 6638.151962857066
INFO:root:current train perplexity188.5895538330078
INFO:root:current mean train loss 6635.068487530929
INFO:root:current train perplexity188.8807830810547
INFO:root:current mean train loss 6634.929003156969
INFO:root:current train perplexity188.77146911621094
INFO:root:current mean train loss 6637.542955463436
INFO:root:current train perplexity189.0726318359375
INFO:root:current mean train loss 6640.747290586017
INFO:root:current train perplexity189.19735717773438
INFO:root:current mean train loss 6638.228052163961
INFO:root:current train perplexity189.134765625
INFO:root:current mean train loss 6640.754892313743
INFO:root:current train perplexity189.09237670898438
INFO:root:current mean train loss 6637.436885649254
INFO:root:current train perplexity189.07127380371094
INFO:root:current mean train loss 6639.099036298164
INFO:root:current train perplexity189.05343627929688
INFO:root:current mean train loss 6641.782197568741
INFO:root:current train perplexity189.19537353515625
INFO:root:current mean train loss 6643.065177522025
INFO:root:current train perplexity189.3189239501953
INFO:root:current mean train loss 6644.4957912594755
INFO:root:current train perplexity189.447509765625
INFO:root:current mean train loss 6646.473105074179
INFO:root:current train perplexity189.47561645507812
INFO:root:current mean train loss 6648.0505228397315
INFO:root:current train perplexity189.51229858398438
INFO:root:current mean train loss 6647.250378159293
INFO:root:current train perplexity189.52503967285156

100%|██████████| 1/1 [07:38<00:00, 458.93s/it][A100%|██████████| 1/1 [07:38<00:00, 458.93s/it]
INFO:root:final mean train loss: 6645.443085193874
INFO:root:final train perplexity: 189.53416442871094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.97s/it][A100%|██████████| 1/1 [00:39<00:00, 39.97s/it]
INFO:root:eval mean loss: 6344.856905889849
INFO:root:eval perplexity: 169.75424194335938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.31s/it][A100%|██████████| 1/1 [00:38<00:00, 38.31s/it]
INFO:root:eval mean loss: 6467.332259807181
INFO:root:eval perplexity: 203.76734924316406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/100
 50%|█████     | 100/200 [15:12:54<15:01:08, 540.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6680.865485913826
INFO:root:current train perplexity190.5984344482422
INFO:root:current mean train loss 6672.965908644786
INFO:root:current train perplexity189.60520935058594
INFO:root:current mean train loss 6667.823583494461
INFO:root:current train perplexity189.1676788330078
INFO:root:current mean train loss 6659.244825932017
INFO:root:current train perplexity189.6493377685547
INFO:root:current mean train loss 6643.4606821064
INFO:root:current train perplexity188.8110809326172
INFO:root:current mean train loss 6645.16294328438
INFO:root:current train perplexity189.0210723876953
INFO:root:current mean train loss 6645.5450685689375
INFO:root:current train perplexity189.2091827392578
INFO:root:current mean train loss 6655.672748283988
INFO:root:current train perplexity189.4966583251953
INFO:root:current mean train loss 6651.079570290774
INFO:root:current train perplexity189.4552764892578
INFO:root:current mean train loss 6658.0502108553865
INFO:root:current train perplexity189.60919189453125
INFO:root:current mean train loss 6653.694315499886
INFO:root:current train perplexity189.5379638671875
INFO:root:current mean train loss 6653.657825205901
INFO:root:current train perplexity189.51600646972656
INFO:root:current mean train loss 6649.780610235037
INFO:root:current train perplexity189.39927673339844
INFO:root:current mean train loss 6650.1893756422
INFO:root:current train perplexity189.4014434814453
INFO:root:current mean train loss 6651.355267443921
INFO:root:current train perplexity189.382568359375
INFO:root:current mean train loss 6648.273867761589
INFO:root:current train perplexity189.2950897216797
INFO:root:current mean train loss 6647.6777536303525
INFO:root:current train perplexity189.2034912109375
INFO:root:current mean train loss 6647.9907614690455
INFO:root:current train perplexity189.23191833496094
INFO:root:current mean train loss 6645.70950891094
INFO:root:current train perplexity189.13043212890625

100%|██████████| 1/1 [07:46<00:00, 466.07s/it][A100%|██████████| 1/1 [07:46<00:00, 466.07s/it]
INFO:root:final mean train loss: 6642.790698476109
INFO:root:final train perplexity: 189.13783264160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.03s/it][A100%|██████████| 1/1 [00:41<00:00, 41.03s/it]
INFO:root:eval mean loss: 6342.606476479388
INFO:root:eval perplexity: 169.4453125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.41s/it][A100%|██████████| 1/1 [00:38<00:00, 38.41s/it]
INFO:root:eval mean loss: 6467.266996343085
INFO:root:eval perplexity: 203.7564697265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/101
 50%|█████     | 101/200 [15:22:02<14:55:40, 542.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6705.091888427734
INFO:root:current train perplexity188.11892700195312
INFO:root:current mean train loss 6634.78215079472
INFO:root:current train perplexity188.30604553222656
INFO:root:current mean train loss 6642.358717176649
INFO:root:current train perplexity188.8802490234375
INFO:root:current mean train loss 6651.13816350623
INFO:root:current train perplexity189.431884765625
INFO:root:current mean train loss 6659.448883056641
INFO:root:current train perplexity189.21170043945312
INFO:root:current mean train loss 6648.723362175993
INFO:root:current train perplexity189.03683471679688
INFO:root:current mean train loss 6645.688184862013
INFO:root:current train perplexity188.84593200683594
INFO:root:current mean train loss 6641.453565544256
INFO:root:current train perplexity188.77174377441406
INFO:root:current mean train loss 6636.474447212967
INFO:root:current train perplexity188.42559814453125
INFO:root:current mean train loss 6638.018407563455
INFO:root:current train perplexity188.5749969482422
INFO:root:current mean train loss 6635.756741260919
INFO:root:current train perplexity188.39352416992188
INFO:root:current mean train loss 6641.9637026769715
INFO:root:current train perplexity188.71136474609375
INFO:root:current mean train loss 6635.787528188605
INFO:root:current train perplexity188.44357299804688
INFO:root:current mean train loss 6636.671060950561
INFO:root:current train perplexity188.515380859375
INFO:root:current mean train loss 6643.755140746381
INFO:root:current train perplexity188.74851989746094
INFO:root:current mean train loss 6643.464572232128
INFO:root:current train perplexity188.6571044921875
INFO:root:current mean train loss 6645.27023194568
INFO:root:current train perplexity188.8203582763672
INFO:root:current mean train loss 6642.990465711046
INFO:root:current train perplexity188.79397583007812
INFO:root:current mean train loss 6644.231494248176
INFO:root:current train perplexity188.93203735351562
INFO:root:current mean train loss 6642.407301231814
INFO:root:current train perplexity188.81045532226562

100%|██████████| 1/1 [07:44<00:00, 464.66s/it][A100%|██████████| 1/1 [07:44<00:00, 464.66s/it]
INFO:root:final mean train loss: 6640.4641056647515
INFO:root:final train perplexity: 188.791015625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.15s/it][A100%|██████████| 1/1 [00:40<00:00, 40.15s/it]
INFO:root:eval mean loss: 6341.756716464428
INFO:root:eval perplexity: 169.32891845703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.80s/it][A100%|██████████| 1/1 [00:37<00:00, 37.80s/it]
INFO:root:eval mean loss: 6466.25253057818
INFO:root:eval perplexity: 203.58660888671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/102
 51%|█████     | 102/200 [15:31:07<14:47:47, 543.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6581.56767874053
INFO:root:current train perplexity188.07040405273438
INFO:root:current mean train loss 6615.390037593985
INFO:root:current train perplexity187.3326416015625
INFO:root:current mean train loss 6615.981487225054
INFO:root:current train perplexity187.76547241210938
INFO:root:current mean train loss 6617.658626888608
INFO:root:current train perplexity187.8748321533203
INFO:root:current mean train loss 6612.533580894558
INFO:root:current train perplexity187.62362670898438
INFO:root:current mean train loss 6611.240193150505
INFO:root:current train perplexity187.3871307373047
INFO:root:current mean train loss 6627.6065432773
INFO:root:current train perplexity187.85018920898438
INFO:root:current mean train loss 6634.14725176927
INFO:root:current train perplexity188.10556030273438
INFO:root:current mean train loss 6639.093871923769
INFO:root:current train perplexity188.2736358642578
INFO:root:current mean train loss 6639.851867087018
INFO:root:current train perplexity188.54010009765625
INFO:root:current mean train loss 6641.520890685503
INFO:root:current train perplexity188.7541046142578
INFO:root:current mean train loss 6641.177552939514
INFO:root:current train perplexity188.68148803710938
INFO:root:current mean train loss 6640.428628171254
INFO:root:current train perplexity188.82855224609375
INFO:root:current mean train loss 6640.8638189820895
INFO:root:current train perplexity188.8016357421875
INFO:root:current mean train loss 6642.667801787116
INFO:root:current train perplexity188.9649200439453
INFO:root:current mean train loss 6642.70027366683
INFO:root:current train perplexity189.0145721435547
INFO:root:current mean train loss 6642.118234030542
INFO:root:current train perplexity188.9048309326172
INFO:root:current mean train loss 6642.1577897905545
INFO:root:current train perplexity188.9611358642578
INFO:root:current mean train loss 6638.2234974896
INFO:root:current train perplexity188.8436737060547
INFO:root:current mean train loss 6642.237457764809
INFO:root:current train perplexity188.89816284179688

100%|██████████| 1/1 [07:40<00:00, 460.72s/it][A100%|██████████| 1/1 [07:40<00:00, 460.72s/it]
INFO:root:final mean train loss: 6641.03942323224
INFO:root:final train perplexity: 188.87664794921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.14s/it][A100%|██████████| 1/1 [00:40<00:00, 40.14s/it]
INFO:root:eval mean loss: 6341.4351070755765
INFO:root:eval perplexity: 169.28475952148438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.42s/it][A100%|██████████| 1/1 [00:37<00:00, 37.42s/it]
INFO:root:eval mean loss: 6465.619426321476
INFO:root:eval perplexity: 203.48072814941406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat/103
 52%|█████▏    | 103/200 [15:40:08<14:37:25, 542.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6637.8116015625
INFO:root:current train perplexity190.6968994140625
slurmstepd: error: *** JOB 29876523 ON ga012 CANCELLED AT 2023-02-07T04:40:20 ***
