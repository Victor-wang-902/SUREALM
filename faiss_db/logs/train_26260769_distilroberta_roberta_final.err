INFO:root:Output: large_distilroberta_roberta_final
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11343.012719973169
INFO:root:current train perplexity7710.19873046875
INFO:root:current mean train loss 9713.091502434045
INFO:root:current train perplexity2148.275146484375
INFO:root:current mean train loss 8954.640985903532
INFO:root:current train perplexity1184.9744873046875
INFO:root:current mean train loss 8520.748365053258
INFO:root:current train perplexity833.951171875
INFO:root:current mean train loss 8163.446889873497
INFO:root:current train perplexity636.325927734375
INFO:root:current mean train loss 7894.538332116027
INFO:root:current train perplexity510.3077392578125
INFO:root:current mean train loss 7661.498211032502
INFO:root:current train perplexity423.9737854003906
INFO:root:current mean train loss 7444.807195517835
INFO:root:current train perplexity357.8048400878906
INFO:root:current mean train loss 7246.924272955193
INFO:root:current train perplexity306.85858154296875
INFO:root:current mean train loss 7074.402419998123
INFO:root:current train perplexity266.0047607421875
INFO:root:current mean train loss 6898.3204244625795
INFO:root:current train perplexity231.98825073242188
INFO:root:current mean train loss 6735.46896909534
INFO:root:current train perplexity203.6498260498047
INFO:root:current mean train loss 6585.330110639495
INFO:root:current train perplexity180.6639404296875
INFO:root:current mean train loss 6440.0116953655515
INFO:root:current train perplexity161.11978149414062
INFO:root:current mean train loss 6307.269739070839
INFO:root:current train perplexity144.85638427734375
INFO:root:current mean train loss 6182.569227379661
INFO:root:current train perplexity131.2262725830078
INFO:root:current mean train loss 6065.612050775502
INFO:root:current train perplexity119.53063201904297
INFO:root:current mean train loss 5954.781122840597
INFO:root:current train perplexity109.5525131225586
INFO:root:current mean train loss 5851.60982844075
INFO:root:current train perplexity101.04078674316406

100%|██████████| 1/1 [05:25<00:00, 325.55s/it][A100%|██████████| 1/1 [05:25<00:00, 325.55s/it]
INFO:root:final mean train loss: 5766.505054991352
INFO:root:final train perplexity: 94.72122955322266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.63s/it][A100%|██████████| 1/1 [00:20<00:00, 20.63s/it]
INFO:root:eval mean loss: 3492.998755921709
INFO:root:eval perplexity: 16.88770866394043
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.73s/it][A100%|██████████| 1/1 [00:20<00:00, 20.73s/it]
INFO:root:eval mean loss: 3758.2636684120125
INFO:root:eval perplexity: 21.9721622467041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/1
  0%|          | 1/200 [06:43<22:18:40, 403.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3844.2975006103516
INFO:root:current train perplexity21.4053955078125
INFO:root:current mean train loss 3808.2024515086205
INFO:root:current train perplexity20.405841827392578
INFO:root:current mean train loss 3769.1418683087386
INFO:root:current train perplexity19.72040557861328
INFO:root:current mean train loss 3756.12327479109
INFO:root:current train perplexity19.34014892578125
INFO:root:current mean train loss 3728.017096886268
INFO:root:current train perplexity18.88896942138672
INFO:root:current mean train loss 3693.7222133902615
INFO:root:current train perplexity18.47589111328125
INFO:root:current mean train loss 3662.872822550984
INFO:root:current train perplexity18.066150665283203
INFO:root:current mean train loss 3631.8347733993105
INFO:root:current train perplexity17.64240837097168
INFO:root:current mean train loss 3603.979019763423
INFO:root:current train perplexity17.26613426208496
INFO:root:current mean train loss 3579.192884686732
INFO:root:current train perplexity16.90955352783203
INFO:root:current mean train loss 3554.328752172275
INFO:root:current train perplexity16.584089279174805
INFO:root:current mean train loss 3535.512499081191
INFO:root:current train perplexity16.28510284423828
INFO:root:current mean train loss 3514.6762771606445
INFO:root:current train perplexity16.007776260375977
INFO:root:current mean train loss 3492.5519366510734
INFO:root:current train perplexity15.735164642333984
INFO:root:current mean train loss 3471.7920925334347
INFO:root:current train perplexity15.488724708557129
INFO:root:current mean train loss 3453.201796398314
INFO:root:current train perplexity15.255434036254883
INFO:root:current mean train loss 3436.38131200205
INFO:root:current train perplexity15.040398597717285
INFO:root:current mean train loss 3418.9146823838596
INFO:root:current train perplexity14.822061538696289
INFO:root:current mean train loss 3400.6936391418726
INFO:root:current train perplexity14.625539779663086
INFO:root:current mean train loss 3384.7753164653736
INFO:root:current train perplexity14.433732986450195

100%|██████████| 1/1 [05:39<00:00, 339.32s/it][A100%|██████████| 1/1 [05:39<00:00, 339.32s/it]
INFO:root:final mean train loss: 3371.7181759063365
INFO:root:final train perplexity: 14.310125350952148
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.88s/it][A100%|██████████| 1/1 [00:22<00:00, 22.88s/it]
INFO:root:eval mean loss: 2774.8526178454676
INFO:root:eval perplexity: 9.44467830657959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.49s/it][A100%|██████████| 1/1 [00:22<00:00, 22.49s/it]
INFO:root:eval mean loss: 3116.8141401574967
INFO:root:eval perplexity: 12.967185974121094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/2
  1%|          | 2/200 [13:46<22:48:38, 414.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3023.658817175663
INFO:root:current train perplexity11.14940357208252
INFO:root:current mean train loss 3013.92616306391
INFO:root:current train perplexity10.832648277282715
INFO:root:current mean train loss 3003.2446896794527
INFO:root:current train perplexity10.717752456665039
INFO:root:current mean train loss 2996.9741621504318
INFO:root:current train perplexity10.625223159790039
INFO:root:current mean train loss 2993.219091120273
INFO:root:current train perplexity10.579837799072266
INFO:root:current mean train loss 2982.3166902409707
INFO:root:current train perplexity10.519176483154297
INFO:root:current mean train loss 2975.737691918444
INFO:root:current train perplexity10.47278881072998
INFO:root:current mean train loss 2968.548802478577
INFO:root:current train perplexity10.419468879699707
INFO:root:current mean train loss 2962.712217992666
INFO:root:current train perplexity10.342920303344727
INFO:root:current mean train loss 2955.2461615232282
INFO:root:current train perplexity10.274219512939453
INFO:root:current mean train loss 2946.393258551776
INFO:root:current train perplexity10.20881462097168
INFO:root:current mean train loss 2939.088090160939
INFO:root:current train perplexity10.139760971069336
INFO:root:current mean train loss 2932.7824689210765
INFO:root:current train perplexity10.08627700805664
INFO:root:current mean train loss 2924.3694437403296
INFO:root:current train perplexity10.040877342224121
INFO:root:current mean train loss 2916.3029441008266
INFO:root:current train perplexity9.985845565795898
INFO:root:current mean train loss 2908.8716105382114
INFO:root:current train perplexity9.925856590270996
INFO:root:current mean train loss 2902.766564186409
INFO:root:current train perplexity9.871956825256348
INFO:root:current mean train loss 2894.891623398505
INFO:root:current train perplexity9.817633628845215
INFO:root:current mean train loss 2888.3127722441013
INFO:root:current train perplexity9.76537036895752
INFO:root:current mean train loss 2882.5334357721968
INFO:root:current train perplexity9.721643447875977

100%|██████████| 1/1 [05:35<00:00, 335.57s/it][A100%|██████████| 1/1 [05:35<00:00, 335.57s/it]
INFO:root:final mean train loss: 2878.2860028011537
INFO:root:final train perplexity: 9.694415092468262
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.60s/it][A100%|██████████| 1/1 [00:21<00:00, 21.60s/it]
INFO:root:eval mean loss: 2534.5233859880595
INFO:root:eval perplexity: 7.775467872619629
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.16s/it][A100%|██████████| 1/1 [00:21<00:00, 21.16s/it]
INFO:root:eval mean loss: 2894.7829507597794
INFO:root:eval perplexity: 10.80364990234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/3
  2%|▏         | 3/200 [21:41<24:12:58, 442.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2724.9070947265627
INFO:root:current train perplexity8.678732872009277
INFO:root:current mean train loss 2730.807177734375
INFO:root:current train perplexity8.605193138122559
INFO:root:current mean train loss 2719.755212890625
INFO:root:current train perplexity8.532503128051758
INFO:root:current mean train loss 2711.1750306919644
INFO:root:current train perplexity8.50898265838623
INFO:root:current mean train loss 2719.4869992404515
INFO:root:current train perplexity8.518171310424805
INFO:root:current mean train loss 2716.3331183416194
INFO:root:current train perplexity8.48635196685791
INFO:root:current mean train loss 2709.3796345402643
INFO:root:current train perplexity8.456469535827637
INFO:root:current mean train loss 2703.5293528645834
INFO:root:current train perplexity8.42464828491211
INFO:root:current mean train loss 2695.4480784696693
INFO:root:current train perplexity8.386702537536621
INFO:root:current mean train loss 2691.861742136102
INFO:root:current train perplexity8.36414623260498
INFO:root:current mean train loss 2688.5625637090775
INFO:root:current train perplexity8.342430114746094
INFO:root:current mean train loss 2681.020608865489
INFO:root:current train perplexity8.315550804138184
INFO:root:current mean train loss 2675.6988443359373
INFO:root:current train perplexity8.291143417358398
INFO:root:current mean train loss 2671.7356568287037
INFO:root:current train perplexity8.258315086364746
INFO:root:current mean train loss 2668.6036759159483
INFO:root:current train perplexity8.242555618286133
INFO:root:current mean train loss 2666.5652529611893
INFO:root:current train perplexity8.218389511108398
INFO:root:current mean train loss 2663.061809156013
INFO:root:current train perplexity8.190653800964355
INFO:root:current mean train loss 2662.011595842634
INFO:root:current train perplexity8.174192428588867
INFO:root:current mean train loss 2659.3453147434543
INFO:root:current train perplexity8.152304649353027
INFO:root:current mean train loss 2655.786748735477
INFO:root:current train perplexity8.128888130187988

100%|██████████| 1/1 [05:38<00:00, 338.37s/it][A100%|██████████| 1/1 [05:38<00:00, 338.37s/it]
INFO:root:final mean train loss: 2654.3246060139113
INFO:root:final train perplexity: 8.123807907104492
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.80s/it][A100%|██████████| 1/1 [00:21<00:00, 21.80s/it]
INFO:root:eval mean loss: 2400.744156208444
INFO:root:eval perplexity: 6.977686405181885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.42s/it][A100%|██████████| 1/1 [00:21<00:00, 21.42s/it]
INFO:root:eval mean loss: 2772.5141761725677
INFO:root:eval perplexity: 9.770458221435547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/4
  2%|▏         | 4/200 [28:05<22:49:37, 419.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2541.396090834888
INFO:root:current train perplexity7.395270347595215
INFO:root:current mean train loss 2574.338982679173
INFO:root:current train perplexity7.4957098960876465
INFO:root:current mean train loss 2571.274335425445
INFO:root:current train perplexity7.502583026885986
INFO:root:current mean train loss 2562.2963494656847
INFO:root:current train perplexity7.4788103103637695
INFO:root:current mean train loss 2555.4720145313336
INFO:root:current train perplexity7.460791110992432
INFO:root:current mean train loss 2548.3405630390903
INFO:root:current train perplexity7.439525604248047
INFO:root:current mean train loss 2543.120366086488
INFO:root:current train perplexity7.42028284072876
INFO:root:current mean train loss 2539.9523594743114
INFO:root:current train perplexity7.411649227142334
INFO:root:current mean train loss 2537.7698812693734
INFO:root:current train perplexity7.407225608825684
INFO:root:current mean train loss 2535.348197045469
INFO:root:current train perplexity7.390657424926758
INFO:root:current mean train loss 2533.673568654083
INFO:root:current train perplexity7.3779296875
INFO:root:current mean train loss 2532.6497454410346
INFO:root:current train perplexity7.3673996925354
INFO:root:current mean train loss 2528.8653336113234
INFO:root:current train perplexity7.352790355682373
INFO:root:current mean train loss 2527.578033201696
INFO:root:current train perplexity7.341542720794678
INFO:root:current mean train loss 2525.5327480448777
INFO:root:current train perplexity7.333704471588135
INFO:root:current mean train loss 2524.036269683935
INFO:root:current train perplexity7.326956748962402
INFO:root:current mean train loss 2522.1213048650034
INFO:root:current train perplexity7.31276273727417
INFO:root:current mean train loss 2521.8464324381234
INFO:root:current train perplexity7.305823802947998
INFO:root:current mean train loss 2519.3946915040633
INFO:root:current train perplexity7.296554088592529
INFO:root:current mean train loss 2516.723462087828
INFO:root:current train perplexity7.285401344299316

100%|██████████| 1/1 [05:25<00:00, 325.26s/it][A100%|██████████| 1/1 [05:25<00:00, 325.26s/it]
INFO:root:final mean train loss: 2515.8864474621196
INFO:root:final train perplexity: 7.283002853393555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.89s/it][A100%|██████████| 1/1 [00:21<00:00, 21.89s/it]
INFO:root:eval mean loss: 2317.4451471423426
INFO:root:eval perplexity: 6.52284574508667
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.99s/it][A100%|██████████| 1/1 [00:20<00:00, 20.99s/it]
INFO:root:eval mean loss: 2695.327854886968
INFO:root:eval perplexity: 9.169716835021973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/5
  2%|▎         | 5/200 [34:15<21:45:00, 401.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2411.7421293712796
INFO:root:current train perplexity6.807297706604004
INFO:root:current mean train loss 2445.2529960300612
INFO:root:current train perplexity6.895364284515381
INFO:root:current mean train loss 2442.008017956371
INFO:root:current train perplexity6.879797458648682
INFO:root:current mean train loss 2435.695622444153
INFO:root:current train perplexity6.863873481750488
INFO:root:current mean train loss 2439.2000946801554
INFO:root:current train perplexity6.871489524841309
INFO:root:current mean train loss 2437.7153004685492
INFO:root:current train perplexity6.874866962432861
INFO:root:current mean train loss 2437.890380323979
INFO:root:current train perplexity6.874683380126953
INFO:root:current mean train loss 2438.53258327562
INFO:root:current train perplexity6.861835479736328
INFO:root:current mean train loss 2438.296746301435
INFO:root:current train perplexity6.861630916595459
INFO:root:current mean train loss 2434.6955620990534
INFO:root:current train perplexity6.840236663818359
INFO:root:current mean train loss 2433.0158216187874
INFO:root:current train perplexity6.821771144866943
INFO:root:current mean train loss 2433.4459065617743
INFO:root:current train perplexity6.821752548217773
INFO:root:current mean train loss 2428.28057262385
INFO:root:current train perplexity6.804232597351074
INFO:root:current mean train loss 2425.2667389798025
INFO:root:current train perplexity6.792873859405518
INFO:root:current mean train loss 2425.910897391183
INFO:root:current train perplexity6.786556720733643
INFO:root:current mean train loss 2424.9266365128333
INFO:root:current train perplexity6.777303695678711
INFO:root:current mean train loss 2423.7245117332477
INFO:root:current train perplexity6.771235942840576
INFO:root:current mean train loss 2422.556918362331
INFO:root:current train perplexity6.76546049118042
INFO:root:current mean train loss 2421.053910565224
INFO:root:current train perplexity6.755202293395996

100%|██████████| 1/1 [05:27<00:00, 327.99s/it][A100%|██████████| 1/1 [05:27<00:00, 327.99s/it]
INFO:root:final mean train loss: 2418.1163730823328
INFO:root:final train perplexity: 6.7421770095825195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.66s/it][A100%|██████████| 1/1 [00:21<00:00, 21.66s/it]
INFO:root:eval mean loss: 2258.230179157663
INFO:root:eval perplexity: 6.217655658721924
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.69s/it][A100%|██████████| 1/1 [00:20<00:00, 20.69s/it]
INFO:root:eval mean loss: 2645.846634235788
INFO:root:eval perplexity: 8.804176330566406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/6
  3%|▎         | 6/200 [40:27<21:06:13, 391.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2268.594482421875
INFO:root:current train perplexity6.489818096160889
INFO:root:current mean train loss 2359.0977577738245
INFO:root:current train perplexity6.437716484069824
INFO:root:current mean train loss 2364.7673667793842
INFO:root:current train perplexity6.472103118896484
INFO:root:current mean train loss 2374.0875629412376
INFO:root:current train perplexity6.481991291046143
INFO:root:current mean train loss 2371.572561820546
INFO:root:current train perplexity6.488306522369385
INFO:root:current mean train loss 2369.473967835813
INFO:root:current train perplexity6.461410045623779
INFO:root:current mean train loss 2364.7071819591047
INFO:root:current train perplexity6.434986591339111
INFO:root:current mean train loss 2365.111341011167
INFO:root:current train perplexity6.429642677307129
INFO:root:current mean train loss 2364.3919133675677
INFO:root:current train perplexity6.424053192138672
INFO:root:current mean train loss 2362.47713654208
INFO:root:current train perplexity6.421297550201416
INFO:root:current mean train loss 2361.5418388984062
INFO:root:current train perplexity6.415623188018799
INFO:root:current mean train loss 2358.8978474380533
INFO:root:current train perplexity6.408877372741699
INFO:root:current mean train loss 2353.4332122929786
INFO:root:current train perplexity6.397253036499023
INFO:root:current mean train loss 2351.775487924703
INFO:root:current train perplexity6.3916215896606445
INFO:root:current mean train loss 2351.6251080422467
INFO:root:current train perplexity6.386017322540283
INFO:root:current mean train loss 2348.8114280421128
INFO:root:current train perplexity6.376195430755615
INFO:root:current mean train loss 2348.3878786085843
INFO:root:current train perplexity6.373051643371582
INFO:root:current mean train loss 2348.0047209850136
INFO:root:current train perplexity6.3706135749816895
INFO:root:current mean train loss 2347.9952865676837
INFO:root:current train perplexity6.367406368255615
INFO:root:current mean train loss 2346.341967490897
INFO:root:current train perplexity6.362870216369629

100%|██████████| 1/1 [05:33<00:00, 333.75s/it][A100%|██████████| 1/1 [05:33<00:00, 333.75s/it]
INFO:root:final mean train loss: 2344.720833105567
INFO:root:final train perplexity: 6.362738609313965
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.36s/it][A100%|██████████| 1/1 [00:22<00:00, 22.36s/it]
INFO:root:eval mean loss: 2209.1767158237753
INFO:root:eval perplexity: 5.975680828094482
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.86s/it][A100%|██████████| 1/1 [00:20<00:00, 20.87s/it]
INFO:root:eval mean loss: 2596.0117295718364
INFO:root:eval perplexity: 8.450754165649414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/7
  4%|▎         | 7/200 [46:46<20:46:21, 387.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2278.836181640625
INFO:root:current train perplexity6.064192771911621
INFO:root:current mean train loss 2289.982844983117
INFO:root:current train perplexity6.145378112792969
INFO:root:current mean train loss 2287.320187069954
INFO:root:current train perplexity6.125436782836914
INFO:root:current mean train loss 2290.492894586527
INFO:root:current train perplexity6.124031066894531
INFO:root:current mean train loss 2300.8356206428493
INFO:root:current train perplexity6.13058614730835
INFO:root:current mean train loss 2296.6233298990255
INFO:root:current train perplexity6.111351013183594
INFO:root:current mean train loss 2298.0527063264817
INFO:root:current train perplexity6.110729217529297
INFO:root:current mean train loss 2296.7057576724083
INFO:root:current train perplexity6.1064677238464355
INFO:root:current mean train loss 2294.3379761339106
INFO:root:current train perplexity6.093120098114014
INFO:root:current mean train loss 2296.2105280724486
INFO:root:current train perplexity6.097486972808838
INFO:root:current mean train loss 2297.5371803628436
INFO:root:current train perplexity6.102508068084717
INFO:root:current mean train loss 2296.740990927053
INFO:root:current train perplexity6.102232933044434
INFO:root:current mean train loss 2295.469311543482
INFO:root:current train perplexity6.100841045379639
INFO:root:current mean train loss 2294.7585884522596
INFO:root:current train perplexity6.09940242767334
INFO:root:current mean train loss 2293.3110132042543
INFO:root:current train perplexity6.095564842224121
INFO:root:current mean train loss 2292.416950210752
INFO:root:current train perplexity6.094942569732666
INFO:root:current mean train loss 2291.6552544253127
INFO:root:current train perplexity6.0891008377075195
INFO:root:current mean train loss 2289.739011611317
INFO:root:current train perplexity6.08423376083374
INFO:root:current mean train loss 2288.4698425897277
INFO:root:current train perplexity6.079118728637695
INFO:root:current mean train loss 2288.13080508617
INFO:root:current train perplexity6.078476905822754

100%|██████████| 1/1 [05:26<00:00, 326.59s/it][A100%|██████████| 1/1 [05:26<00:00, 326.59s/it]
INFO:root:final mean train loss: 2286.7699414751955
INFO:root:final train perplexity: 6.078291893005371
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.60s/it][A100%|██████████| 1/1 [00:22<00:00, 22.60s/it]
INFO:root:eval mean loss: 2174.3622505783187
INFO:root:eval perplexity: 5.809682369232178
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.93s/it][A100%|██████████| 1/1 [00:20<00:00, 20.93s/it]
INFO:root:eval mean loss: 2562.849615868102
INFO:root:eval perplexity: 8.223469734191895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/8
  4%|▍         | 8/200 [52:58<20:24:16, 382.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2256.9022705078123
INFO:root:current train perplexity5.882757186889648
INFO:root:current mean train loss 2247.6407624421295
INFO:root:current train perplexity5.844264507293701
INFO:root:current mean train loss 2240.5445780003324
INFO:root:current train perplexity5.838813781738281
INFO:root:current mean train loss 2235.0408046437733
INFO:root:current train perplexity5.829460144042969
INFO:root:current mean train loss 2236.7570009428878
INFO:root:current train perplexity5.846847057342529
INFO:root:current mean train loss 2236.570721150336
INFO:root:current train perplexity5.860691070556641
INFO:root:current mean train loss 2239.3598296398254
INFO:root:current train perplexity5.866654872894287
INFO:root:current mean train loss 2237.4014141887224
INFO:root:current train perplexity5.860865592956543
INFO:root:current mean train loss 2238.04440128532
INFO:root:current train perplexity5.871341705322266
INFO:root:current mean train loss 2240.675033291903
INFO:root:current train perplexity5.875566005706787
INFO:root:current mean train loss 2240.49111835277
INFO:root:current train perplexity5.876115798950195
INFO:root:current mean train loss 2244.7700237257363
INFO:root:current train perplexity5.883115768432617
INFO:root:current mean train loss 2243.484379942118
INFO:root:current train perplexity5.88032865524292
INFO:root:current mean train loss 2243.2337298103935
INFO:root:current train perplexity5.874992847442627
INFO:root:current mean train loss 2242.6943512494554
INFO:root:current train perplexity5.871983051300049
INFO:root:current mean train loss 2241.3659609120523
INFO:root:current train perplexity5.866512775421143
INFO:root:current mean train loss 2239.4077716605743
INFO:root:current train perplexity5.861278057098389
INFO:root:current mean train loss 2239.1410711370904
INFO:root:current train perplexity5.858807563781738
INFO:root:current mean train loss 2239.306342800047
INFO:root:current train perplexity5.854500770568848
INFO:root:current mean train loss 2239.3987290808705
INFO:root:current train perplexity5.853700160980225

100%|██████████| 1/1 [05:37<00:00, 337.88s/it][A100%|██████████| 1/1 [05:37<00:00, 337.88s/it]
INFO:root:final mean train loss: 2238.388822723866
INFO:root:final train perplexity: 5.850583076477051
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.88s/it][A100%|██████████| 1/1 [00:21<00:00, 21.88s/it]
INFO:root:eval mean loss: 2150.925576500859
INFO:root:eval perplexity: 5.700538635253906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.75s/it][A100%|██████████| 1/1 [00:20<00:00, 20.75s/it]
INFO:root:eval mean loss: 2543.4940484229555
INFO:root:eval perplexity: 8.093647003173828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/9
  4%|▍         | 9/200 [59:21<20:17:52, 382.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2202.061241736779
INFO:root:current train perplexity5.6603546142578125
INFO:root:current mean train loss 2209.2237942344264
INFO:root:current train perplexity5.661560535430908
INFO:root:current mean train loss 2210.109116327195
INFO:root:current train perplexity5.685562610626221
INFO:root:current mean train loss 2210.57219730724
INFO:root:current train perplexity5.696041584014893
INFO:root:current mean train loss 2207.78084354907
INFO:root:current train perplexity5.698111534118652
INFO:root:current mean train loss 2206.9490763346353
INFO:root:current train perplexity5.695455551147461
INFO:root:current mean train loss 2204.660965246657
INFO:root:current train perplexity5.6921563148498535
INFO:root:current mean train loss 2204.4160557199034
INFO:root:current train perplexity5.687115669250488
INFO:root:current mean train loss 2203.477667866738
INFO:root:current train perplexity5.68823766708374
INFO:root:current mean train loss 2201.963331975857
INFO:root:current train perplexity5.682102203369141
INFO:root:current mean train loss 2201.6875198422276
INFO:root:current train perplexity5.683076858520508
INFO:root:current mean train loss 2202.0906557506987
INFO:root:current train perplexity5.681918621063232
INFO:root:current mean train loss 2201.284846004206
INFO:root:current train perplexity5.683871746063232
INFO:root:current mean train loss 2199.5774985770504
INFO:root:current train perplexity5.679054260253906
INFO:root:current mean train loss 2199.228929672031
INFO:root:current train perplexity5.67783784866333
INFO:root:current mean train loss 2198.7594831800952
INFO:root:current train perplexity5.676175117492676
INFO:root:current mean train loss 2199.462810008347
INFO:root:current train perplexity5.6752214431762695
INFO:root:current mean train loss 2198.0493143856797
INFO:root:current train perplexity5.670122146606445
INFO:root:current mean train loss 2198.7055407002986
INFO:root:current train perplexity5.671449661254883
INFO:root:current mean train loss 2199.1395241784267
INFO:root:current train perplexity5.671024799346924

100%|██████████| 1/1 [05:32<00:00, 332.50s/it][A100%|██████████| 1/1 [05:32<00:00, 332.50s/it]
INFO:root:final mean train loss: 2198.328548398694
INFO:root:final train perplexity: 5.668508052825928
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 23.00s/it][A100%|██████████| 1/1 [00:22<00:00, 23.00s/it]
INFO:root:eval mean loss: 2128.5017020584
INFO:root:eval perplexity: 5.598031044006348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.97s/it][A100%|██████████| 1/1 [00:21<00:00, 21.97s/it]
INFO:root:eval mean loss: 2526.6474401595747
INFO:root:eval perplexity: 7.982323169708252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/10
  5%|▌         | 10/200 [1:05:40<20:08:23, 381.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2195.540957243546
INFO:root:current train perplexity5.598627090454102
INFO:root:current mean train loss 2179.050063274316
INFO:root:current train perplexity5.566436767578125
INFO:root:current mean train loss 2171.509038648641
INFO:root:current train perplexity5.536582946777344
INFO:root:current mean train loss 2164.232533690083
INFO:root:current train perplexity5.5150227546691895
INFO:root:current mean train loss 2174.068438759745
INFO:root:current train perplexity5.533890724182129
INFO:root:current mean train loss 2170.5264270427147
INFO:root:current train perplexity5.525984764099121
INFO:root:current mean train loss 2171.2484205670776
INFO:root:current train perplexity5.533555507659912
INFO:root:current mean train loss 2170.2285303877297
INFO:root:current train perplexity5.535754680633545
INFO:root:current mean train loss 2169.9966057710462
INFO:root:current train perplexity5.529787540435791
INFO:root:current mean train loss 2170.2531279730233
INFO:root:current train perplexity5.526794910430908
INFO:root:current mean train loss 2167.9572189802093
INFO:root:current train perplexity5.518964767456055
INFO:root:current mean train loss 2167.056586011849
INFO:root:current train perplexity5.5242719650268555
INFO:root:current mean train loss 2165.9179751950046
INFO:root:current train perplexity5.525916576385498
INFO:root:current mean train loss 2163.75421806876
INFO:root:current train perplexity5.5213141441345215
INFO:root:current mean train loss 2164.084028582901
INFO:root:current train perplexity5.520100116729736
INFO:root:current mean train loss 2165.564442544017
INFO:root:current train perplexity5.519293785095215
INFO:root:current mean train loss 2166.4461883490067
INFO:root:current train perplexity5.521501064300537
INFO:root:current mean train loss 2165.454925899387
INFO:root:current train perplexity5.5204267501831055
INFO:root:current mean train loss 2164.56095044507
INFO:root:current train perplexity5.519432544708252
INFO:root:current mean train loss 2164.2726236772514
INFO:root:current train perplexity5.517324924468994

100%|██████████| 1/1 [05:33<00:00, 333.18s/it][A100%|██████████| 1/1 [05:33<00:00, 333.18s/it]
INFO:root:final mean train loss: 2163.9487456428965
INFO:root:final train perplexity: 5.516774654388428
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.66s/it][A100%|██████████| 1/1 [00:22<00:00, 22.66s/it]
INFO:root:eval mean loss: 2109.922222597379
INFO:root:eval perplexity: 5.514496326446533
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.05s/it][A100%|██████████| 1/1 [00:22<00:00, 22.05s/it]
INFO:root:eval mean loss: 2515.3139786957004
INFO:root:eval perplexity: 7.908292293548584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/11
  6%|▌         | 11/200 [1:12:00<20:00:20, 381.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2120.1710872206577
INFO:root:current train perplexity5.358884811401367
INFO:root:current mean train loss 2136.818791215138
INFO:root:current train perplexity5.410520076751709
INFO:root:current mean train loss 2133.276889614292
INFO:root:current train perplexity5.394191741943359
INFO:root:current mean train loss 2129.012740219195
INFO:root:current train perplexity5.385037899017334
INFO:root:current mean train loss 2133.719528135449
INFO:root:current train perplexity5.39298152923584
INFO:root:current mean train loss 2136.046919578578
INFO:root:current train perplexity5.39906120300293
INFO:root:current mean train loss 2139.454304419871
INFO:root:current train perplexity5.405714988708496
INFO:root:current mean train loss 2137.8177437430422
INFO:root:current train perplexity5.393939018249512
INFO:root:current mean train loss 2136.229500729666
INFO:root:current train perplexity5.39013671875
INFO:root:current mean train loss 2135.058050004754
INFO:root:current train perplexity5.390433311462402
INFO:root:current mean train loss 2136.3685045330126
INFO:root:current train perplexity5.394118309020996
INFO:root:current mean train loss 2136.609917832064
INFO:root:current train perplexity5.391478061676025
INFO:root:current mean train loss 2136.5181998672606
INFO:root:current train perplexity5.394461631774902
INFO:root:current mean train loss 2133.550118318368
INFO:root:current train perplexity5.389298439025879
INFO:root:current mean train loss 2132.5137801446313
INFO:root:current train perplexity5.386612415313721
INFO:root:current mean train loss 2132.5014713090122
INFO:root:current train perplexity5.384005546569824
INFO:root:current mean train loss 2134.3541471904423
INFO:root:current train perplexity5.386432647705078
INFO:root:current mean train loss 2133.9118754182923
INFO:root:current train perplexity5.386847972869873
INFO:root:current mean train loss 2134.1776251848532
INFO:root:current train perplexity5.387997150421143

100%|██████████| 1/1 [05:26<00:00, 326.07s/it][A100%|██████████| 1/1 [05:26<00:00, 326.07s/it]
INFO:root:final mean train loss: 2133.4934267937624
INFO:root:final train perplexity: 5.3857550621032715
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.85s/it][A100%|██████████| 1/1 [00:21<00:00, 21.85s/it]
INFO:root:eval mean loss: 2092.0835489043106
INFO:root:eval perplexity: 5.4354634284973145
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.52s/it][A100%|██████████| 1/1 [00:21<00:00, 21.52s/it]
INFO:root:eval mean loss: 2496.6732082502217
INFO:root:eval perplexity: 7.788021087646484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/12
  6%|▌         | 12/200 [1:18:52<20:23:36, 390.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2003.9127604166667
INFO:root:current train perplexity4.662247657775879
INFO:root:current mean train loss 2100.144906942127
INFO:root:current train perplexity5.228173732757568
INFO:root:current mean train loss 2094.2979585995226
INFO:root:current train perplexity5.235599517822266
INFO:root:current mean train loss 2099.242434057859
INFO:root:current train perplexity5.230842113494873
INFO:root:current mean train loss 2096.071257863388
INFO:root:current train perplexity5.230584621429443
INFO:root:current mean train loss 2099.653712490681
INFO:root:current train perplexity5.241420269012451
INFO:root:current mean train loss 2102.636664698966
INFO:root:current train perplexity5.251222133636475
INFO:root:current mean train loss 2105.6401325513425
INFO:root:current train perplexity5.257920265197754
INFO:root:current mean train loss 2105.6676198690943
INFO:root:current train perplexity5.256031036376953
INFO:root:current mean train loss 2106.751970969165
INFO:root:current train perplexity5.263064384460449
INFO:root:current mean train loss 2106.1461238842066
INFO:root:current train perplexity5.2626471519470215
INFO:root:current mean train loss 2105.5161741503994
INFO:root:current train perplexity5.2629194259643555
INFO:root:current mean train loss 2104.347808254429
INFO:root:current train perplexity5.2627105712890625
INFO:root:current mean train loss 2104.6492468364772
INFO:root:current train perplexity5.26472806930542
INFO:root:current mean train loss 2105.022303716506
INFO:root:current train perplexity5.2678704261779785
INFO:root:current mean train loss 2105.6576484076118
INFO:root:current train perplexity5.268993377685547
INFO:root:current mean train loss 2107.482866750088
INFO:root:current train perplexity5.270023345947266
INFO:root:current mean train loss 2106.823523744862
INFO:root:current train perplexity5.270439624786377
INFO:root:current mean train loss 2107.1170502639916
INFO:root:current train perplexity5.269589424133301
INFO:root:current mean train loss 2106.8933714216655
INFO:root:current train perplexity5.269460678100586

100%|██████████| 1/1 [05:24<00:00, 324.26s/it][A100%|██████████| 1/1 [05:24<00:00, 324.26s/it]
INFO:root:final mean train loss: 2105.7710660240955
INFO:root:final train perplexity: 5.269204139709473
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.73s/it][A100%|██████████| 1/1 [00:22<00:00, 22.73s/it]
INFO:root:eval mean loss: 2080.9178103183176
INFO:root:eval perplexity: 5.386572360992432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.33s/it][A100%|██████████| 1/1 [00:21<00:00, 21.33s/it]
INFO:root:eval mean loss: 2489.315273420185
INFO:root:eval perplexity: 7.741050720214844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/13
  6%|▋         | 13/200 [1:25:03<19:58:05, 384.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2073.2456909179687
INFO:root:current train perplexity5.1224589347839355
INFO:root:current mean train loss 2078.6469950358073
INFO:root:current train perplexity5.138504981994629
INFO:root:current mean train loss 2082.6968677867544
INFO:root:current train perplexity5.148164749145508
INFO:root:current mean train loss 2083.681172943115
INFO:root:current train perplexity5.153504848480225
INFO:root:current mean train loss 2089.173489234561
INFO:root:current train perplexity5.1741180419921875
INFO:root:current mean train loss 2088.0083021897535
INFO:root:current train perplexity5.169919490814209
INFO:root:current mean train loss 2086.516640546245
INFO:root:current train perplexity5.172405242919922
INFO:root:current mean train loss 2085.1510771009657
INFO:root:current train perplexity5.168415546417236
INFO:root:current mean train loss 2082.780061005383
INFO:root:current train perplexity5.1638407707214355
INFO:root:current mean train loss 2084.402825264309
INFO:root:current train perplexity5.167699337005615
INFO:root:current mean train loss 2082.156949750115
INFO:root:current train perplexity5.161355018615723
INFO:root:current mean train loss 2080.6676127842493
INFO:root:current train perplexity5.162647247314453
INFO:root:current mean train loss 2081.342229724321
INFO:root:current train perplexity5.165795803070068
INFO:root:current mean train loss 2080.282086459073
INFO:root:current train perplexity5.163476943969727
INFO:root:current mean train loss 2082.2118957519533
INFO:root:current train perplexity5.164260387420654
INFO:root:current mean train loss 2080.9317198903937
INFO:root:current train perplexity5.1644439697265625
INFO:root:current mean train loss 2081.9364097312646
INFO:root:current train perplexity5.170119762420654
INFO:root:current mean train loss 2081.7851655472155
INFO:root:current train perplexity5.167359352111816
INFO:root:current mean train loss 2081.345303311191
INFO:root:current train perplexity5.167980670928955
INFO:root:current mean train loss 2081.3709277470907
INFO:root:current train perplexity5.167233467102051

100%|██████████| 1/1 [05:26<00:00, 326.32s/it][A100%|██████████| 1/1 [05:26<00:00, 326.32s/it]
INFO:root:final mean train loss: 2080.3337366947667
INFO:root:final train perplexity: 5.164477348327637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.97s/it][A100%|██████████| 1/1 [00:21<00:00, 21.97s/it]
INFO:root:eval mean loss: 2072.2205286770004
INFO:root:eval perplexity: 5.348794460296631
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.65s/it][A100%|██████████| 1/1 [00:20<00:00, 20.65s/it]
INFO:root:eval mean loss: 2485.546378061281
INFO:root:eval perplexity: 7.717103958129883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/14
  7%|▋         | 14/200 [1:31:14<19:39:09, 380.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2036.2048273859798
INFO:root:current train perplexity5.063307285308838
INFO:root:current mean train loss 2056.016068730041
INFO:root:current train perplexity5.052288055419922
INFO:root:current mean train loss 2059.6882252109704
INFO:root:current train perplexity5.0598039627075195
INFO:root:current mean train loss 2057.8934170414504
INFO:root:current train perplexity5.066258907318115
INFO:root:current mean train loss 2057.068738156107
INFO:root:current train perplexity5.064828395843506
INFO:root:current mean train loss 2060.414820154286
INFO:root:current train perplexity5.076624393463135
INFO:root:current mean train loss 2060.427631468002
INFO:root:current train perplexity5.076280117034912
INFO:root:current mean train loss 2058.7969662628793
INFO:root:current train perplexity5.07887601852417
INFO:root:current mean train loss 2058.18865317797
INFO:root:current train perplexity5.077178001403809
INFO:root:current mean train loss 2054.2295663155764
INFO:root:current train perplexity5.063941478729248
INFO:root:current mean train loss 2054.2670571190183
INFO:root:current train perplexity5.06085729598999
INFO:root:current mean train loss 2056.456095807051
INFO:root:current train perplexity5.070989608764648
INFO:root:current mean train loss 2056.0244862981253
INFO:root:current train perplexity5.068665027618408
INFO:root:current mean train loss 2055.4169246242755
INFO:root:current train perplexity5.0663628578186035
INFO:root:current mean train loss 2056.6490977160533
INFO:root:current train perplexity5.070662498474121
INFO:root:current mean train loss 2056.8604399582437
INFO:root:current train perplexity5.070303916931152
INFO:root:current mean train loss 2057.352127737
INFO:root:current train perplexity5.072672367095947
INFO:root:current mean train loss 2059.639132818684
INFO:root:current train perplexity5.075709819793701
INFO:root:current mean train loss 2059.2111687491492
INFO:root:current train perplexity5.074423313140869
INFO:root:current mean train loss 2058.0914888948155
INFO:root:current train perplexity5.072325229644775

100%|██████████| 1/1 [05:30<00:00, 330.70s/it][A100%|██████████| 1/1 [05:30<00:00, 330.70s/it]
INFO:root:final mean train loss: 2057.1681539792335
INFO:root:final train perplexity: 5.070916652679443
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.16s/it][A100%|██████████| 1/1 [00:22<00:00, 22.16s/it]
INFO:root:eval mean loss: 2063.2178868503433
INFO:root:eval perplexity: 5.309971332550049
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.95s/it][A100%|██████████| 1/1 [00:21<00:00, 21.95s/it]
INFO:root:eval mean loss: 2478.9450640306404
INFO:root:eval perplexity: 7.6753339767456055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/15
  8%|▊         | 15/200 [1:37:30<19:29:27, 379.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2018.3020584671585
INFO:root:current train perplexity4.929506301879883
INFO:root:current mean train loss 2036.8867766144988
INFO:root:current train perplexity4.935572624206543
INFO:root:current mean train loss 2034.4114202063854
INFO:root:current train perplexity4.952660083770752
INFO:root:current mean train loss 2041.1776574775997
INFO:root:current train perplexity4.963807106018066
INFO:root:current mean train loss 2034.1978076817181
INFO:root:current train perplexity4.945379734039307
INFO:root:current mean train loss 2037.5350291117863
INFO:root:current train perplexity4.958469867706299
INFO:root:current mean train loss 2033.6646021105098
INFO:root:current train perplexity4.960102081298828
INFO:root:current mean train loss 2034.5459220744572
INFO:root:current train perplexity4.964425086975098
INFO:root:current mean train loss 2035.3480181727532
INFO:root:current train perplexity4.972095489501953
INFO:root:current mean train loss 2036.343351416118
INFO:root:current train perplexity4.975360870361328
INFO:root:current mean train loss 2037.3805405329028
INFO:root:current train perplexity4.980920791625977
INFO:root:current mean train loss 2036.0890934512768
INFO:root:current train perplexity4.978298187255859
INFO:root:current mean train loss 2037.0619737542988
INFO:root:current train perplexity4.978641510009766
INFO:root:current mean train loss 2036.5676471479183
INFO:root:current train perplexity4.981752395629883
INFO:root:current mean train loss 2036.7807534072226
INFO:root:current train perplexity4.985278129577637
INFO:root:current mean train loss 2035.8239184444628
INFO:root:current train perplexity4.984973430633545
INFO:root:current mean train loss 2035.6171994561007
INFO:root:current train perplexity4.987361431121826
INFO:root:current mean train loss 2035.541440574446
INFO:root:current train perplexity4.986069202423096
INFO:root:current mean train loss 2036.3092768565223
INFO:root:current train perplexity4.987553119659424
INFO:root:current mean train loss 2036.7250421186309
INFO:root:current train perplexity4.9879374504089355

100%|██████████| 1/1 [05:27<00:00, 327.52s/it][A100%|██████████| 1/1 [05:27<00:00, 327.52s/it]
INFO:root:final mean train loss: 2036.1612899403228
INFO:root:final train perplexity: 4.987541198730469
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.15s/it][A100%|██████████| 1/1 [00:22<00:00, 22.15s/it]
INFO:root:eval mean loss: 2054.85084955743
INFO:root:eval perplexity: 5.274139881134033
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.76s/it][A100%|██████████| 1/1 [00:20<00:00, 20.76s/it]
INFO:root:eval mean loss: 2475.0546541687445
INFO:root:eval perplexity: 7.650824546813965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/16
  8%|▊         | 16/200 [1:43:43<19:16:45, 377.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2015.833379181338
INFO:root:current train perplexity4.878363132476807
INFO:root:current mean train loss 2009.3911561129387
INFO:root:current train perplexity4.870996475219727
INFO:root:current mean train loss 2019.403119414495
INFO:root:current train perplexity4.881427764892578
INFO:root:current mean train loss 2017.6545972798392
INFO:root:current train perplexity4.884968280792236
INFO:root:current mean train loss 2014.0389720487494
INFO:root:current train perplexity4.878291606903076
INFO:root:current mean train loss 2013.955366732569
INFO:root:current train perplexity4.882077217102051
INFO:root:current mean train loss 2012.0565027273892
INFO:root:current train perplexity4.886054992675781
INFO:root:current mean train loss 2014.922204637342
INFO:root:current train perplexity4.8949103355407715
INFO:root:current mean train loss 2015.0423192966955
INFO:root:current train perplexity4.898624897003174
INFO:root:current mean train loss 2016.0650433619899
INFO:root:current train perplexity4.9000244140625
INFO:root:current mean train loss 2016.417596042323
INFO:root:current train perplexity4.900728225708008
INFO:root:current mean train loss 2015.3902292878684
INFO:root:current train perplexity4.901573181152344
INFO:root:current mean train loss 2015.6307411463968
INFO:root:current train perplexity4.906444549560547
INFO:root:current mean train loss 2017.090798231218
INFO:root:current train perplexity4.912441253662109
INFO:root:current mean train loss 2016.7491879129313
INFO:root:current train perplexity4.909142971038818
INFO:root:current mean train loss 2016.7435865299024
INFO:root:current train perplexity4.9095916748046875
INFO:root:current mean train loss 2016.4436387998626
INFO:root:current train perplexity4.90681266784668
INFO:root:current mean train loss 2016.5335347344146
INFO:root:current train perplexity4.909016132354736
INFO:root:current mean train loss 2017.6169608445935
INFO:root:current train perplexity4.910519123077393
INFO:root:current mean train loss 2016.630707452891
INFO:root:current train perplexity4.908489227294922

100%|██████████| 1/1 [05:28<00:00, 328.79s/it][A100%|██████████| 1/1 [05:28<00:00, 328.79s/it]
INFO:root:final mean train loss: 2015.8563746126745
INFO:root:final train perplexity: 4.908254623413086
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.77s/it][A100%|██████████| 1/1 [00:22<00:00, 22.77s/it]
INFO:root:eval mean loss: 2046.9319380090592
INFO:root:eval perplexity: 5.240450382232666
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.02s/it][A100%|██████████| 1/1 [00:20<00:00, 20.02s/it]
INFO:root:eval mean loss: 2465.3607073844746
INFO:root:eval perplexity: 7.590092658996582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/17
  8%|▊         | 17/200 [1:49:56<19:07:09, 376.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1977.0052517977629
INFO:root:current train perplexity4.7889509201049805
INFO:root:current mean train loss 1995.0797710012882
INFO:root:current train perplexity4.801041603088379
INFO:root:current mean train loss 1995.3126708136665
INFO:root:current train perplexity4.802736282348633
INFO:root:current mean train loss 1998.9197054204253
INFO:root:current train perplexity4.809163570404053
INFO:root:current mean train loss 1997.44183449667
INFO:root:current train perplexity4.810309410095215
INFO:root:current mean train loss 1995.3992701939173
INFO:root:current train perplexity4.809785842895508
INFO:root:current mean train loss 1997.5450883022575
INFO:root:current train perplexity4.813377380371094
INFO:root:current mean train loss 1998.2122477420091
INFO:root:current train perplexity4.817408084869385
INFO:root:current mean train loss 1998.6805584881756
INFO:root:current train perplexity4.82423734664917
INFO:root:current mean train loss 1997.418731936559
INFO:root:current train perplexity4.824950218200684
INFO:root:current mean train loss 1996.0823779386633
INFO:root:current train perplexity4.826276779174805
INFO:root:current mean train loss 1994.69730837016
INFO:root:current train perplexity4.822709560394287
INFO:root:current mean train loss 1994.1512557319973
INFO:root:current train perplexity4.823887348175049
INFO:root:current mean train loss 1994.1248154873806
INFO:root:current train perplexity4.826957702636719
INFO:root:current mean train loss 1995.0107189711705
INFO:root:current train perplexity4.828018665313721
INFO:root:current mean train loss 1994.6912924048281
INFO:root:current train perplexity4.829324722290039
INFO:root:current mean train loss 1995.376023785198
INFO:root:current train perplexity4.830469131469727
INFO:root:current mean train loss 1995.6899810040024
INFO:root:current train perplexity4.829813003540039
INFO:root:current mean train loss 1997.4778638290147
INFO:root:current train perplexity4.833992004394531

100%|██████████| 1/1 [05:32<00:00, 332.50s/it][A100%|██████████| 1/1 [05:32<00:00, 332.50s/it]
INFO:root:final mean train loss: 1996.6209730647515
INFO:root:final train perplexity: 4.834306716918945
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.98s/it][A100%|██████████| 1/1 [00:22<00:00, 22.98s/it]
INFO:root:eval mean loss: 2041.7072325361537
INFO:root:eval perplexity: 5.218340873718262
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.79s/it][A100%|██████████| 1/1 [00:21<00:00, 21.79s/it]
INFO:root:eval mean loss: 2465.9936653299533
INFO:root:eval perplexity: 7.5940423011779785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/18
  9%|▉         | 18/200 [1:56:16<19:03:42, 377.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1971.88076171875
INFO:root:current train perplexity4.827023983001709
INFO:root:current mean train loss 1987.2569242931547
INFO:root:current train perplexity4.756862163543701
INFO:root:current mean train loss 1976.2182373046876
INFO:root:current train perplexity4.74467134475708
INFO:root:current mean train loss 1978.9306672643443
INFO:root:current train perplexity4.7401275634765625
INFO:root:current mean train loss 1980.449011682581
INFO:root:current train perplexity4.7461395263671875
INFO:root:current mean train loss 1977.9306536683941
INFO:root:current train perplexity4.759142875671387
INFO:root:current mean train loss 1978.0201196087294
INFO:root:current train perplexity4.760560512542725
INFO:root:current mean train loss 1979.3863979041998
INFO:root:current train perplexity4.765382289886475
INFO:root:current mean train loss 1977.1877879646254
INFO:root:current train perplexity4.754601955413818
INFO:root:current mean train loss 1976.214883810644
INFO:root:current train perplexity4.757073402404785
INFO:root:current mean train loss 1976.1174682009873
INFO:root:current train perplexity4.759904861450195
INFO:root:current mean train loss 1975.4221436651583
INFO:root:current train perplexity4.7621564865112305
INFO:root:current mean train loss 1978.1487030155927
INFO:root:current train perplexity4.768482685089111
INFO:root:current mean train loss 1979.6958552218032
INFO:root:current train perplexity4.771026134490967
INFO:root:current mean train loss 1979.265946640069
INFO:root:current train perplexity4.77113151550293
INFO:root:current mean train loss 1980.2226835840167
INFO:root:current train perplexity4.770712375640869
INFO:root:current mean train loss 1980.4672050689983
INFO:root:current train perplexity4.772756576538086
INFO:root:current mean train loss 1981.3491747903684
INFO:root:current train perplexity4.772061347961426
INFO:root:current mean train loss 1981.2007262000086
INFO:root:current train perplexity4.771923065185547
INFO:root:current mean train loss 1980.4257587583047
INFO:root:current train perplexity4.770986080169678

100%|██████████| 1/1 [05:31<00:00, 331.46s/it][A100%|██████████| 1/1 [05:31<00:00, 331.46s/it]
INFO:root:final mean train loss: 1979.306100357679
INFO:root:final train perplexity: 4.76869535446167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.61s/it][A100%|██████████| 1/1 [00:22<00:00, 22.61s/it]
INFO:root:eval mean loss: 2038.701478782275
INFO:root:eval perplexity: 5.205664157867432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.40s/it][A100%|██████████| 1/1 [00:21<00:00, 21.40s/it]
INFO:root:eval mean loss: 2461.7977922588375
INFO:root:eval perplexity: 7.567892074584961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/19
 10%|▉         | 19/200 [2:02:33<18:57:44, 377.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1978.8012972745028
INFO:root:current train perplexity4.788876056671143
INFO:root:current mean train loss 1949.4471185402792
INFO:root:current train perplexity4.700664043426514
INFO:root:current mean train loss 1952.1525428015907
INFO:root:current train perplexity4.69478702545166
INFO:root:current mean train loss 1954.8433648340451
INFO:root:current train perplexity4.691622734069824
INFO:root:current mean train loss 1959.0749132780102
INFO:root:current train perplexity4.69802713394165
INFO:root:current mean train loss 1953.6888900113745
INFO:root:current train perplexity4.694888591766357
INFO:root:current mean train loss 1955.3138457172554
INFO:root:current train perplexity4.69902229309082
INFO:root:current mean train loss 1952.5138676609029
INFO:root:current train perplexity4.691085338592529
INFO:root:current mean train loss 1955.2438257964568
INFO:root:current train perplexity4.695202350616455
INFO:root:current mean train loss 1958.119826575422
INFO:root:current train perplexity4.695659160614014
INFO:root:current mean train loss 1958.1656709137262
INFO:root:current train perplexity4.694077491760254
INFO:root:current mean train loss 1959.3532984660483
INFO:root:current train perplexity4.697144031524658
INFO:root:current mean train loss 1960.289548383797
INFO:root:current train perplexity4.698150634765625
INFO:root:current mean train loss 1960.0723732906463
INFO:root:current train perplexity4.695332050323486
INFO:root:current mean train loss 1959.2616302447311
INFO:root:current train perplexity4.693950653076172
INFO:root:current mean train loss 1960.2685752196944
INFO:root:current train perplexity4.695737361907959
INFO:root:current mean train loss 1961.16107990533
INFO:root:current train perplexity4.6982903480529785
INFO:root:current mean train loss 1960.6043139733504
INFO:root:current train perplexity4.696648120880127
INFO:root:current mean train loss 1961.379991952203
INFO:root:current train perplexity4.700573444366455
INFO:root:current mean train loss 1961.6841661848214
INFO:root:current train perplexity4.701384544372559

100%|██████████| 1/1 [05:31<00:00, 331.37s/it][A100%|██████████| 1/1 [05:31<00:00, 331.37s/it]
INFO:root:final mean train loss: 1961.9252222689245
INFO:root:final train perplexity: 4.703729629516602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2036.7926808891566
INFO:root:eval perplexity: 5.197628974914551
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.54s/it][A100%|██████████| 1/1 [00:21<00:00, 21.54s/it]
INFO:root:eval mean loss: 2464.186940294631
INFO:root:eval perplexity: 7.582770824432373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/20
 10%|█         | 20/200 [2:08:50<18:51:25, 377.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1926.2945024539263
INFO:root:current train perplexity4.615390777587891
INFO:root:current mean train loss 1922.454427376068
INFO:root:current train perplexity4.611405849456787
INFO:root:current mean train loss 1936.1542309876766
INFO:root:current train perplexity4.617079734802246
INFO:root:current mean train loss 1940.41544092229
INFO:root:current train perplexity4.624385833740234
INFO:root:current mean train loss 1937.6673689648883
INFO:root:current train perplexity4.628488063812256
INFO:root:current mean train loss 1938.9323961473794
INFO:root:current train perplexity4.627901554107666
INFO:root:current mean train loss 1939.0519049081818
INFO:root:current train perplexity4.635564804077148
INFO:root:current mean train loss 1941.8780535748267
INFO:root:current train perplexity4.640165328979492
INFO:root:current mean train loss 1942.6473593819837
INFO:root:current train perplexity4.6383538246154785
INFO:root:current mean train loss 1941.6114100252096
INFO:root:current train perplexity4.631100177764893
INFO:root:current mean train loss 1942.53134305071
INFO:root:current train perplexity4.6341633796691895
INFO:root:current mean train loss 1942.886578996104
INFO:root:current train perplexity4.633713245391846
INFO:root:current mean train loss 1944.4216494802702
INFO:root:current train perplexity4.631852626800537
INFO:root:current mean train loss 1943.6513330005193
INFO:root:current train perplexity4.6326494216918945
INFO:root:current mean train loss 1942.9245277176806
INFO:root:current train perplexity4.63569974899292
INFO:root:current mean train loss 1944.1074876295738
INFO:root:current train perplexity4.63555383682251
INFO:root:current mean train loss 1944.2401378508237
INFO:root:current train perplexity4.636896133422852
INFO:root:current mean train loss 1944.532111722344
INFO:root:current train perplexity4.638583183288574
INFO:root:current mean train loss 1944.7665910082967
INFO:root:current train perplexity4.639876842498779
INFO:root:current mean train loss 1945.8398526266963
INFO:root:current train perplexity4.643087863922119

100%|██████████| 1/1 [05:29<00:00, 329.41s/it][A100%|██████████| 1/1 [05:29<00:00, 329.41s/it]
INFO:root:final mean train loss: 1945.2032221083803
INFO:root:final train perplexity: 4.642062187194824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.36s/it][A100%|██████████| 1/1 [00:22<00:00, 22.36s/it]
INFO:root:eval mean loss: 2030.9552426169105
INFO:root:eval perplexity: 5.173134803771973
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.25s/it][A100%|██████████| 1/1 [00:21<00:00, 21.26s/it]
INFO:root:eval mean loss: 2460.839664973266
INFO:root:eval perplexity: 7.5619330406188965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/21
 10%|█         | 21/200 [2:15:05<18:43:13, 376.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1943.7516457693916
INFO:root:current train perplexity4.613004207611084
INFO:root:current mean train loss 1924.0609506460337
INFO:root:current train perplexity4.566090106964111
INFO:root:current mean train loss 1922.4650373458862
INFO:root:current train perplexity4.548300743103027
INFO:root:current mean train loss 1916.831993531645
INFO:root:current train perplexity4.5541839599609375
INFO:root:current mean train loss 1917.4662526448567
INFO:root:current train perplexity4.55703592300415
INFO:root:current mean train loss 1921.1318574534903
INFO:root:current train perplexity4.565430641174316
INFO:root:current mean train loss 1920.8716518122976
INFO:root:current train perplexity4.566915035247803
INFO:root:current mean train loss 1922.0500409161602
INFO:root:current train perplexity4.567388534545898
INFO:root:current mean train loss 1924.5481531731436
INFO:root:current train perplexity4.565257549285889
INFO:root:current mean train loss 1924.2213677442223
INFO:root:current train perplexity4.5662617683410645
INFO:root:current mean train loss 1924.7907698660185
INFO:root:current train perplexity4.567512512207031
INFO:root:current mean train loss 1926.276254937723
INFO:root:current train perplexity4.569403171539307
INFO:root:current mean train loss 1926.351530816145
INFO:root:current train perplexity4.57129430770874
INFO:root:current mean train loss 1926.3495688649405
INFO:root:current train perplexity4.574493408203125
INFO:root:current mean train loss 1926.8579397515937
INFO:root:current train perplexity4.576949119567871
INFO:root:current mean train loss 1926.9870332458026
INFO:root:current train perplexity4.5766282081604
INFO:root:current mean train loss 1926.5161260337645
INFO:root:current train perplexity4.577469348907471
INFO:root:current mean train loss 1927.4153303350565
INFO:root:current train perplexity4.581040382385254
INFO:root:current mean train loss 1928.3859247668036
INFO:root:current train perplexity4.582302570343018
INFO:root:current mean train loss 1929.4942440976881
INFO:root:current train perplexity4.58339262008667

100%|██████████| 1/1 [05:33<00:00, 333.46s/it][A100%|██████████| 1/1 [05:33<00:00, 333.46s/it]
INFO:root:final mean train loss: 1929.1807785303497
INFO:root:final train perplexity: 4.583733081817627
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.58s/it][A100%|██████████| 1/1 [00:22<00:00, 22.58s/it]
INFO:root:eval mean loss: 2032.356328436669
INFO:root:eval perplexity: 5.179003715515137
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.70s/it][A100%|██████████| 1/1 [00:20<00:00, 20.70s/it]
INFO:root:eval mean loss: 2467.4185180664062
INFO:root:eval perplexity: 7.60294246673584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/22
 11%|█         | 22/200 [2:21:24<18:38:53, 377.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1894.481612532106
INFO:root:current train perplexity4.463193416595459
INFO:root:current mean train loss 1895.680268921604
INFO:root:current train perplexity4.517380714416504
INFO:root:current mean train loss 1896.744175502232
INFO:root:current train perplexity4.498400688171387
INFO:root:current mean train loss 1900.3113640588347
INFO:root:current train perplexity4.505552768707275
INFO:root:current mean train loss 1902.4282097524115
INFO:root:current train perplexity4.50268030166626
INFO:root:current mean train loss 1902.8870939084998
INFO:root:current train perplexity4.504648208618164
INFO:root:current mean train loss 1906.5058071368871
INFO:root:current train perplexity4.509199142456055
INFO:root:current mean train loss 1907.650147273963
INFO:root:current train perplexity4.509964466094971
INFO:root:current mean train loss 1908.558181815131
INFO:root:current train perplexity4.51724910736084
INFO:root:current mean train loss 1909.5322728563801
INFO:root:current train perplexity4.5186920166015625
INFO:root:current mean train loss 1911.3885633427744
INFO:root:current train perplexity4.522498607635498
INFO:root:current mean train loss 1911.2085118078312
INFO:root:current train perplexity4.521178722381592
INFO:root:current mean train loss 1911.3640293981368
INFO:root:current train perplexity4.5219502449035645
INFO:root:current mean train loss 1911.2143838303157
INFO:root:current train perplexity4.52497673034668
INFO:root:current mean train loss 1911.184967600401
INFO:root:current train perplexity4.525018215179443
INFO:root:current mean train loss 1911.7875823683596
INFO:root:current train perplexity4.525014877319336
INFO:root:current mean train loss 1913.3714093962474
INFO:root:current train perplexity4.527979850769043
INFO:root:current mean train loss 1914.3346924929717
INFO:root:current train perplexity4.530031681060791
INFO:root:current mean train loss 1914.0891199962252
INFO:root:current train perplexity4.529644012451172
INFO:root:current mean train loss 1914.5610080570118
INFO:root:current train perplexity4.5295729637146

100%|██████████| 1/1 [05:25<00:00, 325.06s/it][A100%|██████████| 1/1 [05:25<00:00, 325.06s/it]
INFO:root:final mean train loss: 1914.1267638021325
INFO:root:final train perplexity: 4.529597759246826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.97s/it][A100%|██████████| 1/1 [00:22<00:00, 22.97s/it]
INFO:root:eval mean loss: 2028.289820028535
INFO:root:eval perplexity: 5.161989212036133
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.18s/it][A100%|██████████| 1/1 [00:21<00:00, 21.18s/it]
INFO:root:eval mean loss: 2462.2441410578735
INFO:root:eval perplexity: 7.570669651031494
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/23
 12%|█▏        | 23/200 [2:27:35<18:27:17, 375.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1881.1211059570312
INFO:root:current train perplexity4.439002990722656
INFO:root:current mean train loss 1869.5614598324423
INFO:root:current train perplexity4.440884113311768
INFO:root:current mean train loss 1878.292387021821
INFO:root:current train perplexity4.459940433502197
INFO:root:current mean train loss 1878.0667611929086
INFO:root:current train perplexity4.444365978240967
INFO:root:current mean train loss 1882.1901175362723
INFO:root:current train perplexity4.445202350616455
INFO:root:current mean train loss 1887.8075619455112
INFO:root:current train perplexity4.4529852867126465
INFO:root:current mean train loss 1889.9827923318614
INFO:root:current train perplexity4.453505039215088
INFO:root:current mean train loss 1891.8670037764537
INFO:root:current train perplexity4.459308624267578
INFO:root:current mean train loss 1890.2222237919154
INFO:root:current train perplexity4.45766544342041
INFO:root:current mean train loss 1893.4012954249527
INFO:root:current train perplexity4.461907386779785
INFO:root:current mean train loss 1892.7622899046733
INFO:root:current train perplexity4.459140300750732
INFO:root:current mean train loss 1894.0925734063158
INFO:root:current train perplexity4.4607672691345215
INFO:root:current mean train loss 1895.2494023286094
INFO:root:current train perplexity4.463073253631592
INFO:root:current mean train loss 1894.4950912804911
INFO:root:current train perplexity4.464023590087891
INFO:root:current mean train loss 1895.4764487861787
INFO:root:current train perplexity4.465093612670898
INFO:root:current mean train loss 1896.6306933900846
INFO:root:current train perplexity4.469169616699219
INFO:root:current mean train loss 1897.7765967374723
INFO:root:current train perplexity4.471521377563477
INFO:root:current mean train loss 1899.2411931384208
INFO:root:current train perplexity4.473573207855225
INFO:root:current mean train loss 1899.9741975007234
INFO:root:current train perplexity4.476858139038086

100%|██████████| 1/1 [05:34<00:00, 334.88s/it][A100%|██████████| 1/1 [05:34<00:00, 334.88s/it]
INFO:root:final mean train loss: 1899.5866157197015
INFO:root:final train perplexity: 4.477917194366455
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.30s/it][A100%|██████████| 1/1 [00:22<00:00, 22.30s/it]
INFO:root:eval mean loss: 2027.024174683483
INFO:root:eval perplexity: 5.156704902648926
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.06s/it][A100%|██████████| 1/1 [00:21<00:00, 21.06s/it]
INFO:root:eval mean loss: 2466.248628656915
INFO:root:eval perplexity: 7.5956339836120605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/24
 12%|█▏        | 24/200 [2:33:55<18:25:24, 376.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1741.997593470982
INFO:root:current train perplexity4.17119026184082
INFO:root:current mean train loss 1861.4539099007009
INFO:root:current train perplexity4.3830718994140625
INFO:root:current mean train loss 1864.7339114866395
INFO:root:current train perplexity4.384880065917969
INFO:root:current mean train loss 1873.7724577565148
INFO:root:current train perplexity4.393099308013916
INFO:root:current mean train loss 1876.6322087468327
INFO:root:current train perplexity4.393283367156982
INFO:root:current mean train loss 1878.0857383833363
INFO:root:current train perplexity4.396337032318115
INFO:root:current mean train loss 1880.0121026581162
INFO:root:current train perplexity4.40539026260376
INFO:root:current mean train loss 1882.844268841993
INFO:root:current train perplexity4.412672519683838
INFO:root:current mean train loss 1882.4912015591117
INFO:root:current train perplexity4.415217399597168
INFO:root:current mean train loss 1883.6477828693496
INFO:root:current train perplexity4.416738033294678
INFO:root:current mean train loss 1883.676176069273
INFO:root:current train perplexity4.4196577072143555
INFO:root:current mean train loss 1883.3454748634401
INFO:root:current train perplexity4.418482780456543
INFO:root:current mean train loss 1885.3237840704614
INFO:root:current train perplexity4.42091703414917
INFO:root:current mean train loss 1884.8933379122932
INFO:root:current train perplexity4.420018672943115
INFO:root:current mean train loss 1885.4069872803948
INFO:root:current train perplexity4.419564247131348
INFO:root:current mean train loss 1885.1586884091687
INFO:root:current train perplexity4.419727802276611
INFO:root:current mean train loss 1884.3730553067392
INFO:root:current train perplexity4.421866416931152
INFO:root:current mean train loss 1885.2374070921207
INFO:root:current train perplexity4.425432205200195
INFO:root:current mean train loss 1885.229150160941
INFO:root:current train perplexity4.42707633972168
INFO:root:current mean train loss 1885.1716351481589
INFO:root:current train perplexity4.42609167098999

100%|██████████| 1/1 [05:37<00:00, 337.22s/it][A100%|██████████| 1/1 [05:37<00:00, 337.22s/it]
INFO:root:final mean train loss: 1885.1316592956634
INFO:root:final train perplexity: 4.427123546600342
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.97s/it][A100%|██████████| 1/1 [00:22<00:00, 22.97s/it]
INFO:root:eval mean loss: 2025.1644741626496
INFO:root:eval perplexity: 5.148950099945068
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.66s/it][A100%|██████████| 1/1 [00:21<00:00, 21.66s/it]
INFO:root:eval mean loss: 2463.2657418758313
INFO:root:eval perplexity: 7.577031135559082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/25
 12%|█▎        | 25/200 [2:40:19<18:25:14, 378.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.4951985677083
INFO:root:current train perplexity4.3704752922058105
INFO:root:current mean train loss 1878.422092560799
INFO:root:current train perplexity4.3488287925720215
INFO:root:current mean train loss 1869.9616208757673
INFO:root:current train perplexity4.335299491882324
INFO:root:current mean train loss 1870.1897255226418
INFO:root:current train perplexity4.343181610107422
INFO:root:current mean train loss 1870.7341259650464
INFO:root:current train perplexity4.351653099060059
INFO:root:current mean train loss 1870.8171922523557
INFO:root:current train perplexity4.361685752868652
INFO:root:current mean train loss 1870.7369952079578
INFO:root:current train perplexity4.366738319396973
INFO:root:current mean train loss 1871.5599269129295
INFO:root:current train perplexity4.36873197555542
INFO:root:current mean train loss 1874.0299380404279
INFO:root:current train perplexity4.374430179595947
INFO:root:current mean train loss 1873.0957328499137
INFO:root:current train perplexity4.3742995262146
INFO:root:current mean train loss 1872.1579372882843
INFO:root:current train perplexity4.3739190101623535
INFO:root:current mean train loss 1871.141531078841
INFO:root:current train perplexity4.374032497406006
INFO:root:current mean train loss 1871.8630027023016
INFO:root:current train perplexity4.37459659576416
INFO:root:current mean train loss 1870.759795220597
INFO:root:current train perplexity4.375920295715332
INFO:root:current mean train loss 1870.6398277711332
INFO:root:current train perplexity4.374087810516357
INFO:root:current mean train loss 1870.8177438170264
INFO:root:current train perplexity4.376139163970947
INFO:root:current mean train loss 1871.3033887741014
INFO:root:current train perplexity4.376720428466797
INFO:root:current mean train loss 1870.82543994877
INFO:root:current train perplexity4.376986980438232
INFO:root:current mean train loss 1872.0533977977016
INFO:root:current train perplexity4.380426406860352
INFO:root:current mean train loss 1871.5371330403984
INFO:root:current train perplexity4.377779006958008

100%|██████████| 1/1 [05:27<00:00, 327.92s/it][A100%|██████████| 1/1 [05:27<00:00, 327.92s/it]
INFO:root:final mean train loss: 1871.3222410631972
INFO:root:final train perplexity: 4.37913703918457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.68s/it][A100%|██████████| 1/1 [00:22<00:00, 22.68s/it]
INFO:root:eval mean loss: 2026.8457091852283
INFO:root:eval perplexity: 5.155959129333496
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.21s/it][A100%|██████████| 1/1 [00:21<00:00, 21.21s/it]
INFO:root:eval mean loss: 2467.7259547456783
INFO:root:eval perplexity: 7.604864597320557
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/26
 13%|█▎        | 26/200 [2:46:33<18:14:28, 377.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1832.8219559832316
INFO:root:current train perplexity4.279364585876465
INFO:root:current mean train loss 1832.9232368198693
INFO:root:current train perplexity4.262143135070801
INFO:root:current mean train loss 1839.7219430757261
INFO:root:current train perplexity4.285192966461182
INFO:root:current mean train loss 1845.587740632446
INFO:root:current train perplexity4.296773433685303
INFO:root:current mean train loss 1843.647725561579
INFO:root:current train perplexity4.287055015563965
INFO:root:current mean train loss 1848.2131972674301
INFO:root:current train perplexity4.2997612953186035
INFO:root:current mean train loss 1850.3674918188133
INFO:root:current train perplexity4.307487964630127
INFO:root:current mean train loss 1851.0838369351488
INFO:root:current train perplexity4.305594444274902
INFO:root:current mean train loss 1851.5291525968898
INFO:root:current train perplexity4.306334495544434
INFO:root:current mean train loss 1850.8032886857814
INFO:root:current train perplexity4.3068437576293945
INFO:root:current mean train loss 1850.815631496345
INFO:root:current train perplexity4.3109965324401855
INFO:root:current mean train loss 1852.1810845150223
INFO:root:current train perplexity4.310805320739746
INFO:root:current mean train loss 1853.2945085474794
INFO:root:current train perplexity4.315568447113037
INFO:root:current mean train loss 1853.3946251012246
INFO:root:current train perplexity4.316415309906006
INFO:root:current mean train loss 1854.4100872942508
INFO:root:current train perplexity4.319613933563232
INFO:root:current mean train loss 1855.5608758548883
INFO:root:current train perplexity4.321654319763184
INFO:root:current mean train loss 1856.1788903607746
INFO:root:current train perplexity4.323680400848389
INFO:root:current mean train loss 1856.5722857480212
INFO:root:current train perplexity4.324674606323242
INFO:root:current mean train loss 1857.685013505313
INFO:root:current train perplexity4.329129219055176
INFO:root:current mean train loss 1858.310811329232
INFO:root:current train perplexity4.33284854888916

100%|██████████| 1/1 [05:40<00:00, 340.21s/it][A100%|██████████| 1/1 [05:40<00:00, 340.21s/it]
INFO:root:final mean train loss: 1857.7821893504456
INFO:root:final train perplexity: 4.3325910568237305
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.88s/it][A100%|██████████| 1/1 [00:22<00:00, 22.88s/it]
INFO:root:eval mean loss: 2022.5009462613586
INFO:root:eval perplexity: 5.137864112854004
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.76s/it][A100%|██████████| 1/1 [00:20<00:00, 20.76s/it]
INFO:root:eval mean loss: 2466.533484925615
INFO:root:eval perplexity: 7.597414016723633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/27
 14%|█▎        | 27/200 [2:52:59<18:15:26, 379.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1831.9857619713093
INFO:root:current train perplexity4.293435573577881
INFO:root:current mean train loss 1829.2063644988627
INFO:root:current train perplexity4.264865398406982
INFO:root:current mean train loss 1831.4732363205549
INFO:root:current train perplexity4.263018608093262
INFO:root:current mean train loss 1832.0782665060885
INFO:root:current train perplexity4.260680675506592
INFO:root:current mean train loss 1829.7022318610977
INFO:root:current train perplexity4.249629974365234
INFO:root:current mean train loss 1832.8227967839941
INFO:root:current train perplexity4.253340721130371
INFO:root:current mean train loss 1832.7518212222762
INFO:root:current train perplexity4.259878158569336
INFO:root:current mean train loss 1835.1083066432018
INFO:root:current train perplexity4.259809970855713
INFO:root:current mean train loss 1834.433595457277
INFO:root:current train perplexity4.264764785766602
INFO:root:current mean train loss 1836.2875593022166
INFO:root:current train perplexity4.268080234527588
INFO:root:current mean train loss 1838.1953820731553
INFO:root:current train perplexity4.272517204284668
INFO:root:current mean train loss 1839.4827951487275
INFO:root:current train perplexity4.271613121032715
INFO:root:current mean train loss 1840.0450992553906
INFO:root:current train perplexity4.273232936859131
INFO:root:current mean train loss 1840.256618314358
INFO:root:current train perplexity4.27333402633667
INFO:root:current mean train loss 1840.7477504902906
INFO:root:current train perplexity4.276239395141602
INFO:root:current mean train loss 1841.19499181079
INFO:root:current train perplexity4.279090404510498
INFO:root:current mean train loss 1841.1493037721934
INFO:root:current train perplexity4.281379699707031
INFO:root:current mean train loss 1842.428389721763
INFO:root:current train perplexity4.282611846923828
INFO:root:current mean train loss 1843.4823320633116
INFO:root:current train perplexity4.28475284576416
INFO:root:current mean train loss 1844.4501591527546
INFO:root:current train perplexity4.286888599395752

100%|██████████| 1/1 [05:26<00:00, 326.62s/it][A100%|██████████| 1/1 [05:26<00:00, 326.62s/it]
INFO:root:final mean train loss: 1844.4832767365378
INFO:root:final train perplexity: 4.287356376647949
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.76s/it][A100%|██████████| 1/1 [00:21<00:00, 21.76s/it]
INFO:root:eval mean loss: 2025.9218511919603
INFO:root:eval perplexity: 5.152106761932373
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.26s/it][A100%|██████████| 1/1 [00:21<00:00, 21.26s/it]
INFO:root:eval mean loss: 2472.8605623718695
INFO:root:eval perplexity: 7.637035846710205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/28
 14%|█▍        | 28/200 [2:59:10<18:01:54, 377.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1835.7972037760417
INFO:root:current train perplexity4.244594097137451
INFO:root:current mean train loss 1824.993881138393
INFO:root:current train perplexity4.221969127655029
INFO:root:current mean train loss 1827.4287198153409
INFO:root:current train perplexity4.223678112030029
INFO:root:current mean train loss 1824.6591119791667
INFO:root:current train perplexity4.214690208435059
INFO:root:current mean train loss 1824.1437978001645
INFO:root:current train perplexity4.210243225097656
INFO:root:current mean train loss 1824.619662661345
INFO:root:current train perplexity4.205395221710205
INFO:root:current mean train loss 1826.601511501736
INFO:root:current train perplexity4.212717056274414
INFO:root:current mean train loss 1827.5172334929437
INFO:root:current train perplexity4.217618942260742
INFO:root:current mean train loss 1825.9331046316963
INFO:root:current train perplexity4.2202959060668945
INFO:root:current mean train loss 1829.368372270633
INFO:root:current train perplexity4.226568222045898
INFO:root:current mean train loss 1830.1424467432776
INFO:root:current train perplexity4.231485843658447
INFO:root:current mean train loss 1829.6814967378657
INFO:root:current train perplexity4.233235836029053
INFO:root:current mean train loss 1828.180768324908
INFO:root:current train perplexity4.231541633605957
INFO:root:current mean train loss 1830.0821971768466
INFO:root:current train perplexity4.2350287437438965
INFO:root:current mean train loss 1829.5693615929556
INFO:root:current train perplexity4.236001491546631
INFO:root:current mean train loss 1829.907808624752
INFO:root:current train perplexity4.23843240737915
INFO:root:current mean train loss 1830.7629867508163
INFO:root:current train perplexity4.2394490242004395
INFO:root:current mean train loss 1831.9110406580105
INFO:root:current train perplexity4.242950916290283
INFO:root:current mean train loss 1833.1324085286458
INFO:root:current train perplexity4.247478485107422
INFO:root:current mean train loss 1832.8260753312895
INFO:root:current train perplexity4.2464985847473145

100%|██████████| 1/1 [05:28<00:00, 328.61s/it][A100%|██████████| 1/1 [05:28<00:00, 328.61s/it]
INFO:root:final mean train loss: 1832.3440348922875
INFO:root:final train perplexity: 4.246478080749512
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.33s/it][A100%|██████████| 1/1 [00:22<00:00, 22.33s/it]
INFO:root:eval mean loss: 2027.8016249203513
INFO:root:eval perplexity: 5.159950256347656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.08s/it][A100%|██████████| 1/1 [00:21<00:00, 21.08s/it]
INFO:root:eval mean loss: 2474.519675829732
INFO:root:eval perplexity: 7.647459506988525
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/29
 14%|█▍        | 29/200 [3:05:24<17:52:39, 376.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1818.2947931704314
INFO:root:current train perplexity4.219902515411377
INFO:root:current mean train loss 1810.7103602091472
INFO:root:current train perplexity4.201716899871826
INFO:root:current mean train loss 1808.9140165146082
INFO:root:current train perplexity4.196220397949219
INFO:root:current mean train loss 1812.4291696353835
INFO:root:current train perplexity4.200175762176514
INFO:root:current mean train loss 1814.5639802265944
INFO:root:current train perplexity4.199240207672119
INFO:root:current mean train loss 1814.5555388991897
INFO:root:current train perplexity4.197024345397949
INFO:root:current mean train loss 1814.315734510477
INFO:root:current train perplexity4.19618558883667
INFO:root:current mean train loss 1815.2508688262014
INFO:root:current train perplexity4.199258804321289
INFO:root:current mean train loss 1815.7367518129904
INFO:root:current train perplexity4.200927257537842
INFO:root:current mean train loss 1816.8792088416315
INFO:root:current train perplexity4.19989013671875
INFO:root:current mean train loss 1816.7607719225762
INFO:root:current train perplexity4.198928356170654
INFO:root:current mean train loss 1816.7344415651871
INFO:root:current train perplexity4.198528289794922
INFO:root:current mean train loss 1817.1003908328596
INFO:root:current train perplexity4.1997528076171875
INFO:root:current mean train loss 1817.3837790653624
INFO:root:current train perplexity4.201719760894775
INFO:root:current mean train loss 1818.8180780078387
INFO:root:current train perplexity4.2032904624938965
INFO:root:current mean train loss 1819.9918125478466
INFO:root:current train perplexity4.205333709716797
INFO:root:current mean train loss 1820.3832299342955
INFO:root:current train perplexity4.206577301025391
INFO:root:current mean train loss 1821.3972605296544
INFO:root:current train perplexity4.207724571228027
INFO:root:current mean train loss 1821.5707940712539
INFO:root:current train perplexity4.208275318145752

100%|██████████| 1/1 [05:38<00:00, 338.18s/it][A100%|██████████| 1/1 [05:38<00:00, 338.18s/it]
INFO:root:final mean train loss: 1820.6006985585977
INFO:root:final train perplexity: 4.20730447769165
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.63s/it][A100%|██████████| 1/1 [00:22<00:00, 22.63s/it]
INFO:root:eval mean loss: 2029.573452563996
INFO:root:eval perplexity: 5.167353630065918
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.69s/it][A100%|██████████| 1/1 [00:21<00:00, 21.69s/it]
INFO:root:eval mean loss: 2481.8524836546985
INFO:root:eval perplexity: 7.693702697753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/30
 15%|█▌        | 30/200 [3:11:49<17:53:18, 378.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1747.5570882161458
INFO:root:current train perplexity4.034755706787109
INFO:root:current mean train loss 1802.7226528902665
INFO:root:current train perplexity4.131441116333008
INFO:root:current mean train loss 1802.6383506373356
INFO:root:current train perplexity4.155013561248779
INFO:root:current mean train loss 1799.3054538961367
INFO:root:current train perplexity4.135950565338135
INFO:root:current mean train loss 1796.6680967895209
INFO:root:current train perplexity4.134251594543457
INFO:root:current mean train loss 1799.0685661031127
INFO:root:current train perplexity4.132865905761719
INFO:root:current mean train loss 1797.5504617424826
INFO:root:current train perplexity4.137191295623779
INFO:root:current mean train loss 1796.1963539715377
INFO:root:current train perplexity4.134589195251465
INFO:root:current mean train loss 1796.0851702224486
INFO:root:current train perplexity4.1330180168151855
INFO:root:current mean train loss 1800.0593109970177
INFO:root:current train perplexity4.139239311218262
INFO:root:current mean train loss 1800.0456551437453
INFO:root:current train perplexity4.143154621124268
INFO:root:current mean train loss 1801.7184252863644
INFO:root:current train perplexity4.148739814758301
INFO:root:current mean train loss 1802.6645262460258
INFO:root:current train perplexity4.152889251708984
INFO:root:current mean train loss 1804.0983868067824
INFO:root:current train perplexity4.155362129211426
INFO:root:current mean train loss 1803.7047178919556
INFO:root:current train perplexity4.152986526489258
INFO:root:current mean train loss 1805.4261830546668
INFO:root:current train perplexity4.157077789306641
INFO:root:current mean train loss 1805.6020934944793
INFO:root:current train perplexity4.157693862915039
INFO:root:current mean train loss 1805.560092736295
INFO:root:current train perplexity4.157591819763184
INFO:root:current mean train loss 1806.6375834315834
INFO:root:current train perplexity4.160287380218506
INFO:root:current mean train loss 1807.3260695635804
INFO:root:current train perplexity4.1624836921691895

100%|██████████| 1/1 [05:26<00:00, 326.29s/it][A100%|██████████| 1/1 [05:26<00:00, 326.29s/it]
INFO:root:final mean train loss: 1807.781536031118
INFO:root:final train perplexity: 4.16495418548584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.69s/it][A100%|██████████| 1/1 [00:22<00:00, 22.69s/it]
INFO:root:eval mean loss: 2029.1798840245456
INFO:root:eval perplexity: 5.165708541870117
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.18s/it][A100%|██████████| 1/1 [00:21<00:00, 21.18s/it]
INFO:root:eval mean loss: 2481.8479164069427
INFO:root:eval perplexity: 7.693673610687256
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/31
 16%|█▌        | 31/200 [3:18:01<17:41:21, 376.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1749.2079186072717
INFO:root:current train perplexity4.107074737548828
INFO:root:current mean train loss 1793.1631101578
INFO:root:current train perplexity4.111564636230469
INFO:root:current mean train loss 1789.0219089204231
INFO:root:current train perplexity4.087457656860352
INFO:root:current mean train loss 1786.5065925457727
INFO:root:current train perplexity4.092469692230225
INFO:root:current mean train loss 1789.2049050487822
INFO:root:current train perplexity4.0940470695495605
INFO:root:current mean train loss 1789.2613785312203
INFO:root:current train perplexity4.089903831481934
INFO:root:current mean train loss 1789.8456779699357
INFO:root:current train perplexity4.094565391540527
INFO:root:current mean train loss 1789.7823163497546
INFO:root:current train perplexity4.099026203155518
INFO:root:current mean train loss 1789.6944257907082
INFO:root:current train perplexity4.100229263305664
INFO:root:current mean train loss 1790.7730585019995
INFO:root:current train perplexity4.1000213623046875
INFO:root:current mean train loss 1792.1958098234954
INFO:root:current train perplexity4.103982925415039
INFO:root:current mean train loss 1792.4644839352866
INFO:root:current train perplexity4.107725143432617
INFO:root:current mean train loss 1793.5788873918293
INFO:root:current train perplexity4.1077141761779785
INFO:root:current mean train loss 1793.4804449987628
INFO:root:current train perplexity4.11146879196167
INFO:root:current mean train loss 1793.966787629843
INFO:root:current train perplexity4.113553524017334
INFO:root:current mean train loss 1795.0447198110357
INFO:root:current train perplexity4.117384433746338
INFO:root:current mean train loss 1794.7393066105953
INFO:root:current train perplexity4.119960784912109
INFO:root:current mean train loss 1794.9077719890643
INFO:root:current train perplexity4.120734691619873
INFO:root:current mean train loss 1795.8073784618232
INFO:root:current train perplexity4.124973773956299
INFO:root:current mean train loss 1795.8596848025377
INFO:root:current train perplexity4.12542200088501

100%|██████████| 1/1 [05:26<00:00, 326.19s/it][A100%|██████████| 1/1 [05:26<00:00, 326.19s/it]
INFO:root:final mean train loss: 1795.7986877102835
INFO:root:final train perplexity: 4.125751495361328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.29s/it][A100%|██████████| 1/1 [00:21<00:00, 21.29s/it]
INFO:root:eval mean loss: 2032.3829077321586
INFO:root:eval perplexity: 5.179115295410156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.38s/it][A100%|██████████| 1/1 [00:20<00:00, 20.38s/it]
INFO:root:eval mean loss: 2489.9133789928246
INFO:root:eval perplexity: 7.744858741760254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/32
 16%|█▌        | 32/200 [3:24:11<17:29:14, 374.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1764.466302916061
INFO:root:current train perplexity4.005491733551025
INFO:root:current mean train loss 1776.0866844337304
INFO:root:current train perplexity4.037443161010742
INFO:root:current mean train loss 1778.2777712472673
INFO:root:current train perplexity4.047569751739502
INFO:root:current mean train loss 1780.0734955812682
INFO:root:current train perplexity4.052520751953125
INFO:root:current mean train loss 1774.689646012627
INFO:root:current train perplexity4.047489166259766
INFO:root:current mean train loss 1777.2152546975713
INFO:root:current train perplexity4.051898002624512
INFO:root:current mean train loss 1778.0907701934293
INFO:root:current train perplexity4.056213855743408
INFO:root:current mean train loss 1778.3775991283226
INFO:root:current train perplexity4.0576581954956055
INFO:root:current mean train loss 1776.6272625725182
INFO:root:current train perplexity4.062414169311523
INFO:root:current mean train loss 1778.288279851952
INFO:root:current train perplexity4.065756320953369
INFO:root:current mean train loss 1778.2364020928212
INFO:root:current train perplexity4.066258907318115
INFO:root:current mean train loss 1778.0289523440917
INFO:root:current train perplexity4.067500591278076
INFO:root:current mean train loss 1778.4550030954595
INFO:root:current train perplexity4.070949554443359
INFO:root:current mean train loss 1779.0509364056393
INFO:root:current train perplexity4.074711322784424
INFO:root:current mean train loss 1779.935520819798
INFO:root:current train perplexity4.077790260314941
INFO:root:current mean train loss 1780.9126951226303
INFO:root:current train perplexity4.079196453094482
INFO:root:current mean train loss 1780.557896024921
INFO:root:current train perplexity4.078120708465576
INFO:root:current mean train loss 1781.7937502661314
INFO:root:current train perplexity4.080770015716553
INFO:root:current mean train loss 1783.0039795054345
INFO:root:current train perplexity4.082576274871826
INFO:root:current mean train loss 1784.0894531626955
INFO:root:current train perplexity4.085563659667969

100%|██████████| 1/1 [05:33<00:00, 333.22s/it][A100%|██████████| 1/1 [05:33<00:00, 333.22s/it]
INFO:root:final mean train loss: 1783.335543710898
INFO:root:final train perplexity: 4.085370063781738
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.89s/it][A100%|██████████| 1/1 [00:22<00:00, 22.89s/it]
INFO:root:eval mean loss: 2035.449147325881
INFO:root:eval perplexity: 5.191981792449951
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.31s/it][A100%|██████████| 1/1 [00:21<00:00, 21.31s/it]
INFO:root:eval mean loss: 2492.3623583638077
INFO:root:eval perplexity: 7.760466575622559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/33
 16%|█▋        | 33/200 [3:30:30<17:26:50, 376.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1745.3236938476562
INFO:root:current train perplexity4.032628536224365
INFO:root:current mean train loss 1755.1064186096191
INFO:root:current train perplexity4.009683132171631
INFO:root:current mean train loss 1759.7256610576924
INFO:root:current train perplexity4.024820804595947
INFO:root:current mean train loss 1762.6030368381075
INFO:root:current train perplexity4.0289506912231445
INFO:root:current mean train loss 1764.0488002611244
INFO:root:current train perplexity4.033992767333984
INFO:root:current mean train loss 1763.4735273088727
INFO:root:current train perplexity4.034964561462402
INFO:root:current mean train loss 1762.7351808490175
INFO:root:current train perplexity4.036678314208984
INFO:root:current mean train loss 1763.587808227539
INFO:root:current train perplexity4.0355119705200195
INFO:root:current mean train loss 1764.54354574514
INFO:root:current train perplexity4.033536434173584
INFO:root:current mean train loss 1766.075865427653
INFO:root:current train perplexity4.03446102142334
INFO:root:current mean train loss 1767.367934662441
INFO:root:current train perplexity4.0341668128967285
INFO:root:current mean train loss 1767.6544288372172
INFO:root:current train perplexity4.037030220031738
INFO:root:current mean train loss 1768.9205004495288
INFO:root:current train perplexity4.039556503295898
INFO:root:current mean train loss 1770.2152937945198
INFO:root:current train perplexity4.039008617401123
INFO:root:current mean train loss 1771.4125607843268
INFO:root:current train perplexity4.0420098304748535
INFO:root:current mean train loss 1771.7350885635767
INFO:root:current train perplexity4.043496608734131
INFO:root:current mean train loss 1772.673698627518
INFO:root:current train perplexity4.046473026275635
INFO:root:current mean train loss 1772.6039557717063
INFO:root:current train perplexity4.047765731811523
INFO:root:current mean train loss 1772.60555328041
INFO:root:current train perplexity4.047821044921875
INFO:root:current mean train loss 1773.2687053446866
INFO:root:current train perplexity4.051639080047607

100%|██████████| 1/1 [05:25<00:00, 325.97s/it][A100%|██████████| 1/1 [05:25<00:00, 325.97s/it]
INFO:root:final mean train loss: 1772.569276010395
INFO:root:final train perplexity: 4.05080509185791
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.20s/it][A100%|██████████| 1/1 [00:22<00:00, 22.20s/it]
INFO:root:eval mean loss: 2036.1616388415614
INFO:root:eval perplexity: 5.194976329803467
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.90s/it][A100%|██████████| 1/1 [00:20<00:00, 20.90s/it]
INFO:root:eval mean loss: 2494.495832294437
INFO:root:eval perplexity: 7.774092674255371
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/34
 17%|█▋        | 34/200 [3:36:41<17:16:20, 374.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1751.7420194551541
INFO:root:current train perplexity3.9688031673431396
INFO:root:current mean train loss 1745.3726351463188
INFO:root:current train perplexity3.965178966522217
INFO:root:current mean train loss 1746.487540014384
INFO:root:current train perplexity3.969482183456421
INFO:root:current mean train loss 1748.6092487203664
INFO:root:current train perplexity3.975576400756836
INFO:root:current mean train loss 1751.6152666199882
INFO:root:current train perplexity3.982025623321533
INFO:root:current mean train loss 1753.5550786750568
INFO:root:current train perplexity3.9888267517089844
INFO:root:current mean train loss 1754.8950979663844
INFO:root:current train perplexity3.995309352874756
INFO:root:current mean train loss 1755.3485143555945
INFO:root:current train perplexity3.9982314109802246
INFO:root:current mean train loss 1756.4168549453927
INFO:root:current train perplexity3.99979305267334
INFO:root:current mean train loss 1757.445108341463
INFO:root:current train perplexity3.9985544681549072
INFO:root:current mean train loss 1757.591156260881
INFO:root:current train perplexity3.998790979385376
INFO:root:current mean train loss 1757.8429839750822
INFO:root:current train perplexity3.9992551803588867
INFO:root:current mean train loss 1758.08353174937
INFO:root:current train perplexity4.003408432006836
INFO:root:current mean train loss 1757.4302989153562
INFO:root:current train perplexity4.0013427734375
INFO:root:current mean train loss 1759.381800729678
INFO:root:current train perplexity4.007384777069092
INFO:root:current mean train loss 1759.6380276967243
INFO:root:current train perplexity4.007206916809082
INFO:root:current mean train loss 1760.9319184678043
INFO:root:current train perplexity4.010962009429932
INFO:root:current mean train loss 1760.090040628737
INFO:root:current train perplexity4.010768890380859
INFO:root:current mean train loss 1760.8026072319733
INFO:root:current train perplexity4.013195991516113
INFO:root:current mean train loss 1761.5092932740183
INFO:root:current train perplexity4.013842582702637

100%|██████████| 1/1 [05:28<00:00, 328.25s/it][A100%|██████████| 1/1 [05:28<00:00, 328.25s/it]
INFO:root:final mean train loss: 1760.900777981249
INFO:root:final train perplexity: 4.013673305511475
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.54s/it][A100%|██████████| 1/1 [00:21<00:00, 21.54s/it]
INFO:root:eval mean loss: 2042.2918012903092
INFO:root:eval perplexity: 5.220809459686279
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.60s/it][A100%|██████████| 1/1 [00:20<00:00, 20.60s/it]
INFO:root:eval mean loss: 2507.441675497285
INFO:root:eval perplexity: 7.857272148132324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/35
 18%|█▊        | 35/200 [3:42:53<17:08:13, 373.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1734.8252199862866
INFO:root:current train perplexity3.9399683475494385
INFO:root:current mean train loss 1739.6972467481476
INFO:root:current train perplexity3.9496240615844727
INFO:root:current mean train loss 1738.7093784046822
INFO:root:current train perplexity3.947789192199707
INFO:root:current mean train loss 1740.8953807850176
INFO:root:current train perplexity3.953481912612915
INFO:root:current mean train loss 1742.6760807423457
INFO:root:current train perplexity3.957988739013672
INFO:root:current mean train loss 1744.2482848504576
INFO:root:current train perplexity3.959660768508911
INFO:root:current mean train loss 1743.3071975048406
INFO:root:current train perplexity3.9566640853881836
INFO:root:current mean train loss 1742.9511076112838
INFO:root:current train perplexity3.957662343978882
INFO:root:current mean train loss 1744.3184130367817
INFO:root:current train perplexity3.9588451385498047
INFO:root:current mean train loss 1745.4942800111214
INFO:root:current train perplexity3.962197780609131
INFO:root:current mean train loss 1745.6613847638398
INFO:root:current train perplexity3.9611518383026123
INFO:root:current mean train loss 1746.8217440146698
INFO:root:current train perplexity3.9623801708221436
INFO:root:current mean train loss 1748.216766876268
INFO:root:current train perplexity3.96429181098938
INFO:root:current mean train loss 1748.7082222674464
INFO:root:current train perplexity3.9673755168914795
INFO:root:current mean train loss 1748.7246414041583
INFO:root:current train perplexity3.970134735107422
INFO:root:current mean train loss 1750.1126692136527
INFO:root:current train perplexity3.9738242626190186
INFO:root:current mean train loss 1750.6192239988793
INFO:root:current train perplexity3.9734766483306885
INFO:root:current mean train loss 1750.4682287175785
INFO:root:current train perplexity3.975865602493286
INFO:root:current mean train loss 1750.1495766080798
INFO:root:current train perplexity3.9766855239868164

100%|██████████| 1/1 [05:29<00:00, 329.80s/it][A100%|██████████| 1/1 [05:29<00:00, 329.80s/it]
INFO:root:final mean train loss: 1749.8380624037227
INFO:root:final train perplexity: 3.978783369064331
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.36s/it][A100%|██████████| 1/1 [00:22<00:00, 22.36s/it]
INFO:root:eval mean loss: 2042.4233645175366
INFO:root:eval perplexity: 5.221365451812744
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.36s/it][A100%|██████████| 1/1 [00:21<00:00, 21.36s/it]
INFO:root:eval mean loss: 2506.373904830175
INFO:root:eval perplexity: 7.850379467010498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/36
 18%|█▊        | 36/200 [3:49:09<17:03:17, 374.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1737.1906405362215
INFO:root:current train perplexity3.8852224349975586
INFO:root:current mean train loss 1743.1026413376267
INFO:root:current train perplexity3.891839027404785
INFO:root:current mean train loss 1733.7060593157582
INFO:root:current train perplexity3.891932487487793
INFO:root:current mean train loss 1731.229065922679
INFO:root:current train perplexity3.8984484672546387
INFO:root:current mean train loss 1734.3402509480497
INFO:root:current train perplexity3.912154197692871
INFO:root:current mean train loss 1734.3181458116744
INFO:root:current train perplexity3.917226552963257
INFO:root:current mean train loss 1731.489385077294
INFO:root:current train perplexity3.9149067401885986
INFO:root:current mean train loss 1730.345501219673
INFO:root:current train perplexity3.9112682342529297
INFO:root:current mean train loss 1730.7953358002273
INFO:root:current train perplexity3.9134445190429688
INFO:root:current mean train loss 1733.0639448783531
INFO:root:current train perplexity3.9176626205444336
INFO:root:current mean train loss 1734.4061200814479
INFO:root:current train perplexity3.916964054107666
INFO:root:current mean train loss 1734.898467385801
INFO:root:current train perplexity3.92236328125
INFO:root:current mean train loss 1735.9554337518064
INFO:root:current train perplexity3.9249329566955566
INFO:root:current mean train loss 1738.3810726023194
INFO:root:current train perplexity3.9277472496032715
INFO:root:current mean train loss 1738.368212942533
INFO:root:current train perplexity3.931932210922241
INFO:root:current mean train loss 1737.3527710849603
INFO:root:current train perplexity3.9342470169067383
INFO:root:current mean train loss 1737.159036703838
INFO:root:current train perplexity3.934680461883545
INFO:root:current mean train loss 1738.689368510511
INFO:root:current train perplexity3.9386463165283203
INFO:root:current mean train loss 1739.025480543165
INFO:root:current train perplexity3.9417474269866943
INFO:root:current mean train loss 1739.3770873320782
INFO:root:current train perplexity3.94421124458313

100%|██████████| 1/1 [05:22<00:00, 322.05s/it][A100%|██████████| 1/1 [05:22<00:00, 322.05s/it]
INFO:root:final mean train loss: 1739.0599264586388
INFO:root:final train perplexity: 3.945082664489746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.43s/it][A100%|██████████| 1/1 [00:22<00:00, 22.43s/it]
INFO:root:eval mean loss: 2045.0407078519781
INFO:root:eval perplexity: 5.232436656951904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.03s/it][A100%|██████████| 1/1 [00:22<00:00, 22.03s/it]
INFO:root:eval mean loss: 2512.487577830646
INFO:root:eval perplexity: 7.8899359703063965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/37
 18%|█▊        | 37/200 [3:55:18<16:52:19, 372.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1726.6842433384486
INFO:root:current train perplexity3.8892600536346436
INFO:root:current mean train loss 1721.0151824951172
INFO:root:current train perplexity3.8776206970214844
INFO:root:current mean train loss 1717.8455355460183
INFO:root:current train perplexity3.88877272605896
INFO:root:current mean train loss 1721.020518605302
INFO:root:current train perplexity3.8998053073883057
INFO:root:current mean train loss 1718.8760923581704
INFO:root:current train perplexity3.8972065448760986
INFO:root:current mean train loss 1716.6958997321851
INFO:root:current train perplexity3.885693073272705
INFO:root:current mean train loss 1720.2772626937574
INFO:root:current train perplexity3.8912999629974365
INFO:root:current mean train loss 1721.0530838180375
INFO:root:current train perplexity3.8923768997192383
INFO:root:current mean train loss 1721.9985174649003
INFO:root:current train perplexity3.891631841659546
INFO:root:current mean train loss 1723.096479876288
INFO:root:current train perplexity3.892852544784546
INFO:root:current mean train loss 1723.554577422977
INFO:root:current train perplexity3.895890474319458
INFO:root:current mean train loss 1723.196773123234
INFO:root:current train perplexity3.8999452590942383
INFO:root:current mean train loss 1724.100381459786
INFO:root:current train perplexity3.900984048843384
INFO:root:current mean train loss 1724.6534184835043
INFO:root:current train perplexity3.90448260307312
INFO:root:current mean train loss 1725.7583458310082
INFO:root:current train perplexity3.906409502029419
INFO:root:current mean train loss 1725.3292612605069
INFO:root:current train perplexity3.908055067062378
INFO:root:current mean train loss 1726.941649115935
INFO:root:current train perplexity3.9102835655212402
INFO:root:current mean train loss 1727.2827349768745
INFO:root:current train perplexity3.9111149311065674
INFO:root:current mean train loss 1727.9134645023805
INFO:root:current train perplexity3.9109385013580322
INFO:root:current mean train loss 1728.5040687149492
INFO:root:current train perplexity3.9119088649749756

100%|██████████| 1/1 [05:25<00:00, 325.15s/it][A100%|██████████| 1/1 [05:25<00:00, 325.15s/it]
INFO:root:final mean train loss: 1728.408663581852
INFO:root:final train perplexity: 3.9120590686798096
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.69s/it][A100%|██████████| 1/1 [00:21<00:00, 21.69s/it]
INFO:root:eval mean loss: 2049.0277939383864
INFO:root:eval perplexity: 5.249345302581787
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.69s/it][A100%|██████████| 1/1 [00:20<00:00, 20.70s/it]
INFO:root:eval mean loss: 2519.1007733717033
INFO:root:eval perplexity: 7.932949542999268
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/38
 19%|█▉        | 38/200 [4:01:27<16:43:31, 371.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1690.3283040364583
INFO:root:current train perplexity3.8277907371520996
INFO:root:current mean train loss 1699.0380354256465
INFO:root:current train perplexity3.839784622192383
INFO:root:current mean train loss 1696.8207589285714
INFO:root:current train perplexity3.8425238132476807
INFO:root:current mean train loss 1700.7715608016304
INFO:root:current train perplexity3.848151683807373
INFO:root:current mean train loss 1703.6092820071103
INFO:root:current train perplexity3.853832721710205
INFO:root:current mean train loss 1705.9472414349198
INFO:root:current train perplexity3.857283592224121
INFO:root:current mean train loss 1706.513123978016
INFO:root:current train perplexity3.8617942333221436
INFO:root:current mean train loss 1708.5714945338716
INFO:root:current train perplexity3.8655011653900146
INFO:root:current mean train loss 1709.8917210324983
INFO:root:current train perplexity3.8655412197113037
INFO:root:current mean train loss 1709.0455597408234
INFO:root:current train perplexity3.866811752319336
INFO:root:current mean train loss 1710.303241112814
INFO:root:current train perplexity3.8709497451782227
INFO:root:current mean train loss 1711.959369562807
INFO:root:current train perplexity3.872623920440674
INFO:root:current mean train loss 1713.3426665646962
INFO:root:current train perplexity3.8716609477996826
INFO:root:current mean train loss 1714.0922987700394
INFO:root:current train perplexity3.871521472930908
INFO:root:current mean train loss 1714.6315141618459
INFO:root:current train perplexity3.8733301162719727
INFO:root:current mean train loss 1714.731913209193
INFO:root:current train perplexity3.873194932937622
INFO:root:current mean train loss 1715.529895427669
INFO:root:current train perplexity3.8735768795013428
INFO:root:current mean train loss 1716.5671536421025
INFO:root:current train perplexity3.8756840229034424
INFO:root:current mean train loss 1716.7784478743224
INFO:root:current train perplexity3.876302719116211
INFO:root:current mean train loss 1717.426232439448
INFO:root:current train perplexity3.87758469581604

100%|██████████| 1/1 [05:27<00:00, 327.80s/it][A100%|██████████| 1/1 [05:27<00:00, 327.80s/it]
INFO:root:final mean train loss: 1717.3171806239263
INFO:root:final train perplexity: 3.877965211868286
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.05s/it][A100%|██████████| 1/1 [00:23<00:00, 23.05s/it]
INFO:root:eval mean loss: 2049.0004415309177
INFO:root:eval perplexity: 5.249229431152344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.34s/it][A100%|██████████| 1/1 [00:21<00:00, 21.34s/it]
INFO:root:eval mean loss: 2518.211541791334
INFO:root:eval perplexity: 7.927152633666992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/39
 20%|█▉        | 39/200 [4:07:41<16:39:18, 372.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1699.730208858367
INFO:root:current train perplexity3.8369271755218506
INFO:root:current mean train loss 1690.2985056182486
INFO:root:current train perplexity3.8280234336853027
INFO:root:current mean train loss 1686.5481194649035
INFO:root:current train perplexity3.8239855766296387
INFO:root:current mean train loss 1686.6706886923773
INFO:root:current train perplexity3.8244190216064453
INFO:root:current mean train loss 1688.5269762179553
INFO:root:current train perplexity3.819472551345825
INFO:root:current mean train loss 1691.1731037397826
INFO:root:current train perplexity3.822388172149658
INFO:root:current mean train loss 1694.0297058658657
INFO:root:current train perplexity3.8245832920074463
INFO:root:current mean train loss 1694.7116417271573
INFO:root:current train perplexity3.827793836593628
INFO:root:current mean train loss 1695.0985644134735
INFO:root:current train perplexity3.8262600898742676
INFO:root:current mean train loss 1697.3115518613565
INFO:root:current train perplexity3.8288462162017822
INFO:root:current mean train loss 1699.532238401711
INFO:root:current train perplexity3.829040765762329
INFO:root:current mean train loss 1699.2326541447599
INFO:root:current train perplexity3.831169366836548
INFO:root:current mean train loss 1701.4292432298373
INFO:root:current train perplexity3.8348238468170166
INFO:root:current mean train loss 1700.5605404219439
INFO:root:current train perplexity3.8333959579467773
INFO:root:current mean train loss 1701.9545371581364
INFO:root:current train perplexity3.833613395690918
INFO:root:current mean train loss 1703.2496992006193
INFO:root:current train perplexity3.8367855548858643
INFO:root:current mean train loss 1704.6542536876693
INFO:root:current train perplexity3.8383591175079346
INFO:root:current mean train loss 1704.6674005895954
INFO:root:current train perplexity3.8389976024627686
INFO:root:current mean train loss 1705.3247559380454
INFO:root:current train perplexity3.8418121337890625
INFO:root:current mean train loss 1706.6754435345788
INFO:root:current train perplexity3.844815254211426

100%|██████████| 1/1 [05:25<00:00, 325.51s/it][A100%|██████████| 1/1 [05:25<00:00, 325.51s/it]
INFO:root:final mean train loss: 1706.726685863039
INFO:root:final train perplexity: 3.8456871509552
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.66s/it][A100%|██████████| 1/1 [00:21<00:00, 21.67s/it]
INFO:root:eval mean loss: 2051.552344356023
INFO:root:eval perplexity: 5.260080814361572
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.24s/it][A100%|██████████| 1/1 [00:20<00:00, 20.24s/it]
INFO:root:eval mean loss: 2524.686736411237
INFO:root:eval perplexity: 7.969466686248779
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/40
 20%|██        | 40/200 [4:13:50<16:30:38, 371.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1688.9420552314082
INFO:root:current train perplexity3.7822282314300537
INFO:root:current mean train loss 1687.5900435634167
INFO:root:current train perplexity3.7807064056396484
INFO:root:current mean train loss 1683.8609829154066
INFO:root:current train perplexity3.7710819244384766
INFO:root:current mean train loss 1687.8044649390872
INFO:root:current train perplexity3.778367042541504
INFO:root:current mean train loss 1688.3165061488778
INFO:root:current train perplexity3.781414031982422
INFO:root:current mean train loss 1688.7172469961006
INFO:root:current train perplexity3.7859444618225098
INFO:root:current mean train loss 1687.8727732505292
INFO:root:current train perplexity3.7918694019317627
INFO:root:current mean train loss 1689.5000228783897
INFO:root:current train perplexity3.7982990741729736
INFO:root:current mean train loss 1690.215906831049
INFO:root:current train perplexity3.8001315593719482
INFO:root:current mean train loss 1691.1776383646418
INFO:root:current train perplexity3.7993247509002686
INFO:root:current mean train loss 1692.131309417357
INFO:root:current train perplexity3.803006172180176
INFO:root:current mean train loss 1691.9533032909908
INFO:root:current train perplexity3.8036227226257324
INFO:root:current mean train loss 1693.1908454328332
INFO:root:current train perplexity3.805204153060913
INFO:root:current mean train loss 1693.3250075596843
INFO:root:current train perplexity3.806556224822998
INFO:root:current mean train loss 1693.5549564013374
INFO:root:current train perplexity3.8060660362243652
INFO:root:current mean train loss 1694.184461771196
INFO:root:current train perplexity3.807742118835449
INFO:root:current mean train loss 1695.1690055842623
INFO:root:current train perplexity3.8102993965148926
INFO:root:current mean train loss 1696.2432118888114
INFO:root:current train perplexity3.811593770980835
INFO:root:current mean train loss 1697.0222869596944
INFO:root:current train perplexity3.814434051513672
INFO:root:current mean train loss 1697.6126623245248
INFO:root:current train perplexity3.8164913654327393

100%|██████████| 1/1 [05:30<00:00, 330.72s/it][A100%|██████████| 1/1 [05:30<00:00, 330.72s/it]
INFO:root:final mean train loss: 1697.0701436760808
INFO:root:final train perplexity: 3.8164918422698975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.90s/it][A100%|██████████| 1/1 [00:22<00:00, 22.90s/it]
INFO:root:eval mean loss: 2058.7655358280695
INFO:root:eval perplexity: 5.2908735275268555
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.18s/it][A100%|██████████| 1/1 [00:21<00:00, 21.18s/it]
INFO:root:eval mean loss: 2533.4386168134974
INFO:root:eval perplexity: 8.027013778686523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/41
 20%|██        | 41/200 [4:20:07<16:28:38, 373.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1667.1789944966633
INFO:root:current train perplexity3.730048656463623
INFO:root:current mean train loss 1664.8744251484773
INFO:root:current train perplexity3.736672878265381
INFO:root:current mean train loss 1668.1680029791755
INFO:root:current train perplexity3.7443203926086426
INFO:root:current mean train loss 1670.8273805560489
INFO:root:current train perplexity3.7541778087615967
INFO:root:current mean train loss 1671.5947420673986
INFO:root:current train perplexity3.7543840408325195
INFO:root:current mean train loss 1674.2756282115142
INFO:root:current train perplexity3.7540206909179688
INFO:root:current mean train loss 1674.289785626291
INFO:root:current train perplexity3.7608370780944824
INFO:root:current mean train loss 1676.2627471463763
INFO:root:current train perplexity3.7633180618286133
INFO:root:current mean train loss 1678.3487563814435
INFO:root:current train perplexity3.768284559249878
INFO:root:current mean train loss 1680.3559089875125
INFO:root:current train perplexity3.769664764404297
INFO:root:current mean train loss 1679.9973967614835
INFO:root:current train perplexity3.769301652908325
INFO:root:current mean train loss 1680.4259377163787
INFO:root:current train perplexity3.771697521209717
INFO:root:current mean train loss 1680.6822925143772
INFO:root:current train perplexity3.772627353668213
INFO:root:current mean train loss 1681.4898025818745
INFO:root:current train perplexity3.7730555534362793
INFO:root:current mean train loss 1682.3144949846726
INFO:root:current train perplexity3.7752280235290527
INFO:root:current mean train loss 1683.2720939617109
INFO:root:current train perplexity3.777207851409912
INFO:root:current mean train loss 1684.356174109117
INFO:root:current train perplexity3.7796385288238525
INFO:root:current mean train loss 1685.5670448762005
INFO:root:current train perplexity3.781761646270752
INFO:root:current mean train loss 1685.4224906953577
INFO:root:current train perplexity3.7825019359588623

100%|██████████| 1/1 [05:25<00:00, 325.47s/it][A100%|██████████| 1/1 [05:25<00:00, 325.47s/it]
INFO:root:final mean train loss: 1686.4518661614445
INFO:root:final train perplexity: 3.7846429347991943
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.88s/it][A100%|██████████| 1/1 [00:21<00:00, 21.88s/it]
INFO:root:eval mean loss: 2062.6666203492077
INFO:root:eval perplexity: 5.307602405548096
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:19<00:00, 19.95s/it][A100%|██████████| 1/1 [00:19<00:00, 19.95s/it]
INFO:root:eval mean loss: 2538.9309558192044
INFO:root:eval perplexity: 8.06334114074707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/42
 21%|██        | 42/200 [4:26:17<16:19:27, 371.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1618.8651780348557
INFO:root:current train perplexity3.6759867668151855
INFO:root:current mean train loss 1659.3530899993086
INFO:root:current train perplexity3.7019076347351074
INFO:root:current mean train loss 1668.2702287127713
INFO:root:current train perplexity3.7178738117218018
INFO:root:current mean train loss 1665.0267041483626
INFO:root:current train perplexity3.7133917808532715
INFO:root:current mean train loss 1668.668470923029
INFO:root:current train perplexity3.7183728218078613
INFO:root:current mean train loss 1668.7733939544498
INFO:root:current train perplexity3.7281174659729004
INFO:root:current mean train loss 1668.4869466411349
INFO:root:current train perplexity3.7318365573883057
INFO:root:current mean train loss 1669.422240526111
INFO:root:current train perplexity3.7355072498321533
INFO:root:current mean train loss 1669.9901549467154
INFO:root:current train perplexity3.7406046390533447
INFO:root:current mean train loss 1671.2484184340344
INFO:root:current train perplexity3.73917293548584
INFO:root:current mean train loss 1672.7994989694519
INFO:root:current train perplexity3.7438178062438965
INFO:root:current mean train loss 1671.301331608336
INFO:root:current train perplexity3.7422611713409424
INFO:root:current mean train loss 1672.4039737358628
INFO:root:current train perplexity3.7439098358154297
INFO:root:current mean train loss 1673.9214851745467
INFO:root:current train perplexity3.746239423751831
INFO:root:current mean train loss 1675.2474585531118
INFO:root:current train perplexity3.7480833530426025
INFO:root:current mean train loss 1675.2180452516989
INFO:root:current train perplexity3.749688148498535
INFO:root:current mean train loss 1675.3364366033545
INFO:root:current train perplexity3.7506520748138428
INFO:root:current mean train loss 1675.8949253952997
INFO:root:current train perplexity3.752748727798462
INFO:root:current mean train loss 1677.4586785595784
INFO:root:current train perplexity3.755385398864746
INFO:root:current mean train loss 1677.3618074727196
INFO:root:current train perplexity3.7551679611206055

100%|██████████| 1/1 [05:23<00:00, 323.94s/it][A100%|██████████| 1/1 [05:23<00:00, 323.94s/it]
INFO:root:final mean train loss: 1676.8197079349275
INFO:root:final train perplexity: 3.7559821605682373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.24s/it][A100%|██████████| 1/1 [00:22<00:00, 22.24s/it]
INFO:root:eval mean loss: 2065.6359335175644
INFO:root:eval perplexity: 5.320371627807617
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.59s/it][A100%|██████████| 1/1 [00:20<00:00, 20.59s/it]
INFO:root:eval mean loss: 2546.942016168689
INFO:root:eval perplexity: 8.116623878479004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/43
 22%|██▏       | 43/200 [4:32:25<16:10:42, 370.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1690.8650268554688
INFO:root:current train perplexity3.6783668994903564
INFO:root:current mean train loss 1656.6791316105769
INFO:root:current train perplexity3.6684305667877197
INFO:root:current mean train loss 1655.1296025815218
INFO:root:current train perplexity3.676982879638672
INFO:root:current mean train loss 1656.25048976089
INFO:root:current train perplexity3.69193434715271
INFO:root:current mean train loss 1657.749052109829
INFO:root:current train perplexity3.695254325866699
INFO:root:current mean train loss 1657.070810915389
INFO:root:current train perplexity3.700044870376587
INFO:root:current mean train loss 1659.4100190662202
INFO:root:current train perplexity3.7043306827545166
INFO:root:current mean train loss 1659.4945190429687
INFO:root:current train perplexity3.7024173736572266
INFO:root:current mean train loss 1660.1022702136672
INFO:root:current train perplexity3.7029902935028076
INFO:root:current mean train loss 1660.442436234669
INFO:root:current train perplexity3.700826644897461
INFO:root:current mean train loss 1660.986151300819
INFO:root:current train perplexity3.7054383754730225
INFO:root:current mean train loss 1660.7064510379216
INFO:root:current train perplexity3.706956386566162
INFO:root:current mean train loss 1661.3122433546114
INFO:root:current train perplexity3.7089686393737793
INFO:root:current mean train loss 1661.799572662124
INFO:root:current train perplexity3.70912504196167
INFO:root:current mean train loss 1662.4247392987872
INFO:root:current train perplexity3.7122061252593994
INFO:root:current mean train loss 1663.5869803634343
INFO:root:current train perplexity3.713352680206299
INFO:root:current mean train loss 1664.2591011281395
INFO:root:current train perplexity3.7162020206451416
INFO:root:current mean train loss 1664.9488233974214
INFO:root:current train perplexity3.7188546657562256
INFO:root:current mean train loss 1666.3455893261184
INFO:root:current train perplexity3.7219161987304688
INFO:root:current mean train loss 1666.6223530981824
INFO:root:current train perplexity3.7238845825195312

100%|██████████| 1/1 [05:23<00:00, 323.32s/it][A100%|██████████| 1/1 [05:23<00:00, 323.32s/it]
INFO:root:final mean train loss: 1666.5504282433399
INFO:root:final train perplexity: 3.7256646156311035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.59s/it][A100%|██████████| 1/1 [00:22<00:00, 22.59s/it]
INFO:root:eval mean loss: 2066.275116616107
INFO:root:eval perplexity: 5.323123931884766
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.06s/it][A100%|██████████| 1/1 [00:21<00:00, 21.06s/it]
INFO:root:eval mean loss: 2546.126911569149
INFO:root:eval perplexity: 8.111186027526855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/44
 22%|██▏       | 44/200 [4:38:34<16:02:54, 370.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1627.3312001329787
INFO:root:current train perplexity3.645996332168579
INFO:root:current mean train loss 1640.523773816167
INFO:root:current train perplexity3.6581127643585205
INFO:root:current mean train loss 1639.6226361850013
INFO:root:current train perplexity3.663052558898926
INFO:root:current mean train loss 1642.39152839067
INFO:root:current train perplexity3.66690731048584
INFO:root:current mean train loss 1643.8257032560823
INFO:root:current train perplexity3.671654462814331
INFO:root:current mean train loss 1645.0036206010054
INFO:root:current train perplexity3.6667144298553467
INFO:root:current mean train loss 1645.306978723918
INFO:root:current train perplexity3.669644355773926
INFO:root:current mean train loss 1646.5734629599126
INFO:root:current train perplexity3.6710798740386963
INFO:root:current mean train loss 1648.3549350706999
INFO:root:current train perplexity3.67448353767395
INFO:root:current mean train loss 1650.2356642790555
INFO:root:current train perplexity3.673936128616333
INFO:root:current mean train loss 1651.4149048667696
INFO:root:current train perplexity3.676628351211548
INFO:root:current mean train loss 1652.9856522151877
INFO:root:current train perplexity3.6784212589263916
INFO:root:current mean train loss 1654.3230411189982
INFO:root:current train perplexity3.681945562362671
INFO:root:current mean train loss 1655.4365362154615
INFO:root:current train perplexity3.6873903274536133
INFO:root:current mean train loss 1655.5172253274554
INFO:root:current train perplexity3.6896636486053467
INFO:root:current mean train loss 1656.2238988894837
INFO:root:current train perplexity3.6920294761657715
INFO:root:current mean train loss 1657.3561713769354
INFO:root:current train perplexity3.6950769424438477
INFO:root:current mean train loss 1657.6350872561668
INFO:root:current train perplexity3.6956169605255127
INFO:root:current mean train loss 1657.5894405941222
INFO:root:current train perplexity3.6968541145324707
INFO:root:current mean train loss 1657.8544084248203
INFO:root:current train perplexity3.6986799240112305

100%|██████████| 1/1 [05:30<00:00, 330.60s/it][A100%|██████████| 1/1 [05:30<00:00, 330.60s/it]
INFO:root:final mean train loss: 1657.5262548741944
INFO:root:final train perplexity: 3.6992249488830566
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.98s/it][A100%|██████████| 1/1 [00:22<00:00, 22.00s/it]
INFO:root:eval mean loss: 2071.5630514807735
INFO:root:eval perplexity: 5.345950603485107
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.66s/it][A100%|██████████| 1/1 [00:21<00:00, 21.66s/it]
INFO:root:eval mean loss: 2553.20943629488
INFO:root:eval perplexity: 8.158552169799805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/45
 22%|██▎       | 45/200 [4:44:50<16:01:16, 372.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1632.3064365386963
INFO:root:current train perplexity3.635720729827881
INFO:root:current mean train loss 1642.6968019066787
INFO:root:current train perplexity3.645634174346924
INFO:root:current mean train loss 1641.5771817294035
INFO:root:current train perplexity3.638190746307373
INFO:root:current mean train loss 1641.8622631031078
INFO:root:current train perplexity3.643214702606201
INFO:root:current mean train loss 1639.91400620033
INFO:root:current train perplexity3.6425564289093018
INFO:root:current mean train loss 1639.0961846967116
INFO:root:current train perplexity3.642538070678711
INFO:root:current mean train loss 1638.3690413509507
INFO:root:current train perplexity3.6454598903656006
INFO:root:current mean train loss 1638.7062424265278
INFO:root:current train perplexity3.6488821506500244
INFO:root:current mean train loss 1639.8328367162633
INFO:root:current train perplexity3.6499979496002197
INFO:root:current mean train loss 1639.9615673524215
INFO:root:current train perplexity3.649714231491089
INFO:root:current mean train loss 1640.577116084278
INFO:root:current train perplexity3.6524972915649414
INFO:root:current mean train loss 1642.2542074406679
INFO:root:current train perplexity3.6554763317108154
INFO:root:current mean train loss 1643.2447651730308
INFO:root:current train perplexity3.6592776775360107
INFO:root:current mean train loss 1644.3085667227022
INFO:root:current train perplexity3.663065195083618
INFO:root:current mean train loss 1645.0898771859258
INFO:root:current train perplexity3.6645469665527344
INFO:root:current mean train loss 1645.057453282349
INFO:root:current train perplexity3.665281057357788
INFO:root:current mean train loss 1645.6826043495764
INFO:root:current train perplexity3.66574764251709
INFO:root:current mean train loss 1646.0198236236226
INFO:root:current train perplexity3.667783737182617
INFO:root:current mean train loss 1646.9768090636944
INFO:root:current train perplexity3.6687848567962646
INFO:root:current mean train loss 1648.299756269591
INFO:root:current train perplexity3.6712088584899902

100%|██████████| 1/1 [05:22<00:00, 322.24s/it][A100%|██████████| 1/1 [05:22<00:00, 322.24s/it]
INFO:root:final mean train loss: 1647.8573891259298
INFO:root:final train perplexity: 3.671104669570923
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 22.00s/it][A100%|██████████| 1/1 [00:21<00:00, 22.00s/it]
INFO:root:eval mean loss: 2075.9268266393783
INFO:root:eval perplexity: 5.364861965179443
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.54s/it][A100%|██████████| 1/1 [00:21<00:00, 21.54s/it]
INFO:root:eval mean loss: 2559.9584753158247
INFO:root:eval perplexity: 8.20394515991211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/46
 23%|██▎       | 46/200 [4:50:58<15:51:40, 370.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.3835946542245
INFO:root:current train perplexity3.5811655521392822
INFO:root:current mean train loss 1608.5629262344614
INFO:root:current train perplexity3.5964951515197754
INFO:root:current mean train loss 1624.28157885134
INFO:root:current train perplexity3.616331100463867
INFO:root:current mean train loss 1626.226362573819
INFO:root:current train perplexity3.614244222640991
INFO:root:current mean train loss 1624.0910499874124
INFO:root:current train perplexity3.6124863624572754
INFO:root:current mean train loss 1624.0075944122473
INFO:root:current train perplexity3.6120638847351074
INFO:root:current mean train loss 1627.1561503361324
INFO:root:current train perplexity3.615823745727539
INFO:root:current mean train loss 1629.6069600084527
INFO:root:current train perplexity3.6224172115325928
INFO:root:current mean train loss 1631.1194401891494
INFO:root:current train perplexity3.6208436489105225
INFO:root:current mean train loss 1633.1260320603178
INFO:root:current train perplexity3.6210482120513916
INFO:root:current mean train loss 1634.066928747073
INFO:root:current train perplexity3.6222734451293945
INFO:root:current mean train loss 1634.6454691551783
INFO:root:current train perplexity3.6261606216430664
INFO:root:current mean train loss 1634.8793972947465
INFO:root:current train perplexity3.629704713821411
INFO:root:current mean train loss 1635.6734946193599
INFO:root:current train perplexity3.632744789123535
INFO:root:current mean train loss 1635.8958371797983
INFO:root:current train perplexity3.6336374282836914
INFO:root:current mean train loss 1636.39987516554
INFO:root:current train perplexity3.636474609375
INFO:root:current mean train loss 1636.0003797904428
INFO:root:current train perplexity3.6356921195983887
INFO:root:current mean train loss 1637.1826121840566
INFO:root:current train perplexity3.6389272212982178
INFO:root:current mean train loss 1637.5866736858718
INFO:root:current train perplexity3.639941930770874
INFO:root:current mean train loss 1638.2954147161709
INFO:root:current train perplexity3.6420440673828125

100%|██████████| 1/1 [05:30<00:00, 330.54s/it][A100%|██████████| 1/1 [05:30<00:00, 330.54s/it]
INFO:root:final mean train loss: 1637.7901585781387
INFO:root:final train perplexity: 3.6420536041259766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.23s/it][A100%|██████████| 1/1 [00:23<00:00, 23.23s/it]
INFO:root:eval mean loss: 2079.001180013021
INFO:root:eval perplexity: 5.3782243728637695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.17s/it][A100%|██████████| 1/1 [00:21<00:00, 21.17s/it]
INFO:root:eval mean loss: 2563.2051950008313
INFO:root:eval perplexity: 8.225873947143555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/47
 24%|██▎       | 47/200 [4:57:15<15:50:09, 372.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1620.2212075992506
INFO:root:current train perplexity3.5775535106658936
INFO:root:current mean train loss 1619.529661852904
INFO:root:current train perplexity3.5928776264190674
INFO:root:current mean train loss 1618.4935364179164
INFO:root:current train perplexity3.5830647945404053
INFO:root:current mean train loss 1620.5443271656131
INFO:root:current train perplexity3.5793962478637695
INFO:root:current mean train loss 1623.109937062704
INFO:root:current train perplexity3.5824828147888184
INFO:root:current mean train loss 1623.1814567030074
INFO:root:current train perplexity3.5833351612091064
INFO:root:current mean train loss 1622.2654868401908
INFO:root:current train perplexity3.5874669551849365
INFO:root:current mean train loss 1623.310189995252
INFO:root:current train perplexity3.589015483856201
INFO:root:current mean train loss 1623.5404959425894
INFO:root:current train perplexity3.592474937438965
INFO:root:current mean train loss 1623.8746380700854
INFO:root:current train perplexity3.596290111541748
INFO:root:current mean train loss 1623.7177370832267
INFO:root:current train perplexity3.596707344055176
INFO:root:current mean train loss 1624.471097153296
INFO:root:current train perplexity3.602454423904419
INFO:root:current mean train loss 1624.292235105541
INFO:root:current train perplexity3.6047706604003906
INFO:root:current mean train loss 1626.0779990003855
INFO:root:current train perplexity3.6087539196014404
INFO:root:current mean train loss 1626.1729095010796
INFO:root:current train perplexity3.6106510162353516
INFO:root:current mean train loss 1626.7514979967634
INFO:root:current train perplexity3.611130475997925
INFO:root:current mean train loss 1627.4742873768082
INFO:root:current train perplexity3.613424062728882
INFO:root:current mean train loss 1628.4691206239354
INFO:root:current train perplexity3.6142632961273193
INFO:root:current mean train loss 1628.831695009961
INFO:root:current train perplexity3.6156930923461914

100%|██████████| 1/1 [05:29<00:00, 329.48s/it][A100%|██████████| 1/1 [05:29<00:00, 329.48s/it]
INFO:root:final mean train loss: 1628.9166757157518
INFO:root:final train perplexity: 3.6166367530822754
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.08s/it][A100%|██████████| 1/1 [00:22<00:00, 22.08s/it]
INFO:root:eval mean loss: 2080.893445603391
INFO:root:eval perplexity: 5.386467456817627
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.55s/it][A100%|██████████| 1/1 [00:20<00:00, 20.55s/it]
INFO:root:eval mean loss: 2569.75078956117
INFO:root:eval perplexity: 8.270259857177734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/48
 24%|██▍       | 48/200 [5:03:29<15:45:09, 373.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1618.8493326822916
INFO:root:current train perplexity3.5503132343292236
INFO:root:current mean train loss 1609.5138257897418
INFO:root:current train perplexity3.57857084274292
INFO:root:current mean train loss 1606.519889512173
INFO:root:current train perplexity3.570998191833496
INFO:root:current mean train loss 1611.128015718006
INFO:root:current train perplexity3.571502685546875
INFO:root:current mean train loss 1610.7419724797628
INFO:root:current train perplexity3.5708794593811035
INFO:root:current mean train loss 1610.6152756181737
INFO:root:current train perplexity3.5690579414367676
INFO:root:current mean train loss 1613.3867864345148
INFO:root:current train perplexity3.5721263885498047
INFO:root:current mean train loss 1612.9874648300918
INFO:root:current train perplexity3.577496290206909
INFO:root:current mean train loss 1614.9164640648964
INFO:root:current train perplexity3.5783915519714355
INFO:root:current mean train loss 1615.4250393560023
INFO:root:current train perplexity3.578355312347412
INFO:root:current mean train loss 1615.718560700816
INFO:root:current train perplexity3.581029176712036
INFO:root:current mean train loss 1615.7993119175658
INFO:root:current train perplexity3.5838558673858643
INFO:root:current mean train loss 1616.4567375779643
INFO:root:current train perplexity3.5826356410980225
INFO:root:current mean train loss 1616.6173943233127
INFO:root:current train perplexity3.5832507610321045
INFO:root:current mean train loss 1617.755496787351
INFO:root:current train perplexity3.585141658782959
INFO:root:current mean train loss 1618.3610026847412
INFO:root:current train perplexity3.5865719318389893
INFO:root:current mean train loss 1619.3748005303066
INFO:root:current train perplexity3.5880534648895264
INFO:root:current mean train loss 1620.0567676065962
INFO:root:current train perplexity3.5895798206329346
INFO:root:current mean train loss 1619.9229955583894
INFO:root:current train perplexity3.589110851287842
INFO:root:current mean train loss 1619.9358932614637
INFO:root:current train perplexity3.589402198791504

100%|██████████| 1/1 [05:34<00:00, 334.57s/it][A100%|██████████| 1/1 [05:34<00:00, 334.57s/it]
INFO:root:final mean train loss: 1619.4088217253884
INFO:root:final train perplexity: 3.5896008014678955
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.44s/it][A100%|██████████| 1/1 [00:21<00:00, 21.44s/it]
INFO:root:eval mean loss: 2085.1499226888022
INFO:root:eval perplexity: 5.4050517082214355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.37s/it][A100%|██████████| 1/1 [00:21<00:00, 21.37s/it]
INFO:root:eval mean loss: 2575.3213695769614
INFO:root:eval perplexity: 8.308222770690918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/49
 24%|██▍       | 49/200 [5:09:48<15:43:38, 374.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1615.295181274414
INFO:root:current train perplexity3.539611577987671
INFO:root:current mean train loss 1596.825702089252
INFO:root:current train perplexity3.5233917236328125
INFO:root:current mean train loss 1600.1747657512797
INFO:root:current train perplexity3.5135982036590576
INFO:root:current mean train loss 1603.4990296880883
INFO:root:current train perplexity3.5204708576202393
INFO:root:current mean train loss 1600.3588307698567
INFO:root:current train perplexity3.52409029006958
INFO:root:current mean train loss 1602.19062541302
INFO:root:current train perplexity3.5324227809906006
INFO:root:current mean train loss 1602.4029353660874
INFO:root:current train perplexity3.534186840057373
INFO:root:current mean train loss 1602.01034212373
INFO:root:current train perplexity3.5390663146972656
INFO:root:current mean train loss 1602.1979387723482
INFO:root:current train perplexity3.5381531715393066
INFO:root:current mean train loss 1603.193432198062
INFO:root:current train perplexity3.5415759086608887
INFO:root:current mean train loss 1604.3424656594445
INFO:root:current train perplexity3.5433056354522705
INFO:root:current mean train loss 1605.8365424597641
INFO:root:current train perplexity3.548231601715088
INFO:root:current mean train loss 1607.309129492029
INFO:root:current train perplexity3.551938533782959
INFO:root:current mean train loss 1607.234705286341
INFO:root:current train perplexity3.5552468299865723
INFO:root:current mean train loss 1608.1020498435591
INFO:root:current train perplexity3.556800603866577
INFO:root:current mean train loss 1607.9820119195444
INFO:root:current train perplexity3.557130813598633
INFO:root:current mean train loss 1608.9488328671923
INFO:root:current train perplexity3.5588583946228027
INFO:root:current mean train loss 1609.141155357449
INFO:root:current train perplexity3.5584757328033447
INFO:root:current mean train loss 1609.9346574675048
INFO:root:current train perplexity3.5604658126831055
INFO:root:current mean train loss 1610.2168381337547
INFO:root:current train perplexity3.5610196590423584

100%|██████████| 1/1 [05:24<00:00, 324.58s/it][A100%|██████████| 1/1 [05:24<00:00, 324.58s/it]
INFO:root:final mean train loss: 1609.589120500327
INFO:root:final train perplexity: 3.5618896484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 22.00s/it][A100%|██████████| 1/1 [00:21<00:00, 22.00s/it]
INFO:root:eval mean loss: 2091.161657887993
INFO:root:eval perplexity: 5.431410789489746
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.05s/it][A100%|██████████| 1/1 [00:21<00:00, 21.05s/it]
INFO:root:eval mean loss: 2583.9217395106107
INFO:root:eval perplexity: 8.36717414855957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/50
 25%|██▌       | 50/200 [5:15:58<15:33:18, 373.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.8000064772002
INFO:root:current train perplexity3.4979376792907715
INFO:root:current mean train loss 1584.3755800388003
INFO:root:current train perplexity3.501211643218994
INFO:root:current mean train loss 1592.2935928283446
INFO:root:current train perplexity3.5106871128082275
INFO:root:current mean train loss 1592.784991507544
INFO:root:current train perplexity3.5063960552215576
INFO:root:current mean train loss 1593.6074933772097
INFO:root:current train perplexity3.5069527626037598
INFO:root:current mean train loss 1594.886684952755
INFO:root:current train perplexity3.5121264457702637
INFO:root:current mean train loss 1595.1862010514974
INFO:root:current train perplexity3.5165436267852783
INFO:root:current mean train loss 1594.7325235730975
INFO:root:current train perplexity3.519716739654541
INFO:root:current mean train loss 1596.1967330591137
INFO:root:current train perplexity3.521476984024048
INFO:root:current mean train loss 1596.7175283964618
INFO:root:current train perplexity3.521700382232666
INFO:root:current mean train loss 1598.333358546458
INFO:root:current train perplexity3.5258421897888184
INFO:root:current mean train loss 1597.7897330899152
INFO:root:current train perplexity3.5281221866607666
INFO:root:current mean train loss 1598.7324038918634
INFO:root:current train perplexity3.5283093452453613
INFO:root:current mean train loss 1600.6260238884995
INFO:root:current train perplexity3.534407138824463
INFO:root:current mean train loss 1601.4531682174397
INFO:root:current train perplexity3.5342955589294434
INFO:root:current mean train loss 1601.171257634714
INFO:root:current train perplexity3.534132957458496
INFO:root:current mean train loss 1601.3057624442133
INFO:root:current train perplexity3.5368010997772217
INFO:root:current mean train loss 1600.8516410884345
INFO:root:current train perplexity3.535770893096924
INFO:root:current mean train loss 1600.6919989082992
INFO:root:current train perplexity3.5352983474731445
INFO:root:current mean train loss 1601.6970303781586
INFO:root:current train perplexity3.5378634929656982

100%|██████████| 1/1 [05:32<00:00, 332.07s/it][A100%|██████████| 1/1 [05:32<00:00, 332.07s/it]
INFO:root:final mean train loss: 1601.3824127089538
INFO:root:final train perplexity: 3.5388951301574707
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.82s/it][A100%|██████████| 1/1 [00:21<00:00, 21.82s/it]
INFO:root:eval mean loss: 2095.3534619486923
INFO:root:eval perplexity: 5.449864864349365
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.37s/it][A100%|██████████| 1/1 [00:20<00:00, 20.37s/it]
INFO:root:eval mean loss: 2589.5573578963044
INFO:root:eval perplexity: 8.406030654907227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/51
 26%|██▌       | 51/200 [5:22:14<15:29:09, 374.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1608.1180216471355
INFO:root:current train perplexity3.497278928756714
INFO:root:current mean train loss 1590.9498805769954
INFO:root:current train perplexity3.4858314990997314
INFO:root:current mean train loss 1589.0270941024437
INFO:root:current train perplexity3.4900193214416504
INFO:root:current mean train loss 1586.1626726994748
INFO:root:current train perplexity3.4883196353912354
INFO:root:current mean train loss 1583.5526264501743
INFO:root:current train perplexity3.488330125808716
INFO:root:current mean train loss 1585.1523946485756
INFO:root:current train perplexity3.490671157836914
INFO:root:current mean train loss 1585.9335352808864
INFO:root:current train perplexity3.4928152561187744
INFO:root:current mean train loss 1587.2686213002814
INFO:root:current train perplexity3.4948410987854004
INFO:root:current mean train loss 1587.0067172501986
INFO:root:current train perplexity3.494799852371216
INFO:root:current mean train loss 1588.1169528368837
INFO:root:current train perplexity3.4985005855560303
INFO:root:current mean train loss 1589.0258281772176
INFO:root:current train perplexity3.5007412433624268
INFO:root:current mean train loss 1589.7841722544022
INFO:root:current train perplexity3.503676652908325
INFO:root:current mean train loss 1591.0360952079013
INFO:root:current train perplexity3.507033586502075
INFO:root:current mean train loss 1591.4574487554905
INFO:root:current train perplexity3.5074236392974854
INFO:root:current mean train loss 1591.224122176229
INFO:root:current train perplexity3.5085668563842773
INFO:root:current mean train loss 1590.8514520442957
INFO:root:current train perplexity3.508282423019409
INFO:root:current mean train loss 1591.032729644592
INFO:root:current train perplexity3.509955406188965
INFO:root:current mean train loss 1591.746766864781
INFO:root:current train perplexity3.511387348175049
INFO:root:current mean train loss 1592.4428877753844
INFO:root:current train perplexity3.5124759674072266
INFO:root:current mean train loss 1592.2617464424513
INFO:root:current train perplexity3.512770652770996

100%|██████████| 1/1 [05:22<00:00, 322.94s/it][A100%|██████████| 1/1 [05:22<00:00, 322.94s/it]
INFO:root:final mean train loss: 1592.0780506066703
INFO:root:final train perplexity: 3.5130035877227783
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.65s/it][A100%|██████████| 1/1 [00:22<00:00, 22.65s/it]
INFO:root:eval mean loss: 2102.8695133290394
INFO:root:eval perplexity: 5.483113765716553
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.49s/it][A100%|██████████| 1/1 [00:21<00:00, 21.49s/it]
INFO:root:eval mean loss: 2597.7540897883423
INFO:root:eval perplexity: 8.462869644165039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/52
 26%|██▌       | 52/200 [5:28:23<15:19:05, 372.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.3388260071536
INFO:root:current train perplexity3.4639241695404053
INFO:root:current mean train loss 1576.456733745304
INFO:root:current train perplexity3.4601240158081055
INFO:root:current mean train loss 1579.2716012691862
INFO:root:current train perplexity3.4696147441864014
INFO:root:current mean train loss 1577.970064407229
INFO:root:current train perplexity3.4670050144195557
INFO:root:current mean train loss 1576.1617633827477
INFO:root:current train perplexity3.4678235054016113
INFO:root:current mean train loss 1577.0184901975235
INFO:root:current train perplexity3.470871686935425
INFO:root:current mean train loss 1577.6505972330253
INFO:root:current train perplexity3.4729013442993164
INFO:root:current mean train loss 1576.5333228568009
INFO:root:current train perplexity3.4717423915863037
INFO:root:current mean train loss 1577.3463351810235
INFO:root:current train perplexity3.474355459213257
INFO:root:current mean train loss 1576.7811947392786
INFO:root:current train perplexity3.476459503173828
INFO:root:current mean train loss 1576.7900755821502
INFO:root:current train perplexity3.477945327758789
INFO:root:current mean train loss 1577.0088633571957
INFO:root:current train perplexity3.4795444011688232
INFO:root:current mean train loss 1578.2307121294696
INFO:root:current train perplexity3.4817862510681152
INFO:root:current mean train loss 1580.2673195972015
INFO:root:current train perplexity3.4838027954101562
INFO:root:current mean train loss 1580.947499723428
INFO:root:current train perplexity3.485766649246216
INFO:root:current mean train loss 1582.9545964754916
INFO:root:current train perplexity3.4867796897888184
INFO:root:current mean train loss 1583.1290645860024
INFO:root:current train perplexity3.4870433807373047
INFO:root:current mean train loss 1583.699383609962
INFO:root:current train perplexity3.487281084060669
INFO:root:current mean train loss 1584.2660288757552
INFO:root:current train perplexity3.4888253211975098
INFO:root:current mean train loss 1583.9716091415705
INFO:root:current train perplexity3.4906005859375

100%|██████████| 1/1 [05:26<00:00, 326.29s/it][A100%|██████████| 1/1 [05:26<00:00, 326.29s/it]
INFO:root:final mean train loss: 1583.9716091415705
INFO:root:final train perplexity: 3.4906005859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.91s/it][A100%|██████████| 1/1 [00:22<00:00, 22.91s/it]
INFO:root:eval mean loss: 2107.071271747562
INFO:root:eval perplexity: 5.501788139343262
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.10s/it][A100%|██████████| 1/1 [00:21<00:00, 21.10s/it]
INFO:root:eval mean loss: 2604.6544959967864
INFO:root:eval perplexity: 8.51101303100586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/53
 26%|██▋       | 53/200 [5:34:35<15:12:33, 372.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1563.2885290527345
INFO:root:current train perplexity3.40195894241333
INFO:root:current mean train loss 1560.3871392822266
INFO:root:current train perplexity3.4173460006713867
INFO:root:current mean train loss 1561.4798234049479
INFO:root:current train perplexity3.429128408432007
INFO:root:current mean train loss 1565.4190191650391
INFO:root:current train perplexity3.4279532432556152
INFO:root:current mean train loss 1567.8649990234376
INFO:root:current train perplexity3.4355597496032715
INFO:root:current mean train loss 1571.5809584554036
INFO:root:current train perplexity3.440126419067383
INFO:root:current mean train loss 1572.2434238978794
INFO:root:current train perplexity3.444512367248535
INFO:root:current mean train loss 1573.2570707702637
INFO:root:current train perplexity3.4472789764404297
INFO:root:current mean train loss 1571.654178873698
INFO:root:current train perplexity3.4447860717773438
INFO:root:current mean train loss 1571.5007443847655
INFO:root:current train perplexity3.4470274448394775
INFO:root:current mean train loss 1572.4478250399502
INFO:root:current train perplexity3.449601411819458
INFO:root:current mean train loss 1572.97466796875
INFO:root:current train perplexity3.4524550437927246
INFO:root:current mean train loss 1572.957723858173
INFO:root:current train perplexity3.4532840251922607
INFO:root:current mean train loss 1573.3953696114677
INFO:root:current train perplexity3.456455945968628
INFO:root:current mean train loss 1573.453280110677
INFO:root:current train perplexity3.458071708679199
INFO:root:current mean train loss 1573.0193540191651
INFO:root:current train perplexity3.4586727619171143
INFO:root:current mean train loss 1573.2973922190947
INFO:root:current train perplexity3.4606168270111084
INFO:root:current mean train loss 1573.6313558620877
INFO:root:current train perplexity3.462026834487915
INFO:root:current mean train loss 1574.693235184519
INFO:root:current train perplexity3.4638798236846924

100%|██████████| 1/1 [05:30<00:00, 330.14s/it][A100%|██████████| 1/1 [05:30<00:00, 330.14s/it]
INFO:root:final mean train loss: 1574.8678329090728
INFO:root:final train perplexity: 3.465611696243286
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.86s/it][A100%|██████████| 1/1 [00:22<00:00, 22.86s/it]
INFO:root:eval mean loss: 2110.3785387404423
INFO:root:eval perplexity: 5.516531944274902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.21s/it][A100%|██████████| 1/1 [00:21<00:00, 21.21s/it]
INFO:root:eval mean loss: 2608.4859606154423
INFO:root:eval perplexity: 8.537869453430176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/54
 27%|██▋       | 54/200 [5:40:51<15:09:01, 373.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1529.6554385914521
INFO:root:current train perplexity3.4603383541107178
INFO:root:current mean train loss 1554.0698722122063
INFO:root:current train perplexity3.4055864810943604
INFO:root:current mean train loss 1559.0371774418563
INFO:root:current train perplexity3.4135663509368896
INFO:root:current mean train loss 1557.1219490123472
INFO:root:current train perplexity3.4122114181518555
INFO:root:current mean train loss 1556.1981903734825
INFO:root:current train perplexity3.4164934158325195
INFO:root:current mean train loss 1556.343325941429
INFO:root:current train perplexity3.4193992614746094
INFO:root:current mean train loss 1555.3639054507066
INFO:root:current train perplexity3.416926622390747
INFO:root:current mean train loss 1557.0945401371273
INFO:root:current train perplexity3.4229094982147217
INFO:root:current mean train loss 1557.7916561579616
INFO:root:current train perplexity3.4240477085113525
INFO:root:current mean train loss 1558.6400589771333
INFO:root:current train perplexity3.425097703933716
INFO:root:current mean train loss 1559.1069349140778
INFO:root:current train perplexity3.4237167835235596
INFO:root:current mean train loss 1561.0416071797015
INFO:root:current train perplexity3.4269886016845703
INFO:root:current mean train loss 1560.7081723115243
INFO:root:current train perplexity3.4283640384674072
INFO:root:current mean train loss 1561.610786269991
INFO:root:current train perplexity3.4319727420806885
INFO:root:current mean train loss 1561.689915820588
INFO:root:current train perplexity3.432553768157959
INFO:root:current mean train loss 1563.0158457243685
INFO:root:current train perplexity3.4345781803131104
INFO:root:current mean train loss 1563.7707255309795
INFO:root:current train perplexity3.43615984916687
INFO:root:current mean train loss 1564.8449921027545
INFO:root:current train perplexity3.4381442070007324
INFO:root:current mean train loss 1564.9088332281706
INFO:root:current train perplexity3.438284397125244
INFO:root:current mean train loss 1565.9062462430106
INFO:root:current train perplexity3.440521717071533

100%|██████████| 1/1 [05:26<00:00, 326.58s/it][A100%|██████████| 1/1 [05:26<00:00, 326.58s/it]
INFO:root:final mean train loss: 1566.0557724668472
INFO:root:final train perplexity: 3.441593647003174
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.41s/it][A100%|██████████| 1/1 [00:22<00:00, 22.41s/it]
INFO:root:eval mean loss: 2110.0510743053246
INFO:root:eval perplexity: 5.515070915222168
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.02s/it][A100%|██████████| 1/1 [00:22<00:00, 22.02s/it]
INFO:root:eval mean loss: 2608.9655497665944
INFO:root:eval perplexity: 8.541234970092773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/55
 28%|██▊       | 55/200 [5:47:04<15:02:21, 373.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1557.8914364085479
INFO:root:current train perplexity3.397282600402832
INFO:root:current mean train loss 1551.3079642680154
INFO:root:current train perplexity3.40208101272583
INFO:root:current mean train loss 1549.1826578776042
INFO:root:current train perplexity3.400725841522217
INFO:root:current mean train loss 1547.7718527788174
INFO:root:current train perplexity3.399507522583008
INFO:root:current mean train loss 1550.109160111247
INFO:root:current train perplexity3.4015684127807617
INFO:root:current mean train loss 1551.339021032669
INFO:root:current train perplexity3.4073097705841064
INFO:root:current mean train loss 1552.55227208664
INFO:root:current train perplexity3.4123034477233887
INFO:root:current mean train loss 1555.1194156989739
INFO:root:current train perplexity3.4076716899871826
INFO:root:current mean train loss 1555.8147225871646
INFO:root:current train perplexity3.408982515335083
INFO:root:current mean train loss 1556.3475868502828
INFO:root:current train perplexity3.4072508811950684
INFO:root:current mean train loss 1556.0440181532958
INFO:root:current train perplexity3.40585994720459
INFO:root:current mean train loss 1555.4009810620935
INFO:root:current train perplexity3.407500743865967
INFO:root:current mean train loss 1554.6240342200479
INFO:root:current train perplexity3.4072203636169434
INFO:root:current mean train loss 1556.1867831343118
INFO:root:current train perplexity3.4097139835357666
INFO:root:current mean train loss 1556.303792062331
INFO:root:current train perplexity3.4111437797546387
INFO:root:current mean train loss 1557.0097855191186
INFO:root:current train perplexity3.413588762283325
INFO:root:current mean train loss 1557.3221087414895
INFO:root:current train perplexity3.4144184589385986
INFO:root:current mean train loss 1557.2454765698214
INFO:root:current train perplexity3.4155611991882324
INFO:root:current mean train loss 1557.8806779335214
INFO:root:current train perplexity3.4171557426452637
INFO:root:current mean train loss 1557.784535610195
INFO:root:current train perplexity3.4177186489105225

100%|██████████| 1/1 [05:28<00:00, 328.52s/it][A100%|██████████| 1/1 [05:28<00:00, 328.52s/it]
INFO:root:final mean train loss: 1557.9299531245076
INFO:root:final train perplexity: 3.4195938110351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.78s/it][A100%|██████████| 1/1 [00:21<00:00, 21.78s/it]
INFO:root:eval mean loss: 2118.308815814079
INFO:root:eval perplexity: 5.552047252655029
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.39s/it][A100%|██████████| 1/1 [00:21<00:00, 21.39s/it]
INFO:root:eval mean loss: 2619.9384562174478
INFO:root:eval perplexity: 8.618634223937988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/56
 28%|██▊       | 56/200 [5:53:18<14:56:15, 373.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1530.6211799172795
INFO:root:current train perplexity3.374335289001465
INFO:root:current mean train loss 1532.9294182985823
INFO:root:current train perplexity3.365793228149414
INFO:root:current mean train loss 1538.1855702191235
INFO:root:current train perplexity3.3778433799743652
INFO:root:current mean train loss 1538.585724659455
INFO:root:current train perplexity3.375124216079712
INFO:root:current mean train loss 1541.8289175097007
INFO:root:current train perplexity3.381943464279175
INFO:root:current mean train loss 1542.3895276964467
INFO:root:current train perplexity3.381551742553711
INFO:root:current mean train loss 1541.8648272864464
INFO:root:current train perplexity3.382986545562744
INFO:root:current mean train loss 1544.6295489477573
INFO:root:current train perplexity3.385925531387329
INFO:root:current mean train loss 1547.0521915852673
INFO:root:current train perplexity3.3882579803466797
INFO:root:current mean train loss 1547.1883896628137
INFO:root:current train perplexity3.390608787536621
INFO:root:current mean train loss 1547.1178275851496
INFO:root:current train perplexity3.3902454376220703
INFO:root:current mean train loss 1546.3151315644345
INFO:root:current train perplexity3.3889858722686768
INFO:root:current mean train loss 1547.3856951548137
INFO:root:current train perplexity3.3903024196624756
INFO:root:current mean train loss 1548.1790936834984
INFO:root:current train perplexity3.3925931453704834
INFO:root:current mean train loss 1547.8738827748105
INFO:root:current train perplexity3.393343925476074
INFO:root:current mean train loss 1548.5548466400205
INFO:root:current train perplexity3.394615411758423
INFO:root:current mean train loss 1548.8915903979832
INFO:root:current train perplexity3.3945086002349854
INFO:root:current mean train loss 1549.0072646127437
INFO:root:current train perplexity3.3957293033599854
INFO:root:current mean train loss 1549.6683951057917
INFO:root:current train perplexity3.3966004848480225
INFO:root:current mean train loss 1550.0611958310642
INFO:root:current train perplexity3.39801287651062

100%|██████████| 1/1 [05:26<00:00, 326.30s/it][A100%|██████████| 1/1 [05:26<00:00, 326.30s/it]
INFO:root:final mean train loss: 1549.7592754969978
INFO:root:final train perplexity: 3.3976147174835205
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.67s/it][A100%|██████████| 1/1 [00:22<00:00, 22.67s/it]
INFO:root:eval mean loss: 2125.8175953533632
INFO:root:eval perplexity: 5.585885047912598
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.04s/it][A100%|██████████| 1/1 [00:21<00:00, 21.04s/it]
INFO:root:eval mean loss: 2629.6475830078125
INFO:root:eval perplexity: 8.687703132629395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/57
 28%|██▊       | 57/200 [5:59:30<14:48:55, 372.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1525.0940246582031
INFO:root:current train perplexity3.3613569736480713
INFO:root:current mean train loss 1522.2208557128906
INFO:root:current train perplexity3.3549957275390625
INFO:root:current mean train loss 1520.144378206623
INFO:root:current train perplexity3.3414440155029297
INFO:root:current mean train loss 1524.4738610309103
INFO:root:current train perplexity3.3406121730804443
INFO:root:current mean train loss 1527.9601299579326
INFO:root:current train perplexity3.3476312160491943
INFO:root:current mean train loss 1527.5789659526986
INFO:root:current train perplexity3.3461291790008545
INFO:root:current mean train loss 1530.1069131268712
INFO:root:current train perplexity3.348126173019409
INFO:root:current mean train loss 1530.5624259312947
INFO:root:current train perplexity3.3520755767822266
INFO:root:current mean train loss 1531.9794170889436
INFO:root:current train perplexity3.353565216064453
INFO:root:current mean train loss 1533.1121116196814
INFO:root:current train perplexity3.354771137237549
INFO:root:current mean train loss 1535.2454526522633
INFO:root:current train perplexity3.3585140705108643
INFO:root:current mean train loss 1535.2332105244675
INFO:root:current train perplexity3.3603954315185547
INFO:root:current mean train loss 1537.3620459138408
INFO:root:current train perplexity3.3632922172546387
INFO:root:current mean train loss 1537.8554675007424
INFO:root:current train perplexity3.365778923034668
INFO:root:current mean train loss 1538.1184793830892
INFO:root:current train perplexity3.367299795150757
INFO:root:current mean train loss 1539.316364599734
INFO:root:current train perplexity3.369297981262207
INFO:root:current mean train loss 1539.5249653548644
INFO:root:current train perplexity3.3699755668640137
INFO:root:current mean train loss 1540.2065043729895
INFO:root:current train perplexity3.370760679244995
INFO:root:current mean train loss 1541.3443004926726
INFO:root:current train perplexity3.3726420402526855
INFO:root:current mean train loss 1541.2753325671683
INFO:root:current train perplexity3.373960018157959

100%|██████████| 1/1 [05:23<00:00, 323.30s/it][A100%|██████████| 1/1 [05:23<00:00, 323.30s/it]
INFO:root:final mean train loss: 1541.1589971360568
INFO:root:final train perplexity: 3.374631643295288
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.05s/it][A100%|██████████| 1/1 [00:22<00:00, 22.05s/it]
INFO:root:eval mean loss: 2126.4125777440713
INFO:root:eval perplexity: 5.588574409484863
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.69s/it][A100%|██████████| 1/1 [00:21<00:00, 21.69s/it]
INFO:root:eval mean loss: 2631.9020307824967
INFO:root:eval perplexity: 8.703821182250977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/58
 29%|██▉       | 58/200 [6:05:39<14:39:51, 371.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1519.0012623506434
INFO:root:current train perplexity3.335651159286499
INFO:root:current mean train loss 1506.4999181798987
INFO:root:current train perplexity3.3219897747039795
INFO:root:current mean train loss 1513.132902874863
INFO:root:current train perplexity3.330907106399536
INFO:root:current mean train loss 1515.7771179991882
INFO:root:current train perplexity3.3281209468841553
INFO:root:current mean train loss 1516.4704755960051
INFO:root:current train perplexity3.3309011459350586
INFO:root:current mean train loss 1519.601975035056
INFO:root:current train perplexity3.331988573074341
INFO:root:current mean train loss 1521.1677085709398
INFO:root:current train perplexity3.3313546180725098
INFO:root:current mean train loss 1520.833212455215
INFO:root:current train perplexity3.332732915878296
INFO:root:current mean train loss 1522.182446426995
INFO:root:current train perplexity3.333700180053711
INFO:root:current mean train loss 1523.0046102920764
INFO:root:current train perplexity3.3354153633117676
INFO:root:current mean train loss 1523.3647395683324
INFO:root:current train perplexity3.337249755859375
INFO:root:current mean train loss 1525.0246257540546
INFO:root:current train perplexity3.3389031887054443
INFO:root:current mean train loss 1525.972189532922
INFO:root:current train perplexity3.3405911922454834
INFO:root:current mean train loss 1526.895064481329
INFO:root:current train perplexity3.3433632850646973
INFO:root:current mean train loss 1528.0915767374263
INFO:root:current train perplexity3.3453047275543213
INFO:root:current mean train loss 1528.8706117070435
INFO:root:current train perplexity3.3458516597747803
INFO:root:current mean train loss 1530.6850838047108
INFO:root:current train perplexity3.3470025062561035
INFO:root:current mean train loss 1531.7832265132615
INFO:root:current train perplexity3.349144220352173
INFO:root:current mean train loss 1532.6119505216968
INFO:root:current train perplexity3.351710796356201

100%|██████████| 1/1 [05:26<00:00, 326.25s/it][A100%|██████████| 1/1 [05:26<00:00, 326.25s/it]
INFO:root:final mean train loss: 1533.0735974410418
INFO:root:final train perplexity: 3.3531665802001953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.46s/it][A100%|██████████| 1/1 [00:21<00:00, 21.46s/it]
INFO:root:eval mean loss: 2134.336495041002
INFO:root:eval perplexity: 5.624523639678955
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.44s/it][A100%|██████████| 1/1 [00:21<00:00, 21.44s/it]
INFO:root:eval mean loss: 2644.18786274795
INFO:root:eval perplexity: 8.7921781539917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/59
 30%|██▉       | 59/200 [6:11:50<14:33:10, 371.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1567.2880859375
INFO:root:current train perplexity3.4503421783447266
INFO:root:current mean train loss 1512.9930814855238
INFO:root:current train perplexity3.2921628952026367
INFO:root:current mean train loss 1512.206573184174
INFO:root:current train perplexity3.297008514404297
INFO:root:current mean train loss 1514.7397198203385
INFO:root:current train perplexity3.296929121017456
INFO:root:current mean train loss 1516.747712851757
INFO:root:current train perplexity3.300455331802368
INFO:root:current mean train loss 1516.8373731635957
INFO:root:current train perplexity3.301624059677124
INFO:root:current mean train loss 1519.0760790042307
INFO:root:current train perplexity3.3038642406463623
INFO:root:current mean train loss 1520.9973895733174
INFO:root:current train perplexity3.3073296546936035
INFO:root:current mean train loss 1523.7591286371473
INFO:root:current train perplexity3.313067674636841
INFO:root:current mean train loss 1523.5698244894159
INFO:root:current train perplexity3.3165719509124756
INFO:root:current mean train loss 1522.86306573864
INFO:root:current train perplexity3.3185908794403076
INFO:root:current mean train loss 1523.0801657453424
INFO:root:current train perplexity3.3211894035339355
INFO:root:current mean train loss 1523.2125574197626
INFO:root:current train perplexity3.3234801292419434
INFO:root:current mean train loss 1523.623522405434
INFO:root:current train perplexity3.3244855403900146
INFO:root:current mean train loss 1524.0468395630405
INFO:root:current train perplexity3.325612783432007
INFO:root:current mean train loss 1524.1133332639813
INFO:root:current train perplexity3.327756643295288
INFO:root:current mean train loss 1524.6097842632012
INFO:root:current train perplexity3.3301525115966797
INFO:root:current mean train loss 1525.1664672564675
INFO:root:current train perplexity3.331185817718506
INFO:root:current mean train loss 1525.6697720983846
INFO:root:current train perplexity3.331453561782837
INFO:root:current mean train loss 1525.1388686508035
INFO:root:current train perplexity3.3316426277160645

100%|██████████| 1/1 [05:22<00:00, 322.53s/it][A100%|██████████| 1/1 [05:22<00:00, 322.53s/it]
INFO:root:final mean train loss: 1524.8065147072873
INFO:root:final train perplexity: 3.331360340118408
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.01s/it][A100%|██████████| 1/1 [00:22<00:00, 22.01s/it]
INFO:root:eval mean loss: 2141.6360755000555
INFO:root:eval perplexity: 5.657846927642822
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.51s/it][A100%|██████████| 1/1 [00:21<00:00, 21.51s/it]
INFO:root:eval mean loss: 2651.8661581269394
INFO:root:eval perplexity: 8.847855567932129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/60
 30%|███       | 60/200 [6:17:58<14:24:33, 370.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1503.6169112356085
INFO:root:current train perplexity3.264693260192871
INFO:root:current mean train loss 1499.4574190027574
INFO:root:current train perplexity3.2572593688964844
INFO:root:current mean train loss 1500.787957735802
INFO:root:current train perplexity3.263564348220825
INFO:root:current mean train loss 1504.5723452194356
INFO:root:current train perplexity3.2716662883758545
INFO:root:current mean train loss 1509.4037576097292
INFO:root:current train perplexity3.279994249343872
INFO:root:current mean train loss 1511.1127344032243
INFO:root:current train perplexity3.2854163646698
INFO:root:current mean train loss 1512.2887057549349
INFO:root:current train perplexity3.2858307361602783
INFO:root:current mean train loss 1513.9379458367741
INFO:root:current train perplexity3.2859432697296143
INFO:root:current mean train loss 1514.0823174221612
INFO:root:current train perplexity3.2881510257720947
INFO:root:current mean train loss 1513.584056899908
INFO:root:current train perplexity3.290976047515869
INFO:root:current mean train loss 1513.346462500575
INFO:root:current train perplexity3.2930612564086914
INFO:root:current mean train loss 1512.573459601381
INFO:root:current train perplexity3.2944724559783936
INFO:root:current mean train loss 1513.3345651853465
INFO:root:current train perplexity3.2973642349243164
INFO:root:current mean train loss 1514.5973466782068
INFO:root:current train perplexity3.299551248550415
INFO:root:current mean train loss 1514.9769325820891
INFO:root:current train perplexity3.3006508350372314
INFO:root:current mean train loss 1515.5161779728903
INFO:root:current train perplexity3.3023853302001953
INFO:root:current mean train loss 1515.5395743056092
INFO:root:current train perplexity3.304680585861206
INFO:root:current mean train loss 1516.1196786859412
INFO:root:current train perplexity3.305990695953369
INFO:root:current mean train loss 1516.863965152449
INFO:root:current train perplexity3.3091776371002197
INFO:root:current mean train loss 1517.5861575955087
INFO:root:current train perplexity3.311330795288086

100%|██████████| 1/1 [05:34<00:00, 334.59s/it][A100%|██████████| 1/1 [05:34<00:00, 334.59s/it]
INFO:root:final mean train loss: 1517.4124622893225
INFO:root:final train perplexity: 3.311976909637451
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.21s/it][A100%|██████████| 1/1 [00:22<00:00, 22.21s/it]
INFO:root:eval mean loss: 2145.6636871987203
INFO:root:eval perplexity: 5.6763176918029785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.58s/it][A100%|██████████| 1/1 [00:20<00:00, 20.58s/it]
INFO:root:eval mean loss: 2655.606489898465
INFO:root:eval perplexity: 8.875107765197754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/61
 30%|███       | 61/200 [6:24:17<14:24:27, 373.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1485.661841498481
INFO:root:current train perplexity3.244945764541626
INFO:root:current mean train loss 1493.3115288229549
INFO:root:current train perplexity3.263185739517212
INFO:root:current mean train loss 1499.9751545534295
INFO:root:current train perplexity3.2710399627685547
INFO:root:current mean train loss 1500.335812159947
INFO:root:current train perplexity3.2688815593719482
INFO:root:current mean train loss 1499.9492274293111
INFO:root:current train perplexity3.2677552700042725
INFO:root:current mean train loss 1502.7802702490962
INFO:root:current train perplexity3.2699434757232666
INFO:root:current mean train loss 1503.9621653046997
INFO:root:current train perplexity3.272030830383301
INFO:root:current mean train loss 1503.7069828199303
INFO:root:current train perplexity3.2726597785949707
INFO:root:current mean train loss 1504.6463063801305
INFO:root:current train perplexity3.2765820026397705
INFO:root:current mean train loss 1505.215102497329
INFO:root:current train perplexity3.2781662940979004
INFO:root:current mean train loss 1506.029608295691
INFO:root:current train perplexity3.2818286418914795
INFO:root:current mean train loss 1506.7570710517991
INFO:root:current train perplexity3.2833919525146484
INFO:root:current mean train loss 1507.9068380312626
INFO:root:current train perplexity3.284986734390259
INFO:root:current mean train loss 1507.5448102208668
INFO:root:current train perplexity3.285658597946167
INFO:root:current mean train loss 1507.9187220836416
INFO:root:current train perplexity3.2868099212646484
INFO:root:current mean train loss 1509.03431781133
INFO:root:current train perplexity3.287313461303711
INFO:root:current mean train loss 1509.7359152796216
INFO:root:current train perplexity3.2883141040802
INFO:root:current mean train loss 1509.413573585897
INFO:root:current train perplexity3.2890279293060303
INFO:root:current mean train loss 1510.0342550173823
INFO:root:current train perplexity3.290131092071533
INFO:root:current mean train loss 1509.9935650155564
INFO:root:current train perplexity3.2901079654693604

100%|██████████| 1/1 [05:28<00:00, 328.33s/it][A100%|██████████| 1/1 [05:28<00:00, 328.33s/it]
INFO:root:final mean train loss: 1509.180772005159
INFO:root:final train perplexity: 3.2905306816101074
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.46s/it][A100%|██████████| 1/1 [00:22<00:00, 22.46s/it]
INFO:root:eval mean loss: 2149.8632557104665
INFO:root:eval perplexity: 5.695641040802002
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.23s/it][A100%|██████████| 1/1 [00:21<00:00, 21.23s/it]
INFO:root:eval mean loss: 2663.240414883228
INFO:root:eval perplexity: 8.93097972869873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/62
 31%|███       | 62/200 [6:30:31<14:18:43, 373.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1470.463201558815
INFO:root:current train perplexity3.222996234893799
INFO:root:current mean train loss 1484.0056319891237
INFO:root:current train perplexity3.2268056869506836
INFO:root:current mean train loss 1486.9706838253458
INFO:root:current train perplexity3.2267210483551025
INFO:root:current mean train loss 1488.9520014689936
INFO:root:current train perplexity3.2364189624786377
INFO:root:current mean train loss 1493.7272256678566
INFO:root:current train perplexity3.246919631958008
INFO:root:current mean train loss 1495.229654653594
INFO:root:current train perplexity3.246957778930664
INFO:root:current mean train loss 1495.08919432846
INFO:root:current train perplexity3.2495758533477783
INFO:root:current mean train loss 1494.87944640708
INFO:root:current train perplexity3.24800968170166
INFO:root:current mean train loss 1495.601927422974
INFO:root:current train perplexity3.250227928161621
INFO:root:current mean train loss 1497.610140725423
INFO:root:current train perplexity3.2541439533233643
INFO:root:current mean train loss 1499.0698608514363
INFO:root:current train perplexity3.2564291954040527
INFO:root:current mean train loss 1500.4892237217448
INFO:root:current train perplexity3.258687734603882
INFO:root:current mean train loss 1500.48426822501
INFO:root:current train perplexity3.2607851028442383
INFO:root:current mean train loss 1500.1705928557199
INFO:root:current train perplexity3.261869430541992
INFO:root:current mean train loss 1500.3065619892034
INFO:root:current train perplexity3.2623586654663086
INFO:root:current mean train loss 1500.8531771608882
INFO:root:current train perplexity3.264704942703247
INFO:root:current mean train loss 1501.4556604882694
INFO:root:current train perplexity3.2660646438598633
INFO:root:current mean train loss 1501.313806772096
INFO:root:current train perplexity3.2671144008636475
INFO:root:current mean train loss 1501.4655486352367
INFO:root:current train perplexity3.2696080207824707
INFO:root:current mean train loss 1502.0955505527354
INFO:root:current train perplexity3.27122163772583

100%|██████████| 1/1 [05:28<00:00, 328.85s/it][A100%|██████████| 1/1 [05:28<00:00, 328.85s/it]
INFO:root:final mean train loss: 1501.9520237201762
INFO:root:final train perplexity: 3.2718117237091064
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.38s/it][A100%|██████████| 1/1 [00:21<00:00, 21.38s/it]
INFO:root:eval mean loss: 2150.985007860982
INFO:root:eval perplexity: 5.700813293457031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.72s/it][A100%|██████████| 1/1 [00:20<00:00, 20.72s/it]
INFO:root:eval mean loss: 2664.8700734672816
INFO:root:eval perplexity: 8.94295597076416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/63
 32%|███▏      | 63/200 [6:36:44<14:12:10, 373.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1485.5337559291295
INFO:root:current train perplexity3.232433557510376
INFO:root:current mean train loss 1487.202598661535
INFO:root:current train perplexity3.228487014770508
INFO:root:current mean train loss 1484.8276398835358
INFO:root:current train perplexity3.220876932144165
INFO:root:current mean train loss 1484.435216295397
INFO:root:current train perplexity3.223517417907715
INFO:root:current mean train loss 1484.1647671313997
INFO:root:current train perplexity3.2237894535064697
INFO:root:current mean train loss 1486.2867650082237
INFO:root:current train perplexity3.2267942428588867
INFO:root:current mean train loss 1487.6151190458838
INFO:root:current train perplexity3.2313225269317627
INFO:root:current mean train loss 1489.6247690176035
INFO:root:current train perplexity3.2336056232452393
INFO:root:current mean train loss 1490.3458454000538
INFO:root:current train perplexity3.2389917373657227
INFO:root:current mean train loss 1490.028088882289
INFO:root:current train perplexity3.2391879558563232
INFO:root:current mean train loss 1490.6155276860031
INFO:root:current train perplexity3.2401533126831055
INFO:root:current mean train loss 1491.1605318509614
INFO:root:current train perplexity3.242852210998535
INFO:root:current mean train loss 1492.0376088059793
INFO:root:current train perplexity3.2446072101593018
INFO:root:current mean train loss 1492.7786764548644
INFO:root:current train perplexity3.2435357570648193
INFO:root:current mean train loss 1492.5496828663106
INFO:root:current train perplexity3.245948076248169
INFO:root:current mean train loss 1492.7310939521547
INFO:root:current train perplexity3.2474019527435303
INFO:root:current mean train loss 1493.3169983275636
INFO:root:current train perplexity3.2477409839630127
INFO:root:current mean train loss 1493.3918280477578
INFO:root:current train perplexity3.2491869926452637
INFO:root:current mean train loss 1493.6347736542239
INFO:root:current train perplexity3.249721050262451
INFO:root:current mean train loss 1494.7667015734057
INFO:root:current train perplexity3.251861095428467

100%|██████████| 1/1 [05:25<00:00, 325.41s/it][A100%|██████████| 1/1 [05:25<00:00, 325.41s/it]
INFO:root:final mean train loss: 1494.3255886399138
INFO:root:final train perplexity: 3.25217866897583
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.04s/it][A100%|██████████| 1/1 [00:22<00:00, 22.04s/it]
INFO:root:eval mean loss: 2155.9813678281525
INFO:root:eval perplexity: 5.7239089012146
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.10s/it][A100%|██████████| 1/1 [00:21<00:00, 21.10s/it]
INFO:root:eval mean loss: 2670.735890922817
INFO:root:eval perplexity: 8.986187934875488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/64
 32%|███▏      | 64/200 [6:42:54<14:04:05, 372.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1465.7101924501617
INFO:root:current train perplexity3.2126457691192627
INFO:root:current mean train loss 1466.181942210478
INFO:root:current train perplexity3.2025511264801025
INFO:root:current mean train loss 1472.5763016012902
INFO:root:current train perplexity3.210192918777466
INFO:root:current mean train loss 1475.6032238548732
INFO:root:current train perplexity3.2121024131774902
INFO:root:current mean train loss 1474.5502811878368
INFO:root:current train perplexity3.210634231567383
INFO:root:current mean train loss 1475.425797678543
INFO:root:current train perplexity3.2129318714141846
INFO:root:current mean train loss 1478.4999250164892
INFO:root:current train perplexity3.2143049240112305
INFO:root:current mean train loss 1479.6347775683469
INFO:root:current train perplexity3.217594623565674
INFO:root:current mean train loss 1480.725414736295
INFO:root:current train perplexity3.2204220294952393
INFO:root:current mean train loss 1481.4283870244824
INFO:root:current train perplexity3.2218565940856934
INFO:root:current mean train loss 1482.8442277250317
INFO:root:current train perplexity3.2229974269866943
INFO:root:current mean train loss 1483.4522099354333
INFO:root:current train perplexity3.2220470905303955
INFO:root:current mean train loss 1484.216774585549
INFO:root:current train perplexity3.224080801010132
INFO:root:current mean train loss 1484.3418032997533
INFO:root:current train perplexity3.224644899368286
INFO:root:current mean train loss 1483.415341816577
INFO:root:current train perplexity3.224550724029541
INFO:root:current mean train loss 1483.6062336316556
INFO:root:current train perplexity3.2251169681549072
INFO:root:current mean train loss 1484.676779737399
INFO:root:current train perplexity3.2272064685821533
INFO:root:current mean train loss 1485.8524568853395
INFO:root:current train perplexity3.2293715476989746
INFO:root:current mean train loss 1485.9657787167337
INFO:root:current train perplexity3.2305452823638916

100%|██████████| 1/1 [05:26<00:00, 326.91s/it][A100%|██████████| 1/1 [05:26<00:00, 326.91s/it]
INFO:root:final mean train loss: 1486.3120177206943
INFO:root:final train perplexity: 3.231675386428833
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.84s/it][A100%|██████████| 1/1 [00:21<00:00, 21.84s/it]
INFO:root:eval mean loss: 2167.237736695202
INFO:root:eval perplexity: 5.776284694671631
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.02s/it][A100%|██████████| 1/1 [00:21<00:00, 21.02s/it]
INFO:root:eval mean loss: 2685.375492177111
INFO:root:eval perplexity: 9.094995498657227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/65
 32%|███▎      | 65/200 [6:49:06<13:57:30, 372.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1482.9200744628906
INFO:root:current train perplexity3.174490213394165
INFO:root:current mean train loss 1465.7360241229717
INFO:root:current train perplexity3.192549705505371
INFO:root:current mean train loss 1463.4032826143152
INFO:root:current train perplexity3.1728641986846924
INFO:root:current mean train loss 1466.5710666054174
INFO:root:current train perplexity3.185318946838379
INFO:root:current mean train loss 1470.4018358287244
INFO:root:current train perplexity3.1889686584472656
INFO:root:current mean train loss 1470.7116018628317
INFO:root:current train perplexity3.1953999996185303
INFO:root:current mean train loss 1472.2971864409794
INFO:root:current train perplexity3.198857545852661
INFO:root:current mean train loss 1472.659461108121
INFO:root:current train perplexity3.199711561203003
INFO:root:current mean train loss 1473.0915288972617
INFO:root:current train perplexity3.1984150409698486
INFO:root:current mean train loss 1474.0009326766024
INFO:root:current train perplexity3.1991214752197266
INFO:root:current mean train loss 1474.5120535922715
INFO:root:current train perplexity3.200580596923828
INFO:root:current mean train loss 1475.9068967294002
INFO:root:current train perplexity3.2041900157928467
INFO:root:current mean train loss 1476.5547268382734
INFO:root:current train perplexity3.2057087421417236
INFO:root:current mean train loss 1476.989616440849
INFO:root:current train perplexity3.20632266998291
INFO:root:current mean train loss 1478.4898368639824
INFO:root:current train perplexity3.209822654724121
INFO:root:current mean train loss 1478.8648440584222
INFO:root:current train perplexity3.2099075317382812
INFO:root:current mean train loss 1479.3173198747515
INFO:root:current train perplexity3.2110753059387207
INFO:root:current mean train loss 1479.108784778577
INFO:root:current train perplexity3.212908983230591
INFO:root:current mean train loss 1479.590020427154
INFO:root:current train perplexity3.2134389877319336
INFO:root:current mean train loss 1480.381953007033
INFO:root:current train perplexity3.215017557144165

100%|██████████| 1/1 [05:22<00:00, 322.57s/it][A100%|██████████| 1/1 [05:22<00:00, 322.57s/it]
INFO:root:final mean train loss: 1479.9454065612392
INFO:root:final train perplexity: 3.2154784202575684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.77s/it][A100%|██████████| 1/1 [00:21<00:00, 21.77s/it]
INFO:root:eval mean loss: 2165.9075512175864
INFO:root:eval perplexity: 5.7700700759887695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.48s/it][A100%|██████████| 1/1 [00:21<00:00, 21.48s/it]
INFO:root:eval mean loss: 2682.6142690672095
INFO:root:eval perplexity: 9.074371337890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/66
 33%|███▎      | 66/200 [6:55:14<13:48:17, 370.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1470.8829578218006
INFO:root:current train perplexity3.1477746963500977
INFO:root:current mean train loss 1452.6532698702222
INFO:root:current train perplexity3.157109022140503
INFO:root:current mean train loss 1460.4857321346508
INFO:root:current train perplexity3.163656711578369
INFO:root:current mean train loss 1460.8690877658928
INFO:root:current train perplexity3.170046091079712
INFO:root:current mean train loss 1462.1376483400757
INFO:root:current train perplexity3.1674137115478516
INFO:root:current mean train loss 1462.5887451171875
INFO:root:current train perplexity3.175478935241699
INFO:root:current mean train loss 1462.2512419327445
INFO:root:current train perplexity3.1750049591064453
INFO:root:current mean train loss 1463.1111217398254
INFO:root:current train perplexity3.173966884613037
INFO:root:current mean train loss 1463.4395902124886
INFO:root:current train perplexity3.176023006439209
INFO:root:current mean train loss 1464.1203175895766
INFO:root:current train perplexity3.1753122806549072
INFO:root:current mean train loss 1464.824517768464
INFO:root:current train perplexity3.177586078643799
INFO:root:current mean train loss 1464.847252579484
INFO:root:current train perplexity3.17881441116333
INFO:root:current mean train loss 1465.9010137734504
INFO:root:current train perplexity3.1806540489196777
INFO:root:current mean train loss 1467.2095528659634
INFO:root:current train perplexity3.183962821960449
INFO:root:current mean train loss 1468.7615732277554
INFO:root:current train perplexity3.1866614818573
INFO:root:current mean train loss 1470.2555771124826
INFO:root:current train perplexity3.188955307006836
INFO:root:current mean train loss 1470.775554715815
INFO:root:current train perplexity3.1923646926879883
INFO:root:current mean train loss 1470.6745406865105
INFO:root:current train perplexity3.19221830368042
INFO:root:current mean train loss 1471.6968467247348
INFO:root:current train perplexity3.193021297454834
INFO:root:current mean train loss 1472.3195795697634
INFO:root:current train perplexity3.1943259239196777

100%|██████████| 1/1 [05:29<00:00, 329.65s/it][A100%|██████████| 1/1 [05:29<00:00, 329.65s/it]
INFO:root:final mean train loss: 1471.791979044781
INFO:root:final train perplexity: 3.194854259490967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.67s/it][A100%|██████████| 1/1 [00:21<00:00, 21.67s/it]
INFO:root:eval mean loss: 2176.0769402253713
INFO:root:eval perplexity: 5.817749500274658
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.82s/it][A100%|██████████| 1/1 [00:20<00:00, 20.82s/it]
INFO:root:eval mean loss: 2696.560006216063
INFO:root:eval perplexity: 9.179010391235352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/67
 34%|███▎      | 67/200 [7:01:28<13:44:10, 371.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.4004259611431
INFO:root:current train perplexity3.0750248432159424
INFO:root:current mean train loss 1452.559037802876
INFO:root:current train perplexity3.134275436401367
INFO:root:current mean train loss 1456.58898669331
INFO:root:current train perplexity3.150383234024048
INFO:root:current mean train loss 1454.703945543639
INFO:root:current train perplexity3.150062084197998
INFO:root:current mean train loss 1457.9050635768942
INFO:root:current train perplexity3.153484582901001
INFO:root:current mean train loss 1457.88632553838
INFO:root:current train perplexity3.155158042907715
INFO:root:current mean train loss 1457.6609324870813
INFO:root:current train perplexity3.157182455062866
INFO:root:current mean train loss 1457.573944505314
INFO:root:current train perplexity3.1602377891540527
INFO:root:current mean train loss 1457.3554098998732
INFO:root:current train perplexity3.1597001552581787
INFO:root:current mean train loss 1458.8678767157517
INFO:root:current train perplexity3.161526918411255
INFO:root:current mean train loss 1459.2390979921197
INFO:root:current train perplexity3.162167549133301
INFO:root:current mean train loss 1461.2646977805086
INFO:root:current train perplexity3.165438413619995
INFO:root:current mean train loss 1462.0440478594508
INFO:root:current train perplexity3.166821241378784
INFO:root:current mean train loss 1462.0717684941085
INFO:root:current train perplexity3.1660029888153076
INFO:root:current mean train loss 1462.8162773885713
INFO:root:current train perplexity3.167992115020752
INFO:root:current mean train loss 1463.4658389643357
INFO:root:current train perplexity3.1703507900238037
INFO:root:current mean train loss 1462.9101941827162
INFO:root:current train perplexity3.1705257892608643
INFO:root:current mean train loss 1463.1719579488142
INFO:root:current train perplexity3.172484874725342
INFO:root:current mean train loss 1463.8832832477558
INFO:root:current train perplexity3.174177646636963
INFO:root:current mean train loss 1464.030728587179
INFO:root:current train perplexity3.1743571758270264

100%|██████████| 1/1 [05:20<00:00, 320.13s/it][A100%|██████████| 1/1 [05:20<00:00, 320.13s/it]
INFO:root:final mean train loss: 1464.0021320752767
INFO:root:final train perplexity: 3.1752734184265137
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:20<00:00, 20.44s/it][A100%|██████████| 1/1 [00:20<00:00, 20.44s/it]
INFO:root:eval mean loss: 2178.8934663813166
INFO:root:eval perplexity: 5.831024646759033
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:18<00:00, 18.84s/it][A100%|██████████| 1/1 [00:18<00:00, 18.84s/it]
INFO:root:eval mean loss: 2701.3514806869184
INFO:root:eval perplexity: 9.215237617492676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_roberta_final/68
 34%|███▍      | 68/200 [7:07:29<13:30:58, 368.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1444.3820334694603
INFO:root:current train perplexity3.109161376953125
slurmstepd: error: *** JOB 26260769 ON ga018 CANCELLED AT 2022-10-25T09:42:48 ***
