INFO:root:Output: small_topk_16
INFO:root:Steps per epochs:496
INFO:root:Total steps:99200
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.decoder.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 48964.097971906565
INFO:root:current train perplexity15500.318359375
INFO:root:current mean train loss 40971.04026970791
INFO:root:current train perplexity3178.055908203125
INFO:root:current mean train loss 35188.73652409072
INFO:root:current train perplexity1022.01416015625
INFO:root:current mean train loss 31213.709488565164
INFO:root:current train perplexity468.6083679199219


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:38<00:00, 338.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:38<00:00, 338.76s/it]
INFO:root:final mean train loss: 28434.027109453757
INFO:root:final train perplexity: 272.8776550292969
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.88s/it]
INFO:root:eval mean loss: 16552.433904738653
INFO:root:eval perplexity: 30.759868621826172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/1

  0%|          | 1/200 [06:25<21:18:36, 385.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16156.997395833334
INFO:root:current train perplexity23.79038429260254
INFO:root:current mean train loss 15613.193956689927
INFO:root:current train perplexity21.910411834716797
INFO:root:current mean train loss 15165.946616186884
INFO:root:current train perplexity19.99889373779297
INFO:root:current mean train loss 14824.884888098184
INFO:root:current train perplexity18.5963134765625
INFO:root:current mean train loss 14519.741097045595
INFO:root:current train perplexity17.498319625854492


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.68s/it]
INFO:root:final mean train loss: 14259.927915511593
INFO:root:final train perplexity: 16.659440994262695
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.22s/it]
INFO:root:eval mean loss: 14079.49432954334
INFO:root:eval perplexity: 18.436525344848633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/2

  1%|          | 2/200 [13:01<21:32:14, 391.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 13234.133370535714
INFO:root:current train perplexity13.58291244506836
INFO:root:current mean train loss 12861.948871933411
INFO:root:current train perplexity12.653698921203613
INFO:root:current mean train loss 12733.972009926027
INFO:root:current train perplexity12.30419921875
INFO:root:current mean train loss 12599.371685413274
INFO:root:current train perplexity12.014912605285645
INFO:root:current mean train loss 12499.664168074325
INFO:root:current train perplexity11.74820327758789


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:56<00:00, 356.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:56<00:00, 356.77s/it]
INFO:root:final mean train loss: 12387.433300387474
INFO:root:final train perplexity: 11.514423370361328
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.25s/it]
INFO:root:eval mean loss: 13308.298081170946
INFO:root:eval perplexity: 15.71636962890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/3

  2%|â–         | 3/200 [20:25<22:44:18, 415.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 12058.366654829546
INFO:root:current train perplexity10.497552871704102
INFO:root:current mean train loss 11825.793654983108
INFO:root:current train perplexity10.250619888305664
INFO:root:current mean train loss 11734.056372186018
INFO:root:current train perplexity10.097448348999023
INFO:root:current mean train loss 11663.05126482114
INFO:root:current train perplexity9.960546493530273
INFO:root:current mean train loss 11605.454892791971
INFO:root:current train perplexity9.84518051147461


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:45<00:00, 405.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:45<00:00, 405.37s/it]
INFO:root:final mean train loss: 11550.061184790826
INFO:root:final train perplexity: 9.761212348937988
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.91s/it]
INFO:root:eval mean loss: 12872.278663271949
INFO:root:eval perplexity: 14.360061645507812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/4

  2%|â–         | 4/200 [28:33<24:10:57, 444.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11390.051627604167
INFO:root:current train perplexity9.217915534973145
INFO:root:current mean train loss 11214.01441066576
INFO:root:current train perplexity9.127667427062988
INFO:root:current mean train loss 11169.940075399709
INFO:root:current train perplexity9.016905784606934
INFO:root:current mean train loss 11124.818895709326
INFO:root:current train perplexity8.939814567565918
INFO:root:current mean train loss 11072.117815794427
INFO:root:current train perplexity8.876814842224121


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.29s/it]
INFO:root:final mean train loss: 11031.60297812185
INFO:root:final train perplexity: 8.81225299835205
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.76s/it]
INFO:root:eval mean loss: 12600.371117001489
INFO:root:eval perplexity: 13.574165344238281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/5

  2%|â–Ž         | 5/200 [36:41<24:55:16, 460.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10803.852898848685
INFO:root:current train perplexity8.33862018585205
INFO:root:current mean train loss 10817.35604319853
INFO:root:current train perplexity8.435555458068848
INFO:root:current mean train loss 10771.622908640125
INFO:root:current train perplexity8.34208869934082
INFO:root:current mean train loss 10723.274603864616
INFO:root:current train perplexity8.278130531311035
INFO:root:current mean train loss 10710.864987321002
INFO:root:current train perplexity8.24574089050293


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.67s/it]
INFO:root:final mean train loss: 10671.294163857738
INFO:root:final train perplexity: 8.207653045654297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.52s/it]
INFO:root:eval mean loss: 12398.259323846727
INFO:root:eval perplexity: 13.017997741699219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/6

  3%|â–Ž         | 6/200 [44:50<25:19:08, 469.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10408.52492357337
INFO:root:current train perplexity7.837174892425537
INFO:root:current mean train loss 10466.922327553353
INFO:root:current train perplexity7.888863563537598
INFO:root:current mean train loss 10450.786719625841
INFO:root:current train perplexity7.854007720947266
INFO:root:current mean train loss 10425.245773268189
INFO:root:current train perplexity7.813759803771973
INFO:root:current mean train loss 10407.28073286052
INFO:root:current train perplexity7.783395290374756


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:44<00:00, 404.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:44<00:00, 404.23s/it]
INFO:root:final mean train loss: 10389.159498645413
INFO:root:final train perplexity: 7.763336658477783
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.67s/it]
INFO:root:eval mean loss: 12240.839925130209
INFO:root:eval perplexity: 12.600652694702148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/7

  4%|â–Ž         | 7/200 [52:54<25:25:52, 474.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10167.807472511575
INFO:root:current train perplexity7.52996826171875
INFO:root:current mean train loss 10248.25
INFO:root:current train perplexity7.533829212188721
INFO:root:current mean train loss 10231.40604780424
INFO:root:current train perplexity7.499521255493164
INFO:root:current mean train loss 10210.406939865252
INFO:root:current train perplexity7.4701337814331055
INFO:root:current mean train loss 10192.819059206675
INFO:root:current train perplexity7.441414833068848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:35<00:00, 395.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:35<00:00, 395.34s/it]
INFO:root:final mean train loss: 10163.582123787173
INFO:root:final train perplexity: 7.42545223236084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.51s/it]
INFO:root:eval mean loss: 12118.27786109561
INFO:root:eval perplexity: 12.285002708435059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/8

  4%|â–         | 8/200 [1:00:43<25:12:51, 472.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10044.604618195564
INFO:root:current train perplexity7.186198711395264
INFO:root:current mean train loss 10007.274928435114
INFO:root:current train perplexity7.207212448120117
INFO:root:current mean train loss 10034.480650534362
INFO:root:current train perplexity7.211694717407227
INFO:root:current mean train loss 10005.051902379155
INFO:root:current train perplexity7.187260150909424
INFO:root:current mean train loss 9987.675522948086
INFO:root:current train perplexity7.161719799041748


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:42<00:00, 402.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:42<00:00, 402.84s/it]
INFO:root:final mean train loss: 9974.302494172127
INFO:root:final train perplexity: 7.153313636779785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it]
INFO:root:eval mean loss: 12006.251418340773
INFO:root:eval perplexity: 12.003408432006836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/9

  4%|â–         | 9/200 [1:08:42<25:11:06, 474.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9823.872600446428
INFO:root:current train perplexity6.909279823303223
INFO:root:current mean train loss 9882.002944155092
INFO:root:current train perplexity6.993539810180664
INFO:root:current mean train loss 9860.040795378989
INFO:root:current train perplexity6.979901313781738
INFO:root:current mean train loss 9858.906375349814
INFO:root:current train perplexity6.974776744842529
INFO:root:current mean train loss 9837.831272449712
INFO:root:current train perplexity6.948195934295654


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:47<00:00, 407.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:47<00:00, 407.07s/it]
INFO:root:final mean train loss: 9818.586583291331
INFO:root:final train perplexity: 6.936923027038574
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.62s/it]
INFO:root:eval mean loss: 11905.073506673178
INFO:root:eval perplexity: 11.75463581085205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/10

  5%|â–Œ         | 10/200 [1:16:43<25:09:50, 476.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9800.089017427885
INFO:root:current train perplexity6.81099271774292
INFO:root:current mean train loss 9731.904233644334
INFO:root:current train perplexity6.773587703704834
INFO:root:current mean train loss 9727.451110584467
INFO:root:current train perplexity6.774196624755859
INFO:root:current mean train loss 9707.550104281896
INFO:root:current train perplexity6.764307975769043
INFO:root:current mean train loss 9689.765642796128
INFO:root:current train perplexity6.752921104431152


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:41<00:00, 401.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:41<00:00, 401.54s/it]
INFO:root:final mean train loss: 9680.630221459174
INFO:root:final train perplexity: 6.750687599182129
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.54s/it]
INFO:root:eval mean loss: 11828.381891159785
INFO:root:eval perplexity: 11.56950855255127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/11

  6%|â–Œ         | 11/200 [1:24:41<25:02:12, 476.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9586.011537063954
INFO:root:current train perplexity6.5992655754089355
INFO:root:current mean train loss 9595.474069875438
INFO:root:current train perplexity6.600552082061768
INFO:root:current mean train loss 9584.529602301955
INFO:root:current train perplexity6.608582019805908
INFO:root:current mean train loss 9573.243440233236
INFO:root:current train perplexity6.59832763671875
INFO:root:current mean train loss 9565.919298021303
INFO:root:current train perplexity6.589978218078613


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.67s/it]
INFO:root:final mean train loss: 9556.299145114037
INFO:root:final train perplexity: 6.587132453918457
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.40s/it]
INFO:root:eval mean loss: 11769.97962297712
INFO:root:eval perplexity: 11.430489540100098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/12

  6%|â–Œ         | 12/200 [1:32:57<25:12:42, 482.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9492.802194148937
INFO:root:current train perplexity6.483933448791504
INFO:root:current mean train loss 9461.056056016156
INFO:root:current train perplexity6.481729984283447
INFO:root:current mean train loss 9456.41926556174
INFO:root:current train perplexity6.461884021759033
INFO:root:current mean train loss 9439.488945425072
INFO:root:current train perplexity6.451549053192139
INFO:root:current mean train loss 9448.753270501258
INFO:root:current train perplexity6.4450883865356445


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.95s/it]
INFO:root:final mean train loss: 9446.17737505513
INFO:root:final train perplexity: 6.445582866668701
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 72.00s/it]
INFO:root:eval mean loss: 11706.88134765625
INFO:root:eval perplexity: 11.2821683883667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/13

  6%|â–‹         | 13/200 [1:41:14<25:18:01, 487.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9391.102175245098
INFO:root:current train perplexity6.326007843017578
INFO:root:current mean train loss 9388.244560999587
INFO:root:current train perplexity6.336779594421387
INFO:root:current mean train loss 9379.275522908367
INFO:root:current train perplexity6.341150283813477
INFO:root:current mean train loss 9361.135650373932
INFO:root:current train perplexity6.330460071563721
INFO:root:current mean train loss 9355.616184953575
INFO:root:current train perplexity6.322333335876465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.73s/it]
INFO:root:final mean train loss: 9346.765709661668
INFO:root:final train perplexity: 6.3204145431518555
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.37s/it]
INFO:root:eval mean loss: 11655.018505278087
INFO:root:eval perplexity: 11.161702156066895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/14

  7%|â–‹         | 14/200 [1:49:44<25:31:22, 493.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9264.065500710227
INFO:root:current train perplexity6.241689682006836
INFO:root:current mean train loss 9245.102866683468
INFO:root:current train perplexity6.224870204925537
INFO:root:current mean train loss 9244.195124846814
INFO:root:current train perplexity6.218469619750977
INFO:root:current mean train loss 9260.028570642606
INFO:root:current train perplexity6.218421459197998
INFO:root:current mean train loss 9267.748624227335
INFO:root:current train perplexity6.214879989624023


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.15s/it]
INFO:root:final mean train loss: 9258.471662952054
INFO:root:final train perplexity: 6.211283206939697
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.28s/it]
INFO:root:eval mean loss: 11617.575230189732
INFO:root:eval perplexity: 11.07552719116211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/15

  8%|â–Š         | 15/200 [1:58:06<25:31:01, 496.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9185.32072629767
INFO:root:current train perplexity6.101227760314941
INFO:root:current mean train loss 9175.243200913916
INFO:root:current train perplexity6.112740516662598
INFO:root:current mean train loss 9178.622639659749
INFO:root:current train perplexity6.113152980804443
INFO:root:current mean train loss 9180.596429426358
INFO:root:current train perplexity6.113399505615234
INFO:root:current mean train loss 9177.814814814816
INFO:root:current train perplexity6.1089067459106445


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.62s/it]
INFO:root:final mean train loss: 9177.48249078566
INFO:root:final train perplexity: 6.1128387451171875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.18s/it]
INFO:root:eval mean loss: 11570.23374720982
INFO:root:eval perplexity: 10.967525482177734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/16

  8%|â–Š         | 16/200 [2:06:24<25:23:37, 496.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9074.124364459325
INFO:root:current train perplexity6.044590473175049
INFO:root:current mean train loss 9097.138893548696
INFO:root:current train perplexity6.03114128112793
INFO:root:current mean train loss 9104.429535260218
INFO:root:current train perplexity6.022640705108643
INFO:root:current mean train loss 9102.60170239325
INFO:root:current train perplexity6.024449348449707
INFO:root:current mean train loss 9105.04009601107
INFO:root:current train perplexity6.023140907287598


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.80s/it]
INFO:root:final mean train loss: 9105.540806924144
INFO:root:final train perplexity: 6.026700019836426
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.07s/it]
INFO:root:eval mean loss: 11533.486740838915
INFO:root:eval perplexity: 10.884420394897461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/17

  8%|â–Š         | 17/200 [2:14:43<25:17:34, 497.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9034.327585704292
INFO:root:current train perplexity5.93644905090332
INFO:root:current mean train loss 9031.814078873504
INFO:root:current train perplexity5.939525127410889
INFO:root:current mean train loss 9054.123785697566
INFO:root:current train perplexity5.949060916900635
INFO:root:current mean train loss 9040.90728510303
INFO:root:current train perplexity5.935447692871094
INFO:root:current mean train loss 9037.90918073307
INFO:root:current train perplexity5.943050384521484


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.49s/it]
INFO:root:final mean train loss: 9034.235466741746
INFO:root:final train perplexity: 5.9425225257873535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.78s/it]
INFO:root:eval mean loss: 11501.325422014508
INFO:root:eval perplexity: 10.812202453613281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/18

  9%|â–‰         | 18/200 [2:22:53<25:02:28, 495.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9009.06512709067
INFO:root:current train perplexity5.901984691619873
INFO:root:current mean train loss 8998.827339752375
INFO:root:current train perplexity5.894877910614014
INFO:root:current mean train loss 9002.170449795318
INFO:root:current train perplexity5.888675689697266
INFO:root:current mean train loss 8982.83040583937
INFO:root:current train perplexity5.872228145599365
INFO:root:current mean train loss 8970.811889389264
INFO:root:current train perplexity5.8642096519470215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.41s/it]
INFO:root:final mean train loss: 8968.983534289944
INFO:root:final train perplexity: 5.86652135848999
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.72s/it]
INFO:root:eval mean loss: 11468.247532435826
INFO:root:eval perplexity: 10.738422393798828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/19

 10%|â–‰         | 19/200 [2:31:07<24:53:14, 495.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8845.994661458333
INFO:root:current train perplexity5.720633029937744
INFO:root:current mean train loss 8897.025825892857
INFO:root:current train perplexity5.770057678222656
INFO:root:current mean train loss 8900.966154119318
INFO:root:current train perplexity5.777263164520264
INFO:root:current mean train loss 8916.794889322917
INFO:root:current train perplexity5.790259838104248
INFO:root:current mean train loss 8919.339768708882
INFO:root:current train perplexity5.7957444190979


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:42<00:00, 402.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:42<00:00, 402.69s/it]
INFO:root:final mean train loss: 8905.526758009388
INFO:root:final train perplexity: 5.793542385101318
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.08s/it]
INFO:root:eval mean loss: 11435.955159505209
INFO:root:eval perplexity: 10.666885375976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/20

 10%|â–ˆ         | 20/200 [2:39:06<24:30:19, 490.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8906.225054390823
INFO:root:current train perplexity5.738282680511475
INFO:root:current mean train loss 8884.779703321403
INFO:root:current train perplexity5.74815034866333
INFO:root:current mean train loss 8864.342736685148
INFO:root:current train perplexity5.7342119216918945
INFO:root:current mean train loss 8872.64256652993
INFO:root:current train perplexity5.741954326629639
INFO:root:current mean train loss 8860.387044950417
INFO:root:current train perplexity5.733064651489258


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.76s/it]
INFO:root:final mean train loss: 8850.008887506301
INFO:root:final train perplexity: 5.730439186096191
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.26s/it]
INFO:root:eval mean loss: 11418.79419235956
INFO:root:eval perplexity: 10.629064559936523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/21

 10%|â–ˆ         | 21/200 [2:47:23<24:27:54, 492.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8787.120652532003
INFO:root:current train perplexity5.69003963470459
INFO:root:current mean train loss 8789.311456732412
INFO:root:current train perplexity5.665461540222168
INFO:root:current mean train loss 8796.152059063052
INFO:root:current train perplexity5.663267135620117
INFO:root:current mean train loss 8802.741838181299
INFO:root:current train perplexity5.665134429931641
INFO:root:current mean train loss 8799.88874061853
INFO:root:current train perplexity5.666624546051025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.10s/it]
INFO:root:final mean train loss: 8796.318603515625
INFO:root:final train perplexity: 5.670068264007568
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.84s/it]
INFO:root:eval mean loss: 11382.117495582217
INFO:root:eval perplexity: 10.548674583435059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/22

 11%|â–ˆ         | 22/200 [2:55:27<24:13:14, 489.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8765.540695716594
INFO:root:current train perplexity5.630235195159912
INFO:root:current mean train loss 8743.928802327038
INFO:root:current train perplexity5.608933448791504
INFO:root:current mean train loss 8754.82842273247
INFO:root:current train perplexity5.616342067718506
INFO:root:current mean train loss 8757.04209690932
INFO:root:current train perplexity5.619343280792236
INFO:root:current mean train loss 8757.040518320071
INFO:root:current train perplexity5.615908145904541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.79s/it]
INFO:root:final mean train loss: 8746.366484611264
INFO:root:final train perplexity: 5.614470958709717
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.73s/it]
INFO:root:eval mean loss: 11372.177336193266
INFO:root:eval perplexity: 10.526991844177246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/23

 12%|â–ˆâ–        | 23/200 [3:03:38<24:05:23, 489.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8714.387781164149
INFO:root:current train perplexity5.551715850830078
INFO:root:current mean train loss 8721.414318144634
INFO:root:current train perplexity5.5588178634643555
INFO:root:current mean train loss 8706.101064151095
INFO:root:current train perplexity5.553840160369873
INFO:root:current mean train loss 8707.43787963555
INFO:root:current train perplexity5.5585551261901855
INFO:root:current mean train loss 8705.091428923752
INFO:root:current train perplexity5.561527729034424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.47s/it]
INFO:root:final mean train loss: 8698.29701282132
INFO:root:final train perplexity: 5.561482906341553
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.55s/it]
INFO:root:eval mean loss: 11338.289239792597
INFO:root:eval perplexity: 10.453409194946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/24

 12%|â–ˆâ–        | 24/200 [3:12:01<24:09:30, 494.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8630.294212582237
INFO:root:current train perplexity5.50071907043457
INFO:root:current mean train loss 8645.123687900641
INFO:root:current train perplexity5.509486675262451
INFO:root:current mean train loss 8655.291775357522
INFO:root:current train perplexity5.514618873596191
INFO:root:current mean train loss 8667.320492978639
INFO:root:current train perplexity5.51688814163208
INFO:root:current mean train loss 8659.821537642045
INFO:root:current train perplexity5.511957168579102


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.69s/it]
INFO:root:final mean train loss: 8652.476756434287
INFO:root:final train perplexity: 5.511441707611084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.96s/it]
INFO:root:eval mean loss: 11336.897856212798
INFO:root:eval perplexity: 10.450398445129395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/25

 12%|â–ˆâ–Ž        | 25/200 [3:20:24<24:08:21, 496.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8595.816850142046
INFO:root:current train perplexity5.46237325668335
INFO:root:current mean train loss 8627.737535332915
INFO:root:current train perplexity5.476609230041504
INFO:root:current mean train loss 8621.68175167224
INFO:root:current train perplexity5.465094089508057
INFO:root:current mean train loss 8625.751118518954
INFO:root:current train perplexity5.466156482696533


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.97s/it]
INFO:root:final mean train loss: 8611.637555522304
INFO:root:final train perplexity: 5.467219352722168
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.31s/it]
INFO:root:eval mean loss: 11316.239222935268
INFO:root:eval perplexity: 10.405807495117188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/26

 13%|â–ˆâ–Ž        | 26/200 [3:28:54<24:11:41, 500.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8598.356119791666
INFO:root:current train perplexity5.46267032623291
INFO:root:current mean train loss 8579.41929137591
INFO:root:current train perplexity5.439523696899414
INFO:root:current mean train loss 8596.244443696121
INFO:root:current train perplexity5.448707580566406
INFO:root:current mean train loss 8581.754842525268
INFO:root:current train perplexity5.427298069000244
INFO:root:current mean train loss 8579.857557575993
INFO:root:current train perplexity5.426950454711914


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.41s/it]
INFO:root:final mean train loss: 8569.714028635333
INFO:root:final train perplexity: 5.422192573547363
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.75s/it]
INFO:root:eval mean loss: 11302.848048618862
INFO:root:eval perplexity: 10.377004623413086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/27

 14%|â–ˆâ–Ž        | 27/200 [3:37:25<24:13:02, 503.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8600.9453125
INFO:root:current train perplexity5.353329658508301
INFO:root:current mean train loss 8497.672659900702
INFO:root:current train perplexity5.367002010345459
INFO:root:current mean train loss 8543.549181951992
INFO:root:current train perplexity5.393780708312988
INFO:root:current mean train loss 8539.305570223432
INFO:root:current train perplexity5.3834147453308105
INFO:root:current mean train loss 8545.538634204162
INFO:root:current train perplexity5.384163856506348


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.38s/it]
INFO:root:final mean train loss: 8535.260649650327
INFO:root:final train perplexity: 5.385465621948242
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.23s/it]
INFO:root:eval mean loss: 11282.239007859003
INFO:root:eval perplexity: 10.332829475402832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/28

 14%|â–ˆâ–        | 28/200 [3:45:51<24:06:11, 504.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8599.9326171875
INFO:root:current train perplexity5.3927741050720215
INFO:root:current mean train loss 8522.517903645834
INFO:root:current train perplexity5.356973171234131
INFO:root:current mean train loss 8524.894959363892
INFO:root:current train perplexity5.354813575744629
INFO:root:current mean train loss 8510.752036336917
INFO:root:current train perplexity5.34765100479126
INFO:root:current mean train loss 8510.181443411648
INFO:root:current train perplexity5.347520351409912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.33s/it]
INFO:root:final mean train loss: 8494.186749858241
INFO:root:final train perplexity: 5.342005252838135
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.29s/it]
INFO:root:eval mean loss: 11279.543817429316
INFO:root:eval perplexity: 10.327066421508789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/29

 14%|â–ˆâ–        | 29/200 [3:54:11<23:54:05, 503.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8498.228841145834
INFO:root:current train perplexity5.2948431968688965
INFO:root:current mean train loss 8515.289058254077
INFO:root:current train perplexity5.3202009201049805
INFO:root:current mean train loss 8507.244651617006
INFO:root:current train perplexity5.309787750244141
INFO:root:current mean train loss 8485.312805369544
INFO:root:current train perplexity5.304183006286621
INFO:root:current mean train loss 8478.183432558359
INFO:root:current train perplexity5.303412437438965


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.66s/it]
INFO:root:final mean train loss: 8459.05141719695
INFO:root:final train perplexity: 5.305109024047852
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.52s/it]
INFO:root:eval mean loss: 11259.86333937872
INFO:root:eval perplexity: 10.285083770751953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/30

 15%|â–ˆâ–Œ        | 30/200 [4:02:37<23:47:39, 503.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8442.961759868422
INFO:root:current train perplexity5.314444541931152
INFO:root:current mean train loss 8438.029268152573
INFO:root:current train perplexity5.2738447189331055
INFO:root:current mean train loss 8436.455446008134
INFO:root:current train perplexity5.265486240386963
INFO:root:current mean train loss 8431.243963068182
INFO:root:current train perplexity5.269250869750977
INFO:root:current mean train loss 8435.87081173553
INFO:root:current train perplexity5.273396015167236


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.42s/it]
INFO:root:final mean train loss: 8425.466137301537
INFO:root:final train perplexity: 5.270078182220459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.02s/it]
INFO:root:eval mean loss: 11250.990792410714
INFO:root:eval perplexity: 10.266212463378906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/31

 16%|â–ˆâ–Œ        | 31/200 [4:11:09<23:45:51, 506.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8374.750700577446
INFO:root:current train perplexity5.214347839355469
INFO:root:current mean train loss 8362.436436102642
INFO:root:current train perplexity5.194343090057373
INFO:root:current mean train loss 8395.508883215387
INFO:root:current train perplexity5.219892978668213
INFO:root:current mean train loss 8402.543957406153
INFO:root:current train perplexity5.234005451202393
INFO:root:current mean train loss 8397.35213620161
INFO:root:current train perplexity5.237377166748047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.14s/it]
INFO:root:final mean train loss: 8393.35622282951
INFO:root:final train perplexity: 5.2368011474609375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.23s/it]
INFO:root:eval mean loss: 11242.220406668526
INFO:root:eval perplexity: 10.247590065002441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/32

 16%|â–ˆâ–Œ        | 32/200 [4:19:47<23:47:24, 509.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8376.095757378473
INFO:root:current train perplexity5.148077011108398
INFO:root:current mean train loss 8357.624550166092
INFO:root:current train perplexity5.18610143661499
INFO:root:current mean train loss 8360.47036541506
INFO:root:current train perplexity5.2029948234558105
INFO:root:current mean train loss 8371.673620568139
INFO:root:current train perplexity5.205631256103516
INFO:root:current mean train loss 8373.964451524078
INFO:root:current train perplexity5.204326629638672


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.50s/it]
INFO:root:final mean train loss: 8363.778451242755
INFO:root:final train perplexity: 5.20633602142334
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.85s/it]
INFO:root:eval mean loss: 11245.871404738653
INFO:root:eval perplexity: 10.255337715148926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/33

 16%|â–ˆâ–‹        | 33/200 [4:28:13<23:35:49, 508.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8245.860005040322
INFO:root:current train perplexity5.128413200378418
INFO:root:current mean train loss 8289.115991024571
INFO:root:current train perplexity5.164771556854248
INFO:root:current mean train loss 8330.651618726326
INFO:root:current train perplexity5.1698760986328125
INFO:root:current mean train loss 8326.945588356779
INFO:root:current train perplexity5.168221473693848
INFO:root:current mean train loss 8324.43055190509
INFO:root:current train perplexity5.163206577301025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.37s/it]
INFO:root:final mean train loss: 8331.985809326172
INFO:root:final train perplexity: 5.173786163330078
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.03s/it]
INFO:root:eval mean loss: 11214.66167922247
INFO:root:eval perplexity: 10.189302444458008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/34

 17%|â–ˆâ–‹        | 34/200 [4:36:36<23:22:31, 506.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8394.027329799108
INFO:root:current train perplexity5.167179107666016
INFO:root:current mean train loss 8298.81212745949
INFO:root:current train perplexity5.138471603393555
INFO:root:current mean train loss 8297.26551903258
INFO:root:current train perplexity5.128075122833252
INFO:root:current mean train loss 8311.651972073227
INFO:root:current train perplexity5.143784046173096
INFO:root:current mean train loss 8316.000957480244
INFO:root:current train perplexity5.147003173828125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.85s/it]
INFO:root:final mean train loss: 8306.056802072833
INFO:root:final train perplexity: 5.147390365600586
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.71s/it]
INFO:root:eval mean loss: 11215.282511393229
INFO:root:eval perplexity: 10.1906099319458
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/35

 18%|â–ˆâ–Š        | 35/200 [4:44:47<23:01:02, 502.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8350.167042267629
INFO:root:current train perplexity5.156065464019775
INFO:root:current mean train loss 8305.943148606115
INFO:root:current train perplexity5.127892971038818
INFO:root:current mean train loss 8294.058009446915
INFO:root:current train perplexity5.125190258026123
INFO:root:current mean train loss 8299.399077018805
INFO:root:current train perplexity5.125463962554932
INFO:root:current mean train loss 8285.889450455581
INFO:root:current train perplexity5.118236541748047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.57s/it]
INFO:root:final mean train loss: 8277.315915015435
INFO:root:final train perplexity: 5.118289947509766
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.93s/it]
INFO:root:eval mean loss: 11218.9812273298
INFO:root:eval perplexity: 10.198416709899902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/36

 18%|â–ˆâ–Š        | 36/200 [4:53:15<22:57:54, 504.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8183.904682957849
INFO:root:current train perplexity5.0620808601379395
INFO:root:current mean train loss 8229.461760407561
INFO:root:current train perplexity5.0694756507873535
INFO:root:current mean train loss 8232.998651700746
INFO:root:current train perplexity5.0833306312561035
INFO:root:current mean train loss 8243.85512567192
INFO:root:current train perplexity5.087116718292236
INFO:root:current mean train loss 8250.912086228485
INFO:root:current train perplexity5.0853986740112305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.80s/it]
INFO:root:final mean train loss: 8248.4107390373
INFO:root:final train perplexity: 5.089189052581787
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.85s/it]
INFO:root:eval mean loss: 11201.744413829985
INFO:root:eval perplexity: 10.162094116210938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/37

 18%|â–ˆâ–Š        | 37/200 [5:01:42<22:51:27, 504.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8187.95038231383
INFO:root:current train perplexity5.0274481773376465
INFO:root:current mean train loss 8201.541882573341
INFO:root:current train perplexity5.045963287353516
INFO:root:current mean train loss 8198.987474696356
INFO:root:current train perplexity5.053447723388672
INFO:root:current mean train loss 8209.993403278098
INFO:root:current train perplexity5.056516170501709
INFO:root:current mean train loss 8233.108676987207
INFO:root:current train perplexity5.067136764526367


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.97s/it]
INFO:root:final mean train loss: 8225.629624889743
INFO:root:final train perplexity: 5.066368579864502
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.55s/it]
INFO:root:eval mean loss: 11198.863042922247
INFO:root:eval perplexity: 10.156034469604492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/38

 19%|â–ˆâ–‰        | 38/200 [5:10:04<22:41:01, 504.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8221.925714231005
INFO:root:current train perplexity5.039418697357178
INFO:root:current mean train loss 8201.337337670737
INFO:root:current train perplexity5.025615692138672
INFO:root:current mean train loss 8188.480457077938
INFO:root:current train perplexity5.014711856842041
INFO:root:current mean train loss 8196.394020710915
INFO:root:current train perplexity5.029810428619385
INFO:root:current mean train loss 8205.27742061911
INFO:root:current train perplexity5.034709930419922


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.32s/it]
INFO:root:final mean train loss: 8196.94146039409
INFO:root:final train perplexity: 5.037778377532959
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.15s/it]
INFO:root:eval mean loss: 11203.950939360118
INFO:root:eval perplexity: 10.16673755645752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/39

 20%|â–ˆâ–‰        | 39/200 [5:18:27<22:31:15, 503.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8134.287997159091
INFO:root:current train perplexity4.9780049324035645
INFO:root:current mean train loss 8162.667726184476
INFO:root:current train perplexity5.006082534790039
INFO:root:current mean train loss 8178.072376685049
INFO:root:current train perplexity5.016112804412842
INFO:root:current mean train loss 8175.6686413402285
INFO:root:current train perplexity5.014200210571289
INFO:root:current mean train loss 8180.118665221497
INFO:root:current train perplexity5.01515007019043


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.85s/it]
INFO:root:final mean train loss: 8175.369708645729
INFO:root:final train perplexity: 5.01638650894165
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.62s/it]
INFO:root:eval mean loss: 11196.470924014136
INFO:root:eval perplexity: 10.151005744934082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/40

 20%|â–ˆâ–ˆ        | 40/200 [5:26:54<22:25:56, 504.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8155.675003310382
INFO:root:current train perplexity5.000186920166016
INFO:root:current mean train loss 8164.089438384434
INFO:root:current train perplexity4.994487285614014
INFO:root:current mean train loss 8170.592030646718
INFO:root:current train perplexity4.998639106750488
INFO:root:current mean train loss 8176.234365479196
INFO:root:current train perplexity5.004012584686279
INFO:root:current mean train loss 8159.114910981753
INFO:root:current train perplexity4.995002269744873


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.42s/it]
INFO:root:final mean train loss: 8151.452914330267
INFO:root:final train perplexity: 4.992775917053223
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.86s/it]
INFO:root:eval mean loss: 11193.192690894717
INFO:root:eval perplexity: 10.144120216369629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/41

 20%|â–ˆâ–ˆ        | 41/200 [5:35:22<22:19:59, 505.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8089.855515252976
INFO:root:current train perplexity4.935859680175781
INFO:root:current mean train loss 8111.604324434433
INFO:root:current train perplexity4.950479984283447
INFO:root:current mean train loss 8113.730511451402
INFO:root:current train perplexity4.952798366546631
INFO:root:current mean train loss 8128.38630310563
INFO:root:current train perplexity4.960076332092285
INFO:root:current mean train loss 8137.46517384078
INFO:root:current train perplexity4.9694600105285645


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.14s/it]
INFO:root:final mean train loss: 8128.8548583984375
INFO:root:final train perplexity: 4.970569133758545
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.66s/it]
INFO:root:eval mean loss: 11184.996288481212
INFO:root:eval perplexity: 10.126927375793457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/42

 21%|â–ˆâ–ˆ        | 42/200 [5:43:45<22:09:25, 504.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8053.400018948228
INFO:root:current train perplexity4.939074516296387
INFO:root:current mean train loss 8103.838139151385
INFO:root:current train perplexity4.947035312652588
INFO:root:current mean train loss 8113.56784183345
INFO:root:current train perplexity4.948008060455322
INFO:root:current mean train loss 8119.488160177538
INFO:root:current train perplexity4.951529026031494
INFO:root:current mean train loss 8111.722543328426
INFO:root:current train perplexity4.947820663452148


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.21s/it]
INFO:root:final mean train loss: 8109.0218141617315
INFO:root:final train perplexity: 4.951159954071045
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.94s/it]
INFO:root:eval mean loss: 11180.76677013579
INFO:root:eval perplexity: 10.118064880371094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/43

 22%|â–ˆâ–ˆâ–       | 43/200 [5:52:45<22:28:38, 515.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8139.977133307659
INFO:root:current train perplexity4.953416347503662
INFO:root:current mean train loss 8106.750882332785
INFO:root:current train perplexity4.9269914627075195
INFO:root:current mean train loss 8106.3812791887685
INFO:root:current train perplexity4.928269386291504
INFO:root:current mean train loss 8094.17626953125
INFO:root:current train perplexity4.925719738006592
INFO:root:current mean train loss 8092.658663415605
INFO:root:current train perplexity4.928952217102051


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:13<00:00, 433.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:13<00:00, 433.45s/it]
INFO:root:final mean train loss: 8085.987394271358
INFO:root:final train perplexity: 4.928713321685791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.03s/it]
INFO:root:eval mean loss: 11185.93747093564
INFO:root:eval perplexity: 10.128900527954102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:01:34<22:31:02, 519.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8053.866002604167
INFO:root:current train perplexity4.8826985359191895
INFO:root:current mean train loss 8053.408775111607
INFO:root:current train perplexity4.892576694488525
INFO:root:current mean train loss 8073.802278053978
INFO:root:current train perplexity4.903355121612549
INFO:root:current mean train loss 8067.611946614584
INFO:root:current train perplexity4.903046607971191
INFO:root:current mean train loss 8068.742797080592
INFO:root:current train perplexity4.905452251434326


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.83s/it]
INFO:root:final mean train loss: 8064.545926001764
INFO:root:final train perplexity: 4.9079108238220215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.42s/it]
INFO:root:eval mean loss: 11178.078363327753
INFO:root:eval perplexity: 10.112436294555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:10:27<22:32:15, 523.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8057.7502472310125
INFO:root:current train perplexity4.89154577255249
INFO:root:current mean train loss 8045.644910418121
INFO:root:current train perplexity4.8887248039245605
INFO:root:current mean train loss 8044.331065188172
INFO:root:current train perplexity4.886575222015381
INFO:root:current mean train loss 8060.041411145696
INFO:root:current train perplexity4.887068748474121
INFO:root:current mean train loss 8050.578449161665
INFO:root:current train perplexity4.887265205383301


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.42s/it]
INFO:root:final mean train loss: 8042.684076124622
INFO:root:final train perplexity: 4.8867902755737305
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.15s/it]
INFO:root:eval mean loss: 11177.74896530878
INFO:root:eval perplexity: 10.111746788024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:19:27<22:36:27, 528.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8044.199442300452
INFO:root:current train perplexity4.850622653961182
INFO:root:current mean train loss 8038.4792387081625
INFO:root:current train perplexity4.864345073699951
INFO:root:current mean train loss 8043.456182365283
INFO:root:current train perplexity4.8666157722473145
INFO:root:current mean train loss 8036.189829216302
INFO:root:current train perplexity4.869735240936279
INFO:root:current mean train loss 8031.079171316965
INFO:root:current train perplexity4.868518829345703


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.36s/it]
INFO:root:final mean train loss: 8023.188934326172
INFO:root:final train perplexity: 4.868033409118652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.98s/it]
INFO:root:eval mean loss: 11174.281436011905
INFO:root:eval perplexity: 10.104490280151367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [6:28:28<22:37:17, 532.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8001.134468166308
INFO:root:current train perplexity4.842365741729736
INFO:root:current mean train loss 8012.832527364639
INFO:root:current train perplexity4.849287033081055
INFO:root:current mean train loss 8012.666095587435
INFO:root:current train perplexity4.849977970123291
INFO:root:current mean train loss 8007.368075742894
INFO:root:current train perplexity4.852099418640137
INFO:root:current mean train loss 8010.644603439425
INFO:root:current train perplexity4.851496696472168


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.42s/it]
INFO:root:final mean train loss: 8004.966651178175
INFO:root:final train perplexity: 4.8505659103393555
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.88s/it]
INFO:root:eval mean loss: 11177.369936988467
INFO:root:eval perplexity: 10.110954284667969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/48

 24%|â–ˆâ–ˆâ–       | 48/200 [6:36:50<22:05:41, 523.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8016.922454498626
INFO:root:current train perplexity4.817607879638672
INFO:root:current mean train loss 7975.209114753763
INFO:root:current train perplexity4.8155293464660645
INFO:root:current mean train loss 7986.57658464884
INFO:root:current train perplexity4.816440582275391
INFO:root:current mean train loss 7985.819819223545
INFO:root:current train perplexity4.824657440185547
INFO:root:current mean train loss 7992.106381666878
INFO:root:current train perplexity4.833557605743408


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.88s/it]
INFO:root:final mean train loss: 7987.69241234564
INFO:root:final train perplexity: 4.834065914154053
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.03s/it]
INFO:root:eval mean loss: 11168.280590239025
INFO:root:eval perplexity: 10.091949462890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/49

 24%|â–ˆâ–ˆâ–       | 49/200 [6:45:21<21:47:40, 519.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7974.7655222039475
INFO:root:current train perplexity4.791229724884033
INFO:root:current mean train loss 7962.650065104167
INFO:root:current train perplexity4.803700923919678
INFO:root:current mean train loss 7968.631000066208
INFO:root:current train perplexity4.808919906616211
INFO:root:current mean train loss 7968.332570213607
INFO:root:current train perplexity4.8081793785095215
INFO:root:current mean train loss 7972.037538470644
INFO:root:current train perplexity4.81351900100708


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.59s/it]
INFO:root:final mean train loss: 7966.585580149004
INFO:root:final train perplexity: 4.813980579376221
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.82s/it]
INFO:root:eval mean loss: 11171.5954851423
INFO:root:eval perplexity: 10.098876953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [6:53:39<21:22:40, 513.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7941.915132773043
INFO:root:current train perplexity4.769425868988037
INFO:root:current mean train loss 7955.113946195823
INFO:root:current train perplexity4.784850120544434
INFO:root:current mean train loss 7957.817828634511
INFO:root:current train perplexity4.800226211547852
INFO:root:current mean train loss 7945.075338492717
INFO:root:current train perplexity4.796389579772949


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.26s/it]
INFO:root:final mean train loss: 7950.525289227886
INFO:root:final train perplexity: 4.798753261566162
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.49s/it]
INFO:root:eval mean loss: 11161.77796282087
INFO:root:eval perplexity: 10.078373908996582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:02:02<21:06:38, 510.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7787.24853515625
INFO:root:current train perplexity4.7013654708862305
INFO:root:current mean train loss 7932.918073043083
INFO:root:current train perplexity4.773114204406738
INFO:root:current mean train loss 7956.223096424723
INFO:root:current train perplexity4.782307147979736
INFO:root:current mean train loss 7939.2837413624175
INFO:root:current train perplexity4.777717590332031
INFO:root:current mean train loss 7937.69094220107
INFO:root:current train perplexity4.782237529754639


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:12<00:00, 432.90s/it]
INFO:root:final mean train loss: 7932.789543890184
INFO:root:final train perplexity: 4.781993865966797
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.14s/it]
INFO:root:eval mean loss: 11163.977768670946
INFO:root:eval perplexity: 10.082963943481445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:10:32<20:57:50, 509.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7943.962681361607
INFO:root:current train perplexity4.848811149597168
INFO:root:current mean train loss 7883.10851708528
INFO:root:current train perplexity4.741107940673828
INFO:root:current mean train loss 7896.020078502415
INFO:root:current train perplexity4.746600151062012
INFO:root:current mean train loss 7896.198227873066
INFO:root:current train perplexity4.755767822265625
INFO:root:current mean train loss 7916.067012102657
INFO:root:current train perplexity4.760671138763428


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.28s/it]
INFO:root:final mean train loss: 7912.054359682144
INFO:root:final train perplexity: 4.7624735832214355
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.07s/it]
INFO:root:eval mean loss: 11163.085286458334
INFO:root:eval perplexity: 10.081101417541504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [7:18:46<20:37:55, 505.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7889.885786576705
INFO:root:current train perplexity4.807939052581787
INFO:root:current mean train loss 7890.619646501971
INFO:root:current train perplexity4.751547336578369
INFO:root:current mean train loss 7895.799360374704
INFO:root:current train perplexity4.750866889953613
INFO:root:current mean train loss 7912.589933242062
INFO:root:current train perplexity4.756525039672852
INFO:root:current mean train loss 7905.409134542275
INFO:root:current train perplexity4.750878810882568


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.98s/it]
INFO:root:final mean train loss: 7896.644699588899
INFO:root:final train perplexity: 4.748018741607666
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.92s/it]
INFO:root:eval mean loss: 11161.103474934896
INFO:root:eval perplexity: 10.07696533203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [7:26:59<20:20:11, 501.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7874.949869791667
INFO:root:current train perplexity4.765663146972656
INFO:root:current mean train loss 7890.628825577445
INFO:root:current train perplexity4.736238956451416
INFO:root:current mean train loss 7876.894801507994
INFO:root:current train perplexity4.732818126678467
INFO:root:current mean train loss 7875.201671006945
INFO:root:current train perplexity4.7371697425842285
INFO:root:current mean train loss 7880.405302852033
INFO:root:current train perplexity4.735288143157959


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.64s/it]
INFO:root:final mean train loss: 7881.007204117313
INFO:root:final train perplexity: 4.733395099639893
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.69s/it]
INFO:root:eval mean loss: 11164.244521368117
INFO:root:eval perplexity: 10.08351993560791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [7:35:18<20:10:25, 500.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7765.165167557566
INFO:root:current train perplexity4.664964199066162
INFO:root:current mean train loss 7875.10067210478
INFO:root:current train perplexity4.708941459655762
INFO:root:current mean train loss 7860.37451617794
INFO:root:current train perplexity4.704098224639893
INFO:root:current mean train loss 7854.134213055937
INFO:root:current train perplexity4.710343837738037
INFO:root:current mean train loss 7863.701787179296
INFO:root:current train perplexity4.717029571533203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.50s/it]
INFO:root:final mean train loss: 7864.999184885332
INFO:root:final train perplexity: 4.718471050262451
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.91s/it]
INFO:root:eval mean loss: 11156.5018804641
INFO:root:eval perplexity: 10.06737232208252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/56
########################best############
 28%|â–ˆâ–ˆâ–Š       | 56/200 [7:43:41<20:03:16, 501.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7890.095512058424
INFO:root:current train perplexity4.730887413024902
INFO:root:current mean train loss 7842.182232120173
INFO:root:current train perplexity4.687555313110352
INFO:root:current mean train loss 7840.823829000841
INFO:root:current train perplexity4.6946868896484375
INFO:root:current mean train loss 7855.322688902864
INFO:root:current train perplexity4.699717044830322
INFO:root:current mean train loss 7858.210285304004
INFO:root:current train perplexity4.7032012939453125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.47s/it]
INFO:root:final mean train loss: 7847.348922237273
INFO:root:final train perplexity: 4.702071189880371
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.30s/it]
INFO:root:eval mean loss: 11162.90533156622
INFO:root:eval perplexity: 10.080726623535156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [7:52:05<19:56:56, 502.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7778.642288773148
INFO:root:current train perplexity4.630420684814453
INFO:root:current mean train loss 7813.144331323819
INFO:root:current train perplexity4.662726402282715
INFO:root:current mean train loss 7828.567546289923
INFO:root:current train perplexity4.672918796539307
INFO:root:current mean train loss 7828.184945109423
INFO:root:current train perplexity4.681107521057129
INFO:root:current mean train loss 7832.403133919423
INFO:root:current train perplexity4.686290740966797


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.47s/it]
INFO:root:final mean train loss: 7832.2822265625
INFO:root:final train perplexity: 4.688117504119873
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.73s/it]
INFO:root:eval mean loss: 11168.635498046875
INFO:root:eval perplexity: 10.092687606811523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:00:22<19:44:34, 500.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7826.517940398186
INFO:root:current train perplexity4.6835455894470215
INFO:root:current mean train loss 7792.275006709208
INFO:root:current train perplexity4.637991428375244
INFO:root:current mean train loss 7790.734355976055
INFO:root:current train perplexity4.650148391723633
INFO:root:current mean train loss 7801.837328585253
INFO:root:current train perplexity4.656947135925293
INFO:root:current mean train loss 7809.926112057715
INFO:root:current train perplexity4.668699741363525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.76s/it]
INFO:root:final mean train loss: 7817.066179829259
INFO:root:final train perplexity: 4.674066543579102
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.67s/it]
INFO:root:eval mean loss: 11162.128882998511
INFO:root:eval perplexity: 10.07910442352295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:08:46<19:38:49, 501.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7798.368024553572
INFO:root:current train perplexity4.638280391693115
INFO:root:current mean train loss 7796.92943431713
INFO:root:current train perplexity4.637023448944092
INFO:root:current mean train loss 7802.1201628989365
INFO:root:current train perplexity4.65725040435791
INFO:root:current mean train loss 7802.279171525187
INFO:root:current train perplexity4.658939361572266
INFO:root:current mean train loss 7807.349916936063
INFO:root:current train perplexity4.6596903800964355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.39s/it]
INFO:root:final mean train loss: 7802.116345805506
INFO:root:final train perplexity: 4.660302639007568
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.72s/it]
INFO:root:eval mean loss: 11161.333164760044
INFO:root:eval perplexity: 10.077444076538086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [8:17:06<19:29:09, 501.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7785.044283353365
INFO:root:current train perplexity4.650238990783691
INFO:root:current mean train loss 7798.509006857014
INFO:root:current train perplexity4.656578540802002
INFO:root:current mean train loss 7775.917694985617
INFO:root:current train perplexity4.641307830810547
INFO:root:current mean train loss 7780.107381544985
INFO:root:current train perplexity4.642332077026367
INFO:root:current mean train loss 7787.813680105709
INFO:root:current train perplexity4.644287109375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.27s/it]
INFO:root:final mean train loss: 7789.216311547064
INFO:root:final train perplexity: 4.648458480834961
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.65s/it]
INFO:root:eval mean loss: 11157.364594959077
INFO:root:eval perplexity: 10.069169998168945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [8:25:25<19:19:26, 500.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7762.967058048692
INFO:root:current train perplexity4.600857257843018
INFO:root:current mean train loss 7786.428089488636
INFO:root:current train perplexity4.635378360748291
INFO:root:current mean train loss 7792.282174318416
INFO:root:current train perplexity4.642258167266846
INFO:root:current mean train loss 7785.935370353499
INFO:root:current train perplexity4.6352972984313965
INFO:root:current mean train loss 7777.7728312817435
INFO:root:current train perplexity4.634241104125977


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.82s/it]
INFO:root:final mean train loss: 7774.009108020413
INFO:root:final train perplexity: 4.63453483581543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.37s/it]
INFO:root:eval mean loss: 11168.625453404018
INFO:root:eval perplexity: 10.092666625976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [8:33:44<19:10:39, 500.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7704.182970412234
INFO:root:current train perplexity4.579867839813232
INFO:root:current mean train loss 7753.859139163478
INFO:root:current train perplexity4.61392879486084
INFO:root:current mean train loss 7764.671900699013
INFO:root:current train perplexity4.613978385925293
INFO:root:current mean train loss 7764.295154054845
INFO:root:current train perplexity4.6184210777282715
INFO:root:current mean train loss 7766.4886013090745
INFO:root:current train perplexity4.6224751472473145


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.01s/it]
INFO:root:final mean train loss: 7759.433704007057
INFO:root:final train perplexity: 4.62122917175293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.80s/it]
INFO:root:eval mean loss: 11163.030744280133
INFO:root:eval perplexity: 10.080987930297852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [8:42:07<19:04:11, 501.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7766.174948299632
INFO:root:current train perplexity4.612465858459473
INFO:root:current mean train loss 7741.1918039890315
INFO:root:current train perplexity4.609344959259033
INFO:root:current mean train loss 7762.745484857445
INFO:root:current train perplexity4.608519077301025
INFO:root:current mean train loss 7754.602820067664
INFO:root:current train perplexity4.6039838790893555
INFO:root:current mean train loss 7752.775795541159
INFO:root:current train perplexity4.604598045349121


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.44s/it]
INFO:root:final mean train loss: 7745.334271830897
INFO:root:final train perplexity: 4.608393669128418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 11167.086733863467
INFO:root:eval perplexity: 10.089451789855957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [8:50:27<18:54:28, 500.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7765.366681463068
INFO:root:current train perplexity4.596806049346924
INFO:root:current mean train loss 7691.798617061492
INFO:root:current train perplexity4.57529354095459
INFO:root:current mean train loss 7718.090950520834
INFO:root:current train perplexity4.58141565322876
INFO:root:current mean train loss 7732.780603543134
INFO:root:current train perplexity4.593360900878906
INFO:root:current mean train loss 7734.360777601305
INFO:root:current train perplexity4.5934343338012695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.03s/it]
INFO:root:final mean train loss: 7730.447732248614
INFO:root:final train perplexity: 4.594880104064941
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.65s/it]
INFO:root:eval mean loss: 11174.159955705914
INFO:root:eval perplexity: 10.10423755645752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [8:58:34<18:37:33, 496.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7717.0232223252115
INFO:root:current train perplexity4.573277950286865
INFO:root:current mean train loss 7722.8708757124605
INFO:root:current train perplexity4.574686050415039
INFO:root:current mean train loss 7722.30112625181
INFO:root:current train perplexity4.5753350257873535
INFO:root:current mean train loss 7723.965979445944
INFO:root:current train perplexity4.582245826721191
INFO:root:current mean train loss 7720.9666873042615
INFO:root:current train perplexity4.582552433013916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.51s/it]
INFO:root:final mean train loss: 7720.278516215662
INFO:root:final train perplexity: 4.585672378540039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.42s/it]
INFO:root:eval mean loss: 11160.917672293526
INFO:root:eval perplexity: 10.076579093933105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:06:57<18:32:56, 498.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7677.48039124504
INFO:root:current train perplexity4.53967809677124
INFO:root:current mean train loss 7716.667519411426
INFO:root:current train perplexity4.572402477264404
INFO:root:current mean train loss 7705.387379693441
INFO:root:current train perplexity4.561660289764404
INFO:root:current mean train loss 7701.8570183367765
INFO:root:current train perplexity4.566915988922119
INFO:root:current mean train loss 7708.700661447084
INFO:root:current train perplexity4.570357799530029


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.13s/it]
INFO:root:final mean train loss: 7704.32030757781
INFO:root:final train perplexity: 4.571258068084717
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.57s/it]
INFO:root:eval mean loss: 11173.132306780133
INFO:root:eval perplexity: 10.102087020874023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [9:15:11<18:22:05, 497.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7644.803273670709
INFO:root:current train perplexity4.529301643371582
INFO:root:current mean train loss 7684.504414998129
INFO:root:current train perplexity4.5473127365112305
INFO:root:current mean train loss 7693.9324727147705
INFO:root:current train perplexity4.5512590408325195
INFO:root:current mean train loss 7697.4046467877215
INFO:root:current train perplexity4.554683685302734
INFO:root:current mean train loss 7702.864472154376
INFO:root:current train perplexity4.559940338134766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.46s/it]
INFO:root:final mean train loss: 7691.77101578251
INFO:root:final train perplexity: 4.559957027435303
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.61s/it]
INFO:root:eval mean loss: 11163.814441499257
INFO:root:eval perplexity: 10.082620620727539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [9:23:23<18:10:15, 495.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7714.179577464789
INFO:root:current train perplexity4.562228679656982
INFO:root:current mean train loss 7697.066446226243
INFO:root:current train perplexity4.549539089202881
INFO:root:current mean train loss 7684.3590164466095
INFO:root:current train perplexity4.5431976318359375
INFO:root:current mean train loss 7690.495542294895
INFO:root:current train perplexity4.547317981719971
INFO:root:current mean train loss 7687.37633214736
INFO:root:current train perplexity4.548348426818848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.02s/it]
INFO:root:final mean train loss: 7679.259587441721
INFO:root:final train perplexity: 4.548716068267822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.12s/it]
INFO:root:eval mean loss: 11167.893615722656
INFO:root:eval perplexity: 10.09113883972168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [9:31:40<18:03:01, 496.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7637.916204427083
INFO:root:current train perplexity4.525897026062012
INFO:root:current mean train loss 7664.467843191965
INFO:root:current train perplexity4.536529064178467
INFO:root:current mean train loss 7645.785697798296
INFO:root:current train perplexity4.531219482421875
INFO:root:current mean train loss 7657.191692708333
INFO:root:current train perplexity4.533771991729736
INFO:root:current mean train loss 7671.707069284539
INFO:root:current train perplexity4.536783695220947


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.13s/it]
INFO:root:final mean train loss: 7667.005136797505
INFO:root:final train perplexity: 4.53773307800293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 67.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.01s/it]
INFO:root:eval mean loss: 11182.807422456288
INFO:root:eval perplexity: 10.12234115600586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [9:40:01<17:57:57, 497.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7600.787857248813
INFO:root:current train perplexity4.509193420410156
INFO:root:current mean train loss 7629.453659654329
INFO:root:current train perplexity4.515544414520264
INFO:root:current mean train loss 7638.5073119679655
INFO:root:current train perplexity4.514849662780762
INFO:root:current mean train loss 7643.994569642563
INFO:root:current train perplexity4.520545482635498
INFO:root:current mean train loss 7658.834147475209
INFO:root:current train perplexity4.524837970733643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.24s/it]
INFO:root:final mean train loss: 7655.518487745716
INFO:root:final train perplexity: 4.527462959289551
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.49s/it]
INFO:root:eval mean loss: 11172.580421084449
INFO:root:eval perplexity: 10.100933074951172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [9:48:07<17:42:19, 494.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7601.222797439759
INFO:root:current train perplexity4.472202301025391
INFO:root:current mean train loss 7629.773111979167
INFO:root:current train perplexity4.499380111694336
INFO:root:current mean train loss 7622.8027447272525
INFO:root:current train perplexity4.501354217529297
INFO:root:current mean train loss 7639.586225624184
INFO:root:current train perplexity4.509812355041504
INFO:root:current mean train loss 7649.345901268116
INFO:root:current train perplexity4.517276763916016


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.94s/it]
INFO:root:final mean train loss: 7643.44241726783
INFO:root:final train perplexity: 4.516690731048584
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.97s/it]
INFO:root:eval mean loss: 11170.933099655878
INFO:root:eval perplexity: 10.097489356994629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [9:56:25<17:36:48, 495.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7633.520401176365
INFO:root:current train perplexity4.488217353820801
INFO:root:current mean train loss 7625.5782868900405
INFO:root:current train perplexity4.496805667877197
INFO:root:current mean train loss 7632.111314514373
INFO:root:current train perplexity4.499580383300781
INFO:root:current mean train loss 7633.847883357558
INFO:root:current train perplexity4.5035223960876465
INFO:root:current mean train loss 7636.804840902529
INFO:root:current train perplexity4.505280017852783


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.07s/it]
INFO:root:final mean train loss: 7630.658956220073
INFO:root:final train perplexity: 4.50531530380249
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.41s/it]
INFO:root:eval mean loss: 11176.091575985864
INFO:root:eval perplexity: 10.108278274536133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [10:04:44<17:30:49, 496.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7603.249527815934
INFO:root:current train perplexity4.48599100112915
INFO:root:current mean train loss 7616.469839046139
INFO:root:current train perplexity4.4888176918029785
INFO:root:current mean train loss 7615.738928935782
INFO:root:current train perplexity4.495233535766602
INFO:root:current mean train loss 7620.285045106698
INFO:root:current train perplexity4.493551254272461
INFO:root:current mean train loss 7621.160303430499
INFO:root:current train perplexity4.492861270904541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.50s/it]
INFO:root:final mean train loss: 7617.022453061996
INFO:root:final train perplexity: 4.4932122230529785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.69s/it]
INFO:root:eval mean loss: 11181.428158714658
INFO:root:eval perplexity: 10.119449615478516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [10:12:52<17:16:47, 493.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7601.438522820724
INFO:root:current train perplexity4.469876289367676
INFO:root:current mean train loss 7602.143216646635
INFO:root:current train perplexity4.474536895751953
INFO:root:current mean train loss 7603.550599179025
INFO:root:current train perplexity4.4718017578125
INFO:root:current mean train loss 7606.482224090189
INFO:root:current train perplexity4.480589866638184
INFO:root:current mean train loss 7615.513890861743
INFO:root:current train perplexity4.48646879196167


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:50<00:00, 410.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:50<00:00, 410.07s/it]
INFO:root:final mean train loss: 7609.490068004978
INFO:root:final train perplexity: 4.486541271209717
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.91s/it]
INFO:root:eval mean loss: 11180.076811290923
INFO:root:eval perplexity: 10.116620063781738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [10:21:00<17:05:02, 492.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7600.403211805556
INFO:root:current train perplexity4.463915824890137
INFO:root:current mean train loss 7595.545493581187
INFO:root:current train perplexity4.4696364402771
INFO:root:current mean train loss 7599.7707168426
INFO:root:current train perplexity4.466007232666016
INFO:root:current mean train loss 7603.208214628367
INFO:root:current train perplexity4.475199222564697


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.25s/it]
INFO:root:final mean train loss: 7598.941254646547
INFO:root:final train perplexity: 4.477214336395264
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.04s/it]
INFO:root:eval mean loss: 11178.814046223959
INFO:root:eval perplexity: 10.113977432250977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [10:29:19<17:01:19, 494.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7645.908040364583
INFO:root:current train perplexity4.4745869636535645
INFO:root:current mean train loss 7580.547130992111
INFO:root:current train perplexity4.46205472946167
INFO:root:current mean train loss 7584.672695216287
INFO:root:current train perplexity4.45734977722168
INFO:root:current mean train loss 7592.73743844111
INFO:root:current train perplexity4.46300745010376
INFO:root:current mean train loss 7592.128431296526
INFO:root:current train perplexity4.464365482330322


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.87s/it]
INFO:root:final mean train loss: 7586.1228539251515
INFO:root:final train perplexity: 4.465907573699951
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.65s/it]
INFO:root:eval mean loss: 11196.041817801339
INFO:root:eval perplexity: 10.150105476379395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [10:37:27<16:48:58, 492.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7470.677525111607
INFO:root:current train perplexity4.375234603881836
INFO:root:current mean train loss 7576.848500474591
INFO:root:current train perplexity4.437936782836914
INFO:root:current mean train loss 7576.810089258756
INFO:root:current train perplexity4.448392391204834
INFO:root:current mean train loss 7567.713394811177
INFO:root:current train perplexity4.444660663604736
INFO:root:current mean train loss 7574.853017746084
INFO:root:current train perplexity4.451506614685059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.06s/it]
INFO:root:final mean train loss: 7574.562246014995
INFO:root:final train perplexity: 4.4557342529296875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.35s/it]
INFO:root:eval mean loss: 11188.753324962798
INFO:root:eval perplexity: 10.134803771972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [10:45:33<16:37:11, 490.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7559.91064453125
INFO:root:current train perplexity4.457870006561279
INFO:root:current mean train loss 7552.3010143933
INFO:root:current train perplexity4.44951057434082
INFO:root:current mean train loss 7557.3596573237555
INFO:root:current train perplexity4.442993640899658
INFO:root:current mean train loss 7560.834844754823
INFO:root:current train perplexity4.4432854652404785
INFO:root:current mean train loss 7563.386433622263
INFO:root:current train perplexity4.440265655517578


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.25s/it]
INFO:root:final mean train loss: 7562.614728373866
INFO:root:final train perplexity: 4.44524621963501
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.86s/it]
INFO:root:eval mean loss: 11186.62321544829
INFO:root:eval perplexity: 10.13033676147461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [10:53:42<16:28:13, 490.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7523.465266927084
INFO:root:current train perplexity4.4045209884643555
INFO:root:current mean train loss 7563.231199048913
INFO:root:current train perplexity4.42995548248291
INFO:root:current mean train loss 7564.889010265261
INFO:root:current train perplexity4.435115814208984
INFO:root:current mean train loss 7563.554534040179
INFO:root:current train perplexity4.439107418060303
INFO:root:current mean train loss 7567.401727221386
INFO:root:current train perplexity4.442439556121826


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.28s/it]
INFO:root:final mean train loss: 7554.68410172001
INFO:root:final train perplexity: 4.438296318054199
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.31s/it]
INFO:root:eval mean loss: 11193.53897530692
INFO:root:eval perplexity: 10.144845962524414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [11:01:53<16:20:46, 490.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7483.347322162829
INFO:root:current train perplexity4.4499030113220215
INFO:root:current mean train loss 7514.50631483062
INFO:root:current train perplexity4.40499210357666
INFO:root:current mean train loss 7524.552999696775
INFO:root:current train perplexity4.413055896759033
INFO:root:current mean train loss 7538.687628575627
INFO:root:current train perplexity4.420807838439941
INFO:root:current mean train loss 7545.556405224493
INFO:root:current train perplexity4.428032398223877


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.05s/it]
INFO:root:final mean train loss: 7544.712899484942
INFO:root:final train perplexity: 4.429575443267822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.43s/it]
INFO:root:eval mean loss: 11200.787638346354
INFO:root:eval perplexity: 10.160083770751953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [11:09:59<16:09:39, 488.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7569.920134171196
INFO:root:current train perplexity4.482795715332031
INFO:root:current mean train loss 7536.499718146595
INFO:root:current train perplexity4.417690277099609
INFO:root:current mean train loss 7543.449626015975
INFO:root:current train perplexity4.413538455963135
INFO:root:current mean train loss 7534.42388708156
INFO:root:current train perplexity4.414844036102295
INFO:root:current mean train loss 7543.877260176566
INFO:root:current train perplexity4.416943550109863


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.84s/it]
INFO:root:final mean train loss: 7530.0294179608745
INFO:root:final train perplexity: 4.416763782501221
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.89s/it]
INFO:root:eval mean loss: 11199.995608375186
INFO:root:eval perplexity: 10.158414840698242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [11:18:04<15:59:42, 487.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7546.820999710648
INFO:root:current train perplexity4.417830467224121
INFO:root:current mean train loss 7515.2402420644685
INFO:root:current train perplexity4.405254364013672
INFO:root:current mean train loss 7515.154703417538
INFO:root:current train perplexity4.414441108703613
INFO:root:current mean train loss 7522.532328101109
INFO:root:current train perplexity4.413880348205566
INFO:root:current mean train loss 7529.570016329406
INFO:root:current train perplexity4.412712097167969


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:50<00:00, 410.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:50<00:00, 410.84s/it]
INFO:root:final mean train loss: 7524.593177057081
INFO:root:final train perplexity: 4.41202974319458
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.68s/it]
INFO:root:eval mean loss: 11193.375360398066
INFO:root:eval perplexity: 10.144505500793457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [11:26:14<15:52:42, 488.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7499.7513073336695
INFO:root:current train perplexity4.360834121704102
INFO:root:current mean train loss 7494.610578930105
INFO:root:current train perplexity4.391704082489014
INFO:root:current mean train loss 7500.14924707454
INFO:root:current train perplexity4.393309116363525
INFO:root:current mean train loss 7513.457212695903
INFO:root:current train perplexity4.400632858276367
INFO:root:current mean train loss 7519.232766277552
INFO:root:current train perplexity4.40300989151001


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.39s/it]
INFO:root:final mean train loss: 7513.587093230217
INFO:root:final train perplexity: 4.402461528778076
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.39s/it]
INFO:root:eval mean loss: 11206.10696265811
INFO:root:eval perplexity: 10.171271324157715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [11:34:31<15:48:57, 490.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7451.398814174107
INFO:root:current train perplexity4.369317531585693
INFO:root:current mean train loss 7474.340643084491
INFO:root:current train perplexity4.374924182891846
INFO:root:current mean train loss 7485.975835272607
INFO:root:current train perplexity4.3819379806518555
INFO:root:current mean train loss 7502.75477058069
INFO:root:current train perplexity4.388047218322754
INFO:root:current mean train loss 7511.358343435704
INFO:root:current train perplexity4.392411231994629


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:46<00:00, 406.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:46<00:00, 406.50s/it]
INFO:root:final mean train loss: 7502.791399555822
INFO:root:final train perplexity: 4.39309549331665
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.25s/it]
INFO:root:eval mean loss: 11192.178780691964
INFO:root:eval perplexity: 10.141992568969727
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [11:42:34<15:36:23, 488.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7412.345227363782
INFO:root:current train perplexity4.385514736175537
INFO:root:current mean train loss 7494.888018491457
INFO:root:current train perplexity4.386011123657227
INFO:root:current mean train loss 7497.796640052955
INFO:root:current train perplexity4.380448818206787
INFO:root:current mean train loss 7489.364903092736
INFO:root:current train perplexity4.381906509399414
INFO:root:current mean train loss 7496.939726740461
INFO:root:current train perplexity4.3822832107543945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.49s/it]
INFO:root:final mean train loss: 7491.203018680696
INFO:root:final train perplexity: 4.383065223693848
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.98s/it]
INFO:root:eval mean loss: 11210.3025163923
INFO:root:eval perplexity: 10.1801118850708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [11:50:47<15:30:56, 489.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7426.287211573401
INFO:root:current train perplexity4.35334587097168
INFO:root:current mean train loss 7451.17906946569
INFO:root:current train perplexity4.37089729309082
INFO:root:current mean train loss 7468.153615692516
INFO:root:current train perplexity4.369986534118652
INFO:root:current mean train loss 7479.4644636593475
INFO:root:current train perplexity4.372216701507568
INFO:root:current mean train loss 7486.6544148120065
INFO:root:current train perplexity4.376101493835449


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.51s/it]
INFO:root:final mean train loss: 7484.727859004851
INFO:root:final train perplexity: 4.377469539642334
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.27s/it]
INFO:root:eval mean loss: 11207.517967587426
INFO:root:eval perplexity: 10.174246788024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [11:58:54<15:21:15, 489.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7468.744597739362
INFO:root:current train perplexity4.344384670257568
INFO:root:current mean train loss 7496.176641555059
INFO:root:current train perplexity4.370195388793945
INFO:root:current mean train loss 7472.490135532642
INFO:root:current train perplexity4.361213207244873
INFO:root:current mean train loss 7471.579553257835
INFO:root:current train perplexity4.358579635620117
INFO:root:current mean train loss 7478.76302520274
INFO:root:current train perplexity4.367130279541016


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.34s/it]
INFO:root:final mean train loss: 7473.466236729776
INFO:root:final train perplexity: 4.367755889892578
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.57s/it]
INFO:root:eval mean loss: 11212.7248273577
INFO:root:eval perplexity: 10.18521785736084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [12:06:59<15:10:48, 487.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7502.99862132353
INFO:root:current train perplexity4.366015434265137
INFO:root:current mean train loss 7458.072407905629
INFO:root:current train perplexity4.3435211181640625
INFO:root:current mean train loss 7469.864234468377
INFO:root:current train perplexity4.359179496765137
INFO:root:current mean train loss 7460.921694155092
INFO:root:current train perplexity4.35430383682251
INFO:root:current mean train loss 7467.396004755058
INFO:root:current train perplexity4.359635353088379


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.12s/it]
INFO:root:final mean train loss: 7465.369060885521
INFO:root:final train perplexity: 4.360785007476807
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.44s/it]
INFO:root:eval mean loss: 11208.112656366258
INFO:root:eval perplexity: 10.175498962402344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [12:15:07<15:02:15, 487.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7443.283407315341
INFO:root:current train perplexity4.342959403991699
INFO:root:current mean train loss 7436.768126260081
INFO:root:current train perplexity4.344179153442383
INFO:root:current mean train loss 7439.10716528799
INFO:root:current train perplexity4.338448524475098
INFO:root:current mean train loss 7452.188645741638
INFO:root:current train perplexity4.344350337982178
INFO:root:current mean train loss 7457.92393543956
INFO:root:current train perplexity4.35085391998291


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.32s/it]
INFO:root:final mean train loss: 7457.888313539566
INFO:root:final train perplexity: 4.3543548583984375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.02s/it]
INFO:root:eval mean loss: 11212.394263857886
INFO:root:eval perplexity: 10.184517860412598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [12:23:20<14:57:20, 489.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7445.817631091101
INFO:root:current train perplexity4.32411527633667
INFO:root:current mean train loss 7468.375334733687
INFO:root:current train perplexity4.341841220855713
INFO:root:current mean train loss 7462.653310886221
INFO:root:current train perplexity4.339175701141357
INFO:root:current mean train loss 7447.176530673311
INFO:root:current train perplexity4.342432498931885
INFO:root:current mean train loss 7456.142669611248
INFO:root:current train perplexity4.346770286560059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.74s/it]
INFO:root:final mean train loss: 7447.6480230516
INFO:root:final train perplexity: 4.34556770324707
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.97s/it]
INFO:root:eval mean loss: 11212.345450265067
INFO:root:eval perplexity: 10.184418678283691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [12:31:28<14:48:29, 489.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7429.13826109871
INFO:root:current train perplexity4.330190658569336
INFO:root:current mean train loss 7433.900495470667
INFO:root:current train perplexity4.335390567779541
INFO:root:current mean train loss 7432.6901790488355
INFO:root:current train perplexity4.336181163787842
INFO:root:current mean train loss 7441.930104489497
INFO:root:current train perplexity4.336915969848633
INFO:root:current mean train loss 7441.733560846383
INFO:root:current train perplexity4.337101459503174


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.06s/it]
INFO:root:final mean train loss: 7438.005462646484
INFO:root:final train perplexity: 4.337309837341309
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.70s/it]
INFO:root:eval mean loss: 11221.835611979166
INFO:root:eval perplexity: 10.20444393157959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [12:39:46<14:44:54, 491.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7418.592722423041
INFO:root:current train perplexity4.318249225616455
INFO:root:current mean train loss 7441.255619620135
INFO:root:current train perplexity4.332395076751709
INFO:root:current mean train loss 7449.1873408971205
INFO:root:current train perplexity4.337735176086426
INFO:root:current mean train loss 7430.354581328764
INFO:root:current train perplexity4.326526641845703
INFO:root:current mean train loss 7427.8791979640655
INFO:root:current train perplexity4.324150562286377


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.14s/it]
INFO:root:final mean train loss: 7427.803953109249
INFO:root:final train perplexity: 4.328590393066406
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.71s/it]
INFO:root:eval mean loss: 11213.405424572173
INFO:root:eval perplexity: 10.186655044555664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [12:48:09<14:42:50, 495.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7421.876698668574
INFO:root:current train perplexity4.2988128662109375
INFO:root:current mean train loss 7415.610357273392
INFO:root:current train perplexity4.316279411315918
INFO:root:current mean train loss 7413.723124711716
INFO:root:current train perplexity4.3158392906188965
INFO:root:current mean train loss 7426.4841038788745
INFO:root:current train perplexity4.321532249450684
INFO:root:current mean train loss 7424.785111672306
INFO:root:current train perplexity4.32124137878418


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.35s/it]
INFO:root:final mean train loss: 7420.078145673198
INFO:root:final train perplexity: 4.321998119354248
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.93s/it]
INFO:root:eval mean loss: 11233.251540411085
INFO:root:eval perplexity: 10.228585243225098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [12:56:14<14:29:03, 491.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7428.61466796875
INFO:root:current train perplexity4.318488597869873
INFO:root:current mean train loss 7425.224123883929
INFO:root:current train perplexity4.30836820602417
INFO:root:current mean train loss 7422.878547585227
INFO:root:current train perplexity4.315603256225586
INFO:root:current mean train loss 7419.463243489583
INFO:root:current train perplexity4.315034866333008
INFO:root:current mean train loss 7418.918424136513
INFO:root:current train perplexity4.31763219833374


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.98s/it]
INFO:root:final mean train loss: 7411.121201053743
INFO:root:final train perplexity: 4.314367771148682
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.35s/it]
INFO:root:eval mean loss: 11226.329874674479
INFO:root:eval perplexity: 10.21394157409668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [13:04:19<14:17:17, 489.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7367.933266168909
INFO:root:current train perplexity4.262349605560303
INFO:root:current mean train loss 7394.359585042773
INFO:root:current train perplexity4.286734104156494
INFO:root:current mean train loss 7403.321465823813
INFO:root:current train perplexity4.292996406555176
INFO:root:current mean train loss 7408.0349423853895
INFO:root:current train perplexity4.301159381866455
INFO:root:current mean train loss 7412.957117896986
INFO:root:current train perplexity4.306942939758301


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.59s/it]
INFO:root:final mean train loss: 7403.047174269153
INFO:root:final train perplexity: 4.307501792907715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.21s/it]
INFO:root:eval mean loss: 11227.845476422992
INFO:root:eval perplexity: 10.217144012451172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [13:12:35<14:12:30, 491.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7412.802363751883
INFO:root:current train perplexity4.3133544921875
INFO:root:current mean train loss 7413.900038422131
INFO:root:current train perplexity4.304069995880127
INFO:root:current mean train loss 7402.029019089554
INFO:root:current train perplexity4.301564693450928
INFO:root:current mean train loss 7396.3050699657315
INFO:root:current train perplexity4.301304340362549
INFO:root:current mean train loss 7401.755918009188
INFO:root:current train perplexity4.302240371704102


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.30s/it]
INFO:root:final mean train loss: 7395.823216592112
INFO:root:final train perplexity: 4.301367282867432
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.01s/it]
INFO:root:eval mean loss: 11230.532935732886
INFO:root:eval perplexity: 10.222831726074219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [13:20:54<14:07:47, 493.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7379.705313846983
INFO:root:current train perplexity4.295660018920898
INFO:root:current mean train loss 7408.120145909927
INFO:root:current train perplexity4.291094779968262
INFO:root:current mean train loss 7389.415198987369
INFO:root:current train perplexity4.2932658195495605
INFO:root:current mean train loss 7393.747564902294
INFO:root:current train perplexity4.292964935302734
INFO:root:current mean train loss 7389.67701248075
INFO:root:current train perplexity4.29294490814209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.72s/it]
INFO:root:final mean train loss: 7385.927133867817
INFO:root:final train perplexity: 4.292979717254639
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.93s/it]
INFO:root:eval mean loss: 11233.682875860304
INFO:root:eval perplexity: 10.229499816894531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [13:28:59<13:55:28, 491.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7359.735855940934
INFO:root:current train perplexity4.281108856201172
INFO:root:current mean train loss 7393.060091827552
INFO:root:current train perplexity4.280786037445068
INFO:root:current mean train loss 7386.905879174721
INFO:root:current train perplexity4.2825398445129395
INFO:root:current mean train loss 7383.647726932145
INFO:root:current train perplexity4.284398078918457
INFO:root:current mean train loss 7384.682951327011
INFO:root:current train perplexity4.28640604019165


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.80s/it]
INFO:root:final mean train loss: 7379.083771736391
INFO:root:final train perplexity: 4.287187576293945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.49s/it]
INFO:root:eval mean loss: 11234.459891183036
INFO:root:eval perplexity: 10.231145858764648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [13:37:28<13:55:54, 496.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7345.714216694079
INFO:root:current train perplexity4.243480205535889
INFO:root:current mean train loss 7370.616481370193
INFO:root:current train perplexity4.26932954788208
INFO:root:current mean train loss 7366.4495746159955
INFO:root:current train perplexity4.267642974853516
INFO:root:current mean train loss 7369.524643987342
INFO:root:current train perplexity4.2718281745910645
INFO:root:current mean train loss 7373.95895083649
INFO:root:current train perplexity4.27805233001709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.23s/it]
INFO:root:final mean train loss: 7368.272110477571
INFO:root:final train perplexity: 4.278054714202881
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.95s/it]
INFO:root:eval mean loss: 11235.216991606212
INFO:root:eval perplexity: 10.232748985290527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [13:45:43<13:46:56, 496.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7338.501982717803
INFO:root:current train perplexity4.261788845062256
INFO:root:current mean train loss 7338.409378435144
INFO:root:current train perplexity4.257797718048096
INFO:root:current mean train loss 7351.9245531981605
INFO:root:current train perplexity4.264185428619385
INFO:root:current mean train loss 7372.814035821977
INFO:root:current train perplexity4.27346134185791


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.89s/it]
INFO:root:final mean train loss: 7363.356545725176
INFO:root:final train perplexity: 4.273907661437988
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.51s/it]
INFO:root:eval mean loss: 11236.924092610678
INFO:root:eval perplexity: 10.23636245727539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [13:54:40<13:58:42, 508.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7310.623046875
INFO:root:current train perplexity4.254319667816162
INFO:root:current mean train loss 7377.495757167779
INFO:root:current train perplexity4.2602925300598145
INFO:root:current mean train loss 7369.358720751232
INFO:root:current train perplexity4.26693058013916
INFO:root:current mean train loss 7360.591495526506
INFO:root:current train perplexity4.263656139373779
INFO:root:current mean train loss 7359.490201661368
INFO:root:current train perplexity4.26505184173584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.90s/it]
INFO:root:final mean train loss: 7353.533710110572
INFO:root:final train perplexity: 4.265634536743164
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.64s/it]
INFO:root:eval mean loss: 11237.79014950707
INFO:root:eval perplexity: 10.238198280334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [14:03:04<13:48:15, 507.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7308.581194196428
INFO:root:current train perplexity4.180917263031006
INFO:root:current mean train loss 7315.217490508177
INFO:root:current train perplexity4.228004455566406
INFO:root:current mean train loss 7363.20798658288
INFO:root:current train perplexity4.254786014556885
INFO:root:current mean train loss 7354.746739490024
INFO:root:current train perplexity4.252359390258789
INFO:root:current mean train loss 7347.552244894042
INFO:root:current train perplexity4.255654811859131


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:49<00:00, 409.51s/it]
INFO:root:final mean train loss: 7348.3799684586065
INFO:root:final train perplexity: 4.2612996101379395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.55s/it]
INFO:root:eval mean loss: 11242.994408017114
INFO:root:eval perplexity: 10.249232292175293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [14:11:26<13:37:08, 505.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7227.501997514205
INFO:root:current train perplexity4.208199977874756
INFO:root:current mean train loss 7327.187513196791
INFO:root:current train perplexity4.229866981506348
INFO:root:current mean train loss 7322.572381331457
INFO:root:current train perplexity4.237595558166504
INFO:root:current mean train loss 7336.459745842544
INFO:root:current train perplexity4.250436305999756
INFO:root:current mean train loss 7341.330991721791
INFO:root:current train perplexity4.25360631942749


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.60s/it]
INFO:root:final mean train loss: 7339.110935334236
INFO:root:final train perplexity: 4.253515720367432
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.55s/it]
INFO:root:eval mean loss: 11249.137997581845
INFO:root:eval perplexity: 10.262277603149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [14:19:34<13:20:27, 500.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7227.892154947916
INFO:root:current train perplexity4.187236309051514
INFO:root:current mean train loss 7270.368796705163
INFO:root:current train perplexity4.219542026519775
INFO:root:current mean train loss 7294.914280523256
INFO:root:current train perplexity4.230983734130859
INFO:root:current mean train loss 7317.984289744543
INFO:root:current train perplexity4.239079475402832
INFO:root:current mean train loss 7336.119919521838
INFO:root:current train perplexity4.2487030029296875


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.87s/it]
INFO:root:final mean train loss: 7332.332552017704
INFO:root:final train perplexity: 4.247831344604492
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.99s/it]
INFO:root:eval mean loss: 11252.003063383556
INFO:root:eval perplexity: 10.268366813659668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [14:27:41<13:05:44, 496.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7299.649876644737
INFO:root:current train perplexity4.241504669189453
INFO:root:current mean train loss 7326.47491301208
INFO:root:current train perplexity4.241036891937256
INFO:root:current mean train loss 7333.9112420626425
INFO:root:current train perplexity4.236759662628174
INFO:root:current mean train loss 7338.477237522042
INFO:root:current train perplexity4.239882946014404
INFO:root:current mean train loss 7332.913140708905
INFO:root:current train perplexity4.23876428604126


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:47<00:00, 407.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:47<00:00, 407.82s/it]
INFO:root:final mean train loss: 7323.202534337198
INFO:root:final train perplexity: 4.240188121795654
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.38s/it]
INFO:root:eval mean loss: 11250.56211344401
INFO:root:eval perplexity: 10.265298843383789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [14:35:43<12:50:40, 491.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7176.224354619565
INFO:root:current train perplexity4.186159133911133
INFO:root:current mean train loss 7284.5292770261685
INFO:root:current train perplexity4.218549728393555
INFO:root:current mean train loss 7301.759815985846
INFO:root:current train perplexity4.2212042808532715
INFO:root:current mean train loss 7320.277345261707
INFO:root:current train perplexity4.229535102844238
INFO:root:current mean train loss 7321.918288499187
INFO:root:current train perplexity4.234117031097412


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.79s/it]
INFO:root:final mean train loss: 7317.314311365927
INFO:root:final train perplexity: 4.235266208648682
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.44s/it]
INFO:root:eval mean loss: 11258.25881812686
INFO:root:eval perplexity: 10.281668663024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [14:43:55<12:42:28, 491.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7269.9169921875
INFO:root:current train perplexity4.2007856369018555
INFO:root:current mean train loss 7287.132058932087
INFO:root:current train perplexity4.215252876281738
INFO:root:current mean train loss 7315.936579363987
INFO:root:current train perplexity4.221571445465088
INFO:root:current mean train loss 7311.071339831804
INFO:root:current train perplexity4.227314472198486
INFO:root:current mean train loss 7313.156229416715
INFO:root:current train perplexity4.224719524383545


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.06s/it]
INFO:root:final mean train loss: 7310.8145397555445
INFO:root:final train perplexity: 4.229838848114014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.92s/it]
INFO:root:eval mean loss: 11256.449119931176
INFO:root:eval perplexity: 10.277815818786621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [14:52:07<12:34:26, 492.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7192.673024823589
INFO:root:current train perplexity4.202898025512695
INFO:root:current mean train loss 7268.718451812977
INFO:root:current train perplexity4.215084075927734
INFO:root:current mean train loss 7293.892751454275
INFO:root:current train perplexity4.215172290802002
INFO:root:current mean train loss 7304.120006549754
INFO:root:current train perplexity4.217715263366699
INFO:root:current mean train loss 7305.701099369199
INFO:root:current train perplexity4.221006393432617


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.42s/it]
INFO:root:final mean train loss: 7304.807799308531
INFO:root:final train perplexity: 4.224830150604248
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.47s/it]
INFO:root:eval mean loss: 11266.33729189918
INFO:root:eval perplexity: 10.298873901367188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [15:00:20<12:26:51, 492.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7254.599930245536
INFO:root:current train perplexity4.181397914886475
INFO:root:current mean train loss 7257.692997685185
INFO:root:current train perplexity4.198326110839844
INFO:root:current mean train loss 7278.733953208111
INFO:root:current train perplexity4.205467700958252
INFO:root:current mean train loss 7290.681158173974
INFO:root:current train perplexity4.214611053466797
INFO:root:current mean train loss 7294.979999551006
INFO:root:current train perplexity4.214885711669922


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.96s/it]
INFO:root:final mean train loss: 7296.039043795678
INFO:root:final train perplexity: 4.217528343200684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.26s/it]
INFO:root:eval mean loss: 11267.125261579242
INFO:root:eval perplexity: 10.300556182861328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [15:08:38<12:20:52, 493.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7290.585887419872
INFO:root:current train perplexity4.198531150817871
INFO:root:current mean train loss 7306.291005086556
INFO:root:current train perplexity4.210187911987305
INFO:root:current mean train loss 7308.757019809101
INFO:root:current train perplexity4.20982551574707
INFO:root:current mean train loss 7301.480655996497
INFO:root:current train perplexity4.205915451049805
INFO:root:current mean train loss 7291.631897111689
INFO:root:current train perplexity4.209425926208496


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.74s/it]
INFO:root:final mean train loss: 7289.02180530179
INFO:root:final train perplexity: 4.211694240570068
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.42s/it]
INFO:root:eval mean loss: 11265.278012230283
INFO:root:eval perplexity: 10.296615600585938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [15:16:42<12:08:26, 491.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7311.232683048692
INFO:root:current train perplexity4.199211120605469
INFO:root:current mean train loss 7291.479140488418
INFO:root:current train perplexity4.20389986038208
INFO:root:current mean train loss 7280.018629034851
INFO:root:current train perplexity4.19902229309082
INFO:root:current mean train loss 7287.975951792548
INFO:root:current train perplexity4.206904411315918
INFO:root:current mean train loss 7285.615781073646
INFO:root:current train perplexity4.2066240310668945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.79s/it]
INFO:root:final mean train loss: 7281.0645397555445
INFO:root:final train perplexity: 4.205088138580322
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.92s/it]
INFO:root:eval mean loss: 11268.540977841332
INFO:root:eval perplexity: 10.303572654724121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [15:24:54<12:00:27, 491.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7246.825538148271
INFO:root:current train perplexity4.182199954986572
INFO:root:current mean train loss 7246.390960485757
INFO:root:current train perplexity4.190925121307373
INFO:root:current mean train loss 7267.913119543902
INFO:root:current train perplexity4.198829650878906
INFO:root:current mean train loss 7269.375699353836
INFO:root:current train perplexity4.198281764984131
INFO:root:current mean train loss 7278.314932667436
INFO:root:current train perplexity4.202181339263916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.06s/it]
INFO:root:final mean train loss: 7276.924364643713
INFO:root:final train perplexity: 4.201655387878418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.98s/it]
INFO:root:eval mean loss: 11274.164103190104
INFO:root:eval perplexity: 10.315572738647461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [15:33:00<11:50:09, 489.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7223.917202818628
INFO:root:current train perplexity4.165395259857178
INFO:root:current mean train loss 7261.66948209851
INFO:root:current train perplexity4.1895623207092285
INFO:root:current mean train loss 7271.132767757096
INFO:root:current train perplexity4.196254730224609
INFO:root:current mean train loss 7273.791207598825
INFO:root:current train perplexity4.196985244750977
INFO:root:current mean train loss 7276.45657977931
INFO:root:current train perplexity4.195800304412842


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.08s/it]
INFO:root:final mean train loss: 7269.6315041818925
INFO:root:final train perplexity: 4.195615291595459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.25s/it]
INFO:root:eval mean loss: 11274.297281901041
INFO:root:eval perplexity: 10.315857887268066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [15:41:09<11:41:38, 489.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7254.0720703125
INFO:root:current train perplexity4.208067893981934
INFO:root:current mean train loss 7262.141664566532
INFO:root:current train perplexity4.193789482116699
INFO:root:current mean train loss 7258.781288296569
INFO:root:current train perplexity4.189925670623779
INFO:root:current mean train loss 7265.4411407900525
INFO:root:current train perplexity4.186689853668213
INFO:root:current mean train loss 7264.214602292239
INFO:root:current train perplexity4.186609745025635


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.55s/it]
INFO:root:final mean train loss: 7262.393834267893
INFO:root:final train perplexity: 4.189629077911377
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.13s/it]
INFO:root:eval mean loss: 11275.382536388579
INFO:root:eval perplexity: 10.318175315856934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [15:49:18<11:33:20, 489.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7196.674150887182
INFO:root:current train perplexity4.16756010055542
INFO:root:current mean train loss 7244.039255969929
INFO:root:current train perplexity4.175002098083496
INFO:root:current mean train loss 7250.970684272442
INFO:root:current train perplexity4.1727118492126465
INFO:root:current mean train loss 7261.24058664476
INFO:root:current train perplexity4.179931640625
INFO:root:current mean train loss 7257.848810466026
INFO:root:current train perplexity4.181796073913574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.54s/it]
INFO:root:final mean train loss: 7256.344895885837
INFO:root:final train perplexity: 4.184633255004883
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.33s/it]
INFO:root:eval mean loss: 11285.827930268788
INFO:root:eval perplexity: 10.340509414672852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [15:57:31<11:26:43, 490.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7274.581814236111
INFO:root:current train perplexity4.182703971862793
INFO:root:current mean train loss 7256.323724477569
INFO:root:current train perplexity4.174812316894531
INFO:root:current mean train loss 7262.411038126782
INFO:root:current train perplexity4.178481101989746
INFO:root:current mean train loss 7261.96992429623
INFO:root:current train perplexity4.180174827575684
INFO:root:current mean train loss 7259.6701275226105
INFO:root:current train perplexity4.1823272705078125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:51<00:00, 411.29s/it]
INFO:root:final mean train loss: 7252.339489352318
INFO:root:final train perplexity: 4.1813273429870605
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.16s/it]
INFO:root:eval mean loss: 11286.040390741258
INFO:root:eval perplexity: 10.340962409973145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [16:05:38<11:16:50, 489.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7235.205369636194
INFO:root:current train perplexity4.150446891784668
INFO:root:current mean train loss 7244.425591200412
INFO:root:current train perplexity4.165522575378418
INFO:root:current mean train loss 7250.243242699556
INFO:root:current train perplexity4.1679253578186035
INFO:root:current mean train loss 7253.050890348263
INFO:root:current train perplexity4.172416687011719
INFO:root:current mean train loss 7249.356517456838
INFO:root:current train perplexity4.174112796783447


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:05<00:00, 425.45s/it]
INFO:root:final mean train loss: 7242.4028566422
INFO:root:final train perplexity: 4.173140048980713
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.04s/it]
INFO:root:eval mean loss: 11286.926246279761
INFO:root:eval perplexity: 10.34286117553711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [16:13:57<11:12:41, 492.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7220.715772172095
INFO:root:current train perplexity4.148184776306152
INFO:root:current mean train loss 7225.918040136147
INFO:root:current train perplexity4.15540075302124
INFO:root:current mean train loss 7241.0480677755995
INFO:root:current train perplexity4.159112453460693
INFO:root:current mean train loss 7245.726509855122
INFO:root:current train perplexity4.166571617126465
INFO:root:current mean train loss 7243.555284633758
INFO:root:current train perplexity4.169898986816406


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.64s/it]
INFO:root:final mean train loss: 7238.749909431704
INFO:root:final train perplexity: 4.170133590698242
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.73s/it]
INFO:root:eval mean loss: 11288.794041224888
INFO:root:eval perplexity: 10.346857070922852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [16:22:07<11:03:49, 491.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7224.918756510417
INFO:root:current train perplexity4.156423091888428
INFO:root:current mean train loss 7208.462606026786
INFO:root:current train perplexity4.158894062042236
INFO:root:current mean train loss 7217.858682528409
INFO:root:current train perplexity4.157339096069336
INFO:root:current mean train loss 7234.447942708333
INFO:root:current train perplexity4.163387775421143
INFO:root:current mean train loss 7237.325941611842
INFO:root:current train perplexity4.16506814956665


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:53<00:00, 413.05s/it]
INFO:root:final mean train loss: 7232.318809263168
INFO:root:final train perplexity: 4.164846897125244
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.60s/it]
INFO:root:eval mean loss: 11293.133492606026
INFO:root:eval perplexity: 10.356156349182129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [16:30:13<10:53:27, 490.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7201.677394432358
INFO:root:current train perplexity4.139596462249756
INFO:root:current mean train loss 7215.052453408694
INFO:root:current train perplexity4.147578239440918
INFO:root:current mean train loss 7215.571929603495
INFO:root:current train perplexity4.154958248138428
INFO:root:current mean train loss 7231.262364208856
INFO:root:current train perplexity4.161722660064697
INFO:root:current mean train loss 7230.962221914144
INFO:root:current train perplexity4.159626007080078


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.95s/it]
INFO:root:final mean train loss: 7226.050821611958
INFO:root:final train perplexity: 4.159700393676758
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.17s/it]
INFO:root:eval mean loss: 11296.580519903273
INFO:root:eval perplexity: 10.363546371459961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [16:38:37<10:50:24, 493.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7214.174086972892
INFO:root:current train perplexity4.151453971862793
INFO:root:current mean train loss 7199.257983265027
INFO:root:current train perplexity4.139907360076904
INFO:root:current mean train loss 7206.771039228136
INFO:root:current train perplexity4.139715194702148
INFO:root:current mean train loss 7212.244840537288
INFO:root:current train perplexity4.142760753631592
INFO:root:current mean train loss 7223.214133063212
INFO:root:current train perplexity4.151089191436768


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.60s/it]
INFO:root:final mean train loss: 7218.656467560799
INFO:root:final train perplexity: 4.153637886047363
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.43s/it]
INFO:root:eval mean loss: 11292.216267903646
INFO:root:eval perplexity: 10.354190826416016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [16:46:46<10:40:20, 492.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7199.184031519397
INFO:root:current train perplexity4.156444549560547
INFO:root:current mean train loss 7215.678055543951
INFO:root:current train perplexity4.154041767120361
INFO:root:current mean train loss 7224.180796766115
INFO:root:current train perplexity4.152305603027344
INFO:root:current mean train loss 7225.136359163033
INFO:root:current train perplexity4.152493476867676
INFO:root:current mean train loss 7219.618624270085
INFO:root:current train perplexity4.149921417236328


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.57s/it]
INFO:root:final mean train loss: 7214.125326833418
INFO:root:final train perplexity: 4.14992618560791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it]
INFO:root:eval mean loss: 11300.033828008742
INFO:root:eval perplexity: 10.370962142944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [16:54:54<10:30:28, 491.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7160.845316792583
INFO:root:current train perplexity4.130186557769775
INFO:root:current mean train loss 7197.390377024705
INFO:root:current train perplexity4.141664028167725
INFO:root:current mean train loss 7219.881955071413
INFO:root:current train perplexity4.149386405944824
INFO:root:current mean train loss 7218.489927169917
INFO:root:current train perplexity4.144749641418457
INFO:root:current mean train loss 7215.751277884738
INFO:root:current train perplexity4.146564483642578

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.48s/it]
INFO:root:final mean train loss: 7209.53040141444
INFO:root:final train perplexity: 4.1461663246154785
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.69s/it]
INFO:root:eval mean loss: 11305.824532645089
INFO:root:eval perplexity: 10.383397102355957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [17:03:12<10:24:45, 493.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7200.3564453125
INFO:root:current train perplexity4.128180027008057
INFO:root:current mean train loss 7206.721965144231
INFO:root:current train perplexity4.128206729888916
INFO:root:current mean train loss 7208.670612089512
INFO:root:current train perplexity4.136719703674316
INFO:root:current mean train loss 7216.090977304193
INFO:root:current train perplexity4.138191223144531
INFO:root:current mean train loss 7209.759276357323
INFO:root:current train perplexity4.14143180847168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.59s/it]
INFO:root:final mean train loss: 7203.482089134955
INFO:root:final train perplexity: 4.1412224769592285
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.47s/it]
INFO:root:eval mean loss: 11303.580011276972
INFO:root:eval perplexity: 10.378572463989258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [17:11:29<10:17:54, 494.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7187.075737847223
INFO:root:current train perplexity4.132551193237305
INFO:root:current mean train loss 7196.037234512406
INFO:root:current train perplexity4.1347784996032715
INFO:root:current mean train loss 7199.4964366900085
INFO:root:current train perplexity4.131187438964844
INFO:root:current mean train loss 7202.025579084429
INFO:root:current train perplexity4.134132385253906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.89s/it]
INFO:root:final mean train loss: 7198.350839922505
INFO:root:final train perplexity: 4.137032985687256
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.97s/it]
INFO:root:eval mean loss: 11301.830240885416
INFO:root:eval perplexity: 10.374814987182617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [17:19:41<10:08:51, 493.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7179.55810546875
INFO:root:current train perplexity4.1656599044799805
INFO:root:current mean train loss 7188.4906325849515
INFO:root:current train perplexity4.122453689575195
INFO:root:current mean train loss 7175.054699526631
INFO:root:current train perplexity4.125190258026123
INFO:root:current mean train loss 7185.916226730095
INFO:root:current train perplexity4.126254081726074
INFO:root:current mean train loss 7197.828172253025
INFO:root:current train perplexity4.132630825042725

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.59s/it]
INFO:root:final mean train loss: 7191.799008277155
INFO:root:final train perplexity: 4.131689071655273
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.26s/it]
INFO:root:eval mean loss: 11314.592689150855
INFO:root:eval perplexity: 10.402260780334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [17:27:47<9:57:52, 491.40s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7111.515694754465
INFO:root:current train perplexity4.141444206237793
INFO:root:current mean train loss 7169.833920487734
INFO:root:current train perplexity4.139440059661865
INFO:root:current mean train loss 7183.810334578804
INFO:root:current train perplexity4.127171516418457
INFO:root:current mean train loss 7182.658331954906
INFO:root:current train perplexity4.123440265655518
INFO:root:current mean train loss 7183.381589997312
INFO:root:current train perplexity4.1225152015686035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.47s/it]
INFO:root:final mean train loss: 7186.966086110761
INFO:root:final train perplexity: 4.127752304077148
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.98s/it]
INFO:root:eval mean loss: 11312.033415294829
INFO:root:eval perplexity: 10.396753311157227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [17:36:05<9:51:57, 493.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7143.157670454545
INFO:root:current train perplexity4.142683982849121
INFO:root:current mean train loss 7194.50439013232
INFO:root:current train perplexity4.125204563140869
INFO:root:current mean train loss 7190.650649807464
INFO:root:current train perplexity4.124868869781494
INFO:root:current mean train loss 7184.013339027331
INFO:root:current train perplexity4.124380111694336
INFO:root:current mean train loss 7182.9795753497565
INFO:root:current train perplexity4.122708320617676

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.71s/it]
INFO:root:final mean train loss: 7180.642970915764
INFO:root:final train perplexity: 4.122607231140137
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.68s/it]
INFO:root:eval mean loss: 11311.338634672618
INFO:root:eval perplexity: 10.395256996154785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [17:44:13<9:41:50, 491.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7209.945084635417
INFO:root:current train perplexity4.103543281555176
INFO:root:current mean train loss 7197.042671535326
INFO:root:current train perplexity4.128054618835449
INFO:root:current mean train loss 7187.188896711483
INFO:root:current train perplexity4.126444339752197
INFO:root:current mean train loss 7191.59130859375
INFO:root:current train perplexity4.127026081085205
INFO:root:current mean train loss 7183.177384930346
INFO:root:current train perplexity4.121203899383545

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.20s/it]
INFO:root:final mean train loss: 7178.854296284338
INFO:root:final train perplexity: 4.121152877807617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it]
INFO:root:eval mean loss: 11319.738874162946
INFO:root:eval perplexity: 10.413347244262695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [17:52:24<9:33:41, 491.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7186.429122121711
INFO:root:current train perplexity4.120790958404541
INFO:root:current mean train loss 7187.390501903887
INFO:root:current train perplexity4.120006561279297
INFO:root:current mean train loss 7184.483097442209
INFO:root:current train perplexity4.120990753173828
INFO:root:current mean train loss 7166.118133449256
INFO:root:current train perplexity4.114346027374268
INFO:root:current mean train loss 7173.765005034308
INFO:root:current train perplexity4.113433361053467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.23s/it]
INFO:root:final mean train loss: 7171.409159014302
INFO:root:final train perplexity: 4.115104675292969
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.45s/it]
INFO:root:eval mean loss: 11316.381344749814
INFO:root:eval perplexity: 10.406112670898438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [18:00:37<9:25:54, 492.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7101.989979619565
INFO:root:current train perplexity4.11077880859375
INFO:root:current mean train loss 7173.102812976372
INFO:root:current train perplexity4.119110107421875
INFO:root:current mean train loss 7166.398943298066
INFO:root:current train perplexity4.1055169105529785
INFO:root:current mean train loss 7169.46429651219
INFO:root:current train perplexity4.110114097595215
INFO:root:current mean train loss 7169.060384114583
INFO:root:current train perplexity4.110218048095703

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.97s/it]
INFO:root:final mean train loss: 7167.525083480343
INFO:root:final train perplexity: 4.111952781677246
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.79s/it]
INFO:root:eval mean loss: 11322.406127929688
INFO:root:eval perplexity: 10.419096946716309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [18:08:48<9:17:10, 491.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7113.7847583912035
INFO:root:current train perplexity4.092965126037598
INFO:root:current mean train loss 7121.977223794292
INFO:root:current train perplexity4.101068496704102
INFO:root:current mean train loss 7135.415690821173
INFO:root:current train perplexity4.093670845031738
INFO:root:current mean train loss 7148.489266771789
INFO:root:current train perplexity4.100861072540283
INFO:root:current mean train loss 7158.286956143882
INFO:root:current train perplexity4.1026506423950195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.99s/it]
INFO:root:final mean train loss: 7162.89701695596
INFO:root:final train perplexity: 4.108200550079346
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.05s/it]
INFO:root:eval mean loss: 11322.093194870722
INFO:root:eval perplexity: 10.418423652648926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [18:16:58<9:08:34, 491.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7128.0358020413305
INFO:root:current train perplexity4.10162878036499
INFO:root:current mean train loss 7143.294165225429
INFO:root:current train perplexity4.100799083709717
INFO:root:current mean train loss 7156.31640625
INFO:root:current train perplexity4.102845668792725
INFO:root:current mean train loss 7164.588659188538
INFO:root:current train perplexity4.101399898529053
INFO:root:current mean train loss 7157.477970698594
INFO:root:current train perplexity4.099472999572754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.71s/it]
INFO:root:final mean train loss: 7155.792440106792
INFO:root:final train perplexity: 4.102446556091309
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.89s/it]
INFO:root:eval mean loss: 11325.716988699776
INFO:root:eval perplexity: 10.426240921020508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [18:25:21<9:04:16, 494.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7163.604547991072
INFO:root:current train perplexity4.101256847381592
INFO:root:current mean train loss 7127.284139901621
INFO:root:current train perplexity4.080643177032471
INFO:root:current mean train loss 7145.522637549867
INFO:root:current train perplexity4.0932297706604
INFO:root:current mean train loss 7168.406837395056
INFO:root:current train perplexity4.104829788208008
INFO:root:current mean train loss 7161.887883890086
INFO:root:current train perplexity4.10200834274292

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.62s/it]
INFO:root:final mean train loss: 7152.342414117628
INFO:root:final train perplexity: 4.099655628204346
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.58s/it]
INFO:root:eval mean loss: 11330.15808686756
INFO:root:eval perplexity: 10.435830116271973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [18:33:46<8:59:16, 497.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7174.759690504808
INFO:root:current train perplexity4.09997034072876
INFO:root:current mean train loss 7144.475213579137
INFO:root:current train perplexity4.097756385803223
INFO:root:current mean train loss 7160.2020238134155
INFO:root:current train perplexity4.102146148681641
INFO:root:current mean train loss 7167.178520810287
INFO:root:current train perplexity4.100256443023682
INFO:root:current mean train loss 7160.699610264806
INFO:root:current train perplexity4.09673547744751

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.61s/it]
INFO:root:final mean train loss: 7147.095751362463
INFO:root:final train perplexity: 4.095414638519287
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.64s/it]
INFO:root:eval mean loss: 11339.650931222099
INFO:root:eval perplexity: 10.456357955932617
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [18:42:04<8:50:52, 497.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7104.6186977652615
INFO:root:current train perplexity4.078497886657715
INFO:root:current mean train loss 7131.627062390734
INFO:root:current train perplexity4.081496715545654
INFO:root:current mean train loss 7155.9735865965795
INFO:root:current train perplexity4.092061996459961
INFO:root:current mean train loss 7152.689713636571
INFO:root:current train perplexity4.089662551879883
INFO:root:current mean train loss 7144.399739215928
INFO:root:current train perplexity4.086827754974365

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.04s/it]
INFO:root:final mean train loss: 7141.127576274256
INFO:root:final train perplexity: 4.0905961990356445
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.12s/it]
INFO:root:eval mean loss: 11334.421238490513
INFO:root:eval perplexity: 10.445042610168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [18:50:14<8:40:17, 495.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7097.486483959441
INFO:root:current train perplexity4.062405586242676
INFO:root:current mean train loss 7133.627514482356
INFO:root:current train perplexity4.08073091506958
INFO:root:current mean train loss 7131.368187784666
INFO:root:current train perplexity4.0847392082214355
INFO:root:current mean train loss 7132.675701042417
INFO:root:current train perplexity4.085049152374268
INFO:root:current mean train loss 7141.00007318757
INFO:root:current train perplexity4.088120460510254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:55<00:00, 415.65s/it]
INFO:root:final mean train loss: 7137.290862052671
INFO:root:final train perplexity: 4.08750057220459
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.76s/it]
INFO:root:eval mean loss: 11336.912493024554
INFO:root:eval perplexity: 10.450430870056152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [18:58:23<8:29:58, 493.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7088.209396063113
INFO:root:current train perplexity4.047552108764648
INFO:root:current mean train loss 7111.472756493171
INFO:root:current train perplexity4.069690227508545
INFO:root:current mean train loss 7128.270721800299
INFO:root:current train perplexity4.071014404296875
INFO:root:current mean train loss 7130.579356136485
INFO:root:current train perplexity4.074887275695801
INFO:root:current mean train loss 7135.571660416089
INFO:root:current train perplexity4.0802483558654785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.11s/it]
INFO:root:final mean train loss: 7132.063101491621
INFO:root:final train perplexity: 4.083288192749023
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.63s/it]
INFO:root:eval mean loss: 11335.875
INFO:root:eval perplexity: 10.448188781738281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [19:06:38<8:22:19, 494.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7128.481782670455
INFO:root:current train perplexity4.08006477355957
INFO:root:current mean train loss 7100.448434349798
INFO:root:current train perplexity4.055922508239746
INFO:root:current mean train loss 7113.811245787378
INFO:root:current train perplexity4.071149826049805
INFO:root:current mean train loss 7122.663893320862
INFO:root:current train perplexity4.0713276863098145
INFO:root:current mean train loss 7132.269792024382
INFO:root:current train perplexity4.077994346618652

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.86s/it]
INFO:root:final mean train loss: 7127.482043850807
INFO:root:final train perplexity: 4.079599857330322
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.31s/it]
INFO:root:eval mean loss: 11340.111037481398
INFO:root:eval perplexity: 10.457352638244629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [19:14:53<8:14:13, 494.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7086.263779462394
INFO:root:current train perplexity4.061320781707764
INFO:root:current mean train loss 7100.275845125786
INFO:root:current train perplexity4.070054054260254
INFO:root:current mean train loss 7115.203009999396
INFO:root:current train perplexity4.072944164276123
INFO:root:current mean train loss 7117.441113825296
INFO:root:current train perplexity4.072227478027344
INFO:root:current mean train loss 7123.89769603588
INFO:root:current train perplexity4.07291316986084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.77s/it]
INFO:root:final mean train loss: 7124.139773461126
INFO:root:final train perplexity: 4.076911449432373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.32s/it]
INFO:root:eval mean loss: 11337.300880068824
INFO:root:eval perplexity: 10.45127010345459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [19:23:01<8:04:16, 492.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7113.223400297619
INFO:root:current train perplexity4.078202724456787
INFO:root:current mean train loss 7116.415563290836
INFO:root:current train perplexity4.080795764923096
INFO:root:current mean train loss 7124.865173107771
INFO:root:current train perplexity4.0756707191467285
INFO:root:current mean train loss 7123.826186671402
INFO:root:current train perplexity4.07538366317749
INFO:root:current mean train loss 7124.208369541374
INFO:root:current train perplexity4.074453353881836

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.71s/it]
INFO:root:final mean train loss: 7120.322232154108
INFO:root:final train perplexity: 4.073842525482178
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.81s/it]
INFO:root:eval mean loss: 11345.563929966518
INFO:root:eval perplexity: 10.469164848327637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [19:31:18<7:57:08, 493.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7173.556494869403
INFO:root:current train perplexity4.057961940765381
INFO:root:current mean train loss 7148.523700645584
INFO:root:current train perplexity4.065067291259766
INFO:root:current mean train loss 7142.734671260534
INFO:root:current train perplexity4.072693347930908
INFO:root:current mean train loss 7124.607107884877
INFO:root:current train perplexity4.066001892089844
INFO:root:current mean train loss 7119.537931193121
INFO:root:current train perplexity4.070033073425293

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.97s/it]
INFO:root:final mean train loss: 7117.4043825211065
INFO:root:final train perplexity: 4.071497440338135
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.48s/it]
INFO:root:eval mean loss: 11346.082101004464
INFO:root:eval perplexity: 10.470285415649414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [19:39:41<7:51:41, 496.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7088.626574878961
INFO:root:current train perplexity4.040149688720703
INFO:root:current mean train loss 7098.806771975512
INFO:root:current train perplexity4.048009872436523
INFO:root:current mean train loss 7105.459476259802
INFO:root:current train perplexity4.055785655975342
INFO:root:current mean train loss 7106.749145836843
INFO:root:current train perplexity4.061172008514404
INFO:root:current mean train loss 7113.871917918988
INFO:root:current train perplexity4.067716598510742

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.94s/it]
INFO:root:final mean train loss: 7113.053521925403
INFO:root:final train perplexity: 4.068005561828613
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.15s/it]
INFO:root:eval mean loss: 11344.455043247768
INFO:root:eval perplexity: 10.466758728027344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [19:47:57<7:43:22, 496.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7105.8182161458335
INFO:root:current train perplexity4.056211948394775
INFO:root:current mean train loss 7099.085290178571
INFO:root:current train perplexity4.055211067199707
INFO:root:current mean train loss 7103.821360085227
INFO:root:current train perplexity4.057872772216797
INFO:root:current mean train loss 7117.53926953125
INFO:root:current train perplexity4.065774440765381
INFO:root:current mean train loss 7110.726291118421
INFO:root:current train perplexity4.063780307769775

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:52<00:00, 412.27s/it]
INFO:root:final mean train loss: 7108.964683286606
INFO:root:final train perplexity: 4.064725399017334
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.51s/it]
INFO:root:eval mean loss: 11347.421389625186
INFO:root:eval perplexity: 10.473188400268555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [19:56:05<7:32:42, 493.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7107.749011075949
INFO:root:current train perplexity4.052401065826416
INFO:root:current mean train loss 7098.573441318959
INFO:root:current train perplexity4.057079315185547
INFO:root:current mean train loss 7105.517480118728
INFO:root:current train perplexity4.059013843536377
INFO:root:current mean train loss 7111.097214349027
INFO:root:current train perplexity4.061175346374512
INFO:root:current mean train loss 7108.702645893137
INFO:root:current train perplexity4.06040620803833

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:15<00:00, 435.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:15<00:00, 435.14s/it]
INFO:root:final mean train loss: 7104.0121548560355
INFO:root:final train perplexity: 4.060756206512451
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.12s/it]
INFO:root:eval mean loss: 11347.916657947359
INFO:root:eval perplexity: 10.474260330200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [20:04:36<7:29:01, 498.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7126.789615493223
INFO:root:current train perplexity4.048412799835205
INFO:root:current mean train loss 7118.11393495987
INFO:root:current train perplexity4.04818058013916
INFO:root:current mean train loss 7099.756785901613
INFO:root:current train perplexity4.04831600189209
INFO:root:current mean train loss 7100.148510168489
INFO:root:current train perplexity4.048039436340332
INFO:root:current mean train loss 7105.203321121247
INFO:root:current train perplexity4.0555901527404785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.08s/it]
INFO:root:final mean train loss: 7099.254511679373
INFO:root:final train perplexity: 4.056946754455566
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.95s/it]
INFO:root:eval mean loss: 11350.803873697916
INFO:root:eval perplexity: 10.480522155761719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [20:12:48<7:19:03, 497.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7090.919192259339
INFO:root:current train perplexity4.05045223236084
INFO:root:current mean train loss 7090.919493649732
INFO:root:current train perplexity4.0501322746276855
INFO:root:current mean train loss 7093.817774118032
INFO:root:current train perplexity4.0477471351623535
INFO:root:current mean train loss 7101.139652222626
INFO:root:current train perplexity4.048644542694092
INFO:root:current mean train loss 7103.232350688206
INFO:root:current train perplexity4.054518222808838

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.22s/it]
INFO:root:final mean train loss: 7096.897734611265
INFO:root:final train perplexity: 4.055060863494873
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.21s/it]
INFO:root:eval mean loss: 11353.489292689732
INFO:root:eval perplexity: 10.486351013183594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [20:20:59<7:09:07, 495.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7094.375321943681
INFO:root:current train perplexity4.055857181549072
INFO:root:current mean train loss 7100.805988731185
INFO:root:current train perplexity4.051540374755859
INFO:root:current mean train loss 7097.239736026096
INFO:root:current train perplexity4.053011894226074
INFO:root:current mean train loss 7094.57225438579
INFO:root:current train perplexity4.050151824951172
INFO:root:current mean train loss 7099.088127307154
INFO:root:current train perplexity4.052462577819824

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.46s/it]
INFO:root:final mean train loss: 7094.599672379032
INFO:root:final train perplexity: 4.053223133087158
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.36s/it]
INFO:root:eval mean loss: 11355.4128679548
INFO:root:eval perplexity: 10.490527153015137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [20:29:16<7:01:13, 495.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7075.075848067434
INFO:root:current train perplexity4.032269477844238
INFO:root:current mean train loss 7083.015532351763
INFO:root:current train perplexity4.038211822509766
INFO:root:current mean train loss 7076.253237552966
INFO:root:current train perplexity4.03756046295166
INFO:root:current mean train loss 7086.560313241693
INFO:root:current train perplexity4.0453267097473145
INFO:root:current mean train loss 7093.040433633207
INFO:root:current train perplexity4.047821521759033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.39s/it]
INFO:root:final mean train loss: 7087.774238832535
INFO:root:final train perplexity: 4.047770023345947
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.20s/it]
INFO:root:eval mean loss: 11353.54444812593
INFO:root:eval perplexity: 10.486471176147461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [20:37:34<6:53:45, 496.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7082.171815814394
INFO:root:current train perplexity4.040030002593994
INFO:root:current mean train loss 7076.241873429648
INFO:root:current train perplexity4.03605318069458
INFO:root:current mean train loss 7079.976867879912
INFO:root:current train perplexity4.040024280548096
INFO:root:current mean train loss 7074.7646680177
INFO:root:current train perplexity4.041086196899414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:11<00:00, 431.35s/it]
INFO:root:final mean train loss: 7084.831196446573
INFO:root:final train perplexity: 4.045420169830322
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.09s/it]
INFO:root:eval mean loss: 11358.919026692709
INFO:root:eval perplexity: 10.498141288757324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [20:46:04<6:48:46, 500.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7132.863118489583
INFO:root:current train perplexity4.0205397605896
INFO:root:current mean train loss 7081.769071412318
INFO:root:current train perplexity4.043514728546143
INFO:root:current mean train loss 7073.844560594982
INFO:root:current train perplexity4.042811393737793
INFO:root:current mean train loss 7073.076961504744
INFO:root:current train perplexity4.036813735961914
INFO:root:current mean train loss 7086.956914934864
INFO:root:current train perplexity4.040907382965088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.33s/it]
INFO:root:final mean train loss: 7080.052308113344
INFO:root:final train perplexity: 4.0416083335876465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.36s/it]
INFO:root:eval mean loss: 11361.848356701079
INFO:root:eval perplexity: 10.504510879516602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [20:54:23<6:40:02, 500.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7108.939383370535
INFO:root:current train perplexity4.046072006225586
INFO:root:current mean train loss 7072.457660995911
INFO:root:current train perplexity4.033711910247803
INFO:root:current mean train loss 7073.970691330767
INFO:root:current train perplexity4.033013343811035
INFO:root:current mean train loss 7083.515882659813
INFO:root:current train perplexity4.035134792327881
INFO:root:current mean train loss 7084.7189047623615
INFO:root:current train perplexity4.040567398071289

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.70s/it]
INFO:root:final mean train loss: 7080.058599656628
INFO:root:final train perplexity: 4.041614055633545
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.06s/it]
INFO:root:eval mean loss: 11359.49500093006
INFO:root:eval perplexity: 10.499395370483398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [21:02:35<6:29:48, 497.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6966.169477982955
INFO:root:current train perplexity3.9710965156555176
INFO:root:current mean train loss 7054.196856524493
INFO:root:current train perplexity4.014599800109863
INFO:root:current mean train loss 7058.607500555391
INFO:root:current train perplexity4.021155834197998
INFO:root:current mean train loss 7065.699551597669
INFO:root:current train perplexity4.027350902557373
INFO:root:current mean train loss 7079.21704636177
INFO:root:current train perplexity4.033295631408691

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.08s/it]
INFO:root:final mean train loss: 7073.449802521736
INFO:root:final train perplexity: 4.036347389221191
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.24s/it]
INFO:root:eval mean loss: 11362.95935639881
INFO:root:eval perplexity: 10.506924629211426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [21:11:01<6:23:17, 499.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7135.9125
INFO:root:current train perplexity4.084881782531738
INFO:root:current mean train loss 7064.146641474184
INFO:root:current train perplexity4.025750160217285
INFO:root:current mean train loss 7061.886689226018
INFO:root:current train perplexity4.033558368682861
INFO:root:current mean train loss 7068.504820808532
INFO:root:current train perplexity4.037353515625
INFO:root:current mean train loss 7081.122287980045
INFO:root:current train perplexity4.0370001792907715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.67s/it]
INFO:root:final mean train loss: 7072.276852515436
INFO:root:final train perplexity: 4.035414218902588
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.54s/it]
INFO:root:eval mean loss: 11370.974525088355
INFO:root:eval perplexity: 10.524372100830078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [21:19:19<6:14:33, 499.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7152.3536184210525
INFO:root:current train perplexity4.035065174102783
INFO:root:current mean train loss 7080.9202296152835
INFO:root:current train perplexity4.029898643493652
INFO:root:current mean train loss 7065.243215343179
INFO:root:current train perplexity4.0348734855651855
INFO:root:current mean train loss 7065.00799158748
INFO:root:current train perplexity4.028393745422363
INFO:root:current mean train loss 7070.491158496793
INFO:root:current train perplexity4.0268778800964355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.72s/it]
INFO:root:final mean train loss: 7067.345846852949
INFO:root:final train perplexity: 4.031491279602051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.47s/it]
INFO:root:eval mean loss: 11361.46843029204
INFO:root:eval perplexity: 10.503684043884277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [21:27:40<6:06:42, 500.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7064.529169497283
INFO:root:current train perplexity3.986973524093628
INFO:root:current mean train loss 7056.140823488313
INFO:root:current train perplexity4.01181697845459
INFO:root:current mean train loss 7066.266774541059
INFO:root:current train perplexity4.022047996520996
INFO:root:current mean train loss 7055.574513532798
INFO:root:current train perplexity4.021312236785889
INFO:root:current mean train loss 7069.606378361406
INFO:root:current train perplexity4.027220726013184

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.01s/it]
INFO:root:final mean train loss: 7066.130119077621
INFO:root:final train perplexity: 4.030523777008057
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.93s/it]
INFO:root:eval mean loss: 11365.93530564081
INFO:root:eval perplexity: 10.513397216796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [21:35:55<5:57:14, 498.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7009.976869936342
INFO:root:current train perplexity4.040750503540039
INFO:root:current mean train loss 7030.905538724163
INFO:root:current train perplexity4.018181800842285
INFO:root:current mean train loss 7050.7509163339755
INFO:root:current train perplexity4.024580001831055
INFO:root:current mean train loss 7065.735875680906
INFO:root:current train perplexity4.027528762817383
INFO:root:current mean train loss 7069.558783573624
INFO:root:current train perplexity4.029408931732178

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.62s/it]
INFO:root:final mean train loss: 7061.44046906502
INFO:root:final train perplexity: 4.026796817779541
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.80s/it]
INFO:root:eval mean loss: 11366.392281668526
INFO:root:eval perplexity: 10.514395713806152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [21:44:13<5:48:42, 498.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7099.098538306452
INFO:root:current train perplexity4.02532434463501
INFO:root:current mean train loss 7073.76648228769
INFO:root:current train perplexity4.015807151794434
INFO:root:current mean train loss 7062.695039823458
INFO:root:current train perplexity4.027246475219727
INFO:root:current mean train loss 7061.556941559667
INFO:root:current train perplexity4.028738021850586
INFO:root:current mean train loss 7066.606908669881
INFO:root:current train perplexity4.0273118019104

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.33s/it]
INFO:root:final mean train loss: 7059.76914141255
INFO:root:final train perplexity: 4.0254693031311035
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.17s/it]
INFO:root:eval mean loss: 11371.720839727492
INFO:root:eval perplexity: 10.52599811553955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [21:52:31<5:40:30, 498.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7066.019112723215
INFO:root:current train perplexity4.028467178344727
INFO:root:current mean train loss 7050.730088975694
INFO:root:current train perplexity4.013018608093262
INFO:root:current mean train loss 7048.825986951463
INFO:root:current train perplexity4.018290042877197
INFO:root:current mean train loss 7051.032907241138
INFO:root:current train perplexity4.01560115814209
INFO:root:current mean train loss 7058.8416689116375
INFO:root:current train perplexity4.022324562072754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.75s/it]
INFO:root:final mean train loss: 7055.675967308783
INFO:root:final train perplexity: 4.022220611572266
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.70s/it]
INFO:root:eval mean loss: 11371.939011346727
INFO:root:eval perplexity: 10.526473045349121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [22:00:44<5:31:09, 496.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7042.773212139423
INFO:root:current train perplexity4.001797199249268
INFO:root:current mean train loss 7051.174920610387
INFO:root:current train perplexity4.020506858825684
INFO:root:current mean train loss 7059.154468488494
INFO:root:current train perplexity4.018551826477051
INFO:root:current mean train loss 7052.28961271663
INFO:root:current train perplexity4.018029689788818
INFO:root:current mean train loss 7052.682162273989
INFO:root:current train perplexity4.019923686981201

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.69s/it]
INFO:root:final mean train loss: 7052.598805089151
INFO:root:final train perplexity: 4.019779205322266
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 68.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.01s/it]
INFO:root:eval mean loss: 11381.674252464658
INFO:root:eval perplexity: 10.54770565032959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [22:09:02<5:23:01, 496.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7046.262672601744
INFO:root:current train perplexity3.994948148727417
INFO:root:current mean train loss 7047.805725524476
INFO:root:current train perplexity4.0028181076049805
INFO:root:current mean train loss 7050.215649514532
INFO:root:current train perplexity4.010792255401611
INFO:root:current mean train loss 7049.659620991254
INFO:root:current train perplexity4.0095534324646
INFO:root:current mean train loss 7050.338894742876
INFO:root:current train perplexity4.0131659507751465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.80s/it]
INFO:root:final mean train loss: 7049.070912022745
INFO:root:final train perplexity: 4.0169830322265625
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.37s/it]
INFO:root:eval mean loss: 11376.686863490513
INFO:root:eval perplexity: 10.536822319030762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [22:17:17<5:14:25, 496.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7044.40546043883
INFO:root:current train perplexity4.018418312072754
INFO:root:current mean train loss 7051.651151280825
INFO:root:current train perplexity4.0134596824646
INFO:root:current mean train loss 7062.862025952049
INFO:root:current train perplexity4.013647079467773
INFO:root:current mean train loss 7056.545155461996
INFO:root:current train perplexity4.009773254394531
INFO:root:current mean train loss 7052.665809170512
INFO:root:current train perplexity4.012955665588379

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.27s/it]
INFO:root:final mean train loss: 7045.7902674521165
INFO:root:final train perplexity: 4.0143842697143555
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.95s/it]
INFO:root:eval mean loss: 11379.257777622768
INFO:root:eval perplexity: 10.542430877685547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [22:25:29<5:05:19, 495.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6979.488731234681
INFO:root:current train perplexity4.0056352615356445
INFO:root:current mean train loss 7036.286646962955
INFO:root:current train perplexity4.020859241485596
INFO:root:current mean train loss 7037.502896616658
INFO:root:current train perplexity4.017943859100342
INFO:root:current mean train loss 7041.694024327813
INFO:root:current train perplexity4.011803150177002
INFO:root:current mean train loss 7048.984577458079
INFO:root:current train perplexity4.012312412261963

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.76s/it]
INFO:root:final mean train loss: 7042.469322942919
INFO:root:final train perplexity: 4.01175594329834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.35s/it]
INFO:root:eval mean loss: 11381.929693312872
INFO:root:eval perplexity: 10.548264503479004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [22:33:43<4:56:51, 494.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7053.988600852273
INFO:root:current train perplexity4.0019378662109375
INFO:root:current mean train loss 7057.119247731855
INFO:root:current train perplexity4.015786170959473
INFO:root:current mean train loss 7059.848602175245
INFO:root:current train perplexity4.023365497589111
INFO:root:current mean train loss 7048.950397502201
INFO:root:current train perplexity4.015955448150635
INFO:root:current mean train loss 7051.447657323146
INFO:root:current train perplexity4.0149993896484375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.39s/it]
INFO:root:final mean train loss: 7042.571467245779
INFO:root:final train perplexity: 4.011836528778076
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.97s/it]
INFO:root:eval mean loss: 11379.806100027901
INFO:root:eval perplexity: 10.543624877929688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [22:42:02<4:49:19, 495.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7046.779851363877
INFO:root:current train perplexity4.011289119720459
INFO:root:current mean train loss 7053.858346231329
INFO:root:current train perplexity4.013911247253418
INFO:root:current mean train loss 7041.699263996139
INFO:root:current train perplexity4.011236667633057
INFO:root:current mean train loss 7050.634651375348
INFO:root:current train perplexity4.011719226837158
INFO:root:current mean train loss 7045.651077835648
INFO:root:current train perplexity4.009176254272461

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.35s/it]
INFO:root:final mean train loss: 7038.462318666519
INFO:root:final train perplexity: 4.008585453033447
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.15s/it]
INFO:root:eval mean loss: 11383.64639718192
INFO:root:eval perplexity: 10.552014350891113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [22:50:16<4:40:45, 495.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7038.5346757192465
INFO:root:current train perplexity3.99889874458313
INFO:root:current mean train loss 7037.7891883148
INFO:root:current train perplexity4.006161689758301
INFO:root:current mean train loss 7048.351529081511
INFO:root:current train perplexity4.003332138061523
INFO:root:current mean train loss 7048.23941384728
INFO:root:current train perplexity4.005849838256836
INFO:root:current mean train loss 7042.639296200054
INFO:root:current train perplexity4.007185459136963

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:07<00:00, 427.15s/it]
INFO:root:final mean train loss: 7035.497445383379
INFO:root:final train perplexity: 4.006241798400879
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.43s/it]
INFO:root:eval mean loss: 11386.808384486607
INFO:root:eval perplexity: 10.558919906616211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [22:58:39<4:33:47, 497.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7040.616502448694
INFO:root:current train perplexity4.0045037269592285
INFO:root:current mean train loss 7026.458572113585
INFO:root:current train perplexity3.995378255844116
INFO:root:current mean train loss 7041.162888430477
INFO:root:current train perplexity4.001368999481201
INFO:root:current mean train loss 7042.39188096049
INFO:root:current train perplexity4.002638816833496
INFO:root:current mean train loss 7039.372050446667
INFO:root:current train perplexity4.003861427307129

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.82s/it]
INFO:root:final mean train loss: 7033.087810885521
INFO:root:final train perplexity: 4.00433874130249
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.72s/it]
INFO:root:eval mean loss: 11381.092895507812
INFO:root:eval perplexity: 10.546438217163086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [23:07:02<4:26:18, 499.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7087.794942506602
INFO:root:current train perplexity4.026613712310791
INFO:root:current mean train loss 7031.0596302768645
INFO:root:current train perplexity4.002111911773682
INFO:root:current mean train loss 7027.186069389991
INFO:root:current train perplexity3.995898962020874
INFO:root:current mean train loss 7039.728365587096
INFO:root:current train perplexity4.002808570861816
INFO:root:current mean train loss 7038.995593028463
INFO:root:current train perplexity4.003840923309326

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.14s/it]
INFO:root:final mean train loss: 7031.417244203629
INFO:root:final train perplexity: 4.003018856048584
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.32s/it]
INFO:root:eval mean loss: 11383.10142299107
INFO:root:eval perplexity: 10.550822257995605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [23:15:13<4:16:40, 496.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7023.962552083333
INFO:root:current train perplexity3.988640785217285
INFO:root:current mean train loss 7026.129162946429
INFO:root:current train perplexity3.999168872833252
INFO:root:current mean train loss 7014.243190696023
INFO:root:current train perplexity3.993272304534912
INFO:root:current mean train loss 7027.687350260417
INFO:root:current train perplexity3.998832941055298
INFO:root:current mean train loss 7031.688284333882
INFO:root:current train perplexity3.9984705448150635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.79s/it]
INFO:root:final mean train loss: 7027.898606823337
INFO:root:final train perplexity: 4.000241279602051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:09<00:00, 69.44s/it]
INFO:root:eval mean loss: 11390.84635997954
INFO:root:eval perplexity: 10.567749977111816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [23:23:30<4:08:25, 496.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6983.1318359375
INFO:root:current train perplexity3.9662387371063232
INFO:root:current mean train loss 7005.540423686278
INFO:root:current train perplexity3.9915308952331543
INFO:root:current mean train loss 7013.1757584985435
INFO:root:current train perplexity3.994718313217163
INFO:root:current mean train loss 7024.717336689891
INFO:root:current train perplexity3.9967336654663086
INFO:root:current mean train loss 7027.607880594337
INFO:root:current train perplexity3.9974725246429443

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.81s/it]
INFO:root:final mean train loss: 7023.225835000315
INFO:root:final train perplexity: 3.9965553283691406
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.18s/it]
INFO:root:eval mean loss: 11389.87741234189
INFO:root:eval perplexity: 10.565630912780762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [23:31:43<3:59:38, 495.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7066.296186699925
INFO:root:current train perplexity4.0118279457092285
INFO:root:current mean train loss 7045.625149419399
INFO:root:current train perplexity4.01353645324707
INFO:root:current mean train loss 7019.724561064488
INFO:root:current train perplexity3.9985578060150146
INFO:root:current mean train loss 7021.777273631283
INFO:root:current train perplexity3.9963581562042236
INFO:root:current mean train loss 7028.908804630888
INFO:root:current train perplexity3.9977262020111084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.50s/it]
INFO:root:final mean train loss: 7024.101885395666
INFO:root:final train perplexity: 3.997246265411377
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.73s/it]
INFO:root:eval mean loss: 11389.382934570312
INFO:root:eval perplexity: 10.564550399780273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [23:40:03<3:51:52, 496.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7009.160105738147
INFO:root:current train perplexity3.9926986694335938
INFO:root:current mean train loss 7029.325150923295
INFO:root:current train perplexity3.9978129863739014
INFO:root:current mean train loss 7032.631437826655
INFO:root:current train perplexity3.998164176940918
INFO:root:current mean train loss 7028.319122708737
INFO:root:current train perplexity3.9974892139434814
INFO:root:current mean train loss 7029.618848859407
INFO:root:current train perplexity3.9963390827178955

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:57<00:00, 417.25s/it]
INFO:root:final mean train loss: 7022.830356720955
INFO:root:final train perplexity: 3.996243715286255
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:10<00:00, 70.38s/it]
INFO:root:eval mean loss: 11393.872331891742
INFO:root:eval perplexity: 10.574373245239258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [23:48:19<3:43:27, 496.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7033.132077395261
INFO:root:current train perplexity3.9937093257904053
INFO:root:current mean train loss 7028.71813645288
INFO:root:current train perplexity3.9906210899353027
INFO:root:current mean train loss 7034.5224055654
INFO:root:current train perplexity3.9971094131469727
INFO:root:current mean train loss 7030.200960827606
INFO:root:current train perplexity3.9946935176849365
INFO:root:current mean train loss 7026.83372780359
INFO:root:current train perplexity3.9947948455810547

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:08<00:00, 428.40s/it]
INFO:root:final mean train loss: 7021.240748251638
INFO:root:final train perplexity: 3.994990587234497
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.73s/it]
INFO:root:eval mean loss: 11393.255679175967
INFO:root:eval perplexity: 10.57302188873291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [23:56:41<3:35:57, 498.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6986.2485557154605
INFO:root:current train perplexity3.9760589599609375
INFO:root:current mean train loss 7017.830313501602
INFO:root:current train perplexity3.988309860229492
INFO:root:current mean train loss 7012.151608845339
INFO:root:current train perplexity3.9822349548339844
INFO:root:current mean train loss 7014.97459454114
INFO:root:current train perplexity3.9839513301849365
INFO:root:current mean train loss 7021.619376380998
INFO:root:current train perplexity3.9909474849700928

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.85s/it]
INFO:root:final mean train loss: 7016.124573738344
INFO:root:final train perplexity: 3.9909608364105225
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.59s/it]
INFO:root:eval mean loss: 11394.110921223959
INFO:root:eval perplexity: 10.5748929977417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [24:04:54<3:26:58, 496.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7011.949159564394
INFO:root:current train perplexity3.985283851623535
INFO:root:current mean train loss 7014.379661981784
INFO:root:current train perplexity3.9860236644744873
INFO:root:current mean train loss 7018.765562944189
INFO:root:current train perplexity3.9917564392089844
INFO:root:current mean train loss 7018.313472891213
INFO:root:current train perplexity3.9887306690216064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:10<00:00, 430.17s/it]
INFO:root:final mean train loss: 7018.51029328377
INFO:root:final train perplexity: 3.9928395748138428
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.54s/it]
INFO:root:eval mean loss: 11392.262329101562
INFO:root:eval perplexity: 10.570847511291504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [24:13:19<3:19:36, 499.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7027.988606770833
INFO:root:current train perplexity3.907139778137207
INFO:root:current mean train loss 7014.535568681736
INFO:root:current train perplexity3.972144842147827
INFO:root:current mean train loss 7014.717013354372
INFO:root:current train perplexity3.974827289581299
INFO:root:current mean train loss 7006.78744134179
INFO:root:current train perplexity3.9780306816101074
INFO:root:current mean train loss 7011.8661140082195
INFO:root:current train perplexity3.984250545501709

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.74s/it]
INFO:root:final mean train loss: 7010.990965812437
INFO:root:final train perplexity: 3.9869213104248047
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.04s/it]
INFO:root:eval mean loss: 11395.485866001674
INFO:root:eval perplexity: 10.577903747558594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [24:21:36<3:11:06, 498.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7046.448660714285
INFO:root:current train perplexity3.9741122722625732
INFO:root:current mean train loss 7009.464656651577
INFO:root:current train perplexity3.9875714778900146
INFO:root:current mean train loss 7008.439365847675
INFO:root:current train perplexity3.983585834503174
INFO:root:current mean train loss 7013.162203214068
INFO:root:current train perplexity3.9824376106262207
INFO:root:current mean train loss 7021.245859806895
INFO:root:current train perplexity3.9870998859405518

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.10s/it]
INFO:root:final mean train loss: 7011.977369739163
INFO:root:final train perplexity: 3.9876976013183594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.50s/it]
INFO:root:eval mean loss: 11397.70895531064
INFO:root:eval perplexity: 10.582775115966797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [24:29:48<3:02:07, 496.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7042.925914417614
INFO:root:current train perplexity3.966546058654785
INFO:root:current mean train loss 7031.093626829955
INFO:root:current train perplexity3.975740909576416
INFO:root:current mean train loss 7042.3609231524
INFO:root:current train perplexity3.988246202468872
INFO:root:current mean train loss 7025.393083676648
INFO:root:current train perplexity3.986406087875366
INFO:root:current mean train loss 7011.980510331128
INFO:root:current train perplexity3.982715368270874

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.85s/it]
INFO:root:final mean train loss: 7009.886826053743
INFO:root:final train perplexity: 3.986052989959717
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.29s/it]
INFO:root:eval mean loss: 11399.520763578868
INFO:root:eval perplexity: 10.58674144744873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [24:38:11<2:54:30, 498.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6978.879720052083
INFO:root:current train perplexity3.9813780784606934
INFO:root:current mean train loss 6995.662606148097
INFO:root:current train perplexity3.991279125213623
INFO:root:current mean train loss 7000.472715297965
INFO:root:current train perplexity3.98484206199646
INFO:root:current mean train loss 7006.1022352430555
INFO:root:current train perplexity3.983213424682617
INFO:root:current mean train loss 7013.035446865588
INFO:root:current train perplexity3.984251022338867

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.62s/it]
INFO:root:final mean train loss: 7008.689277895035
INFO:root:final train perplexity: 3.985111951828003
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.12s/it]
INFO:root:eval mean loss: 11395.103498186383
INFO:root:eval perplexity: 10.577066421508789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [24:46:28<2:46:00, 498.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7044.17041015625
INFO:root:current train perplexity4.0161967277526855
INFO:root:current mean train loss 6995.060522255777
INFO:root:current train perplexity3.9807891845703125
INFO:root:current mean train loss 7007.306045323202
INFO:root:current train perplexity3.9826347827911377
INFO:root:current mean train loss 7004.118966129507
INFO:root:current train perplexity3.978965997695923
INFO:root:current mean train loss 7014.470658841737
INFO:root:current train perplexity3.985684871673584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.63s/it]
INFO:root:final mean train loss: 7007.146579865486
INFO:root:final train perplexity: 3.9838991165161133
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.79s/it]
INFO:root:eval mean loss: 11398.366443452382
INFO:root:eval perplexity: 10.584213256835938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [24:54:43<2:37:22, 496.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7028.779254415761
INFO:root:current train perplexity3.9664175510406494
INFO:root:current mean train loss 7007.732445693598
INFO:root:current train perplexity3.9802169799804688
INFO:root:current mean train loss 7015.28978944787
INFO:root:current train perplexity3.982531785964966
INFO:root:current mean train loss 7018.973000919118
INFO:root:current train perplexity3.9856433868408203
INFO:root:current mean train loss 7013.754756990617
INFO:root:current train perplexity3.984349489212036

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:04<00:00, 424.75s/it]
INFO:root:final mean train loss: 7005.862577376827
INFO:root:final train perplexity: 3.9828896522521973
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.72s/it]
INFO:root:eval mean loss: 11399.21837797619
INFO:root:eval perplexity: 10.586080551147461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [25:03:02<2:29:20, 497.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6996.278935185185
INFO:root:current train perplexity4.002452373504639
INFO:root:current mean train loss 6986.532034325787
INFO:root:current train perplexity3.9759204387664795
INFO:root:current mean train loss 7005.887699614537
INFO:root:current train perplexity3.9765963554382324
INFO:root:current mean train loss 7004.2410825210245
INFO:root:current train perplexity3.977626085281372
INFO:root:current mean train loss 7010.795913303205
INFO:root:current train perplexity3.9824533462524414

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.27s/it]
INFO:root:final mean train loss: 7004.940221971081
INFO:root:final train perplexity: 3.982165575027466
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.78s/it]
INFO:root:eval mean loss: 11398.127984909785
INFO:root:eval perplexity: 10.583687782287598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [25:11:15<2:20:35, 496.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6991.286668346775
INFO:root:current train perplexity3.992871046066284
INFO:root:current mean train loss 7006.980584297471
INFO:root:current train perplexity3.980398178100586
INFO:root:current mean train loss 7008.180097571699
INFO:root:current train perplexity3.9828202724456787
INFO:root:current mean train loss 7009.096551347715
INFO:root:current train perplexity3.982159376144409
INFO:root:current mean train loss 7005.544826711137
INFO:root:current train perplexity3.9791409969329834

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:58<00:00, 418.32s/it]
INFO:root:final mean train loss: 7001.2662353515625
INFO:root:final train perplexity: 3.979280948638916
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.41s/it]
INFO:root:eval mean loss: 11397.320928664434
INFO:root:eval perplexity: 10.581924438476562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [25:19:29<2:12:09, 495.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6991.484402901786
INFO:root:current train perplexity3.965902090072632
INFO:root:current mean train loss 7011.611216001157
INFO:root:current train perplexity3.9711666107177734
INFO:root:current mean train loss 7004.196717087766
INFO:root:current train perplexity3.9707021713256836
INFO:root:current mean train loss 7006.390447178172
INFO:root:current train perplexity3.976562738418579
INFO:root:current mean train loss 7011.511064340876
INFO:root:current train perplexity3.979318857192993

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.99s/it]
INFO:root:final mean train loss: 7002.525242959299
INFO:root:final train perplexity: 3.980268716812134
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it]
INFO:root:eval mean loss: 11401.310558500743
INFO:root:eval perplexity: 10.590662002563477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [25:27:44<2:03:51, 495.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6985.513947315705
INFO:root:current train perplexity3.9665234088897705
INFO:root:current mean train loss 6978.481378569019
INFO:root:current train perplexity3.9784011840820312
INFO:root:current mean train loss 6996.535530122254
INFO:root:current train perplexity3.9768662452697754
INFO:root:current mean train loss 7004.270293199207
INFO:root:current train perplexity3.976710796356201
INFO:root:current mean train loss 7006.371600939636
INFO:root:current train perplexity3.9778268337249756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:03<00:00, 423.53s/it]
INFO:root:final mean train loss: 6997.056814870527
INFO:root:final train perplexity: 3.9759774208068848
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.20s/it]
INFO:root:eval mean loss: 11401.07533482143
INFO:root:eval perplexity: 10.590149879455566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [25:36:02<1:55:47, 496.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7007.588833121366
INFO:root:current train perplexity3.992262601852417
INFO:root:current mean train loss 7009.841936871722
INFO:root:current train perplexity3.9752883911132812
INFO:root:current mean train loss 7010.733249742799
INFO:root:current train perplexity3.9732041358947754
INFO:root:current mean train loss 7013.209720355776
INFO:root:current train perplexity3.978569984436035
INFO:root:current mean train loss 7005.708800305093
INFO:root:current train perplexity3.977198600769043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.18s/it]
INFO:root:final mean train loss: 6998.4968936058785
INFO:root:final train perplexity: 3.9771080017089844
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.97s/it]
INFO:root:eval mean loss: 11402.441554478237
INFO:root:eval perplexity: 10.593144416809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [25:44:15<1:47:17, 495.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7007.094601894947
INFO:root:current train perplexity3.9589009284973145
INFO:root:current mean train loss 6995.022477545705
INFO:root:current train perplexity3.9704244136810303
INFO:root:current mean train loss 6991.546639755187
INFO:root:current train perplexity3.9700145721435547
INFO:root:current mean train loss 7009.414052649946
INFO:root:current train perplexity3.980550527572632
INFO:root:current mean train loss 7004.582294506781
INFO:root:current train perplexity3.977813959121704

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.67s/it]
INFO:root:final mean train loss: 6998.369662377142
INFO:root:final train perplexity: 3.9770078659057617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.37s/it]
INFO:root:eval mean loss: 11401.874058314732
INFO:root:eval perplexity: 10.591899871826172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [25:52:31<1:39:04, 495.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7001.69566674326
INFO:root:current train perplexity3.958582878112793
INFO:root:current mean train loss 7003.067408681705
INFO:root:current train perplexity3.9754152297973633
INFO:root:current mean train loss 6995.996916630354
INFO:root:current train perplexity3.9754247665405273
INFO:root:current mean train loss 6992.341860866275
INFO:root:current train perplexity3.970705986022949
INFO:root:current mean train loss 7002.791207256444
INFO:root:current train perplexity3.973895788192749

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.12s/it]
INFO:root:final mean train loss: 6996.590892176474
INFO:root:final train perplexity: 3.975611925125122
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.56s/it]
INFO:root:eval mean loss: 11401.258937290737
INFO:root:eval perplexity: 10.590551376342773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [26:00:48<1:30:55, 495.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6972.417418323864
INFO:root:current train perplexity3.9643990993499756
INFO:root:current mean train loss 7002.553102948588
INFO:root:current train perplexity3.9731287956237793
INFO:root:current mean train loss 6987.315659466912
INFO:root:current train perplexity3.9715378284454346
INFO:root:current mean train loss 6989.162878246038
INFO:root:current train perplexity3.9709062576293945
INFO:root:current mean train loss 6996.36642342033
INFO:root:current train perplexity3.9719865322113037

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.37s/it]
INFO:root:final mean train loss: 6994.239471435547
INFO:root:final train perplexity: 3.9737682342529297
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.38s/it]
INFO:root:eval mean loss: 11402.39246768043
INFO:root:eval perplexity: 10.593036651611328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [26:09:05<1:22:43, 496.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7029.69870564089
INFO:root:current train perplexity3.9785215854644775
INFO:root:current mean train loss 7017.581555252555
INFO:root:current train perplexity3.9721391201019287
INFO:root:current mean train loss 7002.853163082167
INFO:root:current train perplexity3.9714269638061523
INFO:root:current mean train loss 6996.835405695073
INFO:root:current train perplexity3.9711968898773193
INFO:root:current mean train loss 6995.911692367919
INFO:root:current train perplexity3.9721179008483887

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.07s/it]
INFO:root:final mean train loss: 6993.995045323526
INFO:root:final train perplexity: 3.9735774993896484
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.49s/it]
INFO:root:eval mean loss: 11403.92448497954
INFO:root:eval perplexity: 10.59639835357666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [26:17:17<1:14:14, 494.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7040.268926711309
INFO:root:current train perplexity3.9744505882263184
INFO:root:current mean train loss 7017.441720787002
INFO:root:current train perplexity3.9748408794403076
INFO:root:current mean train loss 7012.583444109435
INFO:root:current train perplexity3.973207950592041
INFO:root:current mean train loss 6996.962378131457
INFO:root:current train perplexity3.9693236351013184
INFO:root:current mean train loss 6993.9305354009175
INFO:root:current train perplexity3.9700822830200195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:06<00:00, 426.47s/it]
INFO:root:final mean train loss: 6991.331239761845
INFO:root:final train perplexity: 3.971489906311035
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.53s/it]
INFO:root:eval mean loss: 11406.063900902158
INFO:root:eval perplexity: 10.601093292236328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [26:25:39<1:06:17, 497.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6982.2998046875
INFO:root:current train perplexity3.9621963500976562
INFO:root:current mean train loss 6988.870418343002
INFO:root:current train perplexity3.970391273498535
INFO:root:current mean train loss 6986.39540723022
INFO:root:current train perplexity3.9645204544067383
INFO:root:current mean train loss 6989.5166348241655
INFO:root:current train perplexity3.969616651535034
INFO:root:current mean train loss 6990.81412481598
INFO:root:current train perplexity3.9680893421173096

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:00<00:00, 420.19s/it]
INFO:root:final mean train loss: 6989.592313704952
INFO:root:final train perplexity: 3.9701271057128906
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.45s/it]
INFO:root:eval mean loss: 11405.330496651786
INFO:root:eval perplexity: 10.599480628967285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [26:33:56<57:59, 497.14s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6999.4757991307215
INFO:root:current train perplexity3.988253355026245
INFO:root:current mean train loss 6992.770779079861
INFO:root:current train perplexity3.9795451164245605
INFO:root:current mean train loss 6993.137387208833
INFO:root:current train perplexity3.96767258644104
INFO:root:current mean train loss 6990.12205715128
INFO:root:current train perplexity3.9666311740875244
INFO:root:current mean train loss 6995.639642217357
INFO:root:current train perplexity3.9680731296539307

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:02<00:00, 422.17s/it]
INFO:root:final mean train loss: 6988.512723861202
INFO:root:final train perplexity: 3.9692814350128174
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.63s/it]
INFO:root:eval mean loss: 11403.06120372954
INFO:root:eval perplexity: 10.594501495361328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [26:42:13<49:41, 496.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6963.1790169270835
INFO:root:current train perplexity3.9532270431518555
INFO:root:current mean train loss 6977.026328125
INFO:root:current train perplexity3.9655041694641113
INFO:root:current mean train loss 6990.082917258523
INFO:root:current train perplexity3.970195770263672
INFO:root:current mean train loss 6992.622470052083
INFO:root:current train perplexity3.971897602081299
INFO:root:current mean train loss 6996.155427631579
INFO:root:current train perplexity3.9724862575531006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:09<00:00, 429.18s/it]
INFO:root:final mean train loss: 6991.18305132466
INFO:root:final train perplexity: 3.971374034881592
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.50s/it]
INFO:root:eval mean loss: 11403.851327078683
INFO:root:eval perplexity: 10.59623908996582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [26:50:37<41:35, 499.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7022.048976463608
INFO:root:current train perplexity3.9820384979248047
INFO:root:current mean train loss 7009.214208166027
INFO:root:current train perplexity3.971292495727539
INFO:root:current mean train loss 6991.2586350526435
INFO:root:current train perplexity3.968944549560547
INFO:root:current mean train loss 6990.875276993322
INFO:root:current train perplexity3.9700117111206055
INFO:root:current mean train loss 6993.6664763830895
INFO:root:current train perplexity3.968877077102661

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.34s/it]
INFO:root:final mean train loss: 6988.072032313193
INFO:root:final train perplexity: 3.9689371585845947
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.51s/it]
INFO:root:eval mean loss: 11404.948399135044
INFO:root:eval perplexity: 10.598644256591797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [26:58:52<33:11, 497.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7001.029267460467
INFO:root:current train perplexity3.9804790019989014
INFO:root:current mean train loss 7005.693081881831
INFO:root:current train perplexity3.973306894302368
INFO:root:current mean train loss 7007.020338725707
INFO:root:current train perplexity3.974153518676758
INFO:root:current mean train loss 6998.894796426241
INFO:root:current train perplexity3.9728095531463623
INFO:root:current mean train loss 6993.052365383994
INFO:root:current train perplexity3.9698007106781006

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:54<00:00, 414.72s/it]
INFO:root:final mean train loss: 6987.646288471838
INFO:root:final train perplexity: 3.96860408782959
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.18s/it]
INFO:root:eval mean loss: 11404.897437686011
INFO:root:eval perplexity: 10.598530769348145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [27:07:03<24:47, 495.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6986.4513570851295
INFO:root:current train perplexity3.9582760334014893
INFO:root:current mean train loss 6979.178867605281
INFO:root:current train perplexity3.963304042816162
INFO:root:current mean train loss 6988.9345090646775
INFO:root:current train perplexity3.9610118865966797
INFO:root:current mean train loss 6987.938099311612
INFO:root:current train perplexity3.9633867740631104
INFO:root:current mean train loss 6990.253678652785
INFO:root:current train perplexity3.966625452041626

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:56<00:00, 416.77s/it]
INFO:root:final mean train loss: 6985.0753183672505
INFO:root:final train perplexity: 3.9665915966033936
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.05s/it]
INFO:root:eval mean loss: 11405.651285807291
INFO:root:eval perplexity: 10.600186347961426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [27:15:16<16:29, 494.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7006.033219222184
INFO:root:current train perplexity3.9736087322235107
INFO:root:current mean train loss 7000.414468974967
INFO:root:current train perplexity3.975597381591797
INFO:root:current mean train loss 6994.846537062393
INFO:root:current train perplexity3.9781482219696045
INFO:root:current mean train loss 6996.173457231058
INFO:root:current train perplexity3.970919609069824
INFO:root:current mean train loss 6995.234469473969
INFO:root:current train perplexity3.9698891639709473

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:01<00:00, 421.31s/it]
INFO:root:final mean train loss: 6988.040375740297
INFO:root:final train perplexity: 3.96891188621521
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.64s/it]
INFO:root:eval mean loss: 11405.513340541294
INFO:root:eval perplexity: 10.599883079528809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [27:23:33<08:15, 495.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6981.6122430098685
INFO:root:current train perplexity3.9602367877960205
INFO:root:current mean train loss 6981.167645733173
INFO:root:current train perplexity3.9615139961242676
INFO:root:current mean train loss 6986.905097987288
INFO:root:current train perplexity3.9680099487304688
INFO:root:current mean train loss 6989.0254128757915
INFO:root:current train perplexity3.967099905014038
INFO:root:current mean train loss 6992.470218789457
INFO:root:current train perplexity3.9680697917938232

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:59<00:00, 419.36s/it]
INFO:root:final mean train loss: 6987.03852991904
INFO:root:final train perplexity: 3.968127727508545
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.57s/it]
INFO:root:eval mean loss: 11405.359064011347
INFO:root:eval perplexity: 10.599544525146484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_16/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [27:31:46<00:00, 494.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [27:31:46<00:00, 495.53s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:04<00:00, 64.14s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:04<00:00, 64.15s/it]
INFO:root:eval mean loss: 11405.359064011347
INFO:root:eval perplexity: 10.599544525146484
INFO:root:evalaution complete
INFO:root:save model final: small_topk_16/final
