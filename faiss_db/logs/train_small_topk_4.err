INFO:root:Output: small_topk_4
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 97853.929135101
INFO:root:current train perplexity15209.78125
INFO:root:current mean train loss 81458.76181689698
INFO:root:current train perplexity3055.502197265625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:37<00:00, 337.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:37<00:00, 337.14s/it]
INFO:root:final mean train loss: 75084.65393460181
INFO:root:final train perplexity: 1645.4464111328125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:47<00:00, 47.63s/it]
INFO:root:eval mean loss: 44180.31538318453
INFO:root:eval perplexity: 96.78307342529297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/1
  0%|          | 1/200 [06:25<21:17:24, 385.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42901.35516237745
INFO:root:current train perplexity69.51031494140625
INFO:root:current mean train loss 39106.954625413906
INFO:root:current train perplexity47.22895431518555

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.44s/it]
INFO:root:final mean train loss: 36520.42771862399
INFO:root:final train perplexity: 36.67485427856445
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.75s/it]
INFO:root:eval mean loss: 31765.862816220237
INFO:root:eval perplexity: 26.77931785583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/2
  1%|          | 2/200 [12:45<21:01:42, 382.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31218.861979166668
INFO:root:current train perplexity22.070552825927734
INFO:root:current mean train loss 29706.67686210558
INFO:root:current train perplexity18.682701110839844
INFO:root:current mean train loss 28799.988213900862
INFO:root:current train perplexity17.086118698120117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.96s/it]
INFO:root:final mean train loss: 28415.48781659526
INFO:root:final train perplexity: 16.488828659057617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.27s/it]
INFO:root:eval mean loss: 28535.17257254464
INFO:root:eval perplexity: 19.168481826782227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/3
  2%|â–         | 3/200 [19:06<20:53:45, 381.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26387.719282670456
INFO:root:current train perplexity13.42537784576416
INFO:root:current mean train loss 25898.19247731855
INFO:root:current train perplexity12.832557678222656

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.68s/it]
INFO:root:final mean train loss: 25529.357626638106
INFO:root:final train perplexity: 12.403985023498535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.82s/it]
INFO:root:eval mean loss: 27119.563662574405
INFO:root:eval perplexity: 16.5561580657959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/4
  2%|â–         | 4/200 [25:27<20:45:39, 381.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24862.83426339286
INFO:root:current train perplexity11.298796653747559
INFO:root:current mean train loss 24376.62589442173
INFO:root:current train perplexity11.016467094421387
INFO:root:current mean train loss 24094.955700860508
INFO:root:current train perplexity10.759342193603516

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.45s/it]
INFO:root:final mean train loss: 23980.470010080644
INFO:root:final train perplexity: 10.646671295166016
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.50s/it]
INFO:root:eval mean loss: 26297.193777901786
INFO:root:eval perplexity: 15.205330848693848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/5
  2%|â–Ž         | 5/200 [32:00<20:53:37, 385.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23338.853283898305
INFO:root:current train perplexity9.987391471862793
INFO:root:current mean train loss 23106.995712952044
INFO:root:current train perplexity9.752828598022461

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.61s/it]
INFO:root:final mean train loss: 22959.56302765877
INFO:root:final train perplexity: 9.626821517944336
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.53s/it]
INFO:root:eval mean loss: 25722.361839657737
INFO:root:eval perplexity: 14.327112197875977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/6
  3%|â–Ž         | 6/200 [38:18<20:38:30, 383.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22351.396129261364
INFO:root:current train perplexity9.119575500488281
INFO:root:current mean train loss 22388.170608108107
INFO:root:current train perplexity9.091283798217773
INFO:root:current mean train loss 22261.709734152842
INFO:root:current train perplexity8.975610733032227

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:36<00:00, 336.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:36<00:00, 336.04s/it]
INFO:root:final mean train loss: 22211.810791015625
INFO:root:final train perplexity: 8.942370414733887
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:07<00:00, 67.38s/it]
INFO:root:eval mean loss: 25283.492838541668
INFO:root:eval perplexity: 13.690917015075684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/7
  4%|â–Ž         | 7/200 [45:23<21:15:58, 396.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21853.612785218254
INFO:root:current train perplexity8.618350982666016
INFO:root:current mean train loss 21759.408886119632
INFO:root:current train perplexity8.523256301879883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.95s/it]
INFO:root:final mean train loss: 21638.467340284777
INFO:root:final train perplexity: 8.450709342956543
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.76s/it]
INFO:root:eval mean loss: 24989.144717261905
INFO:root:eval perplexity: 13.280129432678223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/8
  4%|â–         | 8/200 [51:44<20:53:47, 391.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21426.443098958334
INFO:root:current train perplexity8.199469566345215
INFO:root:current mean train loss 21329.01773097826
INFO:root:current train perplexity8.165166854858398
INFO:root:current mean train loss 21212.68496547965
INFO:root:current train perplexity8.090378761291504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.88s/it]
INFO:root:final mean train loss: 21174.775910408265
INFO:root:final train perplexity: 8.072919845581055
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:08<00:00, 68.44s/it]
INFO:root:eval mean loss: 24687.93284970238
INFO:root:eval perplexity: 12.872518539428711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/9
  4%|â–         | 9/200 [58:34<21:05:32, 397.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20947.201929804105
INFO:root:current train perplexity7.854743957519531
INFO:root:current mean train loss 20876.60807681512
INFO:root:current train perplexity7.817574501037598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.08s/it]
INFO:root:final mean train loss: 20783.67917952999
INFO:root:final train perplexity: 7.767441272735596
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.85s/it]
INFO:root:eval mean loss: 24459.936476934523
INFO:root:eval perplexity: 12.572327613830566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/10
  5%|â–Œ         | 10/200 [1:04:55<20:42:25, 392.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20748.007298519737
INFO:root:current train perplexity7.608295440673828
INFO:root:current mean train loss 20577.91445640756
INFO:root:current train perplexity7.564255237579346
INFO:root:current mean train loss 20480.778449628997
INFO:root:current train perplexity7.52687406539917

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.08s/it]
INFO:root:final mean train loss: 20452.501945249496
INFO:root:final train perplexity: 7.517818927764893
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.79s/it]
INFO:root:eval mean loss: 24244.88748604911
INFO:root:eval perplexity: 12.295595169067383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/11
  6%|â–Œ         | 11/200 [1:11:23<20:31:28, 390.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20268.167253521126
INFO:root:current train perplexity7.33478307723999
INFO:root:current mean train loss 20208.549719024122
INFO:root:current train perplexity7.324815273284912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:29<00:00, 329.67s/it]
INFO:root:final mean train loss: 20158.82013136341
INFO:root:final train perplexity: 7.303177356719971
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.62s/it]
INFO:root:eval mean loss: 24079.056175595237
INFO:root:eval perplexity: 12.086372375488281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/12
  6%|â–Œ         | 12/200 [1:18:15<20:44:48, 397.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20041.74515964674
INFO:root:current train perplexity7.190103054046631
INFO:root:current mean train loss 19944.106056275406
INFO:root:current train perplexity7.1550798416137695
INFO:root:current mean train loss 19912.16965036435
INFO:root:current train perplexity7.124391555786133

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.10s/it]
INFO:root:final mean train loss: 19907.56861139113
INFO:root:final train perplexity: 7.124417304992676
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.27s/it]
INFO:root:eval mean loss: 23924.198149181546
INFO:root:eval perplexity: 11.894205093383789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/13
  6%|â–‹         | 13/200 [1:24:37<20:24:07, 392.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19775.533255208335
INFO:root:current train perplexity6.988028526306152
INFO:root:current mean train loss 19710.114241071427
INFO:root:current train perplexity6.977527618408203

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.41s/it]
INFO:root:final mean train loss: 19675.114202683973
INFO:root:final train perplexity: 6.9629292488098145
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.92s/it]
INFO:root:eval mean loss: 23783.214192708332
INFO:root:eval perplexity: 11.721914291381836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/14
  7%|â–‹         | 14/200 [1:31:02<20:10:01, 390.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19485.24710648148
INFO:root:current train perplexity6.861457347869873
INFO:root:current mean train loss 19447.74238742618
INFO:root:current train perplexity6.837381839752197
INFO:root:current mean train loss 19492.550419878855
INFO:root:current train perplexity6.831214427947998

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:35<00:00, 335.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:35<00:00, 335.46s/it]
INFO:root:final mean train loss: 19473.24951171875
INFO:root:final train perplexity: 6.825667858123779
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:06<00:00, 66.93s/it]
INFO:root:eval mean loss: 23676.49144345238
INFO:root:eval perplexity: 11.593155860900879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/15
  8%|â–Š         | 15/200 [1:37:46<20:16:24, 394.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19289.03767800633
INFO:root:current train perplexity6.710328578948975
INFO:root:current mean train loss 19292.98368758729
INFO:root:current train perplexity6.7029643058776855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.19s/it]
INFO:root:final mean train loss: 19280.84539598034
INFO:root:final train perplexity: 6.697357177734375
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.01s/it]
INFO:root:eval mean loss: 23560.698335193454
INFO:root:eval perplexity: 11.45505142211914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/16
  8%|â–Š         | 16/200 [1:44:24<20:12:59, 395.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19089.399760584678
INFO:root:current train perplexity6.634973049163818
INFO:root:current mean train loss 19119.236864861643
INFO:root:current train perplexity6.588298797607422
INFO:root:current mean train loss 19118.1180076434
INFO:root:current train perplexity6.586560249328613

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.78s/it]
INFO:root:final mean train loss: 19113.965993573587
INFO:root:final train perplexity: 6.588021278381348
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.35s/it]
INFO:root:eval mean loss: 23456.58996000744
INFO:root:eval perplexity: 11.332286834716797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/17
  8%|â–Š         | 17/200 [1:50:43<19:51:15, 390.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18968.785862198794
INFO:root:current train perplexity6.4919023513793945
INFO:root:current mean train loss 18974.120271943306
INFO:root:current train perplexity6.481075286865234

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.42s/it]
INFO:root:final mean train loss: 18955.22786983367
INFO:root:final train perplexity: 6.485678672790527
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:54<00:00, 54.05s/it]
INFO:root:eval mean loss: 23402.102632068454
INFO:root:eval perplexity: 11.268560409545898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/18
  9%|â–‰         | 18/200 [1:57:07<19:38:58, 388.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18880.232979910714
INFO:root:current train perplexity6.428017616271973
INFO:root:current mean train loss 18876.269936342593
INFO:root:current train perplexity6.4178032875061035
INFO:root:current mean train loss 18805.864793882978
INFO:root:current train perplexity6.386745452880859

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.23s/it]
INFO:root:final mean train loss: 18805.127220892136
INFO:root:final train perplexity: 6.3903679847717285
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.03s/it]
INFO:root:eval mean loss: 23317.920061383928
INFO:root:eval perplexity: 11.170812606811523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/19
 10%|â–‰         | 19/200 [2:03:28<19:25:36, 386.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18669.599407327587
INFO:root:current train perplexity6.290709018707275
INFO:root:current mean train loss 18703.00309157754
INFO:root:current train perplexity6.306821823120117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.63s/it]
INFO:root:final mean train loss: 18673.239092426917
INFO:root:final train perplexity: 6.307777404785156
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.53s/it]
INFO:root:eval mean loss: 23231.49679129464
INFO:root:eval perplexity: 11.071340560913086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/20
 10%|â–ˆ         | 20/200 [2:09:50<19:15:07, 385.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18694.69140625
INFO:root:current train perplexity6.254587173461914
INFO:root:current mean train loss 18580.28958239658
INFO:root:current train perplexity6.235931873321533
INFO:root:current mean train loss 18569.35466788703
INFO:root:current train perplexity6.232697486877441

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.03s/it]
INFO:root:final mean train loss: 18546.466741746473
INFO:root:final train perplexity: 6.229395389556885
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.34s/it]
INFO:root:eval mean loss: 23163.873883928572
INFO:root:eval perplexity: 10.994125366210938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/21
 10%|â–ˆ         | 21/200 [2:16:15<19:08:50, 385.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18421.785220638736
INFO:root:current train perplexity6.155989170074463
INFO:root:current mean train loss 18442.38020492474
INFO:root:current train perplexity6.151982307434082

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.56s/it]
INFO:root:final mean train loss: 18427.887498424898
INFO:root:final train perplexity: 6.1569647789001465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:04<00:00, 64.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:04<00:00, 64.90s/it]
INFO:root:eval mean loss: 23112.044084821428
INFO:root:eval perplexity: 10.935311317443848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/22
 11%|â–ˆ         | 22/200 [2:22:48<19:08:42, 387.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18368.049236918603
INFO:root:current train perplexity6.11280632019043
INFO:root:current mean train loss 18340.826827469406
INFO:root:current train perplexity6.097846508026123
INFO:root:current mean train loss 18345.786249356996
INFO:root:current train perplexity6.095455646514893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.79s/it]
INFO:root:final mean train loss: 18323.26624716482
INFO:root:final train perplexity: 6.093756198883057
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.34s/it]
INFO:root:eval mean loss: 23071.674293154763
INFO:root:eval perplexity: 10.889715194702148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/23
 12%|â–ˆâ–        | 23/200 [2:28:56<18:46:03, 381.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18269.738671875
INFO:root:current train perplexity6.031614303588867
INFO:root:current mean train loss 18237.196243990384
INFO:root:current train perplexity6.027271747589111

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:54<00:00, 354.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:54<00:00, 354.09s/it]
INFO:root:final mean train loss: 18213.74566453503
INFO:root:final train perplexity: 6.028284072875977
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.79s/it]
INFO:root:eval mean loss: 22995.605375744046
INFO:root:eval perplexity: 10.804319381713867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/24
 12%|â–ˆâ–        | 24/200 [2:35:46<19:04:07, 390.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18063.836560837764
INFO:root:current train perplexity5.958471775054932
INFO:root:current mean train loss 18115.396670386905
INFO:root:current train perplexity5.9695820808410645
INFO:root:current mean train loss 18125.212898532387
INFO:root:current train perplexity5.96866512298584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:36<00:00, 336.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:36<00:00, 336.94s/it]
INFO:root:final mean train loss: 18111.853369928176
INFO:root:final train perplexity: 5.968005180358887
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.90s/it]
INFO:root:eval mean loss: 22981.172456287204
INFO:root:eval perplexity: 10.788190841674805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/25
 12%|â–ˆâ–Ž        | 25/200 [2:42:28<19:07:47, 393.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18069.116141887625
INFO:root:current train perplexity5.931145191192627
INFO:root:current mean train loss 18050.214421717967
INFO:root:current train perplexity5.913700103759766

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:44<00:00, 344.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:44<00:00, 344.24s/it]
INFO:root:final mean train loss: 18019.75220514113
INFO:root:final train perplexity: 5.914035320281982
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.92s/it]
INFO:root:eval mean loss: 22930.351702008928
INFO:root:eval perplexity: 10.73160171508789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/26
 13%|â–ˆâ–Ž        | 26/200 [2:49:06<19:05:51, 395.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17944.529220281864
INFO:root:current train perplexity5.880149841308594
INFO:root:current mean train loss 17955.40859116308
INFO:root:current train perplexity5.869077205657959

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.30s/it]
INFO:root:final mean train loss: 17924.512431483116
INFO:root:final train perplexity: 5.85874080657959
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.02s/it]
INFO:root:eval mean loss: 22892.629441034227
INFO:root:eval perplexity: 10.689783096313477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/27
 14%|â–ˆâ–Ž        | 27/200 [2:55:26<18:46:14, 390.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18016.879557291668
INFO:root:current train perplexity5.813633441925049
INFO:root:current mean train loss 17878.87693416262
INFO:root:current train perplexity5.828505516052246
INFO:root:current mean train loss 17874.542304879928
INFO:root:current train perplexity5.815605640411377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:33<00:00, 333.06s/it]
INFO:root:final mean train loss: 17848.696997857864
INFO:root:final train perplexity: 5.815093994140625
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.09s/it]
INFO:root:eval mean loss: 22836.402320498513
INFO:root:eval perplexity: 10.627758979797363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/28
 14%|â–ˆâ–        | 28/200 [3:01:53<18:36:25, 389.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17833.624360795453
INFO:root:current train perplexity5.790403366088867
INFO:root:current mean train loss 17800.658719758063
INFO:root:current train perplexity5.773536682128906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.98s/it]
INFO:root:final mean train loss: 17760.803746377267
INFO:root:final train perplexity: 5.764899730682373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.26s/it]
INFO:root:eval mean loss: 22813.890555245536
INFO:root:eval perplexity: 10.60302734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/29
 14%|â–ˆâ–        | 29/200 [3:08:27<18:33:53, 390.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17819.34765625
INFO:root:current train perplexity5.739723205566406
INFO:root:current mean train loss 17792.095319801403
INFO:root:current train perplexity5.730104923248291
INFO:root:current mean train loss 17728.192613979467
INFO:root:current train perplexity5.72218132019043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.65s/it]
INFO:root:final mean train loss: 17682.34326171875
INFO:root:final train perplexity: 5.720458507537842
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.67s/it]
INFO:root:eval mean loss: 22788.119303385418
INFO:root:eval perplexity: 10.574782371520996
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/30
 15%|â–ˆâ–Œ        | 30/200 [3:14:40<18:12:08, 385.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17646.653237552968
INFO:root:current train perplexity5.6920390129089355
INFO:root:current mean train loss 17634.866192511792
INFO:root:current train perplexity5.6852030754089355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.91s/it]
INFO:root:final mean train loss: 17612.71312295237
INFO:root:final train perplexity: 5.681306838989258
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.39s/it]
INFO:root:eval mean loss: 22748.602957589286
INFO:root:eval perplexity: 10.531623840332031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/31
 16%|â–ˆâ–Œ        | 31/200 [3:20:48<17:50:59, 380.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17468.347301136364
INFO:root:current train perplexity5.609859466552734
INFO:root:current mean train loss 17550.502287443695
INFO:root:current train perplexity5.626245021820068
INFO:root:current mean train loss 17555.457308945497
INFO:root:current train perplexity5.644899845123291

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.17s/it]
INFO:root:final mean train loss: 17541.646417433214
INFO:root:final train perplexity: 5.641622543334961
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it]
INFO:root:eval mean loss: 22739.42699032738
INFO:root:eval perplexity: 10.521624565124512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/32
 16%|â–ˆâ–Œ        | 32/200 [3:26:58<17:35:57, 377.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17475.340680803572
INFO:root:current train perplexity5.586857795715332
INFO:root:current mean train loss 17493.059672162577
INFO:root:current train perplexity5.60546875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.26s/it]
INFO:root:final mean train loss: 17475.093899634576
INFO:root:final train perplexity: 5.604712009429932
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.18s/it]
INFO:root:eval mean loss: 22704.72335379464
INFO:root:eval perplexity: 10.483903884887695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/33
 16%|â–ˆâ–‹        | 33/200 [3:32:58<17:15:28, 372.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17258.0734375
INFO:root:current train perplexity5.522700309753418
INFO:root:current mean train loss 17413.788060461957
INFO:root:current train perplexity5.571041107177734
INFO:root:current mean train loss 17394.947783430234
INFO:root:current train perplexity5.558309555053711

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.79s/it]
INFO:root:final mean train loss: 17409.78299048639
INFO:root:final train perplexity: 5.568722724914551
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.56s/it]
INFO:root:eval mean loss: 22648.803524925595
INFO:root:eval perplexity: 10.423402786254883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/34
 17%|â–ˆâ–‹        | 34/200 [3:39:23<17:20:02, 375.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17339.929425139926
INFO:root:current train perplexity5.5287628173828125
INFO:root:current mean train loss 17364.76333270958
INFO:root:current train perplexity5.532934188842773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.42s/it]
INFO:root:final mean train loss: 17347.013841198335
INFO:root:final train perplexity: 5.5343523025512695
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.17s/it]
INFO:root:eval mean loss: 22631.64771670387
INFO:root:eval perplexity: 10.404911994934082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/35
 18%|â–ˆâ–Š        | 35/200 [3:45:46<17:19:01, 377.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17489.259148848683
INFO:root:current train perplexity5.562710762023926
INFO:root:current mean train loss 17328.177094275212
INFO:root:current train perplexity5.512300968170166
INFO:root:current mean train loss 17300.56266053082
INFO:root:current train perplexity5.498835563659668

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.99s/it]
INFO:root:final mean train loss: 17283.64310578377
INFO:root:final train perplexity: 5.499869346618652
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it]
INFO:root:eval mean loss: 22641.12132626488
INFO:root:eval perplexity: 10.415118217468262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/36
 18%|â–ˆâ–Š        | 36/200 [3:52:02<17:11:17, 377.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17199.74883087588
INFO:root:current train perplexity5.453671455383301
INFO:root:current mean train loss 17220.443199470028
INFO:root:current train perplexity5.4688286781311035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.42s/it]
INFO:root:final mean train loss: 17227.387498424898
INFO:root:final train perplexity: 5.469437599182129
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.45s/it]
INFO:root:eval mean loss: 22592.351399739582
INFO:root:eval perplexity: 10.36268138885498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/37
 18%|â–ˆâ–Š        | 37/200 [3:58:22<17:07:37, 378.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17111.990064538044
INFO:root:current train perplexity5.40888786315918
INFO:root:current mean train loss 17126.24209222561
INFO:root:current train perplexity5.431346893310547
INFO:root:current mean train loss 17190.124570838005
INFO:root:current train perplexity5.4423980712890625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.80s/it]
INFO:root:final mean train loss: 17173.2840812437
INFO:root:final train perplexity: 5.440328598022461
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.74s/it]
INFO:root:eval mean loss: 22581.48172433036
INFO:root:eval perplexity: 10.351029396057129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/38
 19%|â–ˆâ–‰        | 38/200 [4:04:34<16:56:21, 376.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17139.154661458335
INFO:root:current train perplexity5.404119491577148
INFO:root:current mean train loss 17122.152611607144
INFO:root:current train perplexity5.4053449630737305

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.32s/it]
INFO:root:final mean train loss: 17119.993738974295
INFO:root:final train perplexity: 5.411808013916016
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.39s/it]
INFO:root:eval mean loss: 22558.95275297619
INFO:root:eval perplexity: 10.326921463012695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/39
 20%|â–ˆâ–‰        | 39/200 [4:10:54<16:52:28, 377.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16986.639069733796
INFO:root:current train perplexity5.346301078796387
INFO:root:current mean train loss 17068.820919968013
INFO:root:current train perplexity5.38179349899292
INFO:root:current mean train loss 17076.173888353525
INFO:root:current train perplexity5.382170677185059

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.48s/it]
INFO:root:final mean train loss: 17067.379567792337
INFO:root:final train perplexity: 5.383796215057373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.97s/it]
INFO:root:eval mean loss: 22550.822451636905
INFO:root:eval perplexity: 10.318239212036133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/40
 20%|â–ˆâ–ˆ        | 40/200 [4:17:28<17:00:08, 382.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17059.762645866296
INFO:root:current train perplexity5.3660502433776855
INFO:root:current mean train loss 17074.736213556214
INFO:root:current train perplexity5.372495174407959

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.57s/it]
INFO:root:final mean train loss: 17018.41384592364
INFO:root:final train perplexity: 5.357858180999756
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it]
INFO:root:eval mean loss: 22529.957542782737
INFO:root:eval perplexity: 10.295979499816895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/41
 20%|â–ˆâ–ˆ        | 41/200 [4:24:07<17:06:37, 387.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16893.7556703629
INFO:root:current train perplexity5.300952911376953
INFO:root:current mean train loss 16945.613721075857
INFO:root:current train perplexity5.316107273101807
INFO:root:current mean train loss 16990.642945921267
INFO:root:current train perplexity5.332374095916748

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.95s/it]
INFO:root:final mean train loss: 16971.27209078881
INFO:root:final train perplexity: 5.33300256729126
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.34s/it]
INFO:root:eval mean loss: 22495.307105654763
INFO:root:eval perplexity: 10.259123802185059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/42
 21%|â–ˆâ–ˆ        | 42/200 [4:30:26<16:53:38, 384.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16919.67895801958
INFO:root:current train perplexity5.308173656463623
INFO:root:current mean train loss 16949.70421362705
INFO:root:current train perplexity5.3101277351379395

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.18s/it]
INFO:root:final mean train loss: 16923.641755134828
INFO:root:final train perplexity: 5.30800724029541
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.03s/it]
INFO:root:eval mean loss: 22511.413132440477
INFO:root:eval perplexity: 10.276235580444336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/43
 22%|â–ˆâ–ˆâ–       | 43/200 [4:36:49<16:45:18, 384.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17009.073381696428
INFO:root:current train perplexity5.321083068847656
INFO:root:current mean train loss 16921.373213252315
INFO:root:current train perplexity5.284822463989258
INFO:root:current mean train loss 16890.369211269946
INFO:root:current train perplexity5.283334732055664

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.06s/it]
INFO:root:final mean train loss: 16873.82506142893
INFO:root:final train perplexity: 5.2819905281066895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.69s/it]
INFO:root:eval mean loss: 22503.060221354168
INFO:root:eval perplexity: 10.26735782623291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/44
 22%|â–ˆâ–ˆâ–       | 44/200 [4:43:02<16:30:24, 380.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16822.283045977012
INFO:root:current train perplexity5.249022960662842
INFO:root:current mean train loss 16841.80727774064
INFO:root:current train perplexity5.255989074707031

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.49s/it]
INFO:root:final mean train loss: 16830.957976310485
INFO:root:final train perplexity: 5.259705066680908
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.77s/it]
INFO:root:eval mean loss: 22467.712820870536
INFO:root:eval perplexity: 10.229863166809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [4:49:11<16:14:45, 377.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16815.58836638622
INFO:root:current train perplexity5.243496894836426
INFO:root:current mean train loss 16790.867714422213
INFO:root:current train perplexity5.237451076507568
INFO:root:current mean train loss 16801.927268566946
INFO:root:current train perplexity5.237334728240967

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.92s/it]
INFO:root:final mean train loss: 16786.150851341987
INFO:root:final train perplexity: 5.23651123046875
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.76s/it]
INFO:root:eval mean loss: 22468.603701636905
INFO:root:eval perplexity: 10.230807304382324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [4:55:27<16:07:48, 377.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16789.915843921703
INFO:root:current train perplexity5.215618133544922
INFO:root:current mean train loss 16779.079479916556
INFO:root:current train perplexity5.220822811126709

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.50s/it]
INFO:root:final mean train loss: 16747.00343765751
INFO:root:final train perplexity: 5.216330528259277
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.26s/it]
INFO:root:eval mean loss: 22465.016927083332
INFO:root:eval perplexity: 10.227009773254395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [5:02:12<16:22:45, 385.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16714.6296784157
INFO:root:current train perplexity5.195162773132324
INFO:root:current mean train loss 16721.237550535403
INFO:root:current train perplexity5.195494174957275
INFO:root:current mean train loss 16718.24784995499
INFO:root:current train perplexity5.1964850425720215

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.82s/it]
INFO:root:final mean train loss: 16706.882273027975
INFO:root:final train perplexity: 5.195729732513428
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.55s/it]
INFO:root:eval mean loss: 22454.849539620536
INFO:root:eval perplexity: 10.21625804901123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/48
 24%|â–ˆâ–ˆâ–       | 48/200 [5:08:23<16:05:19, 381.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16650.059303042763
INFO:root:current train perplexity5.158565521240234
INFO:root:current mean train loss 16660.479777644232
INFO:root:current train perplexity5.1635661125183105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.41s/it]
INFO:root:final mean train loss: 16661.114809097784
INFO:root:final train perplexity: 5.172328948974609
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it]
INFO:root:eval mean loss: 22417.65380859375
INFO:root:eval perplexity: 10.177002906799316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/49
 24%|â–ˆâ–ˆâ–       | 49/200 [5:14:36<15:52:26, 378.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16640.27489195479
INFO:root:current train perplexity5.127535820007324
INFO:root:current mean train loss 16624.775058460884
INFO:root:current train perplexity5.1467061042785645
INFO:root:current mean train loss 16627.07055762905
INFO:root:current train perplexity5.14918851852417

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.59s/it]
INFO:root:final mean train loss: 16616.799946446572
INFO:root:final train perplexity: 5.149770259857178
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.16s/it]
INFO:root:eval mean loss: 22432.719401041668
INFO:root:eval perplexity: 10.19288444519043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [5:20:48<15:41:31, 376.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16606.836381392044
INFO:root:current train perplexity5.123773097991943
INFO:root:current mean train loss 16576.135904130024
INFO:root:current train perplexity5.132955551147461

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.66s/it]
INFO:root:final mean train loss: 16588.19943138861
INFO:root:final train perplexity: 5.135263442993164
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.85s/it]
INFO:root:eval mean loss: 22424.01685732887
INFO:root:eval perplexity: 10.183707237243652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [5:27:08<15:37:51, 377.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16568.455499387255
INFO:root:current train perplexity5.1146931648254395
INFO:root:current mean train loss 16573.48491178601
INFO:root:current train perplexity5.1160125732421875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.76s/it]
INFO:root:final mean train loss: 16553.974369172127
INFO:root:final train perplexity: 5.117958068847656
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.76s/it]
INFO:root:eval mean loss: 22424.421223958332
INFO:root:eval perplexity: 10.184134483337402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [5:33:23<15:29:34, 376.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16667.326171875
INFO:root:current train perplexity5.240164279937744
INFO:root:current mean train loss 16487.14175326153
INFO:root:current train perplexity5.083337306976318
INFO:root:current mean train loss 16523.3889364609
INFO:root:current train perplexity5.096874713897705

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.43s/it]
INFO:root:final mean train loss: 16514.337315713205
INFO:root:final train perplexity: 5.097989082336426
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it]
INFO:root:eval mean loss: 22411.981910342263
INFO:root:eval perplexity: 10.171029090881348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [5:39:36<15:20:15, 375.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16482.212428977273
INFO:root:current train perplexity5.092865467071533
INFO:root:current mean train loss 16531.73884828629
INFO:root:current train perplexity5.098681926727295

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.12s/it]
INFO:root:final mean train loss: 16487.128677860383
INFO:root:final train perplexity: 5.084325790405273
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.66s/it]
INFO:root:eval mean loss: 22391.544247581845
INFO:root:eval perplexity: 10.149539947509766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [5:46:01<15:21:07, 378.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16334.735909598214
INFO:root:current train perplexity5.072447299957275
INFO:root:current mean train loss 16437.850257374415
INFO:root:current train perplexity5.0626397132873535
INFO:root:current mean train loss 16442.46851411534
INFO:root:current train perplexity5.06483268737793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.07s/it]
INFO:root:final mean train loss: 16441.269716324347
INFO:root:final train perplexity: 5.061380386352539
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it]
INFO:root:eval mean loss: 22386.560105096727
INFO:root:eval perplexity: 10.144304275512695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [5:52:45<15:32:56, 386.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16434.540717690677
INFO:root:current train perplexity5.037225246429443
INFO:root:current mean train loss 16388.034855296777
INFO:root:current train perplexity5.037193298339844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.89s/it]
INFO:root:final mean train loss: 16411.30123015373
INFO:root:final train perplexity: 5.046442031860352
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.78s/it]
INFO:root:eval mean loss: 22382.807547433036
INFO:root:eval perplexity: 10.14036750793457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [5:59:11<15:26:38, 386.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16523.765092329544
INFO:root:current train perplexity5.072758674621582
INFO:root:current mean train loss 16375.478594805743
INFO:root:current train perplexity5.02714729309082
INFO:root:current mean train loss 16410.879276510663
INFO:root:current train perplexity5.0360212326049805

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.79s/it]
INFO:root:final mean train loss: 16383.63765593498
INFO:root:final train perplexity: 5.032690048217773
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it]
INFO:root:eval mean loss: 22367.067057291668
INFO:root:eval perplexity: 10.123859405517578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [6:05:42<15:23:35, 387.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16328.753859747023
INFO:root:current train perplexity4.995553016662598
INFO:root:current mean train loss 16358.299457199004
INFO:root:current train perplexity5.016156196594238

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.21s/it]
INFO:root:final mean train loss: 16354.8060578377
INFO:root:final train perplexity: 5.018399715423584
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.44s/it]
INFO:root:eval mean loss: 22371.581147693454
INFO:root:eval perplexity: 10.12859058380127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [6:12:04<15:13:44, 386.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16314.400651041668
INFO:root:current train perplexity5.004801273345947
INFO:root:current mean train loss 16265.126010529892
INFO:root:current train perplexity4.975180625915527
INFO:root:current mean train loss 16301.43844022529
INFO:root:current train perplexity4.993743419647217

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.22s/it]
INFO:root:final mean train loss: 16316.783238564769
INFO:root:final train perplexity: 4.99961519241333
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.44s/it]
INFO:root:eval mean loss: 22356.675804501487
INFO:root:eval perplexity: 10.1129789352417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [6:18:24<15:02:45, 384.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16290.598195545708
INFO:root:current train perplexity4.965787887573242
INFO:root:current mean train loss 16294.410951534432
INFO:root:current train perplexity4.987342357635498

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.04s/it]
INFO:root:final mean train loss: 16288.503228956653
INFO:root:final train perplexity: 4.985688209533691
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.32s/it]
INFO:root:eval mean loss: 22354.349144345237
INFO:root:eval perplexity: 10.110544204711914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [6:24:33<14:45:40, 379.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16248.548930921053
INFO:root:current train perplexity4.9743499755859375
INFO:root:current mean train loss 16239.597204897584
INFO:root:current train perplexity4.967713356018066
INFO:root:current mean train loss 16257.557010737728
INFO:root:current train perplexity4.967743396759033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.13s/it]
INFO:root:final mean train loss: 16258.045985068044
INFO:root:final train perplexity: 4.970733642578125
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.29s/it]
INFO:root:eval mean loss: 22337.24195498512
INFO:root:eval perplexity: 10.09266185760498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [6:30:37<14:28:30, 374.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16255.855262433979
INFO:root:current train perplexity4.959541320800781
INFO:root:current mean train loss 16260.226728115862
INFO:root:current train perplexity4.960628032684326

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.83s/it]
INFO:root:final mean train loss: 16230.33383080267
INFO:root:final train perplexity: 4.957165718078613
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:53<00:00, 53.69s/it]
INFO:root:eval mean loss: 22356.83907645089
INFO:root:eval perplexity: 10.113150596618652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [6:37:04<14:30:37, 378.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16126.3818359375
INFO:root:current train perplexity4.909763813018799
INFO:root:current mean train loss 16212.622657837906
INFO:root:current train perplexity4.934924602508545
INFO:root:current mean train loss 16217.230354890695
INFO:root:current train perplexity4.945296287536621

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.79s/it]
INFO:root:final mean train loss: 16201.75886781754
INFO:root:final train perplexity: 4.943213939666748
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it]
INFO:root:eval mean loss: 22352.857259114582
INFO:root:eval perplexity: 10.10898208618164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [6:43:11<14:16:23, 375.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16161.344635416666
INFO:root:current train perplexity4.93034553527832
INFO:root:current mean train loss 16197.808683035715
INFO:root:current train perplexity4.927052021026611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.91s/it]
INFO:root:final mean train loss: 16171.252342962449
INFO:root:final train perplexity: 4.928361892700195
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.98s/it]
INFO:root:eval mean loss: 22336.428757440477
INFO:root:eval perplexity: 10.09180736541748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [6:49:22<14:07:37, 373.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16261.430736400464
INFO:root:current train perplexity4.931011199951172
INFO:root:current mean train loss 16130.61929441437
INFO:root:current train perplexity4.905501842498779
INFO:root:current mean train loss 16153.722299180892
INFO:root:current train perplexity4.914840221405029

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.20s/it]
INFO:root:final mean train loss: 16145.789389333417
INFO:root:final train perplexity: 4.916000843048096
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.02s/it]
INFO:root:eval mean loss: 22344.306105840773
INFO:root:eval perplexity: 10.1000394821167
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [6:55:31<13:57:49, 372.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16128.638350474683
INFO:root:current train perplexity4.893459796905518
INFO:root:current mean train loss 16122.288784261522
INFO:root:current train perplexity4.896945953369141

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.05s/it]
INFO:root:final mean train loss: 16111.391184160786
INFO:root:final train perplexity: 4.899349689483643
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.25s/it]
INFO:root:eval mean loss: 22339.589541480655
INFO:root:eval perplexity: 10.095111846923828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [7:01:38<13:47:45, 370.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16048.33048765121
INFO:root:current train perplexity4.860377788543701
INFO:root:current mean train loss 16103.18000805105
INFO:root:current train perplexity4.882555961608887
INFO:root:current mean train loss 16100.556699810606
INFO:root:current train perplexity4.88857364654541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:09<00:00, 309.92s/it]
INFO:root:final mean train loss: 16091.479007844002
INFO:root:final train perplexity: 4.889737606048584
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.21s/it]
INFO:root:eval mean loss: 22327.987258184523
INFO:root:eval perplexity: 10.082998275756836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [7:07:39<13:35:29, 367.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16065.560993975903
INFO:root:current train perplexity4.867612361907959
INFO:root:current mean train loss 16074.953968152322
INFO:root:current train perplexity4.87029504776001

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.23s/it]
INFO:root:final mean train loss: 16063.173410723286
INFO:root:final train perplexity: 4.876105308532715
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.29s/it]
INFO:root:eval mean loss: 22336.04859561012
INFO:root:eval perplexity: 10.091410636901855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [7:13:39<13:24:03, 365.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16118.010323660714
INFO:root:current train perplexity4.8819475173950195
INFO:root:current mean train loss 16057.232523148148
INFO:root:current train perplexity4.86179780960083
INFO:root:current mean train loss 16055.82894780585
INFO:root:current train perplexity4.863851547241211

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.64s/it]
INFO:root:final mean train loss: 16039.67308388987
INFO:root:final train perplexity: 4.864815711975098
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:05<00:00, 65.13s/it]
INFO:root:eval mean loss: 22325.794526599704
INFO:root:eval perplexity: 10.080707550048828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [7:20:18<13:40:00, 375.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16012.8117030352
INFO:root:current train perplexity4.8561835289001465
INFO:root:current mean train loss 15994.166856408756
INFO:root:current train perplexity4.849154949188232

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.30s/it]
INFO:root:final mean train loss: 16014.470770066784
INFO:root:final train perplexity: 4.852738380432129
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.59s/it]
INFO:root:eval mean loss: 22336.514136904763
INFO:root:eval perplexity: 10.091898918151855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [7:26:29<13:30:54, 374.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15887.343149038461
INFO:root:current train perplexity4.829540252685547
INFO:root:current mean train loss 15949.27998538669
INFO:root:current train perplexity4.82407808303833
INFO:root:current mean train loss 15993.257751209467
INFO:root:current train perplexity4.8362956047058105

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.61s/it]
INFO:root:final mean train loss: 15987.005288400958
INFO:root:final train perplexity: 4.839609146118164
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.40s/it]
INFO:root:eval mean loss: 22310.736351376487
INFO:root:eval perplexity: 10.065010070800781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [7:32:39<13:21:30, 372.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15938.691824776786
INFO:root:current train perplexity4.812530040740967
INFO:root:current mean train loss 15957.680362401832
INFO:root:current train perplexity4.822047710418701

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.40s/it]
INFO:root:final mean train loss: 15963.893353862148
INFO:root:final train perplexity: 4.828590393066406
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it]
INFO:root:eval mean loss: 22312.72288876488
INFO:root:eval perplexity: 10.067079544067383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [7:38:46<13:11:37, 371.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15953.60101744186
INFO:root:current train perplexity4.798868656158447
INFO:root:current mean train loss 15950.044484812062
INFO:root:current train perplexity4.815288066864014
INFO:root:current mean train loss 15956.798828125
INFO:root:current train perplexity4.818653583526611

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.23s/it]
INFO:root:final mean train loss: 15943.07838095388
INFO:root:final train perplexity: 4.818686485290527
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.37s/it]
INFO:root:eval mean loss: 22304.330705915178
INFO:root:eval perplexity: 10.058341979980469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [7:45:12<13:15:14, 375.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15917.1185546875
INFO:root:current train perplexity4.80276346206665
INFO:root:current mean train loss 15927.488321314102
INFO:root:current train perplexity4.808522701263428

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.65s/it]
INFO:root:final mean train loss: 15922.928112399193
INFO:root:final train perplexity: 4.80911922454834
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.44s/it]
INFO:root:eval mean loss: 22317.058012462796
INFO:root:eval perplexity: 10.0715970993042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [7:51:24<13:06:15, 374.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15874.96694232048
INFO:root:current train perplexity4.775422096252441
INFO:root:current mean train loss 15887.097589817176
INFO:root:current train perplexity4.780721664428711
INFO:root:current mean train loss 15904.692256294282
INFO:root:current train perplexity4.794109344482422

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.97s/it]
INFO:root:final mean train loss: 15892.275579637097
INFO:root:final train perplexity: 4.794602394104004
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:56<00:00, 56.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:56<00:00, 56.65s/it]
INFO:root:eval mean loss: 22315.53406343006
INFO:root:eval perplexity: 10.0700101852417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [7:57:33<12:56:41, 372.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15880.160264756945
INFO:root:current train perplexity4.782567977905273
INFO:root:current mean train loss 15884.870342925566
INFO:root:current train perplexity4.785197734832764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.01s/it]
INFO:root:final mean train loss: 15875.921343403477
INFO:root:final train perplexity: 4.786873817443848
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.30s/it]
INFO:root:eval mean loss: 22307.855492001487
INFO:root:eval perplexity: 10.06200885772705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [8:03:41<12:47:51, 371.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15863.080269607843
INFO:root:current train perplexity4.783700466156006
INFO:root:current mean train loss 15870.374223923842
INFO:root:current train perplexity4.775116443634033

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.66s/it]
INFO:root:final mean train loss: 15851.3009781376
INFO:root:final train perplexity: 4.775263786315918
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.92s/it]
INFO:root:eval mean loss: 22321.521158854168
INFO:root:eval perplexity: 10.076251983642578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [8:09:45<12:36:58, 369.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15453.588216145834
INFO:root:current train perplexity4.652076244354248
INFO:root:current mean train loss 15842.443131826456
INFO:root:current train perplexity4.760372161865234
INFO:root:current mean train loss 15833.048712669335
INFO:root:current train perplexity4.7618608474731445

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.68s/it]
INFO:root:final mean train loss: 15832.14666157384
INFO:root:final train perplexity: 4.7662506103515625
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.54s/it]
INFO:root:eval mean loss: 22309.62865048363
INFO:root:eval perplexity: 10.06385612487793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [8:16:01<12:34:39, 371.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15779.86709872159
INFO:root:current train perplexity4.757466793060303
INFO:root:current mean train loss 15806.70257686492
INFO:root:current train perplexity4.7539896965026855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:34<00:00, 334.40s/it]
INFO:root:final mean train loss: 15809.317556073589
INFO:root:final train perplexity: 4.755531311035156
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.65s/it]
INFO:root:eval mean loss: 22316.63195219494
INFO:root:eval perplexity: 10.07115364074707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [8:22:27<12:37:38, 375.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15735.708565848214
INFO:root:current train perplexity4.717179775238037
INFO:root:current mean train loss 15813.563668224298
INFO:root:current train perplexity4.745131969451904
INFO:root:current mean train loss 15816.737295252113
INFO:root:current train perplexity4.751256465911865

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.19s/it]
INFO:root:final mean train loss: 15787.390105216733
INFO:root:final train perplexity: 4.7452569007873535
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it]
INFO:root:eval mean loss: 22305.973609561013
INFO:root:eval perplexity: 10.060049057006836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [8:28:31<12:24:37, 372.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15715.548861228814
INFO:root:current train perplexity4.712639331817627
INFO:root:current mean train loss 15754.45122101022
INFO:root:current train perplexity4.726126194000244

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.30s/it]
INFO:root:final mean train loss: 15767.692398563508
INFO:root:final train perplexity: 4.736046314239502
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.05s/it]
INFO:root:eval mean loss: 22309.39571707589
INFO:root:eval perplexity: 10.063613891601562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [8:34:45<12:19:13, 372.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15818.104758522728
INFO:root:current train perplexity4.789523124694824
INFO:root:current mean train loss 15771.332180813626
INFO:root:current train perplexity4.722315788269043
INFO:root:current mean train loss 15769.50404046949
INFO:root:current train perplexity4.723124980926514

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.74s/it]
INFO:root:final mean train loss: 15739.531628024193
INFO:root:final train perplexity: 4.722910404205322
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.89s/it]
INFO:root:eval mean loss: 22315.775320870536
INFO:root:eval perplexity: 10.070260047912598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [8:41:25<12:28:50, 380.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15732.391679067461
INFO:root:current train perplexity4.7191314697265625
INFO:root:current mean train loss 15733.063009250383
INFO:root:current train perplexity4.724769115447998

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.37s/it]
INFO:root:final mean train loss: 15731.264534242691
INFO:root:final train perplexity: 4.71906042098999
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.86s/it]
INFO:root:eval mean loss: 22302.781901041668
INFO:root:eval perplexity: 10.056727409362793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [8:48:06<12:34:27, 386.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15702.228255208332
INFO:root:current train perplexity4.673287868499756
INFO:root:current mean train loss 15693.57393851902
INFO:root:current train perplexity4.703887939453125
INFO:root:current mean train loss 15720.866360828488
INFO:root:current train perplexity4.709325313568115

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.33s/it]
INFO:root:final mean train loss: 15706.02060231855
INFO:root:final train perplexity: 4.707326412200928
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.60s/it]
INFO:root:eval mean loss: 22319.80347842262
INFO:root:eval perplexity: 10.07446002960205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [8:54:29<12:25:53, 385.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15637.143904500932
INFO:root:current train perplexity4.682058811187744
INFO:root:current mean train loss 15692.51333855726
INFO:root:current train perplexity4.696287155151367

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.28s/it]
INFO:root:final mean train loss: 15690.610131048386
INFO:root:final train perplexity: 4.700176239013672
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.81s/it]
INFO:root:eval mean loss: 22303.54015531994
INFO:root:eval perplexity: 10.057514190673828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [9:00:53<12:18:14, 385.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15468.191252055922
INFO:root:current train perplexity4.684954643249512
INFO:root:current mean train loss 15691.652155002626
INFO:root:current train perplexity4.690068244934082
INFO:root:current mean train loss 15679.617789490581
INFO:root:current train perplexity4.688061237335205

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.56s/it]
INFO:root:final mean train loss: 15667.530962544102
INFO:root:final train perplexity: 4.689488410949707
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.75s/it]
INFO:root:eval mean loss: 22310.827590215773
INFO:root:eval perplexity: 10.065104484558105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [9:07:32<12:20:02, 389.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15587.36709121919
INFO:root:current train perplexity4.676784992218018
INFO:root:current mean train loss 15638.97615131579
INFO:root:current train perplexity4.675692081451416

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.48s/it]
INFO:root:final mean train loss: 15646.35472451487
INFO:root:final train perplexity: 4.679704189300537
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.96s/it]
INFO:root:eval mean loss: 22307.488676525296
INFO:root:eval perplexity: 10.061627388000488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [9:13:54<12:09:08, 387.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15671.663977581522
INFO:root:current train perplexity4.667548179626465
INFO:root:current mean train loss 15634.382066183944
INFO:root:current train perplexity4.6687140464782715
INFO:root:current mean train loss 15640.178737212724
INFO:root:current train perplexity4.6717424392700195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.56s/it]
INFO:root:final mean train loss: 15630.845612556705
INFO:root:final train perplexity: 4.672551155090332
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.29s/it]
INFO:root:eval mean loss: 22314.878836495536
INFO:root:eval perplexity: 10.069326400756836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [9:20:36<12:10:59, 391.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15614.728255208332
INFO:root:current train perplexity4.652267932891846
INFO:root:current mean train loss 15608.627438616071
INFO:root:current train perplexity4.6600661277771

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.06s/it]
INFO:root:final mean train loss: 15613.828061995968
INFO:root:final train perplexity: 4.66471529006958
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.15s/it]
INFO:root:eval mean loss: 22302.194149925595
INFO:root:eval perplexity: 10.056116104125977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [9:27:07<12:04:18, 391.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15554.028464988425
INFO:root:current train perplexity4.639758586883545
INFO:root:current mean train loss 15554.021653543306
INFO:root:current train perplexity4.6379618644714355
INFO:root:current mean train loss 15592.400012045704
INFO:root:current train perplexity4.651176929473877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:30<00:00, 330.67s/it]
INFO:root:final mean train loss: 15593.21136671497
INFO:root:final train perplexity: 4.655239105224609
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.58s/it]
INFO:root:eval mean loss: 22308.96512276786
INFO:root:eval perplexity: 10.063165664672852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [9:33:37<11:56:47, 390.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15639.974028382121
INFO:root:current train perplexity4.652318954467773
INFO:root:current mean train loss 15581.444745111732
INFO:root:current train perplexity4.646547794342041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.13s/it]
INFO:root:final mean train loss: 15580.757946383568
INFO:root:final train perplexity: 4.649524688720703
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.30s/it]
INFO:root:eval mean loss: 22288.28734188988
INFO:root:eval perplexity: 10.04165267944336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [9:39:43<11:36:37, 383.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15580.019657258064
INFO:root:current train perplexity4.649137020111084
INFO:root:current mean train loss 15560.626580391221
INFO:root:current train perplexity4.643845081329346
INFO:root:current mean train loss 15573.021260315205
INFO:root:current train perplexity4.64267635345459

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.91s/it]
INFO:root:final mean train loss: 15562.778060420867
INFO:root:final train perplexity: 4.641286849975586
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.18s/it]
INFO:root:eval mean loss: 22315.37146577381
INFO:root:eval perplexity: 10.069839477539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [9:46:25<11:40:21, 389.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15581.356774755272
INFO:root:current train perplexity4.642786979675293
INFO:root:current mean train loss 15557.590585510587
INFO:root:current train perplexity4.633457660675049

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.58s/it]
INFO:root:final mean train loss: 15544.96730484501
INFO:root:final train perplexity: 4.6331400871276855
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it]
INFO:root:eval mean loss: 22306.17443266369
INFO:root:eval perplexity: 10.060257911682129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [9:53:00<11:36:54, 390.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15525.020452008928
INFO:root:current train perplexity4.599118709564209
INFO:root:current mean train loss 15513.215538194445
INFO:root:current train perplexity4.61912202835083
INFO:root:current mean train loss 15537.825515292554
INFO:root:current train perplexity4.624329566955566

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.24s/it]
INFO:root:final mean train loss: 15527.387699250252
INFO:root:final train perplexity: 4.625113010406494
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.78s/it]
INFO:root:eval mean loss: 22318.415132068454
INFO:root:eval perplexity: 10.07301139831543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [9:59:25<11:27:19, 389.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15548.732073904454
INFO:root:current train perplexity4.614516735076904
INFO:root:current mean train loss 15533.004271808155
INFO:root:current train perplexity4.620415687561035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.92s/it]
INFO:root:final mean train loss: 15513.20400705645
INFO:root:final train perplexity: 4.618647575378418
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it]
INFO:root:eval mean loss: 22313.451683407737
INFO:root:eval perplexity: 10.067840576171875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [10:05:57<11:22:15, 389.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15444.047200520834
INFO:root:current train perplexity4.567351341247559
INFO:root:current mean train loss 15504.387723415019
INFO:root:current train perplexity4.597713470458984
INFO:root:current mean train loss 15520.343071718096
INFO:root:current train perplexity4.611837863922119

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.85s/it]
INFO:root:final mean train loss: 15497.318863407258
INFO:root:final train perplexity: 4.611416816711426
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.37s/it]
INFO:root:eval mean loss: 22318.31661551339
INFO:root:eval perplexity: 10.072908401489258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [10:12:19<11:11:38, 387.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15523.244526957418
INFO:root:current train perplexity4.60919713973999
INFO:root:current mean train loss 15482.014720017998
INFO:root:current train perplexity4.60414457321167

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.65s/it]
INFO:root:final mean train loss: 15480.532443138862
INFO:root:final train perplexity: 4.603788375854492
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.27s/it]
INFO:root:eval mean loss: 22320.15945870536
INFO:root:eval perplexity: 10.074830055236816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [10:18:32<10:57:50, 383.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15459.348292151162
INFO:root:current train perplexity4.602147102355957
INFO:root:current mean train loss 15474.709134615385
INFO:root:current train perplexity4.598987102508545
INFO:root:current mean train loss 15472.075914673354
INFO:root:current train perplexity4.5961995124816895

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.21s/it]
INFO:root:final mean train loss: 15463.75709189138
INFO:root:final train perplexity: 4.596177101135254
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.81s/it]
INFO:root:eval mean loss: 22314.419294084822
INFO:root:eval perplexity: 10.068848609924316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [10:24:54<10:50:41, 382.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15489.219572368422
INFO:root:current train perplexity4.586818218231201
INFO:root:current mean train loss 15463.384505208332
INFO:root:current train perplexity4.589231014251709

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:32<00:00, 332.98s/it]
INFO:root:final mean train loss: 15449.010714623237
INFO:root:final train perplexity: 4.589496612548828
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.38s/it]
INFO:root:eval mean loss: 22318.227399553572
INFO:root:eval perplexity: 10.07281494140625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [10:31:27<10:49:43, 385.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15378.00056100399
INFO:root:current train perplexity4.542978286743164
INFO:root:current mean train loss 15431.584336468963
INFO:root:current train perplexity4.572922229766846
INFO:root:current mean train loss 15444.624822083755
INFO:root:current train perplexity4.582002639770508

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.90s/it]
INFO:root:final mean train loss: 15434.005288400958
INFO:root:final train perplexity: 4.582708835601807
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it]
INFO:root:eval mean loss: 22312.929385230655
INFO:root:eval perplexity: 10.067293167114258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [10:37:50<10:41:52, 385.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15372.776160037878
INFO:root:current train perplexity4.559516429901123
INFO:root:current mean train loss 15438.750726287688
INFO:root:current train perplexity4.575875282287598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.56s/it]
INFO:root:final mean train loss: 15419.325923796623
INFO:root:final train perplexity: 4.576079368591309
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.28s/it]
INFO:root:eval mean loss: 22308.366768973214
INFO:root:eval perplexity: 10.062541961669922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [10:44:06<10:30:41, 382.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15448.754901960785
INFO:root:current train perplexity4.56121301651001
INFO:root:current mean train loss 15428.551499120447
INFO:root:current train perplexity4.570298194885254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.28s/it]
INFO:root:final mean train loss: 15405.782600648941
INFO:root:final train perplexity: 4.569970607757568
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.87s/it]
INFO:root:eval mean loss: 22309.872512090773
INFO:root:eval perplexity: 10.06411075592041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [10:50:15<10:18:04, 378.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15439.931315104166
INFO:root:current train perplexity4.493231773376465
INFO:root:current mean train loss 15424.211165048544
INFO:root:current train perplexity4.556827545166016
INFO:root:current mean train loss 15392.8226216133
INFO:root:current train perplexity4.559171676635742

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.11s/it]
INFO:root:final mean train loss: 15393.350223664314
INFO:root:final train perplexity: 4.564370632171631
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.74s/it]
INFO:root:eval mean loss: 22303.724632626487
INFO:root:eval perplexity: 10.057705879211426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [10:56:34<10:11:53, 378.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15356.4169921875
INFO:root:current train perplexity4.532539367675781
INFO:root:current mean train loss 15370.708726058469
INFO:root:current train perplexity4.553439140319824

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.71s/it]
INFO:root:final mean train loss: 15373.682097404233
INFO:root:final train perplexity: 4.555523872375488
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.00s/it]
INFO:root:eval mean loss: 22299.987397693454
INFO:root:eval perplexity: 10.053817749023438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [11:03:03<10:10:51, 381.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15166.69224330357
INFO:root:current train perplexity4.487386226654053
INFO:root:current mean train loss 15283.797002774532
INFO:root:current train perplexity4.532565116882324
INFO:root:current mean train loss 15369.057390738224
INFO:root:current train perplexity4.55094051361084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.06s/it]
INFO:root:final mean train loss: 15359.328648721019
INFO:root:final train perplexity: 4.549079418182373
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.86s/it]
INFO:root:eval mean loss: 22320.137834821428
INFO:root:eval perplexity: 10.074808120727539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [11:09:17<10:00:34, 379.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15358.120862023305
INFO:root:current train perplexity4.5460591316223145
INFO:root:current mean train loss 15392.87685485456
INFO:root:current train perplexity4.5501508712768555

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.19s/it]
INFO:root:final mean train loss: 15350.605043472782
INFO:root:final train perplexity: 4.545166969299316
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.14s/it]
INFO:root:eval mean loss: 22314.988676525296
INFO:root:eval perplexity: 10.069440841674805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [11:15:30<9:51:32, 377.58s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15082.893110795454
INFO:root:current train perplexity4.5027666091918945
INFO:root:current mean train loss 15299.01001196509
INFO:root:current train perplexity4.521088123321533
INFO:root:current mean train loss 15340.30452551096
INFO:root:current train perplexity4.534517765045166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.61s/it]
INFO:root:final mean train loss: 15331.413015057964
INFO:root:final train perplexity: 4.5365705490112305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.23s/it]
INFO:root:eval mean loss: 22322.923037574405
INFO:root:eval perplexity: 10.077713012695312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [11:21:37<9:39:52, 374.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15289.388625372023
INFO:root:current train perplexity4.523247718811035
INFO:root:current mean train loss 15328.511281393789
INFO:root:current train perplexity4.531243324279785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.15s/it]
INFO:root:final mean train loss: 15321.539716166835
INFO:root:final train perplexity: 4.5321550369262695
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.18s/it]
INFO:root:eval mean loss: 22327.57491629464
INFO:root:eval perplexity: 10.082565307617188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [11:27:49<9:32:53, 373.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15070.753645833332
INFO:root:current train perplexity4.510429859161377
INFO:root:current mean train loss 15289.425492527174
INFO:root:current train perplexity4.517398834228516
INFO:root:current mean train loss 15313.057994186047
INFO:root:current train perplexity4.522613048553467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.61s/it]
INFO:root:final mean train loss: 15309.079314201108
INFO:root:final train perplexity: 4.526589393615723
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.95s/it]
INFO:root:eval mean loss: 22342.99939546131
INFO:root:eval perplexity: 10.098673820495605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [11:33:58<9:24:28, 372.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15222.569292210821
INFO:root:current train perplexity4.5011396408081055
INFO:root:current mean train loss 15286.992854135478
INFO:root:current train perplexity4.518853664398193

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.08s/it]
INFO:root:final mean train loss: 15295.163975869456
INFO:root:final train perplexity: 4.52038049697876
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.31s/it]
INFO:root:eval mean loss: 22331.064127604168
INFO:root:eval perplexity: 10.086210250854492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [11:40:03<9:15:18, 370.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15295.067896792763
INFO:root:current train perplexity4.504360675811768
INFO:root:current mean train loss 15328.953108587184
INFO:root:current train perplexity4.51481294631958
INFO:root:current mean train loss 15289.698995790524
INFO:root:current train perplexity4.513023376464844

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.34s/it]
INFO:root:final mean train loss: 15282.744656470513
INFO:root:final train perplexity: 4.514847278594971
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.65s/it]
INFO:root:eval mean loss: 22328.77662295387
INFO:root:eval perplexity: 10.083817481994629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [11:46:06<9:05:36, 367.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15305.258335167253
INFO:root:current train perplexity4.515934467315674
INFO:root:current mean train loss 15288.48768731725
INFO:root:current train perplexity4.513331890106201

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.39s/it]
INFO:root:final mean train loss: 15273.046181955646
INFO:root:final train perplexity: 4.510529518127441
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.82s/it]
INFO:root:eval mean loss: 22335.452194940477
INFO:root:eval perplexity: 10.090789794921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [11:52:18<9:01:31, 369.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15198.323029891304
INFO:root:current train perplexity4.486656188964844
INFO:root:current mean train loss 15243.188532139227
INFO:root:current train perplexity4.502183437347412
INFO:root:current mean train loss 15264.453878223094
INFO:root:current train perplexity4.505608558654785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.18s/it]
INFO:root:final mean train loss: 15258.772791708669
INFO:root:final train perplexity: 4.504184722900391
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.37s/it]
INFO:root:eval mean loss: 22335.35226004464
INFO:root:eval perplexity: 10.090683937072754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [11:58:29<8:56:01, 369.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15229.093359375
INFO:root:current train perplexity4.492673397064209
INFO:root:current mean train loss 15250.16306919643
INFO:root:current train perplexity4.497589111328125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.57s/it]
INFO:root:final mean train loss: 15238.368447580646
INFO:root:final train perplexity: 4.495129108428955
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.95s/it]
INFO:root:eval mean loss: 22333.10230654762
INFO:root:eval perplexity: 10.088338851928711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [12:04:41<8:50:59, 370.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15220.534071180555
INFO:root:current train perplexity4.518341541290283
INFO:root:current mean train loss 15219.662862942914
INFO:root:current train perplexity4.490992546081543
INFO:root:current mean train loss 15237.854298595816
INFO:root:current train perplexity4.489319801330566

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.10s/it]
INFO:root:final mean train loss: 15233.016282604587
INFO:root:final train perplexity: 4.492756366729736
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:59<00:00, 59.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:59<00:00, 59.06s/it]
INFO:root:eval mean loss: 22324.347632998513
INFO:root:eval perplexity: 10.07919979095459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [12:11:18<8:56:13, 378.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15208.634444224683
INFO:root:current train perplexity4.483479022979736
INFO:root:current mean train loss 15237.517245329958
INFO:root:current train perplexity4.484236717224121

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.09s/it]
INFO:root:final mean train loss: 15220.822037235383
INFO:root:final train perplexity: 4.487356185913086
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.48s/it]
INFO:root:eval mean loss: 22334.491629464286
INFO:root:eval perplexity: 10.089786529541016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [12:17:32<8:47:55, 377.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15279.506016885081
INFO:root:current train perplexity4.496723651885986
INFO:root:current mean train loss 15247.886435472328
INFO:root:current train perplexity4.486656188964844
INFO:root:current mean train loss 15228.373148336039
INFO:root:current train perplexity4.485093116760254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.21s/it]
INFO:root:final mean train loss: 15211.281765845513
INFO:root:final train perplexity: 4.48313570022583
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.50s/it]
INFO:root:eval mean loss: 22349.769135974704
INFO:root:eval perplexity: 10.105752944946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [12:23:53<8:43:04, 378.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15208.780579348644
INFO:root:current train perplexity4.470946788787842
INFO:root:current mean train loss 15224.485415599385
INFO:root:current train perplexity4.47770881652832

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.25s/it]
INFO:root:final mean train loss: 15196.023654076362
INFO:root:final train perplexity: 4.476393222808838
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.52s/it]
INFO:root:eval mean loss: 22349.39636811756
INFO:root:eval perplexity: 10.105362892150879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [12:30:29<8:44:19, 383.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15134.563169642857
INFO:root:current train perplexity4.442979335784912
INFO:root:current mean train loss 15193.979166666666
INFO:root:current train perplexity4.46060848236084
INFO:root:current mean train loss 15195.466817652925
INFO:root:current train perplexity4.471634864807129

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.31s/it]
INFO:root:final mean train loss: 15184.54517782888
INFO:root:final train perplexity: 4.471328258514404
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.74s/it]
INFO:root:eval mean loss: 22334.928152901786
INFO:root:eval perplexity: 10.090241432189941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [12:36:46<8:35:07, 381.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15124.040959500719
INFO:root:current train perplexity4.460352420806885
INFO:root:current mean train loss 15176.121673420788
INFO:root:current train perplexity4.464301586151123

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.90s/it]
INFO:root:final mean train loss: 15171.497204196068
INFO:root:final train perplexity: 4.465578079223633
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.79s/it]
INFO:root:eval mean loss: 22344.93591889881
INFO:root:eval perplexity: 10.100699424743652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [12:42:52<8:22:41, 377.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15107.87580128205
INFO:root:current train perplexity4.436193466186523
INFO:root:current mean train loss 15139.933706160073
INFO:root:current train perplexity4.456011772155762
INFO:root:current mean train loss 15172.654999673117
INFO:root:current train perplexity4.461558818817139

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.51s/it]
INFO:root:final mean train loss: 15163.489655525455
INFO:root:final train perplexity: 4.462052345275879
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.56s/it]
INFO:root:eval mean loss: 22346.85672433036
INFO:root:eval perplexity: 10.102707862854004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [12:49:04<8:14:21, 375.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15129.157784598214
INFO:root:current train perplexity4.448380470275879
INFO:root:current mean train loss 15146.078907272578
INFO:root:current train perplexity4.447175025939941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:06<00:00, 306.36s/it]
INFO:root:final mean train loss: 15153.512057396674
INFO:root:final train perplexity: 4.457663536071777
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.60s/it]
INFO:root:eval mean loss: 22335.241280691964
INFO:root:eval perplexity: 10.090568542480469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [12:55:08<8:03:41, 372.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15117.289471293605
INFO:root:current train perplexity4.462569713592529
INFO:root:current mean train loss 15161.75534719187
INFO:root:current train perplexity4.4546799659729
INFO:root:current mean train loss 15147.908492476852
INFO:root:current train perplexity4.450137615203857

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.06s/it]
INFO:root:final mean train loss: 15135.8853484123
INFO:root:final train perplexity: 4.449920177459717
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.94s/it]
INFO:root:eval mean loss: 22351.27469308036
INFO:root:eval perplexity: 10.10732650756836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [13:01:30<8:01:00, 374.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15107.586379523027
INFO:root:current train perplexity4.442625045776367
INFO:root:current mean train loss 15149.644586338141
INFO:root:current train perplexity4.44623327255249

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.47s/it]
INFO:root:final mean train loss: 15130.830897177419
INFO:root:final train perplexity: 4.447701930999756
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it]
INFO:root:eval mean loss: 22354.288527715773
INFO:root:eval perplexity: 10.110481262207031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [13:07:35<7:51:15, 372.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15120.322847406915
INFO:root:current train perplexity4.43255090713501
INFO:root:current mean train loss 15133.151161245749
INFO:root:current train perplexity4.439311981201172
INFO:root:current mean train loss 15134.265965017712
INFO:root:current train perplexity4.443506240844727

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.50s/it]
INFO:root:final mean train loss: 15120.36140688004
INFO:root:final train perplexity: 4.443111419677734
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.37s/it]
INFO:root:eval mean loss: 22351.4423828125
INFO:root:eval perplexity: 10.107501983642578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [13:14:01<7:50:14, 376.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15107.394156407829
INFO:root:current train perplexity4.436936378479004
INFO:root:current mean train loss 15117.394614674937
INFO:root:current train perplexity4.435333251953125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.71s/it]
INFO:root:final mean train loss: 15110.40640357233
INFO:root:final train perplexity: 4.438751697540283
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.18s/it]
INFO:root:eval mean loss: 22345.66385323661
INFO:root:eval perplexity: 10.101460456848145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [13:20:19<7:44:42, 376.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15102.671932444853
INFO:root:current train perplexity4.42836856842041
INFO:root:current mean train loss 15092.86918589611
INFO:root:current train perplexity4.430264949798584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:31<00:00, 331.69s/it]
INFO:root:final mean train loss: 15105.121022870464
INFO:root:final train perplexity: 4.436438083648682
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.97s/it]
INFO:root:eval mean loss: 22350.000465029763
INFO:root:eval perplexity: 10.10599422454834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [13:26:48<7:42:43, 380.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14984.830403645834
INFO:root:current train perplexity4.465308666229248
INFO:root:current mean train loss 15094.769872572815
INFO:root:current train perplexity4.434309959411621
INFO:root:current mean train loss 15087.406596366995
INFO:root:current train perplexity4.425240993499756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.96s/it]
INFO:root:final mean train loss: 15088.383568548386
INFO:root:final train perplexity: 4.429120063781738
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.25s/it]
INFO:root:eval mean loss: 22353.479189918155
INFO:root:eval perplexity: 10.109635353088379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [13:33:06<7:35:28, 379.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15104.184055397727
INFO:root:current train perplexity4.428765773773193
INFO:root:current mean train loss 15083.440738407258
INFO:root:current train perplexity4.42567253112793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.20s/it]
INFO:root:final mean train loss: 15076.827935987903
INFO:root:final train perplexity: 4.424074649810791
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.16s/it]
INFO:root:eval mean loss: 22354.819149925595
INFO:root:eval perplexity: 10.111035346984863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [13:39:45<7:36:04, 385.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15125.567103794643
INFO:root:current train perplexity4.388755798339844
INFO:root:current mean train loss 15093.190502701518
INFO:root:current train perplexity4.429398059844971
INFO:root:current mean train loss 15079.97855808424
INFO:root:current train perplexity4.421541213989258

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.61s/it]
INFO:root:final mean train loss: 15071.410601215977
INFO:root:final train perplexity: 4.4217119216918945
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.11s/it]
INFO:root:eval mean loss: 22386.48946707589
INFO:root:eval perplexity: 10.144231796264648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [13:46:00<7:26:05, 382.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15104.835291975636
INFO:root:current train perplexity4.428790092468262
INFO:root:current mean train loss 15055.893352004718
INFO:root:current train perplexity4.418368816375732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.52s/it]
INFO:root:final mean train loss: 15064.208429151966
INFO:root:final train perplexity: 4.418571949005127
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.73s/it]
INFO:root:eval mean loss: 22360.557431175595
INFO:root:eval perplexity: 10.117039680480957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [13:52:10<7:15:20, 378.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14975.834517045454
INFO:root:current train perplexity4.436728477478027
INFO:root:current mean train loss 15062.611240146396
INFO:root:current train perplexity4.411515712738037
INFO:root:current mean train loss 15058.83154528288
INFO:root:current train perplexity4.413508415222168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.76s/it]
INFO:root:final mean train loss: 15055.21450904108
INFO:root:final train perplexity: 4.41465425491333
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.87s/it]
INFO:root:eval mean loss: 22364.169968377977
INFO:root:eval perplexity: 10.12082576751709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [13:58:33<7:10:43, 380.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14943.983599950398
INFO:root:current train perplexity4.396721363067627
INFO:root:current mean train loss 15010.286588142255
INFO:root:current train perplexity4.4002909660339355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.16s/it]
INFO:root:final mean train loss: 15042.497562531502
INFO:root:final train perplexity: 4.409120082855225
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.40s/it]
INFO:root:eval mean loss: 22363.83324032738
INFO:root:eval perplexity: 10.120471000671387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [14:04:52<7:03:50, 379.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15013.835611979166
INFO:root:current train perplexity4.420329570770264
INFO:root:current mean train loss 15051.141278872283
INFO:root:current train perplexity4.412815570831299
INFO:root:current mean train loss 15043.570925690407
INFO:root:current train perplexity4.404020309448242

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.95s/it]
INFO:root:final mean train loss: 15036.00697375882
INFO:root:final train perplexity: 4.40629768371582
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.19s/it]
INFO:root:eval mean loss: 22365.237490699405
INFO:root:eval perplexity: 10.12194538116455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [14:11:16<6:58:59, 380.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14988.4541015625
INFO:root:current train perplexity4.386750221252441
INFO:root:current mean train loss 15062.226925056138
INFO:root:current train perplexity4.408905029296875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.32s/it]
INFO:root:final mean train loss: 15025.704802482358
INFO:root:final train perplexity: 4.401823043823242
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.17s/it]
INFO:root:eval mean loss: 22364.605654761905
INFO:root:eval perplexity: 10.121281623840332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [14:17:24<6:48:41, 377.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15067.316509046053
INFO:root:current train perplexity4.400177955627441
INFO:root:current mean train loss 15060.205767463236
INFO:root:current train perplexity4.411940574645996
INFO:root:current mean train loss 15057.054402111871
INFO:root:current train perplexity4.404047012329102

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.88s/it]
INFO:root:final mean train loss: 15023.222439673638
INFO:root:final train perplexity: 4.400745391845703
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:58<00:00, 58.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:58<00:00, 58.24s/it]
INFO:root:eval mean loss: 22376.295549665178
INFO:root:eval perplexity: 10.133532524108887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [14:23:37<6:40:50, 375.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15000.682878521127
INFO:root:current train perplexity4.387301921844482
INFO:root:current mean train loss 15038.470663148757
INFO:root:current train perplexity4.396049976348877

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.77s/it]
INFO:root:final mean train loss: 15007.16218813004
INFO:root:final train perplexity: 4.393780708312988
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.32s/it]
INFO:root:eval mean loss: 22374.629255022322
INFO:root:eval perplexity: 10.131786346435547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [14:29:43<6:31:34, 372.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14911.200534986414
INFO:root:current train perplexity4.3646039962768555
INFO:root:current mean train loss 14994.649961890244
INFO:root:current train perplexity4.39034366607666
INFO:root:current mean train loss 15002.453878223094
INFO:root:current train perplexity4.3896379470825195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:08<00:00, 308.78s/it]
INFO:root:final mean train loss: 14997.424698368195
INFO:root:final train perplexity: 4.389562606811523
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.38s/it]
INFO:root:eval mean loss: 22375.626116071428
INFO:root:eval perplexity: 10.132832527160645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [14:35:43<6:21:28, 369.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14945.781471354167
INFO:root:current train perplexity4.370687961578369
INFO:root:current mean train loss 14993.297996651785
INFO:root:current train perplexity4.378810882568359

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.94s/it]
INFO:root:final mean train loss: 14992.235804403981
INFO:root:final train perplexity: 4.3873162269592285
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.38s/it]
INFO:root:eval mean loss: 22369.800432477678
INFO:root:eval perplexity: 10.126724243164062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [14:42:05<6:19:04, 372.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15011.50339988426
INFO:root:current train perplexity4.39298152923584
INFO:root:current mean train loss 14960.757243479331
INFO:root:current train perplexity4.376572608947754
INFO:root:current mean train loss 15000.943367979075
INFO:root:current train perplexity4.384699821472168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.47s/it]
INFO:root:final mean train loss: 14990.09613233997
INFO:root:final train perplexity: 4.386390686035156
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.93s/it]
INFO:root:eval mean loss: 22372.619977678572
INFO:root:eval perplexity: 10.129679679870605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [14:48:21<6:13:58, 373.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14932.569422468354
INFO:root:current train perplexity4.375258922576904
INFO:root:current mean train loss 14961.537654940643
INFO:root:current train perplexity4.374594688415527

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.81s/it]
INFO:root:final mean train loss: 14971.892373361896
INFO:root:final train perplexity: 4.378521919250488
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it]
INFO:root:eval mean loss: 22376.831868489582
INFO:root:eval perplexity: 10.134096145629883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [14:54:30<6:06:05, 372.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14965.620558215725
INFO:root:current train perplexity4.385288715362549
INFO:root:current mean train loss 14985.124962726622
INFO:root:current train perplexity4.383324146270752
INFO:root:current mean train loss 14981.675629058442
INFO:root:current train perplexity4.380136489868164

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.49s/it]
INFO:root:final mean train loss: 14970.735954038559
INFO:root:final train perplexity: 4.378022193908691
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.35s/it]
INFO:root:eval mean loss: 22389.218656994046
INFO:root:eval perplexity: 10.147095680236816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [15:00:49<6:01:49, 374.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15037.01037744729
INFO:root:current train perplexity4.371397018432617
INFO:root:current mean train loss 14983.027925418375
INFO:root:current train perplexity4.369764804840088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.49s/it]
INFO:root:final mean train loss: 14966.500893869708
INFO:root:final train perplexity: 4.376194000244141
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.30s/it]
INFO:root:eval mean loss: 22379.907877604168
INFO:root:eval perplexity: 10.137327194213867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [15:07:01<5:54:55, 373.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14900.202204241072
INFO:root:current train perplexity4.339595794677734
INFO:root:current mean train loss 14943.523748553242
INFO:root:current train perplexity4.358821392059326
INFO:root:current mean train loss 14952.030069813829
INFO:root:current train perplexity4.369033336639404

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:13<00:00, 313.90s/it]
INFO:root:final mean train loss: 14950.765908518146
INFO:root:final train perplexity: 4.369407653808594
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.92s/it]
INFO:root:eval mean loss: 22381.003278459822
INFO:root:eval perplexity: 10.138472557067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [15:13:08<5:46:51, 371.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14933.989605783046
INFO:root:current train perplexity4.3596391677856445
INFO:root:current mean train loss 14970.603573069853
INFO:root:current train perplexity4.3710527420043945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.03s/it]
INFO:root:final mean train loss: 14948.625807239163
INFO:root:final train perplexity: 4.368485450744629
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.89s/it]
INFO:root:eval mean loss: 22388.069963727678
INFO:root:eval perplexity: 10.145888328552246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [15:19:23<5:41:37, 372.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14954.355869391025
INFO:root:current train perplexity4.358673572540283
INFO:root:current mean train loss 14943.99623426259
INFO:root:current train perplexity4.363709926605225
INFO:root:current mean train loss 14948.465987839958
INFO:root:current train perplexity4.363718032836914

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.25s/it]
INFO:root:final mean train loss: 14937.901209677419
INFO:root:final train perplexity: 4.363867282867432
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.19s/it]
INFO:root:eval mean loss: 22387.221214657737
INFO:root:eval perplexity: 10.144998550415039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [15:25:37<5:35:40, 372.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14981.51239483173
INFO:root:current train perplexity4.355580806732178
INFO:root:current mean train loss 14936.488736297448
INFO:root:current train perplexity4.352223873138428

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.86s/it]
INFO:root:final mean train loss: 14932.725778887348
INFO:root:final train perplexity: 4.361639976501465
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.71s/it]
INFO:root:eval mean loss: 22393.87390718006
INFO:root:eval perplexity: 10.151988983154297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [15:31:46<5:28:36, 372.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14931.890216206395
INFO:root:current train perplexity4.361309051513672
INFO:root:current mean train loss 14920.588621339597
INFO:root:current train perplexity4.350885391235352
INFO:root:current mean train loss 14941.785172325102
INFO:root:current train perplexity4.358880519866943

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.66s/it]
INFO:root:final mean train loss: 14926.352893460182
INFO:root:final train perplexity: 4.358898639678955
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.47s/it]
INFO:root:eval mean loss: 22390.766438802082
INFO:root:eval perplexity: 10.148721694946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [15:38:01<5:23:00, 372.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14941.682000411185
INFO:root:current train perplexity4.35711669921875
INFO:root:current mean train loss 14919.396168870193
INFO:root:current train perplexity4.352657794952393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.07s/it]
INFO:root:final mean train loss: 14919.438980594758
INFO:root:final train perplexity: 4.355927467346191
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.51s/it]
INFO:root:eval mean loss: 22389.481980096727
INFO:root:eval perplexity: 10.147374153137207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [15:44:17<5:17:50, 373.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14891.309050864362
INFO:root:current train perplexity4.337878704071045
INFO:root:current mean train loss 14886.883224383504
INFO:root:current train perplexity4.340702056884766
INFO:root:current mean train loss 14918.898947526568
INFO:root:current train perplexity4.351123332977295

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.40s/it]
INFO:root:final mean train loss: 14908.201195501511
INFO:root:final train perplexity: 4.351101875305176
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:03<00:00, 63.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:03<00:00, 63.57s/it]
INFO:root:eval mean loss: 22382.722121465773
INFO:root:eval perplexity: 10.140277862548828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [15:50:40<5:13:40, 376.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14895.171727035984
INFO:root:current train perplexity4.3412933349609375
INFO:root:current mean train loss 14885.524698688756
INFO:root:current train perplexity4.345695495605469

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.81s/it]
INFO:root:final mean train loss: 14907.1812468498
INFO:root:final train perplexity: 4.3506646156311035
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.92s/it]
INFO:root:eval mean loss: 22395.737351190477
INFO:root:eval perplexity: 10.153944969177246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [15:56:51<5:06:02, 374.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14884.103266697304
INFO:root:current train perplexity4.344131946563721
INFO:root:current mean train loss 14875.626351665976
INFO:root:current train perplexity4.337596416473389

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.71s/it]
INFO:root:final mean train loss: 14892.984851467994
INFO:root:final train perplexity: 4.344576358795166
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.77s/it]
INFO:root:eval mean loss: 22395.77415829613
INFO:root:eval perplexity: 10.153985977172852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [16:03:04<4:59:33, 374.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14962.886393229166
INFO:root:current train perplexity4.3537421226501465
INFO:root:current mean train loss 14883.157596328883
INFO:root:current train perplexity4.337041854858398
INFO:root:current mean train loss 14904.315059267241
INFO:root:current train perplexity4.34438943862915

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.55s/it]
INFO:root:final mean train loss: 14893.399886592742
INFO:root:final train perplexity: 4.344754695892334
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it]
INFO:root:eval mean loss: 22396.40206473214
INFO:root:eval perplexity: 10.154646873474121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [16:09:11<4:51:35, 372.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14846.891761363637
INFO:root:current train perplexity4.319857597351074
INFO:root:current mean train loss 14868.411164314517
INFO:root:current train perplexity4.331357002258301

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.84s/it]
INFO:root:final mean train loss: 14885.304699313256
INFO:root:final train perplexity: 4.341286659240723
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.74s/it]
INFO:root:eval mean loss: 22404.583914620536
INFO:root:eval perplexity: 10.16324520111084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [16:15:47<4:50:44, 379.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14986.19907924107
INFO:root:current train perplexity4.391448497772217
INFO:root:current mean train loss 14850.315165011682
INFO:root:current train perplexity4.334175109863281
INFO:root:current mean train loss 14893.281467013889
INFO:root:current train perplexity4.339188098907471

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:10<00:00, 310.61s/it]
INFO:root:final mean train loss: 14878.282935357864
INFO:root:final train perplexity: 4.338281154632568
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.22s/it]
INFO:root:eval mean loss: 22405.056059337796
INFO:root:eval perplexity: 10.163742065429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [16:21:51<4:41:03, 374.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14921.080988479873
INFO:root:current train perplexity4.342078685760498
INFO:root:current mean train loss 14874.147006436713
INFO:root:current train perplexity4.335896968841553

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:11<00:00, 311.56s/it]
INFO:root:final mean train loss: 14876.783431514617
INFO:root:final train perplexity: 4.337640285491943
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.40s/it]
INFO:root:eval mean loss: 22398.361723400296
INFO:root:eval perplexity: 10.156704902648926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [16:27:54<4:32:16, 371.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14840.549538352272
INFO:root:current train perplexity4.284127712249756
INFO:root:current mean train loss 14868.437878307996
INFO:root:current train perplexity4.324386119842529
INFO:root:current mean train loss 14873.548212566648
INFO:root:current train perplexity4.329361438751221

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.47s/it]
INFO:root:final mean train loss: 14865.493833480343
INFO:root:final train perplexity: 4.332812786102295
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.88s/it]
INFO:root:eval mean loss: 22401.825846354168
INFO:root:eval perplexity: 10.160343170166016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [16:34:47<4:34:59, 383.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14796.735584077382
INFO:root:current train perplexity4.320047378540039
INFO:root:current mean train loss 14873.75384034701
INFO:root:current train perplexity4.333522796630859

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.56s/it]
INFO:root:final mean train loss: 14862.750307144657
INFO:root:final train perplexity: 4.331639766693115
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.03s/it]
INFO:root:eval mean loss: 22406.534737723214
INFO:root:eval perplexity: 10.165300369262695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [16:41:37<4:33:59, 391.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14942.2798828125
INFO:root:current train perplexity4.333462715148926
INFO:root:current mean train loss 14879.240225883152
INFO:root:current train perplexity4.3381195068359375
INFO:root:current mean train loss 14874.766224563953
INFO:root:current train perplexity4.332225799560547

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.65s/it]
INFO:root:final mean train loss: 14860.323053175403
INFO:root:final train perplexity: 4.330604076385498
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:57<00:00, 57.60s/it]
INFO:root:eval mean loss: 22409.954194568454
INFO:root:eval perplexity: 10.16889476776123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [16:48:34<4:32:53, 399.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14824.55420650653
INFO:root:current train perplexity4.308741569519043
INFO:root:current mean train loss 14835.563418085703
INFO:root:current train perplexity4.317047119140625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.60s/it]
INFO:root:final mean train loss: 14847.003638482864
INFO:root:final train perplexity: 4.324917793273926
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.13s/it]
INFO:root:eval mean loss: 22404.94835844494
INFO:root:eval perplexity: 10.163627624511719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [16:55:08<4:25:04, 397.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14825.18791118421
INFO:root:current train perplexity4.303176403045654
INFO:root:current mean train loss 14859.937106092437
INFO:root:current train perplexity4.322779655456543
INFO:root:current mean train loss 14852.432643942637
INFO:root:current train perplexity4.32712459564209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.64s/it]
INFO:root:final mean train loss: 14850.798276839718
INFO:root:final train perplexity: 4.326537132263184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.17s/it]
INFO:root:eval mean loss: 22413.587890625
INFO:root:eval perplexity: 10.17271900177002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [17:01:39<4:17:13, 395.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14850.350420884683
INFO:root:current train perplexity4.310732364654541
INFO:root:current mean train loss 14843.458995796784
INFO:root:current train perplexity4.314272880554199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.94s/it]
INFO:root:final mean train loss: 14839.585260206653
INFO:root:final train perplexity: 4.321754455566406
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.39s/it]
INFO:root:eval mean loss: 22408.18582589286
INFO:root:eval perplexity: 10.167036056518555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [17:08:22<4:11:59, 397.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14832.5380859375
INFO:root:current train perplexity4.331380367279053
INFO:root:current mean train loss 14884.124603023374
INFO:root:current train perplexity4.325204849243164
INFO:root:current mean train loss 14846.57062780269
INFO:root:current train perplexity4.317605018615723

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.73s/it]
INFO:root:final mean train loss: 14835.519495810231
INFO:root:final train perplexity: 4.320022106170654
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.31s/it]
INFO:root:eval mean loss: 22407.91385323661
INFO:root:eval perplexity: 10.166749954223633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [17:14:34<4:00:36, 390.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14806.014778645833
INFO:root:current train perplexity4.322207450866699
INFO:root:current mean train loss 14819.74428013393
INFO:root:current train perplexity4.314797401428223

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.71s/it]
INFO:root:final mean train loss: 14828.721356791835
INFO:root:final train perplexity: 4.3171257972717285
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.66s/it]
INFO:root:eval mean loss: 22409.393508184523
INFO:root:eval perplexity: 10.168306350708008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [17:20:52<3:51:50, 386.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14861.176685474536
INFO:root:current train perplexity4.305981636047363
INFO:root:current mean train loss 14864.428672490158
INFO:root:current train perplexity4.32999849319458
INFO:root:current mean train loss 14841.29899590446
INFO:root:current train perplexity4.317948818206787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.19s/it]
INFO:root:final mean train loss: 14822.83128701487
INFO:root:final train perplexity: 4.314619064331055
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.64s/it]
INFO:root:eval mean loss: 22415.72433035714
INFO:root:eval perplexity: 10.174972534179688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [17:27:06<3:43:13, 382.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14855.362613726265
INFO:root:current train perplexity4.32142972946167
INFO:root:current mean train loss 14848.355550584847
INFO:root:current train perplexity4.317972183227539

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.94s/it]
INFO:root:final mean train loss: 14821.400926159275
INFO:root:final train perplexity: 4.314009666442871
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.43s/it]
INFO:root:eval mean loss: 22414.046061197918
INFO:root:eval perplexity: 10.173201560974121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [17:33:14<3:34:26, 378.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14830.071478074597
INFO:root:current train perplexity4.305142402648926
INFO:root:current mean train loss 14836.725444298665
INFO:root:current train perplexity4.306925296783447
INFO:root:current mean train loss 14829.926554890422
INFO:root:current train perplexity4.312510967254639

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:14<00:00, 314.08s/it]
INFO:root:final mean train loss: 14815.754953692036
INFO:root:final train perplexity: 4.311607837677002
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.45s/it]
INFO:root:eval mean loss: 22415.763020833332
INFO:root:eval perplexity: 10.17501163482666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [17:39:21<3:26:12, 374.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14808.313347138554
INFO:root:current train perplexity4.30411958694458
INFO:root:current mean train loss 14826.704218963456
INFO:root:current train perplexity4.30633020401001

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.81s/it]
INFO:root:final mean train loss: 14805.958507907006
INFO:root:final train perplexity: 4.3074445724487305
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.98s/it]
INFO:root:eval mean loss: 22411.740187872023
INFO:root:eval perplexity: 10.1707763671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [17:45:34<3:19:39, 374.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14949.915262276785
INFO:root:current train perplexity4.340640068054199
INFO:root:current mean train loss 14803.344212962964
INFO:root:current train perplexity4.301690578460693
INFO:root:current mean train loss 14822.362952958776
INFO:root:current train perplexity4.308396339416504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.63s/it]
INFO:root:final mean train loss: 14804.192004788307
INFO:root:final train perplexity: 4.306694030761719
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.09s/it]
INFO:root:eval mean loss: 22415.742582775296
INFO:root:eval perplexity: 10.174991607666016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [17:51:51<3:13:48, 375.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14803.11630073635
INFO:root:current train perplexity4.307366847991943
INFO:root:current mean train loss 14803.834276821524
INFO:root:current train perplexity4.305633068084717

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.35s/it]
INFO:root:final mean train loss: 14805.197639711441
INFO:root:final train perplexity: 4.3071208000183105
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.85s/it]
INFO:root:eval mean loss: 22423.188058035714
INFO:root:eval perplexity: 10.182835578918457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [17:58:13<3:08:31, 377.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14728.043719951924
INFO:root:current train perplexity4.275289535522461
INFO:root:current mean train loss 14779.733068232914
INFO:root:current train perplexity4.303399085998535
INFO:root:current mean train loss 14809.315454203714
INFO:root:current train perplexity4.306731700897217

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 319.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.58s/it]
INFO:root:final mean train loss: 14801.330223821824
INFO:root:final train perplexity: 4.305478572845459
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.32s/it]
INFO:root:eval mean loss: 22417.91090029762
INFO:root:eval perplexity: 10.17727279663086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [18:04:54<3:05:49, 384.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14842.591056404533
INFO:root:current train perplexity4.322195053100586
INFO:root:current mean train loss 14792.20932693881
INFO:root:current train perplexity4.302175521850586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:39<00:00, 339.18s/it]
INFO:root:final mean train loss: 14797.051706621723
INFO:root:final train perplexity: 4.303661823272705
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.37s/it]
INFO:root:eval mean loss: 22417.563151041668
INFO:root:eval perplexity: 10.176908493041992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [18:11:26<3:00:26, 386.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14757.773233103198
INFO:root:current train perplexity4.293976306915283
INFO:root:current mean train loss 14813.806975251311
INFO:root:current train perplexity4.304566860198975
INFO:root:current mean train loss 14806.30103443287
INFO:root:current train perplexity4.301435470581055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.28s/it]
INFO:root:final mean train loss: 14791.992671843498
INFO:root:final train perplexity: 4.301515102386475
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.71s/it]
INFO:root:eval mean loss: 22424.87830171131
INFO:root:eval perplexity: 10.184615135192871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [18:17:43<2:52:43, 383.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14821.119664884869
INFO:root:current train perplexity4.301238059997559
INFO:root:current mean train loss 14809.101166866987
INFO:root:current train perplexity4.300341606140137

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.72s/it]
INFO:root:final mean train loss: 14789.021535565777
INFO:root:final train perplexity: 4.3002543449401855
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.43s/it]
INFO:root:eval mean loss: 22422.129836309523
INFO:root:eval perplexity: 10.181718826293945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [18:24:05<2:46:01, 383.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14722.380734707447
INFO:root:current train perplexity4.280245780944824
INFO:root:current mean train loss 14777.114410607994
INFO:root:current train perplexity4.289294242858887
INFO:root:current mean train loss 14798.11482319079
INFO:root:current train perplexity4.299417018890381

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.77s/it]
INFO:root:final mean train loss: 14786.03943264869
INFO:root:final train perplexity: 4.298990249633789
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.91s/it]
INFO:root:eval mean loss: 22417.077473958332
INFO:root:eval perplexity: 10.176396369934082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [18:30:26<2:39:25, 382.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14774.798818260731
INFO:root:current train perplexity4.290238857269287
INFO:root:current mean train loss 14783.804859257223
INFO:root:current train perplexity4.29336404800415

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.20s/it]
INFO:root:final mean train loss: 14781.021224483367
INFO:root:final train perplexity: 4.296862602233887
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.13s/it]
INFO:root:eval mean loss: 22420.478004092263
INFO:root:eval perplexity: 10.179978370666504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [18:37:19<2:36:36, 391.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14791.999655330883
INFO:root:current train perplexity4.282410621643066
INFO:root:current mean train loss 14779.060721492137
INFO:root:current train perplexity4.288869857788086

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:17<00:00, 317.03s/it]
INFO:root:final mean train loss: 14779.917952998992
INFO:root:final train perplexity: 4.2963948249816895
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.09s/it]
INFO:root:eval mean loss: 22425.78515625
INFO:root:eval perplexity: 10.185569763183594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [18:43:44<2:29:20, 389.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14794.867513020834
INFO:root:current train perplexity4.274066925048828
INFO:root:current mean train loss 14766.227870904126
INFO:root:current train perplexity4.2898759841918945
INFO:root:current mean train loss 14790.519843942426
INFO:root:current train perplexity4.291745662689209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.21s/it]
INFO:root:final mean train loss: 14770.938606508316
INFO:root:final train perplexity: 4.2925920486450195
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.59s/it]
INFO:root:eval mean loss: 22428.892624627977
INFO:root:eval perplexity: 10.188848495483398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [18:50:29<2:24:34, 394.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14824.512251420454
INFO:root:current train perplexity4.2821574211120605
INFO:root:current mean train loss 14803.306905241936
INFO:root:current train perplexity4.292734622955322

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 323.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 323.00s/it]
INFO:root:final mean train loss: 14771.825439453125
INFO:root:final train perplexity: 4.292967319488525
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.42s/it]
INFO:root:eval mean loss: 22427.705961681546
INFO:root:eval perplexity: 10.18759536743164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [18:57:12<2:18:52, 396.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14671.384207589286
INFO:root:current train perplexity4.264456748962402
INFO:root:current mean train loss 14752.758487879673
INFO:root:current train perplexity4.291165351867676
INFO:root:current mean train loss 14775.533755095108
INFO:root:current train perplexity4.289765357971191

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.95s/it]
INFO:root:final mean train loss: 14766.34829810358
INFO:root:final train perplexity: 4.290648460388184
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.49s/it]
INFO:root:eval mean loss: 22424.674386160714
INFO:root:eval perplexity: 10.184401512145996
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [19:03:34<2:10:49, 392.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14742.673017081568
INFO:root:current train perplexity4.288825035095215
INFO:root:current mean train loss 14756.744914504718
INFO:root:current train perplexity4.284405708312988

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:24<00:00, 324.37s/it]
INFO:root:final mean train loss: 14767.441548009072
INFO:root:final train perplexity: 4.291110992431641
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.41s/it]
INFO:root:eval mean loss: 22423.122977120536
INFO:root:eval perplexity: 10.182765007019043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [19:10:09<2:04:32, 393.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14849.69540127841
INFO:root:current train perplexity4.270198822021484
INFO:root:current mean train loss 14790.132706925675
INFO:root:current train perplexity4.2920708656311035
INFO:root:current mean train loss 14780.598549503851
INFO:root:current train perplexity4.291520118713379

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:18<00:00, 318.75s/it]
INFO:root:final mean train loss: 14763.784703408519
INFO:root:final train perplexity: 4.2895636558532715
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.04s/it]
INFO:root:eval mean loss: 22431.255557105655
INFO:root:eval perplexity: 10.191338539123535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [19:16:51<1:58:46, 395.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14710.509285094246
INFO:root:current train perplexity4.277429580688477
INFO:root:current mean train loss 14748.928800805215
INFO:root:current train perplexity4.278616905212402

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:20<00:00, 320.20s/it]
INFO:root:final mean train loss: 14757.250074817288
INFO:root:final train perplexity: 4.286799430847168
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.35s/it]
INFO:root:eval mean loss: 22432.612165178572
INFO:root:eval perplexity: 10.192771911621094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [19:23:20<1:51:35, 393.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14739.233138020832
INFO:root:current train perplexity4.302247047424316
INFO:root:current mean train loss 14779.848199728262
INFO:root:current train perplexity4.294869899749756
INFO:root:current mean train loss 14771.31281340843
INFO:root:current train perplexity4.2880988121032715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.49s/it]
INFO:root:final mean train loss: 14758.6996125252
INFO:root:final train perplexity: 4.287412643432617
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.10s/it]
INFO:root:eval mean loss: 22425.732747395832
INFO:root:eval perplexity: 10.185516357421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [19:29:38<1:43:46, 389.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14779.803842117537
INFO:root:current train perplexity4.275976181030273
INFO:root:current mean train loss 14757.995789670658
INFO:root:current train perplexity4.27979040145874

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.52s/it]
INFO:root:final mean train loss: 14751.65619093372
INFO:root:final train perplexity: 4.284435749053955
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.61s/it]
INFO:root:eval mean loss: 22430.389671688987
INFO:root:eval perplexity: 10.190427780151367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [19:36:21<1:38:16, 393.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14702.014494243422
INFO:root:current train perplexity4.266714572906494
INFO:root:current mean train loss 14749.040055475316
INFO:root:current train perplexity4.28542423248291
INFO:root:current mean train loss 14776.406530928938
INFO:root:current train perplexity4.288520812988281

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:19<00:00, 319.36s/it]
INFO:root:final mean train loss: 14754.767377299648
INFO:root:final train perplexity: 4.285750389099121
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.70s/it]
INFO:root:eval mean loss: 22429.46609933036
INFO:root:eval perplexity: 10.189451217651367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [19:42:32<1:30:11, 386.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14778.645053917253
INFO:root:current train perplexity4.283914566040039
INFO:root:current mean train loss 14779.530067845395
INFO:root:current train perplexity4.284547328948975

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.89s/it]
INFO:root:final mean train loss: 14747.64332236013
INFO:root:final train perplexity: 4.282739639282227
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.70s/it]
INFO:root:eval mean loss: 22431.79615420387
INFO:root:eval perplexity: 10.191906929016113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [19:48:51<1:23:15, 384.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14798.462550951086
INFO:root:current train perplexity4.26897668838501
INFO:root:current mean train loss 14738.456316692073
INFO:root:current train perplexity4.275630474090576
INFO:root:current mean train loss 14758.321762016536
INFO:root:current train perplexity4.28246545791626

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:12<00:00, 312.70s/it]
INFO:root:final mean train loss: 14743.691922095513
INFO:root:final train perplexity: 4.281071662902832
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.98s/it]
INFO:root:eval mean loss: 22433.323590959822
INFO:root:eval perplexity: 10.193520545959473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [19:54:55<1:15:38, 378.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14771.056197916667
INFO:root:current train perplexity4.286717414855957
INFO:root:current mean train loss 14739.870965401786
INFO:root:current train perplexity4.278440952301025

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:22<00:00, 322.93s/it]
INFO:root:final mean train loss: 14745.740218623992
INFO:root:final train perplexity: 4.281936168670654
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.61s/it]
INFO:root:eval mean loss: 22432.788178943454
INFO:root:eval perplexity: 10.192956924438477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [20:01:15<1:09:26, 378.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14689.927300347223
INFO:root:current train perplexity4.265027046203613
INFO:root:current mean train loss 14723.585822157973
INFO:root:current train perplexity4.276552200317383
INFO:root:current mean train loss 14741.004568763767
INFO:root:current train perplexity4.276878833770752

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.94s/it]
INFO:root:final mean train loss: 14739.284636466733
INFO:root:final train perplexity: 4.279211044311523
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.41s/it]
INFO:root:eval mean loss: 22434.860165550595
INFO:root:eval perplexity: 10.195141792297363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [20:08:00<1:04:26, 386.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14809.554576246044
INFO:root:current train perplexity4.2862958908081055
INFO:root:current mean train loss 14753.960337377794
INFO:root:current train perplexity4.280387878417969

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:25<00:00, 325.46s/it]
INFO:root:final mean train loss: 14746.131965883316
INFO:root:final train perplexity: 4.282102108001709
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.33s/it]
INFO:root:eval mean loss: 22435.224562872023
INFO:root:eval perplexity: 10.195526123046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [20:14:23<57:48, 385.41s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14846.346490675403
INFO:root:current train perplexity4.282532215118408
INFO:root:current mean train loss 14791.54504114981
INFO:root:current train perplexity4.283811092376709
INFO:root:current mean train loss 14744.151193858224
INFO:root:current train perplexity4.277609825134277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:27<00:00, 327.44s/it]
INFO:root:final mean train loss: 14736.74855484501
INFO:root:final train perplexity: 4.278140544891357
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:02<00:00, 62.61s/it]
INFO:root:eval mean loss: 22434.283714657737
INFO:root:eval perplexity: 10.194534301757812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [20:21:26<52:54, 396.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14740.386377541416
INFO:root:current train perplexity4.280858516693115
INFO:root:current mean train loss 14741.222885715506
INFO:root:current train perplexity4.278598308563232

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.70s/it]
INFO:root:final mean train loss: 14740.541401524697
INFO:root:final train perplexity: 4.279740810394287
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.77s/it]
INFO:root:eval mean loss: 22435.82384672619
INFO:root:eval perplexity: 10.196158409118652
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [20:27:58<46:07, 395.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14749.67759486607
INFO:root:current train perplexity4.301026344299316
INFO:root:current mean train loss 14751.417050057871
INFO:root:current train perplexity4.278750896453857
INFO:root:current mean train loss 14754.032837433511
INFO:root:current train perplexity4.278006076812744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:28<00:00, 328.22s/it]
INFO:root:final mean train loss: 14738.462355090725
INFO:root:final train perplexity: 4.278863430023193
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.70s/it]
INFO:root:eval mean loss: 22431.900576636905
INFO:root:eval perplexity: 10.19201946258545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [20:34:19<39:06, 391.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14708.74533045977
INFO:root:current train perplexity4.272854804992676
INFO:root:current mean train loss 14739.13044159425
INFO:root:current train perplexity4.278253078460693

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.78s/it]
INFO:root:final mean train loss: 14738.325600900958
INFO:root:final train perplexity: 4.278805732727051
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:52<00:00, 52.09s/it]
INFO:root:eval mean loss: 22434.723539806546
INFO:root:eval perplexity: 10.194998741149902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [20:40:48<32:32, 390.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14817.77649238782
INFO:root:current train perplexity4.298120021820068
INFO:root:current mean train loss 14743.690471841277
INFO:root:current train perplexity4.278071880340576
INFO:root:current mean train loss 14749.79127304524
INFO:root:current train perplexity4.278896331787109

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:23<00:00, 323.07s/it]
INFO:root:final mean train loss: 14739.695517263104
INFO:root:final train perplexity: 4.279383659362793
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.69s/it]
INFO:root:eval mean loss: 22435.292410714286
INFO:root:eval perplexity: 10.195596694946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [20:47:21<26:05, 391.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14774.692060868818
INFO:root:current train perplexity4.283060073852539
INFO:root:current mean train loss 14757.083953697644
INFO:root:current train perplexity4.281217575073242

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:16<00:00, 316.01s/it]
INFO:root:final mean train loss: 14729.86279296875
INFO:root:final train perplexity: 4.275235652923584
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.06s/it]
INFO:root:eval mean loss: 22433.59461030506
INFO:root:eval perplexity: 10.193807601928711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [20:53:45<19:27, 389.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14731.116665152616
INFO:root:current train perplexity4.2658162117004395
INFO:root:current mean train loss 14738.571104676574
INFO:root:current train perplexity4.269052982330322
INFO:root:current mean train loss 14742.041473765432
INFO:root:current train perplexity4.2754974365234375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:26<00:00, 326.83s/it]
INFO:root:final mean train loss: 14729.883166897682
INFO:root:final train perplexity: 4.275245189666748
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:51<00:00, 51.23s/it]
INFO:root:eval mean loss: 22433.730143229168
INFO:root:eval perplexity: 10.193950653076172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [21:00:23<13:03, 391.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14764.678351151315
INFO:root:current train perplexity4.2862091064453125
INFO:root:current mean train loss 14747.131715745192
INFO:root:current train perplexity4.277371883392334

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:15<00:00, 315.53s/it]
INFO:root:final mean train loss: 14730.29322470388
INFO:root:final train perplexity: 4.275417327880859
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.47s/it]
INFO:root:eval mean loss: 22433.56226748512
INFO:root:eval perplexity: 10.19377326965332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [21:06:30<06:24, 384.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14729.92112699468
INFO:root:current train perplexity4.270634651184082
INFO:root:current mean train loss 14732.486135469813
INFO:root:current train perplexity4.275791645050049
INFO:root:current mean train loss 14743.532001201924
INFO:root:current train perplexity4.2762227058410645

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:21<00:00, 321.13s/it]
INFO:root:final mean train loss: 14731.562933152722
INFO:root:final train perplexity: 4.2759528160095215
INFO:root:epoch finished
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.89s/it]
INFO:root:eval mean loss: 22434.03592354911
INFO:root:eval perplexity: 10.1942720413208
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_4/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [21:12:58<00:00, 385.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [21:12:58<00:00, 381.89s/it]
INFO:root:evaluating final model
INFO:root:start evaluating
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.55s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:49<00:00, 49.55s/it]
INFO:root:eval mean loss: 22434.03592354911
INFO:root:eval perplexity: 10.1942720413208
INFO:root:evalaution complete
INFO:root:save model final: small_topk_4/final
