INFO:root:Output: small_topk_8
INFO:root:Steps per epochs:248
INFO:root:Total steps:49600
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'cls.predictions.decoder.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 97896.11852904041
INFO:root:current train perplexity15273.0537109375
INFO:root:current mean train loss 81522.30513897612
INFO:root:current train perplexity3074.689208984375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.85s/it]
INFO:root:final mean train loss: 75146.02945438508
INFO:root:final train perplexity: 1655.43798828125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.19s/it]
INFO:root:eval mean loss: 44213.877278645836
INFO:root:eval perplexity: 97.11985778808594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/1

  0%|          | 1/200 [09:00<29:52:29, 540.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 42945.37982536765
INFO:root:current train perplexity69.81348419189453
INFO:root:current mean train loss 39134.703668253314
INFO:root:current train perplexity47.358333587646484


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:06<00:00, 486.73s/it]
INFO:root:final mean train loss: 36537.03336063508
INFO:root:final train perplexity: 36.73496627807617
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.35s/it]
INFO:root:eval mean loss: 31778.59630766369
INFO:root:eval perplexity: 26.81462860107422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/2

  1%|          | 2/200 [18:23<30:26:46, 553.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 31228.23828125
INFO:root:current train perplexity22.09107780456543
INFO:root:current mean train loss 29711.98761756675
INFO:root:current train perplexity18.692480087280273
INFO:root:current mean train loss 28806.624624769087
INFO:root:current train perplexity17.097301483154297


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.98s/it]
INFO:root:final mean train loss: 28423.35327935988
INFO:root:final train perplexity: 16.501623153686523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.91s/it]
INFO:root:eval mean loss: 28544.63904389881
INFO:root:eval perplexity: 19.187273025512695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/3

  2%|â–         | 3/200 [27:15<29:45:45, 543.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 26406.088103693182
INFO:root:current train perplexity13.449667930603027
INFO:root:current mean train loss 25915.608014112902
INFO:root:current train perplexity12.854601860046387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.41s/it]
INFO:root:final mean train loss: 25544.881087764617
INFO:root:final train perplexity: 12.422985076904297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it]
INFO:root:eval mean loss: 27129.717587425595
INFO:root:eval perplexity: 16.573564529418945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/4

  2%|â–         | 4/200 [36:11<29:26:30, 540.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24870.82003348214
INFO:root:current train perplexity11.307598114013672
INFO:root:current mean train loss 24394.88801474883
INFO:root:current train perplexity11.036286354064941
INFO:root:current mean train loss 24109.818085748793
INFO:root:current train perplexity10.775118827819824


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.79s/it]
INFO:root:final mean train loss: 23996.152351625504
INFO:root:final train perplexity: 10.6631498336792
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:25<00:00, 85.62s/it]
INFO:root:eval mean loss: 26319.977120535714
INFO:root:eval perplexity: 15.241233825683594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/5

  2%|â–Ž         | 5/200 [45:11<29:16:13, 540.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 23364.96689618644
INFO:root:current train perplexity10.013143539428711
INFO:root:current mean train loss 23129.394039897797
INFO:root:current train perplexity9.774386405944824


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.28s/it]
INFO:root:final mean train loss: 22980.200305569557
INFO:root:final train perplexity: 9.646432876586914
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.13s/it]
INFO:root:eval mean loss: 25757.645972842263
INFO:root:eval perplexity: 14.379525184631348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/6

  3%|â–Ž         | 6/200 [54:12<29:07:52, 540.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 22340.561434659092
INFO:root:current train perplexity9.1098051071167
INFO:root:current mean train loss 22401.718820382885
INFO:root:current train perplexity9.103434562683105
INFO:root:current mean train loss 22277.29617150474
INFO:root:current train perplexity8.989413261413574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.57s/it]
INFO:root:final mean train loss: 22225.191949659777
INFO:root:final train perplexity: 8.954176902770996
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.64s/it]
INFO:root:eval mean loss: 25310.441941034227
INFO:root:eval perplexity: 13.729154586791992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/7

  4%|â–Ž         | 7/200 [1:03:03<28:49:33, 537.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21864.77960689484
INFO:root:current train perplexity8.627842903137207
INFO:root:current mean train loss 21772.93040644172
INFO:root:current train perplexity8.534612655639648


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.40s/it]
INFO:root:final mean train loss: 21655.63757717994
INFO:root:final train perplexity: 8.465032577514648
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it]
INFO:root:eval mean loss: 25000.823381696428
INFO:root:eval perplexity: 13.296192169189453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/8

  4%|â–         | 8/200 [1:11:49<28:28:43, 533.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 21428.018359375
INFO:root:current train perplexity8.200740814208984
INFO:root:current mean train loss 21342.392476222827
INFO:root:current train perplexity8.175925254821777
INFO:root:current mean train loss 21228.285483284883
INFO:root:current train perplexity8.102826118469238


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.02s/it]
INFO:root:final mean train loss: 21192.22186082409
INFO:root:final train perplexity: 8.086825370788574
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.45s/it]
INFO:root:eval mean loss: 24703.88895089286
INFO:root:eval perplexity: 12.89379596710205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/9

  4%|â–         | 9/200 [1:20:51<28:27:01, 536.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20965.741983442163
INFO:root:current train perplexity7.869086742401123
INFO:root:current mean train loss 20894.905957616018
INFO:root:current train perplexity7.8316779136657715


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.30s/it]
INFO:root:final mean train loss: 20799.822092363913
INFO:root:final train perplexity: 7.779816627502441
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.02s/it]
INFO:root:eval mean loss: 24461.204520089286
INFO:root:eval perplexity: 12.573973655700684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/10

  5%|â–Œ         | 10/200 [1:29:42<28:12:57, 534.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20757.672183388157
INFO:root:current train perplexity7.615487575531006
INFO:root:current mean train loss 20582.07462907038
INFO:root:current train perplexity7.5673508644104
INFO:root:current mean train loss 20488.300888270547
INFO:root:current train perplexity7.53245735168457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.16s/it]
INFO:root:final mean train loss: 20460.525374873992
INFO:root:final train perplexity: 7.523770332336426
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.00s/it]
INFO:root:eval mean loss: 24261.502371651786
INFO:root:eval perplexity: 12.316761016845703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/11

  6%|â–Œ         | 11/200 [1:38:37<28:04:30, 534.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20280.17264524648
INFO:root:current train perplexity7.34344482421875
INFO:root:current mean train loss 20221.028771472953
INFO:root:current train perplexity7.333826065063477


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.54s/it]
INFO:root:final mean train loss: 20172.68314484627
INFO:root:final train perplexity: 7.313168048858643
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.94s/it]
INFO:root:eval mean loss: 24080.71863374256
INFO:root:eval perplexity: 12.088452339172363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/12

  6%|â–Œ         | 12/200 [1:47:29<27:53:28, 534.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 20043.797299592392
INFO:root:current train perplexity7.191555976867676
INFO:root:current mean train loss 19949.801352896342
INFO:root:current train perplexity7.159101486206055
INFO:root:current mean train loss 19923.627469871077
INFO:root:current train perplexity7.132444858551025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.90s/it]
INFO:root:final mean train loss: 19917.16919732863
INFO:root:final train perplexity: 7.131166934967041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.95s/it]
INFO:root:eval mean loss: 23935.401901971727
INFO:root:eval perplexity: 11.90800666809082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/13

  6%|â–‹         | 13/200 [1:56:32<27:52:45, 536.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19792.563854166667
INFO:root:current train perplexity6.999739170074463
INFO:root:current mean train loss 19721.6178125
INFO:root:current train perplexity6.985443592071533


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.42s/it]
INFO:root:final mean train loss: 19685.295071509576
INFO:root:final train perplexity: 6.969925403594971
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.25s/it]
INFO:root:eval mean loss: 23799.40636625744
INFO:root:eval perplexity: 11.741578102111816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/14

  7%|â–‹         | 14/200 [2:05:20<27:35:58, 534.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19501.118778935186
INFO:root:current train perplexity6.8722310066223145
INFO:root:current mean train loss 19461.57421875
INFO:root:current train perplexity6.846736431121826
INFO:root:current mean train loss 19501.780140074337
INFO:root:current train perplexity6.837432384490967


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.55s/it]
INFO:root:final mean train loss: 19482.028099798386
INFO:root:final train perplexity: 6.831578731536865
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.01s/it]
INFO:root:eval mean loss: 23691.995070684523
INFO:root:eval perplexity: 11.611769676208496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/15

  8%|â–Š         | 15/200 [2:14:11<27:23:36, 533.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19311.486624802215
INFO:root:current train perplexity6.725212574005127
INFO:root:current mean train loss 19313.94194090433
INFO:root:current train perplexity6.716831684112549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.76s/it]
INFO:root:final mean train loss: 19301.20054971018
INFO:root:final train perplexity: 6.710814952850342
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.49s/it]
INFO:root:eval mean loss: 23582.758254278273
INFO:root:eval perplexity: 11.481231689453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/16

  8%|â–Š         | 16/200 [2:23:12<27:21:51, 535.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 19095.256174395163
INFO:root:current train perplexity6.6388258934021
INFO:root:current mean train loss 19128.194895038167
INFO:root:current train perplexity6.594119548797607
INFO:root:current mean train loss 19127.010425121753
INFO:root:current train perplexity6.592336654663086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.66s/it]
INFO:root:final mean train loss: 19124.10235792591
INFO:root:final train perplexity: 6.594610691070557
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:33<00:00, 93.50s/it]
INFO:root:eval mean loss: 23470.68982514881
INFO:root:eval perplexity: 11.348836898803711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/17

  8%|â–Š         | 17/200 [2:32:15<27:19:59, 537.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18970.359280873494
INFO:root:current train perplexity6.492908477783203
INFO:root:current mean train loss 18984.710926827185
INFO:root:current train perplexity6.487839221954346


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.02s/it]
INFO:root:final mean train loss: 18968.497779107864
INFO:root:final train perplexity: 6.4941725730896
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.07s/it]
INFO:root:eval mean loss: 23408.98074776786
INFO:root:eval perplexity: 11.276585578918457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/18

  9%|â–‰         | 18/200 [2:40:59<26:59:10, 533.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18890.533761160714
INFO:root:current train perplexity6.434545516967773
INFO:root:current mean train loss 18887.453616898147
INFO:root:current train perplexity6.424874782562256
INFO:root:current mean train loss 18819.975191156915
INFO:root:current train perplexity6.395637512207031


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.58s/it]
INFO:root:final mean train loss: 18820.557298229585
INFO:root:final train perplexity: 6.400099277496338
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.60s/it]
INFO:root:eval mean loss: 23323.49637276786
INFO:root:eval perplexity: 11.17725944519043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/19

 10%|â–‰         | 19/200 [2:49:47<26:44:11, 531.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18680.9415858477
INFO:root:current train perplexity6.297740459442139
INFO:root:current mean train loss 18716.32168073195
INFO:root:current train perplexity6.315098285675049


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.31s/it]
INFO:root:final mean train loss: 18684.66185735887
INFO:root:final train perplexity: 6.314887523651123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it]
INFO:root:eval mean loss: 23234.632161458332
INFO:root:eval perplexity: 11.074933052062988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/20

 10%|â–ˆ         | 20/200 [2:58:30<26:28:02, 529.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18701.76447315705
INFO:root:current train perplexity6.2589263916015625
INFO:root:current mean train loss 18592.787474707733
INFO:root:current train perplexity6.243612289428711
INFO:root:current mean train loss 18579.879960447175
INFO:root:current train perplexity6.239163875579834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.72s/it]
INFO:root:final mean train loss: 18557.396417433214
INFO:root:final train perplexity: 6.2361159324646
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:28<00:00, 88.11s/it]
INFO:root:eval mean loss: 23176.379092261905
INFO:root:eval perplexity: 11.00836181640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/21

 10%|â–ˆ         | 21/200 [3:07:31<26:29:20, 532.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18428.441535027472
INFO:root:current train perplexity6.160033702850342
INFO:root:current mean train loss 18450.020052765052
INFO:root:current train perplexity6.156613826751709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.02s/it]
INFO:root:final mean train loss: 18435.818249117943
INFO:root:final train perplexity: 6.161781311035156
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.77s/it]
INFO:root:eval mean loss: 23110.652483258928
INFO:root:eval perplexity: 10.933733940124512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/22

 11%|â–ˆ         | 22/200 [3:16:33<26:29:00, 535.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18361.880223473836
INFO:root:current train perplexity6.109090328216553
INFO:root:current mean train loss 18336.152289117133
INFO:root:current train perplexity6.095039367675781
INFO:root:current mean train loss 18345.732839827673
INFO:root:current train perplexity6.095423698425293


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.59s/it]
INFO:root:final mean train loss: 18323.060168850807
INFO:root:final train perplexity: 6.093631267547607
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it]
INFO:root:eval mean loss: 23076.28636532738
INFO:root:eval perplexity: 10.894917488098145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/23

 12%|â–ˆâ–        | 23/200 [3:25:17<26:09:42, 532.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18276.464514802632
INFO:root:current train perplexity6.035606384277344
INFO:root:current mean train loss 18243.76196915064
INFO:root:current train perplexity6.031170845031738


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.04s/it]
INFO:root:final mean train loss: 18222.536676222277
INFO:root:final train perplexity: 6.033513069152832
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.73s/it]
INFO:root:eval mean loss: 22986.622419084822
INFO:root:eval perplexity: 10.794280052185059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/24

 12%|â–ˆâ–        | 24/200 [3:34:02<25:54:53, 530.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18057.32388630319
INFO:root:current train perplexity5.9546380043029785
INFO:root:current mean train loss 18119.29529389881
INFO:root:current train perplexity5.9718780517578125
INFO:root:current mean train loss 18130.894578694333
INFO:root:current train perplexity5.972009181976318


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.83s/it]
INFO:root:final mean train loss: 18117.610524823587
INFO:root:final train perplexity: 5.971394062042236
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.31s/it]
INFO:root:eval mean loss: 22997.422200520832
INFO:root:eval perplexity: 10.806351661682129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/25

 12%|â–ˆâ–Ž        | 25/200 [3:42:49<25:43:22, 529.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18077.316386521463
INFO:root:current train perplexity5.935939311981201
INFO:root:current mean train loss 18058.005417713568
INFO:root:current train perplexity5.918238639831543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.73s/it]
INFO:root:final mean train loss: 18028.5626890121
INFO:root:final train perplexity: 5.919177055358887
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.35s/it]
INFO:root:eval mean loss: 22925.337216331845
INFO:root:eval perplexity: 10.726030349731445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/26

 13%|â–ˆâ–Ž        | 26/200 [3:51:49<25:43:39, 532.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17967.719515931374
INFO:root:current train perplexity5.893628120422363
INFO:root:current mean train loss 17974.048905732616
INFO:root:current train perplexity5.87986946105957


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.80s/it]
INFO:root:final mean train loss: 17940.545831495714
INFO:root:final train perplexity: 5.868013381958008
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.99s/it]
INFO:root:eval mean loss: 22880.017624627977
INFO:root:eval perplexity: 10.675835609436035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/27

 14%|â–ˆâ–Ž        | 27/200 [4:00:40<25:33:16, 531.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 18090.719401041668
INFO:root:current train perplexity5.855724334716797
INFO:root:current mean train loss 17881.854558555824
INFO:root:current train perplexity5.830217361450195
INFO:root:current mean train loss 17880.05222444581
INFO:root:current train perplexity5.818762302398682


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.68s/it]
INFO:root:final mean train loss: 17854.03794811618
INFO:root:final train perplexity: 5.818158149719238
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.80s/it]
INFO:root:eval mean loss: 22841.88623046875
INFO:root:eval perplexity: 10.633790969848633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/28

 14%|â–ˆâ–        | 28/200 [4:09:38<25:29:45, 533.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17845.116903409093
INFO:root:current train perplexity5.796960830688477
INFO:root:current mean train loss 17812.94066280242
INFO:root:current train perplexity5.780525207519531


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.62s/it]
INFO:root:final mean train loss: 17772.1912172379
INFO:root:final train perplexity: 5.771378517150879
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.76s/it]
INFO:root:eval mean loss: 22809.83191499256
INFO:root:eval perplexity: 10.598573684692383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/29

 14%|â–ˆâ–        | 29/200 [4:18:31<25:20:24, 533.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17789.798270089286
INFO:root:current train perplexity5.723114013671875
INFO:root:current mean train loss 17801.385842581774
INFO:root:current train perplexity5.735330104827881
INFO:root:current mean train loss 17740.652145606884
INFO:root:current train perplexity5.729200839996338


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.36s/it]
INFO:root:final mean train loss: 17694.35120810232
INFO:root:final train perplexity: 5.727237701416016
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it]
INFO:root:eval mean loss: 22764.995698474704
INFO:root:eval perplexity: 10.549507141113281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/30

 15%|â–ˆâ–Œ        | 30/200 [4:27:20<25:07:44, 532.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17640.898404396186
INFO:root:current train perplexity5.688811779022217
INFO:root:current mean train loss 17640.611684355346
INFO:root:current train perplexity5.688422679901123


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.01s/it]
INFO:root:final mean train loss: 17620.14792559224
INFO:root:final train perplexity: 5.685473442077637
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.35s/it]
INFO:root:eval mean loss: 22728.406110491072
INFO:root:eval perplexity: 10.509628295898438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/31

 16%|â–ˆâ–Œ        | 31/200 [4:36:05<24:52:49, 530.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17491.990589488636
INFO:root:current train perplexity5.622969150543213
INFO:root:current mean train loss 17559.215635557433
INFO:root:current train perplexity5.631072044372559
INFO:root:current mean train loss 17563.6056909064
INFO:root:current train perplexity5.6494364738464355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.52s/it]
INFO:root:final mean train loss: 17550.157746345765
INFO:root:final train perplexity: 5.646359443664551
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it]
INFO:root:eval mean loss: 22712.35909598214
INFO:root:eval perplexity: 10.492193222045898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/32

 16%|â–ˆâ–Œ        | 32/200 [4:44:48<24:38:00, 527.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17481.965959821428
INFO:root:current train perplexity5.590503215789795
INFO:root:current mean train loss 17503.43958493098
INFO:root:current train perplexity5.611205101013184


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.62s/it]
INFO:root:final mean train loss: 17480.565256426413
INFO:root:final train perplexity: 5.607736110687256
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.08s/it]
INFO:root:eval mean loss: 22691.009928385418
INFO:root:eval perplexity: 10.469034194946289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/33

 16%|â–ˆâ–‹        | 33/200 [4:53:34<24:27:54, 527.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17258.431119791665
INFO:root:current train perplexity5.522895336151123
INFO:root:current mean train loss 17410.78238790761
INFO:root:current train perplexity5.569390773773193
INFO:root:current mean train loss 17400.09520348837
INFO:root:current train perplexity5.561131954193115


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:21<00:00, 441.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:21<00:00, 441.88s/it]
INFO:root:final mean train loss: 17414.72283738659
INFO:root:final train perplexity: 5.571436405181885
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.26s/it]
INFO:root:eval mean loss: 22633.314499627977
INFO:root:eval perplexity: 10.406705856323242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/34

 17%|â–ˆâ–‹        | 34/200 [5:02:13<24:11:49, 524.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17344.189132462685
INFO:root:current train perplexity5.53108549118042
INFO:root:current mean train loss 17368.810909431137
INFO:root:current train perplexity5.5351409912109375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.00s/it]
INFO:root:final mean train loss: 17354.14225129158
INFO:root:final train perplexity: 5.53824520111084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.20s/it]
INFO:root:eval mean loss: 22618.443777901786
INFO:root:eval perplexity: 10.390702247619629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/35

 18%|â–ˆâ–Š        | 35/200 [5:10:59<24:04:50, 525.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17483.088199013157
INFO:root:current train perplexity5.559342384338379
INFO:root:current mean train loss 17345.17853860294
INFO:root:current train perplexity5.521539688110352
INFO:root:current mean train loss 17314.28685074201
INFO:root:current train perplexity5.506275653839111


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.90s/it]
INFO:root:final mean train loss: 17297.298135080644
INFO:root:final train perplexity: 5.50728178024292
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.09s/it]
INFO:root:eval mean loss: 22622.982840401786
INFO:root:eval perplexity: 10.395586013793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/36

 18%|â–ˆâ–Š        | 36/200 [5:19:51<24:00:59, 527.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17215.489794234156
INFO:root:current train perplexity5.462143421173096
INFO:root:current mean train loss 17235.971788194445
INFO:root:current train perplexity5.477213382720947


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.36s/it]
INFO:root:final mean train loss: 17241.14122747606
INFO:root:final train perplexity: 5.476862907409668
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.52s/it]
INFO:root:eval mean loss: 22570.48572358631
INFO:root:eval perplexity: 10.33925724029541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/37

 18%|â–ˆâ–Š        | 37/200 [5:28:37<23:51:43, 527.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17110.889478600544
INFO:root:current train perplexity5.408300399780273
INFO:root:current mean train loss 17131.815437627032
INFO:root:current train perplexity5.434340000152588
INFO:root:current mean train loss 17204.958069121356
INFO:root:current train perplexity5.450361251831055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.29s/it]
INFO:root:final mean train loss: 17187.581885553176
INFO:root:final train perplexity: 5.448004722595215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.23s/it]
INFO:root:eval mean loss: 22569.639950706845
INFO:root:eval perplexity: 10.33835220336914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/38

 19%|â–ˆâ–‰        | 38/200 [5:37:37<23:53:20, 530.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17146.196692708334
INFO:root:current train perplexity5.407866954803467
INFO:root:current mean train loss 17134.489174107144
INFO:root:current train perplexity5.411921501159668


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.82s/it]
INFO:root:final mean train loss: 17131.68220372354
INFO:root:final train perplexity: 5.418050289154053
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.15s/it]
INFO:root:eval mean loss: 22546.20470610119
INFO:root:eval perplexity: 10.313307762145996
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/39

 20%|â–ˆâ–‰        | 39/200 [5:46:25<23:41:53, 529.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16989.160011574073
INFO:root:current train perplexity5.347631454467773
INFO:root:current mean train loss 17083.91353192667
INFO:root:current train perplexity5.389808177947998
INFO:root:current mean train loss 17085.211384911894
INFO:root:current train perplexity5.386967658996582


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.38s/it]
INFO:root:final mean train loss: 17077.38146185106
INFO:root:final train perplexity: 5.389110088348389
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.93s/it]
INFO:root:eval mean loss: 22553.986839657737
INFO:root:eval perplexity: 10.321617126464844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/40

 20%|â–ˆâ–ˆ        | 40/200 [5:55:17<23:34:31, 530.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17064.2822018394
INFO:root:current train perplexity5.368439197540283
INFO:root:current mean train loss 17082.089931040504
INFO:root:current train perplexity5.376386642456055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.08s/it]
INFO:root:final mean train loss: 17030.05788889239
INFO:root:final train perplexity: 5.364015579223633
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.59s/it]
INFO:root:eval mean loss: 22523.64034598214
INFO:root:eval perplexity: 10.289251327514648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/41

 20%|â–ˆâ–ˆ        | 41/200 [6:04:11<23:28:46, 531.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16898.493510584678
INFO:root:current train perplexity5.303433895111084
INFO:root:current mean train loss 16957.012367306776
INFO:root:current train perplexity5.322083950042725
INFO:root:current mean train loss 16998.24701958198
INFO:root:current train perplexity5.33636999130249


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.41s/it]
INFO:root:final mean train loss: 16978.398638325354
INFO:root:final train perplexity: 5.336754322052002
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.17s/it]
INFO:root:eval mean loss: 22488.957868303572
INFO:root:eval perplexity: 10.25238037109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/42

 21%|â–ˆâ–ˆ        | 42/200 [6:13:03<23:20:13, 531.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16925.58463149473
INFO:root:current train perplexity5.311269283294678
INFO:root:current mean train loss 16958.47077249829
INFO:root:current train perplexity5.314715385437012


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.56s/it]
INFO:root:final mean train loss: 16931.686298985634
INFO:root:final train perplexity: 5.312220573425293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it]
INFO:root:eval mean loss: 22501.250790550595
INFO:root:eval perplexity: 10.265436172485352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/43

 22%|â–ˆâ–ˆâ–       | 43/200 [6:21:53<23:09:40, 531.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 17019.907142857144
INFO:root:current train perplexity5.326751708984375
INFO:root:current mean train loss 16937.21792534722
INFO:root:current train perplexity5.29306697845459
INFO:root:current mean train loss 16905.513144115692
INFO:root:current train perplexity5.291225433349609


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.26s/it]
INFO:root:final mean train loss: 16889.29048402848
INFO:root:final train perplexity: 5.290053844451904
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.72s/it]
INFO:root:eval mean loss: 22489.823776971727
INFO:root:eval perplexity: 10.253301620483398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/44

 22%|â–ˆâ–ˆâ–       | 44/200 [6:30:42<22:59:32, 530.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16836.035077676006
INFO:root:current train perplexity5.256141662597656
INFO:root:current mean train loss 16855.988218582886
INFO:root:current train perplexity5.263339042663574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.95s/it]
INFO:root:final mean train loss: 16847.195115612398
INFO:root:final train perplexity: 5.2681355476379395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.07s/it]
INFO:root:eval mean loss: 22471.353701636905
INFO:root:eval perplexity: 10.233719825744629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [6:39:34<22:51:31, 530.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16834.293118990383
INFO:root:current train perplexity5.253170490264893
INFO:root:current mean train loss 16804.573755058453
INFO:root:current train perplexity5.244534015655518
INFO:root:current mean train loss 16814.084482871338
INFO:root:current train perplexity5.243613243103027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.76s/it]
INFO:root:final mean train loss: 16798.135206653227
INFO:root:final train perplexity: 5.242705821990967
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.57s/it]
INFO:root:eval mean loss: 22446.90841238839
INFO:root:eval perplexity: 10.207862854003906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [6:48:40<22:54:50, 535.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16801.89846969437
INFO:root:current train perplexity5.221769332885742
INFO:root:current mean train loss 16790.22087185046
INFO:root:current train perplexity5.226555347442627


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.27s/it]
INFO:root:final mean train loss: 16756.292720671623
INFO:root:final train perplexity: 5.2211127281188965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.35s/it]
INFO:root:eval mean loss: 22441.552594866072
INFO:root:eval perplexity: 10.202208518981934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [6:57:33<22:43:18, 534.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16728.13231286337
INFO:root:current train perplexity5.202082633972168
INFO:root:current mean train loss 16734.904884178322
INFO:root:current train perplexity5.202495574951172
INFO:root:current mean train loss 16733.0574845679
INFO:root:current train perplexity5.204077243804932


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.18s/it]
INFO:root:final mean train loss: 16722.409333259828
INFO:root:final train perplexity: 5.203693389892578
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.49s/it]
INFO:root:eval mean loss: 22439.221098400296
INFO:root:eval perplexity: 10.19974422454834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/48

 24%|â–ˆâ–ˆâ–       | 48/200 [7:06:28<22:34:46, 534.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16660.348396381578
INFO:root:current train perplexity5.163797855377197
INFO:root:current mean train loss 16674.852829527244
INFO:root:current train perplexity5.1708831787109375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.36s/it]
INFO:root:final mean train loss: 16678.1016609438
INFO:root:final train perplexity: 5.181001663208008
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.84s/it]
INFO:root:eval mean loss: 22411.87667410714
INFO:root:eval perplexity: 10.17092227935791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/49

 24%|â–ˆâ–ˆâ–       | 49/200 [7:15:21<22:25:06, 534.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16676.463597074468
INFO:root:current train perplexity5.1457953453063965
INFO:root:current mean train loss 16654.86207217262
INFO:root:current train perplexity5.161989688873291
INFO:root:current mean train loss 16652.957857572117
INFO:root:current train perplexity5.162343502044678


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.44s/it]
INFO:root:final mean train loss: 16642.21910045993
INFO:root:final train perplexity: 5.162698268890381
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.74s/it]
INFO:root:eval mean loss: 22420.37039620536
INFO:root:eval perplexity: 10.179863929748535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [7:24:11<22:12:25, 532.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16621.7815557923
INFO:root:current train perplexity5.131312370300293
INFO:root:current mean train loss 16593.954391096104
INFO:root:current train perplexity5.141988277435303


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.15s/it]
INFO:root:final mean train loss: 16604.98587922127
INFO:root:final train perplexity: 5.143772602081299
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:29<00:00, 89.53s/it]
INFO:root:eval mean loss: 22405.77955264137
INFO:root:eval perplexity: 10.16450309753418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [7:33:32<22:24:40, 541.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16588.412205116423
INFO:root:current train perplexity5.124758243560791
INFO:root:current mean train loss 16586.69687111962
INFO:root:current train perplexity5.122673034667969


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.38s/it]
INFO:root:final mean train loss: 16567.182565996725
INFO:root:final train perplexity: 5.124629974365234
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.55s/it]
INFO:root:eval mean loss: 22405.283156622023
INFO:root:eval perplexity: 10.163981437683105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [7:42:46<22:24:22, 545.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16670.6435546875
INFO:root:current train perplexity5.241891860961914
INFO:root:current mean train loss 16509.549880537015
INFO:root:current train perplexity5.094583988189697
INFO:root:current mean train loss 16545.524433305112
INFO:root:current train perplexity5.108006954193115


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.63s/it]
INFO:root:final mean train loss: 16535.31463032384
INFO:root:final train perplexity: 5.108547210693359
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.03s/it]
INFO:root:eval mean loss: 22377.121000744046
INFO:root:eval perplexity: 10.134400367736816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [7:51:40<22:07:22, 541.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16500.935298295455
INFO:root:current train perplexity5.102291107177734
INFO:root:current mean train loss 16544.680216733872
INFO:root:current train perplexity5.10518741607666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.73s/it]
INFO:root:final mean train loss: 16499.163015057962
INFO:root:final train perplexity: 5.0903639793396
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it]
INFO:root:eval mean loss: 22377.34207589286
INFO:root:eval perplexity: 10.134632110595703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [8:00:40<21:57:08, 541.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16351.40415736607
INFO:root:current train perplexity5.080859184265137
INFO:root:current mean train loss 16463.239120911214
INFO:root:current train perplexity5.0753374099731445
INFO:root:current mean train loss 16467.097108997583
INFO:root:current train perplexity5.0771565437316895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.11s/it]
INFO:root:final mean train loss: 16465.647496377267
INFO:root:final train perplexity: 5.073564052581787
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.37s/it]
INFO:root:eval mean loss: 22364.632393973214
INFO:root:eval perplexity: 10.121310234069824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [8:09:39<21:46:29, 540.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16463.84384931144
INFO:root:current train perplexity5.051768779754639
INFO:root:current mean train loss 16408.564840064857
INFO:root:current train perplexity5.0474066734313965


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.05s/it]
INFO:root:final mean train loss: 16431.332921181955
INFO:root:final train perplexity: 5.056421279907227
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 22368.055036272322
INFO:root:eval perplexity: 10.124897003173828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [8:18:38<21:36:06, 540.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16542.742897727272
INFO:root:current train perplexity5.082229137420654
INFO:root:current mean train loss 16387.123372395832
INFO:root:current train perplexity5.032923221588135
INFO:root:current mean train loss 16424.2111596564
INFO:root:current train perplexity5.042640209197998


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.38s/it]
INFO:root:final mean train loss: 16394.999366021926
INFO:root:final train perplexity: 5.038333415985107
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.67s/it]
INFO:root:eval mean loss: 22353.056245349704
INFO:root:eval perplexity: 10.10919189453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [8:27:26<21:18:49, 536.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16346.7470703125
INFO:root:current train perplexity5.004415035247803
INFO:root:current mean train loss 16368.948230205138
INFO:root:current train perplexity5.021424770355225


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.86s/it]
INFO:root:final mean train loss: 16367.966454290574
INFO:root:final train perplexity: 5.0249176025390625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.92s/it]
INFO:root:eval mean loss: 22350.060872395832
INFO:root:eval perplexity: 10.106056213378906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [8:36:20<21:07:59, 535.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16332.217057291668
INFO:root:current train perplexity5.013609886169434
INFO:root:current mean train loss 16283.644539741848
INFO:root:current train perplexity4.984278202056885
INFO:root:current mean train loss 16319.651071947674
INFO:root:current train perplexity5.002723693847656


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.62s/it]
INFO:root:final mean train loss: 16336.39320422757
INFO:root:final train perplexity: 5.009293556213379
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.56s/it]
INFO:root:eval mean loss: 22348.1611328125
INFO:root:eval perplexity: 10.104068756103516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [8:45:02<20:49:02, 531.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16311.249621035447
INFO:root:current train perplexity4.975886344909668
INFO:root:current mean train loss 16310.189815681138
INFO:root:current train perplexity4.995108604431152


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.96s/it]
INFO:root:final mean train loss: 16305.436310798892
INFO:root:final train perplexity: 4.994022369384766
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.99s/it]
INFO:root:eval mean loss: 22346.065336681546
INFO:root:eval perplexity: 10.101879119873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [8:53:47<20:35:39, 529.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16277.946546052632
INFO:root:current train perplexity4.988809585571289
INFO:root:current mean train loss 16257.828051142333
INFO:root:current train perplexity4.976661205291748
INFO:root:current mean train loss 16273.678813498858
INFO:root:current train perplexity4.97564697265625


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.45s/it]
INFO:root:final mean train loss: 16275.397240423386
INFO:root:final train perplexity: 4.979247093200684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.24s/it]
INFO:root:eval mean loss: 22327.080171130954
INFO:root:eval perplexity: 10.082048416137695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [9:02:26<20:19:28, 526.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16275.078358824823
INFO:root:current train perplexity4.968941688537598
INFO:root:current mean train loss 16278.073579130118
INFO:root:current train perplexity4.969355583190918


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.79s/it]
INFO:root:final mean train loss: 16244.28371109501
INFO:root:final train perplexity: 4.963990211486816
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.33s/it]
INFO:root:eval mean loss: 22336.10816592262
INFO:root:eval perplexity: 10.091470718383789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [9:11:06<20:06:27, 524.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16152.092518682066
INFO:root:current train perplexity4.922235488891602
INFO:root:current mean train loss 16234.965955284553
INFO:root:current train perplexity4.945793151855469
INFO:root:current mean train loss 16234.723878047926
INFO:root:current train perplexity4.953829765319824


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.90s/it]
INFO:root:final mean train loss: 16218.093336536038
INFO:root:final train perplexity: 4.951183795928955
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.70s/it]
INFO:root:eval mean loss: 22324.892740885418
INFO:root:eval perplexity: 10.0797700881958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [9:20:03<20:06:07, 528.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16182.475768229167
INFO:root:current train perplexity4.940641403198242
INFO:root:current mean train loss 16214.492756696429
INFO:root:current train perplexity4.935151100158691


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.68s/it]
INFO:root:final mean train loss: 16192.99699155746
INFO:root:final train perplexity: 4.938943386077881
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.33s/it]
INFO:root:eval mean loss: 22321.151320684523
INFO:root:eval perplexity: 10.075865745544434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [9:28:48<19:55:10, 527.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16267.928421585648
INFO:root:current train perplexity4.9341559410095215
INFO:root:current mean train loss 16146.942667322835
INFO:root:current train perplexity4.913403034210205
INFO:root:current mean train loss 16173.801435159692
INFO:root:current train perplexity4.924577236175537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.17s/it]
INFO:root:final mean train loss: 16166.714485414566
INFO:root:final train perplexity: 4.926156997680664
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.66s/it]
INFO:root:eval mean loss: 22323.705078125
INFO:root:eval perplexity: 10.078529357910156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [9:37:45<19:52:59, 530.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16154.30918710443
INFO:root:current train perplexity4.905843257904053
INFO:root:current mean train loss 16147.437930996857
INFO:root:current train perplexity4.909096717834473


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.01s/it]
INFO:root:final mean train loss: 16139.256493353074
INFO:root:final train perplexity: 4.9128336906433105
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.56s/it]
INFO:root:eval mean loss: 22315.164527529763
INFO:root:eval perplexity: 10.069623947143555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [9:46:42<19:49:07, 532.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16061.784809727822
INFO:root:current train perplexity4.866825103759766
INFO:root:current mean train loss 16122.489608182252
INFO:root:current train perplexity4.891847610473633
INFO:root:current mean train loss 16123.028058204816
INFO:root:current train perplexity4.899413108825684


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.61s/it]
INFO:root:final mean train loss: 16112.822903540826
INFO:root:final train perplexity: 4.9000420570373535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:23<00:00, 83.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:23<00:00, 83.89s/it]
INFO:root:eval mean loss: 22310.8037109375
INFO:root:eval perplexity: 10.065079689025879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [9:55:42<19:45:01, 534.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16085.826936652862
INFO:root:current train perplexity4.8773393630981445
INFO:root:current mean train loss 16099.439981429303
INFO:root:current train perplexity4.882054805755615


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.88s/it]
INFO:root:final mean train loss: 16085.418067193801
INFO:root:final train perplexity: 4.886814117431641
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.04s/it]
INFO:root:eval mean loss: 22316.319684709822
INFO:root:eval perplexity: 10.070829391479492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [10:04:27<19:29:47, 531.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16139.365792410714
INFO:root:current train perplexity4.892215251922607
INFO:root:current mean train loss 16081.96060474537
INFO:root:current train perplexity4.87365198135376
INFO:root:current mean train loss 16079.991551695479
INFO:root:current train perplexity4.875442981719971


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.79s/it]
INFO:root:final mean train loss: 16063.197064799648
INFO:root:final train perplexity: 4.876115798950195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.62s/it]
INFO:root:eval mean loss: 22310.840169270832
INFO:root:eval perplexity: 10.065118789672852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [10:13:21<19:22:03, 532.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 16034.600619612069
INFO:root:current train perplexity4.866637229919434
INFO:root:current mean train loss 16018.22209224599
INFO:root:current train perplexity4.860683441162109


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.37s/it]
INFO:root:final mean train loss: 16037.469131961945
INFO:root:final train perplexity: 4.863757610321045
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.61s/it]
INFO:root:eval mean loss: 22315.898460751487
INFO:root:eval perplexity: 10.070389747619629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [10:22:20<19:17:33, 534.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15921.95825821314
INFO:root:current train perplexity4.846138954162598
INFO:root:current mean train loss 15977.643617918166
INFO:root:current train perplexity4.837596893310547
INFO:root:current mean train loss 16018.99286986794
INFO:root:current train perplexity4.84857702255249


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.94s/it]
INFO:root:final mean train loss: 16012.811342300907
INFO:root:final train perplexity: 4.8519439697265625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.13s/it]
INFO:root:eval mean loss: 22290.34691220238
INFO:root:eval perplexity: 10.043790817260742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [10:31:12<19:07:21, 533.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15965.568595467033
INFO:root:current train perplexity4.8252973556518555
INFO:root:current mean train loss 15982.745817653797
INFO:root:current train perplexity4.833978652954102


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.49s/it]
INFO:root:final mean train loss: 15989.086158014114
INFO:root:final train perplexity: 4.840603351593018
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.95s/it]
INFO:root:eval mean loss: 22296.882463727678
INFO:root:eval perplexity: 10.050591468811035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [10:40:02<18:56:26, 532.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15976.180527797966
INFO:root:current train perplexity4.809533596038818
INFO:root:current mean train loss 15975.014211374562
INFO:root:current train perplexity4.827150344848633
INFO:root:current mean train loss 15982.401363168725
INFO:root:current train perplexity4.830824851989746


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.31s/it]
INFO:root:final mean train loss: 15968.337150327621
INFO:root:final train perplexity: 4.8307061195373535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it]
INFO:root:eval mean loss: 22299.054524739582
INFO:root:eval perplexity: 10.052848815917969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [10:48:48<18:43:06, 530.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15935.376942845394
INFO:root:current train perplexity4.811415672302246
INFO:root:current mean train loss 15945.806224959935
INFO:root:current train perplexity4.817214012145996


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.02s/it]
INFO:root:final mean train loss: 15941.583157447076
INFO:root:final train perplexity: 4.817975997924805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.00s/it]
INFO:root:eval mean loss: 22300.57247488839
INFO:root:eval perplexity: 10.054429054260254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [10:57:33<18:30:44, 528.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15910.572951296543
INFO:root:current train perplexity4.792197227478027
INFO:root:current mean train loss 15913.35041321216
INFO:root:current train perplexity4.793098449707031
INFO:root:current mean train loss 15932.826156060222
INFO:root:current train perplexity4.807419776916504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.82s/it]
INFO:root:final mean train loss: 15920.037711851059
INFO:root:final train perplexity: 4.807748317718506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.02s/it]
INFO:root:eval mean loss: 22287.215378534227
INFO:root:eval perplexity: 10.040539741516113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [11:06:19<18:20:01, 528.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15900.091865924875
INFO:root:current train perplexity4.791971206665039
INFO:root:current mean train loss 15906.768873665202
INFO:root:current train perplexity4.795536041259766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.61s/it]
INFO:root:final mean train loss: 15897.116655903477
INFO:root:final train perplexity: 4.796891689300537
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.23s/it]
INFO:root:eval mean loss: 22297.640206473214
INFO:root:eval perplexity: 10.051377296447754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [11:15:05<18:09:50, 527.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15866.537358302696
INFO:root:current train perplexity4.785332679748535
INFO:root:current mean train loss 15889.311103062913
INFO:root:current train perplexity4.784032821655273


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.70s/it]
INFO:root:final mean train loss: 15871.265321793095
INFO:root:final train perplexity: 4.784675598144531
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.77s/it]
INFO:root:eval mean loss: 22302.99597749256
INFO:root:eval perplexity: 10.056950569152832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [11:23:58<18:04:51, 529.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15407.129557291666
INFO:root:current train perplexity4.6306257247924805
INFO:root:current mean train loss 15861.850917779126
INFO:root:current train perplexity4.769481182098389
INFO:root:current mean train loss 15854.01937249846
INFO:root:current train perplexity4.771714210510254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.61s/it]
INFO:root:final mean train loss: 15854.296945879536
INFO:root:final train perplexity: 4.776676177978516
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.54s/it]
INFO:root:eval mean loss: 22292.235165550595
INFO:root:eval perplexity: 10.045754432678223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [11:32:57<18:02:06, 532.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15816.59112215909
INFO:root:current train perplexity4.774766445159912
INFO:root:current mean train loss 15830.133833165322
INFO:root:current train perplexity4.764989376068115


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.51s/it]
INFO:root:final mean train loss: 15831.78533344884
INFO:root:final train perplexity: 4.766080856323242
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.41s/it]
INFO:root:eval mean loss: 22293.200241815477
INFO:root:eval perplexity: 10.046758651733398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [11:42:03<18:01:32, 536.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15779.250697544643
INFO:root:current train perplexity4.737471580505371
INFO:root:current mean train loss 15845.057069582359
INFO:root:current train perplexity4.759869575500488
INFO:root:current mean train loss 15845.538307669081
INFO:root:current train perplexity4.7647576332092285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.03s/it]
INFO:root:final mean train loss: 15816.064972908267
INFO:root:final train perplexity: 4.758696556091309
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.89s/it]
INFO:root:eval mean loss: 22285.959821428572
INFO:root:eval perplexity: 10.03923225402832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [11:50:57<17:51:09, 535.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15746.023718882416
INFO:root:current train perplexity4.726827621459961
INFO:root:current mean train loss 15783.731199636399
INFO:root:current train perplexity4.739787578582764


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.24s/it]
INFO:root:final mean train loss: 15789.117782100555
INFO:root:final train perplexity: 4.746065616607666
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.96s/it]
INFO:root:eval mean loss: 22302.072800409227
INFO:root:eval perplexity: 10.055992126464844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [12:00:03<17:48:36, 538.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15892.647904829546
INFO:root:current train perplexity4.825008869171143
INFO:root:current mean train loss 15806.065807995496
INFO:root:current train perplexity4.7384867668151855
INFO:root:current mean train loss 15804.216107264514
INFO:root:current train perplexity4.739292621612549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.02s/it]
INFO:root:final mean train loss: 15772.59328140751
INFO:root:final train perplexity: 4.738336563110352
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.90s/it]
INFO:root:eval mean loss: 22288.275041852678
INFO:root:eval perplexity: 10.041641235351562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [12:09:02<17:39:40, 538.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15757.861653645834
INFO:root:current train perplexity4.730999946594238
INFO:root:current mean train loss 15748.306610669095
INFO:root:current train perplexity4.7318830490112305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.66s/it]
INFO:root:final mean train loss: 15748.02331936744
INFO:root:final train perplexity: 4.72686767578125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.44s/it]
INFO:root:eval mean loss: 22291.905412946428
INFO:root:eval perplexity: 10.045412063598633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [12:17:57<17:28:33, 537.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15737.863411458333
INFO:root:current train perplexity4.689669132232666
INFO:root:current mean train loss 15714.890149456522
INFO:root:current train perplexity4.713791370391846
INFO:root:current mean train loss 15744.280214389535
INFO:root:current train perplexity4.720207214355469


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.25s/it]
INFO:root:final mean train loss: 15731.353834582913
INFO:root:final train perplexity: 4.719101905822754
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22305.327776227678
INFO:root:eval perplexity: 10.059377670288086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [12:26:56<17:20:17, 538.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15683.646178288247
INFO:root:current train perplexity4.7036027908325195
INFO:root:current mean train loss 15719.33347562687
INFO:root:current train perplexity4.708718776702881


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.49s/it]
INFO:root:final mean train loss: 15712.507458102318
INFO:root:final train perplexity: 4.710338592529297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.16s/it]
INFO:root:eval mean loss: 22287.73411923363
INFO:root:eval perplexity: 10.041075706481934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [12:35:56<17:12:16, 538.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15492.218904194078
INFO:root:current train perplexity4.696207046508789
INFO:root:current mean train loss 15716.16681985294
INFO:root:current train perplexity4.701406002044678
INFO:root:current mean train loss 15711.874518407534
INFO:root:current train perplexity4.702986240386963


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.08s/it]
INFO:root:final mean train loss: 15697.665810861896
INFO:root:final train perplexity: 4.703447341918945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.77s/it]
INFO:root:eval mean loss: 22295.276506696428
INFO:root:eval perplexity: 10.048918724060059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [12:44:59<17:05:40, 539.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15625.070326254401
INFO:root:current train perplexity4.694268703460693
INFO:root:current mean train loss 15673.55235174525
INFO:root:current train perplexity4.691664695739746


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.84s/it]
INFO:root:final mean train loss: 15680.96087843372
INFO:root:final train perplexity: 4.695704936981201
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.34s/it]
INFO:root:eval mean loss: 22299.664155505954
INFO:root:eval perplexity: 10.053482055664062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [12:53:47<16:50:05, 536.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15687.824813179348
INFO:root:current train perplexity4.674970626831055
INFO:root:current mean train loss 15666.668302210366
INFO:root:current train perplexity4.68359375
INFO:root:current mean train loss 15670.98707696889
INFO:root:current train perplexity4.685951232910156


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.72s/it]
INFO:root:final mean train loss: 15659.836981004284
INFO:root:final train perplexity: 4.6859307289123535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.32s/it]
INFO:root:eval mean loss: 22287.33947172619
INFO:root:eval perplexity: 10.040666580200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [13:02:37<16:37:37, 534.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15651.392942708333
INFO:root:current train perplexity4.66909122467041
INFO:root:current mean train loss 15641.45806919643
INFO:root:current train perplexity4.67517614364624


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.48s/it]
INFO:root:final mean train loss: 15646.10017641129
INFO:root:final train perplexity: 4.679585933685303
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.99s/it]
INFO:root:eval mean loss: 22278.355492001487
INFO:root:eval perplexity: 10.03133487701416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [13:11:24<16:24:19, 532.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15592.739908854166
INFO:root:current train perplexity4.657513618469238
INFO:root:current mean train loss 15584.139156311516
INFO:root:current train perplexity4.651761054992676
INFO:root:current mean train loss 15623.286089792125
INFO:root:current train perplexity4.665360450744629


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.75s/it]
INFO:root:final mean train loss: 15625.189677576866
INFO:root:final train perplexity: 4.669945240020752
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.30s/it]
INFO:root:eval mean loss: 22293.38941592262
INFO:root:eval perplexity: 10.046957015991211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [13:20:13<16:13:48, 531.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15661.730975573575
INFO:root:current train perplexity4.662280559539795
INFO:root:current mean train loss 15604.71153216655
INFO:root:current train perplexity4.657219409942627


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 455.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 455.00s/it]
INFO:root:final mean train loss: 15603.792244203629
INFO:root:final train perplexity: 4.660099506378174
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.22s/it]
INFO:root:eval mean loss: 22273.634347098214
INFO:root:eval perplexity: 10.026437759399414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/91
################## best##################
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [13:29:06<16:06:06, 531.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15605.669984879032
INFO:root:current train perplexity4.660914421081543
INFO:root:current mean train loss 15587.59551675811
INFO:root:current train perplexity4.656219959259033
INFO:root:current mean train loss 15604.62018060065
INFO:root:current train perplexity4.657162189483643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.07s/it]
INFO:root:final mean train loss: 15595.216088079636
INFO:root:final train perplexity: 4.656158924102783
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.03s/it]
INFO:root:eval mean loss: 22299.591657366072
INFO:root:eval perplexity: 10.053407669067383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [13:37:54<15:55:12, 530.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15602.963196536144
INFO:root:current train perplexity4.652682304382324
INFO:root:current mean train loss 15583.326129183743
INFO:root:current train perplexity4.645224571228027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.87s/it]
INFO:root:final mean train loss: 15572.542704920616
INFO:root:final train perplexity: 4.645758152008057
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.45s/it]
INFO:root:eval mean loss: 22292.94308035714
INFO:root:eval perplexity: 10.046492576599121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [13:46:53<15:51:02, 533.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15552.689202008929
INFO:root:current train perplexity4.611642360687256
INFO:root:current mean train loss 15539.69308449074
INFO:root:current train perplexity4.63120174407959
INFO:root:current mean train loss 15566.394240359043
INFO:root:current train perplexity4.637367248535156


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.75s/it]
INFO:root:final mean train loss: 15557.068977602066
INFO:root:final train perplexity: 4.638674259185791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.67s/it]
INFO:root:eval mean loss: 22311.2333984375
INFO:root:eval perplexity: 10.065526008605957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [13:55:46<15:41:37, 533.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15565.794753502156
INFO:root:current train perplexity4.62226676940918
INFO:root:current mean train loss 15561.380159592247
INFO:root:current train perplexity4.633352279663086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.88s/it]
INFO:root:final mean train loss: 15539.377681609123
INFO:root:final train perplexity: 4.63058614730835
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:26<00:00, 86.22s/it]
INFO:root:eval mean loss: 22300.879417782737
INFO:root:eval perplexity: 10.054747581481934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [14:04:45<15:36:00, 534.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15479.154747596154
INFO:root:current train perplexity4.58314847946167
INFO:root:current mean train loss 15529.49400011241
INFO:root:current train perplexity4.609085559844971
INFO:root:current mean train loss 15546.996032459467
INFO:root:current train perplexity4.623960971832275


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.68s/it]
INFO:root:final mean train loss: 15524.647815335182
INFO:root:final train perplexity: 4.623863697052002
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.94s/it]
INFO:root:eval mean loss: 22303.98790922619
INFO:root:eval perplexity: 10.057981491088867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [14:13:38<15:26:23, 534.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15554.41127232143
INFO:root:current train perplexity4.623359680175781
INFO:root:current mean train loss 15511.249263743455
INFO:root:current train perplexity4.617438316345215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.25s/it]
INFO:root:final mean train loss: 15511.811897523941
INFO:root:final train perplexity: 4.618013858795166
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.11s/it]
INFO:root:eval mean loss: 22301.650809151786
INFO:root:eval perplexity: 10.055550575256348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [14:22:26<15:13:48, 532.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15484.089707485466
INFO:root:current train perplexity4.613404273986816
INFO:root:current mean train loss 15505.893138111887
INFO:root:current train perplexity4.613149642944336
INFO:root:current mean train loss 15502.151270736882
INFO:root:current train perplexity4.609847068786621


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:23<00:00, 443.99s/it]
INFO:root:final mean train loss: 15492.865206810737
INFO:root:final train perplexity: 4.609391689300537
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.47s/it]
INFO:root:eval mean loss: 22296.58328683036
INFO:root:eval perplexity: 10.050277709960938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [14:31:04<14:57:50, 528.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15526.549054276316
INFO:root:current train perplexity4.603686809539795
INFO:root:current mean train loss 15493.876181891026
INFO:root:current train perplexity4.603039741516113


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.39s/it]
INFO:root:final mean train loss: 15481.691390498992
INFO:root:final train perplexity: 4.604314804077148
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.55s/it]
INFO:root:eval mean loss: 22299.109561011905
INFO:root:eval perplexity: 10.05290412902832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [14:40:05<14:55:29, 531.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15403.89573636968
INFO:root:current train perplexity4.554571628570557
INFO:root:current mean train loss 15463.586409173044
INFO:root:current train perplexity4.587360382080078
INFO:root:current mean train loss 15479.062017649292
INFO:root:current train perplexity4.597580432891846


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.52s/it]
INFO:root:final mean train loss: 15468.504894625756
INFO:root:final train perplexity: 4.598330020904541
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:19<00:00, 79.19s/it]
INFO:root:eval mean loss: 22304.017485119046
INFO:root:eval perplexity: 10.058012962341309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [14:49:01<14:48:24, 533.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15401.180377998737
INFO:root:current train perplexity4.572316646575928
INFO:root:current mean train loss 15469.66113771985
INFO:root:current train perplexity4.5898284912109375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.17s/it]
INFO:root:final mean train loss: 15451.185389364919
INFO:root:final train perplexity: 4.590481281280518
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.46s/it]
INFO:root:eval mean loss: 22290.93917410714
INFO:root:eval perplexity: 10.044408798217773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [14:57:46<14:35:51, 530.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15476.734872855392
INFO:root:current train perplexity4.573767185211182
INFO:root:current mean train loss 15458.527473096026
INFO:root:current train perplexity4.583810806274414


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.48s/it]
INFO:root:final mean train loss: 15436.603499873992
INFO:root:final train perplexity: 4.583884239196777
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.45s/it]
INFO:root:eval mean loss: 22300.635416666668
INFO:root:eval perplexity: 10.05449104309082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [15:06:26<14:21:35, 527.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15475.619140625
INFO:root:current train perplexity4.508864402770996
INFO:root:current mean train loss 15456.186049378033
INFO:root:current train perplexity4.571176528930664
INFO:root:current mean train loss 15421.800863031096
INFO:root:current train perplexity4.572212219238281


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:20<00:00, 440.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:20<00:00, 440.20s/it]
INFO:root:final mean train loss: 15423.223333543347
INFO:root:final train perplexity: 4.577838897705078
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.58s/it]
INFO:root:eval mean loss: 22289.0595703125
INFO:root:eval perplexity: 10.042457580566406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [15:15:02<14:07:08, 524.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15383.6291015625
INFO:root:current train perplexity4.544694900512695
INFO:root:current mean train loss 15409.37300907258
INFO:root:current train perplexity4.570835113525391


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 450.00s/it]
INFO:root:final mean train loss: 15412.283226751511
INFO:root:final train perplexity: 4.572902202606201
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.23s/it]
INFO:root:eval mean loss: 22302.906389508928
INFO:root:eval perplexity: 10.056857109069824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [15:23:46<13:58:29, 524.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15194.836635044643
INFO:root:current train perplexity4.499905109405518
INFO:root:current mean train loss 15319.743720794393
INFO:root:current train perplexity4.548704624176025
INFO:root:current mean train loss 15404.048016681763
INFO:root:current train perplexity4.5666680335998535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.58s/it]
INFO:root:final mean train loss: 15395.677561113911
INFO:root:final train perplexity: 4.565418243408203
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.36s/it]
INFO:root:eval mean loss: 22304.475283668155
INFO:root:eval perplexity: 10.058487892150879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [15:32:46<13:57:17, 528.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15383.596944518009
INFO:root:current train perplexity4.557493209838867
INFO:root:current mean train loss 15420.214069870282
INFO:root:current train perplexity4.562411308288574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.90s/it]
INFO:root:final mean train loss: 15378.501169512348
INFO:root:final train perplexity: 4.557690143585205
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.25s/it]
INFO:root:eval mean loss: 22306.759184337796
INFO:root:eval perplexity: 10.060867309570312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [15:41:24<13:43:24, 525.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15111.06258877841
INFO:root:current train perplexity4.515437126159668
INFO:root:current mean train loss 15340.349776534347
INFO:root:current train perplexity4.539556980133057
INFO:root:current mean train loss 15376.908184611966
INFO:root:current train perplexity4.5509033203125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.16s/it]
INFO:root:final mean train loss: 15366.501291582661
INFO:root:final train perplexity: 4.552298545837402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.55s/it]
INFO:root:eval mean loss: 22317.556733630954
INFO:root:eval perplexity: 10.072115898132324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [15:50:06<13:32:49, 524.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15323.533466641866
INFO:root:current train perplexity4.53851842880249
INFO:root:current mean train loss 15361.272347105061
INFO:root:current train perplexity4.545899868011475


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.87s/it]
INFO:root:final mean train loss: 15356.832062752017
INFO:root:final train perplexity: 4.547959804534912
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it]
INFO:root:eval mean loss: 22315.391903831845
INFO:root:eval perplexity: 10.06986141204834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [15:58:48<13:23:18, 523.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15078.677213541667
INFO:root:current train perplexity4.514003753662109
INFO:root:current mean train loss 15308.770490828805
INFO:root:current train perplexity4.526026248931885
INFO:root:current mean train loss 15337.026417151163
INFO:root:current train perplexity4.533308029174805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.71s/it]
INFO:root:final mean train loss: 15335.72361312374
INFO:root:final train perplexity: 4.538500785827637
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.39s/it]
INFO:root:eval mean loss: 22316.902157738095
INFO:root:eval perplexity: 10.071434020996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [16:07:47<13:21:29, 528.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15252.357523903918
INFO:root:current train perplexity4.514410018920898
INFO:root:current mean train loss 15318.93392122006
INFO:root:current train perplexity4.533116817474365


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.09s/it]
INFO:root:final mean train loss: 15327.905048985634
INFO:root:final train perplexity: 4.5350022315979
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.51s/it]
INFO:root:eval mean loss: 22314.732073102678
INFO:root:eval perplexity: 10.069172859191895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [16:16:32<13:10:55, 527.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15295.288188733553
INFO:root:current train perplexity4.504457950592041
INFO:root:current mean train loss 15359.269482011554
INFO:root:current train perplexity4.528292655944824
INFO:root:current mean train loss 15317.919984303653
INFO:root:current train perplexity4.525594234466553


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.80s/it]
INFO:root:final mean train loss: 15311.58558704007
INFO:root:final train perplexity: 4.527707576751709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.06s/it]
INFO:root:eval mean loss: 22312.04271298363
INFO:root:eval perplexity: 10.06636905670166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [16:25:19<13:01:55, 527.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15324.362070862677
INFO:root:current train perplexity4.524440765380859
INFO:root:current mean train loss 15314.520410727338
INFO:root:current train perplexity4.524928092956543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.74s/it]
INFO:root:final mean train loss: 15299.91857516381
INFO:root:final train perplexity: 4.522501468658447
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.65s/it]
INFO:root:eval mean loss: 22320.648623511905
INFO:root:eval perplexity: 10.07534122467041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [16:34:04<12:52:20, 526.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15230.312330163044
INFO:root:current train perplexity4.500853538513184
INFO:root:current mean train loss 15271.274032964939
INFO:root:current train perplexity4.514681816101074
INFO:root:current mean train loss 15292.386245795964
INFO:root:current train perplexity4.518036842346191


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.42s/it]
INFO:root:final mean train loss: 15287.52310279108
INFO:root:final train perplexity: 4.516974925994873
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.46s/it]
INFO:root:eval mean loss: 22310.55106026786
INFO:root:eval perplexity: 10.064818382263184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [16:42:52<12:44:06, 526.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15260.6733984375
INFO:root:current train perplexity4.506692886352539
INFO:root:current mean train loss 15288.759296875
INFO:root:current train perplexity4.514736175537109


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.76s/it]
INFO:root:final mean train loss: 15274.450998613911
INFO:root:final train perplexity: 4.511155128479004
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it]
INFO:root:eval mean loss: 22317.878859747023
INFO:root:eval perplexity: 10.072454452514648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [16:51:48<12:39:19, 529.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15236.232638888889
INFO:root:current train perplexity4.5253753662109375
INFO:root:current mean train loss 15257.15435070128
INFO:root:current train perplexity4.507641315460205
INFO:root:current mean train loss 15270.585361026982
INFO:root:current train perplexity4.503823757171631


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.64s/it]
INFO:root:final mean train loss: 15263.95658234627
INFO:root:final train perplexity: 4.506487846374512
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it]
INFO:root:eval mean loss: 22304.45703125
INFO:root:eval perplexity: 10.058473587036133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [17:00:52<12:36:30, 534.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15229.653555181962
INFO:root:current train perplexity4.492785453796387
INFO:root:current mean train loss 15266.782723027234
INFO:root:current train perplexity4.49717903137207


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.31s/it]
INFO:root:final mean train loss: 15251.113982169858
INFO:root:final train perplexity: 4.500783443450928
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.86s/it]
INFO:root:eval mean loss: 22317.58972749256
INFO:root:eval perplexity: 10.072152137756348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [17:09:49<12:28:54, 534.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15293.806766633064
INFO:root:current train perplexity4.503055095672607
INFO:root:current mean train loss 15267.844174916507
INFO:root:current train perplexity4.495481014251709
INFO:root:current mean train loss 15252.660629734848
INFO:root:current train perplexity4.495841026306152


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.69s/it]
INFO:root:final mean train loss: 15236.247873613911
INFO:root:final train perplexity: 4.4941887855529785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it]
INFO:root:eval mean loss: 22331.957356770832
INFO:root:eval perplexity: 10.087141036987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [17:18:46<12:20:44, 535.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15252.019860692772
INFO:root:current train perplexity4.490024089813232
INFO:root:current mean train loss 15262.003836876707
INFO:root:current train perplexity4.494282245635986


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.28s/it]
INFO:root:final mean train loss: 15232.928470734627
INFO:root:final train perplexity: 4.4927167892456055
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.98s/it]
INFO:root:eval mean loss: 22330.74793061756
INFO:root:eval perplexity: 10.085875511169434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [17:27:51<12:15:48, 538.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15176.674051339285
INFO:root:current train perplexity4.461453914642334
INFO:root:current mean train loss 15231.903233506944
INFO:root:current train perplexity4.4772868156433105
INFO:root:current mean train loss 15232.738081781916
INFO:root:current train perplexity4.488092422485352


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.34s/it]
INFO:root:final mean train loss: 15222.295457409275
INFO:root:final train perplexity: 4.48800802230835
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.54s/it]
INFO:root:eval mean loss: 22332.02673921131
INFO:root:eval perplexity: 10.087210655212402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [17:36:50<12:07:00, 538.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15154.052173132184
INFO:root:current train perplexity4.473606586456299
INFO:root:current mean train loss 15212.215621866644
INFO:root:current train perplexity4.480216026306152


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.73s/it]
INFO:root:final mean train loss: 15207.659703408519
INFO:root:final train perplexity: 4.481534004211426
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.21s/it]
INFO:root:eval mean loss: 22341.710588727678
INFO:root:eval perplexity: 10.097328186035156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [17:45:43<11:55:46, 536.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15170.700796274039
INFO:root:current train perplexity4.463762283325195
INFO:root:current mean train loss 15179.030526360162
INFO:root:current train perplexity4.473239898681641
INFO:root:current mean train loss 15206.79773306747
INFO:root:current train perplexity4.476598739624023


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.98s/it]
INFO:root:final mean train loss: 15196.576683782761
INFO:root:final train perplexity: 4.476637840270996
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.40s/it]
INFO:root:eval mean loss: 22328.698707217263
INFO:root:eval perplexity: 10.083738327026367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [17:54:43<11:48:12, 537.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15160.938358516483
INFO:root:current train perplexity4.462348937988281
INFO:root:current mean train loss 15181.356307264397
INFO:root:current train perplexity4.46265983581543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.50s/it]
INFO:root:final mean train loss: 15189.520771641884
INFO:root:final train perplexity: 4.47352409362793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.68s/it]
INFO:root:eval mean loss: 22328.099051339286
INFO:root:eval perplexity: 10.083110809326172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [18:03:44<11:40:30, 538.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15168.81679233285
INFO:root:current train perplexity4.485378742218018
INFO:root:current mean train loss 15202.875689739947
INFO:root:current train perplexity4.472766399383545
INFO:root:current mean train loss 15190.801183127573
INFO:root:current train perplexity4.468990802764893


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.77s/it]
INFO:root:final mean train loss: 15177.883233839466
INFO:root:final train perplexity: 4.4683918952941895
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.50s/it]
INFO:root:eval mean loss: 22343.87732514881
INFO:root:eval perplexity: 10.099591255187988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [18:12:42<11:30:53, 538.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15152.894911595395
INFO:root:current train perplexity4.4625372886657715
INFO:root:current mean train loss 15188.095182291667
INFO:root:current train perplexity4.463101863861084


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:58<00:00, 478.02s/it]
INFO:root:final mean train loss: 15169.42611595892
INFO:root:final train perplexity: 4.46466588973999
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it]
INFO:root:eval mean loss: 22338.930524553572
INFO:root:eval perplexity: 10.094420433044434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [18:21:58<11:28:51, 543.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15154.53339012633
INFO:root:current train perplexity4.447509765625
INFO:root:current mean train loss 15173.716398278062
INFO:root:current train perplexity4.4570841789245605
INFO:root:current mean train loss 15171.39151853492
INFO:root:current train perplexity4.4597930908203125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.85s/it]
INFO:root:final mean train loss: 15157.671682050152
INFO:root:final train perplexity: 4.4594926834106445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.02s/it]
INFO:root:eval mean loss: 22337.60681733631
INFO:root:eval perplexity: 10.093039512634277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [18:30:58<11:18:17, 542.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15145.696131234217
INFO:root:current train perplexity4.453728675842285
INFO:root:current mean train loss 15157.213715059674
INFO:root:current train perplexity4.452769756317139


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.96s/it]
INFO:root:final mean train loss: 15148.575679655998
INFO:root:final train perplexity: 4.455493927001953
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.28s/it]
INFO:root:eval mean loss: 22336.94982328869
INFO:root:eval perplexity: 10.092353820800781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [18:39:48<11:04:39, 538.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15127.592524509804
INFO:root:current train perplexity4.439255714416504
INFO:root:current mean train loss 15119.619166494205
INFO:root:current train perplexity4.441967487335205


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.08s/it]
INFO:root:final mean train loss: 15133.335110572076
INFO:root:final train perplexity: 4.448801040649414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.90s/it]
INFO:root:eval mean loss: 22339.94689360119
INFO:root:eval perplexity: 10.095484733581543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [18:48:53<10:57:54, 540.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14966.126953125
INFO:root:current train perplexity4.456976413726807
INFO:root:current mean train loss 15128.152277381674
INFO:root:current train perplexity4.448939800262451
INFO:root:current mean train loss 15125.972593711514
INFO:root:current train perplexity4.442096710205078


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.12s/it]
INFO:root:final mean train loss: 15127.342395413307
INFO:root:final train perplexity: 4.446172714233398
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.31s/it]
INFO:root:eval mean loss: 22339.13709077381
INFO:root:eval perplexity: 10.094636917114258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [18:57:49<10:46:56, 539.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15144.405149147728
INFO:root:current train perplexity4.446350574493408
INFO:root:current mean train loss 15124.99627016129
INFO:root:current train perplexity4.443846225738525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.95s/it]
INFO:root:final mean train loss: 15115.968785439769
INFO:root:final train perplexity: 4.441187858581543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.40s/it]
INFO:root:eval mean loss: 22338.470772879464
INFO:root:eval perplexity: 10.093941688537598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [19:06:50<10:38:43, 539.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15171.53515625
INFO:root:current train perplexity4.408527851104736
INFO:root:current mean train loss 15129.972063011099
INFO:root:current train perplexity4.445491790771484
INFO:root:current mean train loss 15115.083536194143
INFO:root:current train perplexity4.436868190765381


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:39<00:00, 459.26s/it]
INFO:root:final mean train loss: 15104.304715064263
INFO:root:final train perplexity: 4.436080455780029
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.77s/it]
INFO:root:eval mean loss: 22374.09449404762
INFO:root:eval perplexity: 10.1312255859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [19:15:48<10:29:05, 539.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15134.54194253178
INFO:root:current train perplexity4.441770553588867
INFO:root:current mean train loss 15084.862562647406
INFO:root:current train perplexity4.431017875671387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.09s/it]
INFO:root:final mean train loss: 15094.562102287045
INFO:root:final train perplexity: 4.431820392608643
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.23s/it]
INFO:root:eval mean loss: 22348.00074404762
INFO:root:eval perplexity: 10.103902816772461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [19:24:52<10:21:50, 540.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14995.232421875
INFO:root:current train perplexity4.445298194885254
INFO:root:current mean train loss 15096.16895411036
INFO:root:current train perplexity4.426127910614014
INFO:root:current mean train loss 15093.188171097452
INFO:root:current train perplexity4.428483963012695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.00s/it]
INFO:root:final mean train loss: 15091.128603043095
INFO:root:final train perplexity: 4.4303202629089355
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.52s/it]
INFO:root:eval mean loss: 22354.388625372023
INFO:root:eval perplexity: 10.11058521270752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [19:33:54<10:13:04, 540.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14986.851050967261
INFO:root:current train perplexity4.415438175201416
INFO:root:current mean train loss 15048.836932036043
INFO:root:current train perplexity4.417067050933838


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.85s/it]
INFO:root:final mean train loss: 15079.658246440273
INFO:root:final train perplexity: 4.4253106117248535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.63s/it]
INFO:root:eval mean loss: 22358.169456845237
INFO:root:eval perplexity: 10.114543914794922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [19:43:05<10:07:33, 544.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15038.290364583334
INFO:root:current train perplexity4.431042671203613
INFO:root:current mean train loss 15084.013289741848
INFO:root:current train perplexity4.427146911621094
INFO:root:current mean train loss 15080.941787790698
INFO:root:current train perplexity4.42026948928833


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.01s/it]
INFO:root:final mean train loss: 15072.723821824597
INFO:root:final train perplexity: 4.4222846031188965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.41s/it]
INFO:root:eval mean loss: 22360.48790922619
INFO:root:eval perplexity: 10.116968154907227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [19:51:56<9:54:19, 540.29s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15028.084275886195
INFO:root:current train perplexity4.403933525085449
INFO:root:current mean train loss 15100.388847305388
INFO:root:current train perplexity4.425508975982666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 465.69s/it]
INFO:root:final mean train loss: 15066.249350270917
INFO:root:final train perplexity: 4.419461250305176
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it]
INFO:root:eval mean loss: 22362.321103050595
INFO:root:eval perplexity: 10.118887901306152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [20:01:00<9:46:19, 541.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15113.070929276315
INFO:root:current train perplexity4.420020580291748
INFO:root:current mean train loss 15094.679506959033
INFO:root:current train perplexity4.426956653594971
INFO:root:current mean train loss 15089.772536743722
INFO:root:current train perplexity4.418257713317871


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:52<00:00, 472.68s/it]
INFO:root:final mean train loss: 15055.960342899445
INFO:root:final train perplexity: 4.41497802734375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.32s/it]
INFO:root:eval mean loss: 22372.62853422619
INFO:root:eval perplexity: 10.129687309265137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [20:10:10<9:40:04, 543.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15038.64315580986
INFO:root:current train perplexity4.403750419616699
INFO:root:current mean train loss 15075.188242415936
INFO:root:current train perplexity4.411971092224121


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.22s/it]
INFO:root:final mean train loss: 15047.254378780242
INFO:root:final train perplexity: 4.411189079284668
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.97s/it]
INFO:root:eval mean loss: 22365.80798921131
INFO:root:eval perplexity: 10.122541427612305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [20:19:05<9:28:17, 541.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14945.465098505434
INFO:root:current train perplexity4.3794074058532715
INFO:root:current mean train loss 15037.954585873984
INFO:root:current train perplexity4.40914249420166
INFO:root:current mean train loss 15044.79455402186
INFO:root:current train perplexity4.4080023765563965


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.17s/it]
INFO:root:final mean train loss: 15039.338150516633
INFO:root:final train perplexity: 4.407746315002441
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.25s/it]
INFO:root:eval mean loss: 22366.90452938988
INFO:root:eval perplexity: 10.123692512512207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [20:28:07<9:19:37, 541.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14987.68203125
INFO:root:current train perplexity4.388797760009766
INFO:root:current mean train loss 15028.596902901785
INFO:root:current train perplexity4.394062519073486


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.61s/it]
INFO:root:final mean train loss: 15029.474506993447
INFO:root:final train perplexity: 4.403459548950195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.01s/it]
INFO:root:eval mean loss: 22352.20556640625
INFO:root:eval perplexity: 10.10830020904541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [20:37:04<9:09:03, 540.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15030.844075520834
INFO:root:current train perplexity4.401366233825684
INFO:root:current mean train loss 14995.182894008367
INFO:root:current train perplexity4.391465187072754
INFO:root:current mean train loss 15033.040735992567
INFO:root:current train perplexity4.398589611053467


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:54<00:00, 474.35s/it]
INFO:root:final mean train loss: 15021.624621975807
INFO:root:final train perplexity: 4.400052070617676
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.05s/it]
INFO:root:eval mean loss: 22364.88402157738
INFO:root:eval perplexity: 10.121573448181152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [20:46:16<9:03:45, 543.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14973.606741989715
INFO:root:current train perplexity4.39304256439209
INFO:root:current mean train loss 15004.320508903631
INFO:root:current train perplexity4.39309549331665


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.28s/it]
INFO:root:final mean train loss: 15015.333641790574
INFO:root:final train perplexity: 4.397322177886963
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.28s/it]
INFO:root:eval mean loss: 22359.776553199405
INFO:root:eval perplexity: 10.116225242614746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [20:55:17<8:53:45, 542.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14991.808625252017
INFO:root:current train perplexity4.396646976470947
INFO:root:current mean train loss 15022.820342318702
INFO:root:current train perplexity4.399649143218994
INFO:root:current mean train loss 15016.026375642587
INFO:root:current train perplexity4.394995212554932


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.20s/it]
INFO:root:final mean train loss: 15005.52113391507
INFO:root:final train perplexity: 4.393069267272949
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.80s/it]
INFO:root:eval mean loss: 22377.604678199405
INFO:root:eval perplexity: 10.134907722473145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [21:04:11<8:42:09, 540.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15063.679958113706
INFO:root:current train perplexity4.382848262786865
INFO:root:current mean train loss 15020.159777365096
INFO:root:current train perplexity4.385763645172119


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.23s/it]
INFO:root:final mean train loss: 14999.162526776714
INFO:root:final train perplexity: 4.39031457901001
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.74s/it]
INFO:root:eval mean loss: 22361.24732607887
INFO:root:eval perplexity: 10.117761611938477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [21:13:17<8:34:47, 541.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14945.206752232143
INFO:root:current train perplexity4.358877182006836
INFO:root:current mean train loss 14985.337290219908
INFO:root:current train perplexity4.3768134117126465
INFO:root:current mean train loss 14995.741107047872
INFO:root:current train perplexity4.3879075050354


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.02s/it]
INFO:root:final mean train loss: 14994.058357484879
INFO:root:final train perplexity: 4.388104438781738
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.25s/it]
INFO:root:eval mean loss: 22371.52755301339
INFO:root:eval perplexity: 10.128538131713867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [21:22:11<8:23:38, 539.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14965.037558369253
INFO:root:current train perplexity4.373006343841553
INFO:root:current mean train loss 15003.944967830883
INFO:root:current train perplexity4.385434627532959


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.44s/it]
INFO:root:final mean train loss: 14983.192603326614
INFO:root:final train perplexity: 4.383404731750488
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.55s/it]
INFO:root:eval mean loss: 22370.30236235119
INFO:root:eval perplexity: 10.127250671386719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [21:31:03<8:12:31, 537.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14996.874499198719
INFO:root:current train perplexity4.376955986022949
INFO:root:current mean train loss 14983.702457565198
INFO:root:current train perplexity4.380825996398926
INFO:root:current mean train loss 14986.851905726988
INFO:root:current train perplexity4.380258560180664


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.37s/it]
INFO:root:final mean train loss: 14977.031458700856
INFO:root:final train perplexity: 4.380741596221924
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.76s/it]
INFO:root:eval mean loss: 22372.76492745536
INFO:root:eval perplexity: 10.12983226776123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [21:39:53<8:01:38, 535.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15016.868743561126
INFO:root:current train perplexity4.37073278427124
INFO:root:current mean train loss 14970.61115428665
INFO:root:current train perplexity4.366870403289795


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.87s/it]
INFO:root:final mean train loss: 14968.115777784778
INFO:root:final train perplexity: 4.376891613006592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.05s/it]
INFO:root:eval mean loss: 22380.2734375
INFO:root:eval perplexity: 10.13770580291748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [21:48:54<7:54:14, 536.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14966.863962572674
INFO:root:current train perplexity4.37637996673584
INFO:root:current mean train loss 14961.001830201049
INFO:root:current train perplexity4.368248462677002
INFO:root:current mean train loss 14981.860439975566
INFO:root:current train perplexity4.376126766204834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.90s/it]
INFO:root:final mean train loss: 14966.866321194557
INFO:root:final train perplexity: 4.376352310180664
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.93s/it]
INFO:root:eval mean loss: 22378.038016183036
INFO:root:eval perplexity: 10.135364532470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [21:57:50<7:45:01, 536.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14974.346720805921
INFO:root:current train perplexity4.371159553527832
INFO:root:current mean train loss 14960.681350160256
INFO:root:current train perplexity4.37040901184082


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.31s/it]
INFO:root:final mean train loss: 14959.344986454133
INFO:root:final train perplexity: 4.373106479644775
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it]
INFO:root:eval mean loss: 22376.16429501488
INFO:root:eval perplexity: 10.133395195007324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [22:06:37<7:33:49, 533.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14929.687957114362
INFO:root:current train perplexity4.354315280914307
INFO:root:current mean train loss 14928.223420227467
INFO:root:current train perplexity4.358432769775391
INFO:root:current mean train loss 14961.073087993422
INFO:root:current train perplexity4.369246959686279


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.64s/it]
INFO:root:final mean train loss: 14950.252933625252
INFO:root:final train perplexity: 4.369186878204346
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:12<00:00, 72.71s/it]
INFO:root:eval mean loss: 22377.565964471727
INFO:root:eval perplexity: 10.134866714477539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [22:15:25<7:23:16, 531.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14931.211835148359
INFO:root:current train perplexity4.3567423820495605
INFO:root:current mean train loss 14922.703164258794
INFO:root:current train perplexity4.361671447753906


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.24s/it]
INFO:root:final mean train loss: 14941.913814421623
INFO:root:final train perplexity: 4.365593910217285
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.65s/it]
INFO:root:eval mean loss: 22392.22042410714
INFO:root:eval perplexity: 10.150248527526855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [22:24:09<7:12:26, 529.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14921.987113204657
INFO:root:current train perplexity4.360403537750244
INFO:root:current mean train loss 14923.038228218129
INFO:root:current train perplexity4.357929229736328


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.56s/it]
INFO:root:final mean train loss: 14936.492100869456
INFO:root:final train perplexity: 4.363260269165039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.76s/it]
INFO:root:eval mean loss: 22387.840169270832
INFO:root:eval perplexity: 10.145648956298828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [22:32:52<7:02:07, 527.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15021.940104166666
INFO:root:current train perplexity4.379092693328857
INFO:root:current mean train loss 14915.44237333131
INFO:root:current train perplexity4.35086727142334
INFO:root:current mean train loss 14940.121901939656
INFO:root:current train perplexity4.359746932983398


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.44s/it]
INFO:root:final mean train loss: 14928.84142672631
INFO:root:final train perplexity: 4.359969615936279
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:13<00:00, 73.31s/it]
INFO:root:eval mean loss: 22386.482328869046
INFO:root:eval perplexity: 10.144222259521484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [22:41:32<6:51:29, 525.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14884.989968039772
INFO:root:current train perplexity4.3361077308654785
INFO:root:current mean train loss 14913.36459173387
INFO:root:current train perplexity4.350595951080322


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.05s/it]
INFO:root:final mean train loss: 14924.477271295364
INFO:root:final train perplexity: 4.358092784881592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.91s/it]
INFO:root:eval mean loss: 22392.28150576637
INFO:root:eval perplexity: 10.150314331054688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [22:50:18<6:42:52, 525.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15012.760323660714
INFO:root:current train perplexity4.402980327606201
INFO:root:current mean train loss 14891.70824510806
INFO:root:current train perplexity4.351927757263184
INFO:root:current mean train loss 14936.384086277174
INFO:root:current train perplexity4.357658386230469


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:24<00:00, 444.12s/it]
INFO:root:final mean train loss: 14919.243778351814
INFO:root:final train perplexity: 4.355844020843506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.30s/it]
INFO:root:eval mean loss: 22396.88537016369
INFO:root:eval perplexity: 10.155150413513184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [22:58:59<6:33:11, 524.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14957.698324947034
INFO:root:current train perplexity4.357753276824951
INFO:root:current mean train loss 14907.737335397012
INFO:root:current train perplexity4.350285530090332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:28<00:00, 448.93s/it]
INFO:root:final mean train loss: 14915.388207220261
INFO:root:final train perplexity: 4.354187965393066
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.09s/it]
INFO:root:eval mean loss: 22392.807314918155
INFO:root:eval perplexity: 10.15086555480957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [23:07:45<6:24:49, 524.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14880.506480823864
INFO:root:current train perplexity4.300942897796631
INFO:root:current mean train loss 14909.611037795608
INFO:root:current train perplexity4.341957092285156
INFO:root:current mean train loss 14913.321335345083
INFO:root:current train perplexity4.346359729766846


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.18s/it]
INFO:root:final mean train loss: 14907.037609469506
INFO:root:final train perplexity: 4.350602626800537
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.79s/it]
INFO:root:eval mean loss: 22397.069428943454
INFO:root:eval perplexity: 10.155346870422363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [23:16:31<6:16:20, 525.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14836.478949652777
INFO:root:current train perplexity4.33705997467041
INFO:root:current mean train loss 14914.663499328988
INFO:root:current train perplexity4.351037502288818


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:29<00:00, 449.85s/it]
INFO:root:final mean train loss: 14902.012545677924
INFO:root:final train perplexity: 4.348446846008301
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.20s/it]
INFO:root:eval mean loss: 22394.359398251487
INFO:root:eval perplexity: 10.152497291564941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [23:25:18<6:07:59, 525.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14957.420247395834
INFO:root:current train perplexity4.339906692504883
INFO:root:current mean train loss 14913.596781589675
INFO:root:current train perplexity4.352843761444092
INFO:root:current mean train loss 14913.385342478197
INFO:root:current train perplexity4.3487467765808105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:25<00:00, 445.17s/it]
INFO:root:final mean train loss: 14897.32078109249
INFO:root:final train perplexity: 4.346435546875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it]
INFO:root:eval mean loss: 22395.121535528273
INFO:root:eval perplexity: 10.153298377990723
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [23:34:02<5:58:50, 525.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.340047807837
INFO:root:current train perplexity4.3256659507751465
INFO:root:current mean train loss 14874.780021987275
INFO:root:current train perplexity4.333769798278809


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.18s/it]
INFO:root:final mean train loss: 14886.893885458669
INFO:root:final train perplexity: 4.341967582702637
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22398.287690662204
INFO:root:eval perplexity: 10.156627655029297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [23:42:52<5:51:02, 526.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14853.335680509868
INFO:root:current train perplexity4.315117359161377
INFO:root:current mean train loss 14903.23363642332
INFO:root:current train perplexity4.341256618499756
INFO:root:current mean train loss 14887.721652932363
INFO:root:current train perplexity4.342211723327637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:22<00:00, 442.73s/it]
INFO:root:final mean train loss: 14886.061621881301
INFO:root:final train perplexity: 4.341610908508301
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.71s/it]
INFO:root:eval mean loss: 22397.781389508928
INFO:root:eval perplexity: 10.156092643737793
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [23:51:33<5:41:18, 525.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14894.730056117958
INFO:root:current train perplexity4.329596519470215
INFO:root:current mean train loss 14885.02428271199
INFO:root:current train perplexity4.331970691680908


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:26<00:00, 446.20s/it]
INFO:root:final mean train loss: 14879.9152359501
INFO:root:final train perplexity: 4.338979721069336
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.40s/it]
INFO:root:eval mean loss: 22391.806454613095
INFO:root:eval perplexity: 10.149815559387207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [24:00:19<5:32:38, 525.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14878.02390455163
INFO:root:current train perplexity4.350894451141357
INFO:root:current mean train loss 14921.796335111789
INFO:root:current train perplexity4.341267108917236
INFO:root:current mean train loss 14885.846044702916
INFO:root:current train perplexity4.334343910217285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:30<00:00, 450.79s/it]
INFO:root:final mean train loss: 14873.337957566784
INFO:root:final train perplexity: 4.336165904998779
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.71s/it]
INFO:root:eval mean loss: 22394.131417410714
INFO:root:eval perplexity: 10.152257919311523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [24:09:08<5:24:30, 526.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14852.893880208334
INFO:root:current train perplexity4.34228515625
INFO:root:current mean train loss 14864.82736607143
INFO:root:current train perplexity4.33403205871582


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.80s/it]
INFO:root:final mean train loss: 14869.238466324345
INFO:root:final train perplexity: 4.334412574768066
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.87s/it]
INFO:root:eval mean loss: 22399.44847470238
INFO:root:eval perplexity: 10.157845497131348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [24:18:12<5:19:04, 531.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14917.8369140625
INFO:root:current train perplexity4.330018043518066
INFO:root:current mean train loss 14905.07838644193
INFO:root:current train perplexity4.347387790679932
INFO:root:current mean train loss 14883.74666592098
INFO:root:current train perplexity4.336051940917969


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.20s/it]
INFO:root:final mean train loss: 14865.617057554184
INFO:root:final train perplexity: 4.332864761352539
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:18<00:00, 78.15s/it]
INFO:root:eval mean loss: 22401.663016183036
INFO:root:eval perplexity: 10.160175323486328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [24:27:16<5:12:13, 535.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14898.421380537975
INFO:root:current train perplexity4.3398003578186035
INFO:root:current mean train loss 14886.268620155377
INFO:root:current train perplexity4.334129810333252


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:34<00:00, 454.31s/it]
INFO:root:final mean train loss: 14858.039267263104
INFO:root:final train perplexity: 4.329627990722656
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.70s/it]
INFO:root:eval mean loss: 22405.651041666668
INFO:root:eval perplexity: 10.164366722106934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [24:36:08<5:02:45, 534.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14874.825321320564
INFO:root:current train perplexity4.324150085449219
INFO:root:current mean train loss 14886.236425035782
INFO:root:current train perplexity4.327963352203369
INFO:root:current mean train loss 14875.386955492424
INFO:root:current train perplexity4.331875324249268


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:33<00:00, 453.69s/it]
INFO:root:final mean train loss: 14861.264227098034
INFO:root:final train perplexity: 4.331006050109863
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.61s/it]
INFO:root:eval mean loss: 22403.18126860119
INFO:root:eval perplexity: 10.16176986694336
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [24:44:59<4:53:21, 533.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14850.225432981928
INFO:root:current train perplexity4.32193660736084
INFO:root:current mean train loss 14879.156490138319
INFO:root:current train perplexity4.328631401062012


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it]
INFO:root:final mean train loss: 14856.084287581905
INFO:root:final train perplexity: 4.328793048858643
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.65s/it]
INFO:root:eval mean loss: 22400.756928943454
INFO:root:eval perplexity: 10.159221649169922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [24:53:53<4:44:37, 533.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 15005.784905133929
INFO:root:current train perplexity4.364519119262695
INFO:root:current mean train loss 14846.61665943287
INFO:root:current train perplexity4.320075511932373
INFO:root:current mean train loss 14869.69415724734
INFO:root:current train perplexity4.328537464141846


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:59<00:00, 479.17s/it]
INFO:root:final mean train loss: 14851.49968891759
INFO:root:final train perplexity: 4.326836109161377
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.88s/it]
INFO:root:eval mean loss: 22404.916201636905
INFO:root:eval perplexity: 10.163596153259277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [25:03:11<4:39:28, 540.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14838.37249685704
INFO:root:current train perplexity4.322375297546387
INFO:root:current mean train loss 14838.408798462568
INFO:root:current train perplexity4.320338726043701


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.01s/it]
INFO:root:final mean train loss: 14840.310968214466
INFO:root:final train perplexity: 4.322063446044922
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.85s/it]
INFO:root:eval mean loss: 22413.25125558036
INFO:root:eval perplexity: 10.172367095947266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [25:12:31<4:33:20, 546.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14757.953175080129
INFO:root:current train perplexity4.287921905517578
INFO:root:current mean train loss 14814.286919683003
INFO:root:current train perplexity4.3181071281433105
INFO:root:current mean train loss 14849.837584172332
INFO:root:current train perplexity4.323973178863525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.79s/it]
INFO:root:final mean train loss: 14841.38833716608
INFO:root:final train perplexity: 4.3225226402282715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.51s/it]
INFO:root:eval mean loss: 22410.76536923363
INFO:root:eval perplexity: 10.169748306274414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/171

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [25:21:39<4:24:26, 547.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14879.406174879809
INFO:root:current train perplexity4.337915420532227
INFO:root:current mean train loss 14834.227160708442
INFO:root:current train perplexity4.320043563842773


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.88s/it]
INFO:root:final mean train loss: 14835.144031155494
INFO:root:final train perplexity: 4.319861888885498
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.65s/it]
INFO:root:eval mean loss: 22404.36083984375
INFO:root:eval perplexity: 10.163010597229004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/172

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [25:30:40<4:14:23, 545.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14798.001816860466
INFO:root:current train perplexity4.311066150665283
INFO:root:current mean train loss 14855.575755299387
INFO:root:current train perplexity4.322319030761719
INFO:root:current mean train loss 14846.53457352752
INFO:root:current train perplexity4.3185224533081055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.48s/it]
INFO:root:final mean train loss: 14831.58314563382
INFO:root:final train perplexity: 4.318345069885254
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.13s/it]
INFO:root:eval mean loss: 22413.280668712796
INFO:root:eval perplexity: 10.172398567199707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/173

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [25:39:34<4:03:45, 541.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14864.828567023027
INFO:root:current train perplexity4.319784164428711
INFO:root:current mean train loss 14850.995017027244
INFO:root:current train perplexity4.318124771118164


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [08:00<00:00, 480.69s/it]
INFO:root:final mean train loss: 14831.138990832913
INFO:root:final train perplexity: 4.318155288696289
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.45s/it]
INFO:root:eval mean loss: 22410.269694010418
INFO:root:eval perplexity: 10.169227600097656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/174

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [25:48:52<3:56:53, 546.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14759.06129488032
INFO:root:current train perplexity4.295780181884766
INFO:root:current mean train loss 14817.38502471301
INFO:root:current train perplexity4.30634880065918
INFO:root:current mean train loss 14839.328219888665
INFO:root:current train perplexity4.316915988922119


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:38<00:00, 458.79s/it]
INFO:root:final mean train loss: 14827.168102633568
INFO:root:final train perplexity: 4.316464424133301
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.27s/it]
INFO:root:eval mean loss: 22406.82882254464
INFO:root:eval perplexity: 10.165605545043945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/175

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [25:57:50<3:46:43, 544.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14820.550406407829
INFO:root:current train perplexity4.309629917144775
INFO:root:current mean train loss 14824.866466119662
INFO:root:current train perplexity4.310774326324463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:43<00:00, 463.68s/it]
INFO:root:final mean train loss: 14822.489698840725
INFO:root:final train perplexity: 4.3144731521606445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.69s/it]
INFO:root:eval mean loss: 22407.011672247023
INFO:root:eval perplexity: 10.165799140930176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/176

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [26:06:52<3:37:27, 543.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14831.253178615196
INFO:root:current train perplexity4.298972129821777
INFO:root:current mean train loss 14820.665976821192
INFO:root:current train perplexity4.306485652923584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.83s/it]
INFO:root:final mean train loss: 14819.854633946572
INFO:root:final train perplexity: 4.313352108001709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it]
INFO:root:eval mean loss: 22413.867001488095
INFO:root:eval perplexity: 10.173014640808105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/177

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [26:15:52<3:27:55, 542.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14792.772135416666
INFO:root:current train perplexity4.27318811416626
INFO:root:current mean train loss 14811.348329414443
INFO:root:current train perplexity4.3090081214904785
INFO:root:current mean train loss 14835.638845058498
INFO:root:current train perplexity4.310858726501465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.32s/it]
INFO:root:final mean train loss: 14815.577372889366
INFO:root:final train perplexity: 4.311532497406006
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.22s/it]
INFO:root:eval mean loss: 22418.39962332589
INFO:root:eval perplexity: 10.177789688110352
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/178

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [26:24:59<3:19:20, 543.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14866.33975497159
INFO:root:current train perplexity4.2997660636901855
INFO:root:current mean train loss 14846.368227066532
INFO:root:current train perplexity4.3109660148620605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:51<00:00, 471.17s/it]
INFO:root:final mean train loss: 14812.996259135585
INFO:root:final train perplexity: 4.3104352951049805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.53s/it]
INFO:root:eval mean loss: 22417.52657645089
INFO:root:eval perplexity: 10.17686939239502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/179

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [26:34:06<3:10:42, 544.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14732.638671875
INFO:root:current train perplexity4.2903571128845215
INFO:root:current mean train loss 14794.043625876167
INFO:root:current train perplexity4.308692455291748
INFO:root:current mean train loss 14823.24607959692
INFO:root:current train perplexity4.30998420715332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.51s/it]
INFO:root:final mean train loss: 14813.792744298134
INFO:root:final train perplexity: 4.310773849487305
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.91s/it]
INFO:root:eval mean loss: 22410.494768415178
INFO:root:eval perplexity: 10.169466972351074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/180

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [26:43:12<3:01:41, 545.05s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14782.827942929025
INFO:root:current train perplexity4.3058671951293945
INFO:root:current mean train loss 14791.297992826258
INFO:root:current train perplexity4.299026012420654


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 466.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:45<00:00, 466.00s/it]
INFO:root:final mean train loss: 14805.89134560862
INFO:root:final train perplexity: 4.30741548538208
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.28s/it]
INFO:root:eval mean loss: 22418.675944010418
INFO:root:eval perplexity: 10.178077697753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/181

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [26:52:30<2:53:49, 548.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14877.336381392046
INFO:root:current train perplexity4.2817535400390625
INFO:root:current mean train loss 14828.010874155405
INFO:root:current train perplexity4.308114051818848
INFO:root:current mean train loss 14813.659286137441
INFO:root:current train perplexity4.305525302886963


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:44<00:00, 464.12s/it]
INFO:root:final mean train loss: 14798.326282132057
INFO:root:final train perplexity: 4.304203033447266
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.85s/it]
INFO:root:eval mean loss: 22420.556315104168
INFO:root:eval perplexity: 10.180058479309082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/182

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [27:02:16<2:48:03, 560.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14751.01089719742
INFO:root:current train perplexity4.294580459594727
INFO:root:current mean train loss 14795.172575968174
INFO:root:current train perplexity4.298162460327148


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:41<00:00, 461.16s/it]
INFO:root:final mean train loss: 14802.813464749244
INFO:root:final train perplexity: 4.306108474731445
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.32s/it]
INFO:root:eval mean loss: 22418.208449590773
INFO:root:eval perplexity: 10.17758560180664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/183

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [27:11:29<2:38:06, 558.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14777.1490234375
INFO:root:current train perplexity4.31842565536499
INFO:root:current mean train loss 14815.387151834238
INFO:root:current train perplexity4.3099470138549805
INFO:root:current mean train loss 14805.840915697674
INFO:root:current train perplexity4.3027167320251465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:47<00:00, 467.06s/it]
INFO:root:final mean train loss: 14794.571663148941
INFO:root:final train perplexity: 4.302608966827393
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.61s/it]
INFO:root:eval mean loss: 22415.085286458332
INFO:root:eval perplexity: 10.174298286437988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/184

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [27:21:19<2:31:21, 567.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14830.815604594216
INFO:root:current train perplexity4.297473907470703
INFO:root:current mean train loss 14798.077575318113
INFO:root:current train perplexity4.296723365783691


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.90s/it]
INFO:root:final mean train loss: 14790.973979334678
INFO:root:final train perplexity: 4.301083087921143
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.23s/it]
INFO:root:eval mean loss: 22418.80245535714
INFO:root:eval perplexity: 10.178214073181152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/185

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [27:30:25<2:20:16, 561.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14739.389956825658
INFO:root:current train perplexity4.282481670379639
INFO:root:current mean train loss 14795.423778886554
INFO:root:current train perplexity4.305079936981201
INFO:root:current mean train loss 14819.176775649257
INFO:root:current train perplexity4.306631565093994


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:46<00:00, 466.22s/it]
INFO:root:final mean train loss: 14796.715745495212
INFO:root:final train perplexity: 4.3035197257995605
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.62s/it]
INFO:root:eval mean loss: 22416.52906436012
INFO:root:eval perplexity: 10.175819396972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/186

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [27:39:35<2:10:10, 557.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14816.189219300177
INFO:root:current train perplexity4.299778461456299
INFO:root:current mean train loss 14822.757292808845
INFO:root:current train perplexity4.30281925201416


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.43s/it]
INFO:root:final mean train loss: 14791.150756835938
INFO:root:final train perplexity: 4.301157474517822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.02s/it]
INFO:root:eval mean loss: 22418.36304873512
INFO:root:eval perplexity: 10.177750587463379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/187

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [27:48:43<2:00:13, 554.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14849.122834578804
INFO:root:current train perplexity4.2902398109436035
INFO:root:current mean train loss 14783.525303290142
INFO:root:current train perplexity4.294670104980469
INFO:root:current mean train loss 14803.746698080156
INFO:root:current train perplexity4.301680088043213


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:49<00:00, 469.67s/it]
INFO:root:final mean train loss: 14788.241825226814
INFO:root:final train perplexity: 4.299923419952393
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.20s/it]
INFO:root:eval mean loss: 22416.900483630954
INFO:root:eval perplexity: 10.176209449768066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/188

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [27:57:52<1:50:36, 553.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14813.360169270833
INFO:root:current train perplexity4.304623603820801
INFO:root:current mean train loss 14781.195345982143
INFO:root:current train perplexity4.295912742614746


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:42<00:00, 462.92s/it]
INFO:root:final mean train loss: 14787.119057932208
INFO:root:final train perplexity: 4.299447536468506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.32s/it]
INFO:root:eval mean loss: 22417.297037760418
INFO:root:eval perplexity: 10.176624298095703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/189

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [28:06:53<1:40:44, 549.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14740.053747106482
INFO:root:current train perplexity4.286189079284668
INFO:root:current mean train loss 14773.943251722441
INFO:root:current train perplexity4.297860145568848
INFO:root:current mean train loss 14791.233824339208
INFO:root:current train perplexity4.2981085777282715


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:36<00:00, 456.77s/it]
INFO:root:final mean train loss: 14788.935141286542
INFO:root:final train perplexity: 4.300218105316162
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.29s/it]
INFO:root:eval mean loss: 22419.20751953125
INFO:root:eval perplexity: 10.178638458251953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/190

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [28:16:00<1:31:25, 548.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14852.11016613924
INFO:root:current train perplexity4.304258823394775
INFO:root:current mean train loss 14787.452252094972
INFO:root:current train perplexity4.294539451599121


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:35<00:00, 455.63s/it]
INFO:root:final mean train loss: 14781.542535597278
INFO:root:final train perplexity: 4.297083377838135
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.94s/it]
INFO:root:eval mean loss: 22418.861188616072
INFO:root:eval perplexity: 10.178275108337402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/191

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [28:24:53<1:21:36, 544.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14902.182837701614
INFO:root:current train perplexity4.306022644042969
INFO:root:current mean train loss 14837.642824129294
INFO:root:current train perplexity4.30327844619751
INFO:root:current mean train loss 14794.09995603355
INFO:root:current train perplexity4.298722743988037


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:40<00:00, 460.39s/it]
INFO:root:final mean train loss: 14788.442367061492
INFO:root:final train perplexity: 4.300008773803711
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.64s/it]
INFO:root:eval mean loss: 22418.643973214286
INFO:root:eval perplexity: 10.178046226501465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/192

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [28:33:50<1:12:15, 541.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14776.74649378765
INFO:root:current train perplexity4.296241283416748
INFO:root:current mean train loss 14782.58400038422
INFO:root:current train perplexity4.296084403991699


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.42s/it]
INFO:root:final mean train loss: 14783.110201927924
INFO:root:final train perplexity: 4.29774808883667
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.49s/it]
INFO:root:eval mean loss: 22420.521577380954
INFO:root:eval perplexity: 10.180022239685059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/193

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [28:43:07<1:03:44, 546.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14786.684235491071
INFO:root:current train perplexity4.316798210144043
INFO:root:current mean train loss 14789.107978877315
INFO:root:current train perplexity4.294671535491943
INFO:root:current mean train loss 14793.993031083777
INFO:root:current train perplexity4.2948808670043945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.98s/it]
INFO:root:final mean train loss: 14778.103846396169
INFO:root:final train perplexity: 4.295626163482666
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:15<00:00, 75.33s/it]
INFO:root:eval mean loss: 22417.92989676339
INFO:root:eval perplexity: 10.177294731140137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/194

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [28:51:57<54:09, 541.58s/it]  

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14741.287962464081
INFO:root:current train perplexity4.2866058349609375
INFO:root:current mean train loss 14776.222604027407
INFO:root:current train perplexity4.293931007385254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:53<00:00, 473.89s/it]
INFO:root:final mean train loss: 14777.946001606602
INFO:root:final train perplexity: 4.295559883117676
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:22<00:00, 82.14s/it]
INFO:root:eval mean loss: 22419.863467261905
INFO:root:eval perplexity: 10.179330825805664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/195

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [29:01:16<45:32, 546.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14847.635817307691
INFO:root:current train perplexity4.310768127441406
INFO:root:current mean train loss 14781.379391018436
INFO:root:current train perplexity4.293997287750244
INFO:root:current mean train loss 14786.072768207374
INFO:root:current train perplexity4.294222831726074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:37<00:00, 457.25s/it]
INFO:root:final mean train loss: 14775.148287865424
INFO:root:final train perplexity: 4.294374465942383
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.74s/it]
INFO:root:eval mean loss: 22421.88548642113
INFO:root:eval perplexity: 10.181461334228516
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/196

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [29:10:12<36:13, 543.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14807.616683121565
INFO:root:current train perplexity4.296966552734375
INFO:root:current mean train loss 14794.647777936845
INFO:root:current train perplexity4.29709529876709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:32<00:00, 452.11s/it]
INFO:root:final mean train loss: 14771.114462575604
INFO:root:final train perplexity: 4.292665958404541
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.67s/it]
INFO:root:eval mean loss: 22419.665969122023
INFO:root:eval perplexity: 10.179123878479004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/197

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [29:19:02<26:58, 539.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14792.836777797966
INFO:root:current train perplexity4.2918219566345215
INFO:root:current mean train loss 14785.657909473339
INFO:root:current train perplexity4.288893699645996
INFO:root:current mean train loss 14790.908050411523
INFO:root:current train perplexity4.29613733291626


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:27<00:00, 447.46s/it]
INFO:root:final mean train loss: 14778.761360414566
INFO:root:final train perplexity: 4.295904636383057
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:16<00:00, 76.12s/it]
INFO:root:eval mean loss: 22421.39474051339
INFO:root:eval perplexity: 10.180944442749023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/198

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [29:27:51<17:52, 536.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14813.976706414474
INFO:root:current train perplexity4.307089328765869
INFO:root:current mean train loss 14796.43341846955
INFO:root:current train perplexity4.298203945159912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:31<00:00, 451.83s/it]
INFO:root:final mean train loss: 14778.98189421623
INFO:root:final train perplexity: 4.295998573303223
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:14<00:00, 74.87s/it]
INFO:root:eval mean loss: 22420.329148065477
INFO:root:eval perplexity: 10.17982292175293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [29:36:40<08:54, 534.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 14755.627451795213
INFO:root:current train perplexity4.281468868255615
INFO:root:current mean train loss 14771.943040497448
INFO:root:current train perplexity4.292463302612305
INFO:root:current mean train loss 14784.825302062247
INFO:root:current train perplexity4.293662071228027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [07:48<00:00, 468.31s/it]
INFO:root:final mean train loss: 14773.24361296623
INFO:root:final train perplexity: 4.293568134307861
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:17<00:00, 77.29s/it]
INFO:root:eval mean loss: 22420.54761904762
INFO:root:eval perplexity: 10.1800537109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_topk_8/200

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [29:45:48<00:00, 538.18s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [29:45:48<00:00, 535.74s/it]
INFO:root:evaluating final model
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.86s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.86s/it]
INFO:root:eval mean loss: 22420.54761904762
INFO:root:eval perplexity: 10.1800537109375
INFO:root:evalaution complete
INFO:root:save model final: small_topk_8/final
