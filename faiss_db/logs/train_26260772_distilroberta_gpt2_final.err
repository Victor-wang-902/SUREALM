INFO:root:Output: large_distilroberta_gpt2_final
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Using pad_token, but it is not set yet.
Some weights of RetrievalGenerationModelGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.9.crossattention.masked_bias', 'h.5.crossattention.bias', 'h.8.crossattention.c_attn_v.weight', 'h.11.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.crossattention.bias', 'h.10.crossattention.c_attn.weight', 'h.7.crossattention.c_attn.weight', 'h.3.crossattention.c_attn_v.weight', 'h.11.ln_cross_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.2.crossattention.masked_bias', 'h.10.crossattention.c_attn_v.bias', 'h.3.crossattention.c_proj.bias', 'h.4.crossattention.bias', 'h.7.crossattention.c_attn_v.bias', 'h.6.crossattention.masked_bias', 'h.6.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.weight', 'h.10.crossattention.c_attn_v.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.c_attn_v.weight', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.8.crossattention.masked_bias', 'h.7.ln_cross_attn.weight', 'h.0.crossattention.c_attn_v.weight', 'h.2.crossattention.c_attn_v.bias', 'h.3.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.0.crossattention.masked_bias', 'h.9.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.11.crossattention.bias', 'h.4.crossattention.c_attn_v.bias', 'h.7.crossattention.bias', 'h.5.crossattention.c_attn_v.weight', 'h.8.crossattention.c_proj.weight', 'h.7.crossattention.c_attn_v.weight', 'h.1.crossattention.c_attn_v.bias', 'h.0.crossattention.c_proj.bias', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.9.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.bias', 'h.9.crossattention.c_attn_v.weight', 'h.2.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.8.crossattention.c_attn_v.bias', 'h.3.crossattention.c_attn.weight', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.bias', 'h.5.crossattention.masked_bias', 'h.3.crossattention.c_attn_v.bias', 'h.4.crossattention.c_proj.weight', 'h.6.crossattention.c_attn_v.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.bias', 'h.5.crossattention.c_attn_v.bias', 'h.1.crossattention.c_attn.weight', 'h.10.crossattention.masked_bias', 'h.11.crossattention.q_attn.weight', 'h.2.crossattention.c_attn_v.weight', 'h.8.crossattention.bias', 'h.4.crossattention.c_attn.weight', 'h.11.crossattention.c_attn_v.weight', 'h.6.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.bias', 'h.9.crossattention.bias', 'h.0.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.6.crossattention.bias', 'h.11.crossattention.c_attn.weight', 'h.9.ln_cross_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.6.ln_cross_attn.weight', 'h.11.crossattention.c_attn_v.bias', 'h.1.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.2.crossattention.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.masked_bias', 'h.10.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.c_attn_v.weight', 'h.0.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.bias', 'h.0.crossattention.c_attn_v.bias', 'h.6.crossattention.c_attn_v.weight', 'h.10.crossattention.bias', 'h.9.crossattention.q_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.4.crossattention.masked_bias', 'h.9.crossattention.c_attn_v.bias', 'h.5.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4691.864987768308
INFO:root:current train perplexity61.32014465332031
INFO:root:current mean train loss 4413.417096468672
INFO:root:current train perplexity48.20191955566406
INFO:root:current mean train loss 4166.290695547659
INFO:root:current train perplexity38.67478561401367
INFO:root:current mean train loss 3963.2774600074404
INFO:root:current train perplexity32.50483322143555
INFO:root:current mean train loss 3805.387732006983
INFO:root:current train perplexity28.316120147705078
INFO:root:current mean train loss 3681.721559451299
INFO:root:current train perplexity25.339723587036133
INFO:root:current mean train loss 3577.6400336837223
INFO:root:current train perplexity23.113136291503906
INFO:root:current mean train loss 3491.4043482086986
INFO:root:current train perplexity21.368600845336914
INFO:root:current mean train loss 3417.361242852301
INFO:root:current train perplexity20.008241653442383
INFO:root:current mean train loss 3351.3938846072633
INFO:root:current train perplexity18.87567138671875
INFO:root:current mean train loss 3292.9332256508046
INFO:root:current train perplexity17.961393356323242
INFO:root:current mean train loss 3242.8678873426425
INFO:root:current train perplexity17.18949317932129
INFO:root:current mean train loss 3198.0087907540055
INFO:root:current train perplexity16.52410316467285
INFO:root:current mean train loss 3154.72592797869
INFO:root:current train perplexity15.936531066894531
INFO:root:current mean train loss 3114.881922909544
INFO:root:current train perplexity15.414676666259766
INFO:root:current mean train loss 3082.7609269343143
INFO:root:current train perplexity14.97952651977539
INFO:root:current mean train loss 3051.923317211526
INFO:root:current train perplexity14.580018043518066
INFO:root:current mean train loss 3024.333829055964
INFO:root:current train perplexity14.22160816192627
INFO:root:current mean train loss 2998.608328177968
INFO:root:current train perplexity13.9006929397583

100%|██████████| 1/1 [05:42<00:00, 342.05s/it][A100%|██████████| 1/1 [05:42<00:00, 342.05s/it]
INFO:root:final mean train loss: 2977.3439750575203
INFO:root:final train perplexity: 13.649186134338379
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.13s/it][A100%|██████████| 1/1 [00:23<00:00, 23.13s/it]
INFO:root:eval mean loss: 2299.549824166805
INFO:root:eval perplexity: 7.9701104164123535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.87s/it][A100%|██████████| 1/1 [00:22<00:00, 22.87s/it]
INFO:root:eval mean loss: 2503.9870891165224
INFO:root:eval perplexity: 9.977216720581055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/1
  0%|          | 1/200 [06:48<22:36:13, 408.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2483.681915283203
INFO:root:current train perplexity8.910066604614258
INFO:root:current mean train loss 2451.0380501582704
INFO:root:current train perplexity8.600947380065918
INFO:root:current mean train loss 2449.6075891565392
INFO:root:current train perplexity8.600194931030273
INFO:root:current mean train loss 2445.8817185027688
INFO:root:current train perplexity8.585051536560059
INFO:root:current mean train loss 2443.4095632112944
INFO:root:current train perplexity8.524420738220215
INFO:root:current mean train loss 2439.024206826853
INFO:root:current train perplexity8.489618301391602
INFO:root:current mean train loss 2429.3602901310114
INFO:root:current train perplexity8.437185287475586
INFO:root:current mean train loss 2420.5809568266627
INFO:root:current train perplexity8.3942289352417
INFO:root:current mean train loss 2413.631561129701
INFO:root:current train perplexity8.344035148620605
INFO:root:current mean train loss 2407.908536552862
INFO:root:current train perplexity8.297188758850098
INFO:root:current mean train loss 2401.7381311852164
INFO:root:current train perplexity8.253236770629883
INFO:root:current mean train loss 2396.4613207745297
INFO:root:current train perplexity8.211536407470703
INFO:root:current mean train loss 2391.588506899382
INFO:root:current train perplexity8.179298400878906
INFO:root:current mean train loss 2386.712755754001
INFO:root:current train perplexity8.142403602600098
INFO:root:current mean train loss 2382.901059597899
INFO:root:current train perplexity8.106616973876953
INFO:root:current mean train loss 2379.0929956763275
INFO:root:current train perplexity8.071913719177246
INFO:root:current mean train loss 2375.422083939656
INFO:root:current train perplexity8.042564392089844
INFO:root:current mean train loss 2370.7821923458214
INFO:root:current train perplexity8.008957862854004
INFO:root:current mean train loss 2365.4607540584348
INFO:root:current train perplexity7.9728779792785645
INFO:root:current mean train loss 2361.895194417997
INFO:root:current train perplexity7.948418617248535

100%|██████████| 1/1 [05:50<00:00, 350.82s/it][A100%|██████████| 1/1 [05:50<00:00, 350.82s/it]
INFO:root:final mean train loss: 2358.680820667076
INFO:root:final train perplexity: 7.929443359375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.98s/it][A100%|██████████| 1/1 [00:23<00:00, 23.98s/it]
INFO:root:eval mean loss: 2109.381436395307
INFO:root:eval perplexity: 6.712973594665527
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.69s/it][A100%|██████████| 1/1 [00:23<00:00, 23.69s/it]
INFO:root:eval mean loss: 2354.5805958416445
INFO:root:eval perplexity: 8.697632789611816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/2
  1%|          | 2/200 [13:51<22:55:49, 416.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2262.124004941998
INFO:root:current train perplexity7.340695381164551
INFO:root:current mean train loss 2259.822163746769
INFO:root:current train perplexity7.27559232711792
INFO:root:current mean train loss 2249.892796070279
INFO:root:current train perplexity7.237120151519775
INFO:root:current mean train loss 2260.243851028763
INFO:root:current train perplexity7.224564552307129
INFO:root:current mean train loss 2252.783226524159
INFO:root:current train perplexity7.194833278656006
INFO:root:current mean train loss 2249.286739041613
INFO:root:current train perplexity7.160324573516846
INFO:root:current mean train loss 2248.804620968787
INFO:root:current train perplexity7.149770259857178
INFO:root:current mean train loss 2240.980827633388
INFO:root:current train perplexity7.127685070037842
INFO:root:current mean train loss 2236.333755767932
INFO:root:current train perplexity7.11818790435791
INFO:root:current mean train loss 2232.202890541265
INFO:root:current train perplexity7.109375
INFO:root:current mean train loss 2231.7551374703157
INFO:root:current train perplexity7.09664249420166
INFO:root:current mean train loss 2228.0856262368643
INFO:root:current train perplexity7.076069355010986
INFO:root:current mean train loss 2226.414503457966
INFO:root:current train perplexity7.065794944763184
INFO:root:current mean train loss 2223.828563280957
INFO:root:current train perplexity7.048864364624023
INFO:root:current mean train loss 2219.990424848984
INFO:root:current train perplexity7.024518013000488
INFO:root:current mean train loss 2217.300545788869
INFO:root:current train perplexity7.0205159187316895
INFO:root:current mean train loss 2216.1379893875824
INFO:root:current train perplexity7.006371974945068
INFO:root:current mean train loss 2214.8965501647704
INFO:root:current train perplexity6.995884418487549
INFO:root:current mean train loss 2213.3949589289673
INFO:root:current train perplexity6.982725143432617
INFO:root:current mean train loss 2211.748437146356
INFO:root:current train perplexity6.968221187591553

100%|██████████| 1/1 [05:45<00:00, 345.38s/it][A100%|██████████| 1/1 [05:45<00:00, 345.38s/it]
INFO:root:final mean train loss: 2210.440253291897
INFO:root:final train perplexity: 6.961874485015869
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.95s/it][A100%|██████████| 1/1 [00:23<00:00, 23.95s/it]
INFO:root:eval mean loss: 2023.9882479187445
INFO:root:eval perplexity: 6.2149739265441895
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.80s/it][A100%|██████████| 1/1 [00:23<00:00, 23.80s/it]
INFO:root:eval mean loss: 2294.8699089753713
INFO:root:eval perplexity: 8.23338508605957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/3
  2%|▏         | 3/200 [21:47<24:16:53, 443.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2153.4526049804685
INFO:root:current train perplexity6.523248195648193
INFO:root:current mean train loss 2157.2973364257814
INFO:root:current train perplexity6.565609455108643
INFO:root:current mean train loss 2156.927547363281
INFO:root:current train perplexity6.5838236808776855
INFO:root:current mean train loss 2156.8613176618305
INFO:root:current train perplexity6.580167770385742
INFO:root:current mean train loss 2156.2270309787327
INFO:root:current train perplexity6.574970722198486
INFO:root:current mean train loss 2152.145226162997
INFO:root:current train perplexity6.568903923034668
INFO:root:current mean train loss 2149.455005821815
INFO:root:current train perplexity6.559123516082764
INFO:root:current mean train loss 2147.6138834635417
INFO:root:current train perplexity6.551260948181152
INFO:root:current mean train loss 2148.771229319853
INFO:root:current train perplexity6.547358512878418
INFO:root:current mean train loss 2145.5453018349094
INFO:root:current train perplexity6.531909465789795
INFO:root:current mean train loss 2142.3502153087798
INFO:root:current train perplexity6.523582935333252
INFO:root:current mean train loss 2140.6178261931045
INFO:root:current train perplexity6.52178955078125
INFO:root:current mean train loss 2138.3522265625
INFO:root:current train perplexity6.512033939361572
INFO:root:current mean train loss 2135.7402227105035
INFO:root:current train perplexity6.503708362579346
INFO:root:current mean train loss 2132.818348262392
INFO:root:current train perplexity6.494484901428223
INFO:root:current mean train loss 2131.793295425907
INFO:root:current train perplexity6.495661735534668
INFO:root:current mean train loss 2130.1402561996924
INFO:root:current train perplexity6.483829021453857
INFO:root:current mean train loss 2129.88463671875
INFO:root:current train perplexity6.476578235626221
INFO:root:current mean train loss 2127.487786436339
INFO:root:current train perplexity6.469730377197266
INFO:root:current mean train loss 2126.6953629557293
INFO:root:current train perplexity6.462425231933594

100%|██████████| 1/1 [05:45<00:00, 345.10s/it][A100%|██████████| 1/1 [05:45<00:00, 345.10s/it]
INFO:root:final mean train loss: 2125.051872064895
INFO:root:final train perplexity: 6.459098815917969
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.63s/it][A100%|██████████| 1/1 [00:23<00:00, 23.63s/it]
INFO:root:eval mean loss: 1972.1140868274879
INFO:root:eval perplexity: 5.93066930770874
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.29s/it][A100%|██████████| 1/1 [00:22<00:00, 22.29s/it]
INFO:root:eval mean loss: 2258.109372402759
INFO:root:eval perplexity: 7.959985256195068
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/4
  2%|▏         | 4/200 [28:20<23:04:06, 423.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2070.6850330865204
INFO:root:current train perplexity6.221717357635498
INFO:root:current mean train loss 2085.073481211405
INFO:root:current train perplexity6.192401885986328
INFO:root:current mean train loss 2091.6361710337665
INFO:root:current train perplexity6.202027320861816
INFO:root:current mean train loss 2086.535172548216
INFO:root:current train perplexity6.211026191711426
INFO:root:current mean train loss 2089.832783276315
INFO:root:current train perplexity6.213699817657471
INFO:root:current mean train loss 2082.2857762896824
INFO:root:current train perplexity6.19704008102417
INFO:root:current mean train loss 2081.902261759745
INFO:root:current train perplexity6.1847662925720215
INFO:root:current mean train loss 2080.754518988857
INFO:root:current train perplexity6.1840715408325195
INFO:root:current mean train loss 2075.8031770382786
INFO:root:current train perplexity6.168247222900391
INFO:root:current mean train loss 2075.489512430722
INFO:root:current train perplexity6.1690826416015625
INFO:root:current mean train loss 2074.85573359796
INFO:root:current train perplexity6.165858745574951
INFO:root:current mean train loss 2073.786426534383
INFO:root:current train perplexity6.1674275398254395
INFO:root:current mean train loss 2071.639393987859
INFO:root:current train perplexity6.1614837646484375
INFO:root:current mean train loss 2070.4045161014938
INFO:root:current train perplexity6.155482292175293
INFO:root:current mean train loss 2070.698967037188
INFO:root:current train perplexity6.151587009429932
INFO:root:current mean train loss 2071.051177686388
INFO:root:current train perplexity6.1523966789245605
INFO:root:current mean train loss 2069.654632476825
INFO:root:current train perplexity6.14615535736084
INFO:root:current mean train loss 2069.597351523261
INFO:root:current train perplexity6.145047187805176
INFO:root:current mean train loss 2069.5536971416586
INFO:root:current train perplexity6.143394947052002
INFO:root:current mean train loss 2067.313825583058
INFO:root:current train perplexity6.134851455688477

100%|██████████| 1/1 [05:41<00:00, 341.11s/it][A100%|██████████| 1/1 [05:41<00:00, 341.11s/it]
INFO:root:final mean train loss: 2066.2330492474607
INFO:root:final train perplexity: 6.134052753448486
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.91s/it][A100%|██████████| 1/1 [00:23<00:00, 23.91s/it]
INFO:root:eval mean loss: 1939.5765822390292
INFO:root:eval perplexity: 5.7590179443359375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.30s/it][A100%|██████████| 1/1 [00:22<00:00, 22.30s/it]
INFO:root:eval mean loss: 2242.85817377618
INFO:root:eval perplexity: 7.849236011505127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/5
  2%|▎         | 5/200 [34:49<22:16:40, 411.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2012.2233712332588
INFO:root:current train perplexity5.961361885070801
INFO:root:current mean train loss 2025.5306582243545
INFO:root:current train perplexity5.993882179260254
INFO:root:current mean train loss 2035.7141620474802
INFO:root:current train perplexity5.996592044830322
INFO:root:current mean train loss 2032.5590750376384
INFO:root:current train perplexity5.985030651092529
INFO:root:current mean train loss 2032.6720502869157
INFO:root:current train perplexity5.97517728805542
INFO:root:current mean train loss 2032.1730252618659
INFO:root:current train perplexity5.974721431732178
INFO:root:current mean train loss 2033.8510089004249
INFO:root:current train perplexity5.966275691986084
INFO:root:current mean train loss 2031.6260020976165
INFO:root:current train perplexity5.959115028381348
INFO:root:current mean train loss 2028.7312359701994
INFO:root:current train perplexity5.939366817474365
INFO:root:current mean train loss 2027.3166023812644
INFO:root:current train perplexity5.934020519256592
INFO:root:current mean train loss 2027.9775787015683
INFO:root:current train perplexity5.934248924255371
INFO:root:current mean train loss 2026.1096031601364
INFO:root:current train perplexity5.925148010253906
INFO:root:current mean train loss 2025.308868693414
INFO:root:current train perplexity5.917386531829834
INFO:root:current mean train loss 2024.7723064091854
INFO:root:current train perplexity5.916774272918701
INFO:root:current mean train loss 2024.50739619417
INFO:root:current train perplexity5.915284633636475
INFO:root:current mean train loss 2022.8623688823045
INFO:root:current train perplexity5.906938076019287
INFO:root:current mean train loss 2021.8645156534153
INFO:root:current train perplexity5.901293754577637
INFO:root:current mean train loss 2020.4597311661382
INFO:root:current train perplexity5.894472599029541
INFO:root:current mean train loss 2020.2769137825935
INFO:root:current train perplexity5.890923976898193

100%|██████████| 1/1 [05:42<00:00, 342.23s/it][A100%|██████████| 1/1 [05:42<00:00, 342.23s/it]
INFO:root:final mean train loss: 2019.8416955806483
INFO:root:final train perplexity: 5.889261245727539
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.77s/it][A100%|██████████| 1/1 [00:22<00:00, 22.77s/it]
INFO:root:eval mean loss: 1913.3166737657912
INFO:root:eval perplexity: 5.624113082885742
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.42s/it][A100%|██████████| 1/1 [00:22<00:00, 22.42s/it]
INFO:root:eval mean loss: 2225.7231904158357
INFO:root:eval perplexity: 7.726647853851318
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/6
  3%|▎         | 6/200 [41:18<21:45:43, 403.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1858.3189697265625
INFO:root:current train perplexity5.032563209533691
INFO:root:current mean train loss 1977.8541537747524
INFO:root:current train perplexity5.785965442657471
INFO:root:current mean train loss 1978.6301980089786
INFO:root:current train perplexity5.728094577789307
INFO:root:current mean train loss 1990.6640831830098
INFO:root:current train perplexity5.748645305633545
INFO:root:current mean train loss 1989.5378795443032
INFO:root:current train perplexity5.749623775482178
INFO:root:current mean train loss 1991.229074322059
INFO:root:current train perplexity5.7465314865112305
INFO:root:current mean train loss 1990.2035300053296
INFO:root:current train perplexity5.74556827545166
INFO:root:current mean train loss 1989.4826374570926
INFO:root:current train perplexity5.736352443695068
INFO:root:current mean train loss 1990.183735784371
INFO:root:current train perplexity5.733466625213623
INFO:root:current mean train loss 1991.6373567401238
INFO:root:current train perplexity5.7334885597229
INFO:root:current mean train loss 1991.5295997947364
INFO:root:current train perplexity5.7286834716796875
INFO:root:current mean train loss 1989.3400428765044
INFO:root:current train perplexity5.727903366088867
INFO:root:current mean train loss 1989.2905318159346
INFO:root:current train perplexity5.721071720123291
INFO:root:current mean train loss 1988.7388950732009
INFO:root:current train perplexity5.714467525482178
INFO:root:current mean train loss 1985.9185852878525
INFO:root:current train perplexity5.711915016174316
INFO:root:current mean train loss 1985.4141840823565
INFO:root:current train perplexity5.712319374084473
INFO:root:current mean train loss 1986.490720292615
INFO:root:current train perplexity5.713199138641357
INFO:root:current mean train loss 1985.3371484432412
INFO:root:current train perplexity5.710498809814453
INFO:root:current mean train loss 1985.393386324004
INFO:root:current train perplexity5.707387924194336
INFO:root:current mean train loss 1984.3231554347424
INFO:root:current train perplexity5.702676773071289

100%|██████████| 1/1 [05:48<00:00, 348.39s/it][A100%|██████████| 1/1 [05:48<00:00, 348.39s/it]
INFO:root:final mean train loss: 1983.0857264777956
INFO:root:final train perplexity: 5.702269077301025
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.05s/it][A100%|██████████| 1/1 [00:24<00:00, 24.05s/it]
INFO:root:eval mean loss: 1891.867263685727
INFO:root:eval perplexity: 5.516270160675049
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.09s/it][A100%|██████████| 1/1 [00:23<00:00, 23.09s/it]
INFO:root:eval mean loss: 2217.718537026263
INFO:root:eval perplexity: 7.670039653778076
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/7
  4%|▎         | 7/200 [47:56<21:32:20, 401.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.4502292209202
INFO:root:current train perplexity5.573057174682617
INFO:root:current mean train loss 1954.0961458885063
INFO:root:current train perplexity5.587186813354492
INFO:root:current mean train loss 1948.7276622527236
INFO:root:current train perplexity5.545426845550537
INFO:root:current mean train loss 1958.7188528768672
INFO:root:current train perplexity5.556361675262451
INFO:root:current mean train loss 1958.7574165015699
INFO:root:current train perplexity5.5601396560668945
INFO:root:current mean train loss 1959.8574176331745
INFO:root:current train perplexity5.572613716125488
INFO:root:current mean train loss 1961.9901142799354
INFO:root:current train perplexity5.584264278411865
INFO:root:current mean train loss 1958.6890731428991
INFO:root:current train perplexity5.576515197753906
INFO:root:current mean train loss 1959.1641038367684
INFO:root:current train perplexity5.576181411743164
INFO:root:current mean train loss 1956.775688221252
INFO:root:current train perplexity5.564639568328857
INFO:root:current mean train loss 1956.183093717384
INFO:root:current train perplexity5.564269542694092
INFO:root:current mean train loss 1953.8858979963873
INFO:root:current train perplexity5.561967372894287
INFO:root:current mean train loss 1952.9581166535174
INFO:root:current train perplexity5.557973384857178
INFO:root:current mean train loss 1952.7448623032055
INFO:root:current train perplexity5.554961681365967
INFO:root:current mean train loss 1952.9994653182575
INFO:root:current train perplexity5.556070327758789
INFO:root:current mean train loss 1952.914875819592
INFO:root:current train perplexity5.554762363433838
INFO:root:current mean train loss 1952.349596096647
INFO:root:current train perplexity5.550825595855713
INFO:root:current mean train loss 1952.0315259016434
INFO:root:current train perplexity5.551477432250977
INFO:root:current mean train loss 1952.628586369379
INFO:root:current train perplexity5.553534030914307
INFO:root:current mean train loss 1952.775898190559
INFO:root:current train perplexity5.55283784866333

100%|██████████| 1/1 [05:39<00:00, 339.37s/it][A100%|██████████| 1/1 [05:39<00:00, 339.37s/it]
INFO:root:final mean train loss: 1952.4165912576234
INFO:root:final train perplexity: 5.550795078277588
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.83s/it][A100%|██████████| 1/1 [00:24<00:00, 24.83s/it]
INFO:root:eval mean loss: 1876.854189176086
INFO:root:eval perplexity: 5.442019939422607
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.59s/it][A100%|██████████| 1/1 [00:23<00:00, 23.59s/it]
INFO:root:eval mean loss: 2210.38304971465
INFO:root:eval perplexity: 7.618525981903076
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/8
  4%|▍         | 8/200 [54:25<21:13:25, 397.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1941.5081019810268
INFO:root:current train perplexity5.454519271850586
INFO:root:current mean train loss 1944.7901095920138
INFO:root:current train perplexity5.440501689910889
INFO:root:current mean train loss 1931.6644011801861
INFO:root:current train perplexity5.424471378326416
INFO:root:current mean train loss 1929.0483405725279
INFO:root:current train perplexity5.408613681793213
INFO:root:current mean train loss 1933.9645852976832
INFO:root:current train perplexity5.424004077911377
INFO:root:current mean train loss 1934.7623539719625
INFO:root:current train perplexity5.431877613067627
INFO:root:current mean train loss 1931.77096745048
INFO:root:current train perplexity5.426782608032227
INFO:root:current mean train loss 1927.9521462784332
INFO:root:current train perplexity5.41880464553833
INFO:root:current mean train loss 1929.1944292079902
INFO:root:current train perplexity5.424076557159424
INFO:root:current mean train loss 1928.9022032712232
INFO:root:current train perplexity5.420601844787598
INFO:root:current mean train loss 1927.6448667959314
INFO:root:current train perplexity5.417525768280029
INFO:root:current mean train loss 1924.8771344558784
INFO:root:current train perplexity5.41480827331543
INFO:root:current mean train loss 1927.509929999842
INFO:root:current train perplexity5.424706935882568
INFO:root:current mean train loss 1926.8361729539736
INFO:root:current train perplexity5.419707775115967
INFO:root:current mean train loss 1927.0470164654562
INFO:root:current train perplexity5.422989368438721
INFO:root:current mean train loss 1926.2017783298554
INFO:root:current train perplexity5.420550346374512
INFO:root:current mean train loss 1925.684607269567
INFO:root:current train perplexity5.421922206878662
INFO:root:current mean train loss 1926.1658079999324
INFO:root:current train perplexity5.42221212387085
INFO:root:current mean train loss 1926.725349646628
INFO:root:current train perplexity5.422898292541504
INFO:root:current mean train loss 1926.7660223120558
INFO:root:current train perplexity5.4227495193481445

100%|██████████| 1/1 [05:37<00:00, 337.31s/it][A100%|██████████| 1/1 [05:37<00:00, 337.31s/it]
INFO:root:final mean train loss: 1925.4273574529004
INFO:root:final train perplexity: 5.420827388763428
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.08s/it][A100%|██████████| 1/1 [00:24<00:00, 24.08s/it]
INFO:root:eval mean loss: 1862.5890134987255
INFO:root:eval perplexity: 5.372395038604736
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.24s/it][A100%|██████████| 1/1 [00:22<00:00, 22.24s/it]
INFO:root:eval mean loss: 2201.1670640444927
INFO:root:eval perplexity: 7.554297924041748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/9
  4%|▍         | 9/200 [1:00:51<20:54:29, 394.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1893.0787423940806
INFO:root:current train perplexity5.292259216308594
INFO:root:current mean train loss 1910.0338022332442
INFO:root:current train perplexity5.31387186050415
INFO:root:current mean train loss 1914.6792946467324
INFO:root:current train perplexity5.320400714874268
INFO:root:current mean train loss 1914.4102862964976
INFO:root:current train perplexity5.328765869140625
INFO:root:current mean train loss 1915.8948885487243
INFO:root:current train perplexity5.343949317932129
INFO:root:current mean train loss 1910.0950275365858
INFO:root:current train perplexity5.330854415893555
INFO:root:current mean train loss 1910.1056834963933
INFO:root:current train perplexity5.332258701324463
INFO:root:current mean train loss 1910.2329526860663
INFO:root:current train perplexity5.330873966217041
INFO:root:current mean train loss 1907.6401304146493
INFO:root:current train perplexity5.329247951507568
INFO:root:current mean train loss 1906.7353339956587
INFO:root:current train perplexity5.326611042022705
INFO:root:current mean train loss 1905.3815190420405
INFO:root:current train perplexity5.326506614685059
INFO:root:current mean train loss 1903.961293856303
INFO:root:current train perplexity5.323937892913818
INFO:root:current mean train loss 1904.2575590968513
INFO:root:current train perplexity5.320308685302734
INFO:root:current mean train loss 1903.1138146755964
INFO:root:current train perplexity5.316827774047852
INFO:root:current mean train loss 1904.174707249833
INFO:root:current train perplexity5.319067001342773
INFO:root:current mean train loss 1905.1580599755357
INFO:root:current train perplexity5.321470737457275
INFO:root:current mean train loss 1906.175809920267
INFO:root:current train perplexity5.3188252449035645
INFO:root:current mean train loss 1905.162201485133
INFO:root:current train perplexity5.31456995010376
INFO:root:current mean train loss 1903.225412850761
INFO:root:current train perplexity5.312852382659912
INFO:root:current mean train loss 1903.0677454588842
INFO:root:current train perplexity5.312762260437012

100%|██████████| 1/1 [05:43<00:00, 343.10s/it][A100%|██████████| 1/1 [05:43<00:00, 343.10s/it]
INFO:root:final mean train loss: 1902.187146254639
INFO:root:final train perplexity: 5.31135368347168
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.87s/it][A100%|██████████| 1/1 [00:24<00:00, 24.87s/it]
INFO:root:eval mean loss: 1851.7556628504544
INFO:root:eval perplexity: 5.32011604309082
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.83s/it][A100%|██████████| 1/1 [00:22<00:00, 22.83s/it]
INFO:root:eval mean loss: 2198.1000171417886
INFO:root:eval perplexity: 7.5330424308776855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/10
  5%|▌         | 10/200 [1:07:24<20:46:38, 393.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1848.4913170855978
INFO:root:current train perplexity5.138693332672119
INFO:root:current mean train loss 1854.731689453125
INFO:root:current train perplexity5.154232978820801
INFO:root:current mean train loss 1868.6492901770155
INFO:root:current train perplexity5.184288024902344
INFO:root:current mean train loss 1878.0727893033325
INFO:root:current train perplexity5.199512958526611
INFO:root:current mean train loss 1876.9794882833323
INFO:root:current train perplexity5.199843883514404
INFO:root:current mean train loss 1879.4808036388538
INFO:root:current train perplexity5.1993584632873535
INFO:root:current mean train loss 1881.15110315799
INFO:root:current train perplexity5.201706409454346
INFO:root:current mean train loss 1881.4958875480027
INFO:root:current train perplexity5.204092979431152
INFO:root:current mean train loss 1880.5516534416804
INFO:root:current train perplexity5.207464694976807
INFO:root:current mean train loss 1880.0956649544069
INFO:root:current train perplexity5.211130142211914
INFO:root:current mean train loss 1879.2874243141225
INFO:root:current train perplexity5.2127766609191895
INFO:root:current mean train loss 1881.446151955297
INFO:root:current train perplexity5.211903095245361
INFO:root:current mean train loss 1882.7449968486812
INFO:root:current train perplexity5.215158939361572
INFO:root:current mean train loss 1883.4148897960988
INFO:root:current train perplexity5.2173752784729
INFO:root:current mean train loss 1884.3764967532122
INFO:root:current train perplexity5.219756126403809
INFO:root:current mean train loss 1883.6142819309173
INFO:root:current train perplexity5.218273639678955
INFO:root:current mean train loss 1883.3277514750832
INFO:root:current train perplexity5.217101097106934
INFO:root:current mean train loss 1883.5194363677617
INFO:root:current train perplexity5.21884298324585
INFO:root:current mean train loss 1883.2106366675446
INFO:root:current train perplexity5.219842433929443
INFO:root:current mean train loss 1882.1516520595599
INFO:root:current train perplexity5.217857837677002

100%|██████████| 1/1 [05:45<00:00, 345.40s/it][A100%|██████████| 1/1 [05:45<00:00, 345.40s/it]
INFO:root:final mean train loss: 1881.6347240730781
INFO:root:final train perplexity: 5.216385364532471
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.87s/it][A100%|██████████| 1/1 [00:24<00:00, 24.87s/it]
INFO:root:eval mean loss: 1844.323756441157
INFO:root:eval perplexity: 5.284544467926025
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.97s/it][A100%|██████████| 1/1 [00:22<00:00, 22.97s/it]
INFO:root:eval mean loss: 2197.840503882009
INFO:root:eval perplexity: 7.531246662139893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/11
  6%|▌         | 11/200 [1:17:26<24:00:39, 457.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.7113221634265
INFO:root:current train perplexity5.124941825866699
INFO:root:current mean train loss 1862.6287310200353
INFO:root:current train perplexity5.136647701263428
INFO:root:current mean train loss 1867.7008052372432
INFO:root:current train perplexity5.138146877288818
INFO:root:current mean train loss 1867.6978671217212
INFO:root:current train perplexity5.129964351654053
INFO:root:current mean train loss 1872.1956031077193
INFO:root:current train perplexity5.140665054321289
INFO:root:current mean train loss 1873.1525708091137
INFO:root:current train perplexity5.1396636962890625
INFO:root:current mean train loss 1870.8503702680848
INFO:root:current train perplexity5.137058258056641
INFO:root:current mean train loss 1868.4377790844167
INFO:root:current train perplexity5.132909774780273
INFO:root:current mean train loss 1866.3408617833397
INFO:root:current train perplexity5.137263298034668
INFO:root:current mean train loss 1865.77184476717
INFO:root:current train perplexity5.135322093963623
INFO:root:current mean train loss 1865.3561654275293
INFO:root:current train perplexity5.1352128982543945
INFO:root:current mean train loss 1865.963013518721
INFO:root:current train perplexity5.139551162719727
INFO:root:current mean train loss 1865.287279950701
INFO:root:current train perplexity5.139387130737305
INFO:root:current mean train loss 1864.8342156568474
INFO:root:current train perplexity5.1352105140686035
INFO:root:current mean train loss 1864.1356258674714
INFO:root:current train perplexity5.13093900680542
INFO:root:current mean train loss 1864.9803946303841
INFO:root:current train perplexity5.134089469909668
INFO:root:current mean train loss 1864.4517383507562
INFO:root:current train perplexity5.130845069885254
INFO:root:current mean train loss 1865.0308730583574
INFO:root:current train perplexity5.1318769454956055
INFO:root:current mean train loss 1863.7109476617386
INFO:root:current train perplexity5.1294965744018555

100%|██████████| 1/1 [05:35<00:00, 335.69s/it][A100%|██████████| 1/1 [05:35<00:00, 335.70s/it]
INFO:root:final mean train loss: 1862.1468562185314
INFO:root:final train perplexity: 5.127904891967773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.52s/it][A100%|██████████| 1/1 [00:24<00:00, 24.52s/it]
INFO:root:eval mean loss: 1834.5067091055796
INFO:root:eval perplexity: 5.237923622131348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.70s/it][A100%|██████████| 1/1 [00:23<00:00, 23.70s/it]
INFO:root:eval mean loss: 2189.4576446316764
INFO:root:eval perplexity: 7.473474025726318
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/12
  6%|▌         | 12/200 [1:23:52<22:45:00, 435.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1875.1929931640625
INFO:root:current train perplexity4.79650354385376
INFO:root:current mean train loss 1864.6142317392294
INFO:root:current train perplexity5.086803913116455
INFO:root:current mean train loss 1855.4102656923492
INFO:root:current train perplexity5.0588698387146
INFO:root:current mean train loss 1848.162407903388
INFO:root:current train perplexity5.056783199310303
INFO:root:current mean train loss 1847.9234364701265
INFO:root:current train perplexity5.052814483642578
INFO:root:current mean train loss 1850.9566686793303
INFO:root:current train perplexity5.061961650848389
INFO:root:current mean train loss 1847.5881904361656
INFO:root:current train perplexity5.054429054260254
INFO:root:current mean train loss 1847.5769060332948
INFO:root:current train perplexity5.052826404571533
INFO:root:current mean train loss 1845.4239416823143
INFO:root:current train perplexity5.045237064361572
INFO:root:current mean train loss 1846.4782649955876
INFO:root:current train perplexity5.04988431930542
INFO:root:current mean train loss 1848.0348524738286
INFO:root:current train perplexity5.04612922668457
INFO:root:current mean train loss 1845.5087610626913
INFO:root:current train perplexity5.044973373413086
INFO:root:current mean train loss 1845.3475162192176
INFO:root:current train perplexity5.047335147857666
INFO:root:current mean train loss 1846.262447424513
INFO:root:current train perplexity5.0531487464904785
INFO:root:current mean train loss 1846.087416960865
INFO:root:current train perplexity5.052443981170654
INFO:root:current mean train loss 1846.2199021553247
INFO:root:current train perplexity5.055451393127441
INFO:root:current mean train loss 1846.4888765236324
INFO:root:current train perplexity5.057009696960449
INFO:root:current mean train loss 1846.02032660654
INFO:root:current train perplexity5.053244113922119
INFO:root:current mean train loss 1846.4113024787248
INFO:root:current train perplexity5.056644439697266
INFO:root:current mean train loss 1845.9798977200883
INFO:root:current train perplexity5.056205749511719

100%|██████████| 1/1 [05:39<00:00, 339.22s/it][A100%|██████████| 1/1 [05:39<00:00, 339.22s/it]
INFO:root:final mean train loss: 1845.73180795305
INFO:root:final train perplexity: 5.05454158782959
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.49s/it][A100%|██████████| 1/1 [00:24<00:00, 24.49s/it]
INFO:root:eval mean loss: 1826.9559386774158
INFO:root:eval perplexity: 5.2023444175720215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.15s/it][A100%|██████████| 1/1 [00:23<00:00, 23.15s/it]
INFO:root:eval mean loss: 2186.27981026291
INFO:root:eval perplexity: 7.451685428619385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/13
  6%|▋         | 13/200 [1:30:20<21:53:30, 421.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1831.2576110839843
INFO:root:current train perplexity4.962650299072266
INFO:root:current mean train loss 1834.9222819010417
INFO:root:current train perplexity4.930641174316406
INFO:root:current mean train loss 1829.6735984108666
INFO:root:current train perplexity4.944108486175537
INFO:root:current mean train loss 1832.0695152282715
INFO:root:current train perplexity4.9531168937683105
INFO:root:current mean train loss 1833.617057001023
INFO:root:current train perplexity4.964463710784912
INFO:root:current mean train loss 1830.408164860652
INFO:root:current train perplexity4.972630977630615
INFO:root:current mean train loss 1830.368283376386
INFO:root:current train perplexity4.972185134887695
INFO:root:current mean train loss 1829.0413079155817
INFO:root:current train perplexity4.967469692230225
INFO:root:current mean train loss 1831.01460020484
INFO:root:current train perplexity4.971877098083496
INFO:root:current mean train loss 1831.789891649329
INFO:root:current train perplexity4.976596355438232
INFO:root:current mean train loss 1830.0896569345512
INFO:root:current train perplexity4.978592395782471
INFO:root:current mean train loss 1829.5896072387695
INFO:root:current train perplexity4.981435775756836
INFO:root:current mean train loss 1829.151660956711
INFO:root:current train perplexity4.97902774810791
INFO:root:current mean train loss 1829.363597800515
INFO:root:current train perplexity4.980746746063232
INFO:root:current mean train loss 1829.6344179825044
INFO:root:current train perplexity4.979592323303223
INFO:root:current mean train loss 1828.9728769402755
INFO:root:current train perplexity4.976569175720215
INFO:root:current mean train loss 1829.5101584352094
INFO:root:current train perplexity4.977298736572266
INFO:root:current mean train loss 1831.2310108273527
INFO:root:current train perplexity4.982447147369385
INFO:root:current mean train loss 1829.730436555632
INFO:root:current train perplexity4.982370853424072
INFO:root:current mean train loss 1829.1579291025798
INFO:root:current train perplexity4.980653285980225

100%|██████████| 1/1 [05:42<00:00, 342.31s/it][A100%|██████████| 1/1 [05:42<00:00, 342.31s/it]
INFO:root:final mean train loss: 1829.2854819862878
INFO:root:final train perplexity: 4.982090473175049
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:25<00:00, 25.10s/it][A100%|██████████| 1/1 [00:25<00:00, 25.10s/it]
INFO:root:eval mean loss: 1821.0461031866412
INFO:root:eval perplexity: 5.174666881561279
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.71s/it][A100%|██████████| 1/1 [00:22<00:00, 22.71s/it]
INFO:root:eval mean loss: 2185.1005552034853
INFO:root:eval perplexity: 7.44361686706543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/14
  7%|▋         | 14/200 [1:36:52<21:19:01, 412.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.8280458192567
INFO:root:current train perplexity5.043003559112549
INFO:root:current mean train loss 1829.2143697251367
INFO:root:current train perplexity4.97042989730835
INFO:root:current mean train loss 1824.956401841047
INFO:root:current train perplexity4.935386657714844
INFO:root:current mean train loss 1821.0320997832437
INFO:root:current train perplexity4.9073710441589355
INFO:root:current mean train loss 1819.052865942774
INFO:root:current train perplexity4.917289733886719
INFO:root:current mean train loss 1819.8814165339124
INFO:root:current train perplexity4.913057327270508
INFO:root:current mean train loss 1821.7972703391754
INFO:root:current train perplexity4.917645454406738
INFO:root:current mean train loss 1817.653156171822
INFO:root:current train perplexity4.914912700653076
INFO:root:current mean train loss 1819.650040456756
INFO:root:current train perplexity4.920887470245361
INFO:root:current mean train loss 1819.025538360042
INFO:root:current train perplexity4.920165538787842
INFO:root:current mean train loss 1817.3169825819673
INFO:root:current train perplexity4.9189348220825195
INFO:root:current mean train loss 1815.4396720356133
INFO:root:current train perplexity4.915834426879883
INFO:root:current mean train loss 1815.8986214442705
INFO:root:current train perplexity4.91574764251709
INFO:root:current mean train loss 1816.1009584482517
INFO:root:current train perplexity4.915626049041748
INFO:root:current mean train loss 1816.9253074948624
INFO:root:current train perplexity4.9206223487854
INFO:root:current mean train loss 1817.2398957232026
INFO:root:current train perplexity4.922745227813721
INFO:root:current mean train loss 1815.929732390854
INFO:root:current train perplexity4.92503023147583
INFO:root:current mean train loss 1815.584739425914
INFO:root:current train perplexity4.92207670211792
INFO:root:current mean train loss 1815.7204375207327
INFO:root:current train perplexity4.922465801239014
INFO:root:current mean train loss 1814.7017115808112
INFO:root:current train perplexity4.917602062225342

100%|██████████| 1/1 [05:37<00:00, 337.96s/it][A100%|██████████| 1/1 [05:37<00:00, 337.96s/it]
INFO:root:final mean train loss: 1814.5659436693832
INFO:root:final train perplexity: 4.918128490447998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.50s/it][A100%|██████████| 1/1 [00:24<00:00, 24.50s/it]
INFO:root:eval mean loss: 1815.2800375214706
INFO:root:eval perplexity: 5.14780330657959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.85s/it][A100%|██████████| 1/1 [00:22<00:00, 22.85s/it]
INFO:root:eval mean loss: 2183.0685481078235
INFO:root:eval perplexity: 7.4297356605529785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/15
  8%|▊         | 15/200 [1:43:20<20:48:38, 404.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.639239275897
INFO:root:current train perplexity4.81368350982666
INFO:root:current mean train loss 1805.4293712269175
INFO:root:current train perplexity4.877798080444336
INFO:root:current mean train loss 1802.1837888702632
INFO:root:current train perplexity4.854245185852051
INFO:root:current mean train loss 1802.698632536635
INFO:root:current train perplexity4.860961437225342
INFO:root:current mean train loss 1803.9819231075337
INFO:root:current train perplexity4.866847038269043
INFO:root:current mean train loss 1802.1229003465562
INFO:root:current train perplexity4.864160060882568
INFO:root:current mean train loss 1800.724897005507
INFO:root:current train perplexity4.859656810760498
INFO:root:current mean train loss 1802.5135867171957
INFO:root:current train perplexity4.8617424964904785
INFO:root:current mean train loss 1801.574495052007
INFO:root:current train perplexity4.860773086547852
INFO:root:current mean train loss 1801.9354788022472
INFO:root:current train perplexity4.863666534423828
INFO:root:current mean train loss 1801.3225190309238
INFO:root:current train perplexity4.862903594970703
INFO:root:current mean train loss 1800.9813917877357
INFO:root:current train perplexity4.865684509277344
INFO:root:current mean train loss 1801.6354404187848
INFO:root:current train perplexity4.868451118469238
INFO:root:current mean train loss 1800.6336097435549
INFO:root:current train perplexity4.865748882293701
INFO:root:current mean train loss 1801.0983116853024
INFO:root:current train perplexity4.86196231842041
INFO:root:current mean train loss 1801.458396175193
INFO:root:current train perplexity4.860414505004883
INFO:root:current mean train loss 1801.0565507328351
INFO:root:current train perplexity4.859419822692871
INFO:root:current mean train loss 1800.7575797730187
INFO:root:current train perplexity4.859126567840576
INFO:root:current mean train loss 1800.125236371317
INFO:root:current train perplexity4.859051704406738
INFO:root:current mean train loss 1800.8010165820713
INFO:root:current train perplexity4.8581695556640625

100%|██████████| 1/1 [05:39<00:00, 339.43s/it][A100%|██████████| 1/1 [05:39<00:00, 339.43s/it]
INFO:root:final mean train loss: 1800.9301441710102
INFO:root:final train perplexity: 4.859608173370361
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:25<00:00, 25.06s/it][A100%|██████████| 1/1 [00:25<00:00, 25.06s/it]
INFO:root:eval mean loss: 1811.029518939079
INFO:root:eval perplexity: 5.1280903816223145
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.26s/it][A100%|██████████| 1/1 [00:23<00:00, 23.26s/it]
INFO:root:eval mean loss: 2181.2911273063496
INFO:root:eval perplexity: 7.417613506317139
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/16
  8%|▊         | 16/200 [1:49:49<20:27:49, 400.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1774.294072540713
INFO:root:current train perplexity4.7086968421936035
INFO:root:current mean train loss 1773.5403645833333
INFO:root:current train perplexity4.748395919799805
INFO:root:current mean train loss 1778.4532781509456
INFO:root:current train perplexity4.755826473236084
INFO:root:current mean train loss 1783.333217075893
INFO:root:current train perplexity4.781386852264404
INFO:root:current mean train loss 1784.506718532295
INFO:root:current train perplexity4.7824296951293945
INFO:root:current mean train loss 1784.7506915892486
INFO:root:current train perplexity4.7840256690979
INFO:root:current mean train loss 1784.169624249022
INFO:root:current train perplexity4.779876708984375
INFO:root:current mean train loss 1783.57029635062
INFO:root:current train perplexity4.782947540283203
INFO:root:current mean train loss 1783.7321662421068
INFO:root:current train perplexity4.781855583190918
INFO:root:current mean train loss 1784.541952209787
INFO:root:current train perplexity4.786649227142334
INFO:root:current mean train loss 1784.5955761536386
INFO:root:current train perplexity4.790664196014404
INFO:root:current mean train loss 1785.8352212360228
INFO:root:current train perplexity4.7940449714660645
INFO:root:current mean train loss 1787.223711279412
INFO:root:current train perplexity4.799771308898926
INFO:root:current mean train loss 1786.6517403433568
INFO:root:current train perplexity4.800305366516113
INFO:root:current mean train loss 1786.2647282686628
INFO:root:current train perplexity4.798527240753174
INFO:root:current mean train loss 1787.2356368076414
INFO:root:current train perplexity4.799184322357178
INFO:root:current mean train loss 1788.1739257228082
INFO:root:current train perplexity4.801750183105469
INFO:root:current mean train loss 1788.8737820539684
INFO:root:current train perplexity4.8043012619018555
INFO:root:current mean train loss 1788.8291076301318
INFO:root:current train perplexity4.805236339569092
INFO:root:current mean train loss 1788.1486654760631
INFO:root:current train perplexity4.802795886993408

100%|██████████| 1/1 [05:30<00:00, 330.88s/it][A100%|██████████| 1/1 [05:30<00:00, 330.88s/it]
INFO:root:final mean train loss: 1787.7946592976334
INFO:root:final train perplexity: 4.803893089294434
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.81s/it][A100%|██████████| 1/1 [00:24<00:00, 24.81s/it]
INFO:root:eval mean loss: 1807.5734798350234
INFO:root:eval perplexity: 5.112117767333984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.66s/it][A100%|██████████| 1/1 [00:23<00:00, 23.66s/it]
INFO:root:eval mean loss: 2181.777496121454
INFO:root:eval perplexity: 7.4209303855896
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/17
  8%|▊         | 17/200 [1:56:11<20:03:40, 394.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1756.1653345281427
INFO:root:current train perplexity4.733181953430176
INFO:root:current mean train loss 1761.568501573928
INFO:root:current train perplexity4.718188285827637
INFO:root:current mean train loss 1770.9993256462944
INFO:root:current train perplexity4.74871301651001
INFO:root:current mean train loss 1772.5052757656451
INFO:root:current train perplexity4.75110387802124
INFO:root:current mean train loss 1776.4912972372085
INFO:root:current train perplexity4.759420871734619
INFO:root:current mean train loss 1775.0878422536007
INFO:root:current train perplexity4.76000452041626
INFO:root:current mean train loss 1775.023174729458
INFO:root:current train perplexity4.758913040161133
INFO:root:current mean train loss 1773.1753309220833
INFO:root:current train perplexity4.755738735198975
INFO:root:current mean train loss 1772.8266875120971
INFO:root:current train perplexity4.752639293670654
INFO:root:current mean train loss 1771.4480400548773
INFO:root:current train perplexity4.74793815612793
INFO:root:current mean train loss 1772.0699689528522
INFO:root:current train perplexity4.74708890914917
INFO:root:current mean train loss 1773.2490143952546
INFO:root:current train perplexity4.749913692474365
INFO:root:current mean train loss 1774.3892286786381
INFO:root:current train perplexity4.752125263214111
INFO:root:current mean train loss 1775.2730894061262
INFO:root:current train perplexity4.7526726722717285
INFO:root:current mean train loss 1776.141741106587
INFO:root:current train perplexity4.756333351135254
INFO:root:current mean train loss 1775.4067522716762
INFO:root:current train perplexity4.753594875335693
INFO:root:current mean train loss 1774.623423137936
INFO:root:current train perplexity4.7525200843811035
INFO:root:current mean train loss 1774.7390016560053
INFO:root:current train perplexity4.750414848327637
INFO:root:current mean train loss 1774.9597158916927
INFO:root:current train perplexity4.749801158905029

100%|██████████| 1/1 [05:48<00:00, 348.15s/it][A100%|██████████| 1/1 [05:48<00:00, 348.15s/it]
INFO:root:final mean train loss: 1775.3928819772757
INFO:root:final train perplexity: 4.7518768310546875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:25<00:00, 25.13s/it][A100%|██████████| 1/1 [00:25<00:00, 25.13s/it]
INFO:root:eval mean loss: 1802.391721035572
INFO:root:eval perplexity: 5.088263034820557
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.04s/it][A100%|██████████| 1/1 [00:23<00:00, 23.04s/it]
INFO:root:eval mean loss: 2178.440024517952
INFO:root:eval perplexity: 7.39821195602417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/18
  9%|▉         | 18/200 [2:02:49<20:00:18, 395.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1771.807763671875
INFO:root:current train perplexity4.550070285797119
INFO:root:current mean train loss 1763.526410202753
INFO:root:current train perplexity4.660855770111084
INFO:root:current mean train loss 1759.23872248952
INFO:root:current train perplexity4.6448516845703125
INFO:root:current mean train loss 1761.6117387615266
INFO:root:current train perplexity4.673373699188232
INFO:root:current mean train loss 1762.2529592255016
INFO:root:current train perplexity4.687389373779297
INFO:root:current mean train loss 1762.7034653465346
INFO:root:current train perplexity4.703577995300293
INFO:root:current mean train loss 1761.5932786673554
INFO:root:current train perplexity4.700926303863525
INFO:root:current mean train loss 1762.1586460411127
INFO:root:current train perplexity4.696400165557861
INFO:root:current mean train loss 1759.7172734799592
INFO:root:current train perplexity4.6882429122924805
INFO:root:current mean train loss 1762.4894033526848
INFO:root:current train perplexity4.689695835113525
INFO:root:current mean train loss 1763.4367040529773
INFO:root:current train perplexity4.69152307510376
INFO:root:current mean train loss 1765.6499140536623
INFO:root:current train perplexity4.694112777709961
INFO:root:current mean train loss 1764.9463361684711
INFO:root:current train perplexity4.698273181915283
INFO:root:current mean train loss 1764.4021952077346
INFO:root:current train perplexity4.698864459991455
INFO:root:current mean train loss 1764.696381505783
INFO:root:current train perplexity4.699761390686035
INFO:root:current mean train loss 1763.3683642415906
INFO:root:current train perplexity4.6978535652160645
INFO:root:current mean train loss 1765.146517079196
INFO:root:current train perplexity4.7030158042907715
INFO:root:current mean train loss 1764.918236087564
INFO:root:current train perplexity4.705205917358398
INFO:root:current mean train loss 1764.3578883120888
INFO:root:current train perplexity4.702887535095215
INFO:root:current mean train loss 1765.152103646346
INFO:root:current train perplexity4.705237865447998

100%|██████████| 1/1 [05:36<00:00, 336.73s/it][A100%|██████████| 1/1 [05:36<00:00, 336.73s/it]
INFO:root:final mean train loss: 1763.8495609900956
INFO:root:final train perplexity: 4.703967094421387
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.31s/it][A100%|██████████| 1/1 [00:23<00:00, 23.31s/it]
INFO:root:eval mean loss: 1800.4474534920766
INFO:root:eval perplexity: 5.07934045791626
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.32s/it][A100%|██████████| 1/1 [00:21<00:00, 21.32s/it]
INFO:root:eval mean loss: 2180.718179472795
INFO:root:eval perplexity: 7.41370964050293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/19
 10%|▉         | 19/200 [2:09:12<19:42:27, 391.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1782.5915194424715
INFO:root:current train perplexity4.700329780578613
INFO:root:current mean train loss 1763.0597494156634
INFO:root:current train perplexity4.660067558288574
INFO:root:current mean train loss 1769.0254808030686
INFO:root:current train perplexity4.645256519317627
INFO:root:current mean train loss 1762.5342987250096
INFO:root:current train perplexity4.646540641784668
INFO:root:current mean train loss 1761.6018960238632
INFO:root:current train perplexity4.646791934967041
INFO:root:current mean train loss 1763.994561557112
INFO:root:current train perplexity4.659435272216797
INFO:root:current mean train loss 1762.7803073895323
INFO:root:current train perplexity4.65775203704834
INFO:root:current mean train loss 1761.5025240826806
INFO:root:current train perplexity4.659952640533447
INFO:root:current mean train loss 1758.7789343766633
INFO:root:current train perplexity4.6648030281066895
INFO:root:current mean train loss 1758.9015961024352
INFO:root:current train perplexity4.664458274841309
INFO:root:current mean train loss 1759.3580599372401
INFO:root:current train perplexity4.666418075561523
INFO:root:current mean train loss 1757.258336466689
INFO:root:current train perplexity4.6611833572387695
INFO:root:current mean train loss 1755.7219100427706
INFO:root:current train perplexity4.6589555740356445
INFO:root:current mean train loss 1755.4782020464968
INFO:root:current train perplexity4.663148880004883
INFO:root:current mean train loss 1756.2576887128055
INFO:root:current train perplexity4.6633830070495605
INFO:root:current mean train loss 1754.8370401430068
INFO:root:current train perplexity4.663095474243164
INFO:root:current mean train loss 1754.532573130804
INFO:root:current train perplexity4.6622114181518555
INFO:root:current mean train loss 1754.4002716737896
INFO:root:current train perplexity4.663530349731445
INFO:root:current mean train loss 1753.6620853227266
INFO:root:current train perplexity4.662272930145264
INFO:root:current mean train loss 1754.1382732728766
INFO:root:current train perplexity4.66159200668335

100%|██████████| 1/1 [05:45<00:00, 345.37s/it][A100%|██████████| 1/1 [05:45<00:00, 345.37s/it]
INFO:root:final mean train loss: 1753.430673234702
INFO:root:final train perplexity: 4.661139965057373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.11s/it][A100%|██████████| 1/1 [00:23<00:00, 23.12s/it]
INFO:root:eval mean loss: 1796.2333045039616
INFO:root:eval perplexity: 5.060056209564209
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.01s/it][A100%|██████████| 1/1 [00:23<00:00, 23.02s/it]
INFO:root:eval mean loss: 2180.7728466277426
INFO:root:eval perplexity: 7.414084434509277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/20
 10%|█         | 20/200 [2:15:51<19:41:53, 393.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1737.8587364783655
INFO:root:current train perplexity4.6360182762146
INFO:root:current mean train loss 1727.3481919542492
INFO:root:current train perplexity4.567235469818115
INFO:root:current mean train loss 1729.8998722092376
INFO:root:current train perplexity4.556573390960693
INFO:root:current mean train loss 1736.420044305402
INFO:root:current train perplexity4.5837507247924805
INFO:root:current mean train loss 1741.416039260482
INFO:root:current train perplexity4.594297409057617
INFO:root:current mean train loss 1738.8484681647872
INFO:root:current train perplexity4.590020656585693
INFO:root:current mean train loss 1739.1317280036556
INFO:root:current train perplexity4.58627462387085
INFO:root:current mean train loss 1739.7550030657983
INFO:root:current train perplexity4.592793941497803
INFO:root:current mean train loss 1738.1377978864814
INFO:root:current train perplexity4.5877180099487305
INFO:root:current mean train loss 1736.4738874831519
INFO:root:current train perplexity4.587760925292969
INFO:root:current mean train loss 1739.561098247451
INFO:root:current train perplexity4.598024845123291
INFO:root:current mean train loss 1740.3713064888677
INFO:root:current train perplexity4.602655410766602
INFO:root:current mean train loss 1741.6878183286358
INFO:root:current train perplexity4.60534143447876
INFO:root:current mean train loss 1742.8164431719392
INFO:root:current train perplexity4.607247829437256
INFO:root:current mean train loss 1742.9500291306083
INFO:root:current train perplexity4.606454372406006
INFO:root:current mean train loss 1742.1844323785992
INFO:root:current train perplexity4.606147289276123
INFO:root:current mean train loss 1741.2430596435993
INFO:root:current train perplexity4.60729455947876
INFO:root:current mean train loss 1741.7263196930933
INFO:root:current train perplexity4.6099677085876465
INFO:root:current mean train loss 1741.586048352323
INFO:root:current train perplexity4.608150482177734
INFO:root:current mean train loss 1742.2844286127272
INFO:root:current train perplexity4.612288475036621

100%|██████████| 1/1 [05:42<00:00, 342.03s/it][A100%|██████████| 1/1 [05:42<00:00, 342.03s/it]
INFO:root:final mean train loss: 1741.8671683245575
INFO:root:final train perplexity: 4.614063739776611
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.98s/it][A100%|██████████| 1/1 [00:23<00:00, 23.98s/it]
INFO:root:eval mean loss: 1791.903548003934
INFO:root:eval perplexity: 5.040318489074707
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.51s/it][A100%|██████████| 1/1 [00:22<00:00, 22.51s/it]
INFO:root:eval mean loss: 2175.9526475405864
INFO:root:eval perplexity: 7.381324768066406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/21
 10%|█         | 21/200 [2:22:21<19:32:10, 392.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1765.4878191266741
INFO:root:current train perplexity4.61240816116333
INFO:root:current mean train loss 1737.6327279897837
INFO:root:current train perplexity4.570588111877441
INFO:root:current mean train loss 1727.3202991485596
INFO:root:current train perplexity4.548693656921387
INFO:root:current mean train loss 1726.344132326962
INFO:root:current train perplexity4.535022735595703
INFO:root:current mean train loss 1729.3291149474028
INFO:root:current train perplexity4.5511555671691895
INFO:root:current mean train loss 1727.7222116593834
INFO:root:current train perplexity4.551567077636719
INFO:root:current mean train loss 1729.5815798131432
INFO:root:current train perplexity4.555149555206299
INFO:root:current mean train loss 1732.200342894862
INFO:root:current train perplexity4.5562520027160645
INFO:root:current mean train loss 1732.3547173615927
INFO:root:current train perplexity4.563101291656494
INFO:root:current mean train loss 1732.0678488759315
INFO:root:current train perplexity4.561786651611328
INFO:root:current mean train loss 1730.09001344623
INFO:root:current train perplexity4.562796115875244
INFO:root:current mean train loss 1731.1207240543563
INFO:root:current train perplexity4.567621231079102
INFO:root:current mean train loss 1731.7001258218365
INFO:root:current train perplexity4.569940090179443
INFO:root:current mean train loss 1731.7467443415549
INFO:root:current train perplexity4.56862211227417
INFO:root:current mean train loss 1732.8955657455947
INFO:root:current train perplexity4.571516036987305
INFO:root:current mean train loss 1733.4791853642403
INFO:root:current train perplexity4.57103967666626
INFO:root:current mean train loss 1733.1771049315228
INFO:root:current train perplexity4.5702409744262695
INFO:root:current mean train loss 1732.2915702246316
INFO:root:current train perplexity4.568799018859863
INFO:root:current mean train loss 1731.4222747539652
INFO:root:current train perplexity4.567751407623291
INFO:root:current mean train loss 1731.8432591600165
INFO:root:current train perplexity4.570677757263184

100%|██████████| 1/1 [05:34<00:00, 334.09s/it][A100%|██████████| 1/1 [05:34<00:00, 334.09s/it]
INFO:root:final mean train loss: 1731.1113901143115
INFO:root:final train perplexity: 4.570703029632568
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.54s/it][A100%|██████████| 1/1 [00:24<00:00, 24.56s/it]
INFO:root:eval mean loss: 1790.8712885430518
INFO:root:eval perplexity: 5.035624027252197
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.64s/it][A100%|██████████| 1/1 [00:22<00:00, 22.65s/it]
INFO:root:eval mean loss: 2177.914052111037
INFO:root:eval perplexity: 7.394637584686279
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/22
 11%|█         | 22/200 [2:28:45<19:17:04, 390.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1689.6838713345462
INFO:root:current train perplexity4.494919776916504
INFO:root:current mean train loss 1695.8567783598266
INFO:root:current train perplexity4.475677490234375
INFO:root:current mean train loss 1710.8761084699806
INFO:root:current train perplexity4.498351573944092
INFO:root:current mean train loss 1713.9912387551315
INFO:root:current train perplexity4.514979362487793
INFO:root:current mean train loss 1715.647804179605
INFO:root:current train perplexity4.5248003005981445
INFO:root:current mean train loss 1718.5304756097976
INFO:root:current train perplexity4.527571201324463
INFO:root:current mean train loss 1720.2312069761097
INFO:root:current train perplexity4.531167507171631
INFO:root:current mean train loss 1720.5772003923937
INFO:root:current train perplexity4.52942419052124
INFO:root:current mean train loss 1722.4999503608694
INFO:root:current train perplexity4.527897357940674
INFO:root:current mean train loss 1721.0331657386146
INFO:root:current train perplexity4.523705005645752
INFO:root:current mean train loss 1721.1124764733079
INFO:root:current train perplexity4.521626949310303
INFO:root:current mean train loss 1721.3161301608789
INFO:root:current train perplexity4.523073673248291
INFO:root:current mean train loss 1720.7139459147008
INFO:root:current train perplexity4.522341728210449
INFO:root:current mean train loss 1721.7372832607373
INFO:root:current train perplexity4.52531623840332
INFO:root:current mean train loss 1720.7672977693535
INFO:root:current train perplexity4.524763584136963
INFO:root:current mean train loss 1721.0981370813136
INFO:root:current train perplexity4.527464866638184
INFO:root:current mean train loss 1721.5889624067217
INFO:root:current train perplexity4.529999256134033
INFO:root:current mean train loss 1721.6465650667126
INFO:root:current train perplexity4.530557155609131
INFO:root:current mean train loss 1722.628669148133
INFO:root:current train perplexity4.5348920822143555
INFO:root:current mean train loss 1721.823708999719
INFO:root:current train perplexity4.532220840454102

100%|██████████| 1/1 [05:33<00:00, 333.72s/it][A100%|██████████| 1/1 [05:33<00:00, 333.72s/it]
INFO:root:final mean train loss: 1721.3976655268514
INFO:root:final train perplexity: 4.531892776489258
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.04s/it][A100%|██████████| 1/1 [00:24<00:00, 24.04s/it]
INFO:root:eval mean loss: 1789.8570777406085
INFO:root:eval perplexity: 5.031015872955322
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.80s/it][A100%|██████████| 1/1 [00:23<00:00, 23.80s/it]
INFO:root:eval mean loss: 2183.4072075160684
INFO:root:eval perplexity: 7.4320478439331055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/23
 12%|█▏        | 23/200 [2:35:08<19:04:48, 388.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1685.2753743489584
INFO:root:current train perplexity4.445306301116943
INFO:root:current mean train loss 1702.243347810444
INFO:root:current train perplexity4.466085910797119
INFO:root:current mean train loss 1700.8967045224947
INFO:root:current train perplexity4.466945171356201
INFO:root:current mean train loss 1697.192728052384
INFO:root:current train perplexity4.454958438873291
INFO:root:current mean train loss 1699.0956998863999
INFO:root:current train perplexity4.465766429901123
INFO:root:current mean train loss 1700.2245841333422
INFO:root:current train perplexity4.4724531173706055
INFO:root:current mean train loss 1700.1367720009623
INFO:root:current train perplexity4.471897602081299
INFO:root:current mean train loss 1700.3852908363824
INFO:root:current train perplexity4.47404146194458
INFO:root:current mean train loss 1700.2972785178194
INFO:root:current train perplexity4.4760870933532715
INFO:root:current mean train loss 1703.6433285491635
INFO:root:current train perplexity4.479912281036377
INFO:root:current mean train loss 1703.8255221025659
INFO:root:current train perplexity4.477855682373047
INFO:root:current mean train loss 1705.3094800420167
INFO:root:current train perplexity4.483224391937256
INFO:root:current mean train loss 1707.6473833424177
INFO:root:current train perplexity4.488276958465576
INFO:root:current mean train loss 1707.6669463452674
INFO:root:current train perplexity4.487138748168945
INFO:root:current mean train loss 1708.3383156590814
INFO:root:current train perplexity4.492031097412109
INFO:root:current mean train loss 1710.5852165941922
INFO:root:current train perplexity4.493762493133545
INFO:root:current mean train loss 1711.3139260557277
INFO:root:current train perplexity4.49375057220459
INFO:root:current mean train loss 1712.3119364306913
INFO:root:current train perplexity4.493804454803467
INFO:root:current mean train loss 1712.2846463965361
INFO:root:current train perplexity4.493461608886719

100%|██████████| 1/1 [05:45<00:00, 345.34s/it][A100%|██████████| 1/1 [05:45<00:00, 345.34s/it]
INFO:root:final mean train loss: 1711.923735941292
INFO:root:final train perplexity: 4.494358539581299
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.41s/it][A100%|██████████| 1/1 [00:24<00:00, 24.41s/it]
INFO:root:eval mean loss: 1786.4151022620235
INFO:root:eval perplexity: 5.015409469604492
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.02s/it][A100%|██████████| 1/1 [00:23<00:00, 23.02s/it]
INFO:root:eval mean loss: 2176.9253366889684
INFO:root:eval perplexity: 7.387923240661621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/24
 12%|█▏        | 24/200 [2:41:43<19:04:09, 390.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1789.4707205636162
INFO:root:current train perplexity4.674290657043457
INFO:root:current mean train loss 1720.1141368830315
INFO:root:current train perplexity4.501041889190674
INFO:root:current mean train loss 1718.835546521173
INFO:root:current train perplexity4.483966827392578
INFO:root:current mean train loss 1715.4690048764505
INFO:root:current train perplexity4.472164630889893
INFO:root:current mean train loss 1714.8666275361832
INFO:root:current train perplexity4.476387023925781
INFO:root:current mean train loss 1710.5632967074241
INFO:root:current train perplexity4.459969520568848
INFO:root:current mean train loss 1709.884026566683
INFO:root:current train perplexity4.463212490081787
INFO:root:current mean train loss 1710.6970530810754
INFO:root:current train perplexity4.463306903839111
INFO:root:current mean train loss 1707.4510839904256
INFO:root:current train perplexity4.460479259490967
INFO:root:current mean train loss 1706.2103865012575
INFO:root:current train perplexity4.459832668304443
INFO:root:current mean train loss 1705.900919394318
INFO:root:current train perplexity4.4569501876831055
INFO:root:current mean train loss 1704.977678776218
INFO:root:current train perplexity4.458675384521484
INFO:root:current mean train loss 1702.8604173004478
INFO:root:current train perplexity4.4538350105285645
INFO:root:current mean train loss 1703.2928734847217
INFO:root:current train perplexity4.455943584442139
INFO:root:current mean train loss 1702.720054078797
INFO:root:current train perplexity4.454806804656982
INFO:root:current mean train loss 1703.6137291921555
INFO:root:current train perplexity4.456653118133545
INFO:root:current mean train loss 1702.495717588091
INFO:root:current train perplexity4.455142974853516
INFO:root:current mean train loss 1702.3329978723866
INFO:root:current train perplexity4.45343017578125
INFO:root:current mean train loss 1702.125618863383
INFO:root:current train perplexity4.452672004699707
INFO:root:current mean train loss 1703.0100782581444
INFO:root:current train perplexity4.455913543701172

100%|██████████| 1/1 [05:33<00:00, 333.55s/it][A100%|██████████| 1/1 [05:33<00:00, 333.55s/it]
INFO:root:final mean train loss: 1702.429146340082
INFO:root:final train perplexity: 4.457054615020752
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.13s/it][A100%|██████████| 1/1 [00:24<00:00, 24.13s/it]
INFO:root:eval mean loss: 1786.1129643866357
INFO:root:eval perplexity: 5.014042377471924
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.26s/it][A100%|██████████| 1/1 [00:22<00:00, 22.26s/it]
INFO:root:eval mean loss: 2181.8940451331173
INFO:root:eval perplexity: 7.421722888946533
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/25
 12%|█▎        | 25/200 [2:48:05<18:50:30, 387.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1710.0271301269531
INFO:root:current train perplexity4.450357913970947
INFO:root:current mean train loss 1692.9275266585812
INFO:root:current train perplexity4.399276256561279
INFO:root:current mean train loss 1683.2790145874023
INFO:root:current train perplexity4.381409168243408
INFO:root:current mean train loss 1681.5924968954957
INFO:root:current train perplexity4.382175922393799
INFO:root:current mean train loss 1685.0496572818397
INFO:root:current train perplexity4.391599178314209
INFO:root:current mean train loss 1686.4451901967288
INFO:root:current train perplexity4.394435405731201
INFO:root:current mean train loss 1686.5056363619292
INFO:root:current train perplexity4.399138927459717
INFO:root:current mean train loss 1688.7574950160242
INFO:root:current train perplexity4.406998157501221
INFO:root:current mean train loss 1690.7517965372326
INFO:root:current train perplexity4.409808158874512
INFO:root:current mean train loss 1691.908194405692
INFO:root:current train perplexity4.412343978881836
INFO:root:current mean train loss 1692.1932961940765
INFO:root:current train perplexity4.413284778594971
INFO:root:current mean train loss 1692.4068848959491
INFO:root:current train perplexity4.41415548324585
INFO:root:current mean train loss 1691.9722722870072
INFO:root:current train perplexity4.4136786460876465
INFO:root:current mean train loss 1692.7459527790727
INFO:root:current train perplexity4.414299011230469
INFO:root:current mean train loss 1692.3458076219881
INFO:root:current train perplexity4.416999816894531
INFO:root:current mean train loss 1691.685943042825
INFO:root:current train perplexity4.415909767150879
INFO:root:current mean train loss 1691.7112677062087
INFO:root:current train perplexity4.4179840087890625
INFO:root:current mean train loss 1692.407468295706
INFO:root:current train perplexity4.417423248291016
INFO:root:current mean train loss 1692.0845545049299
INFO:root:current train perplexity4.416533946990967
INFO:root:current mean train loss 1692.6217712275452
INFO:root:current train perplexity4.419378280639648

100%|██████████| 1/1 [05:35<00:00, 335.20s/it][A100%|██████████| 1/1 [05:35<00:00, 335.20s/it]
INFO:root:final mean train loss: 1693.4222596784582
INFO:root:final train perplexity: 4.421951770782471
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.67s/it][A100%|██████████| 1/1 [00:23<00:00, 23.67s/it]
INFO:root:eval mean loss: 1783.2366077612478
INFO:root:eval perplexity: 5.001040458679199
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.18s/it][A100%|██████████| 1/1 [00:23<00:00, 23.18s/it]
INFO:root:eval mean loss: 2179.6619738856107
INFO:root:eval perplexity: 7.406520843505859
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/26
 13%|█▎        | 26/200 [2:54:29<18:40:54, 386.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1665.4618426067073
INFO:root:current train perplexity4.3142876625061035
INFO:root:current mean train loss 1673.396720723903
INFO:root:current train perplexity4.347184181213379
INFO:root:current mean train loss 1677.9478106360218
INFO:root:current train perplexity4.355990409851074
INFO:root:current mean train loss 1679.042691675449
INFO:root:current train perplexity4.3649983406066895
INFO:root:current mean train loss 1677.4333759057008
INFO:root:current train perplexity4.363646984100342
INFO:root:current mean train loss 1678.7701668730506
INFO:root:current train perplexity4.365931034088135
INFO:root:current mean train loss 1681.2139612635287
INFO:root:current train perplexity4.368122577667236
INFO:root:current mean train loss 1680.044096376571
INFO:root:current train perplexity4.363539218902588
INFO:root:current mean train loss 1680.4280693789017
INFO:root:current train perplexity4.365967750549316
INFO:root:current mean train loss 1680.4018098058914
INFO:root:current train perplexity4.364740371704102
INFO:root:current mean train loss 1682.361092075491
INFO:root:current train perplexity4.371485710144043
INFO:root:current mean train loss 1682.204020467586
INFO:root:current train perplexity4.373978614807129
INFO:root:current mean train loss 1684.0163294863644
INFO:root:current train perplexity4.3793230056762695
INFO:root:current mean train loss 1684.3936375241774
INFO:root:current train perplexity4.380532264709473
INFO:root:current mean train loss 1684.7800176913006
INFO:root:current train perplexity4.383137226104736
INFO:root:current mean train loss 1684.985299122171
INFO:root:current train perplexity4.384397029876709
INFO:root:current mean train loss 1685.5844906580876
INFO:root:current train perplexity4.386648654937744
INFO:root:current mean train loss 1686.1346544225212
INFO:root:current train perplexity4.3872270584106445
INFO:root:current mean train loss 1685.0553493717325
INFO:root:current train perplexity4.386784076690674
INFO:root:current mean train loss 1685.4949627286192
INFO:root:current train perplexity4.389316082000732

100%|██████████| 1/1 [05:33<00:00, 333.19s/it][A100%|██████████| 1/1 [05:33<00:00, 333.19s/it]
INFO:root:final mean train loss: 1685.0641387770168
INFO:root:final train perplexity: 4.389626502990723
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.84s/it][A100%|██████████| 1/1 [00:24<00:00, 24.84s/it]
INFO:root:eval mean loss: 1784.5409273188166
INFO:root:eval perplexity: 5.006931781768799
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.07s/it][A100%|██████████| 1/1 [00:22<00:00, 22.07s/it]
INFO:root:eval mean loss: 2185.5531620539673
INFO:root:eval perplexity: 7.44671106338501
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/27
 14%|█▎        | 27/200 [3:00:51<18:30:38, 385.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.2205052869074
INFO:root:current train perplexity4.281357765197754
INFO:root:current mean train loss 1661.2684542499012
INFO:root:current train perplexity4.296407222747803
INFO:root:current mean train loss 1663.4300040311591
INFO:root:current train perplexity4.300953388214111
INFO:root:current mean train loss 1666.7235250632857
INFO:root:current train perplexity4.318551540374756
INFO:root:current mean train loss 1668.93583525945
INFO:root:current train perplexity4.319082260131836
INFO:root:current mean train loss 1665.306843419229
INFO:root:current train perplexity4.325648307800293
INFO:root:current mean train loss 1664.9870471896372
INFO:root:current train perplexity4.328082084655762
INFO:root:current mean train loss 1668.9506660401034
INFO:root:current train perplexity4.33213996887207
INFO:root:current mean train loss 1669.6599858068364
INFO:root:current train perplexity4.336761951446533
INFO:root:current mean train loss 1670.4247869758367
INFO:root:current train perplexity4.3431549072265625
INFO:root:current mean train loss 1671.691517128611
INFO:root:current train perplexity4.342648029327393
INFO:root:current mean train loss 1673.5931137164023
INFO:root:current train perplexity4.343770980834961
INFO:root:current mean train loss 1674.2151769689613
INFO:root:current train perplexity4.344718933105469
INFO:root:current mean train loss 1674.918303409627
INFO:root:current train perplexity4.345663070678711
INFO:root:current mean train loss 1675.567999024777
INFO:root:current train perplexity4.347238540649414
INFO:root:current mean train loss 1676.1112811929606
INFO:root:current train perplexity4.350167751312256
INFO:root:current mean train loss 1676.1478722069605
INFO:root:current train perplexity4.3510422706604
INFO:root:current mean train loss 1676.7991273986331
INFO:root:current train perplexity4.354761600494385
INFO:root:current mean train loss 1677.2204327701368
INFO:root:current train perplexity4.3545637130737305
INFO:root:current mean train loss 1676.9141790840063
INFO:root:current train perplexity4.356085777282715

100%|██████████| 1/1 [05:40<00:00, 340.94s/it][A100%|██████████| 1/1 [05:40<00:00, 340.94s/it]
INFO:root:final mean train loss: 1676.3159187997885
INFO:root:final train perplexity: 4.356044292449951
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:25<00:00, 25.14s/it][A100%|██████████| 1/1 [00:25<00:00, 25.14s/it]
INFO:root:eval mean loss: 1782.0798257597794
INFO:root:eval perplexity: 4.995820999145508
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.59s/it][A100%|██████████| 1/1 [00:23<00:00, 23.59s/it]
INFO:root:eval mean loss: 2183.625698657746
INFO:root:eval perplexity: 7.433538436889648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/28
 14%|█▍        | 28/200 [3:07:22<18:29:43, 387.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1681.6486604817708
INFO:root:current train perplexity4.314060211181641
INFO:root:current mean train loss 1654.7980043247767
INFO:root:current train perplexity4.269128322601318
INFO:root:current mean train loss 1658.6393452592329
INFO:root:current train perplexity4.2899909019470215
INFO:root:current mean train loss 1660.020867513021
INFO:root:current train perplexity4.291263103485107
INFO:root:current mean train loss 1660.1643307976974
INFO:root:current train perplexity4.297273635864258
INFO:root:current mean train loss 1662.277701893682
INFO:root:current train perplexity4.297080993652344
INFO:root:current mean train loss 1664.4646135344328
INFO:root:current train perplexity4.305983543395996
INFO:root:current mean train loss 1666.334580235635
INFO:root:current train perplexity4.3134942054748535
INFO:root:current mean train loss 1668.5027647879465
INFO:root:current train perplexity4.313816070556641
INFO:root:current mean train loss 1669.095133964343
INFO:root:current train perplexity4.316829204559326
INFO:root:current mean train loss 1666.0987820221658
INFO:root:current train perplexity4.313255310058594
INFO:root:current mean train loss 1665.7342264378324
INFO:root:current train perplexity4.314192771911621
INFO:root:current mean train loss 1665.7544363702511
INFO:root:current train perplexity4.317691326141357
INFO:root:current mean train loss 1667.027077947443
INFO:root:current train perplexity4.319066047668457
INFO:root:current mean train loss 1667.6956109308792
INFO:root:current train perplexity4.321132659912109
INFO:root:current mean train loss 1667.5454024832588
INFO:root:current train perplexity4.320112705230713
INFO:root:current mean train loss 1668.32700741896
INFO:root:current train perplexity4.32255744934082
INFO:root:current mean train loss 1668.3383056640625
INFO:root:current train perplexity4.322386741638184
INFO:root:current mean train loss 1667.6747229817709
INFO:root:current train perplexity4.322826385498047
INFO:root:current mean train loss 1668.5089017998419
INFO:root:current train perplexity4.325013637542725

100%|██████████| 1/1 [05:38<00:00, 338.01s/it][A100%|██████████| 1/1 [05:38<00:00, 338.02s/it]
INFO:root:final mean train loss: 1668.2415634516929
INFO:root:final train perplexity: 4.325276851654053
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.68s/it][A100%|██████████| 1/1 [00:23<00:00, 23.69s/it]
INFO:root:eval mean loss: 1781.548882234181
INFO:root:eval perplexity: 4.993427753448486
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.92s/it][A100%|██████████| 1/1 [00:21<00:00, 21.92s/it]
INFO:root:eval mean loss: 2184.9129772862643
INFO:root:eval perplexity: 7.442334175109863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/29
 14%|█▍        | 29/200 [3:13:48<18:21:54, 386.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1655.9572528341541
INFO:root:current train perplexity4.275490760803223
INFO:root:current mean train loss 1651.729939142863
INFO:root:current train perplexity4.267523765563965
INFO:root:current mean train loss 1649.124051028735
INFO:root:current train perplexity4.2655816078186035
INFO:root:current mean train loss 1649.9847916583626
INFO:root:current train perplexity4.272156715393066
INFO:root:current mean train loss 1652.5535712513497
INFO:root:current train perplexity4.280823707580566
INFO:root:current mean train loss 1652.5120672277503
INFO:root:current train perplexity4.28217887878418
INFO:root:current mean train loss 1651.6579879143335
INFO:root:current train perplexity4.286077499389648
INFO:root:current mean train loss 1651.6236689403804
INFO:root:current train perplexity4.288434028625488
INFO:root:current mean train loss 1653.5139848512385
INFO:root:current train perplexity4.291116714477539
INFO:root:current mean train loss 1653.7359502238612
INFO:root:current train perplexity4.289785385131836
INFO:root:current mean train loss 1656.2582447764637
INFO:root:current train perplexity4.294261455535889
INFO:root:current mean train loss 1656.550583295374
INFO:root:current train perplexity4.294358730316162
INFO:root:current mean train loss 1656.5981454760667
INFO:root:current train perplexity4.292530536651611
INFO:root:current mean train loss 1656.8941547788422
INFO:root:current train perplexity4.293557167053223
INFO:root:current mean train loss 1657.4820812726468
INFO:root:current train perplexity4.293581485748291
INFO:root:current mean train loss 1658.035333144605
INFO:root:current train perplexity4.293946743011475
INFO:root:current mean train loss 1658.7568230234422
INFO:root:current train perplexity4.292075157165527
INFO:root:current mean train loss 1659.1060686792646
INFO:root:current train perplexity4.289839744567871
INFO:root:current mean train loss 1660.012886547387
INFO:root:current train perplexity4.292202949523926

100%|██████████| 1/1 [05:35<00:00, 335.26s/it][A100%|██████████| 1/1 [05:35<00:00, 335.26s/it]
INFO:root:final mean train loss: 1659.6778243118263
INFO:root:final train perplexity: 4.292882919311523
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.28s/it][A100%|██████████| 1/1 [00:23<00:00, 23.28s/it]
INFO:root:eval mean loss: 1781.7930427713598
INFO:root:eval perplexity: 4.994527816772461
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.57s/it][A100%|██████████| 1/1 [00:22<00:00, 22.57s/it]
INFO:root:eval mean loss: 2187.626094304078
INFO:root:eval perplexity: 7.46090841293335
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/30
 15%|█▌        | 30/200 [3:20:11<18:12:24, 385.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1660.412868923611
INFO:root:current train perplexity4.16909122467041
INFO:root:current mean train loss 1639.2765161356795
INFO:root:current train perplexity4.203980922698975
INFO:root:current mean train loss 1643.41850317266
INFO:root:current train perplexity4.221994400024414
INFO:root:current mean train loss 1643.7425600317304
INFO:root:current train perplexity4.221243381500244
INFO:root:current mean train loss 1642.3337999264593
INFO:root:current train perplexity4.222552299499512
INFO:root:current mean train loss 1641.4345508867725
INFO:root:current train perplexity4.22617769241333
INFO:root:current mean train loss 1645.8715696037304
INFO:root:current train perplexity4.24046516418457
INFO:root:current mean train loss 1646.112725993642
INFO:root:current train perplexity4.239936828613281
INFO:root:current mean train loss 1646.0662191459364
INFO:root:current train perplexity4.244636058807373
INFO:root:current mean train loss 1647.0085971609856
INFO:root:current train perplexity4.245948791503906
INFO:root:current mean train loss 1645.13761679552
INFO:root:current train perplexity4.243539810180664
INFO:root:current mean train loss 1644.3033085127367
INFO:root:current train perplexity4.242430686950684
INFO:root:current mean train loss 1645.9403481053557
INFO:root:current train perplexity4.246457576751709
INFO:root:current mean train loss 1647.2413754386698
INFO:root:current train perplexity4.24952507019043
INFO:root:current mean train loss 1647.7588025777368
INFO:root:current train perplexity4.252724647521973
INFO:root:current mean train loss 1649.3242596018938
INFO:root:current train perplexity4.257356643676758
INFO:root:current mean train loss 1649.1344246019703
INFO:root:current train perplexity4.258615016937256
INFO:root:current mean train loss 1651.0462120060618
INFO:root:current train perplexity4.263464450836182
INFO:root:current mean train loss 1651.947414754569
INFO:root:current train perplexity4.263735294342041
INFO:root:current mean train loss 1652.6842913220478
INFO:root:current train perplexity4.26383113861084

100%|██████████| 1/1 [05:41<00:00, 341.19s/it][A100%|██████████| 1/1 [05:41<00:00, 341.19s/it]
INFO:root:final mean train loss: 1651.7655194581187
INFO:root:final train perplexity: 4.263167858123779
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.98s/it][A100%|██████████| 1/1 [00:23<00:00, 23.98s/it]
INFO:root:eval mean loss: 1782.411149261691
INFO:root:eval perplexity: 4.997315883636475
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.24s/it][A100%|██████████| 1/1 [00:23<00:00, 23.24s/it]
INFO:root:eval mean loss: 2191.172207879682
INFO:root:eval perplexity: 7.4852519035339355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/31
 16%|█▌        | 31/200 [3:26:41<18:10:06, 387.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1618.4331852839543
INFO:root:current train perplexity4.1923699378967285
INFO:root:current mean train loss 1615.7383510044642
INFO:root:current train perplexity4.158989429473877
INFO:root:current mean train loss 1629.5953795846585
INFO:root:current train perplexity4.181495666503906
INFO:root:current mean train loss 1632.7953753325105
INFO:root:current train perplexity4.206329345703125
INFO:root:current mean train loss 1636.3060698173415
INFO:root:current train perplexity4.219752788543701
INFO:root:current mean train loss 1639.219199757159
INFO:root:current train perplexity4.22730016708374
INFO:root:current mean train loss 1636.3570849141374
INFO:root:current train perplexity4.228116989135742
INFO:root:current mean train loss 1641.6096319193355
INFO:root:current train perplexity4.236116886138916
INFO:root:current mean train loss 1639.5291798293736
INFO:root:current train perplexity4.234503269195557
INFO:root:current mean train loss 1641.019010276053
INFO:root:current train perplexity4.238666534423828
INFO:root:current mean train loss 1641.5892564799585
INFO:root:current train perplexity4.235328674316406
INFO:root:current mean train loss 1642.2003405826765
INFO:root:current train perplexity4.2363386154174805
INFO:root:current mean train loss 1643.0909926646295
INFO:root:current train perplexity4.236745834350586
INFO:root:current mean train loss 1643.5296751456742
INFO:root:current train perplexity4.237461566925049
INFO:root:current mean train loss 1643.0737250757418
INFO:root:current train perplexity4.235972881317139
INFO:root:current mean train loss 1642.989174219134
INFO:root:current train perplexity4.236316204071045
INFO:root:current mean train loss 1644.067584536319
INFO:root:current train perplexity4.237069606781006
INFO:root:current mean train loss 1644.5233734236947
INFO:root:current train perplexity4.236491680145264
INFO:root:current mean train loss 1644.8270472916167
INFO:root:current train perplexity4.237560272216797
INFO:root:current mean train loss 1645.0681888188158
INFO:root:current train perplexity4.2364888191223145

100%|██████████| 1/1 [05:39<00:00, 339.99s/it][A100%|██████████| 1/1 [05:39<00:00, 340.00s/it]
INFO:root:final mean train loss: 1644.7150619745375
INFO:root:final train perplexity: 4.2368645668029785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.12s/it][A100%|██████████| 1/1 [00:24<00:00, 24.12s/it]
INFO:root:eval mean loss: 1780.8540355060118
INFO:root:eval perplexity: 4.990296840667725
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.64s/it][A100%|██████████| 1/1 [00:22<00:00, 22.64s/it]
INFO:root:eval mean loss: 2191.3865118364915
INFO:root:eval perplexity: 7.4867262840271
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/32
 16%|█▌        | 32/200 [3:33:10<18:05:05, 387.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1599.7148948492006
INFO:root:current train perplexity4.138827800750732
INFO:root:current mean train loss 1619.7692136964597
INFO:root:current train perplexity4.171921730041504
INFO:root:current mean train loss 1616.0412803618506
INFO:root:current train perplexity4.168133735656738
INFO:root:current mean train loss 1623.9339318456177
INFO:root:current train perplexity4.172895431518555
INFO:root:current mean train loss 1627.298717627945
INFO:root:current train perplexity4.174782752990723
INFO:root:current mean train loss 1627.682864475426
INFO:root:current train perplexity4.172110080718994
INFO:root:current mean train loss 1630.586307128147
INFO:root:current train perplexity4.179152965545654
INFO:root:current mean train loss 1630.9581643845158
INFO:root:current train perplexity4.181797027587891
INFO:root:current mean train loss 1630.7380411639049
INFO:root:current train perplexity4.181468963623047
INFO:root:current mean train loss 1632.4204432951683
INFO:root:current train perplexity4.184556007385254
INFO:root:current mean train loss 1634.5406509355525
INFO:root:current train perplexity4.1914873123168945
INFO:root:current mean train loss 1632.4937954533027
INFO:root:current train perplexity4.189671993255615
INFO:root:current mean train loss 1631.9667736983356
INFO:root:current train perplexity4.187702655792236
INFO:root:current mean train loss 1633.714232125925
INFO:root:current train perplexity4.188746452331543
INFO:root:current mean train loss 1633.9366651338087
INFO:root:current train perplexity4.191267967224121
INFO:root:current mean train loss 1634.6963986330657
INFO:root:current train perplexity4.191349506378174
INFO:root:current mean train loss 1635.5542612569186
INFO:root:current train perplexity4.195335865020752
INFO:root:current mean train loss 1635.3770978164891
INFO:root:current train perplexity4.198331832885742
INFO:root:current mean train loss 1637.118049609163
INFO:root:current train perplexity4.2032470703125
INFO:root:current mean train loss 1637.3304710996808
INFO:root:current train perplexity4.204983711242676

100%|██████████| 1/1 [05:34<00:00, 334.03s/it][A100%|██████████| 1/1 [05:34<00:00, 334.03s/it]
INFO:root:final mean train loss: 1636.6526219016425
INFO:root:final train perplexity: 4.20698356628418
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.06s/it][A100%|██████████| 1/1 [00:24<00:00, 24.06s/it]
INFO:root:eval mean loss: 1779.5641778174868
INFO:root:eval perplexity: 4.984489917755127
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.01s/it][A100%|██████████| 1/1 [00:23<00:00, 23.02s/it]
INFO:root:eval mean loss: 2189.6935836034463
INFO:root:eval perplexity: 7.475090026855469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/33
 16%|█▋        | 33/200 [3:39:33<17:54:51, 386.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1606.2082438151042
INFO:root:current train perplexity4.124439239501953
INFO:root:current mean train loss 1608.2125450134276
INFO:root:current train perplexity4.105673313140869
INFO:root:current mean train loss 1610.812528170072
INFO:root:current train perplexity4.112607479095459
INFO:root:current mean train loss 1620.4696485731338
INFO:root:current train perplexity4.1373820304870605
INFO:root:current mean train loss 1628.6336537236753
INFO:root:current train perplexity4.1597113609313965
INFO:root:current mean train loss 1626.4552860804968
INFO:root:current train perplexity4.151200294494629
INFO:root:current mean train loss 1628.263331187855
INFO:root:current train perplexity4.154537677764893
INFO:root:current mean train loss 1632.7377860621402
INFO:root:current train perplexity4.16877555847168
INFO:root:current mean train loss 1633.1209857319677
INFO:root:current train perplexity4.173163414001465
INFO:root:current mean train loss 1633.9117191314697
INFO:root:current train perplexity4.177739143371582
INFO:root:current mean train loss 1634.1020724314565
INFO:root:current train perplexity4.180137634277344
INFO:root:current mean train loss 1633.3834849390491
INFO:root:current train perplexity4.177709102630615
INFO:root:current mean train loss 1632.8748343331474
INFO:root:current train perplexity4.180298328399658
INFO:root:current mean train loss 1632.8214581657858
INFO:root:current train perplexity4.179716110229492
INFO:root:current mean train loss 1631.5204629140358
INFO:root:current train perplexity4.178853988647461
INFO:root:current mean train loss 1632.1639667217548
INFO:root:current train perplexity4.1822381019592285
INFO:root:current mean train loss 1631.2022105757012
INFO:root:current train perplexity4.181975364685059
INFO:root:current mean train loss 1630.8103770862926
INFO:root:current train perplexity4.183171272277832
INFO:root:current mean train loss 1630.696761658371
INFO:root:current train perplexity4.1821370124816895
INFO:root:current mean train loss 1629.954365072445
INFO:root:current train perplexity4.180931568145752

100%|██████████| 1/1 [05:31<00:00, 331.40s/it][A100%|██████████| 1/1 [05:31<00:00, 331.40s/it]
INFO:root:final mean train loss: 1629.3227979512872
INFO:root:final train perplexity: 4.179999351501465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.28s/it][A100%|██████████| 1/1 [00:24<00:00, 24.28s/it]
INFO:root:eval mean loss: 1781.6888687458445
INFO:root:eval perplexity: 4.994058132171631
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.54s/it][A100%|██████████| 1/1 [00:22<00:00, 22.54s/it]
INFO:root:eval mean loss: 2196.5462711415394
INFO:root:eval perplexity: 7.522297382354736
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/34
 17%|█▋        | 34/200 [3:45:53<17:43:26, 384.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1597.729539747362
INFO:root:current train perplexity4.092113971710205
INFO:root:current mean train loss 1616.9455711235435
INFO:root:current train perplexity4.126626014709473
INFO:root:current mean train loss 1616.4003765230145
INFO:root:current train perplexity4.131557464599609
INFO:root:current mean train loss 1614.0091750248673
INFO:root:current train perplexity4.1322340965271
INFO:root:current mean train loss 1614.2239020325603
INFO:root:current train perplexity4.128990650177002
INFO:root:current mean train loss 1614.4923537864222
INFO:root:current train perplexity4.124581813812256
INFO:root:current mean train loss 1616.7485728411766
INFO:root:current train perplexity4.124279975891113
INFO:root:current mean train loss 1618.7120633119168
INFO:root:current train perplexity4.132106304168701
INFO:root:current mean train loss 1618.2725721509319
INFO:root:current train perplexity4.135805606842041
INFO:root:current mean train loss 1619.7197641706516
INFO:root:current train perplexity4.138453960418701
INFO:root:current mean train loss 1619.7502009569769
INFO:root:current train perplexity4.141145706176758
INFO:root:current mean train loss 1618.9416432344215
INFO:root:current train perplexity4.143301486968994
INFO:root:current mean train loss 1620.926616910667
INFO:root:current train perplexity4.145877838134766
INFO:root:current mean train loss 1621.57256357585
INFO:root:current train perplexity4.149112224578857
INFO:root:current mean train loss 1620.92150168138
INFO:root:current train perplexity4.1498565673828125
INFO:root:current mean train loss 1621.2421142732937
INFO:root:current train perplexity4.150583267211914
INFO:root:current mean train loss 1623.3286386852685
INFO:root:current train perplexity4.156469821929932
INFO:root:current mean train loss 1622.8546154943153
INFO:root:current train perplexity4.1548333168029785
INFO:root:current mean train loss 1623.0338040465172
INFO:root:current train perplexity4.1554274559021
INFO:root:current mean train loss 1623.0661269763411
INFO:root:current train perplexity4.155111789703369

100%|██████████| 1/1 [05:44<00:00, 344.84s/it][A100%|██████████| 1/1 [05:44<00:00, 344.84s/it]
INFO:root:final mean train loss: 1622.5109496700961
INFO:root:final train perplexity: 4.155078411102295
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.93s/it][A100%|██████████| 1/1 [00:23<00:00, 23.93s/it]
INFO:root:eval mean loss: 1781.918296868074
INFO:root:eval perplexity: 4.995092391967773
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.92s/it][A100%|██████████| 1/1 [00:22<00:00, 22.92s/it]
INFO:root:eval mean loss: 2200.5937088770224
INFO:root:eval perplexity: 7.550318717956543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/35
 18%|█▊        | 35/200 [3:52:27<17:44:39, 387.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1607.0434739133145
INFO:root:current train perplexity4.128348350524902
INFO:root:current mean train loss 1602.9598854300903
INFO:root:current train perplexity4.104515552520752
INFO:root:current mean train loss 1611.5146106538318
INFO:root:current train perplexity4.1031365394592285
INFO:root:current mean train loss 1610.1459003584034
INFO:root:current train perplexity4.106827735900879
INFO:root:current mean train loss 1613.9049966788969
INFO:root:current train perplexity4.116010665893555
INFO:root:current mean train loss 1614.3162471886837
INFO:root:current train perplexity4.118598461151123
INFO:root:current mean train loss 1615.8594782496734
INFO:root:current train perplexity4.123936176300049
INFO:root:current mean train loss 1616.3558092861992
INFO:root:current train perplexity4.12221097946167
INFO:root:current mean train loss 1615.7408383089958
INFO:root:current train perplexity4.122811794281006
INFO:root:current mean train loss 1615.2575829734265
INFO:root:current train perplexity4.122016906738281
INFO:root:current mean train loss 1615.527077404629
INFO:root:current train perplexity4.124107360839844
INFO:root:current mean train loss 1616.0279529769616
INFO:root:current train perplexity4.125775337219238
INFO:root:current mean train loss 1615.9647456409389
INFO:root:current train perplexity4.12864875793457
INFO:root:current mean train loss 1614.8656795726104
INFO:root:current train perplexity4.125931262969971
INFO:root:current mean train loss 1615.1623228754863
INFO:root:current train perplexity4.1259307861328125
INFO:root:current mean train loss 1614.4455567937873
INFO:root:current train perplexity4.122532844543457
INFO:root:current mean train loss 1614.8078414394554
INFO:root:current train perplexity4.124237060546875
INFO:root:current mean train loss 1614.2641036119749
INFO:root:current train perplexity4.124050617218018
INFO:root:current mean train loss 1614.1079367100879
INFO:root:current train perplexity4.124626159667969

100%|██████████| 1/1 [05:33<00:00, 333.83s/it][A100%|██████████| 1/1 [05:33<00:00, 333.83s/it]
INFO:root:final mean train loss: 1614.3369989823166
INFO:root:final train perplexity: 4.125370502471924
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.52s/it][A100%|██████████| 1/1 [00:24<00:00, 24.52s/it]
INFO:root:eval mean loss: 1781.1144881011746
INFO:root:eval perplexity: 4.991469860076904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.22s/it][A100%|██████████| 1/1 [00:22<00:00, 22.22s/it]
INFO:root:eval mean loss: 2201.3792733266846
INFO:root:eval perplexity: 7.555769443511963
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/36
 18%|█▊        | 36/200 [3:58:49<17:34:20, 385.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1565.0197531960227
INFO:root:current train perplexity4.014420986175537
INFO:root:current mean train loss 1601.9539816916526
INFO:root:current train perplexity4.057707786560059
INFO:root:current mean train loss 1598.023833216084
INFO:root:current train perplexity4.055412769317627
INFO:root:current mean train loss 1598.2025224986185
INFO:root:current train perplexity4.064488887786865
INFO:root:current mean train loss 1601.9737014213617
INFO:root:current train perplexity4.082961082458496
INFO:root:current mean train loss 1601.48317770762
INFO:root:current train perplexity4.075517654418945
INFO:root:current mean train loss 1602.1389997266904
INFO:root:current train perplexity4.079344272613525
INFO:root:current mean train loss 1602.5224399915392
INFO:root:current train perplexity4.084477424621582
INFO:root:current mean train loss 1606.228706783196
INFO:root:current train perplexity4.089592933654785
INFO:root:current mean train loss 1608.0563324343098
INFO:root:current train perplexity4.090810775756836
INFO:root:current mean train loss 1607.411874048552
INFO:root:current train perplexity4.09075927734375
INFO:root:current mean train loss 1607.5475394097027
INFO:root:current train perplexity4.0907392501831055
INFO:root:current mean train loss 1609.0970652522774
INFO:root:current train perplexity4.096275329589844
INFO:root:current mean train loss 1610.6829820948644
INFO:root:current train perplexity4.101817607879639
INFO:root:current mean train loss 1610.1288312429406
INFO:root:current train perplexity4.104485034942627
INFO:root:current mean train loss 1610.041454464138
INFO:root:current train perplexity4.104611873626709
INFO:root:current mean train loss 1609.664550099293
INFO:root:current train perplexity4.103516101837158
INFO:root:current mean train loss 1608.3223181344974
INFO:root:current train perplexity4.101967811584473
INFO:root:current mean train loss 1609.0123821492355
INFO:root:current train perplexity4.103649616241455
INFO:root:current mean train loss 1609.5315772455317
INFO:root:current train perplexity4.105966091156006

100%|██████████| 1/1 [05:32<00:00, 332.50s/it][A100%|██████████| 1/1 [05:32<00:00, 332.50s/it]
INFO:root:final mean train loss: 1608.370707748036
INFO:root:final train perplexity: 4.103819847106934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.01s/it][A100%|██████████| 1/1 [00:24<00:00, 24.01s/it]
INFO:root:eval mean loss: 1784.3003003275987
INFO:root:eval perplexity: 5.005844593048096
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.31s/it][A100%|██████████| 1/1 [00:23<00:00, 23.31s/it]
INFO:root:eval mean loss: 2206.3489544374725
INFO:root:eval perplexity: 7.590343952178955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/37
 18%|█▊        | 37/200 [4:05:11<17:24:39, 384.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1555.8751482282366
INFO:root:current train perplexity3.9570131301879883
INFO:root:current mean train loss 1575.9181022644043
INFO:root:current train perplexity4.004481315612793
INFO:root:current mean train loss 1591.8956711083129
INFO:root:current train perplexity4.037048816680908
INFO:root:current mean train loss 1593.2204790813166
INFO:root:current train perplexity4.0400261878967285
INFO:root:current mean train loss 1593.2370930609302
INFO:root:current train perplexity4.052012920379639
INFO:root:current mean train loss 1593.0007825909238
INFO:root:current train perplexity4.0508294105529785
INFO:root:current mean train loss 1593.0362410818695
INFO:root:current train perplexity4.054251194000244
INFO:root:current mean train loss 1595.6948144933679
INFO:root:current train perplexity4.058112621307373
INFO:root:current mean train loss 1596.8517459003247
INFO:root:current train perplexity4.06043004989624
INFO:root:current mean train loss 1596.5162043078192
INFO:root:current train perplexity4.061096668243408
INFO:root:current mean train loss 1596.531905831066
INFO:root:current train perplexity4.0611748695373535
INFO:root:current mean train loss 1599.409993381365
INFO:root:current train perplexity4.067215442657471
INFO:root:current mean train loss 1600.2787989513881
INFO:root:current train perplexity4.070469379425049
INFO:root:current mean train loss 1601.1057287009367
INFO:root:current train perplexity4.073301315307617
INFO:root:current mean train loss 1601.0999473764116
INFO:root:current train perplexity4.073751926422119
INFO:root:current mean train loss 1599.54239011435
INFO:root:current train perplexity4.0730085372924805
INFO:root:current mean train loss 1600.703201106491
INFO:root:current train perplexity4.077537536621094
INFO:root:current mean train loss 1600.904459564774
INFO:root:current train perplexity4.077943801879883
INFO:root:current mean train loss 1601.1565335396865
INFO:root:current train perplexity4.077826499938965
INFO:root:current mean train loss 1602.0282604803187
INFO:root:current train perplexity4.0793585777282715

100%|██████████| 1/1 [05:36<00:00, 336.95s/it][A100%|██████████| 1/1 [05:36<00:00, 336.95s/it]
INFO:root:final mean train loss: 1601.405764488878
INFO:root:final train perplexity: 4.078805446624756
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.30s/it][A100%|██████████| 1/1 [00:23<00:00, 23.30s/it]
INFO:root:eval mean loss: 1783.076002188608
INFO:root:eval perplexity: 5.000316143035889
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.58s/it][A100%|██████████| 1/1 [00:22<00:00, 22.58s/it]
INFO:root:eval mean loss: 2206.7520176231437
INFO:root:eval perplexity: 7.593156814575195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/38
 19%|█▉        | 38/200 [4:11:36<17:18:27, 384.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.9979275173612
INFO:root:current train perplexity4.008703231811523
INFO:root:current mean train loss 1574.2628426387391
INFO:root:current train perplexity3.9966230392456055
INFO:root:current mean train loss 1590.7504269969706
INFO:root:current train perplexity4.027398586273193
INFO:root:current mean train loss 1590.0820938773777
INFO:root:current train perplexity4.033903121948242
INFO:root:current mean train loss 1588.048749945137
INFO:root:current train perplexity4.038086414337158
INFO:root:current mean train loss 1587.694549168578
INFO:root:current train perplexity4.035619735717773
INFO:root:current mean train loss 1588.5329809381055
INFO:root:current train perplexity4.04100227355957
INFO:root:current mean train loss 1588.672463067586
INFO:root:current train perplexity4.044267654418945
INFO:root:current mean train loss 1588.3309119591347
INFO:root:current train perplexity4.043933868408203
INFO:root:current mean train loss 1589.6164683831432
INFO:root:current train perplexity4.042134761810303
INFO:root:current mean train loss 1591.363419206975
INFO:root:current train perplexity4.0411176681518555
INFO:root:current mean train loss 1592.317482174536
INFO:root:current train perplexity4.044857501983643
INFO:root:current mean train loss 1591.870932264213
INFO:root:current train perplexity4.048595905303955
INFO:root:current mean train loss 1592.0999211307794
INFO:root:current train perplexity4.048093318939209
INFO:root:current mean train loss 1593.0506623053634
INFO:root:current train perplexity4.050338268280029
INFO:root:current mean train loss 1593.1489717650181
INFO:root:current train perplexity4.049238204956055
INFO:root:current mean train loss 1594.5839424481146
INFO:root:current train perplexity4.051586151123047
INFO:root:current mean train loss 1594.8521336771355
INFO:root:current train perplexity4.053399562835693
INFO:root:current mean train loss 1594.919253631013
INFO:root:current train perplexity4.055023193359375
INFO:root:current mean train loss 1594.808349672136
INFO:root:current train perplexity4.054981231689453

100%|██████████| 1/1 [05:41<00:00, 341.82s/it][A100%|██████████| 1/1 [05:41<00:00, 341.82s/it]
INFO:root:final mean train loss: 1594.892780652145
INFO:root:final train perplexity: 4.055551528930664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:25<00:00, 25.01s/it][A100%|██████████| 1/1 [00:25<00:00, 25.01s/it]
INFO:root:eval mean loss: 1784.198365556433
INFO:root:eval perplexity: 5.0053839683532715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.17s/it][A100%|██████████| 1/1 [00:23<00:00, 23.17s/it]
INFO:root:eval mean loss: 2208.5296262916945
INFO:root:eval perplexity: 7.605566024780273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/39
 20%|█▉        | 39/200 [4:18:08<17:17:54, 386.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1580.1617766349545
INFO:root:current train perplexity3.9990930557250977
INFO:root:current mean train loss 1572.8866652500483
INFO:root:current train perplexity3.9987900257110596
INFO:root:current mean train loss 1586.9361041119992
INFO:root:current train perplexity4.023573398590088
INFO:root:current mean train loss 1587.5309964174724
INFO:root:current train perplexity4.03016471862793
INFO:root:current mean train loss 1584.0863496854709
INFO:root:current train perplexity4.022544860839844
INFO:root:current mean train loss 1585.3241170971419
INFO:root:current train perplexity4.018177509307861
INFO:root:current mean train loss 1586.0444546149217
INFO:root:current train perplexity4.02148962020874
INFO:root:current mean train loss 1587.6761497036991
INFO:root:current train perplexity4.026544094085693
INFO:root:current mean train loss 1588.5822018935344
INFO:root:current train perplexity4.027477264404297
INFO:root:current mean train loss 1588.8900456101385
INFO:root:current train perplexity4.027684688568115
INFO:root:current mean train loss 1589.6962137743128
INFO:root:current train perplexity4.0292558670043945
INFO:root:current mean train loss 1590.2323360475943
INFO:root:current train perplexity4.0282135009765625
INFO:root:current mean train loss 1588.7612971141107
INFO:root:current train perplexity4.025555610656738
INFO:root:current mean train loss 1588.6125295585823
INFO:root:current train perplexity4.026177406311035
INFO:root:current mean train loss 1588.5736317771568
INFO:root:current train perplexity4.0265727043151855
INFO:root:current mean train loss 1588.1638797852813
INFO:root:current train perplexity4.028183460235596
INFO:root:current mean train loss 1588.6218029623572
INFO:root:current train perplexity4.027812480926514
INFO:root:current mean train loss 1588.591802348073
INFO:root:current train perplexity4.029130935668945
INFO:root:current mean train loss 1589.196948058623
INFO:root:current train perplexity4.0309906005859375
INFO:root:current mean train loss 1589.1246034270275
INFO:root:current train perplexity4.032779693603516

100%|██████████| 1/1 [05:33<00:00, 333.77s/it][A100%|██████████| 1/1 [05:33<00:00, 333.77s/it]
INFO:root:final mean train loss: 1588.6132564111845
INFO:root:final train perplexity: 4.0332560539245605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.72s/it][A100%|██████████| 1/1 [00:23<00:00, 23.72s/it]
INFO:root:eval mean loss: 1783.2044127985096
INFO:root:eval perplexity: 5.0008955001831055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2209.2576458437225
INFO:root:eval perplexity: 7.610652446746826
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/40
 20%|██        | 40/200 [4:24:29<17:07:15, 385.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.1703483484969
INFO:root:current train perplexity3.9321835041046143
INFO:root:current mean train loss 1557.9827103428333
INFO:root:current train perplexity3.92661452293396
INFO:root:current mean train loss 1558.534915609599
INFO:root:current train perplexity3.940098285675049
INFO:root:current mean train loss 1563.3067626953125
INFO:root:current train perplexity3.951486110687256
INFO:root:current mean train loss 1568.9763290628262
INFO:root:current train perplexity3.967695713043213
INFO:root:current mean train loss 1570.9479331113719
INFO:root:current train perplexity3.9720089435577393
INFO:root:current mean train loss 1575.076462039189
INFO:root:current train perplexity3.982928514480591
INFO:root:current mean train loss 1577.5626587384165
INFO:root:current train perplexity3.992238998413086
INFO:root:current mean train loss 1578.6845785060705
INFO:root:current train perplexity3.9965624809265137
INFO:root:current mean train loss 1579.247218318078
INFO:root:current train perplexity3.9942734241485596
INFO:root:current mean train loss 1580.1521394321276
INFO:root:current train perplexity3.9948015213012695
INFO:root:current mean train loss 1580.3655245896616
INFO:root:current train perplexity3.9980268478393555
INFO:root:current mean train loss 1580.1330525366193
INFO:root:current train perplexity3.9957573413848877
INFO:root:current mean train loss 1579.545912600843
INFO:root:current train perplexity3.99780535697937
INFO:root:current mean train loss 1581.1637163452397
INFO:root:current train perplexity4.00055456161499
INFO:root:current mean train loss 1580.732657666294
INFO:root:current train perplexity4.002622604370117
INFO:root:current mean train loss 1581.3201298671083
INFO:root:current train perplexity4.004267692565918
INFO:root:current mean train loss 1581.7711782042668
INFO:root:current train perplexity4.0067291259765625
INFO:root:current mean train loss 1582.2263739699058
INFO:root:current train perplexity4.0089287757873535
INFO:root:current mean train loss 1581.8716792433836
INFO:root:current train perplexity4.008382320404053

100%|██████████| 1/1 [05:35<00:00, 335.26s/it][A100%|██████████| 1/1 [05:35<00:00, 335.26s/it]
INFO:root:final mean train loss: 1581.636932480774
INFO:root:final train perplexity: 4.008631706237793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.76s/it][A100%|██████████| 1/1 [00:23<00:00, 23.76s/it]
INFO:root:eval mean loss: 1784.6439758733654
INFO:root:eval perplexity: 5.007397651672363
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.21s/it][A100%|██████████| 1/1 [00:23<00:00, 23.21s/it]
INFO:root:eval mean loss: 2210.0724439688606
INFO:root:eval perplexity: 7.616352081298828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/41
 20%|██        | 41/200 [4:30:53<17:00:00, 384.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.422327677409
INFO:root:current train perplexity3.9700021743774414
INFO:root:current mean train loss 1576.8268127441406
INFO:root:current train perplexity3.986037492752075
INFO:root:current mean train loss 1569.9205235661686
INFO:root:current train perplexity3.9731686115264893
INFO:root:current mean train loss 1567.847343676018
INFO:root:current train perplexity3.966754198074341
INFO:root:current mean train loss 1566.0324015463552
INFO:root:current train perplexity3.964137554168701
INFO:root:current mean train loss 1566.825480621133
INFO:root:current train perplexity3.9609618186950684
INFO:root:current mean train loss 1570.824221906991
INFO:root:current train perplexity3.9655725955963135
INFO:root:current mean train loss 1570.4507521432847
INFO:root:current train perplexity3.962932825088501
INFO:root:current mean train loss 1570.4808461325508
INFO:root:current train perplexity3.9657797813415527
INFO:root:current mean train loss 1570.9993597436621
INFO:root:current train perplexity3.9681942462921143
INFO:root:current mean train loss 1571.683503756558
INFO:root:current train perplexity3.9709625244140625
INFO:root:current mean train loss 1572.5592113482114
INFO:root:current train perplexity3.972141981124878
INFO:root:current mean train loss 1571.9904169153285
INFO:root:current train perplexity3.972588062286377
INFO:root:current mean train loss 1573.1180762698111
INFO:root:current train perplexity3.977057695388794
INFO:root:current mean train loss 1572.553922846993
INFO:root:current train perplexity3.976407289505005
INFO:root:current mean train loss 1573.2587181607582
INFO:root:current train perplexity3.977158308029175
INFO:root:current mean train loss 1574.2558243229705
INFO:root:current train perplexity3.9801714420318604
INFO:root:current mean train loss 1574.9503908560907
INFO:root:current train perplexity3.9839396476745605
INFO:root:current mean train loss 1575.100725391243
INFO:root:current train perplexity3.9856040477752686

100%|██████████| 1/1 [05:33<00:00, 333.26s/it][A100%|██████████| 1/1 [05:33<00:00, 333.26s/it]
INFO:root:final mean train loss: 1575.4878946940105
INFO:root:final train perplexity: 3.987051010131836
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.18s/it][A100%|██████████| 1/1 [00:24<00:00, 24.18s/it]
INFO:root:eval mean loss: 1785.7863098577404
INFO:root:eval perplexity: 5.012563228607178
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.63s/it][A100%|██████████| 1/1 [00:22<00:00, 22.63s/it]
INFO:root:eval mean loss: 2216.3459883886026
INFO:root:eval perplexity: 7.660376071929932
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/42
 21%|██        | 42/200 [4:37:15<16:51:19, 384.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1584.4737173227163
INFO:root:current train perplexity4.088705539703369
INFO:root:current mean train loss 1568.4188005565543
INFO:root:current train perplexity3.955604314804077
INFO:root:current mean train loss 1567.9366478002128
INFO:root:current train perplexity3.950507402420044
INFO:root:current mean train loss 1565.8990825616513
INFO:root:current train perplexity3.947232484817505
INFO:root:current mean train loss 1565.1703581359716
INFO:root:current train perplexity3.9479007720947266
INFO:root:current mean train loss 1560.7237974765474
INFO:root:current train perplexity3.9454569816589355
INFO:root:current mean train loss 1562.8799186569638
INFO:root:current train perplexity3.9523181915283203
INFO:root:current mean train loss 1564.409563190305
INFO:root:current train perplexity3.9528496265411377
INFO:root:current mean train loss 1566.0701685080817
INFO:root:current train perplexity3.9560961723327637
INFO:root:current mean train loss 1564.0959084919223
INFO:root:current train perplexity3.9560444355010986
INFO:root:current mean train loss 1564.0116544009902
INFO:root:current train perplexity3.9547462463378906
INFO:root:current mean train loss 1563.8538722940532
INFO:root:current train perplexity3.9511709213256836
INFO:root:current mean train loss 1565.5920366883179
INFO:root:current train perplexity3.9517323970794678
INFO:root:current mean train loss 1566.1204798655572
INFO:root:current train perplexity3.9529240131378174
INFO:root:current mean train loss 1566.3213018483502
INFO:root:current train perplexity3.95259428024292
INFO:root:current mean train loss 1567.1288563084777
INFO:root:current train perplexity3.956104278564453
INFO:root:current mean train loss 1568.5446626742435
INFO:root:current train perplexity3.9595558643341064
INFO:root:current mean train loss 1569.4837316545352
INFO:root:current train perplexity3.9631354808807373
INFO:root:current mean train loss 1570.0781062821031
INFO:root:current train perplexity3.964522361755371
INFO:root:current mean train loss 1568.9329981387627
INFO:root:current train perplexity3.9637866020202637

100%|██████████| 1/1 [05:36<00:00, 336.99s/it][A100%|██████████| 1/1 [05:36<00:00, 336.99s/it]
INFO:root:final mean train loss: 1569.176622199339
INFO:root:final train perplexity: 3.965022325515747
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.92s/it][A100%|██████████| 1/1 [00:22<00:00, 22.92s/it]
INFO:root:eval mean loss: 1787.148492474928
INFO:root:eval perplexity: 5.018730640411377
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.93s/it][A100%|██████████| 1/1 [00:21<00:00, 21.93s/it]
INFO:root:eval mean loss: 2218.671570689966
INFO:root:eval perplexity: 7.676755905151367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/43
 22%|██▏       | 43/200 [4:43:39<16:44:41, 383.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.6608072916667
INFO:root:current train perplexity3.9264113903045654
INFO:root:current mean train loss 1564.5731745793269
INFO:root:current train perplexity3.937136173248291
INFO:root:current mean train loss 1559.1662502122963
INFO:root:current train perplexity3.930042266845703
INFO:root:current mean train loss 1562.635889041785
INFO:root:current train perplexity3.9392707347869873
INFO:root:current mean train loss 1558.2765000454215
INFO:root:current train perplexity3.9325649738311768
INFO:root:current mean train loss 1561.3815010502653
INFO:root:current train perplexity3.931776523590088
INFO:root:current mean train loss 1562.3720034644716
INFO:root:current train perplexity3.9302713871002197
INFO:root:current mean train loss 1561.8032694777398
INFO:root:current train perplexity3.927464485168457
INFO:root:current mean train loss 1562.8147996282003
INFO:root:current train perplexity3.930976629257202
INFO:root:current mean train loss 1563.9487367691531
INFO:root:current train perplexity3.9367659091949463
INFO:root:current mean train loss 1564.61571388615
INFO:root:current train perplexity3.940777063369751
INFO:root:current mean train loss 1563.1184332653484
INFO:root:current train perplexity3.938852071762085
INFO:root:current mean train loss 1562.9181122570503
INFO:root:current train perplexity3.9398138523101807
INFO:root:current mean train loss 1563.1606194747121
INFO:root:current train perplexity3.941765785217285
INFO:root:current mean train loss 1563.7636438756556
INFO:root:current train perplexity3.943270206451416
INFO:root:current mean train loss 1563.9418012631486
INFO:root:current train perplexity3.9447591304779053
INFO:root:current mean train loss 1562.9461204856452
INFO:root:current train perplexity3.943697929382324
INFO:root:current mean train loss 1563.2468622284819
INFO:root:current train perplexity3.9429917335510254
INFO:root:current mean train loss 1562.3962341642118
INFO:root:current train perplexity3.941761016845703
INFO:root:current mean train loss 1562.5800675624394
INFO:root:current train perplexity3.9413375854492188

100%|██████████| 1/1 [05:33<00:00, 333.40s/it][A100%|██████████| 1/1 [05:33<00:00, 333.40s/it]
INFO:root:final mean train loss: 1562.908550468287
INFO:root:final train perplexity: 3.943264961242676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.01s/it][A100%|██████████| 1/1 [00:23<00:00, 23.01s/it]
INFO:root:eval mean loss: 1788.699109665891
INFO:root:eval perplexity: 5.025760650634766
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.71s/it][A100%|██████████| 1/1 [00:22<00:00, 22.71s/it]
INFO:root:eval mean loss: 2223.1795952979555
INFO:root:eval perplexity: 7.708615779876709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/44
 22%|██▏       | 44/200 [4:50:00<16:36:01, 383.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1537.89918810256
INFO:root:current train perplexity3.882690668106079
INFO:root:current mean train loss 1545.7305791945685
INFO:root:current train perplexity3.89743971824646
INFO:root:current mean train loss 1547.5988329682755
INFO:root:current train perplexity3.911808729171753
INFO:root:current mean train loss 1552.9546109510086
INFO:root:current train perplexity3.923274278640747
INFO:root:current mean train loss 1553.8118265651217
INFO:root:current train perplexity3.925476551055908
INFO:root:current mean train loss 1554.4666337426445
INFO:root:current train perplexity3.9215636253356934
INFO:root:current mean train loss 1556.1486729617466
INFO:root:current train perplexity3.921793222427368
INFO:root:current mean train loss 1556.5543508670098
INFO:root:current train perplexity3.9243593215942383
INFO:root:current mean train loss 1557.7904816574583
INFO:root:current train perplexity3.921478271484375
INFO:root:current mean train loss 1556.6803058434944
INFO:root:current train perplexity3.9216954708099365
INFO:root:current mean train loss 1557.5303270691559
INFO:root:current train perplexity3.923945426940918
INFO:root:current mean train loss 1557.338822169416
INFO:root:current train perplexity3.9224727153778076
INFO:root:current mean train loss 1556.817987682156
INFO:root:current train perplexity3.9196693897247314
INFO:root:current mean train loss 1557.4103892438927
INFO:root:current train perplexity3.9188947677612305
INFO:root:current mean train loss 1557.5953931828244
INFO:root:current train perplexity3.9186720848083496
INFO:root:current mean train loss 1557.6126493566176
INFO:root:current train perplexity3.9190495014190674
INFO:root:current mean train loss 1557.8170095604698
INFO:root:current train perplexity3.922703981399536
INFO:root:current mean train loss 1557.9940474127523
INFO:root:current train perplexity3.922121047973633
INFO:root:current mean train loss 1558.0483850500812
INFO:root:current train perplexity3.9231138229370117
INFO:root:current mean train loss 1558.0756951299275
INFO:root:current train perplexity3.924083709716797

100%|██████████| 1/1 [05:41<00:00, 341.09s/it][A100%|██████████| 1/1 [05:41<00:00, 341.09s/it]
INFO:root:final mean train loss: 1557.5926281288905
INFO:root:final train perplexity: 3.924905776977539
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.82s/it][A100%|██████████| 1/1 [00:23<00:00, 23.82s/it]
INFO:root:eval mean loss: 1789.6813090439384
INFO:root:eval perplexity: 5.03021764755249
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.48s/it][A100%|██████████| 1/1 [00:22<00:00, 22.48s/it]
INFO:root:eval mean loss: 2225.012539045185
INFO:root:eval perplexity: 7.7216057777404785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/45
 22%|██▎       | 45/200 [4:56:30<16:34:28, 384.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.2235107421875
INFO:root:current train perplexity3.8366546630859375
INFO:root:current mean train loss 1540.3309207078887
INFO:root:current train perplexity3.8414969444274902
INFO:root:current mean train loss 1543.4340390292082
INFO:root:current train perplexity3.858203649520874
INFO:root:current mean train loss 1546.0691984407194
INFO:root:current train perplexity3.8649144172668457
INFO:root:current mean train loss 1548.625368315598
INFO:root:current train perplexity3.8772783279418945
INFO:root:current mean train loss 1549.567498173274
INFO:root:current train perplexity3.8769075870513916
INFO:root:current mean train loss 1551.5194273799298
INFO:root:current train perplexity3.879892349243164
INFO:root:current mean train loss 1550.912996781434
INFO:root:current train perplexity3.8853752613067627
INFO:root:current mean train loss 1550.3962886951588
INFO:root:current train perplexity3.8904192447662354
INFO:root:current mean train loss 1549.9871323454925
INFO:root:current train perplexity3.8922061920166016
INFO:root:current mean train loss 1550.3746133675252
INFO:root:current train perplexity3.892275810241699
INFO:root:current mean train loss 1550.7553839929326
INFO:root:current train perplexity3.894744873046875
INFO:root:current mean train loss 1550.4647397391404
INFO:root:current train perplexity3.898305892944336
INFO:root:current mean train loss 1550.6828031567884
INFO:root:current train perplexity3.8977162837982178
INFO:root:current mean train loss 1551.5874880597892
INFO:root:current train perplexity3.899045705795288
INFO:root:current mean train loss 1552.422594543613
INFO:root:current train perplexity3.901139736175537
INFO:root:current mean train loss 1551.8715272316565
INFO:root:current train perplexity3.9035484790802
INFO:root:current mean train loss 1551.5027914933878
INFO:root:current train perplexity3.9022011756896973
INFO:root:current mean train loss 1551.341636166552
INFO:root:current train perplexity3.9025397300720215
INFO:root:current mean train loss 1551.4637739566097
INFO:root:current train perplexity3.9016621112823486

100%|██████████| 1/1 [05:37<00:00, 337.97s/it][A100%|██████████| 1/1 [05:37<00:00, 337.97s/it]
INFO:root:final mean train loss: 1550.8746178454362
INFO:root:final train perplexity: 3.9018266201019287
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.07s/it][A100%|██████████| 1/1 [00:23<00:00, 23.07s/it]
INFO:root:eval mean loss: 1793.245609364611
INFO:root:eval perplexity: 5.0464277267456055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.97s/it][A100%|██████████| 1/1 [00:21<00:00, 21.97s/it]
INFO:root:eval mean loss: 2231.900364652593
INFO:root:eval perplexity: 7.770617961883545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/46
 23%|██▎       | 46/200 [5:02:55<16:27:59, 384.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1553.4591682339892
INFO:root:current train perplexity3.857065200805664
INFO:root:current mean train loss 1540.7519767297565
INFO:root:current train perplexity3.8535704612731934
INFO:root:current mean train loss 1542.5395959602981
INFO:root:current train perplexity3.8586549758911133
INFO:root:current mean train loss 1544.659926847523
INFO:root:current train perplexity3.8565101623535156
INFO:root:current mean train loss 1540.3287779873474
INFO:root:current train perplexity3.859508752822876
INFO:root:current mean train loss 1541.6180598510111
INFO:root:current train perplexity3.869811534881592
INFO:root:current mean train loss 1542.9765427823284
INFO:root:current train perplexity3.8736464977264404
INFO:root:current mean train loss 1542.4098261443662
INFO:root:current train perplexity3.8765125274658203
INFO:root:current mean train loss 1541.9291476748724
INFO:root:current train perplexity3.873622417449951
INFO:root:current mean train loss 1541.7908106812642
INFO:root:current train perplexity3.8765859603881836
INFO:root:current mean train loss 1541.6636067407203
INFO:root:current train perplexity3.8762600421905518
INFO:root:current mean train loss 1542.808110120032
INFO:root:current train perplexity3.875800371170044
INFO:root:current mean train loss 1544.6522087198416
INFO:root:current train perplexity3.8795320987701416
INFO:root:current mean train loss 1545.2870206287337
INFO:root:current train perplexity3.8816661834716797
INFO:root:current mean train loss 1545.2849036196774
INFO:root:current train perplexity3.8816120624542236
INFO:root:current mean train loss 1545.7735835828787
INFO:root:current train perplexity3.883361577987671
INFO:root:current mean train loss 1546.094131315414
INFO:root:current train perplexity3.8845107555389404
INFO:root:current mean train loss 1545.6130136685852
INFO:root:current train perplexity3.8830490112304688
INFO:root:current mean train loss 1545.3846367405552
INFO:root:current train perplexity3.882730484008789
INFO:root:current mean train loss 1545.96669409192
INFO:root:current train perplexity3.8834266662597656

100%|██████████| 1/1 [05:31<00:00, 331.11s/it][A100%|██████████| 1/1 [05:31<00:00, 331.11s/it]
INFO:root:final mean train loss: 1545.3855559425526
INFO:root:final train perplexity: 3.883070707321167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.95s/it][A100%|██████████| 1/1 [00:23<00:00, 23.95s/it]
INFO:root:eval mean loss: 1794.8216630651596
INFO:root:eval perplexity: 5.053612232208252
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.23s/it][A100%|██████████| 1/1 [00:22<00:00, 22.23s/it]
INFO:root:eval mean loss: 2234.3829401976673
INFO:root:eval perplexity: 7.788362979888916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/47
 24%|██▎       | 47/200 [5:09:14<16:17:13, 383.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.6877167370856
INFO:root:current train perplexity3.829829454421997
INFO:root:current mean train loss 1519.672484118529
INFO:root:current train perplexity3.818237543106079
INFO:root:current mean train loss 1522.949795511745
INFO:root:current train perplexity3.8209917545318604
INFO:root:current mean train loss 1528.353604570705
INFO:root:current train perplexity3.829709529876709
INFO:root:current mean train loss 1527.9456833682386
INFO:root:current train perplexity3.8326711654663086
INFO:root:current mean train loss 1531.1327514648438
INFO:root:current train perplexity3.8313419818878174
INFO:root:current mean train loss 1532.1807172277936
INFO:root:current train perplexity3.8353400230407715
INFO:root:current mean train loss 1532.7920389964167
INFO:root:current train perplexity3.8396785259246826
INFO:root:current mean train loss 1533.6273563104642
INFO:root:current train perplexity3.845172882080078
INFO:root:current mean train loss 1534.7004960849433
INFO:root:current train perplexity3.8470475673675537
INFO:root:current mean train loss 1534.9651364741646
INFO:root:current train perplexity3.846959114074707
INFO:root:current mean train loss 1534.547079809122
INFO:root:current train perplexity3.849759817123413
INFO:root:current mean train loss 1535.3463663298103
INFO:root:current train perplexity3.8505754470825195
INFO:root:current mean train loss 1535.7894537711518
INFO:root:current train perplexity3.8520920276641846
INFO:root:current mean train loss 1536.25266517466
INFO:root:current train perplexity3.855354070663452
INFO:root:current mean train loss 1537.7323659579356
INFO:root:current train perplexity3.8561625480651855
INFO:root:current mean train loss 1538.764513354981
INFO:root:current train perplexity3.8607194423675537
INFO:root:current mean train loss 1539.4066115263704
INFO:root:current train perplexity3.8607985973358154
INFO:root:current mean train loss 1540.2146160740747
INFO:root:current train perplexity3.862907648086548

100%|██████████| 1/1 [05:35<00:00, 335.80s/it][A100%|██████████| 1/1 [05:35<00:00, 335.80s/it]
INFO:root:final mean train loss: 1539.5003603013308
INFO:root:final train perplexity: 3.8630614280700684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.91s/it][A100%|██████████| 1/1 [00:23<00:00, 23.91s/it]
INFO:root:eval mean loss: 1794.78043663079
INFO:root:eval perplexity: 5.053424835205078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.08s/it][A100%|██████████| 1/1 [00:23<00:00, 23.08s/it]
INFO:root:eval mean loss: 2235.975932236259
INFO:root:eval perplexity: 7.799768447875977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/48
 24%|██▍       | 48/200 [5:15:39<16:12:02, 383.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1514.397615559896
INFO:root:current train perplexity3.811278820037842
INFO:root:current mean train loss 1518.3007069463315
INFO:root:current train perplexity3.8180203437805176
INFO:root:current mean train loss 1529.2679897574492
INFO:root:current train perplexity3.826416015625
INFO:root:current mean train loss 1528.7079527839783
INFO:root:current train perplexity3.8313889503479004
INFO:root:current mean train loss 1527.836359304405
INFO:root:current train perplexity3.8341307640075684
INFO:root:current mean train loss 1526.9464810565837
INFO:root:current train perplexity3.832336902618408
INFO:root:current mean train loss 1529.8663528566437
INFO:root:current train perplexity3.8348467350006104
INFO:root:current mean train loss 1530.374287553267
INFO:root:current train perplexity3.8267297744750977
INFO:root:current mean train loss 1528.7867734195265
INFO:root:current train perplexity3.8277902603149414
INFO:root:current mean train loss 1530.4608107603312
INFO:root:current train perplexity3.8289060592651367
INFO:root:current mean train loss 1529.6334902007004
INFO:root:current train perplexity3.826909065246582
INFO:root:current mean train loss 1532.0113296577215
INFO:root:current train perplexity3.829822540283203
INFO:root:current mean train loss 1532.8176767859438
INFO:root:current train perplexity3.831829309463501
INFO:root:current mean train loss 1531.5946058846246
INFO:root:current train perplexity3.8335933685302734
INFO:root:current mean train loss 1532.8541320153765
INFO:root:current train perplexity3.8364007472991943
INFO:root:current mean train loss 1533.9742825649753
INFO:root:current train perplexity3.838740825653076
INFO:root:current mean train loss 1533.3504863160313
INFO:root:current train perplexity3.8405675888061523
INFO:root:current mean train loss 1532.850562733464
INFO:root:current train perplexity3.838942050933838
INFO:root:current mean train loss 1532.5147280690428
INFO:root:current train perplexity3.8390932083129883
INFO:root:current mean train loss 1533.6010065223156
INFO:root:current train perplexity3.840837240219116

100%|██████████| 1/1 [05:36<00:00, 336.44s/it][A100%|██████████| 1/1 [05:36<00:00, 336.44s/it]
INFO:root:final mean train loss: 1533.4161765694437
INFO:root:final train perplexity: 3.8424835205078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.29s/it][A100%|██████████| 1/1 [00:23<00:00, 23.29s/it]
INFO:root:eval mean loss: 1795.981647897274
INFO:root:eval perplexity: 5.058906555175781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.06s/it][A100%|██████████| 1/1 [00:22<00:00, 22.06s/it]
INFO:root:eval mean loss: 2241.2678607913617
INFO:root:eval perplexity: 7.8377766609191895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/49
 24%|██▍       | 49/200 [5:22:02<16:05:41, 383.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.1538734436035
INFO:root:current train perplexity3.7870841026306152
INFO:root:current mean train loss 1523.9769795735676
INFO:root:current train perplexity3.7932565212249756
INFO:root:current mean train loss 1519.8555566195785
INFO:root:current train perplexity3.810030460357666
INFO:root:current mean train loss 1521.5740694712445
INFO:root:current train perplexity3.8029606342315674
INFO:root:current mean train loss 1522.0768630416305
INFO:root:current train perplexity3.802255392074585
INFO:root:current mean train loss 1523.6501042645677
INFO:root:current train perplexity3.8089654445648193
INFO:root:current mean train loss 1530.308665601513
INFO:root:current train perplexity3.817559242248535
INFO:root:current mean train loss 1528.9345236189379
INFO:root:current train perplexity3.8166561126708984
INFO:root:current mean train loss 1528.1274964259221
INFO:root:current train perplexity3.815985918045044
INFO:root:current mean train loss 1528.2574335843196
INFO:root:current train perplexity3.817802906036377
INFO:root:current mean train loss 1527.9076341732527
INFO:root:current train perplexity3.8217201232910156
INFO:root:current mean train loss 1527.5455504508407
INFO:root:current train perplexity3.8196778297424316
INFO:root:current mean train loss 1526.4703760518655
INFO:root:current train perplexity3.818035364151001
INFO:root:current mean train loss 1527.4334023048928
INFO:root:current train perplexity3.8200061321258545
INFO:root:current mean train loss 1527.7853591322232
INFO:root:current train perplexity3.8211958408355713
INFO:root:current mean train loss 1529.1240253498286
INFO:root:current train perplexity3.823324680328369
INFO:root:current mean train loss 1529.3539638145296
INFO:root:current train perplexity3.8250985145568848
INFO:root:current mean train loss 1529.4540991662006
INFO:root:current train perplexity3.825777053833008
INFO:root:current mean train loss 1529.352671527446
INFO:root:current train perplexity3.825145959854126
INFO:root:current mean train loss 1529.0851947160488
INFO:root:current train perplexity3.8257555961608887

100%|██████████| 1/1 [05:40<00:00, 340.70s/it][A100%|██████████| 1/1 [05:40<00:00, 340.70s/it]
INFO:root:final mean train loss: 1528.4545371189781
INFO:root:final train perplexity: 3.8257839679718018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.61s/it][A100%|██████████| 1/1 [00:24<00:00, 24.62s/it]
INFO:root:eval mean loss: 1798.8829202854888
INFO:root:eval perplexity: 5.072172164916992
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.40s/it][A100%|██████████| 1/1 [00:23<00:00, 23.40s/it]
INFO:root:eval mean loss: 2244.6203591637577
INFO:root:eval perplexity: 7.861953258514404
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/50
 25%|██▌       | 50/200 [5:28:33<16:04:27, 385.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1509.2147191884567
INFO:root:current train perplexity3.746324300765991
INFO:root:current mean train loss 1521.490172110948
INFO:root:current train perplexity3.8049404621124268
INFO:root:current mean train loss 1517.7357462074863
INFO:root:current train perplexity3.7851998805999756
INFO:root:current mean train loss 1514.9363894749508
INFO:root:current train perplexity3.7842655181884766
INFO:root:current mean train loss 1519.92734804557
INFO:root:current train perplexity3.7954037189483643
INFO:root:current mean train loss 1520.2912097368085
INFO:root:current train perplexity3.796424627304077
INFO:root:current mean train loss 1522.2045226956736
INFO:root:current train perplexity3.800016403198242
INFO:root:current mean train loss 1520.973036151066
INFO:root:current train perplexity3.801119327545166
INFO:root:current mean train loss 1522.9601658258337
INFO:root:current train perplexity3.804314136505127
INFO:root:current mean train loss 1523.8423570606806
INFO:root:current train perplexity3.801447868347168
INFO:root:current mean train loss 1522.3765371084442
INFO:root:current train perplexity3.8011391162872314
INFO:root:current mean train loss 1521.1907034692192
INFO:root:current train perplexity3.800241708755493
INFO:root:current mean train loss 1520.6921068104484
INFO:root:current train perplexity3.7996675968170166
INFO:root:current mean train loss 1522.1935001766356
INFO:root:current train perplexity3.8005125522613525
INFO:root:current mean train loss 1522.423995940088
INFO:root:current train perplexity3.801220178604126
INFO:root:current mean train loss 1523.2256377917709
INFO:root:current train perplexity3.8026280403137207
INFO:root:current mean train loss 1522.5285669700386
INFO:root:current train perplexity3.802086353302002
INFO:root:current mean train loss 1522.4091310408403
INFO:root:current train perplexity3.8037781715393066
INFO:root:current mean train loss 1522.3471684969072
INFO:root:current train perplexity3.8064005374908447
INFO:root:current mean train loss 1523.2211638480471
INFO:root:current train perplexity3.8063437938690186

100%|██████████| 1/1 [05:35<00:00, 335.69s/it][A100%|██████████| 1/1 [05:35<00:00, 335.69s/it]
INFO:root:final mean train loss: 1522.8754075166257
INFO:root:final train perplexity: 3.80709171295166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.03s/it][A100%|██████████| 1/1 [00:24<00:00, 24.03s/it]
INFO:root:eval mean loss: 1798.0511773292055
INFO:root:eval perplexity: 5.068365573883057
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.04s/it][A100%|██████████| 1/1 [00:22<00:00, 22.04s/it]
INFO:root:eval mean loss: 2243.7283935546875
INFO:root:eval perplexity: 7.855513572692871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/51
 26%|██▌       | 51/200 [5:34:57<15:56:26, 385.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1527.312899502841
INFO:root:current train perplexity3.7876574993133545
INFO:root:current mean train loss 1509.3953247070312
INFO:root:current train perplexity3.7806081771850586
INFO:root:current mean train loss 1503.5083241857085
INFO:root:current train perplexity3.7761459350585938
INFO:root:current mean train loss 1503.7824767065829
INFO:root:current train perplexity3.783522605895996
INFO:root:current mean train loss 1507.531539720527
INFO:root:current train perplexity3.783442735671997
INFO:root:current mean train loss 1509.0787383709696
INFO:root:current train perplexity3.7882444858551025
INFO:root:current mean train loss 1510.4172739023202
INFO:root:current train perplexity3.7907636165618896
INFO:root:current mean train loss 1511.4651003207612
INFO:root:current train perplexity3.7853126525878906
INFO:root:current mean train loss 1511.1283727209782
INFO:root:current train perplexity3.7840704917907715
INFO:root:current mean train loss 1513.6130532843233
INFO:root:current train perplexity3.786198616027832
INFO:root:current mean train loss 1513.4194125234521
INFO:root:current train perplexity3.7874345779418945
INFO:root:current mean train loss 1514.425014803381
INFO:root:current train perplexity3.785374402999878
INFO:root:current mean train loss 1513.456242324805
INFO:root:current train perplexity3.7828221321105957
INFO:root:current mean train loss 1513.0969999656845
INFO:root:current train perplexity3.7828991413116455
INFO:root:current mean train loss 1513.687044609455
INFO:root:current train perplexity3.785161018371582
INFO:root:current mean train loss 1514.8968888595796
INFO:root:current train perplexity3.7871644496917725
INFO:root:current mean train loss 1515.6713158652133
INFO:root:current train perplexity3.787806510925293
INFO:root:current mean train loss 1516.0519822532162
INFO:root:current train perplexity3.7886881828308105
INFO:root:current mean train loss 1516.859580413066
INFO:root:current train perplexity3.788301944732666
INFO:root:current mean train loss 1517.7056746924275
INFO:root:current train perplexity3.7882707118988037

100%|██████████| 1/1 [05:36<00:00, 336.93s/it][A100%|██████████| 1/1 [05:36<00:00, 336.93s/it]
INFO:root:final mean train loss: 1517.294755082508
INFO:root:final train perplexity: 3.7884867191314697
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.18s/it][A100%|██████████| 1/1 [00:24<00:00, 24.18s/it]
INFO:root:eval mean loss: 1801.6335055303912
INFO:root:eval perplexity: 5.084782123565674
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.61s/it][A100%|██████████| 1/1 [00:22<00:00, 22.61s/it]
INFO:root:eval mean loss: 2251.4699113994625
INFO:root:eval perplexity: 7.911579132080078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/52
 26%|██▌       | 52/200 [5:41:22<15:50:22, 385.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1498.5522622717433
INFO:root:current train perplexity3.7170770168304443
INFO:root:current mean train loss 1501.8510595436305
INFO:root:current train perplexity3.727468252182007
INFO:root:current mean train loss 1502.3281396656912
INFO:root:current train perplexity3.7364282608032227
INFO:root:current mean train loss 1504.1451020801037
INFO:root:current train perplexity3.745347738265991
INFO:root:current mean train loss 1500.9032065823951
INFO:root:current train perplexity3.74538516998291
INFO:root:current mean train loss 1502.4844202267368
INFO:root:current train perplexity3.7463674545288086
INFO:root:current mean train loss 1504.5121326809572
INFO:root:current train perplexity3.7468855381011963
INFO:root:current mean train loss 1506.7569622171336
INFO:root:current train perplexity3.7517426013946533
INFO:root:current mean train loss 1507.1243965606861
INFO:root:current train perplexity3.7571778297424316
INFO:root:current mean train loss 1508.2671751563691
INFO:root:current train perplexity3.759223461151123
INFO:root:current mean train loss 1509.5475024842378
INFO:root:current train perplexity3.762817859649658
INFO:root:current mean train loss 1510.3446088260448
INFO:root:current train perplexity3.763991117477417
INFO:root:current mean train loss 1510.8058813723937
INFO:root:current train perplexity3.76596999168396
INFO:root:current mean train loss 1510.2049298400218
INFO:root:current train perplexity3.7643470764160156
INFO:root:current mean train loss 1511.1633211059982
INFO:root:current train perplexity3.7649118900299072
INFO:root:current mean train loss 1511.5876437854104
INFO:root:current train perplexity3.7672770023345947
INFO:root:current mean train loss 1512.8023605482676
INFO:root:current train perplexity3.7693889141082764
INFO:root:current mean train loss 1512.4269376659554
INFO:root:current train perplexity3.7691502571105957
INFO:root:current mean train loss 1512.392915228359
INFO:root:current train perplexity3.768111228942871
INFO:root:current mean train loss 1511.6918817886606
INFO:root:current train perplexity3.7698988914489746

100%|██████████| 1/1 [05:43<00:00, 343.93s/it][A100%|██████████| 1/1 [05:43<00:00, 343.93s/it]
INFO:root:final mean train loss: 1511.6918817886606
INFO:root:final train perplexity: 3.7698988914489746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.21s/it][A100%|██████████| 1/1 [00:24<00:00, 24.21s/it]
INFO:root:eval mean loss: 1804.4439389925476
INFO:root:eval perplexity: 5.097696781158447
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.11s/it][A100%|██████████| 1/1 [00:24<00:00, 24.11s/it]
INFO:root:eval mean loss: 2254.6028818982713
INFO:root:eval perplexity: 7.934383392333984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/53
 26%|██▋       | 53/200 [5:47:56<15:50:29, 387.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1500.871649169922
INFO:root:current train perplexity3.705552816390991
INFO:root:current mean train loss 1504.955541381836
INFO:root:current train perplexity3.72025465965271
INFO:root:current mean train loss 1504.320928548177
INFO:root:current train perplexity3.7262513637542725
INFO:root:current mean train loss 1502.452896118164
INFO:root:current train perplexity3.7275068759918213
INFO:root:current mean train loss 1502.82144140625
INFO:root:current train perplexity3.73093581199646
INFO:root:current mean train loss 1500.1671624755859
INFO:root:current train perplexity3.7295453548431396
INFO:root:current mean train loss 1504.0693376813615
INFO:root:current train perplexity3.7375571727752686
INFO:root:current mean train loss 1502.463885345459
INFO:root:current train perplexity3.736865282058716
INFO:root:current mean train loss 1503.3108026801215
INFO:root:current train perplexity3.735016107559204
INFO:root:current mean train loss 1505.4225013427733
INFO:root:current train perplexity3.7415053844451904
INFO:root:current mean train loss 1504.7463596413352
INFO:root:current train perplexity3.7418551445007324
INFO:root:current mean train loss 1504.0860211181641
INFO:root:current train perplexity3.744386911392212
INFO:root:current mean train loss 1504.2395310621996
INFO:root:current train perplexity3.745039463043213
INFO:root:current mean train loss 1504.2994908796038
INFO:root:current train perplexity3.7446959018707275
INFO:root:current mean train loss 1506.3677460123697
INFO:root:current train perplexity3.747750997543335
INFO:root:current mean train loss 1506.4072612762452
INFO:root:current train perplexity3.7476227283477783
INFO:root:current mean train loss 1506.3686071777345
INFO:root:current train perplexity3.748558282852173
INFO:root:current mean train loss 1506.4672099473742
INFO:root:current train perplexity3.7499794960021973
INFO:root:current mean train loss 1506.20797864412
INFO:root:current train perplexity3.7510266304016113

100%|██████████| 1/1 [05:39<00:00, 339.54s/it][A100%|██████████| 1/1 [05:39<00:00, 339.54s/it]
INFO:root:final mean train loss: 1506.4264942809782
INFO:root:final train perplexity: 3.752513885498047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.93s/it][A100%|██████████| 1/1 [00:23<00:00, 23.93s/it]
INFO:root:eval mean loss: 1806.1349153992132
INFO:root:eval perplexity: 5.1054840087890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.49s/it][A100%|██████████| 1/1 [00:23<00:00, 23.49s/it]
INFO:root:eval mean loss: 2257.7870838354665
INFO:root:eval perplexity: 7.95762825012207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/54
 27%|██▋       | 54/200 [5:54:25<15:44:41, 388.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1493.1654986213234
INFO:root:current train perplexity3.816113233566284
INFO:root:current mean train loss 1500.5064572065305
INFO:root:current train perplexity3.7273261547088623
INFO:root:current mean train loss 1503.849106467814
INFO:root:current train perplexity3.7165772914886475
INFO:root:current mean train loss 1497.593619072851
INFO:root:current train perplexity3.7165634632110596
INFO:root:current mean train loss 1498.3542749784547
INFO:root:current train perplexity3.7220067977905273
INFO:root:current mean train loss 1499.682624979222
INFO:root:current train perplexity3.7248685359954834
INFO:root:current mean train loss 1500.0232220468877
INFO:root:current train perplexity3.72567081451416
INFO:root:current mean train loss 1499.4013087912417
INFO:root:current train perplexity3.721616744995117
INFO:root:current mean train loss 1499.7246748178359
INFO:root:current train perplexity3.7245309352874756
INFO:root:current mean train loss 1498.5599005812517
INFO:root:current train perplexity3.7255451679229736
INFO:root:current mean train loss 1498.6754625708656
INFO:root:current train perplexity3.72754168510437
INFO:root:current mean train loss 1500.2627303926884
INFO:root:current train perplexity3.7315797805786133
INFO:root:current mean train loss 1499.9546453120186
INFO:root:current train perplexity3.730067253112793
INFO:root:current mean train loss 1501.1473570340677
INFO:root:current train perplexity3.7304422855377197
INFO:root:current mean train loss 1501.363461383397
INFO:root:current train perplexity3.730541706085205
INFO:root:current mean train loss 1501.9255323617492
INFO:root:current train perplexity3.730797529220581
INFO:root:current mean train loss 1502.7968753019675
INFO:root:current train perplexity3.7333765029907227
INFO:root:current mean train loss 1502.7327942712125
INFO:root:current train perplexity3.735313892364502
INFO:root:current mean train loss 1502.6214816608335
INFO:root:current train perplexity3.735333204269409
INFO:root:current mean train loss 1501.8103338728115
INFO:root:current train perplexity3.7340381145477295

100%|██████████| 1/1 [05:35<00:00, 335.32s/it][A100%|██████████| 1/1 [05:35<00:00, 335.32s/it]
INFO:root:final mean train loss: 1501.3643359608923
INFO:root:final train perplexity: 3.7358758449554443
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.89s/it][A100%|██████████| 1/1 [00:23<00:00, 23.89s/it]
INFO:root:eval mean loss: 1806.1913270341588
INFO:root:eval perplexity: 5.105743408203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.02s/it][A100%|██████████| 1/1 [00:23<00:00, 23.02s/it]
INFO:root:eval mean loss: 2259.072503272523
INFO:root:eval perplexity: 7.967029094696045
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/55
 28%|██▊       | 55/200 [6:00:49<15:35:13, 386.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1504.1511769014246
INFO:root:current train perplexity3.7590999603271484
INFO:root:current mean train loss 1487.7637775478079
INFO:root:current train perplexity3.7092549800872803
INFO:root:current mean train loss 1485.9035112429888
INFO:root:current train perplexity3.703266143798828
INFO:root:current mean train loss 1487.563282858112
INFO:root:current train perplexity3.7122135162353516
INFO:root:current mean train loss 1484.4150910970802
INFO:root:current train perplexity3.6965439319610596
INFO:root:current mean train loss 1485.8660538919855
INFO:root:current train perplexity3.697772264480591
INFO:root:current mean train loss 1488.1255878243912
INFO:root:current train perplexity3.7043297290802
INFO:root:current mean train loss 1488.7999764840024
INFO:root:current train perplexity3.7087814807891846
INFO:root:current mean train loss 1489.8108958145983
INFO:root:current train perplexity3.708786964416504
INFO:root:current mean train loss 1491.0087835732568
INFO:root:current train perplexity3.710803508758545
INFO:root:current mean train loss 1491.8443187957114
INFO:root:current train perplexity3.7092156410217285
INFO:root:current mean train loss 1492.0786688264716
INFO:root:current train perplexity3.7097764015197754
INFO:root:current mean train loss 1493.9317619039327
INFO:root:current train perplexity3.709883451461792
INFO:root:current mean train loss 1495.594649513622
INFO:root:current train perplexity3.7133989334106445
INFO:root:current mean train loss 1494.6259112710568
INFO:root:current train perplexity3.7128565311431885
INFO:root:current mean train loss 1494.5129672253147
INFO:root:current train perplexity3.714501142501831
INFO:root:current mean train loss 1495.0739164130632
INFO:root:current train perplexity3.713993787765503
INFO:root:current mean train loss 1496.124897218768
INFO:root:current train perplexity3.717392921447754
INFO:root:current mean train loss 1496.01481457027
INFO:root:current train perplexity3.717115640640259
INFO:root:current mean train loss 1496.057482304223
INFO:root:current train perplexity3.718252420425415

100%|██████████| 1/1 [05:42<00:00, 342.88s/it][A100%|██████████| 1/1 [05:42<00:00, 342.88s/it]
INFO:root:final mean train loss: 1495.8179811293949
INFO:root:final train perplexity: 3.717729330062866
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.91s/it][A100%|██████████| 1/1 [00:24<00:00, 24.91s/it]
INFO:root:eval mean loss: 1811.591135011497
INFO:root:eval perplexity: 5.1306915283203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.64s/it][A100%|██████████| 1/1 [00:23<00:00, 23.64s/it]
INFO:root:eval mean loss: 2267.071880800504
INFO:root:eval perplexity: 8.025793075561523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/56
 28%|██▊       | 56/200 [6:07:23<15:33:23, 388.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.610538258272
INFO:root:current train perplexity3.713188648223877
INFO:root:current mean train loss 1489.1377688780526
INFO:root:current train perplexity3.6925668716430664
INFO:root:current mean train loss 1491.5958096325635
INFO:root:current train perplexity3.6921796798706055
INFO:root:current mean train loss 1489.969399650552
INFO:root:current train perplexity3.6943016052246094
INFO:root:current mean train loss 1487.446169969512
INFO:root:current train perplexity3.688337802886963
INFO:root:current mean train loss 1486.949243341297
INFO:root:current train perplexity3.691572666168213
INFO:root:current mean train loss 1485.8038671349966
INFO:root:current train perplexity3.691896915435791
INFO:root:current mean train loss 1487.1930108813249
INFO:root:current train perplexity3.6963016986846924
INFO:root:current mean train loss 1487.9345218286392
INFO:root:current train perplexity3.696159839630127
INFO:root:current mean train loss 1488.4125062639655
INFO:root:current train perplexity3.699875593185425
INFO:root:current mean train loss 1489.5119272335498
INFO:root:current train perplexity3.6978671550750732
INFO:root:current mean train loss 1488.5363687868225
INFO:root:current train perplexity3.69771409034729
INFO:root:current mean train loss 1488.6527676296462
INFO:root:current train perplexity3.6983859539031982
INFO:root:current mean train loss 1488.5652035276241
INFO:root:current train perplexity3.6976308822631836
INFO:root:current mean train loss 1490.223688337246
INFO:root:current train perplexity3.7007150650024414
INFO:root:current mean train loss 1489.7746001193786
INFO:root:current train perplexity3.7022500038146973
INFO:root:current mean train loss 1490.1866191861702
INFO:root:current train perplexity3.7027995586395264
INFO:root:current mean train loss 1490.3662737503792
INFO:root:current train perplexity3.7026150226593018
INFO:root:current mean train loss 1490.8885953749661
INFO:root:current train perplexity3.7008190155029297
INFO:root:current mean train loss 1490.9057447002338
INFO:root:current train perplexity3.7021846771240234

100%|██████████| 1/1 [05:36<00:00, 336.15s/it][A100%|██████████| 1/1 [05:36<00:00, 336.15s/it]
INFO:root:final mean train loss: 1490.9084329840755
INFO:root:final train perplexity: 3.7017407417297363
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.37s/it][A100%|██████████| 1/1 [00:23<00:00, 23.37s/it]
INFO:root:eval mean loss: 1811.0287795912288
INFO:root:eval perplexity: 5.128087043762207
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.70s/it][A100%|██████████| 1/1 [00:22<00:00, 22.70s/it]
INFO:root:eval mean loss: 2267.518777617326
INFO:root:eval perplexity: 8.029088020324707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/57
 28%|██▊       | 57/200 [6:13:47<15:23:28, 387.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.6557276108686
INFO:root:current train perplexity3.688749313354492
INFO:root:current mean train loss 1490.6741165887743
INFO:root:current train perplexity3.677689552307129
INFO:root:current mean train loss 1485.8070086578825
INFO:root:current train perplexity3.6712422370910645
INFO:root:current mean train loss 1486.0775587662406
INFO:root:current train perplexity3.6738996505737305
INFO:root:current mean train loss 1486.3413850181123
INFO:root:current train perplexity3.6779873371124268
INFO:root:current mean train loss 1486.7269906057438
INFO:root:current train perplexity3.6801857948303223
INFO:root:current mean train loss 1487.0489688347914
INFO:root:current train perplexity3.6816177368164062
INFO:root:current mean train loss 1487.3751180966694
INFO:root:current train perplexity3.6787703037261963
INFO:root:current mean train loss 1485.4966845534364
INFO:root:current train perplexity3.6747634410858154
INFO:root:current mean train loss 1485.2045947618722
INFO:root:current train perplexity3.6736342906951904
INFO:root:current mean train loss 1486.018491709277
INFO:root:current train perplexity3.6773064136505127
INFO:root:current mean train loss 1486.2262566971451
INFO:root:current train perplexity3.6790926456451416
INFO:root:current mean train loss 1486.555849382178
INFO:root:current train perplexity3.6795830726623535
INFO:root:current mean train loss 1484.8666736089694
INFO:root:current train perplexity3.678028106689453
INFO:root:current mean train loss 1485.1043745243583
INFO:root:current train perplexity3.6798524856567383
INFO:root:current mean train loss 1485.2015420563366
INFO:root:current train perplexity3.6813197135925293
INFO:root:current mean train loss 1485.7353416827086
INFO:root:current train perplexity3.683155059814453
INFO:root:current mean train loss 1485.8872031647695
INFO:root:current train perplexity3.6831493377685547
INFO:root:current mean train loss 1485.4213531298096
INFO:root:current train perplexity3.6834092140197754
INFO:root:current mean train loss 1486.08594860294
INFO:root:current train perplexity3.6845290660858154

100%|██████████| 1/1 [05:32<00:00, 332.75s/it][A100%|██████████| 1/1 [05:32<00:00, 332.75s/it]
INFO:root:final mean train loss: 1485.7751065637508
INFO:root:final train perplexity: 3.6850974559783936
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.16s/it][A100%|██████████| 1/1 [00:23<00:00, 23.16s/it]
INFO:root:eval mean loss: 1813.7025748178469
INFO:root:eval perplexity: 5.140478610992432
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.40s/it][A100%|██████████| 1/1 [00:22<00:00, 22.40s/it]
INFO:root:eval mean loss: 2269.5734547283632
INFO:root:eval perplexity: 8.044256210327148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/58
 29%|██▉       | 58/200 [6:20:07<15:11:53, 385.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1471.6975126378677
INFO:root:current train perplexity3.634930372238159
INFO:root:current mean train loss 1467.0897414748733
INFO:root:current train perplexity3.6462948322296143
INFO:root:current mean train loss 1469.5623068290845
INFO:root:current train perplexity3.644653558731079
INFO:root:current mean train loss 1469.3828882787134
INFO:root:current train perplexity3.650764226913452
INFO:root:current mean train loss 1473.0182229582797
INFO:root:current train perplexity3.6564645767211914
INFO:root:current mean train loss 1474.4648243439503
INFO:root:current train perplexity3.6578476428985596
INFO:root:current mean train loss 1474.893253164918
INFO:root:current train perplexity3.658651828765869
INFO:root:current mean train loss 1473.3905848800757
INFO:root:current train perplexity3.655334949493408
INFO:root:current mean train loss 1473.9575333245057
INFO:root:current train perplexity3.6553022861480713
INFO:root:current mean train loss 1474.1531874603427
INFO:root:current train perplexity3.657932996749878
INFO:root:current mean train loss 1473.6183267479119
INFO:root:current train perplexity3.6593687534332275
INFO:root:current mean train loss 1474.5398046050896
INFO:root:current train perplexity3.65898060798645
INFO:root:current mean train loss 1476.2617496238145
INFO:root:current train perplexity3.6627118587493896
INFO:root:current mean train loss 1476.739259134561
INFO:root:current train perplexity3.6621687412261963
INFO:root:current mean train loss 1477.5261782045718
INFO:root:current train perplexity3.6637978553771973
INFO:root:current mean train loss 1478.671068180698
INFO:root:current train perplexity3.664025068283081
INFO:root:current mean train loss 1479.2647869528932
INFO:root:current train perplexity3.6677987575531006
INFO:root:current mean train loss 1479.9825358756784
INFO:root:current train perplexity3.667719841003418
INFO:root:current mean train loss 1481.0123074721278
INFO:root:current train perplexity3.668829917907715

100%|██████████| 1/1 [05:34<00:00, 334.13s/it][A100%|██████████| 1/1 [05:34<00:00, 334.13s/it]
INFO:root:final mean train loss: 1481.1315407034008
INFO:root:final train perplexity: 3.6701061725616455
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.33s/it][A100%|██████████| 1/1 [00:23<00:00, 23.33s/it]
INFO:root:eval mean loss: 1811.4016048350234
INFO:root:eval perplexity: 5.129813194274902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.63s/it][A100%|██████████| 1/1 [00:21<00:00, 21.63s/it]
INFO:root:eval mean loss: 2269.3317905723625
INFO:root:eval perplexity: 8.042470932006836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/59
 30%|██▉       | 59/200 [6:26:28<15:02:28, 384.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1435.9061889648438
INFO:root:current train perplexity3.753941535949707
INFO:root:current mean train loss 1482.9046020507812
INFO:root:current train perplexity3.6484158039093018
INFO:root:current mean train loss 1476.7407752310876
INFO:root:current train perplexity3.6608362197875977
INFO:root:current mean train loss 1471.1365695978632
INFO:root:current train perplexity3.6506359577178955
INFO:root:current mean train loss 1472.4568742590757
INFO:root:current train perplexity3.6549220085144043
INFO:root:current mean train loss 1475.9006826697118
INFO:root:current train perplexity3.6573169231414795
INFO:root:current mean train loss 1475.3314799058478
INFO:root:current train perplexity3.6522977352142334
INFO:root:current mean train loss 1477.2593430739182
INFO:root:current train perplexity3.652324914932251
INFO:root:current mean train loss 1476.7324862587184
INFO:root:current train perplexity3.648711919784546
INFO:root:current mean train loss 1476.2650838035702
INFO:root:current train perplexity3.6499788761138916
INFO:root:current mean train loss 1475.5937207616018
INFO:root:current train perplexity3.6478707790374756
INFO:root:current mean train loss 1474.9763870377722
INFO:root:current train perplexity3.6481688022613525
INFO:root:current mean train loss 1474.8138572959456
INFO:root:current train perplexity3.6507842540740967
INFO:root:current mean train loss 1474.8268266669068
INFO:root:current train perplexity3.650080680847168
INFO:root:current mean train loss 1474.7277757152171
INFO:root:current train perplexity3.6517419815063477
INFO:root:current mean train loss 1475.2766706565724
INFO:root:current train perplexity3.650261163711548
INFO:root:current mean train loss 1475.9470197318049
INFO:root:current train perplexity3.6517996788024902
INFO:root:current mean train loss 1476.1508279838517
INFO:root:current train perplexity3.6532156467437744
INFO:root:current mean train loss 1476.8738946607718
INFO:root:current train perplexity3.6536154747009277
INFO:root:current mean train loss 1476.531394084044
INFO:root:current train perplexity3.653693199157715

100%|██████████| 1/1 [05:45<00:00, 345.21s/it][A100%|██████████| 1/1 [05:45<00:00, 345.21s/it]
INFO:root:final mean train loss: 1476.3357776943867
INFO:root:final train perplexity: 3.654686689376831
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.80s/it][A100%|██████████| 1/1 [00:23<00:00, 23.80s/it]
INFO:root:eval mean loss: 1816.2899412331005
INFO:root:eval perplexity: 5.152499198913574
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.49s/it][A100%|██████████| 1/1 [00:23<00:00, 23.49s/it]
INFO:root:eval mean loss: 2275.5428843396776
INFO:root:eval perplexity: 8.08849048614502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/60
 30%|███       | 60/200 [6:33:03<15:03:19, 387.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1434.7449822676808
INFO:root:current train perplexity3.597503662109375
INFO:root:current mean train loss 1463.5179074071034
INFO:root:current train perplexity3.604968786239624
INFO:root:current mean train loss 1466.9589971951698
INFO:root:current train perplexity3.6069767475128174
INFO:root:current mean train loss 1463.4239727725803
INFO:root:current train perplexity3.619952440261841
INFO:root:current mean train loss 1464.457344437556
INFO:root:current train perplexity3.625641107559204
INFO:root:current mean train loss 1463.375896828712
INFO:root:current train perplexity3.6272621154785156
INFO:root:current mean train loss 1465.8233447344508
INFO:root:current train perplexity3.6290202140808105
INFO:root:current mean train loss 1466.9755533401426
INFO:root:current train perplexity3.6273624897003174
INFO:root:current mean train loss 1465.8400434743207
INFO:root:current train perplexity3.6253082752227783
INFO:root:current mean train loss 1468.0441525265235
INFO:root:current train perplexity3.6267242431640625
INFO:root:current mean train loss 1468.3674127131378
INFO:root:current train perplexity3.630075693130493
INFO:root:current mean train loss 1469.2355954849475
INFO:root:current train perplexity3.633458137512207
INFO:root:current mean train loss 1468.6307334993783
INFO:root:current train perplexity3.6328577995300293
INFO:root:current mean train loss 1469.7233506348027
INFO:root:current train perplexity3.633134603500366
INFO:root:current mean train loss 1470.6593482976568
INFO:root:current train perplexity3.6332449913024902
INFO:root:current mean train loss 1470.4357596743334
INFO:root:current train perplexity3.6342499256134033
INFO:root:current mean train loss 1471.3526400966066
INFO:root:current train perplexity3.634751319885254
INFO:root:current mean train loss 1471.3720208878708
INFO:root:current train perplexity3.6360809803009033
INFO:root:current mean train loss 1471.3649760744872
INFO:root:current train perplexity3.6358108520507812
INFO:root:current mean train loss 1470.8828321559283
INFO:root:current train perplexity3.6359622478485107

100%|██████████| 1/1 [05:33<00:00, 333.53s/it][A100%|██████████| 1/1 [05:33<00:00, 333.53s/it]
INFO:root:final mean train loss: 1471.0583589662522
INFO:root:final train perplexity: 3.6377956867218018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.70s/it][A100%|██████████| 1/1 [00:23<00:00, 23.70s/it]
INFO:root:eval mean loss: 1816.275416597407
INFO:root:eval perplexity: 5.152430057525635
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.90s/it][A100%|██████████| 1/1 [00:22<00:00, 22.90s/it]
INFO:root:eval mean loss: 2276.7202347559287
INFO:root:eval perplexity: 8.097245216369629
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/61
 30%|███       | 61/200 [6:39:25<14:53:20, 385.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1456.0467427571614
INFO:root:current train perplexity3.5863702297210693
INFO:root:current mean train loss 1469.413958381204
INFO:root:current train perplexity3.620286464691162
INFO:root:current mean train loss 1465.3843285512116
INFO:root:current train perplexity3.611116409301758
INFO:root:current mean train loss 1463.5906204950243
INFO:root:current train perplexity3.6115963459014893
INFO:root:current mean train loss 1464.0640558365287
INFO:root:current train perplexity3.6104328632354736
INFO:root:current mean train loss 1463.7822279289587
INFO:root:current train perplexity3.613495111465454
INFO:root:current mean train loss 1464.0846281231575
INFO:root:current train perplexity3.6150107383728027
INFO:root:current mean train loss 1464.5269383969514
INFO:root:current train perplexity3.618706703186035
INFO:root:current mean train loss 1463.892828252327
INFO:root:current train perplexity3.613745927810669
INFO:root:current mean train loss 1463.8655765892095
INFO:root:current train perplexity3.612724542617798
INFO:root:current mean train loss 1464.1559815867067
INFO:root:current train perplexity3.614400625228882
INFO:root:current mean train loss 1464.7566969965544
INFO:root:current train perplexity3.6158931255340576
INFO:root:current mean train loss 1465.8323191423633
INFO:root:current train perplexity3.619455337524414
INFO:root:current mean train loss 1466.4547867460878
INFO:root:current train perplexity3.6209070682525635
INFO:root:current mean train loss 1466.4608189999892
INFO:root:current train perplexity3.619617462158203
INFO:root:current mean train loss 1467.33980401357
INFO:root:current train perplexity3.6220874786376953
INFO:root:current mean train loss 1466.772890496662
INFO:root:current train perplexity3.622535228729248
INFO:root:current mean train loss 1467.4303799818188
INFO:root:current train perplexity3.624518394470215
INFO:root:current mean train loss 1468.0583403011813
INFO:root:current train perplexity3.624972343444824
INFO:root:current mean train loss 1467.6344535134056
INFO:root:current train perplexity3.625016927719116

100%|██████████| 1/1 [05:33<00:00, 333.41s/it][A100%|██████████| 1/1 [05:33<00:00, 333.41s/it]
INFO:root:final mean train loss: 1466.9827300979223
INFO:root:final train perplexity: 3.6248035430908203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.83s/it][A100%|██████████| 1/1 [00:23<00:00, 23.83s/it]
INFO:root:eval mean loss: 1823.0661820215537
INFO:root:eval perplexity: 5.184110641479492
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.82s/it][A100%|██████████| 1/1 [00:22<00:00, 22.82s/it]
INFO:root:eval mean loss: 2287.5342130187555
INFO:root:eval perplexity: 8.17808723449707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/62
 31%|███       | 62/200 [6:45:47<14:44:26, 384.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1447.430295548349
INFO:root:current train perplexity3.5886480808258057
INFO:root:current mean train loss 1449.8196933721406
INFO:root:current train perplexity3.582676649093628
INFO:root:current mean train loss 1452.3383451318552
INFO:root:current train perplexity3.5849292278289795
INFO:root:current mean train loss 1457.3766557299043
INFO:root:current train perplexity3.5919530391693115
INFO:root:current mean train loss 1457.3036756368135
INFO:root:current train perplexity3.597111701965332
INFO:root:current mean train loss 1459.1892407712194
INFO:root:current train perplexity3.5994787216186523
INFO:root:current mean train loss 1460.049483528546
INFO:root:current train perplexity3.59968638420105
INFO:root:current mean train loss 1458.5916054207648
INFO:root:current train perplexity3.5954761505126953
INFO:root:current mean train loss 1458.9656736277752
INFO:root:current train perplexity3.599672555923462
INFO:root:current mean train loss 1460.1400991882133
INFO:root:current train perplexity3.599246025085449
INFO:root:current mean train loss 1460.0288822069015
INFO:root:current train perplexity3.60042405128479
INFO:root:current mean train loss 1461.233702819242
INFO:root:current train perplexity3.6006252765655518
INFO:root:current mean train loss 1460.1523929483303
INFO:root:current train perplexity3.604006767272949
INFO:root:current mean train loss 1460.688102502252
INFO:root:current train perplexity3.604710578918457
INFO:root:current mean train loss 1461.3471947687704
INFO:root:current train perplexity3.607511520385742
INFO:root:current mean train loss 1461.6442359388834
INFO:root:current train perplexity3.6083455085754395
INFO:root:current mean train loss 1461.1977143238619
INFO:root:current train perplexity3.607048511505127
INFO:root:current mean train loss 1461.1297289189515
INFO:root:current train perplexity3.6069748401641846
INFO:root:current mean train loss 1462.0908964005794
INFO:root:current train perplexity3.607916831970215
INFO:root:current mean train loss 1462.12844734563
INFO:root:current train perplexity3.608672618865967

100%|██████████| 1/1 [05:41<00:00, 341.84s/it][A100%|██████████| 1/1 [05:41<00:00, 341.84s/it]
INFO:root:final mean train loss: 1461.8812423913814
INFO:root:final train perplexity: 3.6086063385009766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.11s/it][A100%|██████████| 1/1 [00:23<00:00, 23.11s/it]
INFO:root:eval mean loss: 1822.8139025099733
INFO:root:eval perplexity: 5.182929515838623
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.68s/it][A100%|██████████| 1/1 [00:22<00:00, 22.68s/it]
INFO:root:eval mean loss: 2288.6648057437114
INFO:root:eval perplexity: 8.1865873336792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/63
 32%|███▏      | 63/200 [6:52:16<14:41:28, 386.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1454.6707467215401
INFO:root:current train perplexity3.5697765350341797
INFO:root:current mean train loss 1457.284120088465
INFO:root:current train perplexity3.5923542976379395
INFO:root:current mean train loss 1458.5660680700232
INFO:root:current train perplexity3.5924453735351562
INFO:root:current mean train loss 1459.8494417757602
INFO:root:current train perplexity3.5891289710998535
INFO:root:current mean train loss 1459.5602474131483
INFO:root:current train perplexity3.583024740219116
INFO:root:current mean train loss 1458.134175832648
INFO:root:current train perplexity3.5851540565490723
INFO:root:current mean train loss 1458.88084315969
INFO:root:current train perplexity3.5848190784454346
INFO:root:current mean train loss 1456.3860608385755
INFO:root:current train perplexity3.582108497619629
INFO:root:current mean train loss 1456.5826421628053
INFO:root:current train perplexity3.5819945335388184
INFO:root:current mean train loss 1456.7750276860502
INFO:root:current train perplexity3.5860097408294678
INFO:root:current mean train loss 1455.9238695376387
INFO:root:current train perplexity3.5836403369903564
INFO:root:current mean train loss 1453.9176589835404
INFO:root:current train perplexity3.5808448791503906
INFO:root:current mean train loss 1455.1054865318958
INFO:root:current train perplexity3.5836565494537354
INFO:root:current mean train loss 1454.6280039098142
INFO:root:current train perplexity3.5819778442382812
INFO:root:current mean train loss 1455.8277680066167
INFO:root:current train perplexity3.5852811336517334
INFO:root:current mean train loss 1456.195817186878
INFO:root:current train perplexity3.5874404907226562
INFO:root:current mean train loss 1456.627066935465
INFO:root:current train perplexity3.589179277420044
INFO:root:current mean train loss 1455.7426226082496
INFO:root:current train perplexity3.5886197090148926
INFO:root:current mean train loss 1455.5959280738218
INFO:root:current train perplexity3.588654041290283
INFO:root:current mean train loss 1457.1262373096447
INFO:root:current train perplexity3.5918843746185303

100%|██████████| 1/1 [05:33<00:00, 333.41s/it][A100%|██████████| 1/1 [05:33<00:00, 333.41s/it]
INFO:root:final mean train loss: 1456.7057685640443
INFO:root:final train perplexity: 3.59224796295166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.56s/it][A100%|██████████| 1/1 [00:24<00:00, 24.56s/it]
INFO:root:eval mean loss: 1824.6832708264074
INFO:root:eval perplexity: 5.191683292388916
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.17s/it][A100%|██████████| 1/1 [00:23<00:00, 23.17s/it]
INFO:root:eval mean loss: 2290.075964528618
INFO:root:eval perplexity: 8.19720458984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/64
 32%|███▏      | 64/200 [6:58:39<14:33:05, 385.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1453.4662482601473
INFO:root:current train perplexity3.5685572624206543
INFO:root:current mean train loss 1441.194466493984
INFO:root:current train perplexity3.5561165809631348
INFO:root:current mean train loss 1439.401684059914
INFO:root:current train perplexity3.543158531188965
INFO:root:current mean train loss 1440.8264049756742
INFO:root:current train perplexity3.5451831817626953
INFO:root:current mean train loss 1445.5770008000995
INFO:root:current train perplexity3.5498549938201904
INFO:root:current mean train loss 1445.745816544333
INFO:root:current train perplexity3.5501303672790527
INFO:root:current mean train loss 1447.8668207560045
INFO:root:current train perplexity3.5541489124298096
INFO:root:current mean train loss 1447.386751943198
INFO:root:current train perplexity3.5553219318389893
INFO:root:current mean train loss 1449.7035111660618
INFO:root:current train perplexity3.5618951320648193
INFO:root:current mean train loss 1450.2320932622135
INFO:root:current train perplexity3.5631866455078125
INFO:root:current mean train loss 1449.9392974769291
INFO:root:current train perplexity3.564976215362549
INFO:root:current mean train loss 1450.704109172612
INFO:root:current train perplexity3.566377639770508
INFO:root:current mean train loss 1450.98815898999
INFO:root:current train perplexity3.5695769786834717
INFO:root:current mean train loss 1451.5503981410811
INFO:root:current train perplexity3.5728423595428467
INFO:root:current mean train loss 1450.9891996095064
INFO:root:current train perplexity3.5727732181549072
INFO:root:current mean train loss 1450.9930862974804
INFO:root:current train perplexity3.5714659690856934
INFO:root:current mean train loss 1450.670201037576
INFO:root:current train perplexity3.572680950164795
INFO:root:current mean train loss 1451.7540183470246
INFO:root:current train perplexity3.5740513801574707
INFO:root:current mean train loss 1452.060174777259
INFO:root:current train perplexity3.5759737491607666

100%|██████████| 1/1 [05:40<00:00, 340.75s/it][A100%|██████████| 1/1 [05:40<00:00, 340.75s/it]
INFO:root:final mean train loss: 1452.2809510416175
INFO:root:final train perplexity: 3.578321695327759
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.61s/it][A100%|██████████| 1/1 [00:24<00:00, 24.61s/it]
INFO:root:eval mean loss: 1825.4195547983156
INFO:root:eval perplexity: 5.195135116577148
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.99s/it][A100%|██████████| 1/1 [00:22<00:00, 22.99s/it]
INFO:root:eval mean loss: 2293.320223760943
INFO:root:eval perplexity: 8.221672058105469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_distilroberta_gpt2_final/65
 32%|███▎      | 65/200 [7:05:10<14:30:22, 386.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.3309936523438
INFO:root:current train perplexity3.4250588417053223
INFO:root:current mean train loss 1424.9322098952073
INFO:root:current train perplexity3.5283873081207275
INFO:root:current mean train loss 1429.69288186466
INFO:root:current train perplexity3.524655818939209
INFO:root:current mean train loss 1435.894511172646
INFO:root:current train perplexity3.5301876068115234
INFO:root:current mean train loss 1434.312015646755
INFO:root:current train perplexity3.5311760902404785
INFO:root:current mean train loss 1437.169138106089
INFO:root:current train perplexity3.5360541343688965
INFO:root:current mean train loss 1438.682215204302
INFO:root:current train perplexity3.534579038619995
INFO:root:current mean train loss 1441.368952144276
INFO:root:current train perplexity3.534543752670288
slurmstepd: error: *** JOB 26260772 ON ga018 CANCELLED AT 2022-10-25T09:41:51 ***
