INFO:root:Output: big_baseline_roberta_64_low
INFO:root:Steps per epochs:496
INFO:root:Total steps:99200
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][A/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
INFO:root:current mean train loss 45848.598879419194
INFO:root:current train perplexity8509.5107421875
INFO:root:current mean train loss 34294.3268294598
INFO:root:current train perplexity870.959228515625
INFO:root:current mean train loss 28502.40125614026
INFO:root:current train perplexity277.6824645996094
INFO:root:current mean train loss 25112.61498227992
INFO:root:current train perplexity142.17556762695312


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:07<00:00, 367.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:07<00:00, 367.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 13444.95345342727
INFO:root:eval perplexity: 16.40485191345215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/1

  0%|          | 1/200 [06:52<22:47:21, 412.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 13872.997395833334
INFO:root:current train perplexity14.561151504516602
INFO:root:current mean train loss 12847.263946829491
INFO:root:current train perplexity12.673039436340332
INFO:root:current mean train loss 12574.738714208745
INFO:root:current train perplexity11.972993850708008
INFO:root:current mean train loss 12337.92600041254
INFO:root:current train perplexity11.420578956604004
INFO:root:current mean train loss 12168.371980652915
INFO:root:current train perplexity11.029142379760742


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it]
INFO:root:eval mean loss: 12197.545006161645
INFO:root:eval perplexity: 12.654594421386719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/2

  1%|          | 2/200 [13:36<22:24:53, 407.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11145.353097098214
INFO:root:current train perplexity9.110595703125
INFO:root:current mean train loss 11074.13257520444
INFO:root:current train perplexity8.882027626037598
INFO:root:current mean train loss 10998.923648852657
INFO:root:current train perplexity8.745026588439941
INFO:root:current mean train loss 10943.948830669788
INFO:root:current train perplexity8.65322494506836
INFO:root:current mean train loss 10868.409321253072
INFO:root:current train perplexity8.528610229492188


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.96s/it]
INFO:root:eval mean loss: 11792.374645414806
INFO:root:eval perplexity: 11.631467819213867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/3

  2%|â–         | 3/200 [20:21<22:13:41, 406.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10481.92178622159
INFO:root:current train perplexity7.8846893310546875
INFO:root:current mean train loss 10399.73366237331
INFO:root:current train perplexity7.8274102210998535
INFO:root:current mean train loss 10353.785244186907
INFO:root:current train perplexity7.735228061676025
INFO:root:current mean train loss 10325.391585862139
INFO:root:current train perplexity7.676682472229004
INFO:root:current mean train loss 10301.235247015664
INFO:root:current train perplexity7.627766132354736


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.84s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 11587.598871140253
INFO:root:eval perplexity: 11.146272659301758
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/4

  2%|â–         | 4/200 [27:05<22:05:13, 405.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9937.443424479166
INFO:root:current train perplexity7.2181901931762695
INFO:root:current mean train loss 9987.603668478261
INFO:root:current train perplexity7.180302143096924
INFO:root:current mean train loss 9991.462954215116
INFO:root:current train perplexity7.171892166137695
INFO:root:current mean train loss 9966.754287574404
INFO:root:current train perplexity7.147174835205078
INFO:root:current mean train loss 9956.850091773344
INFO:root:current train perplexity7.124361038208008


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it]
INFO:root:eval mean loss: 11469.823954264322
INFO:root:eval perplexity: 10.87643814086914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/5

  2%|â–Ž         | 5/200 [34:25<22:38:04, 417.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9767.064915707237
INFO:root:current train perplexity6.871643543243408
INFO:root:current mean train loss 9755.06647190126
INFO:root:current train perplexity6.850436210632324
INFO:root:current mean train loss 9728.026924586187
INFO:root:current train perplexity6.832061290740967
INFO:root:current mean train loss 9711.180596713362
INFO:root:current train perplexity6.800513744354248
INFO:root:current mean train loss 9690.84527660725
INFO:root:current train perplexity6.776546001434326


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.37s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it]
INFO:root:eval mean loss: 11372.679048084077
INFO:root:eval perplexity: 10.658796310424805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/6

  3%|â–Ž         | 6/200 [41:09<22:16:03, 413.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9539.510190217392
INFO:root:current train perplexity6.627383708953857
INFO:root:current mean train loss 9538.11406726372
INFO:root:current train perplexity6.558390140533447
INFO:root:current mean train loss 9522.056180808577
INFO:root:current train perplexity6.544495582580566
INFO:root:current mean train loss 9499.355834583012
INFO:root:current train perplexity6.519440174102783
INFO:root:current mean train loss 9499.789074043292
INFO:root:current train perplexity6.518243312835693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.64s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11308.694338843936
INFO:root:eval perplexity: 10.517827987670898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/7

  4%|â–Ž         | 7/200 [47:54<21:59:57, 410.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9393.254448784723
INFO:root:current train perplexity6.302495956420898
INFO:root:current mean train loss 9390.353138841043
INFO:root:current train perplexity6.369766712188721
INFO:root:current mean train loss 9370.979655664923
INFO:root:current train perplexity6.355498790740967
INFO:root:current mean train loss 9355.36931682435
INFO:root:current train perplexity6.326622486114502
INFO:root:current mean train loss 9343.385323660714
INFO:root:current train perplexity6.319666385650635


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.19s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.97s/it]
INFO:root:eval mean loss: 11255.242489769345
INFO:root:eval perplexity: 10.401496887207031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/8

  4%|â–         | 8/200 [54:39<21:47:44, 408.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9159.726814516129
INFO:root:current train perplexity6.087366104125977
INFO:root:current mean train loss 9212.054083671279
INFO:root:current train perplexity6.15157413482666
INFO:root:current mean train loss 9230.22955560065
INFO:root:current train perplexity6.175869941711426
INFO:root:current mean train loss 9242.418476208459
INFO:root:current train perplexity6.182352066040039
INFO:root:current mean train loss 9229.124286271026
INFO:root:current train perplexity6.1705241203308105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.91s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.85s/it]
INFO:root:eval mean loss: 11225.84429640997
INFO:root:eval perplexity: 10.338061332702637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/9

  4%|â–         | 9/200 [1:01:23<21:36:54, 407.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9246.182254464286
INFO:root:current train perplexity6.061841011047363
INFO:root:current mean train loss 9118.653559027778
INFO:root:current train perplexity6.0370330810546875
INFO:root:current mean train loss 9100.052393617021
INFO:root:current train perplexity6.019972324371338
INFO:root:current mean train loss 9102.543094099814
INFO:root:current train perplexity6.0203857421875
INFO:root:current mean train loss 9101.361741199713
INFO:root:current train perplexity6.022628307342529


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.04s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11175.196492513021
INFO:root:eval perplexity: 10.229687690734863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/10

  5%|â–Œ         | 10/200 [1:08:08<21:27:38, 406.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 9022.668795072115
INFO:root:current train perplexity5.96384334564209
INFO:root:current mean train loss 9001.316536224145
INFO:root:current train perplexity5.909878730773926
INFO:root:current mean train loss 9010.27443244966
INFO:root:current train perplexity5.920071125030518
INFO:root:current mean train loss 9012.591029164361
INFO:root:current train perplexity5.912434101104736
INFO:root:current mean train loss 9010.238240096454
INFO:root:current train perplexity5.9107465744018555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.47s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11154.738133021763
INFO:root:eval perplexity: 10.186229705810547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/11

  6%|â–Œ         | 11/200 [1:14:53<21:19:30, 406.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8922.764080668605
INFO:root:current train perplexity5.83114767074585
INFO:root:current mean train loss 8921.193605222903
INFO:root:current train perplexity5.808474063873291
INFO:root:current mean train loss 8921.262052308384
INFO:root:current train perplexity5.804893970489502
INFO:root:current mean train loss 8908.062619579081
INFO:root:current train perplexity5.803829669952393
INFO:root:current mean train loss 8910.156452807561
INFO:root:current train perplexity5.79720401763916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11134.63295200893
INFO:root:eval perplexity: 10.143709182739258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/12

  6%|â–Œ         | 12/200 [1:21:38<21:10:52, 405.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8844.324042137632
INFO:root:current train perplexity5.686923503875732
INFO:root:current mean train loss 8851.352462664754
INFO:root:current train perplexity5.73134708404541
INFO:root:current mean train loss 8856.017781740258
INFO:root:current train perplexity5.723918437957764
INFO:root:current mean train loss 8852.046278368156
INFO:root:current train perplexity5.727949142456055
INFO:root:current mean train loss 8840.460605425056
INFO:root:current train perplexity5.715036392211914


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.80s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11135.715927850633
INFO:root:eval perplexity: 10.145992279052734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/13

  6%|â–‹         | 13/200 [1:28:22<21:03:10, 405.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8739.664100796568
INFO:root:current train perplexity5.637673854827881
INFO:root:current mean train loss 8754.24233301428
INFO:root:current train perplexity5.633341312408447
INFO:root:current mean train loss 8746.72051053598
INFO:root:current train perplexity5.639307975769043
INFO:root:current mean train loss 8741.397203581286
INFO:root:current train perplexity5.626418113708496
INFO:root:current mean train loss 8762.173974284577
INFO:root:current train perplexity5.631553649902344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.76s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11102.51506405785
INFO:root:eval perplexity: 10.076144218444824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/14

  7%|â–‹         | 14/200 [1:35:07<20:55:46, 405.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8675.52567471591
INFO:root:current train perplexity5.536098957061768
INFO:root:current mean train loss 8651.273220136089
INFO:root:current train perplexity5.520711898803711
INFO:root:current mean train loss 8675.726384420956
INFO:root:current train perplexity5.544249057769775
INFO:root:current mean train loss 8688.577790768046
INFO:root:current train perplexity5.552850246429443
INFO:root:current mean train loss 8694.898262577266
INFO:root:current train perplexity5.558777809143066


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11092.957752046132
INFO:root:eval perplexity: 10.05612564086914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/15

  8%|â–Š         | 15/200 [1:41:52<20:48:46, 405.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8680.975776284427
INFO:root:current train perplexity5.5396223068237305
INFO:root:current mean train loss 8628.288589573507
INFO:root:current train perplexity5.490766525268555
INFO:root:current mean train loss 8619.086252337716
INFO:root:current train perplexity5.4815778732299805
INFO:root:current mean train loss 8629.921200383009
INFO:root:current train perplexity5.485833168029785
INFO:root:current mean train loss 8636.737500425517
INFO:root:current train perplexity5.4884796142578125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.56s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11084.471025739398
INFO:root:eval perplexity: 10.038382530212402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/16

  8%|â–Š         | 16/200 [1:48:37<20:42:22, 405.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8576.304485987102
INFO:root:current train perplexity5.398524761199951
INFO:root:current mean train loss 8572.592743481595
INFO:root:current train perplexity5.42724609375
INFO:root:current mean train loss 8562.774945045152
INFO:root:current train perplexity5.419309616088867
INFO:root:current mean train loss 8575.283839370266
INFO:root:current train perplexity5.4248127937316895
INFO:root:current mean train loss 8575.452482746692
INFO:root:current train perplexity5.424166202545166


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.53s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11074.431602841332
INFO:root:eval perplexity: 10.017434120178223
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/17

  8%|â–Š         | 17/200 [1:55:21<20:34:48, 404.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8535.053404850747
INFO:root:current train perplexity5.38483190536499
INFO:root:current mean train loss 8523.097910624065
INFO:root:current train perplexity5.36380672454834
INFO:root:current mean train loss 8514.920614978348
INFO:root:current train perplexity5.359776020050049
INFO:root:current mean train loss 8520.877985567098
INFO:root:current train perplexity5.365406513214111
INFO:root:current mean train loss 8518.86158638082
INFO:root:current train perplexity5.364139556884766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.69s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11077.090858096168
INFO:root:eval perplexity: 10.022978782653809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/18

  9%|â–‰         | 18/200 [2:02:06<20:27:47, 404.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8439.142915107834
INFO:root:current train perplexity5.303255558013916
INFO:root:current mean train loss 8436.27338895742
INFO:root:current train perplexity5.292335510253906
INFO:root:current mean train loss 8445.205995228898
INFO:root:current train perplexity5.301327228546143
INFO:root:current mean train loss 8450.032037040937
INFO:root:current train perplexity5.306228160858154
INFO:root:current mean train loss 8459.342861556197
INFO:root:current train perplexity5.307582378387451


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 11051.975179036459
INFO:root:eval perplexity: 9.970739364624023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/19
##################best#############
 10%|â–‰         | 19/200 [2:08:49<20:19:53, 404.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8455.157395833334
INFO:root:current train perplexity5.2621941566467285
INFO:root:current mean train loss 8435.95365234375
INFO:root:current train perplexity5.258589267730713
INFO:root:current mean train loss 8416.575072798296
INFO:root:current train perplexity5.2566399574279785
INFO:root:current mean train loss 8405.637561197917
INFO:root:current train perplexity5.249172687530518
INFO:root:current mean train loss 8413.596745476974
INFO:root:current train perplexity5.254985332489014


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.39s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11075.302080426898
INFO:root:eval perplexity: 10.01924991607666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/20

 10%|â–ˆ         | 20/200 [2:15:34<20:13:00, 404.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8395.119950306567
INFO:root:current train perplexity5.223801612854004
INFO:root:current mean train loss 8385.172947036488
INFO:root:current train perplexity5.210536003112793
INFO:root:current mean train loss 8369.565216173834
INFO:root:current train perplexity5.204015254974365
INFO:root:current mean train loss 8379.293383595812
INFO:root:current train perplexity5.209768295288086
INFO:root:current mean train loss 8360.86831289144
INFO:root:current train perplexity5.201741695404053


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11058.638689313617
INFO:root:eval perplexity: 9.984569549560547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/21

 10%|â–ˆ         | 21/200 [2:22:18<20:06:16, 404.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8365.21187876506
INFO:root:current train perplexity5.158052921295166
INFO:root:current mean train loss 8341.419759114584
INFO:root:current train perplexity5.162295818328857
INFO:root:current mean train loss 8331.036008585468
INFO:root:current train perplexity5.170507907867432
INFO:root:current mean train loss 8322.979292030433
INFO:root:current train perplexity5.156801700592041
INFO:root:current mean train loss 8317.265123576604
INFO:root:current train perplexity5.157732009887695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.23s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11082.785792759487
INFO:root:eval perplexity: 10.034862518310547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/22

 11%|â–ˆ         | 22/200 [2:29:02<19:59:13, 404.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8203.075335623204
INFO:root:current train perplexity5.051706314086914
INFO:root:current mean train loss 8259.648429666611
INFO:root:current train perplexity5.091705799102783
INFO:root:current mean train loss 8269.837512930097
INFO:root:current train perplexity5.1007819175720215
INFO:root:current mean train loss 8270.683135749758
INFO:root:current train perplexity5.107834815979004
INFO:root:current mean train loss 8273.045645774513
INFO:root:current train perplexity5.113671779632568


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.62s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11075.062043689546
INFO:root:eval perplexity: 10.018750190734863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/23

 12%|â–ˆâ–        | 23/200 [2:35:47<19:53:30, 404.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8184.509411486951
INFO:root:current train perplexity5.06487512588501
INFO:root:current mean train loss 8210.839314565608
INFO:root:current train perplexity5.059179782867432
INFO:root:current mean train loss 8223.219166129726
INFO:root:current train perplexity5.068085193634033
INFO:root:current mean train loss 8221.64493336397
INFO:root:current train perplexity5.066511631011963
INFO:root:current mean train loss 8230.752702949974
INFO:root:current train perplexity5.071303844451904


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.22s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11074.767589750743
INFO:root:eval perplexity: 10.018136978149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/24

 12%|â–ˆâ–        | 24/200 [2:42:32<19:47:10, 404.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8207.907622327302
INFO:root:current train perplexity5.02112340927124
INFO:root:current mean train loss 8200.974904847757
INFO:root:current train perplexity5.023223400115967
INFO:root:current mean train loss 8203.780604475636
INFO:root:current train perplexity5.024191856384277
INFO:root:current mean train loss 8188.945664804193
INFO:root:current train perplexity5.02150821685791
INFO:root:current mean train loss 8187.892986505682
INFO:root:current train perplexity5.027867794036865


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11081.866158621651
INFO:root:eval perplexity: 10.032944679260254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/25

 12%|â–ˆâ–Ž        | 25/200 [2:49:17<19:40:17, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8153.391147806187
INFO:root:current train perplexity4.965052127838135
INFO:root:current mean train loss 8144.51712174152
INFO:root:current train perplexity4.973572254180908
INFO:root:current mean train loss 8139.4422733983065
INFO:root:current train perplexity4.979377269744873
INFO:root:current mean train loss 8149.277460007441
INFO:root:current train perplexity4.98765754699707


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it]
INFO:root:eval mean loss: 11101.163359142485
INFO:root:eval perplexity: 10.073311805725098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/26

 13%|â–ˆâ–Ž        | 26/200 [2:56:01<19:33:25, 404.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7997.245930989583
INFO:root:current train perplexity4.880693435668945
INFO:root:current mean train loss 8108.320791300061
INFO:root:current train perplexity4.941662311553955
INFO:root:current mean train loss 8125.792138912408
INFO:root:current train perplexity4.942120552062988
INFO:root:current mean train loss 8113.450596573329
INFO:root:current train perplexity4.944541931152344
INFO:root:current mean train loss 8113.815738649581
INFO:root:current train perplexity4.953579902648926


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.95s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11095.394374302456
INFO:root:eval perplexity: 10.061224937438965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/27

 14%|â–ˆâ–Ž        | 27/200 [3:02:46<19:26:47, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8033.012974330357
INFO:root:current train perplexity4.8858466148376465
INFO:root:current mean train loss 8095.991900007301
INFO:root:current train perplexity4.903914928436279
INFO:root:current mean train loss 8084.524475392512
INFO:root:current train perplexity4.919171333312988
INFO:root:current mean train loss 8079.9444981677525
INFO:root:current train perplexity4.91642951965332
INFO:root:current mean train loss 8078.224160684122
INFO:root:current train perplexity4.919074535369873


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11109.444620768229
INFO:root:eval perplexity: 10.090683937072754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/28

 14%|â–ˆâ–        | 28/200 [3:09:31<19:19:49, 404.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8130.522638494318
INFO:root:current train perplexity4.905398845672607
INFO:root:current mean train loss 8072.185516082489
INFO:root:current train perplexity4.889158248901367
INFO:root:current mean train loss 8047.599190517624
INFO:root:current train perplexity4.89042854309082
INFO:root:current mean train loss 8042.133663459606
INFO:root:current train perplexity4.893458366394043
INFO:root:current mean train loss 8032.786899093294
INFO:root:current train perplexity4.883415699005127


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.76s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11097.154552641368
INFO:root:eval perplexity: 10.064911842346191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/29

 14%|â–ˆâ–        | 29/200 [3:16:15<19:13:05, 404.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8004.81455078125
INFO:root:current train perplexity4.829278945922852
INFO:root:current mean train loss 8028.47802734375
INFO:root:current train perplexity4.847646713256836
INFO:root:current mean train loss 8006.741794603924
INFO:root:current train perplexity4.848172664642334
INFO:root:current mean train loss 8018.42413969494
INFO:root:current train perplexity4.852528095245361
INFO:root:current mean train loss 8009.368240540286
INFO:root:current train perplexity4.85196590423584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.22s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11103.31771123977
INFO:root:eval perplexity: 10.077825546264648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/30

 15%|â–ˆâ–Œ        | 30/200 [3:23:00<19:06:38, 404.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7986.701146175987
INFO:root:current train perplexity4.811521053314209
INFO:root:current mean train loss 7969.808368073792
INFO:root:current train perplexity4.813203811645508
INFO:root:current mean train loss 7961.891458868436
INFO:root:current train perplexity4.8270134925842285
INFO:root:current mean train loss 7969.415401829448
INFO:root:current train perplexity4.822236061096191
INFO:root:current mean train loss 7975.77637884099
INFO:root:current train perplexity4.823070049285889


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11115.688648042225
INFO:root:eval perplexity: 10.103801727294922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/31

 16%|â–ˆâ–Œ        | 31/200 [3:29:45<18:59:38, 404.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7902.281780740489
INFO:root:current train perplexity4.784207344055176
INFO:root:current mean train loss 7921.142463001778
INFO:root:current train perplexity4.781247138977051
INFO:root:current mean train loss 7916.118682998178
INFO:root:current train perplexity4.779562473297119
INFO:root:current mean train loss 7928.537259033959
INFO:root:current train perplexity4.783512592315674
INFO:root:current mean train loss 7936.171986969932
INFO:root:current train perplexity4.787021160125732


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 11095.536469959077
INFO:root:eval perplexity: 10.061524391174316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/32

 16%|â–ˆâ–Œ        | 32/200 [3:36:29<18:52:36, 404.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7876.727159288194
INFO:root:current train perplexity4.769519805908203
INFO:root:current mean train loss 7881.5471595103345
INFO:root:current train perplexity4.742419242858887
INFO:root:current mean train loss 7887.698412117979
INFO:root:current train perplexity4.750772953033447
INFO:root:current mean train loss 7893.964951261468
INFO:root:current train perplexity4.745777606964111
INFO:root:current mean train loss 7902.503645528396
INFO:root:current train perplexity4.750737190246582


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.90s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11124.50406610398
INFO:root:eval perplexity: 10.122352600097656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/33

 16%|â–ˆâ–‹        | 33/200 [3:43:13<18:46:02, 404.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7860.450636340725
INFO:root:current train perplexity4.712306976318359
INFO:root:current mean train loss 7858.719883110687
INFO:root:current train perplexity4.706984519958496
INFO:root:current mean train loss 7856.909287489854
INFO:root:current train perplexity4.711569309234619
INFO:root:current mean train loss 7868.637841354324
INFO:root:current train perplexity4.717885971069336
INFO:root:current mean train loss 7879.716628072433
INFO:root:current train perplexity4.72930383682251


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.88s/it]
INFO:root:eval mean loss: 11148.706313360304
INFO:root:eval perplexity: 10.173454284667969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/34

 17%|â–ˆâ–‹        | 34/200 [3:49:58<18:39:05, 404.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7829.105329241072
INFO:root:current train perplexity4.735245227813721
INFO:root:current mean train loss 7838.132005931713
INFO:root:current train perplexity4.690652847290039
INFO:root:current mean train loss 7846.257288896277
INFO:root:current train perplexity4.696874141693115
INFO:root:current mean train loss 7839.197802005597
INFO:root:current train perplexity4.6964874267578125
INFO:root:current mean train loss 7843.857308503952
INFO:root:current train perplexity4.699695587158203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.42s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11140.527779715401
INFO:root:eval perplexity: 10.156157493591309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/35

 18%|â–ˆâ–Š        | 35/200 [3:56:43<18:32:57, 404.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7817.17516776843
INFO:root:current train perplexity4.670929431915283
INFO:root:current mean train loss 7813.093613000225
INFO:root:current train perplexity4.669034004211426
INFO:root:current mean train loss 7815.336971266998
INFO:root:current train perplexity4.669023513793945
INFO:root:current mean train loss 7818.586876613201
INFO:root:current train perplexity4.67434024810791
INFO:root:current mean train loss 7813.670561423334
INFO:root:current train perplexity4.673684597015381


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.49s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11157.73475283668
INFO:root:eval perplexity: 10.192584991455078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/36

 18%|â–ˆâ–Š        | 36/200 [4:03:27<18:25:47, 404.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7727.08984375
INFO:root:current train perplexity4.618163585662842
INFO:root:current mean train loss 7793.704412286932
INFO:root:current train perplexity4.642125129699707
INFO:root:current mean train loss 7787.187968187372
INFO:root:current train perplexity4.647113800048828
INFO:root:current mean train loss 7791.999039096665
INFO:root:current train perplexity4.647040843963623
INFO:root:current mean train loss 7794.186833159918
INFO:root:current train perplexity4.649634838104248


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.36s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.96s/it]
INFO:root:eval mean loss: 11158.276291620165
INFO:root:eval perplexity: 10.193735122680664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/37

 18%|â–ˆâ–Š        | 37/200 [4:10:13<18:19:38, 404.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7753.145902593085
INFO:root:current train perplexity4.620537757873535
INFO:root:current mean train loss 7758.286086309524
INFO:root:current train perplexity4.6099653244018555
INFO:root:current mean train loss 7762.535235323887
INFO:root:current train perplexity4.622711658477783
INFO:root:current mean train loss 7760.848088245227
INFO:root:current train perplexity4.621451377868652
INFO:root:current mean train loss 7765.176536065087
INFO:root:current train perplexity4.622964382171631


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.42s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11162.234988257998
INFO:root:eval perplexity: 10.20213508605957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/38

 19%|â–ˆâ–‰        | 38/200 [4:16:57<18:12:22, 404.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7725.41690602022
INFO:root:current train perplexity4.573218822479248
INFO:root:current mean train loss 7728.456652912872
INFO:root:current train perplexity4.578418731689453
INFO:root:current mean train loss 7727.804679718625
INFO:root:current train perplexity4.591871738433838
INFO:root:current mean train loss 7728.543487635773
INFO:root:current train perplexity4.591409206390381
INFO:root:current mean train loss 7735.708072772311
INFO:root:current train perplexity4.596186637878418


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.44s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11171.215535481771
INFO:root:eval perplexity: 10.22121524810791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/39

 20%|â–ˆâ–‰        | 39/200 [4:23:41<18:05:21, 404.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7689.387881747159
INFO:root:current train perplexity4.565627574920654
INFO:root:current mean train loss 7664.873897429436
INFO:root:current train perplexity4.553635120391846
INFO:root:current mean train loss 7690.906667432598
INFO:root:current train perplexity4.5555219650268555
INFO:root:current mean train loss 7698.127794894366
INFO:root:current train perplexity4.56483268737793
INFO:root:current mean train loss 7708.543519273695
INFO:root:current train perplexity4.573163032531738


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.74s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it]
INFO:root:eval mean loss: 11201.131742931548
INFO:root:eval perplexity: 10.285039901733398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/40

 20%|â–ˆâ–ˆ        | 40/200 [4:30:25<17:58:35, 404.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7651.718667240466
INFO:root:current train perplexity4.542442798614502
INFO:root:current mean train loss 7659.208542158019
INFO:root:current train perplexity4.537076950073242
INFO:root:current mean train loss 7670.793549408784
INFO:root:current train perplexity4.538156509399414
INFO:root:current mean train loss 7666.7576397654075
INFO:root:current train perplexity4.5375237464904785
INFO:root:current mean train loss 7676.268396182258
INFO:root:current train perplexity4.546158313751221


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.40s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11207.359252929688
INFO:root:eval perplexity: 10.298376083374023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/41

 20%|â–ˆâ–ˆ        | 41/200 [4:37:09<17:51:36, 404.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7591.666232638889
INFO:root:current train perplexity4.508482456207275
INFO:root:current mean train loss 7626.407541099502
INFO:root:current train perplexity4.503464698791504
INFO:root:current mean train loss 7634.855268239068
INFO:root:current train perplexity4.507521629333496
INFO:root:current mean train loss 7647.7365460356405
INFO:root:current train perplexity4.519692897796631
INFO:root:current mean train loss 7655.820350465713
INFO:root:current train perplexity4.523379325866699


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.90s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11206.416268484933
INFO:root:eval perplexity: 10.296358108520508
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/42

 21%|â–ˆâ–ˆ        | 42/200 [4:43:55<17:45:49, 404.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7617.325377506996
INFO:root:current train perplexity4.49142599105835
INFO:root:current mean train loss 7615.1810295424775
INFO:root:current train perplexity4.497808933258057
INFO:root:current mean train loss 7614.919078812617
INFO:root:current train perplexity4.501272201538086
INFO:root:current mean train loss 7618.861399970197
INFO:root:current train perplexity4.499177932739258
INFO:root:current mean train loss 7620.2154878211995
INFO:root:current train perplexity4.498378276824951


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.32s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11229.520824614025
INFO:root:eval perplexity: 10.34597396850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/43

 22%|â–ˆâ–ˆâ–       | 43/200 [4:50:39<17:38:36, 404.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7546.315512213909
INFO:root:current train perplexity4.459911823272705
INFO:root:current mean train loss 7568.755725169042
INFO:root:current train perplexity4.46292781829834
INFO:root:current mean train loss 7583.0147529404985
INFO:root:current train perplexity4.4673309326171875
INFO:root:current mean train loss 7593.651510644794
INFO:root:current train perplexity4.473118782043457
INFO:root:current mean train loss 7604.581362584594
INFO:root:current train perplexity4.482177734375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.30s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11241.299950009301
INFO:root:eval perplexity: 10.37136459350586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/44

 22%|â–ˆâ–ˆâ–       | 44/200 [4:57:23<17:31:27, 404.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7536.806901041667
INFO:root:current train perplexity4.4181342124938965
INFO:root:current mean train loss 7559.720167410715
INFO:root:current train perplexity4.436831951141357
INFO:root:current mean train loss 7554.892333096591
INFO:root:current train perplexity4.44130802154541
INFO:root:current mean train loss 7575.218315104166
INFO:root:current train perplexity4.450377464294434
INFO:root:current mean train loss 7575.348485814145
INFO:root:current train perplexity4.456035614013672


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.97s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11236.304675874257
INFO:root:eval perplexity: 10.360587120056152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [5:04:08<17:24:59, 404.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7539.782016416139
INFO:root:current train perplexity4.409306049346924
INFO:root:current mean train loss 7555.865523524791
INFO:root:current train perplexity4.4218339920043945
INFO:root:current mean train loss 7553.862607456877
INFO:root:current train perplexity4.428645610809326
INFO:root:current mean train loss 7554.162600232932
INFO:root:current train perplexity4.4359025955200195
INFO:root:current mean train loss 7552.89751598382
INFO:root:current train perplexity4.436524391174316


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11274.692237490699
INFO:root:eval perplexity: 10.443675994873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [5:10:53<17:18:15, 404.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7487.369552428464
INFO:root:current train perplexity4.396956920623779
INFO:root:current mean train loss 7511.677670338115
INFO:root:current train perplexity4.411932945251465
INFO:root:current mean train loss 7508.481780035336
INFO:root:current train perplexity4.405159950256348
INFO:root:current mean train loss 7519.149196057033
INFO:root:current train perplexity4.411905765533447
INFO:root:current mean train loss 7526.642307194617
INFO:root:current train perplexity4.414567470550537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.61s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11261.915091378349
INFO:root:eval perplexity: 10.415946960449219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [5:17:37<17:11:26, 404.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7489.808537625719
INFO:root:current train perplexity4.364767074584961
INFO:root:current mean train loss 7504.129136029412
INFO:root:current train perplexity4.376825332641602
INFO:root:current mean train loss 7501.98775043554
INFO:root:current train perplexity4.384671211242676
INFO:root:current mean train loss 7501.985722504845
INFO:root:current train perplexity4.388655662536621
INFO:root:current mean train loss 7504.800831381545
INFO:root:current train perplexity4.393474578857422


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.55s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it]
INFO:root:eval mean loss: 11285.786048525855
INFO:root:eval perplexity: 10.467808723449707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/48

 24%|â–ˆâ–ˆâ–       | 48/200 [5:24:21<17:04:35, 404.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7462.678984589629
INFO:root:current train perplexity4.343395233154297
INFO:root:current mean train loss 7491.16614089087
INFO:root:current train perplexity4.361460208892822
INFO:root:current mean train loss 7477.084581722509
INFO:root:current train perplexity4.369353771209717
INFO:root:current mean train loss 7482.631184063299
INFO:root:current train perplexity4.374632358551025
INFO:root:current mean train loss 7484.345311306644
INFO:root:current train perplexity4.377433776855469


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.54s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11312.867350260416
INFO:root:eval perplexity: 10.526962280273438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/49

 24%|â–ˆâ–ˆâ–       | 49/200 [5:31:07<16:59:00, 404.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7455.4495271381575
INFO:root:current train perplexity4.344792366027832
INFO:root:current mean train loss 7440.458708934295
INFO:root:current train perplexity4.337829113006592
INFO:root:current mean train loss 7455.172926046081
INFO:root:current train perplexity4.34689998626709
INFO:root:current mean train loss 7455.990077383307
INFO:root:current train perplexity4.3520307540893555
INFO:root:current mean train loss 7460.439841777146
INFO:root:current train perplexity4.356995582580566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.11s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11279.270083472842
INFO:root:eval perplexity: 10.453628540039062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [5:37:51<16:51:27, 404.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7412.6295622238
INFO:root:current train perplexity4.32609748840332
INFO:root:current mean train loss 7417.068079656093
INFO:root:current train perplexity4.32440710067749
INFO:root:current mean train loss 7432.40072539977
INFO:root:current train perplexity4.326236248016357
INFO:root:current mean train loss 7440.6673776726975
INFO:root:current train perplexity4.334232807159424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11311.145342145648
INFO:root:eval perplexity: 10.523193359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [5:44:36<16:44:56, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7188.336263020833
INFO:root:current train perplexity4.228301525115967
INFO:root:current mean train loss 7407.073512401395
INFO:root:current train perplexity4.297890663146973
INFO:root:current mean train loss 7413.487641433189
INFO:root:current train perplexity4.307865619659424
INFO:root:current mean train loss 7416.978122421617
INFO:root:current train perplexity4.312664985656738
INFO:root:current mean train loss 7415.156008888415
INFO:root:current train perplexity4.313846588134766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.08s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.89s/it]
INFO:root:eval mean loss: 11336.263276599702
INFO:root:eval perplexity: 10.57833480834961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [5:51:21<16:38:18, 404.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7540.965192522322
INFO:root:current train perplexity4.31787633895874
INFO:root:current mean train loss 7422.29357567903
INFO:root:current train perplexity4.30668306350708
INFO:root:current mean train loss 7399.224043251812
INFO:root:current train perplexity4.295068740844727
INFO:root:current mean train loss 7397.96998263182
INFO:root:current train perplexity4.2977519035339355
INFO:root:current mean train loss 7393.52062298449
INFO:root:current train perplexity4.296180725097656


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.36s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11322.59372093564
INFO:root:eval perplexity: 10.54828929901123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [5:58:06<16:31:50, 404.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7455.475008877841
INFO:root:current train perplexity4.2967376708984375
INFO:root:current mean train loss 7361.096582911036
INFO:root:current train perplexity4.261433124542236
INFO:root:current mean train loss 7359.751481042654
INFO:root:current train perplexity4.269049644470215
INFO:root:current mean train loss 7368.499587080486
INFO:root:current train perplexity4.272951126098633
INFO:root:current mean train loss 7371.6428739450275
INFO:root:current train perplexity4.278780937194824


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.32s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11352.154421851748
INFO:root:eval perplexity: 10.613371849060059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [6:04:51<16:25:20, 404.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7291.17802734375
INFO:root:current train perplexity4.265573024749756
INFO:root:current mean train loss 7344.988510529892
INFO:root:current train perplexity4.238889694213867
INFO:root:current mean train loss 7345.397688045058
INFO:root:current train perplexity4.251642227172852
INFO:root:current mean train loss 7339.9144546750995
INFO:root:current train perplexity4.251063346862793
INFO:root:current mean train loss 7350.705402861446
INFO:root:current train perplexity4.256699085235596


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.96s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11362.938514346168
INFO:root:eval perplexity: 10.637214660644531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [6:11:36<16:18:23, 404.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7185.701197574013
INFO:root:current train perplexity4.16016960144043
INFO:root:current mean train loss 7285.46349789916
INFO:root:current train perplexity4.216004371643066
INFO:root:current mean train loss 7301.035368061501
INFO:root:current train perplexity4.227291584014893
INFO:root:current mean train loss 7318.4897139498435
INFO:root:current train perplexity4.233026504516602
INFO:root:current mean train loss 7326.325807120749
INFO:root:current train perplexity4.238865852355957


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11379.540690104166
INFO:root:eval perplexity: 10.674023628234863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [6:18:21<16:11:40, 404.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7259.45603345788
INFO:root:current train perplexity4.166382789611816
INFO:root:current mean train loss 7295.4190088287605
INFO:root:current train perplexity4.217853546142578
INFO:root:current mean train loss 7315.4919751086045
INFO:root:current train perplexity4.2205634117126465
INFO:root:current mean train loss 7316.695728219331
INFO:root:current train perplexity4.226123809814453
INFO:root:current mean train loss 7315.796000018469
INFO:root:current train perplexity4.225252628326416


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.60s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11396.465343656993
INFO:root:eval perplexity: 10.711678504943848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [6:25:05<16:04:34, 404.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7250.688187210648
INFO:root:current train perplexity4.217408657073975
INFO:root:current mean train loss 7281.478934701034
INFO:root:current train perplexity4.196996212005615
INFO:root:current mean train loss 7285.337088295017
INFO:root:current train perplexity4.207370281219482
INFO:root:current mean train loss 7282.381229692278
INFO:root:current train perplexity4.205929279327393
INFO:root:current mean train loss 7285.061892793106
INFO:root:current train perplexity4.208345890045166


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.36s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11423.54581124442
INFO:root:eval perplexity: 10.77220630645752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [6:31:49<15:57:26, 404.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7250.779832409275
INFO:root:current train perplexity4.143764972686768
INFO:root:current mean train loss 7252.272103113072
INFO:root:current train perplexity4.1681809425354
INFO:root:current mean train loss 7247.764718191965
INFO:root:current train perplexity4.174869060516357
INFO:root:current mean train loss 7264.304827641145
INFO:root:current train perplexity4.187252998352051
INFO:root:current mean train loss 7264.347491979046
INFO:root:current train perplexity4.1948113441467285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11422.032421293712
INFO:root:eval perplexity: 10.768816947937012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [6:38:34<15:50:53, 404.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7227.77998046875
INFO:root:current train perplexity4.132559776306152
INFO:root:current mean train loss 7193.8984338831015
INFO:root:current train perplexity4.1482625007629395
INFO:root:current mean train loss 7229.828951961436
INFO:root:current train perplexity4.167328357696533
INFO:root:current mean train loss 7240.540286847015
INFO:root:current train perplexity4.173557758331299
INFO:root:current mean train loss 7251.316126751078
INFO:root:current train perplexity4.178729057312012


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.89s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11449.695562453497
INFO:root:eval perplexity: 10.830982208251953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [6:45:19<15:44:08, 404.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7231.95837089343
INFO:root:current train perplexity4.143616199493408
INFO:root:current mean train loss 7208.929399449191
INFO:root:current train perplexity4.151556491851807
INFO:root:current mean train loss 7206.135431648797
INFO:root:current train perplexity4.147568702697754
INFO:root:current mean train loss 7211.731641201143
INFO:root:current train perplexity4.152961254119873
INFO:root:current mean train loss 7225.041403803032
INFO:root:current train perplexity4.159825801849365


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.33s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11453.159787132627
INFO:root:eval perplexity: 10.838793754577637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [6:52:04<15:37:45, 404.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7198.808128179506
INFO:root:current train perplexity4.134429931640625
INFO:root:current mean train loss 7211.470610932037
INFO:root:current train perplexity4.134465217590332
INFO:root:current mean train loss 7200.5536647215795
INFO:root:current train perplexity4.132028102874756
INFO:root:current mean train loss 7206.4149308719025
INFO:root:current train perplexity4.140250205993652
INFO:root:current mean train loss 7206.958109216281
INFO:root:current train perplexity4.141442775726318


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11460.408921014696
INFO:root:eval perplexity: 10.855154037475586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [6:58:48<15:30:48, 404.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7203.25428025266
INFO:root:current train perplexity4.121241569519043
INFO:root:current mean train loss 7176.543526785715
INFO:root:current train perplexity4.119426727294922
INFO:root:current mean train loss 7173.936685538968
INFO:root:current train perplexity4.121611595153809
INFO:root:current mean train loss 7175.570070470101
INFO:root:current train perplexity4.122717380523682
INFO:root:current mean train loss 7190.840141962039
INFO:root:current train perplexity4.129717826843262


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11464.691458565849
INFO:root:eval perplexity: 10.864831924438477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [7:05:33<15:24:19, 404.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7127.687136182598
INFO:root:current train perplexity4.0721540451049805
INFO:root:current mean train loss 7144.41348044288
INFO:root:current train perplexity4.089324951171875
INFO:root:current mean train loss 7166.904567277764
INFO:root:current train perplexity4.100350856781006
INFO:root:current mean train loss 7158.158151653757
INFO:root:current train perplexity4.103450298309326
INFO:root:current mean train loss 7170.093334257206
INFO:root:current train perplexity4.110689640045166


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.10s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11471.78182547433
INFO:root:eval perplexity: 10.880870819091797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [7:12:18<15:17:35, 404.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7155.265358664773
INFO:root:current train perplexity4.104795455932617
INFO:root:current mean train loss 7142.745473160282
INFO:root:current train perplexity4.0975775718688965
INFO:root:current mean train loss 7148.119182751226
INFO:root:current train perplexity4.100584983825684
INFO:root:current mean train loss 7148.76142303037
INFO:root:current train perplexity4.100412845611572
INFO:root:current mean train loss 7152.148794857486
INFO:root:current train perplexity4.098820686340332


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.46s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11486.697745186942
INFO:root:eval perplexity: 10.914694786071777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/65

 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [7:19:04<15:11:09, 404.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7126.420575675318
INFO:root:current train perplexity4.0767502784729
INFO:root:current mean train loss 7119.7625970420595
INFO:root:current train perplexity4.072530746459961
INFO:root:current mean train loss 7108.697784070343
INFO:root:current train perplexity4.074473857879639
INFO:root:current mean train loss 7120.450211633879
INFO:root:current train perplexity4.078887939453125
INFO:root:current mean train loss 7129.9313619110835
INFO:root:current train perplexity4.079895973205566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.71s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11512.932983398438
INFO:root:eval perplexity: 10.974438667297363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [7:25:48<15:04:05, 404.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7074.289767795139
INFO:root:current train perplexity4.0454206466674805
INFO:root:current mean train loss 7108.081677770322
INFO:root:current train perplexity4.0587005615234375
INFO:root:current mean train loss 7108.104315812144
INFO:root:current train perplexity4.05780553817749
INFO:root:current mean train loss 7107.101144165375
INFO:root:current train perplexity4.059298992156982
INFO:root:current mean train loss 7110.806928531655
INFO:root:current train perplexity4.0660929679870605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11517.002984909785
INFO:root:eval perplexity: 10.983736991882324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [7:32:33<14:57:30, 404.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7036.140435517724
INFO:root:current train perplexity4.0337677001953125
INFO:root:current mean train loss 7081.424676038548
INFO:root:current train perplexity4.049589157104492
INFO:root:current mean train loss 7083.735108336259
INFO:root:current train perplexity4.048102378845215
INFO:root:current mean train loss 7085.843024895691
INFO:root:current train perplexity4.049579620361328
INFO:root:current mean train loss 7097.529197545838
INFO:root:current train perplexity4.055300712585449


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.06s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11541.061471121651
INFO:root:eval perplexity: 11.038863182067871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [7:39:18<14:51:02, 405.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7051.835793078785
INFO:root:current train perplexity4.001743793487549
INFO:root:current mean train loss 7063.1008886147665
INFO:root:current train perplexity4.019408702850342
INFO:root:current mean train loss 7072.1765361940725
INFO:root:current train perplexity4.026332378387451
INFO:root:current mean train loss 7077.035506338443
INFO:root:current train perplexity4.033496379852295
INFO:root:current mean train loss 7079.088581060908
INFO:root:current train perplexity4.039009094238281


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.59s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11554.576038178944
INFO:root:eval perplexity: 11.069944381713867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [7:46:03<14:44:05, 404.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7022.1094140625
INFO:root:current train perplexity4.00455379486084
INFO:root:current mean train loss 7025.078582589285
INFO:root:current train perplexity4.016298294067383
INFO:root:current mean train loss 7047.711962002841
INFO:root:current train perplexity4.017026901245117
INFO:root:current mean train loss 7055.650287760417
INFO:root:current train perplexity4.023726940155029
INFO:root:current mean train loss 7058.403997738486
INFO:root:current train perplexity4.023501396179199


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.87s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11559.807390485492
INFO:root:eval perplexity: 11.081998825073242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [7:52:49<14:37:44, 405.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7027.460875692247
INFO:root:current train perplexity4.0127763748168945
INFO:root:current mean train loss 7028.769059335719
INFO:root:current train perplexity4.0014190673828125
INFO:root:current mean train loss 7029.3640968021955
INFO:root:current train perplexity4.004880428314209
INFO:root:current mean train loss 7032.286475511214
INFO:root:current train perplexity4.0062785148620605
INFO:root:current mean train loss 7039.609665522247
INFO:root:current train perplexity4.010117530822754


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.27s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11579.76209658668
INFO:root:eval perplexity: 11.12811279296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [7:59:33<14:30:18, 404.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7017.565488516566
INFO:root:current train perplexity3.994487762451172
INFO:root:current mean train loss 6993.997537248121
INFO:root:current train perplexity3.984605550765991
INFO:root:current mean train loss 7006.426882039532
INFO:root:current train perplexity3.987992286682129
INFO:root:current mean train loss 7014.666117615861
INFO:root:current train perplexity3.992297410964966
INFO:root:current mean train loss 7024.481766789596
INFO:root:current train perplexity3.9982128143310547


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.56s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 11580.92261032831
INFO:root:eval perplexity: 11.1308012008667
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [8:06:18<14:23:55, 404.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7012.442186377514
INFO:root:current train perplexity3.9912219047546387
INFO:root:current mean train loss 7012.183614639038
INFO:root:current train perplexity3.981969118118286
INFO:root:current mean train loss 6993.857826791159
INFO:root:current train perplexity3.9766135215759277
INFO:root:current mean train loss 7000.693157501615
INFO:root:current train perplexity3.9804091453552246
INFO:root:current mean train loss 7006.67469339547
INFO:root:current train perplexity3.9835259914398193


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.81s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11592.78347342355
INFO:root:eval perplexity: 11.158304214477539
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [8:13:03<14:17:06, 404.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6962.371270819025
INFO:root:current train perplexity3.9490854740142822
INFO:root:current mean train loss 6964.507288428501
INFO:root:current train perplexity3.9548654556274414
INFO:root:current mean train loss 6967.236326447058
INFO:root:current train perplexity3.9560160636901855
INFO:root:current mean train loss 6980.68016454204
INFO:root:current train perplexity3.964805841445923
INFO:root:current mean train loss 6987.941566358516
INFO:root:current train perplexity3.968200922012329


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.73s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11611.251316615513
INFO:root:eval perplexity: 11.201263427734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [8:19:47<14:10:04, 404.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6954.90283203125
INFO:root:current train perplexity3.9453794956207275
INFO:root:current mean train loss 6956.484269831731
INFO:root:current train perplexity3.9378223419189453
INFO:root:current mean train loss 6970.283211400953
INFO:root:current train perplexity3.949963092803955
INFO:root:current mean train loss 6976.107332871836
INFO:root:current train perplexity3.9543182849884033
INFO:root:current mean train loss 6972.062418126578
INFO:root:current train perplexity3.9561052322387695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.51s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 11634.25673421224
INFO:root:eval perplexity: 11.255012512207031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [8:26:33<14:03:37, 404.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6932.978431778724
INFO:root:current train perplexity3.934438943862915
INFO:root:current mean train loss 6940.043280366677
INFO:root:current train perplexity3.9389450550079346
INFO:root:current mean train loss 6947.532427427362
INFO:root:current train perplexity3.94130802154541
INFO:root:current mean train loss 6949.0715423813435
INFO:root:current train perplexity3.9435789585113525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.36s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11651.56063406808
INFO:root:eval perplexity: 11.295604705810547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [8:33:17<13:56:19, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6884.25439453125
INFO:root:current train perplexity3.8937017917633057
INFO:root:current mean train loss 6894.323422330097
INFO:root:current train perplexity3.9121718406677246
INFO:root:current mean train loss 6911.492416006004
INFO:root:current train perplexity3.918565034866333
INFO:root:current mean train loss 6917.399362494843
INFO:root:current train perplexity3.9232301712036133
INFO:root:current mean train loss 6931.43520398767
INFO:root:current train perplexity3.9263620376586914


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.51s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11664.287280854725
INFO:root:eval perplexity: 11.32555866241455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [8:40:01<13:49:32, 404.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6986.898786272322
INFO:root:current train perplexity3.9141204357147217
INFO:root:current mean train loss 6902.6183739778035
INFO:root:current train perplexity3.9063498973846436
INFO:root:current mean train loss 6932.519922818538
INFO:root:current train perplexity3.909153938293457
INFO:root:current mean train loss 6926.475663871641
INFO:root:current train perplexity3.913674831390381
INFO:root:current mean train loss 6926.008570715602
INFO:root:current train perplexity3.917957305908203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.81s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11663.373668852306
INFO:root:eval perplexity: 11.323407173156738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [8:46:46<13:42:43, 404.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6869.943314985795
INFO:root:current train perplexity3.9037184715270996
INFO:root:current mean train loss 6877.0908862964525
INFO:root:current train perplexity3.8864059448242188
INFO:root:current mean train loss 6887.5742719749705
INFO:root:current train perplexity3.885050058364868
INFO:root:current mean train loss 6892.613714579984
INFO:root:current train perplexity3.8939919471740723
INFO:root:current mean train loss 6898.37873636139
INFO:root:current train perplexity3.902360200881958


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.07s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.95s/it]
INFO:root:eval mean loss: 11668.696934291294
INFO:root:eval perplexity: 11.335956573486328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [8:53:34<13:37:51, 405.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6819.209830729166
INFO:root:current train perplexity3.8970298767089844
INFO:root:current mean train loss 6866.087631623642
INFO:root:current train perplexity3.8799710273742676
INFO:root:current mean train loss 6892.15263671875
INFO:root:current train perplexity3.8963303565979004
INFO:root:current mean train loss 6885.041995287698
INFO:root:current train perplexity3.8962504863739014
INFO:root:current mean train loss 6884.456845350151
INFO:root:current train perplexity3.896810293197632


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11692.572370256696
INFO:root:eval perplexity: 11.392413139343262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [9:00:19<13:30:56, 405.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6825.761975740132
INFO:root:current train perplexity3.854462146759033
INFO:root:current mean train loss 6873.77727809874
INFO:root:current train perplexity3.8783349990844727
INFO:root:current mean train loss 6871.709715682077
INFO:root:current train perplexity3.8820650577545166
INFO:root:current mean train loss 6863.97003575627
INFO:root:current train perplexity3.8807833194732666
INFO:root:current mean train loss 6876.398188115304
INFO:root:current train perplexity3.8830981254577637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 11699.363455636161
INFO:root:eval perplexity: 11.408520698547363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [9:07:04<13:24:02, 405.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6769.123980978261
INFO:root:current train perplexity3.8355369567871094
INFO:root:current mean train loss 6822.947666571392
INFO:root:current train perplexity3.8488237857818604
INFO:root:current mean train loss 6819.490702949832
INFO:root:current train perplexity3.8524534702301025
INFO:root:current mean train loss 6841.401807094137
INFO:root:current train perplexity3.8585095405578613
INFO:root:current mean train loss 6855.482857057107
INFO:root:current train perplexity3.8672566413879395


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.99s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11721.252717517671
INFO:root:eval perplexity: 11.460601806640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [9:13:49<13:16:54, 405.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6848.317563657408
INFO:root:current train perplexity3.8372299671173096
INFO:root:current mean train loss 6819.1034425750495
INFO:root:current train perplexity3.833940267562866
INFO:root:current mean train loss 6815.428302243943
INFO:root:current train perplexity3.8460865020751953
INFO:root:current mean train loss 6827.897499761086
INFO:root:current train perplexity3.8512208461761475
INFO:root:current mean train loss 6841.551840145638
INFO:root:current train perplexity3.8566811084747314


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.77s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11738.788109188989
INFO:root:eval perplexity: 11.502495765686035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [9:20:33<13:09:48, 405.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6797.154596144153
INFO:root:current train perplexity3.8438658714294434
INFO:root:current mean train loss 6809.121425483063
INFO:root:current train perplexity3.837993621826172
INFO:root:current mean train loss 6813.428886380547
INFO:root:current train perplexity3.8433406352996826
INFO:root:current mean train loss 6821.0742777567975
INFO:root:current train perplexity3.8412485122680664
INFO:root:current mean train loss 6828.337914415966
INFO:root:current train perplexity3.845224142074585


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11726.043148949033
INFO:root:eval perplexity: 11.472034454345703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [9:27:18<13:02:40, 404.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6797.5810546875
INFO:root:current train perplexity3.808237314224243
INFO:root:current mean train loss 6790.609754774306
INFO:root:current train perplexity3.8204398155212402
INFO:root:current mean train loss 6804.447814162234
INFO:root:current train perplexity3.827986478805542
INFO:root:current mean train loss 6811.257137651586
INFO:root:current train perplexity3.8329498767852783
INFO:root:current mean train loss 6814.185866783405
INFO:root:current train perplexity3.834357500076294


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.55s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11749.014314197359
INFO:root:eval perplexity: 11.526997566223145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [9:34:02<12:55:40, 404.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6798.220991085737
INFO:root:current train perplexity3.815908670425415
INFO:root:current mean train loss 6812.371012955261
INFO:root:current train perplexity3.815725803375244
INFO:root:current mean train loss 6797.535297218227
INFO:root:current train perplexity3.8174195289611816
INFO:root:current mean train loss 6802.059648091814
INFO:root:current train perplexity3.821280002593994
INFO:root:current mean train loss 6802.095351651481
INFO:root:current train perplexity3.824545383453369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.87s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 11756.846935453868
INFO:root:eval perplexity: 11.54580020904541
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [9:40:46<12:48:18, 404.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6774.118288971657
INFO:root:current train perplexity3.7846953868865967
INFO:root:current mean train loss 6767.287652289117
INFO:root:current train perplexity3.7978246212005615
INFO:root:current mean train loss 6764.337842399691
INFO:root:current train perplexity3.797795295715332
INFO:root:current mean train loss 6775.178780691965
INFO:root:current train perplexity3.8104233741760254
INFO:root:current mean train loss 6779.567434616605
INFO:root:current train perplexity3.8122289180755615


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.65s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11784.683997744605
INFO:root:eval perplexity: 11.612869262695312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [9:47:31<12:42:11, 404.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6706.956667636303
INFO:root:current train perplexity3.7796168327331543
INFO:root:current mean train loss 6734.378706951531
INFO:root:current train perplexity3.791926383972168
INFO:root:current mean train loss 6745.435440125253
INFO:root:current train perplexity3.794792890548706
INFO:root:current mean train loss 6757.599547460374
INFO:root:current train perplexity3.7970690727233887
INFO:root:current mean train loss 6770.542068652126
INFO:root:current train perplexity3.8031325340270996


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.39s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11790.058050246465
INFO:root:eval perplexity: 11.625861167907715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [9:54:15<12:35:05, 404.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6734.527219286152
INFO:root:current train perplexity3.778674840927124
INFO:root:current mean train loss 6752.66620964404
INFO:root:current train perplexity3.7921836376190186
INFO:root:current mean train loss 6747.589277655005
INFO:root:current train perplexity3.786844491958618
INFO:root:current mean train loss 6754.501385550214
INFO:root:current train perplexity3.785195827484131
INFO:root:current mean train loss 6753.443622462237
INFO:root:current train perplexity3.7897188663482666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.37s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11797.142874581474
INFO:root:eval perplexity: 11.643009185791016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [10:01:01<12:28:42, 404.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6733.431685014205
INFO:root:current train perplexity3.7704827785491943
INFO:root:current mean train loss 6726.742861643145
INFO:root:current train perplexity3.775829553604126
INFO:root:current mean train loss 6729.405221737133
INFO:root:current train perplexity3.774855613708496
INFO:root:current mean train loss 6731.502039777729
INFO:root:current train perplexity3.779522657394409
INFO:root:current mean train loss 6738.816512491415
INFO:root:current train perplexity3.780662775039673


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.05s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.84s/it]
INFO:root:eval mean loss: 11801.321451822916
INFO:root:eval perplexity: 11.653142929077148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [10:07:45<12:21:59, 404.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6757.435869637182
INFO:root:current train perplexity3.755625009536743
INFO:root:current mean train loss 6722.848473123035
INFO:root:current train perplexity3.75677490234375
INFO:root:current mean train loss 6728.834532984435
INFO:root:current train perplexity3.763594388961792
INFO:root:current mean train loss 6734.650677609245
INFO:root:current train perplexity3.768594980239868
INFO:root:current mean train loss 6730.71340869247
INFO:root:current train perplexity3.7718772888183594


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.42s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11844.696367536273
INFO:root:eval perplexity: 11.758790016174316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [10:14:30<12:14:57, 404.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6647.639547681051
INFO:root:current train perplexity3.7417824268341064
INFO:root:current mean train loss 6690.002031010353
INFO:root:current train perplexity3.754545211791992
INFO:root:current mean train loss 6712.158799088046
INFO:root:current train perplexity3.752403736114502
INFO:root:current mean train loss 6707.922191104942
INFO:root:current train perplexity3.75710129737854
INFO:root:current mean train loss 6714.716694578497
INFO:root:current train perplexity3.760094404220581


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.26s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11808.43418375651
INFO:root:eval perplexity: 11.670397758483887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [10:21:15<12:08:27, 404.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6677.569394239738
INFO:root:current train perplexity3.720639228820801
INFO:root:current mean train loss 6688.432354041916
INFO:root:current train perplexity3.736605167388916
INFO:root:current mean train loss 6699.969609521301
INFO:root:current train perplexity3.7455310821533203
INFO:root:current mean train loss 6694.1611368039
INFO:root:current train perplexity3.7470755577087402
INFO:root:current mean train loss 6693.476540543027
INFO:root:current train perplexity3.747807264328003


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.76s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11838.043509347099
INFO:root:eval perplexity: 11.742521286010742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [10:27:58<12:01:06, 404.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6674.078764579665
INFO:root:current train perplexity3.744075298309326
INFO:root:current mean train loss 6668.768723158809
INFO:root:current train perplexity3.7347512245178223
INFO:root:current mean train loss 6685.943348564345
INFO:root:current train perplexity3.7395763397216797
INFO:root:current mean train loss 6687.707886729279
INFO:root:current train perplexity3.738524913787842
INFO:root:current mean train loss 6690.2600932192145
INFO:root:current train perplexity3.7425036430358887


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.58s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11854.468819754464
INFO:root:eval perplexity: 11.78272533416748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [10:34:42<11:54:22, 404.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6658.42814453125
INFO:root:current train perplexity3.7233235836029053
INFO:root:current mean train loss 6655.964946986607
INFO:root:current train perplexity3.7211930751800537
INFO:root:current mean train loss 6664.4523508522725
INFO:root:current train perplexity3.7267649173736572
INFO:root:current mean train loss 6675.4474296875
INFO:root:current train perplexity3.730863332748413
INFO:root:current mean train loss 6678.321134868421
INFO:root:current train perplexity3.7314469814300537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.72s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11862.929917108446
INFO:root:eval perplexity: 11.803483963012695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [10:41:27<11:47:40, 404.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6636.297369462025
INFO:root:current train perplexity3.7161333560943604
INFO:root:current mean train loss 6645.250169125349
INFO:root:current train perplexity3.715733289718628
INFO:root:current mean train loss 6641.5165963121635
INFO:root:current train perplexity3.7138407230377197
INFO:root:current mean train loss 6646.525814489199
INFO:root:current train perplexity3.713994264602661
INFO:root:current mean train loss 6658.8525665856605
INFO:root:current train perplexity3.7195560932159424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.88s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11860.756472632998
INFO:root:eval perplexity: 11.798150062561035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [10:48:12<11:41:06, 404.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6641.683329019202
INFO:root:current train perplexity3.707096576690674
INFO:root:current mean train loss 6632.011641372097
INFO:root:current train perplexity3.705930471420288
INFO:root:current mean train loss 6640.348377456935
INFO:root:current train perplexity3.7040798664093018
INFO:root:current mean train loss 6647.837361547406
INFO:root:current train perplexity3.710758686065674
INFO:root:current mean train loss 6648.372691026138
INFO:root:current train perplexity3.7118754386901855


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11881.407081240699
INFO:root:eval perplexity: 11.848953247070312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [10:54:56<11:34:16, 404.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6635.9307650862065
INFO:root:current train perplexity3.691647529602051
INFO:root:current mean train loss 6630.651852857621
INFO:root:current train perplexity3.6920742988586426
INFO:root:current mean train loss 6630.868300168771
INFO:root:current train perplexity3.6958465576171875
INFO:root:current mean train loss 6639.484371214874
INFO:root:current train perplexity3.7004926204681396
INFO:root:current mean train loss 6638.273645044597
INFO:root:current train perplexity3.70316743850708


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.78s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 11894.657479422433
INFO:root:eval perplexity: 11.88166618347168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [11:01:41<11:27:37, 404.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6606.167834606799
INFO:root:current train perplexity3.68463134765625
INFO:root:current mean train loss 6603.670448502945
INFO:root:current train perplexity3.689408540725708
INFO:root:current mean train loss 6603.475562446306
INFO:root:current train perplexity3.6844608783721924
INFO:root:current mean train loss 6618.422549352622
INFO:root:current train perplexity3.690147638320923
INFO:root:current mean train loss 6622.303968503374
INFO:root:current train perplexity3.6922624111175537


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.94s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11918.544372558594
INFO:root:eval perplexity: 11.940869331359863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [11:08:24<11:20:27, 404.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6591.166658100329
INFO:root:current train perplexity3.6782686710357666
INFO:root:current mean train loss 6601.265159254807
INFO:root:current train perplexity3.679311990737915
INFO:root:current mean train loss 6603.67883342161
INFO:root:current train perplexity3.6839048862457275
INFO:root:current mean train loss 6608.704141119462
INFO:root:current train perplexity3.6847877502441406
INFO:root:current mean train loss 6610.832814472854
INFO:root:current train perplexity3.684105634689331


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11911.826416015625
INFO:root:eval perplexity: 11.924188613891602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [11:15:09<11:13:51, 404.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6541.189334753788
INFO:root:current train perplexity3.6520652770996094
INFO:root:current mean train loss 6581.220393961998
INFO:root:current train perplexity3.668165445327759
INFO:root:current mean train loss 6585.833760647471
INFO:root:current train perplexity3.666537046432495
INFO:root:current mean train loss 6593.804010759321
INFO:root:current train perplexity3.669696807861328


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 11935.594650995165
INFO:root:eval perplexity: 11.983308792114258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [11:21:54<11:07:25, 404.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6801.541015625
INFO:root:current train perplexity3.671992778778076
INFO:root:current mean train loss 6567.483303625607
INFO:root:current train perplexity3.641413688659668
INFO:root:current mean train loss 6575.003105276324
INFO:root:current train perplexity3.6575253009796143
INFO:root:current mean train loss 6574.383743940801
INFO:root:current train perplexity3.660947799682617
INFO:root:current mean train loss 6581.479755108173
INFO:root:current train perplexity3.664564609527588


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.71s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11953.098507835752
INFO:root:eval perplexity: 12.027032852172852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [11:28:38<11:00:40, 404.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6488.19091796875
INFO:root:current train perplexity3.6641736030578613
INFO:root:current mean train loss 6545.983247846087
INFO:root:current train perplexity3.6509175300598145
INFO:root:current mean train loss 6561.105893342391
INFO:root:current train perplexity3.6542985439300537
INFO:root:current mean train loss 6561.513191546213
INFO:root:current train perplexity3.655588150024414
INFO:root:current mean train loss 6572.1100480363175
INFO:root:current train perplexity3.656585931777954


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.68s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11963.1711164202
INFO:root:eval perplexity: 12.052267074584961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [11:35:23<10:54:22, 404.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6545.462713068182
INFO:root:current train perplexity3.6061718463897705
INFO:root:current mean train loss 6545.888346354167
INFO:root:current train perplexity3.6400835514068604
INFO:root:current mean train loss 6549.602224340936
INFO:root:current train perplexity3.6376044750213623
INFO:root:current mean train loss 6546.575115240656
INFO:root:current train perplexity3.637943983078003
INFO:root:current mean train loss 6555.423071348464
INFO:root:current train perplexity3.644132137298584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.20s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 11975.365417480469
INFO:root:eval perplexity: 12.0828857421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [11:42:09<10:47:44, 404.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6468.033072916666
INFO:root:current train perplexity3.624999761581421
INFO:root:current mean train loss 6515.361909816576
INFO:root:current train perplexity3.619007110595703
INFO:root:current mean train loss 6539.294878724564
INFO:root:current train perplexity3.63216495513916
INFO:root:current mean train loss 6547.631391059028
INFO:root:current train perplexity3.6315035820007324
INFO:root:current mean train loss 6547.640327324924
INFO:root:current train perplexity3.637907028198242


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.70s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 11984.430425734747
INFO:root:eval perplexity: 12.105703353881836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [11:48:53<10:40:48, 404.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6552.0193513569075
INFO:root:current train perplexity3.632282018661499
INFO:root:current mean train loss 6529.136607963498
INFO:root:current train perplexity3.617342233657837
INFO:root:current mean train loss 6529.332049086758
INFO:root:current train perplexity3.622877597808838
INFO:root:current mean train loss 6534.715283050059
INFO:root:current train perplexity3.6265881061553955
INFO:root:current mean train loss 6532.188246988738
INFO:root:current train perplexity3.629927158355713


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.93s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.97s/it]
INFO:root:eval mean loss: 11987.741882324219
INFO:root:eval perplexity: 12.114041328430176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [11:55:38<10:34:04, 404.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6468.581882642663
INFO:root:current train perplexity3.624250888824463
INFO:root:current mean train loss 6495.418774612551
INFO:root:current train perplexity3.604332447052002
INFO:root:current mean train loss 6515.61693788537
INFO:root:current train perplexity3.6130776405334473
INFO:root:current mean train loss 6514.84673410894
INFO:root:current train perplexity3.6144015789031982
INFO:root:current mean train loss 6527.061145971853
INFO:root:current train perplexity3.621708393096924


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.89s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 11990.60021391369
INFO:root:eval perplexity: 12.121252059936523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [12:02:22<10:27:17, 404.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6483.67455150463
INFO:root:current train perplexity3.6175637245178223
INFO:root:current mean train loss 6501.739034817913
INFO:root:current train perplexity3.6039767265319824
INFO:root:current mean train loss 6499.193486285105
INFO:root:current train perplexity3.60947322845459
INFO:root:current mean train loss 6506.240806276281
INFO:root:current train perplexity3.611727237701416
INFO:root:current mean train loss 6513.113680337017
INFO:root:current train perplexity3.613858699798584


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.01s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.92s/it]
INFO:root:eval mean loss: 12009.501307896206
INFO:root:eval perplexity: 12.169015884399414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [12:09:07<10:20:36, 404.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6469.9916834677415
INFO:root:current train perplexity3.5799953937530518
INFO:root:current mean train loss 6488.263630874284
INFO:root:current train perplexity3.5912299156188965
INFO:root:current mean train loss 6488.942556141775
INFO:root:current train perplexity3.5968260765075684
INFO:root:current mean train loss 6488.76513671875
INFO:root:current train perplexity3.601067066192627
INFO:root:current mean train loss 6501.17582090161
INFO:root:current train perplexity3.603167772293091


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.14s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 12020.194969540551
INFO:root:eval perplexity: 12.196124076843262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [12:15:52<10:13:54, 404.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6449.93681640625
INFO:root:current train perplexity3.5867927074432373
INFO:root:current mean train loss 6478.152709056713
INFO:root:current train perplexity3.588099241256714
INFO:root:current mean train loss 6484.613536818484
INFO:root:current train perplexity3.586005926132202
INFO:root:current mean train loss 6486.090906308303
INFO:root:current train perplexity3.592108964920044
INFO:root:current mean train loss 6486.335738820043
INFO:root:current train perplexity3.5954573154449463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.61s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 12054.187790643602
INFO:root:eval perplexity: 12.282694816589355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [12:22:36<10:07:00, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6473.347806490385
INFO:root:current train perplexity3.5849435329437256
INFO:root:current mean train loss 6460.691462455036
INFO:root:current train perplexity3.577355146408081
INFO:root:current mean train loss 6461.291687777851
INFO:root:current train perplexity3.581301212310791
INFO:root:current mean train loss 6476.474832630439
INFO:root:current train perplexity3.5884344577789307
INFO:root:current mean train loss 6478.08039623078
INFO:root:current train perplexity3.590912103652954


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.65s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 12054.537923177084
INFO:root:eval perplexity: 12.283585548400879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [12:29:21<10:00:08, 404.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6426.550599563953
INFO:root:current train perplexity3.5607872009277344
INFO:root:current mean train loss 6476.601046902317
INFO:root:current train perplexity3.5766191482543945
INFO:root:current mean train loss 6457.529449588477
INFO:root:current train perplexity3.5739243030548096
INFO:root:current mean train loss 6465.823448603772
INFO:root:current train perplexity3.5768306255340576
INFO:root:current mean train loss 6464.792501410835
INFO:root:current train perplexity3.5789988040924072


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.96s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 12064.866019112724
INFO:root:eval perplexity: 12.310014724731445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [12:36:06<9:53:28, 404.64s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6460.606040142952
INFO:root:current train perplexity3.553933620452881
INFO:root:current mean train loss 6444.054813722364
INFO:root:current train perplexity3.563958168029785
INFO:root:current mean train loss 6437.250170008856
INFO:root:current train perplexity3.5657451152801514
INFO:root:current mean train loss 6447.695254806827
INFO:root:current train perplexity3.570629596710205
INFO:root:current mean train loss 6456.3998597420305
INFO:root:current train perplexity3.573903799057007


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.04s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 12081.841340564546
INFO:root:eval perplexity: 12.353572845458984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [12:42:50<9:46:46, 404.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6409.908653109681
INFO:root:current train perplexity3.541137218475342
INFO:root:current mean train loss 6426.257304816846
INFO:root:current train perplexity3.5525436401367188
INFO:root:current mean train loss 6442.593849212525
INFO:root:current train perplexity3.558875799179077
INFO:root:current mean train loss 6442.6886977497325
INFO:root:current train perplexity3.5651772022247314
INFO:root:current mean train loss 6449.227065938539
INFO:root:current train perplexity3.5661137104034424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.63s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 12083.830987839472
INFO:root:eval perplexity: 12.35869026184082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [12:49:35<9:39:55, 404.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6432.339834872159
INFO:root:current train perplexity3.541762590408325
INFO:root:current mean train loss 6435.654523689516
INFO:root:current train perplexity3.5511043071746826
INFO:root:current mean train loss 6430.383668428309
INFO:root:current train perplexity3.5553224086761475
INFO:root:current mean train loss 6432.483080710827
INFO:root:current train perplexity3.5553338527679443
INFO:root:current mean train loss 6435.0146634615385
INFO:root:current train perplexity3.5582497119903564


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.45s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.89s/it]
INFO:root:eval mean loss: 12100.30369640532
INFO:root:eval perplexity: 12.401119232177734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [12:56:20<9:33:26, 404.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6398.270913334216
INFO:root:current train perplexity3.5433688163757324
INFO:root:current mean train loss 6414.764187794812
INFO:root:current train perplexity3.547779083251953
INFO:root:current mean train loss 6425.581407230333
INFO:root:current train perplexity3.5499343872070312
INFO:root:current mean train loss 6427.031398252524
INFO:root:current train perplexity3.5534818172454834
INFO:root:current mean train loss 6429.278256484886
INFO:root:current train perplexity3.555655002593994


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.41s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.91s/it]
INFO:root:eval mean loss: 12095.39354015532
INFO:root:eval perplexity: 12.388456344604492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [13:03:05<9:26:51, 404.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6407.3222888764885
INFO:root:current train perplexity3.522362470626831
INFO:root:current mean train loss 6406.55145226227
INFO:root:current train perplexity3.530189275741577
INFO:root:current mean train loss 6413.652826461502
INFO:root:current train perplexity3.542999744415283
INFO:root:current mean train loss 6413.99439485408
INFO:root:current train perplexity3.5452842712402344
INFO:root:current mean train loss 6412.74492524973
INFO:root:current train perplexity3.545542001724243


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.16s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 12110.839140392485
INFO:root:eval perplexity: 12.428339004516602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [13:09:50<9:20:07, 404.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6382.2968094099815
INFO:root:current train perplexity3.522977113723755
INFO:root:current mean train loss 6384.854585750374
INFO:root:current train perplexity3.528409481048584
INFO:root:current mean train loss 6394.211952466643
INFO:root:current train perplexity3.5339386463165283
INFO:root:current mean train loss 6402.939942736717
INFO:root:current train perplexity3.5353338718414307
INFO:root:current mean train loss 6405.563097020543
INFO:root:current train perplexity3.5382158756256104


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.46s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 12109.586521693638
INFO:root:eval perplexity: 12.425097465515137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [13:16:34<9:13:06, 404.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6371.696282185299
INFO:root:current train perplexity3.511672019958496
INFO:root:current mean train loss 6389.589943690607
INFO:root:current train perplexity3.526721239089966
INFO:root:current mean train loss 6389.920581324954
INFO:root:current train perplexity3.52675724029541
INFO:root:current mean train loss 6388.777830715128
INFO:root:current train perplexity3.5272979736328125
INFO:root:current mean train loss 6395.981167479432
INFO:root:current train perplexity3.5302395820617676


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.13s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.90s/it]
INFO:root:eval mean loss: 12112.873195103237
INFO:root:eval perplexity: 12.43359661102295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [13:23:19<9:06:26, 404.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6364.744576822916
INFO:root:current train perplexity3.518916368484497
INFO:root:current mean train loss 6368.081342075893
INFO:root:current train perplexity3.5138580799102783
INFO:root:current mean train loss 6378.1564861505685
INFO:root:current train perplexity3.5164270401000977
INFO:root:current mean train loss 6383.111333333333
INFO:root:current train perplexity3.5206727981567383
INFO:root:current mean train loss 6386.221960320724
INFO:root:current train perplexity3.524195671081543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.21s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 12130.41307140532
INFO:root:eval perplexity: 12.479058265686035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [13:30:03<8:59:24, 404.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6367.368571993671
INFO:root:current train perplexity3.5101959705352783
INFO:root:current mean train loss 6375.85400390625
INFO:root:current train perplexity3.5060136318206787
INFO:root:current mean train loss 6370.361016605063
INFO:root:current train perplexity3.5107851028442383
INFO:root:current mean train loss 6379.57657254906
INFO:root:current train perplexity3.5177695751190186
INFO:root:current mean train loss 6376.4249514776875
INFO:root:current train perplexity3.516960859298706


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.88s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.86s/it]
INFO:root:eval mean loss: 12159.463454473585
INFO:root:eval perplexity: 12.554718017578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [13:36:48<8:52:40, 404.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6336.261112810617
INFO:root:current train perplexity3.504434108734131
INFO:root:current mean train loss 6352.340721588968
INFO:root:current train perplexity3.505969285964966
INFO:root:current mean train loss 6356.187776060071
INFO:root:current train perplexity3.5083444118499756
INFO:root:current mean train loss 6366.36019475155
INFO:root:current train perplexity3.5106332302093506
INFO:root:current mean train loss 6364.092833082622
INFO:root:current train perplexity3.510634422302246


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 12171.46175420852
INFO:root:eval perplexity: 12.586102485656738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [13:43:33<8:46:08, 404.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6353.033708243534
INFO:root:current train perplexity3.4983901977539062
INFO:root:current mean train loss 6342.714668804311
INFO:root:current train perplexity3.495567798614502
INFO:root:current mean train loss 6350.029208405924
INFO:root:current train perplexity3.498795509338379
INFO:root:current mean train loss 6355.491363604247
INFO:root:current train perplexity3.5004653930664062
INFO:root:current mean train loss 6357.170120395919
INFO:root:current train perplexity3.502821922302246


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.24s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.83s/it]
INFO:root:eval mean loss: 12161.309480212984
INFO:root:eval perplexity: 12.559540748596191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [13:50:18<8:39:28, 404.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6329.141242058722
INFO:root:current train perplexity3.4860079288482666
INFO:root:current mean train loss 6334.003356614038
INFO:root:current train perplexity3.4904229640960693
INFO:root:current mean train loss 6340.991140463918
INFO:root:current train perplexity3.4926555156707764
INFO:root:current mean train loss 6342.656569693095
INFO:root:current train perplexity3.4957637786865234
INFO:root:current mean train loss 6347.817269443737
INFO:root:current train perplexity3.4990320205688477


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:58<00:00, 358.82s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.94s/it]
INFO:root:eval mean loss: 12175.043779645648
INFO:root:eval perplexity: 12.595486640930176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [13:57:02<8:32:17, 404.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6317.140537623355
INFO:root:current train perplexity3.4790289402008057
INFO:root:current mean train loss 6325.030731670673
INFO:root:current train perplexity3.4872283935546875
INFO:root:current mean train loss 6325.80205078125
INFO:root:current train perplexity3.4876489639282227
INFO:root:current mean train loss 6340.359336679193
INFO:root:current train perplexity3.4933767318725586
INFO:root:current mean train loss 6338.293029908459
INFO:root:current train perplexity3.4908838272094727


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [06:00<00:00, 360.25s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.82s/it]
INFO:root:eval mean loss: 12181.054664248511
INFO:root:eval perplexity: 12.611249923706055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [14:03:46<8:25:43, 404.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6321.994323113952
INFO:root:current train perplexity3.4735870361328125
INFO:root:current mean train loss 6314.71784214039
INFO:root:current train perplexity3.477936267852783
INFO:root:current mean train loss 6320.442183580685
INFO:root:current train perplexity3.479790449142456
INFO:root:current mean train loss 6324.610511875391
INFO:root:current train perplexity3.482591152191162


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [05:59<00:00, 359.35s/it]
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.93s/it]
INFO:root:eval mean loss: 12199.463913690477
INFO:root:eval perplexity: 12.659649848937988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: big_baseline_roberta_64_low/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [14:10:31<8:18:49, 404.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6188.675944010417
INFO:root:current train perplexity3.374234199523926
INFO:root:current mean train loss 6307.4902580779735
INFO:root:current train perplexity3.4758920669555664
INFO:root:current mean train loss 6300.488363031096
INFO:root:current train perplexity3.47151517868042
slurmstepd: error: *** JOB 25937130 ON gv010 CANCELLED AT 2022-10-16T04:12:40 ***
