INFO:root:Output: multiqal6_allminilml12
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Downloading:   0%|          | 0.00/352 [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 352/352 [00:00<00:00, 729kB/s]
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 3.89MB/s]
Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]Downloading:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 419k/466k [00:00<00:00, 4.18MB/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 4.35MB/s]
Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:00<00:00, 246kB/s]
/scratch/zw2374/public/faiss_db/models.py:436: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v2 and are newly initialized: ['encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.weight', 'cls.predictions.decoder.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.key.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'cls.predictions.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:450: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11593.158489188763
INFO:root:current train perplexity9788.708984375
INFO:root:current mean train loss 9662.100593298524
INFO:root:current train perplexity2105.7001953125
INFO:root:current mean train loss 8521.43803727268
INFO:root:current train perplexity853.1299438476562
INFO:root:current mean train loss 7716.665830836858
INFO:root:current train perplexity444.2658386230469
INFO:root:current mean train loss 7101.782300929985
INFO:root:current train perplexity273.3055725097656
INFO:root:current mean train loss 6625.720352605906
INFO:root:current train perplexity187.7112274169922
INFO:root:current mean train loss 6251.217016566636
INFO:root:current train perplexity139.3173828125
INFO:root:current mean train loss 5953.623883186503
INFO:root:current train perplexity109.55204010009766
INFO:root:current mean train loss 5702.004993341126
INFO:root:current train perplexity89.89201354980469
INFO:root:current mean train loss 5487.999189374922
INFO:root:current train perplexity76.0162353515625
INFO:root:current mean train loss 5306.254224810196
INFO:root:current train perplexity65.73721313476562
INFO:root:current mean train loss 5148.4991983472555
INFO:root:current train perplexity58.0472526550293
INFO:root:current mean train loss 5009.011833208538
INFO:root:current train perplexity51.977909088134766
INFO:root:current mean train loss 4885.60197556709
INFO:root:current train perplexity47.16530990600586
INFO:root:current mean train loss 4774.349609863607
INFO:root:current train perplexity43.23210906982422
INFO:root:current mean train loss 4675.143942961177
INFO:root:current train perplexity39.93971633911133
INFO:root:current mean train loss 4584.296354530698
INFO:root:current train perplexity37.171485900878906
INFO:root:current mean train loss 4502.923956234366
INFO:root:current train perplexity34.84954071044922
INFO:root:current mean train loss 4428.068651855212
INFO:root:current train perplexity32.84141540527344

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.29s/it]
INFO:root:final mean train loss: 4367.8910841641255
INFO:root:final train perplexity: 31.336437225341797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 2826.8866468930073
INFO:root:eval perplexity: 9.83768367767334
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it]
INFO:root:eval mean loss: 3129.088252507203
INFO:root:eval perplexity: 12.923587799072266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/1
  0%|          | 1/200 [10:27<34:40:45, 627.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2977.2345581054688
INFO:root:current train perplexity10.465588569641113
INFO:root:current mean train loss 3017.846014614763
INFO:root:current train perplexity10.663493156433105
INFO:root:current mean train loss 2999.187602855541
INFO:root:current train perplexity10.542976379394531
INFO:root:current mean train loss 2982.775536645817
INFO:root:current train perplexity10.411724090576172
INFO:root:current mean train loss 2962.1726115300107
INFO:root:current train perplexity10.252580642700195
INFO:root:current mean train loss 2947.3143584968507
INFO:root:current train perplexity10.164102554321289
INFO:root:current mean train loss 2935.9981867802608
INFO:root:current train perplexity10.08730697631836
INFO:root:current mean train loss 2921.5676504806434
INFO:root:current train perplexity9.9860200881958
INFO:root:current mean train loss 2908.0674109365427
INFO:root:current train perplexity9.892844200134277
INFO:root:current mean train loss 2901.895396669879
INFO:root:current train perplexity9.831229209899902
INFO:root:current mean train loss 2889.750499094565
INFO:root:current train perplexity9.74166488647461
INFO:root:current mean train loss 2879.093513297351
INFO:root:current train perplexity9.666093826293945
INFO:root:current mean train loss 2869.63576487491
INFO:root:current train perplexity9.60296630859375
INFO:root:current mean train loss 2861.582075959643
INFO:root:current train perplexity9.537758827209473
INFO:root:current mean train loss 2855.497737561242
INFO:root:current train perplexity9.481478691101074
INFO:root:current mean train loss 2845.9889916090347
INFO:root:current train perplexity9.421502113342285
INFO:root:current mean train loss 2837.3717130151126
INFO:root:current train perplexity9.363673210144043
INFO:root:current mean train loss 2829.834629725743
INFO:root:current train perplexity9.30075740814209
INFO:root:current mean train loss 2820.429133209363
INFO:root:current train perplexity9.235050201416016
INFO:root:current mean train loss 2813.5997104206763
INFO:root:current train perplexity9.189770698547363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.31s/it]
INFO:root:final mean train loss: 2808.0010109120885
INFO:root:final train perplexity: 9.157365798950195
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.46s/it]
INFO:root:eval mean loss: 2516.942493628103
INFO:root:eval perplexity: 7.656503200531006
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it]
INFO:root:eval mean loss: 2863.0090106937055
INFO:root:eval perplexity: 10.396275520324707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/2
  1%|          | 2/200 [21:05<34:51:07, 633.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2633.8167317708335
INFO:root:current train perplexity8.034266471862793
INFO:root:current mean train loss 2642.1221786154842
INFO:root:current train perplexity8.03566837310791
INFO:root:current mean train loss 2630.8856395017438
INFO:root:current train perplexity7.999094009399414
INFO:root:current mean train loss 2633.829541455518
INFO:root:current train perplexity7.970338821411133
INFO:root:current mean train loss 2631.7326502282403
INFO:root:current train perplexity7.965538024902344
INFO:root:current mean train loss 2628.1037061737807
INFO:root:current train perplexity7.933630466461182
INFO:root:current mean train loss 2622.817657036804
INFO:root:current train perplexity7.911143779754639
INFO:root:current mean train loss 2621.4699710361956
INFO:root:current train perplexity7.891822814941406
INFO:root:current mean train loss 2617.7960285676772
INFO:root:current train perplexity7.868576526641846
INFO:root:current mean train loss 2611.913291350566
INFO:root:current train perplexity7.837419509887695
INFO:root:current mean train loss 2605.7202807829894
INFO:root:current train perplexity7.806923866271973
INFO:root:current mean train loss 2599.703001313576
INFO:root:current train perplexity7.77199649810791
INFO:root:current mean train loss 2596.1978829067507
INFO:root:current train perplexity7.753152370452881
INFO:root:current mean train loss 2592.40976494734
INFO:root:current train perplexity7.726428508758545
INFO:root:current mean train loss 2589.1259303069664
INFO:root:current train perplexity7.703440189361572
INFO:root:current mean train loss 2584.215762263408
INFO:root:current train perplexity7.674865245819092
INFO:root:current mean train loss 2581.0400353996433
INFO:root:current train perplexity7.65066385269165
INFO:root:current mean train loss 2578.1698826406296
INFO:root:current train perplexity7.63266134262085
INFO:root:current mean train loss 2575.943865570551
INFO:root:current train perplexity7.620139122009277
INFO:root:current mean train loss 2573.037904505525
INFO:root:current train perplexity7.601507186889648

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.07s/it]
INFO:root:final mean train loss: 2570.811581056165
INFO:root:final train perplexity: 7.595050811767578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2377.276327796016
INFO:root:eval perplexity: 6.838726997375488
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it]
INFO:root:eval mean loss: 2744.148604589151
INFO:root:eval perplexity: 9.433243751525879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/3
  2%|â–         | 3/200 [31:37<34:37:53, 632.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2452.2962109375
INFO:root:current train perplexity7.0022807121276855
INFO:root:current mean train loss 2467.73337890625
INFO:root:current train perplexity7.076425075531006
INFO:root:current mean train loss 2478.30744140625
INFO:root:current train perplexity7.064752101898193
INFO:root:current mean train loss 2481.3863037109377
INFO:root:current train perplexity7.040524482727051
INFO:root:current mean train loss 2481.2239276801215
INFO:root:current train perplexity7.042337894439697
INFO:root:current mean train loss 2475.249986905185
INFO:root:current train perplexity7.02583122253418
INFO:root:current mean train loss 2470.5469852388824
INFO:root:current train perplexity7.014680862426758
INFO:root:current mean train loss 2465.820169108073
INFO:root:current train perplexity7.005273818969727
INFO:root:current mean train loss 2464.664629911535
INFO:root:current train perplexity6.985469341278076
INFO:root:current mean train loss 2461.3961632658306
INFO:root:current train perplexity6.967048645019531
INFO:root:current mean train loss 2455.7183862304687
INFO:root:current train perplexity6.953279495239258
INFO:root:current mean train loss 2455.467900284477
INFO:root:current train perplexity6.94895076751709
INFO:root:current mean train loss 2455.6312545898436
INFO:root:current train perplexity6.943876266479492
INFO:root:current mean train loss 2453.840371545862
INFO:root:current train perplexity6.931396961212158
INFO:root:current mean train loss 2452.5512054653827
INFO:root:current train perplexity6.915794372558594
INFO:root:current mean train loss 2450.695431183846
INFO:root:current train perplexity6.910131931304932
INFO:root:current mean train loss 2449.1876605409566
INFO:root:current train perplexity6.897719383239746
INFO:root:current mean train loss 2447.2579873046875
INFO:root:current train perplexity6.885079383850098
INFO:root:current mean train loss 2444.5291593644424
INFO:root:current train perplexity6.875141620635986
INFO:root:current mean train loss 2442.410266050681
INFO:root:current train perplexity6.860651016235352

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.01s/it]
INFO:root:final mean train loss: 2441.0681921515993
INFO:root:final train perplexity: 6.856335639953613
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.58s/it]
INFO:root:eval mean loss: 2287.754637806128
INFO:root:eval perplexity: 6.3611016273498535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2664.154138876191
INFO:root:eval perplexity: 8.835858345031738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/4
  2%|â–         | 4/200 [42:01<34:15:58, 629.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2395.1085587686566
INFO:root:current train perplexity6.543790340423584
INFO:root:current mean train loss 2373.4363640882298
INFO:root:current train perplexity6.456760406494141
INFO:root:current mean train loss 2372.895884538858
INFO:root:current train perplexity6.483566761016846
INFO:root:current mean train loss 2367.4791299679623
INFO:root:current train perplexity6.463782787322998
INFO:root:current mean train loss 2370.7317629044264
INFO:root:current train perplexity6.490531921386719
INFO:root:current mean train loss 2372.479643537464
INFO:root:current train perplexity6.4851298332214355
INFO:root:current mean train loss 2369.2027556778253
INFO:root:current train perplexity6.4787187576293945
INFO:root:current mean train loss 2372.067890032951
INFO:root:current train perplexity6.481386661529541
INFO:root:current mean train loss 2369.6684695621125
INFO:root:current train perplexity6.477331638336182
INFO:root:current mean train loss 2368.307141024916
INFO:root:current train perplexity6.4669084548950195
INFO:root:current mean train loss 2367.8684257071154
INFO:root:current train perplexity6.46332311630249
INFO:root:current mean train loss 2365.2139616429345
INFO:root:current train perplexity6.448463439941406
INFO:root:current mean train loss 2365.7187776512865
INFO:root:current train perplexity6.44520902633667
INFO:root:current mean train loss 2363.03969660482
INFO:root:current train perplexity6.439007759094238
INFO:root:current mean train loss 2360.855043708994
INFO:root:current train perplexity6.426426887512207
INFO:root:current mean train loss 2360.4314371485125
INFO:root:current train perplexity6.421575546264648
INFO:root:current mean train loss 2357.298960666851
INFO:root:current train perplexity6.413846969604492
INFO:root:current mean train loss 2357.5948729501583
INFO:root:current train perplexity6.408985137939453
INFO:root:current mean train loss 2356.0538733492067
INFO:root:current train perplexity6.405745983123779
INFO:root:current mean train loss 2354.516135001946
INFO:root:current train perplexity6.399660587310791

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.44s/it]
INFO:root:final mean train loss: 2353.3163915991004
INFO:root:final train perplexity: 6.397881507873535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2228.911563954455
INFO:root:eval perplexity: 6.065475940704346
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.05s/it]
INFO:root:eval mean loss: 2615.3340350211934
INFO:root:eval perplexity: 8.490026473999023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/5
  2%|â–Ž         | 5/200 [52:25<33:59:50, 627.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2301.144507998512
INFO:root:current train perplexity6.195183753967285
INFO:root:current mean train loss 2319.864197440769
INFO:root:current train perplexity6.198400974273682
INFO:root:current mean train loss 2309.6195747482943
INFO:root:current train perplexity6.17002534866333
INFO:root:current mean train loss 2308.7068440119424
INFO:root:current train perplexity6.176907062530518
INFO:root:current mean train loss 2303.1184319109957
INFO:root:current train perplexity6.165239334106445
INFO:root:current mean train loss 2306.3852720913824
INFO:root:current train perplexity6.161145210266113
INFO:root:current mean train loss 2301.9118115162987
INFO:root:current train perplexity6.152266979217529
INFO:root:current mean train loss 2298.9973161658463
INFO:root:current train perplexity6.138849258422852
INFO:root:current mean train loss 2298.5018193171572
INFO:root:current train perplexity6.131053924560547
INFO:root:current mean train loss 2295.6974055592605
INFO:root:current train perplexity6.123623847961426
INFO:root:current mean train loss 2294.8905462849184
INFO:root:current train perplexity6.11629056930542
INFO:root:current mean train loss 2294.5827392371925
INFO:root:current train perplexity6.1101508140563965
INFO:root:current mean train loss 2294.291476145712
INFO:root:current train perplexity6.103168487548828
INFO:root:current mean train loss 2293.2653064176525
INFO:root:current train perplexity6.098572254180908
INFO:root:current mean train loss 2290.7568185811415
INFO:root:current train perplexity6.095437526702881
INFO:root:current mean train loss 2289.0522751471008
INFO:root:current train perplexity6.0888895988464355
INFO:root:current mean train loss 2288.9379452956828
INFO:root:current train perplexity6.084928035736084
INFO:root:current mean train loss 2288.7060921160096
INFO:root:current train perplexity6.080036163330078
INFO:root:current mean train loss 2287.9228691862395
INFO:root:current train perplexity6.077366828918457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.13s/it]
INFO:root:final mean train loss: 2287.9914363643707
INFO:root:final train perplexity: 6.076613903045654
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.45s/it]
INFO:root:eval mean loss: 2186.8797503532246
INFO:root:eval perplexity: 5.862756729125977
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.82s/it]
INFO:root:eval mean loss: 2578.837510562112
INFO:root:eval perplexity: 8.240362167358398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/6
  3%|â–Ž         | 6/200 [1:02:46<33:41:39, 625.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2298.845458984375
INFO:root:current train perplexity6.093779563903809
INFO:root:current mean train loss 2229.10262245707
INFO:root:current train perplexity5.833639144897461
INFO:root:current mean train loss 2233.7251140537546
INFO:root:current train perplexity5.855423450469971
INFO:root:current mean train loss 2235.2095261887457
INFO:root:current train perplexity5.839833736419678
INFO:root:current mean train loss 2237.652273430194
INFO:root:current train perplexity5.8506622314453125
INFO:root:current mean train loss 2243.142580561533
INFO:root:current train perplexity5.853305339813232
INFO:root:current mean train loss 2244.9019643367824
INFO:root:current train perplexity5.857702732086182
INFO:root:current mean train loss 2246.762164367589
INFO:root:current train perplexity5.865070819854736
INFO:root:current mean train loss 2245.292045526588
INFO:root:current train perplexity5.858341217041016
INFO:root:current mean train loss 2245.4582638756415
INFO:root:current train perplexity5.859232425689697
INFO:root:current mean train loss 2242.7901469868025
INFO:root:current train perplexity5.85706901550293
INFO:root:current mean train loss 2243.3341898212207
INFO:root:current train perplexity5.856051921844482
INFO:root:current mean train loss 2242.159430028199
INFO:root:current train perplexity5.854175567626953
INFO:root:current mean train loss 2240.387901734243
INFO:root:current train perplexity5.847049236297607
INFO:root:current mean train loss 2240.8233691371397
INFO:root:current train perplexity5.847830295562744
INFO:root:current mean train loss 2240.165932753815
INFO:root:current train perplexity5.8481669425964355
INFO:root:current mean train loss 2239.260826973376
INFO:root:current train perplexity5.84506893157959
INFO:root:current mean train loss 2236.811147753734
INFO:root:current train perplexity5.838689804077148
INFO:root:current mean train loss 2236.2787151262537
INFO:root:current train perplexity5.835241317749023
INFO:root:current mean train loss 2236.9663873840045
INFO:root:current train perplexity5.83270788192749

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.46s/it]
INFO:root:final mean train loss: 2235.530852240381
INFO:root:final train perplexity: 5.830333709716797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.70s/it]
INFO:root:eval mean loss: 2152.7453855690383
INFO:root:eval perplexity: 5.703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.11s/it]
INFO:root:eval mean loss: 2548.204774247839
INFO:root:eval perplexity: 8.036486625671387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/7
  4%|â–Ž         | 7/200 [1:13:24<33:44:04, 629.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2297.303019205729
INFO:root:current train perplexity5.743715286254883
INFO:root:current mean train loss 2209.7079209149892
INFO:root:current train perplexity5.685379981994629
INFO:root:current mean train loss 2204.8408628691227
INFO:root:current train perplexity5.645344257354736
INFO:root:current mean train loss 2194.3263354031546
INFO:root:current train perplexity5.629551887512207
INFO:root:current mean train loss 2204.750897421221
INFO:root:current train perplexity5.66508674621582
INFO:root:current mean train loss 2203.4271094127052
INFO:root:current train perplexity5.656192779541016
INFO:root:current mean train loss 2203.7980506674758
INFO:root:current train perplexity5.655966758728027
INFO:root:current mean train loss 2205.3084097944593
INFO:root:current train perplexity5.662814617156982
INFO:root:current mean train loss 2199.835287005511
INFO:root:current train perplexity5.656825065612793
INFO:root:current mean train loss 2201.3066111047283
INFO:root:current train perplexity5.6576948165893555
INFO:root:current mean train loss 2199.624763653648
INFO:root:current train perplexity5.654060363769531
INFO:root:current mean train loss 2198.48566034143
INFO:root:current train perplexity5.655231475830078
INFO:root:current mean train loss 2196.5296062601024
INFO:root:current train perplexity5.6519060134887695
INFO:root:current mean train loss 2197.799336041232
INFO:root:current train perplexity5.6531901359558105
INFO:root:current mean train loss 2197.956646874862
INFO:root:current train perplexity5.65470552444458
INFO:root:current mean train loss 2197.4489155845995
INFO:root:current train perplexity5.654085159301758
INFO:root:current mean train loss 2195.9629389099196
INFO:root:current train perplexity5.647602558135986
INFO:root:current mean train loss 2196.3490237501364
INFO:root:current train perplexity5.647378921508789
INFO:root:current mean train loss 2195.2863195438194
INFO:root:current train perplexity5.645353317260742
INFO:root:current mean train loss 2194.3246904200137
INFO:root:current train perplexity5.643086910247803

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.28s/it]
INFO:root:final mean train loss: 2193.3641641821696
INFO:root:final train perplexity: 5.639632225036621
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.70s/it]
INFO:root:eval mean loss: 2126.3455057347073
INFO:root:eval perplexity: 5.582648754119873
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.73s/it]
INFO:root:eval mean loss: 2527.987193871897
INFO:root:eval perplexity: 7.904699325561523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/8
  4%|â–         | 8/200 [1:23:46<33:26:14, 626.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2111.095040457589
INFO:root:current train perplexity5.316283226013184
INFO:root:current mean train loss 2146.745580150463
INFO:root:current train perplexity5.455123424530029
INFO:root:current mean train loss 2142.1530746135304
INFO:root:current train perplexity5.460631847381592
INFO:root:current mean train loss 2154.152773729011
INFO:root:current train perplexity5.472497940063477
INFO:root:current mean train loss 2160.318844850036
INFO:root:current train perplexity5.4944071769714355
INFO:root:current mean train loss 2158.596263279425
INFO:root:current train perplexity5.483168601989746
INFO:root:current mean train loss 2158.2692123292936
INFO:root:current train perplexity5.47573709487915
INFO:root:current mean train loss 2157.5511047778486
INFO:root:current train perplexity5.478696823120117
INFO:root:current mean train loss 2155.8774938891747
INFO:root:current train perplexity5.477748870849609
INFO:root:current mean train loss 2158.7989797010778
INFO:root:current train perplexity5.478233337402344
INFO:root:current mean train loss 2159.544309046649
INFO:root:current train perplexity5.479924201965332
INFO:root:current mean train loss 2160.2714585627755
INFO:root:current train perplexity5.4855217933654785
INFO:root:current mean train loss 2157.9924814571737
INFO:root:current train perplexity5.484391689300537
INFO:root:current mean train loss 2156.319417957777
INFO:root:current train perplexity5.481079578399658
INFO:root:current mean train loss 2156.6130345573824
INFO:root:current train perplexity5.479254245758057
INFO:root:current mean train loss 2157.4957627684753
INFO:root:current train perplexity5.482050895690918
INFO:root:current mean train loss 2158.20061214342
INFO:root:current train perplexity5.482867240905762
INFO:root:current mean train loss 2158.1677970776295
INFO:root:current train perplexity5.483758449554443
INFO:root:current mean train loss 2157.3470065166257
INFO:root:current train perplexity5.481257915496826
INFO:root:current mean train loss 2157.977601138566
INFO:root:current train perplexity5.481351375579834

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.38s/it]
INFO:root:final mean train loss: 2157.0246718383114
INFO:root:final train perplexity: 5.480298042297363
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.74s/it]
INFO:root:eval mean loss: 2105.036708534187
INFO:root:eval perplexity: 5.487265110015869
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.61s/it]
INFO:root:eval mean loss: 2511.7629974581673
INFO:root:eval perplexity: 7.800508975982666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/9
  4%|â–         | 9/200 [1:34:08<33:10:52, 625.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2090.3430786132812
INFO:root:current train perplexity5.325497150421143
INFO:root:current mean train loss 2127.5010375976562
INFO:root:current train perplexity5.392877578735352
INFO:root:current mean train loss 2137.975033714658
INFO:root:current train perplexity5.3862528800964355
INFO:root:current mean train loss 2132.3067522915926
INFO:root:current train perplexity5.379032135009766
INFO:root:current mean train loss 2129.5429379623547
INFO:root:current train perplexity5.372306823730469
INFO:root:current mean train loss 2126.7324990535126
INFO:root:current train perplexity5.361644744873047
INFO:root:current mean train loss 2126.792082055215
INFO:root:current train perplexity5.361320972442627
INFO:root:current mean train loss 2126.5140767198927
INFO:root:current train perplexity5.358229160308838
INFO:root:current mean train loss 2129.956756305247
INFO:root:current train perplexity5.362182140350342
INFO:root:current mean train loss 2128.2150049289735
INFO:root:current train perplexity5.358677864074707
INFO:root:current mean train loss 2128.2315821194375
INFO:root:current train perplexity5.356072902679443
INFO:root:current mean train loss 2129.63272857666
INFO:root:current train perplexity5.359278202056885
INFO:root:current mean train loss 2127.2919353448547
INFO:root:current train perplexity5.356344699859619
INFO:root:current mean train loss 2128.203857421875
INFO:root:current train perplexity5.35320520401001
INFO:root:current mean train loss 2128.5987521925576
INFO:root:current train perplexity5.3532609939575195
INFO:root:current mean train loss 2127.9684118683804
INFO:root:current train perplexity5.35222864151001
INFO:root:current mean train loss 2129.073317631682
INFO:root:current train perplexity5.3543853759765625
INFO:root:current mean train loss 2129.2788630794716
INFO:root:current train perplexity5.354752063751221
INFO:root:current mean train loss 2127.9614523440664
INFO:root:current train perplexity5.351280212402344
INFO:root:current mean train loss 2128.1850325787655
INFO:root:current train perplexity5.351290702819824

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.42s/it]
INFO:root:final mean train loss: 2126.2593385943605
INFO:root:final train perplexity: 5.348926544189453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it]
INFO:root:eval mean loss: 2088.7172037760415
INFO:root:eval perplexity: 5.415319442749023
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.75s/it]
INFO:root:eval mean loss: 2496.9730406416224
INFO:root:eval perplexity: 7.706722736358643
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/10
  5%|â–Œ         | 10/200 [1:44:29<32:56:16, 624.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2110.2426421676855
INFO:root:current train perplexity5.265480995178223
INFO:root:current mean train loss 2118.310614772097
INFO:root:current train perplexity5.269263744354248
INFO:root:current mean train loss 2114.3980971552623
INFO:root:current train perplexity5.254815578460693
INFO:root:current mean train loss 2108.774842797256
INFO:root:current train perplexity5.241291046142578
INFO:root:current mean train loss 2107.4897991904318
INFO:root:current train perplexity5.243314266204834
INFO:root:current mean train loss 2102.9539839974186
INFO:root:current train perplexity5.240621566772461
INFO:root:current mean train loss 2100.6447680919514
INFO:root:current train perplexity5.232303142547607
INFO:root:current mean train loss 2102.4650934464908
INFO:root:current train perplexity5.241177558898926
INFO:root:current mean train loss 2101.254187756221
INFO:root:current train perplexity5.24080228805542
INFO:root:current mean train loss 2100.8344943240454
INFO:root:current train perplexity5.24001407623291
INFO:root:current mean train loss 2099.844226405373
INFO:root:current train perplexity5.237377643585205
INFO:root:current mean train loss 2101.220870201561
INFO:root:current train perplexity5.236307144165039
INFO:root:current mean train loss 2100.947014750788
INFO:root:current train perplexity5.238988876342773
INFO:root:current mean train loss 2100.779909723253
INFO:root:current train perplexity5.235084056854248
INFO:root:current mean train loss 2101.550575084959
INFO:root:current train perplexity5.239845275878906
INFO:root:current mean train loss 2100.4850829144507
INFO:root:current train perplexity5.236605167388916
INFO:root:current mean train loss 2099.31517684324
INFO:root:current train perplexity5.233180999755859
INFO:root:current mean train loss 2099.3031634497333
INFO:root:current train perplexity5.231257915496826
INFO:root:current mean train loss 2098.7071684076545
INFO:root:current train perplexity5.228713035583496
INFO:root:current mean train loss 2099.2995893750594
INFO:root:current train perplexity5.234510898590088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.36s/it]
INFO:root:final mean train loss: 2098.6358223980988
INFO:root:final train perplexity: 5.233658313751221
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 2075.1633430643283
INFO:root:eval perplexity: 5.356282711029053
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 2487.0075484471963
INFO:root:eval perplexity: 7.644169330596924
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/11
  6%|â–Œ         | 11/200 [1:54:59<32:52:17, 626.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2083.0568152139354
INFO:root:current train perplexity5.1072540283203125
INFO:root:current mean train loss 2065.718501265331
INFO:root:current train perplexity5.097214221954346
INFO:root:current mean train loss 2065.2499210384344
INFO:root:current train perplexity5.097295761108398
INFO:root:current mean train loss 2069.1689857917745
INFO:root:current train perplexity5.11344575881958
INFO:root:current mean train loss 2064.5846555105454
INFO:root:current train perplexity5.1202392578125
INFO:root:current mean train loss 2068.4515599586043
INFO:root:current train perplexity5.1217474937438965
INFO:root:current mean train loss 2070.234820574435
INFO:root:current train perplexity5.114648342132568
INFO:root:current mean train loss 2070.581055464029
INFO:root:current train perplexity5.112480640411377
INFO:root:current mean train loss 2071.2202760166833
INFO:root:current train perplexity5.1151862144470215
INFO:root:current mean train loss 2073.3255392387964
INFO:root:current train perplexity5.1200852394104
INFO:root:current mean train loss 2074.5260239068975
INFO:root:current train perplexity5.12625789642334
INFO:root:current mean train loss 2075.243941668917
INFO:root:current train perplexity5.1280837059021
INFO:root:current mean train loss 2075.2657474500024
INFO:root:current train perplexity5.128191947937012
INFO:root:current mean train loss 2074.8991501933397
INFO:root:current train perplexity5.129812717437744
INFO:root:current mean train loss 2073.7242572111845
INFO:root:current train perplexity5.1318745613098145
INFO:root:current mean train loss 2075.576243608626
INFO:root:current train perplexity5.135265350341797
INFO:root:current mean train loss 2074.6252634720445
INFO:root:current train perplexity5.135536193847656
INFO:root:current mean train loss 2075.0127823200633
INFO:root:current train perplexity5.136603355407715
INFO:root:current mean train loss 2074.981472173147
INFO:root:current train perplexity5.138505458831787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.29s/it]
INFO:root:final mean train loss: 2075.2836196291523
INFO:root:final train perplexity: 5.138152599334717
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it]
INFO:root:eval mean loss: 2063.728464545933
INFO:root:eval perplexity: 5.306976795196533
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.90s/it]
INFO:root:eval mean loss: 2477.400380236037
INFO:root:eval perplexity: 7.584343910217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/12
  6%|â–Œ         | 12/200 [2:05:19<32:35:25, 624.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2123.1092122395835
INFO:root:current train perplexity4.9566779136657715
INFO:root:current mean train loss 2054.195429829718
INFO:root:current train perplexity5.05586576461792
INFO:root:current mean train loss 2046.1875505118535
INFO:root:current train perplexity5.023025989532471
INFO:root:current mean train loss 2050.9484263001495
INFO:root:current train perplexity5.029048442840576
INFO:root:current mean train loss 2049.2671280096542
INFO:root:current train perplexity5.025984764099121
INFO:root:current mean train loss 2050.3205124720425
INFO:root:current train perplexity5.032874584197998
INFO:root:current mean train loss 2053.886061432745
INFO:root:current train perplexity5.04929256439209
INFO:root:current mean train loss 2051.016539051387
INFO:root:current train perplexity5.036759376525879
INFO:root:current mean train loss 2051.0208566934057
INFO:root:current train perplexity5.036954879760742
INFO:root:current mean train loss 2051.880119382873
INFO:root:current train perplexity5.04235315322876
INFO:root:current mean train loss 2050.717113065102
INFO:root:current train perplexity5.037120819091797
INFO:root:current mean train loss 2050.7079121926
INFO:root:current train perplexity5.0425591468811035
INFO:root:current mean train loss 2049.7543721060306
INFO:root:current train perplexity5.0425333976745605
INFO:root:current mean train loss 2050.2164945565823
INFO:root:current train perplexity5.043668746948242
INFO:root:current mean train loss 2051.1564095701733
INFO:root:current train perplexity5.041547775268555
INFO:root:current mean train loss 2051.0044893936088
INFO:root:current train perplexity5.040226936340332
INFO:root:current mean train loss 2052.3105538047557
INFO:root:current train perplexity5.045994758605957
INFO:root:current mean train loss 2052.9623705896993
INFO:root:current train perplexity5.047785758972168
INFO:root:current mean train loss 2053.225519519876
INFO:root:current train perplexity5.0476837158203125
INFO:root:current mean train loss 2054.185764010054
INFO:root:current train perplexity5.049657821655273

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.47s/it]
INFO:root:final mean train loss: 2053.169567206263
INFO:root:final train perplexity: 5.049317359924316
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.28s/it]
INFO:root:eval mean loss: 2053.08574920005
INFO:root:eval perplexity: 5.261495590209961
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it]
INFO:root:eval mean loss: 2469.6892345239085
INFO:root:eval perplexity: 7.5366644859313965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/13
  6%|â–‹         | 13/200 [2:15:46<32:27:38, 624.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1996.2493713378906
INFO:root:current train perplexity4.896017074584961
INFO:root:current mean train loss 2020.1792399088542
INFO:root:current train perplexity4.907964706420898
INFO:root:current mean train loss 2020.9495383522728
INFO:root:current train perplexity4.942152500152588
INFO:root:current mean train loss 2023.423293685913
INFO:root:current train perplexity4.935082912445068
INFO:root:current mean train loss 2024.4725533621652
INFO:root:current train perplexity4.936690807342529
INFO:root:current mean train loss 2029.6512894850512
INFO:root:current train perplexity4.9451727867126465
INFO:root:current mean train loss 2026.405563452936
INFO:root:current train perplexity4.936914920806885
INFO:root:current mean train loss 2025.0353725857206
INFO:root:current train perplexity4.937830448150635
INFO:root:current mean train loss 2024.7189088402724
INFO:root:current train perplexity4.945000648498535
INFO:root:current mean train loss 2024.3436321756114
INFO:root:current train perplexity4.947807312011719
INFO:root:current mean train loss 2026.1505104214536
INFO:root:current train perplexity4.951216697692871
INFO:root:current mean train loss 2026.873835536412
INFO:root:current train perplexity4.9544782638549805
INFO:root:current mean train loss 2027.4054421346696
INFO:root:current train perplexity4.952199935913086
INFO:root:current mean train loss 2029.561167306611
INFO:root:current train perplexity4.953927516937256
INFO:root:current mean train loss 2029.8484266684088
INFO:root:current train perplexity4.9588093757629395
INFO:root:current mean train loss 2028.96734980533
INFO:root:current train perplexity4.960297584533691
INFO:root:current mean train loss 2029.6093137387877
INFO:root:current train perplexity4.961724281311035
INFO:root:current mean train loss 2031.5655303955077
INFO:root:current train perplexity4.963354110717773
INFO:root:current mean train loss 2030.976046384036
INFO:root:current train perplexity4.963441848754883
INFO:root:current mean train loss 2031.6549059549968
INFO:root:current train perplexity4.96677303314209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.82s/it]
INFO:root:final mean train loss: 2032.4425258742278
INFO:root:final train perplexity: 4.967448711395264
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 2043.9955098037178
INFO:root:eval perplexity: 5.222956657409668
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.83s/it]
INFO:root:eval mean loss: 2465.267775082419
INFO:root:eval perplexity: 7.509461879730225
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/14
  7%|â–‹         | 14/200 [2:26:11<32:18:01, 625.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2039.2345366606842
INFO:root:current train perplexity4.806553363800049
INFO:root:current mean train loss 2025.5763518618842
INFO:root:current train perplexity4.876393795013428
INFO:root:current mean train loss 2015.2137451171875
INFO:root:current train perplexity4.871490001678467
INFO:root:current mean train loss 2013.2684260971114
INFO:root:current train perplexity4.8643059730529785
INFO:root:current mean train loss 2013.9054510958954
INFO:root:current train perplexity4.868905067443848
INFO:root:current mean train loss 2016.4181196898278
INFO:root:current train perplexity4.879777908325195
INFO:root:current mean train loss 2013.0905010516826
INFO:root:current train perplexity4.876372337341309
INFO:root:current mean train loss 2012.1000254409769
INFO:root:current train perplexity4.877620697021484
INFO:root:current mean train loss 2012.5620083643687
INFO:root:current train perplexity4.8768630027771
INFO:root:current mean train loss 2010.0226631286687
INFO:root:current train perplexity4.873308181762695
INFO:root:current mean train loss 2009.6831403123492
INFO:root:current train perplexity4.876784801483154
INFO:root:current mean train loss 2010.2444837531607
INFO:root:current train perplexity4.879904747009277
INFO:root:current mean train loss 2010.511017807858
INFO:root:current train perplexity4.880842685699463
INFO:root:current mean train loss 2011.922926429857
INFO:root:current train perplexity4.885222434997559
INFO:root:current mean train loss 2010.7652852078984
INFO:root:current train perplexity4.881449222564697
INFO:root:current mean train loss 2012.582992325375
INFO:root:current train perplexity4.885812282562256
INFO:root:current mean train loss 2012.932208024516
INFO:root:current train perplexity4.887392044067383
INFO:root:current mean train loss 2013.4689887996096
INFO:root:current train perplexity4.890140533447266
INFO:root:current mean train loss 2013.59882445691
INFO:root:current train perplexity4.891787528991699
INFO:root:current mean train loss 2014.08128924904
INFO:root:current train perplexity4.894914627075195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.81s/it]
INFO:root:final mean train loss: 2014.540533776603
INFO:root:final train perplexity: 4.89780855178833
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.97s/it]
INFO:root:eval mean loss: 2035.1816129210993
INFO:root:eval perplexity: 5.185857772827148
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.96s/it]
INFO:root:eval mean loss: 2457.421279799008
INFO:root:eval perplexity: 7.461427688598633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/15
  8%|â–Š         | 15/200 [2:36:32<32:03:40, 623.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1969.4925695348668
INFO:root:current train perplexity4.7836594581604
INFO:root:current mean train loss 1986.6199110947646
INFO:root:current train perplexity4.827191352844238
INFO:root:current mean train loss 1994.085050808163
INFO:root:current train perplexity4.83341121673584
INFO:root:current mean train loss 1990.9848277636167
INFO:root:current train perplexity4.823858737945557
INFO:root:current mean train loss 1992.1738235540852
INFO:root:current train perplexity4.8151774406433105
INFO:root:current mean train loss 1996.7608019005952
INFO:root:current train perplexity4.8296661376953125
INFO:root:current mean train loss 2000.033112225547
INFO:root:current train perplexity4.834812641143799
INFO:root:current mean train loss 1997.1927451379104
INFO:root:current train perplexity4.8242692947387695
INFO:root:current mean train loss 1997.482859412736
INFO:root:current train perplexity4.828246593475342
INFO:root:current mean train loss 1997.2385578915257
INFO:root:current train perplexity4.8298797607421875
INFO:root:current mean train loss 2000.2318425621886
INFO:root:current train perplexity4.832855701446533
INFO:root:current mean train loss 1997.5615130710437
INFO:root:current train perplexity4.82867431640625
INFO:root:current mean train loss 1998.5629826352547
INFO:root:current train perplexity4.831801414489746
INFO:root:current mean train loss 1999.909536432128
INFO:root:current train perplexity4.830681800842285
INFO:root:current mean train loss 2000.1284446663838
INFO:root:current train perplexity4.831634521484375
INFO:root:current mean train loss 1999.2508261347982
INFO:root:current train perplexity4.831737518310547
INFO:root:current mean train loss 1998.1328194374905
INFO:root:current train perplexity4.830094814300537
INFO:root:current mean train loss 1998.1549572656472
INFO:root:current train perplexity4.831482887268066
INFO:root:current mean train loss 1998.6239767558109
INFO:root:current train perplexity4.83254337310791
INFO:root:current mean train loss 1997.5881160864933
INFO:root:current train perplexity4.830942630767822

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.68s/it]
INFO:root:final mean train loss: 1997.285030116833
INFO:root:final train perplexity: 4.831606864929199
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.23s/it]
INFO:root:eval mean loss: 2031.9760724872563
INFO:root:eval perplexity: 5.172431945800781
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.04s/it]
INFO:root:eval mean loss: 2457.675635371648
INFO:root:eval perplexity: 7.462979316711426
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/16
  8%|â–Š         | 16/200 [2:47:05<32:01:02, 626.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2006.9008376430459
INFO:root:current train perplexity4.785216808319092
INFO:root:current mean train loss 1985.8135807862757
INFO:root:current train perplexity4.763967990875244
INFO:root:current mean train loss 1982.9941653994176
INFO:root:current train perplexity4.768491744995117
INFO:root:current mean train loss 1983.224788038557
INFO:root:current train perplexity4.76654052734375
INFO:root:current mean train loss 1981.400016898056
INFO:root:current train perplexity4.771397590637207
INFO:root:current mean train loss 1980.6449848983418
INFO:root:current train perplexity4.768097877502441
INFO:root:current mean train loss 1979.9483564351249
INFO:root:current train perplexity4.759637832641602
INFO:root:current mean train loss 1980.2965873193803
INFO:root:current train perplexity4.762172698974609
INFO:root:current mean train loss 1979.9691190139297
INFO:root:current train perplexity4.762610912322998
INFO:root:current mean train loss 1979.4554458445305
INFO:root:current train perplexity4.763720512390137
INFO:root:current mean train loss 1978.9301424997082
INFO:root:current train perplexity4.765079021453857
INFO:root:current mean train loss 1978.1971809784638
INFO:root:current train perplexity4.761712551116943
INFO:root:current mean train loss 1977.8376378405292
INFO:root:current train perplexity4.758040904998779
INFO:root:current mean train loss 1979.1968146504319
INFO:root:current train perplexity4.761594772338867
INFO:root:current mean train loss 1980.1094156624426
INFO:root:current train perplexity4.764688491821289
INFO:root:current mean train loss 1979.8220730787018
INFO:root:current train perplexity4.7665815353393555
INFO:root:current mean train loss 1979.6573328535262
INFO:root:current train perplexity4.766568183898926
INFO:root:current mean train loss 1978.921198547122
INFO:root:current train perplexity4.763349533081055
INFO:root:current mean train loss 1979.1866006856294
INFO:root:current train perplexity4.764496803283691
INFO:root:current mean train loss 1980.3521131479697
INFO:root:current train perplexity4.765244960784912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.74s/it]
INFO:root:final mean train loss: 1980.0892213022114
INFO:root:final train perplexity: 4.7665252685546875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.39s/it]
INFO:root:eval mean loss: 2023.4402292670934
INFO:root:eval perplexity: 5.136847496032715
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it]
INFO:root:eval mean loss: 2451.6439343175143
INFO:root:eval perplexity: 7.426255702972412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/17
  8%|â–Š         | 17/200 [2:57:36<31:55:27, 628.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1971.3658183704722
INFO:root:current train perplexity4.705652236938477
INFO:root:current mean train loss 1950.0285371820978
INFO:root:current train perplexity4.673373222351074
INFO:root:current mean train loss 1954.6005952623154
INFO:root:current train perplexity4.688482761383057
INFO:root:current mean train loss 1963.701203651035
INFO:root:current train perplexity4.69936466217041
INFO:root:current mean train loss 1959.1024840308016
INFO:root:current train perplexity4.6870293617248535
INFO:root:current mean train loss 1964.0757344563801
INFO:root:current train perplexity4.700264930725098
INFO:root:current mean train loss 1960.9260933454646
INFO:root:current train perplexity4.696691513061523
INFO:root:current mean train loss 1960.745355596397
INFO:root:current train perplexity4.699254989624023
INFO:root:current mean train loss 1961.9973075797966
INFO:root:current train perplexity4.703789710998535
INFO:root:current mean train loss 1961.571419781519
INFO:root:current train perplexity4.70252799987793
INFO:root:current mean train loss 1961.853058422313
INFO:root:current train perplexity4.703225612640381
INFO:root:current mean train loss 1962.5841352160933
INFO:root:current train perplexity4.704635143280029
INFO:root:current mean train loss 1963.9168460443152
INFO:root:current train perplexity4.708548545837402
INFO:root:current mean train loss 1963.6622285430644
INFO:root:current train perplexity4.7098917961120605
INFO:root:current mean train loss 1963.49390386766
INFO:root:current train perplexity4.709980010986328
INFO:root:current mean train loss 1962.9576721191406
INFO:root:current train perplexity4.708377838134766
INFO:root:current mean train loss 1963.092976285383
INFO:root:current train perplexity4.709300994873047
INFO:root:current mean train loss 1964.0543948862644
INFO:root:current train perplexity4.7087321281433105
INFO:root:current mean train loss 1966.0908675112967
INFO:root:current train perplexity4.711478233337402

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.06s/it]
INFO:root:final mean train loss: 1965.2379531437139
INFO:root:final train perplexity: 4.711021900177002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.38s/it]
INFO:root:eval mean loss: 2020.8324623919548
INFO:root:eval perplexity: 5.126025676727295
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.81s/it]
INFO:root:eval mean loss: 2453.6424551889404
INFO:root:eval perplexity: 7.438403129577637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/18
  9%|â–‰         | 18/200 [3:08:01<31:41:48, 626.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1855.7723876953125
INFO:root:current train perplexity4.381059646606445
INFO:root:current mean train loss 1918.5382742745535
INFO:root:current train perplexity4.613854885101318
INFO:root:current mean train loss 1929.6145460175305
INFO:root:current train perplexity4.610488414764404
INFO:root:current mean train loss 1939.785901479252
INFO:root:current train perplexity4.624571323394775
INFO:root:current mean train loss 1936.099425817419
INFO:root:current train perplexity4.625066757202148
INFO:root:current mean train loss 1938.2913818359375
INFO:root:current train perplexity4.62280797958374
INFO:root:current mean train loss 1937.3516024502842
INFO:root:current train perplexity4.6208343505859375
INFO:root:current mean train loss 1939.3555534200466
INFO:root:current train perplexity4.6245880126953125
INFO:root:current mean train loss 1942.1380081461084
INFO:root:current train perplexity4.634317874908447
INFO:root:current mean train loss 1946.5078847979973
INFO:root:current train perplexity4.642749786376953
INFO:root:current mean train loss 1946.6353559351678
INFO:root:current train perplexity4.643233299255371
INFO:root:current mean train loss 1947.3071091319641
INFO:root:current train perplexity4.644929885864258
INFO:root:current mean train loss 1947.1623560482042
INFO:root:current train perplexity4.645875453948975
INFO:root:current mean train loss 1947.053569223689
INFO:root:current train perplexity4.645831108093262
INFO:root:current mean train loss 1948.0346799585743
INFO:root:current train perplexity4.645867824554443
INFO:root:current mean train loss 1949.3542654043813
INFO:root:current train perplexity4.648189067840576
INFO:root:current mean train loss 1950.1393556969188
INFO:root:current train perplexity4.650047779083252
INFO:root:current mean train loss 1950.7595362330462
INFO:root:current train perplexity4.652716159820557
INFO:root:current mean train loss 1950.8457599333449
INFO:root:current train perplexity4.654850959777832
INFO:root:current mean train loss 1951.6533401128813
INFO:root:current train perplexity4.657996654510498

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.83s/it]
INFO:root:final mean train loss: 1950.8560775933815
INFO:root:final train perplexity: 4.657890319824219
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.81s/it]
INFO:root:eval mean loss: 2015.8729767495013
INFO:root:eval perplexity: 5.10550594329834
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.97s/it]
INFO:root:eval mean loss: 2448.332354606466
INFO:root:eval perplexity: 7.406169414520264
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/19
 10%|â–‰         | 19/200 [3:18:25<31:28:31, 626.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1904.9214699485085
INFO:root:current train perplexity4.599952220916748
INFO:root:current mean train loss 1920.9278224257173
INFO:root:current train perplexity4.5887346267700195
INFO:root:current mean train loss 1938.9687208570876
INFO:root:current train perplexity4.621610164642334
INFO:root:current mean train loss 1930.615448187597
INFO:root:current train perplexity4.601095676422119
INFO:root:current mean train loss 1927.4831430154954
INFO:root:current train perplexity4.5900349617004395
INFO:root:current mean train loss 1933.301679706208
INFO:root:current train perplexity4.602066516876221
INFO:root:current mean train loss 1932.4164507212747
INFO:root:current train perplexity4.5979323387146
INFO:root:current mean train loss 1935.3175485035058
INFO:root:current train perplexity4.608255863189697
INFO:root:current mean train loss 1933.7642266860553
INFO:root:current train perplexity4.608429431915283
INFO:root:current mean train loss 1934.8846403771522
INFO:root:current train perplexity4.6070170402526855
INFO:root:current mean train loss 1936.2002928970844
INFO:root:current train perplexity4.607188701629639
INFO:root:current mean train loss 1937.8603356781277
INFO:root:current train perplexity4.608641147613525
INFO:root:current mean train loss 1937.7360428280995
INFO:root:current train perplexity4.605820178985596
INFO:root:current mean train loss 1937.041369832049
INFO:root:current train perplexity4.60388708114624
INFO:root:current mean train loss 1936.1792190487374
INFO:root:current train perplexity4.600836753845215
INFO:root:current mean train loss 1936.0456330428456
INFO:root:current train perplexity4.600845813751221
INFO:root:current mean train loss 1937.2015667596668
INFO:root:current train perplexity4.603525638580322
INFO:root:current mean train loss 1936.9116251344049
INFO:root:current train perplexity4.603714942932129
INFO:root:current mean train loss 1937.3657640609993
INFO:root:current train perplexity4.6047539710998535
INFO:root:current mean train loss 1937.475597242659
INFO:root:current train perplexity4.606882095336914

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.25s/it]
INFO:root:final mean train loss: 1936.9765062971783
INFO:root:final train perplexity: 4.607182025909424
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.91s/it]
INFO:root:eval mean loss: 2011.21012413079
INFO:root:eval perplexity: 5.086289882659912
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it]
INFO:root:eval mean loss: 2446.8736221638133
INFO:root:eval perplexity: 7.397340774536133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/20
 10%|â–ˆ         | 20/200 [3:28:59<31:25:17, 628.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1884.5944229517227
INFO:root:current train perplexity4.471899032592773
INFO:root:current mean train loss 1908.9658097740557
INFO:root:current train perplexity4.514167308807373
INFO:root:current mean train loss 1919.5897364915663
INFO:root:current train perplexity4.523293972015381
INFO:root:current mean train loss 1918.6225323072226
INFO:root:current train perplexity4.516270637512207
INFO:root:current mean train loss 1920.9419198351188
INFO:root:current train perplexity4.538620948791504
INFO:root:current mean train loss 1916.2303210879522
INFO:root:current train perplexity4.533391952514648
INFO:root:current mean train loss 1919.4247307575924
INFO:root:current train perplexity4.538427352905273
INFO:root:current mean train loss 1918.9986089930967
INFO:root:current train perplexity4.533194065093994
INFO:root:current mean train loss 1920.9387959240446
INFO:root:current train perplexity4.539675712585449
INFO:root:current mean train loss 1922.2736958106614
INFO:root:current train perplexity4.543437480926514
INFO:root:current mean train loss 1923.4454181219546
INFO:root:current train perplexity4.5465617179870605
INFO:root:current mean train loss 1923.5761246116042
INFO:root:current train perplexity4.550248622894287
INFO:root:current mean train loss 1922.0205267289648
INFO:root:current train perplexity4.549023628234863
INFO:root:current mean train loss 1923.2043312990104
INFO:root:current train perplexity4.548206806182861
INFO:root:current mean train loss 1924.4004126807897
INFO:root:current train perplexity4.549635887145996
INFO:root:current mean train loss 1924.0093724459623
INFO:root:current train perplexity4.552070140838623
INFO:root:current mean train loss 1924.8005749444687
INFO:root:current train perplexity4.552889823913574
INFO:root:current mean train loss 1925.8543762944087
INFO:root:current train perplexity4.556096076965332
INFO:root:current mean train loss 1924.5219395333104
INFO:root:current train perplexity4.553934097290039
INFO:root:current mean train loss 1923.1841473788438
INFO:root:current train perplexity4.555593967437744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.91s/it]
INFO:root:final mean train loss: 1923.180753660755
INFO:root:final train perplexity: 4.557325839996338
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it]
INFO:root:eval mean loss: 2009.0701345890127
INFO:root:eval perplexity: 5.077494144439697
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it]
INFO:root:eval mean loss: 2446.031144811752
INFO:root:eval perplexity: 7.392245292663574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/21
 10%|â–ˆ         | 21/200 [3:39:21<31:09:38, 626.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1911.090351649693
INFO:root:current train perplexity4.506570816040039
INFO:root:current mean train loss 1915.4557800292969
INFO:root:current train perplexity4.491379261016846
INFO:root:current mean train loss 1910.6983432769775
INFO:root:current train perplexity4.494026184082031
INFO:root:current mean train loss 1909.7728501223446
INFO:root:current train perplexity4.493557929992676
INFO:root:current mean train loss 1904.69312969007
INFO:root:current train perplexity4.498816967010498
INFO:root:current mean train loss 1904.1613161375196
INFO:root:current train perplexity4.49474573135376
INFO:root:current mean train loss 1905.1073295779345
INFO:root:current train perplexity4.496413707733154
INFO:root:current mean train loss 1908.96010755105
INFO:root:current train perplexity4.500513076782227
INFO:root:current mean train loss 1904.7231382566079
INFO:root:current train perplexity4.491261959075928
INFO:root:current mean train loss 1907.03404267762
INFO:root:current train perplexity4.496087074279785
INFO:root:current mean train loss 1906.693213260535
INFO:root:current train perplexity4.4968485832214355
INFO:root:current mean train loss 1907.0074586439298
INFO:root:current train perplexity4.499903202056885
INFO:root:current mean train loss 1907.8769696472557
INFO:root:current train perplexity4.501373767852783
INFO:root:current mean train loss 1907.8413440625577
INFO:root:current train perplexity4.499595642089844
INFO:root:current mean train loss 1908.1769206288097
INFO:root:current train perplexity4.499452590942383
INFO:root:current mean train loss 1909.6156398900678
INFO:root:current train perplexity4.503267765045166
INFO:root:current mean train loss 1911.5201398324275
INFO:root:current train perplexity4.507037162780762
INFO:root:current mean train loss 1910.9247839021791
INFO:root:current train perplexity4.507957458496094
INFO:root:current mean train loss 1911.5250624952646
INFO:root:current train perplexity4.510746479034424
INFO:root:current mean train loss 1911.7888886933433
INFO:root:current train perplexity4.514024257659912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.60s/it]
INFO:root:final mean train loss: 1910.9708728415162
INFO:root:final train perplexity: 4.5136518478393555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 2007.0165725599788
INFO:root:eval perplexity: 5.069068431854248
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it]
INFO:root:eval mean loss: 2446.4596038169047
INFO:root:eval perplexity: 7.394834995269775
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/22
 11%|â–ˆ         | 22/200 [3:49:48<30:58:39, 626.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1903.1247090378852
INFO:root:current train perplexity4.444579601287842
INFO:root:current mean train loss 1898.9053454095917
INFO:root:current train perplexity4.439733982086182
INFO:root:current mean train loss 1904.0264159261963
INFO:root:current train perplexity4.440291881561279
INFO:root:current mean train loss 1900.0782536156375
INFO:root:current train perplexity4.437028408050537
INFO:root:current mean train loss 1897.1915610960623
INFO:root:current train perplexity4.434690952301025
INFO:root:current mean train loss 1894.6445112245037
INFO:root:current train perplexity4.435704231262207
INFO:root:current mean train loss 1892.6012862547013
INFO:root:current train perplexity4.430395126342773
INFO:root:current mean train loss 1894.4216019604523
INFO:root:current train perplexity4.437752723693848
INFO:root:current mean train loss 1892.6426568484662
INFO:root:current train perplexity4.441427230834961
INFO:root:current mean train loss 1894.5072922270442
INFO:root:current train perplexity4.4495415687561035
INFO:root:current mean train loss 1896.5198256749475
INFO:root:current train perplexity4.453136444091797
INFO:root:current mean train loss 1896.298494591026
INFO:root:current train perplexity4.453587055206299
INFO:root:current mean train loss 1897.379290105036
INFO:root:current train perplexity4.457191467285156
INFO:root:current mean train loss 1897.6316166910392
INFO:root:current train perplexity4.456223964691162
INFO:root:current mean train loss 1897.6609064893341
INFO:root:current train perplexity4.457054138183594
INFO:root:current mean train loss 1898.3745365518714
INFO:root:current train perplexity4.457935810089111
INFO:root:current mean train loss 1898.7511786753307
INFO:root:current train perplexity4.460796356201172
INFO:root:current mean train loss 1899.3582123370743
INFO:root:current train perplexity4.466711044311523
INFO:root:current mean train loss 1899.9060051424644
INFO:root:current train perplexity4.471577167510986
INFO:root:current mean train loss 1899.7132143928384
INFO:root:current train perplexity4.471685409545898

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.40s/it]
INFO:root:final mean train loss: 1899.0011074048848
INFO:root:final train perplexity: 4.471242427825928
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.82s/it]
INFO:root:eval mean loss: 2006.0954568200077
INFO:root:eval perplexity: 5.0652947425842285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it]
INFO:root:eval mean loss: 2446.282424385666
INFO:root:eval perplexity: 7.393764972686768
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/23
 12%|â–ˆâ–        | 23/200 [4:00:19<30:52:31, 627.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1862.6560302734374
INFO:root:current train perplexity4.353454113006592
INFO:root:current mean train loss 1874.0173185649671
INFO:root:current train perplexity4.386641025543213
INFO:root:current mean train loss 1873.373985974542
INFO:root:current train perplexity4.3915300369262695
INFO:root:current mean train loss 1878.3036877754407
INFO:root:current train perplexity4.397620677947998
INFO:root:current mean train loss 1878.336598174426
INFO:root:current train perplexity4.39922571182251
INFO:root:current mean train loss 1881.5440584861626
INFO:root:current train perplexity4.401703834533691
INFO:root:current mean train loss 1881.7592943274456
INFO:root:current train perplexity4.405704021453857
INFO:root:current mean train loss 1881.9375248776207
INFO:root:current train perplexity4.412101745605469
INFO:root:current mean train loss 1883.03261485582
INFO:root:current train perplexity4.41478157043457
INFO:root:current mean train loss 1881.1036410245028
INFO:root:current train perplexity4.411977767944336
INFO:root:current mean train loss 1881.2830096043579
INFO:root:current train perplexity4.407982349395752
INFO:root:current mean train loss 1882.112034491531
INFO:root:current train perplexity4.412722110748291
INFO:root:current mean train loss 1882.6449476138566
INFO:root:current train perplexity4.416966915130615
INFO:root:current mean train loss 1883.1311781629383
INFO:root:current train perplexity4.419476509094238
INFO:root:current mean train loss 1884.2777997522546
INFO:root:current train perplexity4.42322301864624
INFO:root:current mean train loss 1884.0686801358588
INFO:root:current train perplexity4.42395544052124
INFO:root:current mean train loss 1884.1359507182647
INFO:root:current train perplexity4.425081253051758
INFO:root:current mean train loss 1885.7654277098245
INFO:root:current train perplexity4.425848960876465
INFO:root:current mean train loss 1886.4661265216807
INFO:root:current train perplexity4.427474498748779

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.78s/it]
INFO:root:final mean train loss: 1887.3256610633746
INFO:root:final train perplexity: 4.430261611938477
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2002.8991742506096
INFO:root:eval perplexity: 5.052217960357666
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.50s/it]
INFO:root:eval mean loss: 2448.2181928918717
INFO:root:eval perplexity: 7.405477523803711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/24
 12%|â–ˆâ–        | 24/200 [4:10:39<30:35:03, 625.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1861.500697544643
INFO:root:current train perplexity4.324368953704834
INFO:root:current mean train loss 1862.0378475010953
INFO:root:current train perplexity4.335375785827637
INFO:root:current mean train loss 1864.8586337324502
INFO:root:current train perplexity4.33583402633667
INFO:root:current mean train loss 1862.484268834614
INFO:root:current train perplexity4.340246677398682
INFO:root:current mean train loss 1856.961124354557
INFO:root:current train perplexity4.329817295074463
INFO:root:current mean train loss 1858.4338203144262
INFO:root:current train perplexity4.337671279907227
INFO:root:current mean train loss 1863.6134379102527
INFO:root:current train perplexity4.349841117858887
INFO:root:current mean train loss 1864.7942913222685
INFO:root:current train perplexity4.356700897216797
INFO:root:current mean train loss 1867.2076258700724
INFO:root:current train perplexity4.363239288330078
INFO:root:current mean train loss 1868.77971799739
INFO:root:current train perplexity4.368732452392578
INFO:root:current mean train loss 1870.3202604958649
INFO:root:current train perplexity4.3736772537231445
INFO:root:current mean train loss 1870.6268543219287
INFO:root:current train perplexity4.3733625411987305
INFO:root:current mean train loss 1871.39373905716
INFO:root:current train perplexity4.374305725097656
INFO:root:current mean train loss 1870.4413997121867
INFO:root:current train perplexity4.3731889724731445
INFO:root:current mean train loss 1871.733299445157
INFO:root:current train perplexity4.378206729888916
INFO:root:current mean train loss 1873.3970325978767
INFO:root:current train perplexity4.381721496582031
INFO:root:current mean train loss 1874.2786507455176
INFO:root:current train perplexity4.385109901428223
INFO:root:current mean train loss 1874.3054995142932
INFO:root:current train perplexity4.385814189910889
INFO:root:current mean train loss 1874.7792442503328
INFO:root:current train perplexity4.387730598449707
INFO:root:current mean train loss 1875.2669126593635
INFO:root:current train perplexity4.388119220733643

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.02s/it]
INFO:root:final mean train loss: 1875.8588111250315
INFO:root:final train perplexity: 4.390377044677734
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.65s/it]
INFO:root:eval mean loss: 2001.2747910952737
INFO:root:eval perplexity: 5.045584678649902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it]
INFO:root:eval mean loss: 2446.6017382466202
INFO:root:eval perplexity: 7.395695209503174
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/25
 12%|â–ˆâ–Ž        | 25/200 [4:21:13<30:31:42, 628.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1846.5880177815754
INFO:root:current train perplexity4.318193435668945
INFO:root:current mean train loss 1869.686774469191
INFO:root:current train perplexity4.343288421630859
INFO:root:current mean train loss 1862.587871006557
INFO:root:current train perplexity4.3180975914001465
INFO:root:current mean train loss 1861.4176451129679
INFO:root:current train perplexity4.320333957672119
INFO:root:current mean train loss 1867.2453146880528
INFO:root:current train perplexity4.344061851501465
INFO:root:current mean train loss 1862.7299124448355
INFO:root:current train perplexity4.336544513702393
INFO:root:current mean train loss 1862.8666735918093
INFO:root:current train perplexity4.336552619934082
INFO:root:current mean train loss 1858.830918622939
INFO:root:current train perplexity4.338191986083984
INFO:root:current mean train loss 1860.685683018953
INFO:root:current train perplexity4.340176582336426
INFO:root:current mean train loss 1860.702059130648
INFO:root:current train perplexity4.3423171043396
INFO:root:current mean train loss 1863.5812180042267
INFO:root:current train perplexity4.346421241760254
INFO:root:current mean train loss 1863.2295963382383
INFO:root:current train perplexity4.342683792114258
INFO:root:current mean train loss 1865.2453490612554
INFO:root:current train perplexity4.347298622131348
INFO:root:current mean train loss 1866.1610600681825
INFO:root:current train perplexity4.348202228546143
INFO:root:current mean train loss 1865.983946125159
INFO:root:current train perplexity4.351909160614014
INFO:root:current mean train loss 1865.5570694730663
INFO:root:current train perplexity4.349269866943359
INFO:root:current mean train loss 1866.3988135577422
INFO:root:current train perplexity4.351454257965088
INFO:root:current mean train loss 1866.99680216849
INFO:root:current train perplexity4.352878570556641
INFO:root:current mean train loss 1865.7972299676192
INFO:root:current train perplexity4.352500915527344
INFO:root:current mean train loss 1865.809136404565
INFO:root:current train perplexity4.352560997009277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.34s/it]
INFO:root:final mean train loss: 1865.0867688770072
INFO:root:final train perplexity: 4.353237628936768
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.41s/it]
INFO:root:eval mean loss: 1999.565825333832
INFO:root:eval perplexity: 5.038616180419922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2446.3623544679467
INFO:root:eval perplexity: 7.3942461013793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/26
 13%|â–ˆâ–Ž        | 26/200 [4:31:36<30:17:15, 626.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1820.5933421065167
INFO:root:current train perplexity4.276353359222412
INFO:root:current mean train loss 1834.087941704067
INFO:root:current train perplexity4.263261795043945
INFO:root:current mean train loss 1838.1767537603735
INFO:root:current train perplexity4.273557662963867
INFO:root:current mean train loss 1837.8264518133706
INFO:root:current train perplexity4.282658100128174
INFO:root:current mean train loss 1841.6167767237102
INFO:root:current train perplexity4.290645122528076
INFO:root:current mean train loss 1843.9431709670316
INFO:root:current train perplexity4.290141582489014
INFO:root:current mean train loss 1848.7811547813476
INFO:root:current train perplexity4.306400775909424
INFO:root:current mean train loss 1849.652702218286
INFO:root:current train perplexity4.303778171539307
INFO:root:current mean train loss 1851.4429664566458
INFO:root:current train perplexity4.305451393127441
INFO:root:current mean train loss 1852.2868073774575
INFO:root:current train perplexity4.306833267211914
INFO:root:current mean train loss 1852.2850837817452
INFO:root:current train perplexity4.308578014373779
INFO:root:current mean train loss 1852.206531842271
INFO:root:current train perplexity4.310199737548828
INFO:root:current mean train loss 1850.6852992916183
INFO:root:current train perplexity4.30648946762085
INFO:root:current mean train loss 1852.8180849216128
INFO:root:current train perplexity4.310993194580078
INFO:root:current mean train loss 1852.419646136716
INFO:root:current train perplexity4.310651779174805
INFO:root:current mean train loss 1853.6840986663997
INFO:root:current train perplexity4.313363552093506
INFO:root:current mean train loss 1853.8627806947698
INFO:root:current train perplexity4.312400817871094
INFO:root:current mean train loss 1854.6835056854898
INFO:root:current train perplexity4.314537525177002
INFO:root:current mean train loss 1854.5911834070307
INFO:root:current train perplexity4.314983367919922
INFO:root:current mean train loss 1855.3552040442064
INFO:root:current train perplexity4.317549228668213

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.97s/it]
INFO:root:final mean train loss: 1854.7725322221304
INFO:root:final train perplexity: 4.31796932220459
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 1999.9550408978835
INFO:root:eval perplexity: 5.0402021408081055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.75s/it]
INFO:root:eval mean loss: 2451.7809898430573
INFO:root:eval perplexity: 7.427088260650635
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/27
 14%|â–ˆâ–Ž        | 27/200 [4:41:55<30:00:27, 624.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1834.635843211207
INFO:root:current train perplexity4.211348533630371
INFO:root:current mean train loss 1832.1669952778877
INFO:root:current train perplexity4.2379961013793945
INFO:root:current mean train loss 1833.7437848231589
INFO:root:current train perplexity4.255664348602295
INFO:root:current mean train loss 1830.0653638786443
INFO:root:current train perplexity4.249415397644043
INFO:root:current mean train loss 1832.029693203722
INFO:root:current train perplexity4.253522872924805
INFO:root:current mean train loss 1833.860397284176
INFO:root:current train perplexity4.256287574768066
INFO:root:current mean train loss 1834.1668972026976
INFO:root:current train perplexity4.2572150230407715
INFO:root:current mean train loss 1836.600094274355
INFO:root:current train perplexity4.258334159851074
INFO:root:current mean train loss 1837.3086976093568
INFO:root:current train perplexity4.262704849243164
INFO:root:current mean train loss 1840.0260722054818
INFO:root:current train perplexity4.264972686767578
INFO:root:current mean train loss 1839.1375877798619
INFO:root:current train perplexity4.263355731964111
INFO:root:current mean train loss 1838.1230815564618
INFO:root:current train perplexity4.26114559173584
INFO:root:current mean train loss 1838.5406791068413
INFO:root:current train perplexity4.264543533325195
INFO:root:current mean train loss 1839.124298500207
INFO:root:current train perplexity4.268336772918701
INFO:root:current mean train loss 1840.299394186305
INFO:root:current train perplexity4.2735161781311035
INFO:root:current mean train loss 1841.7957331176287
INFO:root:current train perplexity4.275736331939697
INFO:root:current mean train loss 1842.4404595056403
INFO:root:current train perplexity4.279125213623047
INFO:root:current mean train loss 1843.8337405815603
INFO:root:current train perplexity4.283010482788086
INFO:root:current mean train loss 1844.5208319755366
INFO:root:current train perplexity4.285419464111328
INFO:root:current mean train loss 1846.137150048479
INFO:root:current train perplexity4.286238670349121

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.59s/it]
INFO:root:final mean train loss: 1845.2978618735324
INFO:root:final train perplexity: 4.285824298858643
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 1999.9242783133866
INFO:root:eval perplexity: 5.040076732635498
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it]
INFO:root:eval mean loss: 2452.2680772280864
INFO:root:eval perplexity: 7.430047512054443
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/28
 14%|â–ˆâ–        | 28/200 [4:52:16<29:46:33, 623.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1824.6637809244792
INFO:root:current train perplexity4.221371650695801
INFO:root:current mean train loss 1824.6084361049107
INFO:root:current train perplexity4.207224369049072
INFO:root:current mean train loss 1823.7945237038352
INFO:root:current train perplexity4.204715251922607
INFO:root:current mean train loss 1826.3530807291668
INFO:root:current train perplexity4.210549831390381
INFO:root:current mean train loss 1829.7684151418584
INFO:root:current train perplexity4.225500106811523
INFO:root:current mean train loss 1830.7972053328804
INFO:root:current train perplexity4.226053714752197
INFO:root:current mean train loss 1828.8093422670718
INFO:root:current train perplexity4.226984024047852
INFO:root:current mean train loss 1831.8814769720261
INFO:root:current train perplexity4.235101699829102
INFO:root:current mean train loss 1833.8722858537947
INFO:root:current train perplexity4.236947536468506
INFO:root:current mean train loss 1835.1264806189904
INFO:root:current train perplexity4.244111061096191
INFO:root:current mean train loss 1835.6556336300873
INFO:root:current train perplexity4.240825176239014
INFO:root:current mean train loss 1832.7586443442488
INFO:root:current train perplexity4.237680435180664
INFO:root:current mean train loss 1832.6641328699448
INFO:root:current train perplexity4.239532470703125
INFO:root:current mean train loss 1832.647869762074
INFO:root:current train perplexity4.241306781768799
INFO:root:current mean train loss 1833.9757704912606
INFO:root:current train perplexity4.244924545288086
INFO:root:current mean train loss 1834.3807242063492
INFO:root:current train perplexity4.244900703430176
INFO:root:current mean train loss 1835.1396271571828
INFO:root:current train perplexity4.24627161026001
INFO:root:current mean train loss 1835.1501410513865
INFO:root:current train perplexity4.247216701507568
INFO:root:current mean train loss 1834.85478046875
INFO:root:current train perplexity4.247431755065918
INFO:root:current mean train loss 1835.264988256527
INFO:root:current train perplexity4.250062942504883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.74s/it]
INFO:root:final mean train loss: 1834.6760779922802
INFO:root:final train perplexity: 4.250072002410889
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.90s/it]
INFO:root:eval mean loss: 1998.8923149379432
INFO:root:eval perplexity: 5.035871982574463
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.18s/it]
INFO:root:eval mean loss: 2453.477017017121
INFO:root:eval perplexity: 7.437397480010986
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/29
 14%|â–ˆâ–        | 29/200 [5:02:49<29:44:37, 626.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1818.0816278872283
INFO:root:current train perplexity4.175661563873291
INFO:root:current mean train loss 1819.873342514038
INFO:root:current train perplexity4.1851582527160645
INFO:root:current mean train loss 1816.342070279056
INFO:root:current train perplexity4.187039375305176
INFO:root:current mean train loss 1816.1942029680524
INFO:root:current train perplexity4.194830417633057
INFO:root:current mean train loss 1817.5575248904345
INFO:root:current train perplexity4.1985578536987305
INFO:root:current mean train loss 1820.0901664527687
INFO:root:current train perplexity4.194735050201416
INFO:root:current mean train loss 1817.5831392321284
INFO:root:current train perplexity4.1935296058654785
INFO:root:current mean train loss 1818.9809314458057
INFO:root:current train perplexity4.197409629821777
INFO:root:current mean train loss 1820.5206287880114
INFO:root:current train perplexity4.200352191925049
INFO:root:current mean train loss 1822.4609909057617
INFO:root:current train perplexity4.208812713623047
INFO:root:current mean train loss 1823.122771993225
INFO:root:current train perplexity4.208550930023193
INFO:root:current mean train loss 1824.1807013390048
INFO:root:current train perplexity4.210094928741455
INFO:root:current mean train loss 1824.566198012408
INFO:root:current train perplexity4.212512016296387
INFO:root:current mean train loss 1824.8871542393476
INFO:root:current train perplexity4.213065147399902
INFO:root:current mean train loss 1824.0035528024464
INFO:root:current train perplexity4.212713718414307
INFO:root:current mean train loss 1824.1451113906937
INFO:root:current train perplexity4.216063022613525
INFO:root:current mean train loss 1824.8475395906057
INFO:root:current train perplexity4.217136383056641
INFO:root:current mean train loss 1825.5598566872734
INFO:root:current train perplexity4.216914176940918
INFO:root:current mean train loss 1825.5167975072882
INFO:root:current train perplexity4.216937065124512

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.26s/it]
INFO:root:final mean train loss: 1825.1302995628862
INFO:root:final train perplexity: 4.21819543838501
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.10s/it]
INFO:root:eval mean loss: 1999.9805908203125
INFO:root:eval perplexity: 5.040306091308594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.41s/it]
INFO:root:eval mean loss: 2453.6295230946644
INFO:root:eval perplexity: 7.438324928283691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/30
 15%|â–ˆâ–Œ        | 30/200 [5:13:15<29:34:01, 626.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1841.8565266927083
INFO:root:current train perplexity4.163356304168701
INFO:root:current mean train loss 1797.0249885769065
INFO:root:current train perplexity4.1269145011901855
INFO:root:current mean train loss 1792.7243938537306
INFO:root:current train perplexity4.1182942390441895
INFO:root:current mean train loss 1801.956572202417
INFO:root:current train perplexity4.138062953948975
INFO:root:current mean train loss 1803.0830901875763
INFO:root:current train perplexity4.139471530914307
INFO:root:current mean train loss 1804.4327495702357
INFO:root:current train perplexity4.140076637268066
INFO:root:current mean train loss 1804.9982878085232
INFO:root:current train perplexity4.1506242752075195
INFO:root:current mean train loss 1806.5496075499714
INFO:root:current train perplexity4.150849342346191
INFO:root:current mean train loss 1806.5875719445303
INFO:root:current train perplexity4.156435012817383
INFO:root:current mean train loss 1809.9813184077198
INFO:root:current train perplexity4.1639933586120605
INFO:root:current mean train loss 1810.5993270042277
INFO:root:current train perplexity4.166467666625977
INFO:root:current mean train loss 1810.1622439935682
INFO:root:current train perplexity4.172301292419434
INFO:root:current mean train loss 1810.9276815687358
INFO:root:current train perplexity4.174699306488037
INFO:root:current mean train loss 1811.6058417685256
INFO:root:current train perplexity4.178767681121826
INFO:root:current mean train loss 1812.4921170648233
INFO:root:current train perplexity4.179877758026123
INFO:root:current mean train loss 1812.373463645212
INFO:root:current train perplexity4.1797075271606445
INFO:root:current mean train loss 1813.5602500825435
INFO:root:current train perplexity4.182182788848877
INFO:root:current mean train loss 1814.048039632209
INFO:root:current train perplexity4.180394172668457
INFO:root:current mean train loss 1814.499383777726
INFO:root:current train perplexity4.1814165115356445
INFO:root:current mean train loss 1815.3057760295599
INFO:root:current train perplexity4.183259010314941

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.06s/it]
INFO:root:final mean train loss: 1815.250117730445
INFO:root:final train perplexity: 4.185454845428467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it]
INFO:root:eval mean loss: 1999.0474537518007
INFO:root:eval perplexity: 5.03650426864624
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.36s/it]
INFO:root:eval mean loss: 2453.792498649435
INFO:root:eval perplexity: 7.439316272735596
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/31
 16%|â–ˆâ–Œ        | 31/200 [5:23:48<29:29:09, 628.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1793.9586463341345
INFO:root:current train perplexity4.110347747802734
INFO:root:current mean train loss 1812.7445184616815
INFO:root:current train perplexity4.134827136993408
INFO:root:current mean train loss 1804.8871405947525
INFO:root:current train perplexity4.12620735168457
INFO:root:current mean train loss 1800.8354383597346
INFO:root:current train perplexity4.129580974578857
INFO:root:current mean train loss 1804.639151273199
INFO:root:current train perplexity4.129905700683594
INFO:root:current mean train loss 1804.4360838915459
INFO:root:current train perplexity4.130810737609863
INFO:root:current mean train loss 1801.5483708488293
INFO:root:current train perplexity4.130171775817871
INFO:root:current mean train loss 1800.406653706364
INFO:root:current train perplexity4.130473613739014
INFO:root:current mean train loss 1802.5155768221284
INFO:root:current train perplexity4.132160663604736
INFO:root:current mean train loss 1802.6273706160148
INFO:root:current train perplexity4.139191627502441
INFO:root:current mean train loss 1802.6811869660316
INFO:root:current train perplexity4.140043258666992
INFO:root:current mean train loss 1802.5064601864315
INFO:root:current train perplexity4.144255638122559
INFO:root:current mean train loss 1802.908093201978
INFO:root:current train perplexity4.146389007568359
INFO:root:current mean train loss 1803.599575221089
INFO:root:current train perplexity4.147488117218018
INFO:root:current mean train loss 1803.2032066655527
INFO:root:current train perplexity4.147874355316162
INFO:root:current mean train loss 1804.0675831965975
INFO:root:current train perplexity4.151050090789795
INFO:root:current mean train loss 1805.2193604266365
INFO:root:current train perplexity4.152760982513428
INFO:root:current mean train loss 1805.294011439813
INFO:root:current train perplexity4.150694370269775
INFO:root:current mean train loss 1806.1624734466986
INFO:root:current train perplexity4.155023097991943
INFO:root:current mean train loss 1805.8422058042088
INFO:root:current train perplexity4.154693603515625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.66s/it]
INFO:root:final mean train loss: 1806.229783574199
INFO:root:final train perplexity: 4.15578556060791
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it]
INFO:root:eval mean loss: 1998.877159605635
INFO:root:eval perplexity: 5.035810470581055
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.35s/it]
INFO:root:eval mean loss: 2458.817601846465
INFO:root:eval perplexity: 7.46995210647583
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/32
 16%|â–ˆâ–Œ        | 32/200 [5:34:12<29:15:54, 627.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1779.5911694903707
INFO:root:current train perplexity4.065814971923828
INFO:root:current mean train loss 1794.9550183703016
INFO:root:current train perplexity4.1062188148498535
INFO:root:current mean train loss 1798.6352157278807
INFO:root:current train perplexity4.091928005218506
INFO:root:current mean train loss 1791.1291454081634
INFO:root:current train perplexity4.084494590759277
INFO:root:current mean train loss 1789.2467162259277
INFO:root:current train perplexity4.098015785217285
INFO:root:current mean train loss 1790.3860418555048
INFO:root:current train perplexity4.104252338409424
INFO:root:current mean train loss 1792.3462460816
INFO:root:current train perplexity4.112218856811523
INFO:root:current mean train loss 1793.1296937103066
INFO:root:current train perplexity4.117132186889648
INFO:root:current mean train loss 1794.6627491219047
INFO:root:current train perplexity4.122051239013672
INFO:root:current mean train loss 1794.9545266726868
INFO:root:current train perplexity4.118866443634033
INFO:root:current mean train loss 1794.402746593735
INFO:root:current train perplexity4.119504928588867
INFO:root:current mean train loss 1793.33643806304
INFO:root:current train perplexity4.117392539978027
INFO:root:current mean train loss 1794.3202768511476
INFO:root:current train perplexity4.118465423583984
INFO:root:current mean train loss 1795.302754371626
INFO:root:current train perplexity4.119574546813965
INFO:root:current mean train loss 1796.3598450933655
INFO:root:current train perplexity4.123183727264404
INFO:root:current mean train loss 1796.103649403936
INFO:root:current train perplexity4.123788833618164
INFO:root:current mean train loss 1796.928941481736
INFO:root:current train perplexity4.1252055168151855
INFO:root:current mean train loss 1796.8228860615452
INFO:root:current train perplexity4.123610019683838
INFO:root:current mean train loss 1798.2513957610936
INFO:root:current train perplexity4.126891136169434
INFO:root:current mean train loss 1797.806923277772
INFO:root:current train perplexity4.125411033630371

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.53s/it]
INFO:root:final mean train loss: 1797.4255451735257
INFO:root:final train perplexity: 4.127029895782471
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 1999.8993967475622
INFO:root:eval perplexity: 5.039975643157959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it]
INFO:root:eval mean loss: 2460.9387375851893
INFO:root:eval perplexity: 7.482921600341797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/33
 16%|â–ˆâ–‹        | 33/200 [5:44:39<29:05:24, 627.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1771.1773701985678
INFO:root:current train perplexity4.0564422607421875
INFO:root:current mean train loss 1779.7540672302246
INFO:root:current train perplexity4.092124938964844
INFO:root:current mean train loss 1783.2216050368088
INFO:root:current train perplexity4.092358589172363
INFO:root:current mean train loss 1779.6618459065755
INFO:root:current train perplexity4.086155414581299
INFO:root:current mean train loss 1780.5989533797554
INFO:root:current train perplexity4.089630126953125
INFO:root:current mean train loss 1777.4184413364956
INFO:root:current train perplexity4.078133583068848
INFO:root:current mean train loss 1776.2729044596354
INFO:root:current train perplexity4.078650951385498
INFO:root:current mean train loss 1779.5951379073294
INFO:root:current train perplexity4.077854156494141
INFO:root:current mean train loss 1778.6916360544603
INFO:root:current train perplexity4.0755133628845215
INFO:root:current mean train loss 1780.442722829183
INFO:root:current train perplexity4.079123020172119
INFO:root:current mean train loss 1781.7306444851856
INFO:root:current train perplexity4.0800089836120605
INFO:root:current mean train loss 1782.7853466165477
INFO:root:current train perplexity4.080986022949219
INFO:root:current mean train loss 1783.306600031777
INFO:root:current train perplexity4.083147048950195
INFO:root:current mean train loss 1785.0697165994084
INFO:root:current train perplexity4.085873603820801
INFO:root:current mean train loss 1786.7663651975868
INFO:root:current train perplexity4.0878520011901855
INFO:root:current mean train loss 1787.9121636023888
INFO:root:current train perplexity4.089941501617432
INFO:root:current mean train loss 1788.80029738093
INFO:root:current train perplexity4.0932793617248535
INFO:root:current mean train loss 1788.900564436479
INFO:root:current train perplexity4.094326972961426
INFO:root:current mean train loss 1788.6807957803048
INFO:root:current train perplexity4.096014976501465
INFO:root:current mean train loss 1788.476179286412
INFO:root:current train perplexity4.096705436706543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.10s/it]
INFO:root:final mean train loss: 1788.1751409502747
INFO:root:final train perplexity: 4.097031116485596
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 2000.6037082536845
INFO:root:eval perplexity: 5.042847156524658
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.09s/it]
INFO:root:eval mean loss: 2463.3615376357493
INFO:root:eval perplexity: 7.497763633728027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/34
 17%|â–ˆâ–‹        | 34/200 [5:55:03<28:52:04, 626.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1772.6635123909293
INFO:root:current train perplexity4.022021293640137
INFO:root:current mean train loss 1770.358756372484
INFO:root:current train perplexity4.048590660095215
INFO:root:current mean train loss 1771.3069319191393
INFO:root:current train perplexity4.049955368041992
INFO:root:current mean train loss 1771.4006723257212
INFO:root:current train perplexity4.049863338470459
INFO:root:current mean train loss 1772.5227851787704
INFO:root:current train perplexity4.048649311065674
INFO:root:current mean train loss 1772.7302428035637
INFO:root:current train perplexity4.053810119628906
INFO:root:current mean train loss 1772.041845414628
INFO:root:current train perplexity4.052894115447998
INFO:root:current mean train loss 1772.9753026778174
INFO:root:current train perplexity4.0510406494140625
INFO:root:current mean train loss 1775.3050525974113
INFO:root:current train perplexity4.055995941162109
INFO:root:current mean train loss 1776.8465833556568
INFO:root:current train perplexity4.0587286949157715
INFO:root:current mean train loss 1775.879862977491
INFO:root:current train perplexity4.057570934295654
INFO:root:current mean train loss 1776.0922757183585
INFO:root:current train perplexity4.063268661499023
INFO:root:current mean train loss 1776.601332889279
INFO:root:current train perplexity4.063838005065918
INFO:root:current mean train loss 1776.6413616770492
INFO:root:current train perplexity4.064072132110596
INFO:root:current mean train loss 1777.5234778319652
INFO:root:current train perplexity4.064438343048096
INFO:root:current mean train loss 1778.5636631125653
INFO:root:current train perplexity4.065156936645508
INFO:root:current mean train loss 1778.5871108162596
INFO:root:current train perplexity4.064789295196533
INFO:root:current mean train loss 1778.9674784628676
INFO:root:current train perplexity4.065874099731445
INFO:root:current mean train loss 1779.917614830639
INFO:root:current train perplexity4.067512035369873
INFO:root:current mean train loss 1779.9138621984857
INFO:root:current train perplexity4.068907737731934

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.07s/it]
INFO:root:final mean train loss: 1779.3638372147138
INFO:root:final train perplexity: 4.068658828735352
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2003.4375199121787
INFO:root:eval perplexity: 5.054417610168457
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.84s/it]
INFO:root:eval mean loss: 2469.973793841423
INFO:root:eval perplexity: 7.538418292999268
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/35
 18%|â–ˆâ–Š        | 35/200 [6:05:36<28:47:39, 628.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1777.9449540807846
INFO:root:current train perplexity4.0090179443359375
INFO:root:current mean train loss 1770.1091484777705
INFO:root:current train perplexity4.008452892303467
INFO:root:current mean train loss 1768.2785457688935
INFO:root:current train perplexity4.020230293273926
INFO:root:current mean train loss 1764.6438583141655
INFO:root:current train perplexity4.018267631530762
INFO:root:current mean train loss 1765.970165175465
INFO:root:current train perplexity4.025224685668945
INFO:root:current mean train loss 1765.7863239326862
INFO:root:current train perplexity4.025987148284912
INFO:root:current mean train loss 1769.445278024811
INFO:root:current train perplexity4.0359416007995605
INFO:root:current mean train loss 1772.480748404784
INFO:root:current train perplexity4.037552833557129
INFO:root:current mean train loss 1771.624443992939
INFO:root:current train perplexity4.031807899475098
INFO:root:current mean train loss 1769.6490250094316
INFO:root:current train perplexity4.026848793029785
INFO:root:current mean train loss 1768.6460031233933
INFO:root:current train perplexity4.029242992401123
INFO:root:current mean train loss 1768.2965175813965
INFO:root:current train perplexity4.0306501388549805
INFO:root:current mean train loss 1769.351406091516
INFO:root:current train perplexity4.034085273742676
INFO:root:current mean train loss 1770.055637354146
INFO:root:current train perplexity4.034840106964111
INFO:root:current mean train loss 1771.1328096402538
INFO:root:current train perplexity4.036994457244873
INFO:root:current mean train loss 1771.8674170136303
INFO:root:current train perplexity4.038295269012451
INFO:root:current mean train loss 1772.7444125088778
INFO:root:current train perplexity4.03914213180542
INFO:root:current mean train loss 1773.0244207988217
INFO:root:current train perplexity4.040254592895508
INFO:root:current mean train loss 1772.5196223193514
INFO:root:current train perplexity4.042862892150879

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.52s/it]
INFO:root:final mean train loss: 1771.309682379572
INFO:root:final train perplexity: 4.042896747589111
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.26s/it]
INFO:root:eval mean loss: 2005.566484600094
INFO:root:eval perplexity: 5.0631279945373535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it]
INFO:root:eval mean loss: 2468.810473719387
INFO:root:eval perplexity: 7.531250476837158
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/36
 18%|â–ˆâ–Š        | 36/200 [6:16:12<28:43:04, 630.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1794.0284312855113
INFO:root:current train perplexity4.02128791809082
INFO:root:current mean train loss 1746.9729894689613
INFO:root:current train perplexity3.9656524658203125
INFO:root:current mean train loss 1753.045753225896
INFO:root:current train perplexity3.987354278564453
INFO:root:current mean train loss 1755.0860756631832
INFO:root:current train perplexity3.989673137664795
INFO:root:current mean train loss 1759.5731908051057
INFO:root:current train perplexity3.9937126636505127
INFO:root:current mean train loss 1760.1261012605491
INFO:root:current train perplexity3.996854543685913
INFO:root:current mean train loss 1760.0417398555774
INFO:root:current train perplexity3.9960989952087402
INFO:root:current mean train loss 1758.0454829520481
INFO:root:current train perplexity3.995737075805664
INFO:root:current mean train loss 1758.4353274193704
INFO:root:current train perplexity3.9969961643218994
INFO:root:current mean train loss 1760.8470138734049
INFO:root:current train perplexity4.0003662109375
INFO:root:current mean train loss 1761.7553808738642
INFO:root:current train perplexity4.0046162605285645
INFO:root:current mean train loss 1760.8289696035033
INFO:root:current train perplexity4.00595760345459
INFO:root:current mean train loss 1761.1867563891863
INFO:root:current train perplexity4.0071702003479
INFO:root:current mean train loss 1761.4165060478344
INFO:root:current train perplexity4.007748126983643
INFO:root:current mean train loss 1761.6687567134347
INFO:root:current train perplexity4.0102434158325195
INFO:root:current mean train loss 1762.2862867939796
INFO:root:current train perplexity4.012261867523193
INFO:root:current mean train loss 1761.6230297503007
INFO:root:current train perplexity4.0112690925598145
INFO:root:current mean train loss 1762.3026796538254
INFO:root:current train perplexity4.012137413024902
INFO:root:current mean train loss 1763.3412855412669
INFO:root:current train perplexity4.014252662658691
INFO:root:current mean train loss 1763.2244706964818
INFO:root:current train perplexity4.014989852905273

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.43s/it]
INFO:root:final mean train loss: 1762.696753982336
INFO:root:final train perplexity: 4.015527725219727
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it]
INFO:root:eval mean loss: 2006.7786354443706
INFO:root:eval perplexity: 5.068094253540039
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.04s/it]
INFO:root:eval mean loss: 2473.919188154505
INFO:root:eval perplexity: 7.562782287597656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/37
 18%|â–ˆâ–Š        | 37/200 [6:26:44<28:33:47, 630.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1715.0099400111608
INFO:root:current train perplexity3.9677138328552246
INFO:root:current mean train loss 1760.1003875732422
INFO:root:current train perplexity3.9797704219818115
INFO:root:current mean train loss 1754.6906277840599
INFO:root:current train perplexity3.984112024307251
INFO:root:current mean train loss 1747.802605605707
INFO:root:current train perplexity3.9782028198242188
INFO:root:current mean train loss 1745.8911771685164
INFO:root:current train perplexity3.9761409759521484
INFO:root:current mean train loss 1747.2192183985856
INFO:root:current train perplexity3.970233678817749
INFO:root:current mean train loss 1749.1778199019705
INFO:root:current train perplexity3.973818063735962
INFO:root:current mean train loss 1748.8214761922648
INFO:root:current train perplexity3.974543333053589
INFO:root:current mean train loss 1750.7166899897627
INFO:root:current train perplexity3.9764328002929688
INFO:root:current mean train loss 1752.8220859396047
INFO:root:current train perplexity3.977515697479248
INFO:root:current mean train loss 1754.210108538082
INFO:root:current train perplexity3.977739095687866
INFO:root:current mean train loss 1754.7814826694787
INFO:root:current train perplexity3.98305344581604
INFO:root:current mean train loss 1754.1377974022482
INFO:root:current train perplexity3.986032247543335
INFO:root:current mean train loss 1754.045985669975
INFO:root:current train perplexity3.9833555221557617
INFO:root:current mean train loss 1754.9360856769465
INFO:root:current train perplexity3.985257148742676
INFO:root:current mean train loss 1755.3479658995623
INFO:root:current train perplexity3.9853084087371826
INFO:root:current mean train loss 1754.7292514210544
INFO:root:current train perplexity3.9849889278411865
INFO:root:current mean train loss 1755.6866693849918
INFO:root:current train perplexity3.9857993125915527
INFO:root:current mean train loss 1754.9413136288165
INFO:root:current train perplexity3.9875991344451904
INFO:root:current mean train loss 1754.9937954344691
INFO:root:current train perplexity3.989220380783081

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.36s/it]
INFO:root:final mean train loss: 1754.822952801449
INFO:root:final train perplexity: 3.9906692504882812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 2006.9771499092697
INFO:root:eval perplexity: 5.068907260894775
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it]
INFO:root:eval mean loss: 2474.366815228834
INFO:root:eval perplexity: 7.565552234649658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/38
 19%|â–ˆâ–‰        | 38/200 [6:37:08<28:17:54, 628.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1740.3978461371528
INFO:root:current train perplexity3.946741819381714
INFO:root:current mean train loss 1741.7620723329742
INFO:root:current train perplexity3.950958490371704
INFO:root:current mean train loss 1743.7806939572704
INFO:root:current train perplexity3.948754072189331
INFO:root:current mean train loss 1748.5942977241848
INFO:root:current train perplexity3.955848217010498
INFO:root:current mean train loss 1745.6037839053722
INFO:root:current train perplexity3.958998203277588
INFO:root:current mean train loss 1742.7141675476635
INFO:root:current train perplexity3.9562387466430664
INFO:root:current mean train loss 1743.415455047844
INFO:root:current train perplexity3.9524013996124268
INFO:root:current mean train loss 1743.1014733640939
INFO:root:current train perplexity3.9540693759918213
INFO:root:current mean train loss 1743.171938129854
INFO:root:current train perplexity3.9535715579986572
INFO:root:current mean train loss 1741.1950576378556
INFO:root:current train perplexity3.948568105697632
INFO:root:current mean train loss 1741.0778994327527
INFO:root:current train perplexity3.9498372077941895
INFO:root:current mean train loss 1741.5932504179175
INFO:root:current train perplexity3.951261043548584
INFO:root:current mean train loss 1740.9862611579128
INFO:root:current train perplexity3.952219247817993
INFO:root:current mean train loss 1742.6245797876975
INFO:root:current train perplexity3.954373359680176
INFO:root:current mean train loss 1743.498339252406
INFO:root:current train perplexity3.9561994075775146
INFO:root:current mean train loss 1744.3012184908475
INFO:root:current train perplexity3.958446979522705
INFO:root:current mean train loss 1745.5336657306707
INFO:root:current train perplexity3.9599647521972656
INFO:root:current mean train loss 1746.4634587940993
INFO:root:current train perplexity3.9620494842529297
INFO:root:current mean train loss 1747.0694120246867
INFO:root:current train perplexity3.963909864425659
INFO:root:current mean train loss 1747.3102285507712
INFO:root:current train perplexity3.964808702468872

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.91s/it]
INFO:root:final mean train loss: 1747.13000420567
INFO:root:final train perplexity: 3.9665305614471436
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.56s/it]
INFO:root:eval mean loss: 2007.8116212668995
INFO:root:eval perplexity: 5.072329521179199
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2478.405624497867
INFO:root:eval perplexity: 7.59058141708374
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/39
 20%|â–ˆâ–‰        | 39/200 [6:47:31<28:02:54, 627.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1726.1793783864666
INFO:root:current train perplexity3.8907127380371094
INFO:root:current mean train loss 1733.1944851345486
INFO:root:current train perplexity3.8929898738861084
INFO:root:current mean train loss 1726.2099940176229
INFO:root:current train perplexity3.892362117767334
INFO:root:current mean train loss 1726.005662443888
INFO:root:current train perplexity3.8940963745117188
INFO:root:current mean train loss 1727.5598820938176
INFO:root:current train perplexity3.8947064876556396
INFO:root:current mean train loss 1727.2979738500194
INFO:root:current train perplexity3.9018490314483643
INFO:root:current mean train loss 1730.705074990264
INFO:root:current train perplexity3.911984920501709
INFO:root:current mean train loss 1734.3930213908197
INFO:root:current train perplexity3.9172286987304688
INFO:root:current mean train loss 1735.3254846276375
INFO:root:current train perplexity3.917959213256836
INFO:root:current mean train loss 1737.1456768329326
INFO:root:current train perplexity3.9211552143096924
INFO:root:current mean train loss 1740.0692614539196
INFO:root:current train perplexity3.9302635192871094
INFO:root:current mean train loss 1739.268342482654
INFO:root:current train perplexity3.9287214279174805
INFO:root:current mean train loss 1738.3129468818097
INFO:root:current train perplexity3.927307367324829
INFO:root:current mean train loss 1739.1379522696116
INFO:root:current train perplexity3.929037094116211
INFO:root:current mean train loss 1740.1323892616867
INFO:root:current train perplexity3.9333484172821045
INFO:root:current mean train loss 1740.8631557410872
INFO:root:current train perplexity3.9358155727386475
INFO:root:current mean train loss 1739.963239502247
INFO:root:current train perplexity3.9354782104492188
INFO:root:current mean train loss 1740.1417223165038
INFO:root:current train perplexity3.9368255138397217
INFO:root:current mean train loss 1739.7093480947067
INFO:root:current train perplexity3.9382810592651367
INFO:root:current mean train loss 1739.1833634216123
INFO:root:current train perplexity3.9391016960144043

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.42s/it]
INFO:root:final mean train loss: 1738.4595430482834
INFO:root:final train perplexity: 3.939500093460083
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.04s/it]
INFO:root:eval mean loss: 2010.1350430968805
INFO:root:eval perplexity: 5.0818705558776855
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it]
INFO:root:eval mean loss: 2480.8132609915224
INFO:root:eval perplexity: 7.6055426597595215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/40
 20%|â–ˆâ–ˆ        | 40/200 [6:57:50<27:45:52, 624.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1720.3752827704707
INFO:root:current train perplexity3.895639657974243
INFO:root:current mean train loss 1716.691651072582
INFO:root:current train perplexity3.8943629264831543
INFO:root:current mean train loss 1718.1792482218862
INFO:root:current train perplexity3.891066789627075
INFO:root:current mean train loss 1721.1231444668329
INFO:root:current train perplexity3.896066665649414
INFO:root:current mean train loss 1720.2763488387266
INFO:root:current train perplexity3.895392656326294
INFO:root:current mean train loss 1720.60398493159
INFO:root:current train perplexity3.8944714069366455
INFO:root:current mean train loss 1720.1973842795012
INFO:root:current train perplexity3.8934733867645264
INFO:root:current mean train loss 1723.0246663515925
INFO:root:current train perplexity3.896286725997925
INFO:root:current mean train loss 1727.2447905556742
INFO:root:current train perplexity3.903214454650879
INFO:root:current mean train loss 1727.4088626039406
INFO:root:current train perplexity3.9061548709869385
INFO:root:current mean train loss 1728.2056992241803
INFO:root:current train perplexity3.909804344177246
INFO:root:current mean train loss 1730.411874242108
INFO:root:current train perplexity3.9106578826904297
INFO:root:current mean train loss 1729.4763043294017
INFO:root:current train perplexity3.9076266288757324
INFO:root:current mean train loss 1730.2517513858832
INFO:root:current train perplexity3.9100942611694336
INFO:root:current mean train loss 1730.3682856375983
INFO:root:current train perplexity3.910334348678589
INFO:root:current mean train loss 1731.0754815863233
INFO:root:current train perplexity3.9125137329101562
INFO:root:current mean train loss 1730.4000518962412
INFO:root:current train perplexity3.9122400283813477
INFO:root:current mean train loss 1731.0814560991516
INFO:root:current train perplexity3.913759708404541
INFO:root:current mean train loss 1731.285316455104
INFO:root:current train perplexity3.915724992752075
INFO:root:current mean train loss 1731.274465074197
INFO:root:current train perplexity3.9159696102142334

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.19s/it]
INFO:root:final mean train loss: 1730.8968059868748
INFO:root:final train perplexity: 3.9160730838775635
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it]
INFO:root:eval mean loss: 2011.076004785849
INFO:root:eval perplexity: 5.0857391357421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.74s/it]
INFO:root:eval mean loss: 2483.156798017786
INFO:root:eval perplexity: 7.620131969451904
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/41
 20%|â–ˆâ–ˆ        | 41/200 [7:08:13<27:33:55, 624.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1711.6160519917805
INFO:root:current train perplexity3.8455734252929688
INFO:root:current mean train loss 1721.2243347167969
INFO:root:current train perplexity3.874993324279785
INFO:root:current mean train loss 1730.0607390532623
INFO:root:current train perplexity3.8824992179870605
INFO:root:current mean train loss 1727.385512535018
INFO:root:current train perplexity3.8766260147094727
INFO:root:current mean train loss 1722.0771951983052
INFO:root:current train perplexity3.8779711723327637
INFO:root:current mean train loss 1723.9357357153156
INFO:root:current train perplexity3.8778579235076904
INFO:root:current mean train loss 1725.2804661893297
INFO:root:current train perplexity3.8803021907806396
INFO:root:current mean train loss 1723.7457838202242
INFO:root:current train perplexity3.8752994537353516
INFO:root:current mean train loss 1723.2306680679321
INFO:root:current train perplexity3.878127336502075
INFO:root:current mean train loss 1723.0871218026402
INFO:root:current train perplexity3.880117177963257
INFO:root:current mean train loss 1723.6128090071852
INFO:root:current train perplexity3.8829517364501953
INFO:root:current mean train loss 1724.0288740177218
INFO:root:current train perplexity3.884155035018921
INFO:root:current mean train loss 1724.7884669362763
INFO:root:current train perplexity3.885465145111084
INFO:root:current mean train loss 1725.5685271779582
INFO:root:current train perplexity3.888152599334717
INFO:root:current mean train loss 1724.9389299198906
INFO:root:current train perplexity3.889721393585205
INFO:root:current mean train loss 1724.5981523327362
INFO:root:current train perplexity3.8890786170959473
INFO:root:current mean train loss 1723.7893403251217
INFO:root:current train perplexity3.889404296875
INFO:root:current mean train loss 1723.1292182905372
INFO:root:current train perplexity3.890686511993408
INFO:root:current mean train loss 1723.3872912443137
INFO:root:current train perplexity3.890580177307129

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.65s/it]
INFO:root:final mean train loss: 1723.364313769088
INFO:root:final train perplexity: 3.892878532409668
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.94s/it]
INFO:root:eval mean loss: 2014.190603269753
INFO:root:eval perplexity: 5.098564624786377
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2489.173824229139
INFO:root:eval perplexity: 7.657722473144531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/42
 21%|â–ˆâ–ˆ        | 42/200 [7:18:44<27:28:55, 626.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1693.078387920673
INFO:root:current train perplexity3.849689245223999
INFO:root:current mean train loss 1711.0323734789822
INFO:root:current train perplexity3.816122055053711
INFO:root:current mean train loss 1699.1668139533817
INFO:root:current train perplexity3.8287837505340576
INFO:root:current mean train loss 1699.5127772127096
INFO:root:current train perplexity3.828752279281616
INFO:root:current mean train loss 1703.3877813233014
INFO:root:current train perplexity3.8320372104644775
INFO:root:current mean train loss 1704.4515409413834
INFO:root:current train perplexity3.8391664028167725
INFO:root:current mean train loss 1707.4836451668918
INFO:root:current train perplexity3.8465564250946045
INFO:root:current mean train loss 1708.9383623676915
INFO:root:current train perplexity3.84958553314209
INFO:root:current mean train loss 1707.9357524275927
INFO:root:current train perplexity3.847029685974121
INFO:root:current mean train loss 1707.4435385362472
INFO:root:current train perplexity3.847996234893799
INFO:root:current mean train loss 1708.1713487600646
INFO:root:current train perplexity3.848266839981079
INFO:root:current mean train loss 1709.9081506994748
INFO:root:current train perplexity3.8516156673431396
INFO:root:current mean train loss 1711.4455810949414
INFO:root:current train perplexity3.852524995803833
INFO:root:current mean train loss 1710.6332367245514
INFO:root:current train perplexity3.85211443901062
INFO:root:current mean train loss 1711.0249489948249
INFO:root:current train perplexity3.85465931892395
INFO:root:current mean train loss 1710.9978047513994
INFO:root:current train perplexity3.8540122509002686
INFO:root:current mean train loss 1712.6920060064951
INFO:root:current train perplexity3.8590340614318848
INFO:root:current mean train loss 1714.1944586491627
INFO:root:current train perplexity3.862757444381714
INFO:root:current mean train loss 1715.3113378744656
INFO:root:current train perplexity3.8664140701293945
INFO:root:current mean train loss 1716.9559569291525
INFO:root:current train perplexity3.870591163635254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.74s/it]
INFO:root:final mean train loss: 1716.070500253128
INFO:root:final train perplexity: 3.870549201965332
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.28s/it]
INFO:root:eval mean loss: 2016.3295604083555
INFO:root:eval perplexity: 5.10739278793335
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.72s/it]
INFO:root:eval mean loss: 2489.6822232726618
INFO:root:eval perplexity: 7.66090726852417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/43
 22%|â–ˆâ–ˆâ–       | 43/200 [7:29:12<27:19:58, 626.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1703.7923461914063
INFO:root:current train perplexity3.796616792678833
INFO:root:current mean train loss 1694.2645216721755
INFO:root:current train perplexity3.800997257232666
INFO:root:current mean train loss 1697.672759213655
INFO:root:current train perplexity3.8021647930145264
INFO:root:current mean train loss 1701.4567985765862
INFO:root:current train perplexity3.816340208053589
INFO:root:current mean train loss 1700.5521115325218
INFO:root:current train perplexity3.81270432472229
INFO:root:current mean train loss 1702.45009765625
INFO:root:current train perplexity3.8180339336395264
INFO:root:current mean train loss 1701.487340533544
INFO:root:current train perplexity3.8166911602020264
INFO:root:current mean train loss 1702.6253976482235
INFO:root:current train perplexity3.824392080307007
INFO:root:current mean train loss 1704.2765583819653
INFO:root:current train perplexity3.8296587467193604
INFO:root:current mean train loss 1703.4085083007812
INFO:root:current train perplexity3.8290767669677734
INFO:root:current mean train loss 1701.8382529249468
INFO:root:current train perplexity3.8299083709716797
INFO:root:current mean train loss 1701.5035833578195
INFO:root:current train perplexity3.828341007232666
INFO:root:current mean train loss 1701.8136570876206
INFO:root:current train perplexity3.828110456466675
INFO:root:current mean train loss 1702.4567971136337
INFO:root:current train perplexity3.833197593688965
INFO:root:current mean train loss 1703.7713240616806
INFO:root:current train perplexity3.8351616859436035
INFO:root:current mean train loss 1705.750918160233
INFO:root:current train perplexity3.8366971015930176
INFO:root:current mean train loss 1706.2349905938459
INFO:root:current train perplexity3.8392858505249023
INFO:root:current mean train loss 1708.0360948507496
INFO:root:current train perplexity3.8414623737335205
INFO:root:current mean train loss 1708.8619594219604
INFO:root:current train perplexity3.843412160873413
INFO:root:current mean train loss 1708.5590354800843
INFO:root:current train perplexity3.845019817352295

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.63s/it]
INFO:root:final mean train loss: 1708.1166391993074
INFO:root:final train perplexity: 3.8463456630706787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.34s/it]
INFO:root:eval mean loss: 2017.8975215397827
INFO:root:eval perplexity: 5.113872528076172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 2495.085045780696
INFO:root:eval perplexity: 7.6948347091674805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/44
 22%|â–ˆâ–ˆâ–       | 44/200 [7:39:38<27:08:51, 626.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1668.7345749875333
INFO:root:current train perplexity3.7573800086975098
INFO:root:current mean train loss 1690.0759169390412
INFO:root:current train perplexity3.787001132965088
INFO:root:current mean train loss 1692.033127016384
INFO:root:current train perplexity3.7971925735473633
INFO:root:current mean train loss 1690.1535718406656
INFO:root:current train perplexity3.7978761196136475
INFO:root:current mean train loss 1691.5501799103397
INFO:root:current train perplexity3.8009660243988037
INFO:root:current mean train loss 1689.4648707527565
INFO:root:current train perplexity3.8025240898132324
INFO:root:current mean train loss 1692.4596541580129
INFO:root:current train perplexity3.8040692806243896
INFO:root:current mean train loss 1693.6049169006756
INFO:root:current train perplexity3.807845115661621
INFO:root:current mean train loss 1694.7423035172392
INFO:root:current train perplexity3.810492515563965
INFO:root:current mean train loss 1695.0767122327086
INFO:root:current train perplexity3.813638925552368
INFO:root:current mean train loss 1695.9599411171055
INFO:root:current train perplexity3.815359115600586
INFO:root:current mean train loss 1697.3970730157137
INFO:root:current train perplexity3.816967248916626
INFO:root:current mean train loss 1697.078301693596
INFO:root:current train perplexity3.8157544136047363
INFO:root:current mean train loss 1698.1596519283303
INFO:root:current train perplexity3.8173913955688477
INFO:root:current mean train loss 1699.1187334146348
INFO:root:current train perplexity3.8176372051239014
INFO:root:current mean train loss 1700.4406436853587
INFO:root:current train perplexity3.821319818496704
INFO:root:current mean train loss 1700.8816107707669
INFO:root:current train perplexity3.8231492042541504
INFO:root:current mean train loss 1701.4709982738266
INFO:root:current train perplexity3.822683811187744
INFO:root:current mean train loss 1701.3365033722337
INFO:root:current train perplexity3.8234431743621826
INFO:root:current mean train loss 1702.0205461201328
INFO:root:current train perplexity3.825824022293091

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.61s/it]
INFO:root:final mean train loss: 1701.335912137938
INFO:root:final train perplexity: 3.825831174850464
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.94s/it]
INFO:root:eval mean loss: 2020.4269850710605
INFO:root:eval perplexity: 5.124344348907471
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.04s/it]
INFO:root:eval mean loss: 2499.473050597712
INFO:root:eval perplexity: 7.722496509552002
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [7:50:11<27:03:22, 628.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1706.8807315826416
INFO:root:current train perplexity3.809572696685791
INFO:root:current mean train loss 1691.962118753573
INFO:root:current train perplexity3.767508029937744
INFO:root:current mean train loss 1685.3909121426668
INFO:root:current train perplexity3.7662317752838135
INFO:root:current mean train loss 1681.026612669557
INFO:root:current train perplexity3.7557287216186523
INFO:root:current mean train loss 1678.9635170245992
INFO:root:current train perplexity3.760676622390747
INFO:root:current mean train loss 1683.0074683656085
INFO:root:current train perplexity3.7704224586486816
INFO:root:current mean train loss 1685.897666839232
INFO:root:current train perplexity3.7720320224761963
INFO:root:current mean train loss 1685.325589804125
INFO:root:current train perplexity3.779111385345459
INFO:root:current mean train loss 1688.5553935015644
INFO:root:current train perplexity3.7835030555725098
INFO:root:current mean train loss 1689.9691826911387
INFO:root:current train perplexity3.7847015857696533
INFO:root:current mean train loss 1690.1034293497416
INFO:root:current train perplexity3.787912607192993
INFO:root:current mean train loss 1690.9472516771034
INFO:root:current train perplexity3.7896809577941895
INFO:root:current mean train loss 1690.2251873740668
INFO:root:current train perplexity3.789447784423828
INFO:root:current mean train loss 1690.1238849897188
INFO:root:current train perplexity3.792234182357788
INFO:root:current mean train loss 1690.2313164882974
INFO:root:current train perplexity3.794362783432007
INFO:root:current mean train loss 1690.298970332231
INFO:root:current train perplexity3.7958855628967285
INFO:root:current mean train loss 1691.8433180588943
INFO:root:current train perplexity3.7991604804992676
INFO:root:current mean train loss 1692.642408236895
INFO:root:current train perplexity3.7996535301208496
INFO:root:current mean train loss 1693.5247727422754
INFO:root:current train perplexity3.8011391162872314
INFO:root:current mean train loss 1694.165916054409
INFO:root:current train perplexity3.8029847145080566

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.71s/it]
INFO:root:final mean train loss: 1693.8847071752966
INFO:root:final train perplexity: 3.803415536880493
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2021.7098626752272
INFO:root:eval perplexity: 5.129664421081543
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.09s/it]
INFO:root:eval mean loss: 2501.3906843036625
INFO:root:eval perplexity: 7.734617710113525
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [8:00:38<26:51:58, 628.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1668.7843906732253
INFO:root:current train perplexity3.733977794647217
INFO:root:current mean train loss 1674.0385243115504
INFO:root:current train perplexity3.7445459365844727
INFO:root:current mean train loss 1668.8628181647575
INFO:root:current train perplexity3.7423436641693115
INFO:root:current mean train loss 1672.6623804287647
INFO:root:current train perplexity3.7439043521881104
INFO:root:current mean train loss 1672.1408397016307
INFO:root:current train perplexity3.755218505859375
INFO:root:current mean train loss 1674.7280714655496
INFO:root:current train perplexity3.7542741298675537
INFO:root:current mean train loss 1675.7698910078814
INFO:root:current train perplexity3.754960060119629
INFO:root:current mean train loss 1676.715518340869
INFO:root:current train perplexity3.7583425045013428
INFO:root:current mean train loss 1677.2519858248795
INFO:root:current train perplexity3.7644236087799072
INFO:root:current mean train loss 1678.827168969204
INFO:root:current train perplexity3.766995906829834
INFO:root:current mean train loss 1679.9720481569077
INFO:root:current train perplexity3.769961357116699
INFO:root:current mean train loss 1681.5371060674217
INFO:root:current train perplexity3.772869825363159
INFO:root:current mean train loss 1682.6936741277261
INFO:root:current train perplexity3.773817539215088
INFO:root:current mean train loss 1683.8551019203137
INFO:root:current train perplexity3.7742421627044678
INFO:root:current mean train loss 1684.6529166809535
INFO:root:current train perplexity3.7734732627868652
INFO:root:current mean train loss 1685.8046606306334
INFO:root:current train perplexity3.775092124938965
INFO:root:current mean train loss 1687.1004471651222
INFO:root:current train perplexity3.777700185775757
INFO:root:current mean train loss 1687.303174678025
INFO:root:current train perplexity3.7780725955963135
INFO:root:current mean train loss 1686.6542041379046
INFO:root:current train perplexity3.780050277709961
INFO:root:current mean train loss 1687.4060319248683
INFO:root:current train perplexity3.782815456390381

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.29s/it]
INFO:root:final mean train loss: 1687.0252058882336
INFO:root:final train perplexity: 3.7828946113586426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.92s/it]
INFO:root:eval mean loss: 2024.3569755824744
INFO:root:eval perplexity: 5.1406569480896
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 2505.631832907386
INFO:root:eval perplexity: 7.7614898681640625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [8:11:12<26:46:30, 630.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1680.23757747728
INFO:root:current train perplexity3.738790512084961
INFO:root:current mean train loss 1668.50367998836
INFO:root:current train perplexity3.7227277755737305
INFO:root:current mean train loss 1664.1201466809982
INFO:root:current train perplexity3.713832139968872
INFO:root:current mean train loss 1664.9621434810772
INFO:root:current train perplexity3.727339506149292
INFO:root:current mean train loss 1665.913128588573
INFO:root:current train perplexity3.732725143432617
INFO:root:current mean train loss 1668.6212721604568
INFO:root:current train perplexity3.738025426864624
INFO:root:current mean train loss 1671.0065197439112
INFO:root:current train perplexity3.7422473430633545
INFO:root:current mean train loss 1671.926493326823
INFO:root:current train perplexity3.744152545928955
INFO:root:current mean train loss 1674.9775094285044
INFO:root:current train perplexity3.749854803085327
INFO:root:current mean train loss 1675.436930990888
INFO:root:current train perplexity3.7465882301330566
INFO:root:current mean train loss 1676.2136159316656
INFO:root:current train perplexity3.7479214668273926
INFO:root:current mean train loss 1678.7113679048414
INFO:root:current train perplexity3.751277208328247
INFO:root:current mean train loss 1679.0706736325114
INFO:root:current train perplexity3.750549554824829
INFO:root:current mean train loss 1678.9762875361846
INFO:root:current train perplexity3.7529540061950684
INFO:root:current mean train loss 1679.4824156003579
INFO:root:current train perplexity3.756242036819458
INFO:root:current mean train loss 1680.1572860698675
INFO:root:current train perplexity3.7584855556488037
INFO:root:current mean train loss 1680.1707897388471
INFO:root:current train perplexity3.7608981132507324
INFO:root:current mean train loss 1681.0129393852328
INFO:root:current train perplexity3.76100754737854
INFO:root:current mean train loss 1680.7292222564665
INFO:root:current train perplexity3.762235403060913

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.76s/it]
INFO:root:final mean train loss: 1680.3880224954103
INFO:root:final train perplexity: 3.7631454467773438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 2024.549974373892
INFO:root:eval perplexity: 5.141460418701172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.77s/it]
INFO:root:eval mean loss: 2508.3268475904533
INFO:root:eval perplexity: 7.778616428375244
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/48
 24%|â–ˆâ–ˆâ–       | 48/200 [8:21:36<26:31:04, 628.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1647.7694010416667
INFO:root:current train perplexity3.676520824432373
INFO:root:current mean train loss 1668.5002282184103
INFO:root:current train perplexity3.711034059524536
INFO:root:current mean train loss 1658.60126442133
INFO:root:current train perplexity3.7073726654052734
INFO:root:current mean train loss 1665.0455698164683
INFO:root:current train perplexity3.7185165882110596
INFO:root:current mean train loss 1665.1318765295557
INFO:root:current train perplexity3.7155513763427734
INFO:root:current mean train loss 1664.4761680825243
INFO:root:current train perplexity3.7152771949768066
INFO:root:current mean train loss 1664.806513790968
INFO:root:current train perplexity3.7170257568359375
INFO:root:current mean train loss 1665.703975053267
INFO:root:current train perplexity3.718770980834961
INFO:root:current mean train loss 1664.1678300541603
INFO:root:current train perplexity3.7192258834838867
INFO:root:current mean train loss 1664.3603270150272
INFO:root:current train perplexity3.720531702041626
INFO:root:current mean train loss 1664.9626419142548
INFO:root:current train perplexity3.724473714828491
INFO:root:current mean train loss 1666.4755790402537
INFO:root:current train perplexity3.726520538330078
INFO:root:current mean train loss 1667.3438937717015
INFO:root:current train perplexity3.726980447769165
INFO:root:current mean train loss 1668.0431611847969
INFO:root:current train perplexity3.729492425918579
INFO:root:current mean train loss 1670.5436532927065
INFO:root:current train perplexity3.734365701675415
INFO:root:current mean train loss 1670.9932156301568
INFO:root:current train perplexity3.7366976737976074
INFO:root:current mean train loss 1671.079937536281
INFO:root:current train perplexity3.7366669178009033
INFO:root:current mean train loss 1671.7315307261297
INFO:root:current train perplexity3.7384331226348877
INFO:root:current mean train loss 1672.2447375925447
INFO:root:current train perplexity3.7406184673309326
INFO:root:current mean train loss 1673.1830912537737
INFO:root:current train perplexity3.7407896518707275

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.75s/it]
INFO:root:final mean train loss: 1673.4324346976152
INFO:root:final train perplexity: 3.742558240890503
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.13s/it]
INFO:root:eval mean loss: 2028.1261605337156
INFO:root:eval perplexity: 5.156352519989014
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 2513.5424120747452
INFO:root:eval perplexity: 7.811869144439697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/49
 24%|â–ˆâ–ˆâ–       | 49/200 [8:32:02<26:19:00, 627.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1652.425880432129
INFO:root:current train perplexity3.7190515995025635
INFO:root:current mean train loss 1657.3813208377724
INFO:root:current train perplexity3.6885836124420166
INFO:root:current mean train loss 1661.0144642797009
INFO:root:current train perplexity3.6881043910980225
INFO:root:current mean train loss 1658.3312304393355
INFO:root:current train perplexity3.6841983795166016
INFO:root:current mean train loss 1661.570196081091
INFO:root:current train perplexity3.692342519760132
INFO:root:current mean train loss 1662.1779695668615
INFO:root:current train perplexity3.6880383491516113
INFO:root:current mean train loss 1665.2294902560077
INFO:root:current train perplexity3.6981747150421143
INFO:root:current mean train loss 1665.7850078311774
INFO:root:current train perplexity3.7019293308258057
INFO:root:current mean train loss 1665.9420821850117
INFO:root:current train perplexity3.704246759414673
INFO:root:current mean train loss 1666.8642919974266
INFO:root:current train perplexity3.7079403400421143
INFO:root:current mean train loss 1666.1533863156342
INFO:root:current train perplexity3.709653854370117
INFO:root:current mean train loss 1665.7264798976507
INFO:root:current train perplexity3.710691213607788
INFO:root:current mean train loss 1665.594268600662
INFO:root:current train perplexity3.7105863094329834
INFO:root:current mean train loss 1665.4990337016704
INFO:root:current train perplexity3.7125790119171143
INFO:root:current mean train loss 1665.7680581375207
INFO:root:current train perplexity3.714376449584961
INFO:root:current mean train loss 1665.7618394657463
INFO:root:current train perplexity3.7149910926818848
INFO:root:current mean train loss 1665.1202690274108
INFO:root:current train perplexity3.7177422046661377
INFO:root:current mean train loss 1665.6236857002382
INFO:root:current train perplexity3.7183640003204346
INFO:root:current mean train loss 1666.5617643131438
INFO:root:current train perplexity3.72041654586792
INFO:root:current mean train loss 1667.051057803705
INFO:root:current train perplexity3.721909999847412

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.05s/it]
INFO:root:final mean train loss: 1666.6698107928623
INFO:root:final train perplexity: 3.7226510047912598
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it]
INFO:root:eval mean loss: 2028.9946237117686
INFO:root:eval perplexity: 5.159975051879883
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.85s/it]
INFO:root:eval mean loss: 2515.3347812950187
INFO:root:eval perplexity: 7.823328018188477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [8:42:25<26:05:05, 626.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1635.009145308514
INFO:root:current train perplexity3.6567554473876953
INFO:root:current mean train loss 1650.0758228686032
INFO:root:current train perplexity3.662855863571167
INFO:root:current mean train loss 1647.5928445226218
INFO:root:current train perplexity3.6605024337768555
INFO:root:current mean train loss 1648.302750114725
INFO:root:current train perplexity3.6713340282440186
INFO:root:current mean train loss 1650.3023026430262
INFO:root:current train perplexity3.67684006690979
INFO:root:current mean train loss 1649.1749625562102
INFO:root:current train perplexity3.6766936779022217
INFO:root:current mean train loss 1651.4173136330533
INFO:root:current train perplexity3.677886962890625
INFO:root:current mean train loss 1652.1887022866426
INFO:root:current train perplexity3.677708387374878
INFO:root:current mean train loss 1652.3690737667016
INFO:root:current train perplexity3.676875114440918
INFO:root:current mean train loss 1652.418841893605
INFO:root:current train perplexity3.6789450645446777
INFO:root:current mean train loss 1653.2749707682913
INFO:root:current train perplexity3.684373617172241
INFO:root:current mean train loss 1653.1653396168203
INFO:root:current train perplexity3.6846656799316406
INFO:root:current mean train loss 1655.5615346769603
INFO:root:current train perplexity3.689972400665283
INFO:root:current mean train loss 1655.7949174410153
INFO:root:current train perplexity3.6926052570343018
INFO:root:current mean train loss 1655.8124210628828
INFO:root:current train perplexity3.6931800842285156
INFO:root:current mean train loss 1656.5189882774673
INFO:root:current train perplexity3.695866346359253
INFO:root:current mean train loss 1657.272313549997
INFO:root:current train perplexity3.697765588760376
INFO:root:current mean train loss 1658.256920458454
INFO:root:current train perplexity3.6997599601745605
INFO:root:current mean train loss 1658.832210295261
INFO:root:current train perplexity3.6999504566192627
INFO:root:current mean train loss 1660.153279726783
INFO:root:current train perplexity3.7028653621673584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.48s/it]
INFO:root:final mean train loss: 1659.8895971901297
INFO:root:final train perplexity: 3.7027978897094727
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.27s/it]
INFO:root:eval mean loss: 2033.859497503186
INFO:root:eval perplexity: 5.18031644821167
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.27s/it]
INFO:root:eval mean loss: 2522.1108718763853
INFO:root:eval perplexity: 7.866802215576172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [8:52:49<25:53:20, 625.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1624.6542524857955
INFO:root:current train perplexity3.621267080307007
INFO:root:current mean train loss 1637.989846838526
INFO:root:current train perplexity3.629058361053467
INFO:root:current mean train loss 1638.8432929246947
INFO:root:current train perplexity3.640141725540161
INFO:root:current mean train loss 1635.4970969945355
INFO:root:current train perplexity3.636474847793579
INFO:root:current mean train loss 1642.6603359500737
INFO:root:current train perplexity3.643855333328247
INFO:root:current mean train loss 1644.0988381321777
INFO:root:current train perplexity3.6494319438934326
INFO:root:current mean train loss 1644.0914808851821
INFO:root:current train perplexity3.653585910797119
INFO:root:current mean train loss 1645.8074761532616
INFO:root:current train perplexity3.6558196544647217
INFO:root:current mean train loss 1646.128045978502
INFO:root:current train perplexity3.6567132472991943
INFO:root:current mean train loss 1647.942703278662
INFO:root:current train perplexity3.662163019180298
INFO:root:current mean train loss 1650.8866377896709
INFO:root:current train perplexity3.6657984256744385
INFO:root:current mean train loss 1651.431768243963
INFO:root:current train perplexity3.6690590381622314
INFO:root:current mean train loss 1652.2131493253542
INFO:root:current train perplexity3.672752857208252
INFO:root:current mean train loss 1652.8128404742977
INFO:root:current train perplexity3.674341917037964
INFO:root:current mean train loss 1653.8246163528256
INFO:root:current train perplexity3.6774051189422607
INFO:root:current mean train loss 1654.0280290118883
INFO:root:current train perplexity3.677799940109253
INFO:root:current mean train loss 1652.6161134131387
INFO:root:current train perplexity3.6775572299957275
INFO:root:current mean train loss 1653.8001745619292
INFO:root:current train perplexity3.6795287132263184
INFO:root:current mean train loss 1653.6689836475498
INFO:root:current train perplexity3.680021286010742
INFO:root:current mean train loss 1653.430916833732
INFO:root:current train perplexity3.6827378273010254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.03s/it]
INFO:root:final mean train loss: 1653.0334457882234
INFO:root:final train perplexity: 3.682831048965454
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it]
INFO:root:eval mean loss: 2037.6213218743073
INFO:root:eval perplexity: 5.196101188659668
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.52s/it]
INFO:root:eval mean loss: 2525.0787916251106
INFO:root:eval perplexity: 7.885918617248535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [9:03:12<25:41:18, 624.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1607.7780732304218
INFO:root:current train perplexity3.600104331970215
INFO:root:current mean train loss 1621.8494299383112
INFO:root:current train perplexity3.6244258880615234
INFO:root:current mean train loss 1625.7028398817083
INFO:root:current train perplexity3.6277031898498535
INFO:root:current mean train loss 1629.8954676535982
INFO:root:current train perplexity3.6267731189727783
INFO:root:current mean train loss 1630.6637953606205
INFO:root:current train perplexity3.6291215419769287
INFO:root:current mean train loss 1633.7629482472128
INFO:root:current train perplexity3.632371664047241
INFO:root:current mean train loss 1634.5675901354318
INFO:root:current train perplexity3.6352245807647705
INFO:root:current mean train loss 1637.1629464062999
INFO:root:current train perplexity3.6357645988464355
INFO:root:current mean train loss 1638.791791041062
INFO:root:current train perplexity3.6399128437042236
INFO:root:current mean train loss 1640.4115496894472
INFO:root:current train perplexity3.64471697807312
INFO:root:current mean train loss 1641.3498154179651
INFO:root:current train perplexity3.6455817222595215
INFO:root:current mean train loss 1641.9453370585245
INFO:root:current train perplexity3.647430419921875
INFO:root:current mean train loss 1643.0161292655155
INFO:root:current train perplexity3.6486520767211914
INFO:root:current mean train loss 1643.1754025054513
INFO:root:current train perplexity3.6514463424682617
INFO:root:current mean train loss 1644.3487775189124
INFO:root:current train perplexity3.6544954776763916
INFO:root:current mean train loss 1644.9911051380882
INFO:root:current train perplexity3.655972957611084
INFO:root:current mean train loss 1645.26570384161
INFO:root:current train perplexity3.6579344272613525
INFO:root:current mean train loss 1646.4921911285624
INFO:root:current train perplexity3.660877227783203
INFO:root:current mean train loss 1646.831344401906
INFO:root:current train perplexity3.6602423191070557
INFO:root:current mean train loss 1646.20976555113
INFO:root:current train perplexity3.6630637645721436

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.25s/it]
INFO:root:final mean train loss: 1646.20976555113
INFO:root:final train perplexity: 3.6630637645721436
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.30s/it]
INFO:root:eval mean loss: 2034.883002531444
INFO:root:eval perplexity: 5.184606075286865
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.65s/it]
INFO:root:eval mean loss: 2525.738044901097
INFO:root:eval perplexity: 7.890172958374023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [9:13:33<25:27:39, 623.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.2910205078124
INFO:root:current train perplexity3.583801746368408
INFO:root:current mean train loss 1610.0797546386718
INFO:root:current train perplexity3.583742380142212
INFO:root:current mean train loss 1618.8760579427083
INFO:root:current train perplexity3.5951366424560547
INFO:root:current mean train loss 1619.449482421875
INFO:root:current train perplexity3.604264974594116
INFO:root:current mean train loss 1624.0486520996094
INFO:root:current train perplexity3.6114559173583984
INFO:root:current mean train loss 1626.8340364583332
INFO:root:current train perplexity3.6105964183807373
INFO:root:current mean train loss 1628.2949829101562
INFO:root:current train perplexity3.6139559745788574
INFO:root:current mean train loss 1628.8077082824707
INFO:root:current train perplexity3.615931987762451
INFO:root:current mean train loss 1630.4596091037326
INFO:root:current train perplexity3.6196231842041016
INFO:root:current mean train loss 1631.8609294433593
INFO:root:current train perplexity3.621920347213745
INFO:root:current mean train loss 1631.817401677912
INFO:root:current train perplexity3.6234819889068604
INFO:root:current mean train loss 1634.1216197713215
INFO:root:current train perplexity3.6288342475891113
INFO:root:current mean train loss 1634.4303713754507
INFO:root:current train perplexity3.6296441555023193
INFO:root:current mean train loss 1635.9018860735212
INFO:root:current train perplexity3.63303804397583
INFO:root:current mean train loss 1636.4558354492187
INFO:root:current train perplexity3.635178565979004
INFO:root:current mean train loss 1637.8949398040772
INFO:root:current train perplexity3.6391689777374268
INFO:root:current mean train loss 1639.2552970616957
INFO:root:current train perplexity3.640578269958496
INFO:root:current mean train loss 1639.7061458333333
INFO:root:current train perplexity3.6413142681121826
INFO:root:current mean train loss 1640.3820737176193
INFO:root:current train perplexity3.643261671066284

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.43s/it]
INFO:root:final mean train loss: 1639.7672270266503
INFO:root:final train perplexity: 3.6444997787475586
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.73s/it]
INFO:root:eval mean loss: 2039.708074475011
INFO:root:eval perplexity: 5.204877853393555
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.23s/it]
INFO:root:eval mean loss: 2530.425859600094
INFO:root:eval perplexity: 7.920479774475098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [9:24:07<25:25:21, 626.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1594.8905819163604
INFO:root:current train perplexity3.574033260345459
INFO:root:current mean train loss 1624.3504356971155
INFO:root:current train perplexity3.5865306854248047
INFO:root:current mean train loss 1623.6853961153513
INFO:root:current train perplexity3.587843894958496
INFO:root:current mean train loss 1622.1716762987974
INFO:root:current train perplexity3.590259552001953
INFO:root:current mean train loss 1620.4034230623313
INFO:root:current train perplexity3.5914523601531982
INFO:root:current mean train loss 1625.8249766720564
INFO:root:current train perplexity3.6005983352661133
INFO:root:current mean train loss 1628.3102279489972
INFO:root:current train perplexity3.6033623218536377
INFO:root:current mean train loss 1628.982665334619
INFO:root:current train perplexity3.605393886566162
INFO:root:current mean train loss 1630.4066357840231
INFO:root:current train perplexity3.6079347133636475
INFO:root:current mean train loss 1631.6154968860756
INFO:root:current train perplexity3.6106858253479004
INFO:root:current mean train loss 1630.7234777339909
INFO:root:current train perplexity3.6118574142456055
INFO:root:current mean train loss 1631.4767529821438
INFO:root:current train perplexity3.614027976989746
INFO:root:current mean train loss 1630.8225039479767
INFO:root:current train perplexity3.613856554031372
INFO:root:current mean train loss 1631.5608076994945
INFO:root:current train perplexity3.6166698932647705
INFO:root:current mean train loss 1631.5254027717283
INFO:root:current train perplexity3.617318868637085
INFO:root:current mean train loss 1631.7403196713292
INFO:root:current train perplexity3.6190860271453857
INFO:root:current mean train loss 1631.2675950351731
INFO:root:current train perplexity3.6199593544006348
INFO:root:current mean train loss 1631.4185429852441
INFO:root:current train perplexity3.621337413787842
INFO:root:current mean train loss 1632.1986008740155
INFO:root:current train perplexity3.6237056255340576
INFO:root:current mean train loss 1633.806636804333
INFO:root:current train perplexity3.626288414001465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.55s/it]
INFO:root:final mean train loss: 1633.703958931681
INFO:root:final train perplexity: 3.6271138191223145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:41<00:00, 41.30s/it]
INFO:root:eval mean loss: 2044.3088768492353
INFO:root:eval perplexity: 5.22428035736084
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it]
INFO:root:eval mean loss: 2539.8073003241357
INFO:root:eval perplexity: 7.981482028961182
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [9:34:35<25:15:18, 627.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.8732946059283
INFO:root:current train perplexity3.5693821907043457
INFO:root:current mean train loss 1612.1552588619404
INFO:root:current train perplexity3.5851895809173584
INFO:root:current mean train loss 1616.747795431023
INFO:root:current train perplexity3.5797595977783203
INFO:root:current mean train loss 1617.749170360451
INFO:root:current train perplexity3.5749473571777344
INFO:root:current mean train loss 1616.294458345334
INFO:root:current train perplexity3.577557325363159
INFO:root:current mean train loss 1617.645397171992
INFO:root:current train perplexity3.5768165588378906
INFO:root:current mean train loss 1617.89357991023
INFO:root:current train perplexity3.5817532539367676
INFO:root:current mean train loss 1618.733360851818
INFO:root:current train perplexity3.586747407913208
INFO:root:current mean train loss 1620.6369286406812
INFO:root:current train perplexity3.591531753540039
INFO:root:current mean train loss 1622.7276900166873
INFO:root:current train perplexity3.5958166122436523
INFO:root:current mean train loss 1623.9640199524752
INFO:root:current train perplexity3.5995328426361084
INFO:root:current mean train loss 1624.5455090827202
INFO:root:current train perplexity3.600088357925415
INFO:root:current mean train loss 1624.9281932762801
INFO:root:current train perplexity3.6019668579101562
INFO:root:current mean train loss 1625.5412667201556
INFO:root:current train perplexity3.6037166118621826
INFO:root:current mean train loss 1625.8432724445934
INFO:root:current train perplexity3.604454755783081
INFO:root:current mean train loss 1626.9558999112562
INFO:root:current train perplexity3.6066129207611084
INFO:root:current mean train loss 1627.1490575633989
INFO:root:current train perplexity3.6078624725341797
INFO:root:current mean train loss 1627.7630347721579
INFO:root:current train perplexity3.608610153198242
INFO:root:current mean train loss 1627.1140802314783
INFO:root:current train perplexity3.6089611053466797
INFO:root:current mean train loss 1627.3978771379348
INFO:root:current train perplexity3.609645366668701

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.46s/it]
INFO:root:final mean train loss: 1627.558348101194
INFO:root:final train perplexity: 3.609576463699341
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2046.4052375090037
INFO:root:eval perplexity: 5.233144760131836
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it]
INFO:root:eval mean loss: 2544.4745344878934
INFO:root:eval perplexity: 8.012005805969238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [9:44:57<25:01:40, 625.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1597.5185044232537
INFO:root:current train perplexity3.5378060340881348
INFO:root:current mean train loss 1592.9018279827194
INFO:root:current train perplexity3.534395456314087
INFO:root:current mean train loss 1605.3467219987238
INFO:root:current train perplexity3.5505595207214355
INFO:root:current mean train loss 1613.427583439058
INFO:root:current train perplexity3.561946153640747
INFO:root:current mean train loss 1615.0947365771376
INFO:root:current train perplexity3.5662670135498047
INFO:root:current mean train loss 1613.73497759755
INFO:root:current train perplexity3.564275026321411
INFO:root:current mean train loss 1615.5408931796635
INFO:root:current train perplexity3.5685009956359863
INFO:root:current mean train loss 1616.2517530337154
INFO:root:current train perplexity3.569305658340454
INFO:root:current mean train loss 1615.959714501781
INFO:root:current train perplexity3.571953058242798
INFO:root:current mean train loss 1615.9764963946257
INFO:root:current train perplexity3.5726518630981445
INFO:root:current mean train loss 1616.6022144321257
INFO:root:current train perplexity3.575333833694458
INFO:root:current mean train loss 1618.0689349402353
INFO:root:current train perplexity3.5763912200927734
INFO:root:current mean train loss 1618.0705720579786
INFO:root:current train perplexity3.5776119232177734
INFO:root:current mean train loss 1619.4764086245432
INFO:root:current train perplexity3.5815091133117676
INFO:root:current mean train loss 1620.5809277377402
INFO:root:current train perplexity3.583545446395874
INFO:root:current mean train loss 1621.3490650563144
INFO:root:current train perplexity3.584869384765625
INFO:root:current mean train loss 1621.9115797628567
INFO:root:current train perplexity3.5872297286987305
INFO:root:current mean train loss 1622.99602173479
INFO:root:current train perplexity3.590824842453003
INFO:root:current mean train loss 1622.5354816389367
INFO:root:current train perplexity3.59165358543396
INFO:root:current mean train loss 1622.0795380999527
INFO:root:current train perplexity3.5927200317382812

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.95s/it]
INFO:root:final mean train loss: 1621.6887327070135
INFO:root:final train perplexity: 3.5929062366485596
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2048.477105756178
INFO:root:eval perplexity: 5.241921424865723
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.29s/it]
INFO:root:eval mean loss: 2545.5709518540834
INFO:root:eval perplexity: 8.019193649291992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [9:55:26<24:53:37, 626.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1592.8936606014477
INFO:root:current train perplexity3.519115924835205
INFO:root:current mean train loss 1601.404774257115
INFO:root:current train perplexity3.5287675857543945
INFO:root:current mean train loss 1602.4955325909516
INFO:root:current train perplexity3.542156457901001
INFO:root:current mean train loss 1607.0144056237261
INFO:root:current train perplexity3.5561556816101074
INFO:root:current mean train loss 1607.214828882462
INFO:root:current train perplexity3.553253412246704
INFO:root:current mean train loss 1608.6230930811923
INFO:root:current train perplexity3.5514256954193115
INFO:root:current mean train loss 1608.7201053824967
INFO:root:current train perplexity3.5513689517974854
INFO:root:current mean train loss 1606.5106264750164
INFO:root:current train perplexity3.5518476963043213
INFO:root:current mean train loss 1606.4362597487489
INFO:root:current train perplexity3.552525758743286
INFO:root:current mean train loss 1608.6625713001597
INFO:root:current train perplexity3.5589072704315186
INFO:root:current mean train loss 1611.0323433751023
INFO:root:current train perplexity3.562547445297241
INFO:root:current mean train loss 1611.562270282066
INFO:root:current train perplexity3.5664894580841064
INFO:root:current mean train loss 1612.2767098122968
INFO:root:current train perplexity3.5682373046875
INFO:root:current mean train loss 1611.9161829362836
INFO:root:current train perplexity3.5682802200317383
INFO:root:current mean train loss 1612.5027438379439
INFO:root:current train perplexity3.5687673091888428
INFO:root:current mean train loss 1613.1154882469955
INFO:root:current train perplexity3.5694191455841064
INFO:root:current mean train loss 1613.483865715045
INFO:root:current train perplexity3.569885492324829
INFO:root:current mean train loss 1614.2460875360134
INFO:root:current train perplexity3.570223569869995
INFO:root:current mean train loss 1614.5659175766611
INFO:root:current train perplexity3.5707874298095703
INFO:root:current mean train loss 1615.1708018605302
INFO:root:current train perplexity3.5726518630981445

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.10s/it]
INFO:root:final mean train loss: 1614.720045619701
INFO:root:final train perplexity: 3.573213815689087
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 2050.1111826795213
INFO:root:eval perplexity: 5.2488532066345215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it]
INFO:root:eval mean loss: 2548.017329222767
INFO:root:eval perplexity: 8.035252571105957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [10:05:50<24:40:51, 625.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1594.2311250574448
INFO:root:current train perplexity3.5131003856658936
INFO:root:current mean train loss 1595.405578943201
INFO:root:current train perplexity3.530076742172241
INFO:root:current mean train loss 1599.8518203467654
INFO:root:current train perplexity3.5397136211395264
INFO:root:current mean train loss 1600.5174468597809
INFO:root:current train perplexity3.544630527496338
INFO:root:current mean train loss 1600.894407417848
INFO:root:current train perplexity3.541659355163574
INFO:root:current mean train loss 1601.9520791599894
INFO:root:current train perplexity3.5419600009918213
INFO:root:current mean train loss 1602.177478116446
INFO:root:current train perplexity3.5432181358337402
INFO:root:current mean train loss 1602.6791897330315
INFO:root:current train perplexity3.546865224838257
INFO:root:current mean train loss 1604.1455245023394
INFO:root:current train perplexity3.544834613800049
INFO:root:current mean train loss 1604.2835788784898
INFO:root:current train perplexity3.544475793838501
INFO:root:current mean train loss 1603.2040212323589
INFO:root:current train perplexity3.5432498455047607
INFO:root:current mean train loss 1604.4810262559336
INFO:root:current train perplexity3.545393705368042
INFO:root:current mean train loss 1606.2888069598127
INFO:root:current train perplexity3.5471384525299072
INFO:root:current mean train loss 1606.3891546035932
INFO:root:current train perplexity3.5488462448120117
INFO:root:current mean train loss 1606.7989358921243
INFO:root:current train perplexity3.5505456924438477
INFO:root:current mean train loss 1608.0195666773461
INFO:root:current train perplexity3.552461862564087
INFO:root:current mean train loss 1608.657608711401
INFO:root:current train perplexity3.5555384159088135
INFO:root:current mean train loss 1609.405406244529
INFO:root:current train perplexity3.5565295219421387
INFO:root:current mean train loss 1609.4098252678423
INFO:root:current train perplexity3.557990550994873

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.74s/it]
INFO:root:final mean train loss: 1609.5534215206699
INFO:root:final train perplexity: 3.5586836338043213
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.52s/it]
INFO:root:eval mean loss: 2054.4182605067044
INFO:root:eval perplexity: 5.267168045043945
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it]
INFO:root:eval mean loss: 2551.6875281367743
INFO:root:eval perplexity: 8.059410095214844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [10:16:16<24:30:53, 625.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1584.8814697265625
INFO:root:current train perplexity3.3954310417175293
INFO:root:current mean train loss 1570.0078160903033
INFO:root:current train perplexity3.4558725357055664
INFO:root:current mean train loss 1580.7740744411356
INFO:root:current train perplexity3.485332727432251
INFO:root:current mean train loss 1583.873972507502
INFO:root:current train perplexity3.487870693206787
INFO:root:current mean train loss 1587.1028494004586
INFO:root:current train perplexity3.4960246086120605
INFO:root:current mean train loss 1589.1187024363483
INFO:root:current train perplexity3.504404067993164
INFO:root:current mean train loss 1593.3741309080408
INFO:root:current train perplexity3.515791654586792
INFO:root:current mean train loss 1595.1082947994569
INFO:root:current train perplexity3.5197460651397705
INFO:root:current mean train loss 1596.2800973335704
INFO:root:current train perplexity3.520120143890381
INFO:root:current mean train loss 1597.652058603494
INFO:root:current train perplexity3.5223608016967773
INFO:root:current mean train loss 1598.2809566414046
INFO:root:current train perplexity3.526676654815674
INFO:root:current mean train loss 1600.7034971926043
INFO:root:current train perplexity3.529967784881592
INFO:root:current mean train loss 1601.020262046979
INFO:root:current train perplexity3.5297255516052246
INFO:root:current mean train loss 1601.0564826461393
INFO:root:current train perplexity3.5333166122436523
INFO:root:current mean train loss 1601.2869695426734
INFO:root:current train perplexity3.536097526550293
INFO:root:current mean train loss 1601.7497207499375
INFO:root:current train perplexity3.5373568534851074
INFO:root:current mean train loss 1601.6207526846324
INFO:root:current train perplexity3.539000988006592
INFO:root:current mean train loss 1601.8324166106281
INFO:root:current train perplexity3.5385682582855225
INFO:root:current mean train loss 1602.3833052521938
INFO:root:current train perplexity3.538814067840576
INFO:root:current mean train loss 1602.5593008849648
INFO:root:current train perplexity3.53944730758667

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.19s/it]
INFO:root:final mean train loss: 1603.1769810848266
INFO:root:final train perplexity: 3.54083251953125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.06s/it]
INFO:root:eval mean loss: 2057.8043225876827
INFO:root:eval perplexity: 5.28161096572876
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.50s/it]
INFO:root:eval mean loss: 2557.9619958755816
INFO:root:eval perplexity: 8.100869178771973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [10:26:46<24:23:17, 627.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1584.7807938425165
INFO:root:current train perplexity3.512561321258545
INFO:root:current mean train loss 1588.56414948792
INFO:root:current train perplexity3.505934476852417
INFO:root:current mean train loss 1593.5278844267266
INFO:root:current train perplexity3.5036144256591797
INFO:root:current mean train loss 1590.3451187946953
INFO:root:current train perplexity3.5012128353118896
INFO:root:current mean train loss 1585.3785135273718
INFO:root:current train perplexity3.4927000999450684
INFO:root:current mean train loss 1589.2078013043413
INFO:root:current train perplexity3.4990665912628174
INFO:root:current mean train loss 1588.7167964805888
INFO:root:current train perplexity3.5047194957733154
INFO:root:current mean train loss 1590.7466969708905
INFO:root:current train perplexity3.5103185176849365
INFO:root:current mean train loss 1590.7303024124312
INFO:root:current train perplexity3.5100247859954834
INFO:root:current mean train loss 1591.2167485250611
INFO:root:current train perplexity3.51198148727417
INFO:root:current mean train loss 1591.782903998827
INFO:root:current train perplexity3.513315200805664
INFO:root:current mean train loss 1592.380820321227
INFO:root:current train perplexity3.5148580074310303
INFO:root:current mean train loss 1593.2524243824985
INFO:root:current train perplexity3.5159659385681152
INFO:root:current mean train loss 1593.6686612838503
INFO:root:current train perplexity3.517205238342285
INFO:root:current mean train loss 1594.379903200565
INFO:root:current train perplexity3.5177016258239746
INFO:root:current mean train loss 1595.1265210973502
INFO:root:current train perplexity3.518829822540283
INFO:root:current mean train loss 1595.9834154775806
INFO:root:current train perplexity3.5208284854888916
INFO:root:current mean train loss 1596.4763741041077
INFO:root:current train perplexity3.5219626426696777
INFO:root:current mean train loss 1596.7997416055352
INFO:root:current train perplexity3.5239484310150146
INFO:root:current mean train loss 1597.1848165523018
INFO:root:current train perplexity3.52378249168396

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.90s/it]
INFO:root:final mean train loss: 1597.2882581779106
INFO:root:final train perplexity: 3.524425983428955
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.95s/it]
INFO:root:eval mean loss: 2058.6785654920213
INFO:root:eval perplexity: 5.285348415374756
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.62s/it]
INFO:root:eval mean loss: 2558.258788196753
INFO:root:eval perplexity: 8.102835655212402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [10:37:09<24:09:43, 625.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1561.4536709255642
INFO:root:current train perplexity3.435774803161621
INFO:root:current mean train loss 1563.5183267032398
INFO:root:current train perplexity3.4425015449523926
INFO:root:current mean train loss 1572.9525974079713
INFO:root:current train perplexity3.4702999591827393
INFO:root:current mean train loss 1579.1890709286645
INFO:root:current train perplexity3.4774749279022217
INFO:root:current mean train loss 1578.1818212106687
INFO:root:current train perplexity3.4787333011627197
INFO:root:current mean train loss 1579.112749469814
INFO:root:current train perplexity3.4793860912323
INFO:root:current mean train loss 1579.7041560718849
INFO:root:current train perplexity3.4800820350646973
INFO:root:current mean train loss 1583.1486529474673
INFO:root:current train perplexity3.485236406326294
INFO:root:current mean train loss 1584.2287994822816
INFO:root:current train perplexity3.4903974533081055
INFO:root:current mean train loss 1584.3526478302786
INFO:root:current train perplexity3.489222526550293
INFO:root:current mean train loss 1584.1637197369314
INFO:root:current train perplexity3.488251209259033
INFO:root:current mean train loss 1584.1050136727347
INFO:root:current train perplexity3.4886794090270996
INFO:root:current mean train loss 1585.05999864498
INFO:root:current train perplexity3.493844747543335
INFO:root:current mean train loss 1587.022604388391
INFO:root:current train perplexity3.4978013038635254
INFO:root:current mean train loss 1586.8755142084403
INFO:root:current train perplexity3.4996724128723145
INFO:root:current mean train loss 1588.6610339482625
INFO:root:current train perplexity3.5042614936828613
INFO:root:current mean train loss 1589.546742035882
INFO:root:current train perplexity3.505408763885498
INFO:root:current mean train loss 1590.5327722927393
INFO:root:current train perplexity3.50749135017395
INFO:root:current mean train loss 1591.312393886591
INFO:root:current train perplexity3.50736927986145
INFO:root:current mean train loss 1591.9934169674707
INFO:root:current train perplexity3.5081021785736084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:05<00:00, 545.91s/it]
INFO:root:final mean train loss: 1591.24409901152
INFO:root:final train perplexity: 3.5076661109924316
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2062.3460567826073
INFO:root:eval perplexity: 5.3010478019714355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it]
INFO:root:eval mean loss: 2565.18325610871
INFO:root:eval perplexity: 8.148853302001953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [10:47:30<23:56:14, 624.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.5660884065448
INFO:root:current train perplexity3.444960594177246
INFO:root:current mean train loss 1578.87648638557
INFO:root:current train perplexity3.470921754837036
INFO:root:current mean train loss 1584.9091652127595
INFO:root:current train perplexity3.480837345123291
INFO:root:current mean train loss 1584.9139217560419
INFO:root:current train perplexity3.484280586242676
INFO:root:current mean train loss 1585.739716721423
INFO:root:current train perplexity3.4869613647460938
INFO:root:current mean train loss 1585.0781121969653
INFO:root:current train perplexity3.486178398132324
INFO:root:current mean train loss 1585.2323833658356
INFO:root:current train perplexity3.483513355255127
INFO:root:current mean train loss 1585.1290814930383
INFO:root:current train perplexity3.4790585041046143
INFO:root:current mean train loss 1584.0776149664787
INFO:root:current train perplexity3.4775917530059814
INFO:root:current mean train loss 1582.8417303959945
INFO:root:current train perplexity3.4770665168762207
INFO:root:current mean train loss 1583.477433337785
INFO:root:current train perplexity3.480314016342163
INFO:root:current mean train loss 1583.4190367858305
INFO:root:current train perplexity3.482027530670166
INFO:root:current mean train loss 1583.8419667797286
INFO:root:current train perplexity3.483482599258423
INFO:root:current mean train loss 1584.549521751432
INFO:root:current train perplexity3.487015962600708
INFO:root:current mean train loss 1584.5031345942393
INFO:root:current train perplexity3.4876840114593506
INFO:root:current mean train loss 1584.1073820233269
INFO:root:current train perplexity3.489474296569824
INFO:root:current mean train loss 1584.7337232937036
INFO:root:current train perplexity3.489211082458496
INFO:root:current mean train loss 1584.8567494089373
INFO:root:current train perplexity3.4898529052734375
INFO:root:current mean train loss 1585.3182858561286
INFO:root:current train perplexity3.4914345741271973
INFO:root:current mean train loss 1585.8911897861462
INFO:root:current train perplexity3.4925551414489746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.10s/it]
INFO:root:final mean train loss: 1585.8779182068579
INFO:root:final train perplexity: 3.4928524494171143
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 2065.3591200375386
INFO:root:eval perplexity: 5.313981533050537
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.76s/it]
INFO:root:eval mean loss: 2570.582606971687
INFO:root:eval perplexity: 8.184918403625488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [10:57:56<23:46:35, 624.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1577.786785016741
INFO:root:current train perplexity3.446533441543579
INFO:root:current mean train loss 1575.830109719669
INFO:root:current train perplexity3.4394338130950928
INFO:root:current mean train loss 1571.556805645978
INFO:root:current train perplexity3.4401988983154297
INFO:root:current mean train loss 1567.036280616554
INFO:root:current train perplexity3.4414799213409424
INFO:root:current mean train loss 1570.4454259994181
INFO:root:current train perplexity3.448409080505371
INFO:root:current mean train loss 1571.8128501490544
INFO:root:current train perplexity3.4545042514801025
INFO:root:current mean train loss 1571.9537626807369
INFO:root:current train perplexity3.4542527198791504
INFO:root:current mean train loss 1573.059999778054
INFO:root:current train perplexity3.456624984741211
INFO:root:current mean train loss 1573.7586793395294
INFO:root:current train perplexity3.4616785049438477
INFO:root:current mean train loss 1573.5478727045747
INFO:root:current train perplexity3.46049427986145
INFO:root:current mean train loss 1573.3581970785265
INFO:root:current train perplexity3.4630379676818848
INFO:root:current mean train loss 1574.2089648646167
INFO:root:current train perplexity3.4664626121520996
INFO:root:current mean train loss 1574.3651808370755
INFO:root:current train perplexity3.4679012298583984
INFO:root:current mean train loss 1576.1864532247946
INFO:root:current train perplexity3.470623254776001
INFO:root:current mean train loss 1577.1777650171396
INFO:root:current train perplexity3.4719016551971436
INFO:root:current mean train loss 1577.6142675314738
INFO:root:current train perplexity3.473607301712036
INFO:root:current mean train loss 1578.3679253309788
INFO:root:current train perplexity3.474134922027588
INFO:root:current mean train loss 1579.0849574202198
INFO:root:current train perplexity3.474255323410034
INFO:root:current mean train loss 1579.9860803940717
INFO:root:current train perplexity3.475412368774414
INFO:root:current mean train loss 1581.182152762631
INFO:root:current train perplexity3.478332281112671

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.41s/it]
INFO:root:final mean train loss: 1580.6348493136484
INFO:root:final train perplexity: 3.4784395694732666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:40<00:00, 40.47s/it]
INFO:root:eval mean loss: 2066.8074587558176
INFO:root:eval perplexity: 5.32020902633667
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it]
INFO:root:eval mean loss: 2573.323050424562
INFO:root:eval perplexity: 8.203282356262207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [11:08:23<23:37:50, 625.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1560.4894149604886
INFO:root:current train perplexity3.396235704421997
INFO:root:current mean train loss 1561.1877578490557
INFO:root:current train perplexity3.4219796657562256
INFO:root:current mean train loss 1564.974150441665
INFO:root:current train perplexity3.4371562004089355
INFO:root:current mean train loss 1563.7445831693112
INFO:root:current train perplexity3.4328742027282715
INFO:root:current mean train loss 1563.781196108589
INFO:root:current train perplexity3.437166690826416
INFO:root:current mean train loss 1566.346583403761
INFO:root:current train perplexity3.435650110244751
INFO:root:current mean train loss 1564.8201253965954
INFO:root:current train perplexity3.436856508255005
INFO:root:current mean train loss 1565.603011522693
INFO:root:current train perplexity3.4401965141296387
INFO:root:current mean train loss 1566.4405997877325
INFO:root:current train perplexity3.4418890476226807
INFO:root:current mean train loss 1567.0658482142858
INFO:root:current train perplexity3.4459099769592285
INFO:root:current mean train loss 1567.9462065218563
INFO:root:current train perplexity3.4496874809265137
INFO:root:current mean train loss 1569.29259338636
INFO:root:current train perplexity3.4523208141326904
INFO:root:current mean train loss 1570.3647699956293
INFO:root:current train perplexity3.455003023147583
INFO:root:current mean train loss 1570.2434700743793
INFO:root:current train perplexity3.455906867980957
INFO:root:current mean train loss 1571.3034729537503
INFO:root:current train perplexity3.4565255641937256
INFO:root:current mean train loss 1571.9908544952643
INFO:root:current train perplexity3.457486391067505
INFO:root:current mean train loss 1572.8810100996361
INFO:root:current train perplexity3.458249807357788
INFO:root:current mean train loss 1572.5896838358412
INFO:root:current train perplexity3.459096670150757
INFO:root:current mean train loss 1573.7351265054692
INFO:root:current train perplexity3.4601094722747803

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.41s/it]
INFO:root:final mean train loss: 1574.2664560076569
INFO:root:final train perplexity: 3.461012840270996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it]
INFO:root:eval mean loss: 2072.1316355170934
INFO:root:eval perplexity: 5.343166351318359
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.51s/it]
INFO:root:eval mean loss: 2578.3385897156195
INFO:root:eval perplexity: 8.237001419067383
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [11:18:44<23:24:26, 624.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1506.7322387695312
INFO:root:current train perplexity3.368178367614746
INFO:root:current mean train loss 1544.9706808236929
INFO:root:current train perplexity3.396806240081787
INFO:root:current mean train loss 1548.1505216710707
INFO:root:current train perplexity3.406391143798828
INFO:root:current mean train loss 1549.5886013633326
INFO:root:current train perplexity3.41438889503479
INFO:root:current mean train loss 1553.250524841913
INFO:root:current train perplexity3.4179863929748535
INFO:root:current mean train loss 1556.2737339564733
INFO:root:current train perplexity3.422072410583496
INFO:root:current mean train loss 1558.9401400736626
INFO:root:current train perplexity3.423349380493164
INFO:root:current mean train loss 1561.3374609513717
INFO:root:current train perplexity3.4291865825653076
INFO:root:current mean train loss 1563.056143993169
INFO:root:current train perplexity3.4325640201568604
INFO:root:current mean train loss 1564.3834286580043
INFO:root:current train perplexity3.436518907546997
INFO:root:current mean train loss 1565.1850405993214
INFO:root:current train perplexity3.4384572505950928
INFO:root:current mean train loss 1566.899132548899
INFO:root:current train perplexity3.4407291412353516
INFO:root:current mean train loss 1566.8483065481598
INFO:root:current train perplexity3.439223051071167
INFO:root:current mean train loss 1567.3835450154872
INFO:root:current train perplexity3.4405741691589355
INFO:root:current mean train loss 1567.664664070151
INFO:root:current train perplexity3.4414587020874023
INFO:root:current mean train loss 1568.789938094768
INFO:root:current train perplexity3.4433979988098145
INFO:root:current mean train loss 1568.7662547580026
INFO:root:current train perplexity3.4435338973999023
INFO:root:current mean train loss 1568.5448191468145
INFO:root:current train perplexity3.4437692165374756
INFO:root:current mean train loss 1568.2792251485414
INFO:root:current train perplexity3.4435482025146484
INFO:root:current mean train loss 1568.7022866641773
INFO:root:current train perplexity3.444880485534668

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.34s/it]
INFO:root:final mean train loss: 1568.5876410980147
INFO:root:final train perplexity: 3.445546865463257
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.52s/it]
INFO:root:eval mean loss: 2074.628095478031
INFO:root:eval perplexity: 5.353964805603027
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2584.1918296002327
INFO:root:eval perplexity: 8.276524543762207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [11:29:17<23:20:10, 626.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1564.8979782831102
INFO:root:current train perplexity3.422769069671631
INFO:root:current mean train loss 1558.147951236441
INFO:root:current train perplexity3.414280891418457
INFO:root:current mean train loss 1566.5147594607254
INFO:root:current train perplexity3.424391031265259
INFO:root:current mean train loss 1558.8331869250146
INFO:root:current train perplexity3.414661169052124
INFO:root:current mean train loss 1556.008877498237
INFO:root:current train perplexity3.410740852355957
INFO:root:current mean train loss 1556.7843540067179
INFO:root:current train perplexity3.414231300354004
INFO:root:current mean train loss 1556.6058774201767
INFO:root:current train perplexity3.415182590484619
INFO:root:current mean train loss 1557.1023629832698
INFO:root:current train perplexity3.41363263130188
INFO:root:current mean train loss 1557.3638730159485
INFO:root:current train perplexity3.414586305618286
INFO:root:current mean train loss 1557.1414915534235
INFO:root:current train perplexity3.41717267036438
INFO:root:current mean train loss 1556.6256144165875
INFO:root:current train perplexity3.4181969165802
INFO:root:current mean train loss 1557.85726844318
INFO:root:current train perplexity3.418168783187866
INFO:root:current mean train loss 1557.6971268587479
INFO:root:current train perplexity3.419903039932251
INFO:root:current mean train loss 1557.5598197203526
INFO:root:current train perplexity3.4207541942596436
INFO:root:current mean train loss 1558.850234244425
INFO:root:current train perplexity3.4223902225494385
INFO:root:current mean train loss 1561.1977229271963
INFO:root:current train perplexity3.424628257751465
INFO:root:current mean train loss 1562.247306244819
INFO:root:current train perplexity3.426074504852295
INFO:root:current mean train loss 1562.7568913337313
INFO:root:current train perplexity3.4287548065185547
INFO:root:current mean train loss 1563.2929881900827
INFO:root:current train perplexity3.430521011352539
INFO:root:current mean train loss 1563.2495839060873
INFO:root:current train perplexity3.430074453353882

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.39s/it]
INFO:root:final mean train loss: 1563.2575587870433
INFO:root:final train perplexity: 3.4310927391052246
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.68s/it]
INFO:root:eval mean loss: 2076.2554568026926
INFO:root:eval perplexity: 5.361015796661377
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.44s/it]
INFO:root:eval mean loss: 2587.2745893762467
INFO:root:eval perplexity: 8.297417640686035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [11:39:45<23:10:18, 627.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1521.0157470703125
INFO:root:current train perplexity3.3596532344818115
INFO:root:current mean train loss 1530.9877000891645
INFO:root:current train perplexity3.34899640083313
INFO:root:current mean train loss 1537.4608985195641
INFO:root:current train perplexity3.3615429401397705
INFO:root:current mean train loss 1541.7587211654031
INFO:root:current train perplexity3.371457099914551
INFO:root:current mean train loss 1541.930198634596
INFO:root:current train perplexity3.374901533126831
INFO:root:current mean train loss 1544.578141563444
INFO:root:current train perplexity3.3798654079437256
INFO:root:current mean train loss 1547.4229992714047
INFO:root:current train perplexity3.387827157974243
INFO:root:current mean train loss 1547.8552275866996
INFO:root:current train perplexity3.393791675567627
INFO:root:current mean train loss 1550.0627437036192
INFO:root:current train perplexity3.399132251739502
INFO:root:current mean train loss 1551.0004712330508
INFO:root:current train perplexity3.4007408618927
INFO:root:current mean train loss 1549.4021524124294
INFO:root:current train perplexity3.3973145484924316
INFO:root:current mean train loss 1549.9751118155482
INFO:root:current train perplexity3.3976094722747803
INFO:root:current mean train loss 1551.8410989641181
INFO:root:current train perplexity3.400376081466675
INFO:root:current mean train loss 1553.5789842363251
INFO:root:current train perplexity3.4046471118927
INFO:root:current mean train loss 1554.8433253005749
INFO:root:current train perplexity3.406477212905884
INFO:root:current mean train loss 1555.7479898083195
INFO:root:current train perplexity3.408604621887207
INFO:root:current mean train loss 1556.5440743880686
INFO:root:current train perplexity3.410663604736328
INFO:root:current mean train loss 1556.5298422581857
INFO:root:current train perplexity3.412416458129883
INFO:root:current mean train loss 1556.2886610228297
INFO:root:current train perplexity3.4128196239471436
INFO:root:current mean train loss 1557.3813854489165
INFO:root:current train perplexity3.414064884185791

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.43s/it]
INFO:root:final mean train loss: 1557.4298738988432
INFO:root:final train perplexity: 3.4153597354888916
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it]
INFO:root:eval mean loss: 2078.2016527974015
INFO:root:eval perplexity: 5.369460105895996
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it]
INFO:root:eval mean loss: 2588.6761149954286
INFO:root:eval perplexity: 8.306933403015137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [11:50:08<22:57:09, 625.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1549.7388427734375
INFO:root:current train perplexity3.3759562969207764
INFO:root:current mean train loss 1546.4277083858367
INFO:root:current train perplexity3.3731391429901123
INFO:root:current mean train loss 1543.8727869370405
INFO:root:current train perplexity3.368913173675537
INFO:root:current mean train loss 1542.0980344960387
INFO:root:current train perplexity3.368032217025757
INFO:root:current mean train loss 1545.2428885323661
INFO:root:current train perplexity3.370330810546875
INFO:root:current mean train loss 1545.1283271308419
INFO:root:current train perplexity3.3679487705230713
INFO:root:current mean train loss 1545.2107278372496
INFO:root:current train perplexity3.373689651489258
INFO:root:current mean train loss 1547.1434219461403
INFO:root:current train perplexity3.3794915676116943
INFO:root:current mean train loss 1547.656833938688
INFO:root:current train perplexity3.382101535797119
INFO:root:current mean train loss 1547.655270625409
INFO:root:current train perplexity3.3822824954986572
INFO:root:current mean train loss 1548.9906577449274
INFO:root:current train perplexity3.3844761848449707
INFO:root:current mean train loss 1547.9328995873918
INFO:root:current train perplexity3.383409261703491
INFO:root:current mean train loss 1549.4968378439366
INFO:root:current train perplexity3.389496326446533
INFO:root:current mean train loss 1549.275484767787
INFO:root:current train perplexity3.390698194503784
INFO:root:current mean train loss 1549.7437297807935
INFO:root:current train perplexity3.3909783363342285
INFO:root:current mean train loss 1550.8419727975534
INFO:root:current train perplexity3.3933069705963135
INFO:root:current mean train loss 1550.8425357876226
INFO:root:current train perplexity3.394444227218628
INFO:root:current mean train loss 1552.132037996906
INFO:root:current train perplexity3.396855592727661
INFO:root:current mean train loss 1551.5175953003916
INFO:root:current train perplexity3.398084878921509
INFO:root:current mean train loss 1552.7102044537244
INFO:root:current train perplexity3.4018452167510986

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.03s/it]
INFO:root:final mean train loss: 1552.5726783433108
INFO:root:final train perplexity: 3.4023020267486572
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 2081.8980730205562
INFO:root:eval perplexity: 5.385536193847656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.93s/it]
INFO:root:eval mean loss: 2594.7560558995456
INFO:root:eval perplexity: 8.348340034484863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [12:00:30<22:44:11, 624.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1555.7931247287327
INFO:root:current train perplexity3.3849527835845947
INFO:root:current mean train loss 1546.4995464945948
INFO:root:current train perplexity3.3672921657562256
INFO:root:current mean train loss 1545.889410131118
INFO:root:current train perplexity3.3740696907043457
INFO:root:current mean train loss 1544.1188741704468
INFO:root:current train perplexity3.3711023330688477
INFO:root:current mean train loss 1539.880816960739
INFO:root:current train perplexity3.3666160106658936
INFO:root:current mean train loss 1537.8411517376667
INFO:root:current train perplexity3.363309860229492
INFO:root:current mean train loss 1539.1108805338542
INFO:root:current train perplexity3.365949869155884
INFO:root:current mean train loss 1540.37351618159
INFO:root:current train perplexity3.3680505752563477
INFO:root:current mean train loss 1540.732980570662
INFO:root:current train perplexity3.3705532550811768
INFO:root:current mean train loss 1542.6660982610758
INFO:root:current train perplexity3.373429775238037
INFO:root:current mean train loss 1541.9063793580924
INFO:root:current train perplexity3.3762404918670654
INFO:root:current mean train loss 1542.0511779785156
INFO:root:current train perplexity3.3756229877471924
INFO:root:current mean train loss 1541.389539226796
INFO:root:current train perplexity3.376291036605835
INFO:root:current mean train loss 1542.8084606470936
INFO:root:current train perplexity3.379073143005371
INFO:root:current mean train loss 1543.758756969286
INFO:root:current train perplexity3.3810229301452637
INFO:root:current mean train loss 1544.08648394325
INFO:root:current train perplexity3.3815536499023438
INFO:root:current mean train loss 1544.4956845370207
INFO:root:current train perplexity3.3812286853790283
INFO:root:current mean train loss 1545.7509186273235
INFO:root:current train perplexity3.3829028606414795
INFO:root:current mean train loss 1546.772130004361
INFO:root:current train perplexity3.3851380348205566
INFO:root:current mean train loss 1547.2925273903002
INFO:root:current train perplexity3.3870716094970703

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.58s/it]
INFO:root:final mean train loss: 1546.8939225912936
INFO:root:final train perplexity: 3.387098550796509
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2086.480365726119
INFO:root:eval perplexity: 5.405531883239746
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.71s/it]
INFO:root:eval mean loss: 2600.4727571095136
INFO:root:eval perplexity: 8.387462615966797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [12:11:04<22:39:08, 627.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1514.0083885621489
INFO:root:current train perplexity3.335430145263672
INFO:root:current mean train loss 1517.7958080150463
INFO:root:current train perplexity3.331650495529175
INFO:root:current mean train loss 1528.027165079612
INFO:root:current train perplexity3.3338868618011475
INFO:root:current mean train loss 1528.871388099494
INFO:root:current train perplexity3.3428077697753906
INFO:root:current mean train loss 1529.7142561149988
INFO:root:current train perplexity3.345109701156616
INFO:root:current mean train loss 1531.4096223737267
INFO:root:current train perplexity3.3501241207122803
INFO:root:current mean train loss 1532.7217803202104
INFO:root:current train perplexity3.3524792194366455
INFO:root:current mean train loss 1534.2861515330421
INFO:root:current train perplexity3.352661371231079
INFO:root:current mean train loss 1535.4158666415478
INFO:root:current train perplexity3.356173038482666
INFO:root:current mean train loss 1537.0625406078188
INFO:root:current train perplexity3.3603806495666504
INFO:root:current mean train loss 1535.6106037738894
INFO:root:current train perplexity3.3581748008728027
INFO:root:current mean train loss 1535.928987931363
INFO:root:current train perplexity3.358755111694336
INFO:root:current mean train loss 1535.5849253297129
INFO:root:current train perplexity3.3580925464630127
INFO:root:current mean train loss 1536.5282681975286
INFO:root:current train perplexity3.361391067504883
INFO:root:current mean train loss 1537.775720108268
INFO:root:current train perplexity3.363705635070801
INFO:root:current mean train loss 1538.8059861775537
INFO:root:current train perplexity3.366591453552246
INFO:root:current mean train loss 1539.0565536797153
INFO:root:current train perplexity3.3670570850372314
INFO:root:current mean train loss 1540.1537147040071
INFO:root:current train perplexity3.3679583072662354
INFO:root:current mean train loss 1541.3355851180975
INFO:root:current train perplexity3.37131404876709

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.87s/it]
INFO:root:final mean train loss: 1541.12377794259
INFO:root:final train perplexity: 3.3717200756073
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it]
INFO:root:eval mean loss: 2087.4117431640625
INFO:root:eval perplexity: 5.409605503082275
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.69s/it]
INFO:root:eval mean loss: 2603.713442538647
INFO:root:eval perplexity: 8.409719467163086
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [12:21:28<22:27:11, 626.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1564.5799967447917
INFO:root:current train perplexity3.3333775997161865
INFO:root:current mean train loss 1514.9564358693249
INFO:root:current train perplexity3.3142404556274414
INFO:root:current mean train loss 1524.1392217839805
INFO:root:current train perplexity3.313282012939453
INFO:root:current mean train loss 1525.2804609311172
INFO:root:current train perplexity3.317441463470459
INFO:root:current mean train loss 1529.0129881609837
INFO:root:current train perplexity3.32484769821167
INFO:root:current mean train loss 1526.7428463419435
INFO:root:current train perplexity3.3275628089904785
INFO:root:current mean train loss 1526.9175574173628
INFO:root:current train perplexity3.327035903930664
INFO:root:current mean train loss 1526.0505801625022
INFO:root:current train perplexity3.327962636947632
INFO:root:current mean train loss 1526.108933214514
INFO:root:current train perplexity3.334123373031616
INFO:root:current mean train loss 1526.964754285665
INFO:root:current train perplexity3.338710069656372
INFO:root:current mean train loss 1528.631558549096
INFO:root:current train perplexity3.339965343475342
INFO:root:current mean train loss 1530.4737113966432
INFO:root:current train perplexity3.3432867527008057
INFO:root:current mean train loss 1531.4868870572268
INFO:root:current train perplexity3.345313549041748
INFO:root:current mean train loss 1532.2842324973979
INFO:root:current train perplexity3.3483498096466064
INFO:root:current mean train loss 1532.9485335587437
INFO:root:current train perplexity3.349649667739868
INFO:root:current mean train loss 1533.5541245661884
INFO:root:current train perplexity3.3506085872650146
INFO:root:current mean train loss 1534.2734451769002
INFO:root:current train perplexity3.3524200916290283
INFO:root:current mean train loss 1535.3366209077108
INFO:root:current train perplexity3.3533847332000732
INFO:root:current mean train loss 1535.5451791283829
INFO:root:current train perplexity3.3542771339416504
INFO:root:current mean train loss 1536.0453119748286
INFO:root:current train perplexity3.355686664581299

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.14s/it]
INFO:root:final mean train loss: 1536.053808322893
INFO:root:final train perplexity: 3.3582653999328613
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it]
INFO:root:eval mean loss: 2096.9217624529033
INFO:root:eval perplexity: 5.451371192932129
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.98s/it]
INFO:root:eval mean loss: 2612.5931106459166
INFO:root:eval perplexity: 8.471014976501465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [12:31:50<22:13:13, 624.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1546.8492537788723
INFO:root:current train perplexity3.3263063430786133
INFO:root:current mean train loss 1531.1002604166667
INFO:root:current train perplexity3.3241796493530273
INFO:root:current mean train loss 1524.8170144119604
INFO:root:current train perplexity3.315805673599243
INFO:root:current mean train loss 1524.3234591174053
INFO:root:current train perplexity3.3164138793945312
INFO:root:current mean train loss 1525.4462573184471
INFO:root:current train perplexity3.3174288272857666
INFO:root:current mean train loss 1525.2794362172115
INFO:root:current train perplexity3.3207080364227295
INFO:root:current mean train loss 1524.8558404472437
INFO:root:current train perplexity3.321387529373169
INFO:root:current mean train loss 1524.7276273650912
INFO:root:current train perplexity3.325220823287964
INFO:root:current mean train loss 1524.4041989814323
INFO:root:current train perplexity3.3261115550994873
INFO:root:current mean train loss 1525.6069002657773
INFO:root:current train perplexity3.3296256065368652
INFO:root:current mean train loss 1524.7578357685347
INFO:root:current train perplexity3.330782890319824
INFO:root:current mean train loss 1525.673067006275
INFO:root:current train perplexity3.332252025604248
INFO:root:current mean train loss 1526.0441083058117
INFO:root:current train perplexity3.3329949378967285
INFO:root:current mean train loss 1526.4951418230053
INFO:root:current train perplexity3.333911180496216
INFO:root:current mean train loss 1527.0450379404483
INFO:root:current train perplexity3.335447311401367
INFO:root:current mean train loss 1529.1155064082507
INFO:root:current train perplexity3.338711738586426
INFO:root:current mean train loss 1528.8800638496177
INFO:root:current train perplexity3.339729070663452
INFO:root:current mean train loss 1529.0532441939024
INFO:root:current train perplexity3.3399643898010254
INFO:root:current mean train loss 1529.6398045241147
INFO:root:current train perplexity3.342144250869751
INFO:root:current mean train loss 1530.7979343392485
INFO:root:current train perplexity3.344240188598633

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.86s/it]
INFO:root:final mean train loss: 1530.9225753807264
INFO:root:final train perplexity: 3.3447017669677734
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.95s/it]
INFO:root:eval mean loss: 2094.8552683295934
INFO:root:eval perplexity: 5.442268371582031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 2611.0168526291
INFO:root:eval perplexity: 8.460103988647461
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [12:42:21<22:06:54, 626.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1517.7611358642578
INFO:root:current train perplexity3.307816982269287
INFO:root:current mean train loss 1513.8599931989397
INFO:root:current train perplexity3.292213201522827
INFO:root:current mean train loss 1523.537846883138
INFO:root:current train perplexity3.3046481609344482
INFO:root:current mean train loss 1520.8271771599266
INFO:root:current train perplexity3.3104095458984375
INFO:root:current mean train loss 1520.3250743519177
INFO:root:current train perplexity3.309070348739624
INFO:root:current mean train loss 1522.139648663556
INFO:root:current train perplexity3.310307741165161
INFO:root:current mean train loss 1521.7039281845093
INFO:root:current train perplexity3.312406301498413
INFO:root:current mean train loss 1520.9648630503061
INFO:root:current train perplexity3.312086343765259
INFO:root:current mean train loss 1521.9690329415457
INFO:root:current train perplexity3.314375877380371
INFO:root:current mean train loss 1522.781186237741
INFO:root:current train perplexity3.3165364265441895
INFO:root:current mean train loss 1521.9737552349384
INFO:root:current train perplexity3.3168740272521973
INFO:root:current mean train loss 1522.305076625891
INFO:root:current train perplexity3.3180959224700928
INFO:root:current mean train loss 1523.8889403312437
INFO:root:current train perplexity3.3223767280578613
INFO:root:current mean train loss 1524.6470563746211
INFO:root:current train perplexity3.324960231781006
INFO:root:current mean train loss 1524.9259235805935
INFO:root:current train perplexity3.3258566856384277
INFO:root:current mean train loss 1524.9935675286627
INFO:root:current train perplexity3.3264684677124023
INFO:root:current mean train loss 1524.5459131752573
INFO:root:current train perplexity3.3269736766815186
INFO:root:current mean train loss 1525.4440168008036
INFO:root:current train perplexity3.3273932933807373
INFO:root:current mean train loss 1526.5424468330716
INFO:root:current train perplexity3.3302159309387207
INFO:root:current mean train loss 1526.8072219691326
INFO:root:current train perplexity3.331610918045044

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.86s/it]
INFO:root:final mean train loss: 1526.452039910036
INFO:root:final train perplexity: 3.332930564880371
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2100.6819825950242
INFO:root:eval perplexity: 5.4679741859436035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it]
INFO:root:eval mean loss: 2618.180189200327
INFO:root:eval perplexity: 8.509810447692871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [12:52:46<21:55:33, 626.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1481.3306627775494
INFO:root:current train perplexity3.264495611190796
INFO:root:current mean train loss 1503.7953673927648
INFO:root:current train perplexity3.2776715755462646
INFO:root:current mean train loss 1510.3117680531068
INFO:root:current train perplexity3.285745859146118
INFO:root:current mean train loss 1507.7787583295037
INFO:root:current train perplexity3.288506031036377
INFO:root:current mean train loss 1511.9117805597818
INFO:root:current train perplexity3.2927937507629395
INFO:root:current mean train loss 1508.6824000031559
INFO:root:current train perplexity3.293450117111206
INFO:root:current mean train loss 1510.565058645774
INFO:root:current train perplexity3.29508638381958
INFO:root:current mean train loss 1512.115275333863
INFO:root:current train perplexity3.2986464500427246
INFO:root:current mean train loss 1513.128100187108
INFO:root:current train perplexity3.303241729736328
INFO:root:current mean train loss 1514.854721914389
INFO:root:current train perplexity3.3046953678131104
INFO:root:current mean train loss 1514.8599478181172
INFO:root:current train perplexity3.303558349609375
INFO:root:current mean train loss 1515.4559921225084
INFO:root:current train perplexity3.3059515953063965
INFO:root:current mean train loss 1516.349382811723
INFO:root:current train perplexity3.3073229789733887
INFO:root:current mean train loss 1516.6816336983868
INFO:root:current train perplexity3.307521343231201
INFO:root:current mean train loss 1517.4212154684283
INFO:root:current train perplexity3.3088290691375732
INFO:root:current mean train loss 1518.0580819484585
INFO:root:current train perplexity3.311335802078247
INFO:root:current mean train loss 1518.579363236459
INFO:root:current train perplexity3.3128042221069336
INFO:root:current mean train loss 1519.0800253228158
INFO:root:current train perplexity3.313166856765747
INFO:root:current mean train loss 1520.3826450723823
INFO:root:current train perplexity3.315605878829956
INFO:root:current mean train loss 1520.7139850162278
INFO:root:current train perplexity3.316840887069702

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.39s/it]
INFO:root:final mean train loss: 1520.3135967552812
INFO:root:final train perplexity: 3.316833972930908
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it]
INFO:root:eval mean loss: 2100.19056777413
INFO:root:eval perplexity: 5.465800762176514
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it]
INFO:root:eval mean loss: 2620.441867260223
INFO:root:eval perplexity: 8.525567054748535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [13:03:08<21:42:02, 624.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1510.3158288904137
INFO:root:current train perplexity3.283870220184326
INFO:root:current mean train loss 1503.290017314341
INFO:root:current train perplexity3.274789333343506
INFO:root:current mean train loss 1503.1584183073392
INFO:root:current train perplexity3.2828431129455566
INFO:root:current mean train loss 1503.8509018841912
INFO:root:current train perplexity3.282273054122925
INFO:root:current mean train loss 1504.1883771550304
INFO:root:current train perplexity3.279792547225952
INFO:root:current mean train loss 1507.3887278061711
INFO:root:current train perplexity3.2821168899536133
INFO:root:current mean train loss 1508.5482364280995
INFO:root:current train perplexity3.2852578163146973
INFO:root:current mean train loss 1509.1279170704133
INFO:root:current train perplexity3.2841405868530273
INFO:root:current mean train loss 1508.8290905286847
INFO:root:current train perplexity3.2866475582122803
INFO:root:current mean train loss 1510.2934576578944
INFO:root:current train perplexity3.2874457836151123
INFO:root:current mean train loss 1511.7069962428714
INFO:root:current train perplexity3.2889299392700195
INFO:root:current mean train loss 1511.3466801034124
INFO:root:current train perplexity3.290677785873413
INFO:root:current mean train loss 1511.5812398051169
INFO:root:current train perplexity3.2934622764587402
INFO:root:current mean train loss 1513.3333945461757
INFO:root:current train perplexity3.2958693504333496
INFO:root:current mean train loss 1513.8208137833117
INFO:root:current train perplexity3.297736167907715
INFO:root:current mean train loss 1514.3663880712952
INFO:root:current train perplexity3.3002071380615234
INFO:root:current mean train loss 1515.3089223335294
INFO:root:current train perplexity3.301475763320923
INFO:root:current mean train loss 1516.042925261591
INFO:root:current train perplexity3.302677869796753
INFO:root:current mean train loss 1516.0057106628744
INFO:root:current train perplexity3.303433895111084
INFO:root:current mean train loss 1515.7946563380588
INFO:root:current train perplexity3.3040802478790283

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.39s/it]
INFO:root:final mean train loss: 1515.4932978904674
INFO:root:final train perplexity: 3.304248809814453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.69s/it]
INFO:root:eval mean loss: 2105.6969673751937
INFO:root:eval perplexity: 5.4901957511901855
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it]
INFO:root:eval mean loss: 2626.469659034242
INFO:root:eval perplexity: 8.567699432373047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [13:13:30<21:29:29, 623.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.4825063852163
INFO:root:current train perplexity3.2736926078796387
INFO:root:current mean train loss 1498.8996971889317
INFO:root:current train perplexity3.2808001041412354
INFO:root:current mean train loss 1496.3951764188682
INFO:root:current train perplexity3.283241033554077
INFO:root:current mean train loss 1495.4540915720909
INFO:root:current train perplexity3.2695229053497314
INFO:root:current mean train loss 1496.0202718761934
INFO:root:current train perplexity3.272351026535034
INFO:root:current mean train loss 1495.7943047073286
INFO:root:current train perplexity3.2692108154296875
INFO:root:current mean train loss 1498.7452072828103
INFO:root:current train perplexity3.269604444503784
INFO:root:current mean train loss 1499.1190748829606
INFO:root:current train perplexity3.2721197605133057
INFO:root:current mean train loss 1501.5109368697829
INFO:root:current train perplexity3.2766411304473877
INFO:root:current mean train loss 1502.4683538565844
INFO:root:current train perplexity3.2765328884124756
INFO:root:current mean train loss 1503.085817219903
INFO:root:current train perplexity3.2787702083587646
INFO:root:current mean train loss 1505.053775508698
INFO:root:current train perplexity3.281470537185669
INFO:root:current mean train loss 1505.856053288088
INFO:root:current train perplexity3.2834737300872803
INFO:root:current mean train loss 1507.1678365876055
INFO:root:current train perplexity3.284022808074951
INFO:root:current mean train loss 1508.1858333431578
INFO:root:current train perplexity3.285891532897949
INFO:root:current mean train loss 1509.286406108825
INFO:root:current train perplexity3.2888729572296143
INFO:root:current mean train loss 1510.0114518700796
INFO:root:current train perplexity3.2913105487823486
INFO:root:current mean train loss 1511.3557803666772
INFO:root:current train perplexity3.2925620079040527
INFO:root:current mean train loss 1511.172342365977
INFO:root:current train perplexity3.2931134700775146

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.09s/it]
INFO:root:final mean train loss: 1511.280437767656
INFO:root:final train perplexity: 3.2932891845703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it]
INFO:root:eval mean loss: 2110.047214372784
INFO:root:eval perplexity: 5.509546279907227
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it]
INFO:root:eval mean loss: 2631.902624251995
INFO:root:eval perplexity: 8.605850219726562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [13:23:51<21:17:19, 623.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1448.626937866211
INFO:root:current train perplexity3.199801206588745
INFO:root:current mean train loss 1495.8322979962384
INFO:root:current train perplexity3.241753339767456
INFO:root:current mean train loss 1494.8811252300557
INFO:root:current train perplexity3.2538270950317383
INFO:root:current mean train loss 1492.747882793476
INFO:root:current train perplexity3.2563815116882324
INFO:root:current mean train loss 1493.6355405321308
INFO:root:current train perplexity3.2539761066436768
INFO:root:current mean train loss 1494.7376197154128
INFO:root:current train perplexity3.256274461746216
INFO:root:current mean train loss 1494.4466343929894
INFO:root:current train perplexity3.257925033569336
INFO:root:current mean train loss 1494.2321973897642
INFO:root:current train perplexity3.2579314708709717
INFO:root:current mean train loss 1497.4883844356725
INFO:root:current train perplexity3.2601401805877686
INFO:root:current mean train loss 1498.9628158770995
INFO:root:current train perplexity3.2622973918914795
INFO:root:current mean train loss 1500.775800553579
INFO:root:current train perplexity3.2670257091522217
INFO:root:current mean train loss 1501.550563991285
INFO:root:current train perplexity3.2680466175079346
INFO:root:current mean train loss 1502.2744796449774
INFO:root:current train perplexity3.269623279571533
INFO:root:current mean train loss 1503.7286770788537
INFO:root:current train perplexity3.2720277309417725
INFO:root:current mean train loss 1504.0677924589677
INFO:root:current train perplexity3.273556709289551
INFO:root:current mean train loss 1504.7019616893494
INFO:root:current train perplexity3.2748866081237793
INFO:root:current mean train loss 1505.9260863498669
INFO:root:current train perplexity3.276198625564575
INFO:root:current mean train loss 1505.8144382592945
INFO:root:current train perplexity3.276606321334839
INFO:root:current mean train loss 1505.870320412965
INFO:root:current train perplexity3.2780609130859375
INFO:root:current mean train loss 1506.0888360301403
INFO:root:current train perplexity3.279259204864502

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:06<00:00, 546.82s/it]
INFO:root:final mean train loss: 1505.9858110651967
INFO:root:final train perplexity: 3.2795655727386475
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it]
INFO:root:eval mean loss: 2113.6518251676084
INFO:root:eval perplexity: 5.525631427764893
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.84s/it]
INFO:root:eval mean loss: 2635.028609904837
INFO:root:eval perplexity: 8.627880096435547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [13:34:11<21:05:09, 622.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1489.164609375
INFO:root:current train perplexity3.241672992706299
INFO:root:current mean train loss 1484.323873046875
INFO:root:current train perplexity3.2587709426879883
INFO:root:current mean train loss 1485.360060763889
INFO:root:current train perplexity3.242896556854248
INFO:root:current mean train loss 1488.1947412109375
INFO:root:current train perplexity3.2454423904418945
INFO:root:current mean train loss 1489.8912741268382
INFO:root:current train perplexity3.249763011932373
INFO:root:current mean train loss 1491.6103057570685
INFO:root:current train perplexity3.2499160766601562
INFO:root:current mean train loss 1489.7762423828126
INFO:root:current train perplexity3.2535147666931152
INFO:root:current mean train loss 1490.1053286637932
INFO:root:current train perplexity3.251373767852783
INFO:root:current mean train loss 1492.6093798828124
INFO:root:current train perplexity3.251652956008911
INFO:root:current mean train loss 1493.7154314030827
INFO:root:current train perplexity3.2542495727539062
INFO:root:current mean train loss 1493.2275446598703
INFO:root:current train perplexity3.2561380863189697
INFO:root:current mean train loss 1494.7503140190972
INFO:root:current train perplexity3.2563204765319824
INFO:root:current mean train loss 1496.9719288105866
INFO:root:current train perplexity3.259348154067993
INFO:root:current mean train loss 1497.4396228257665
INFO:root:current train perplexity3.259629487991333
INFO:root:current mean train loss 1498.5888875753838
INFO:root:current train perplexity3.2613918781280518
INFO:root:current mean train loss 1498.9362113377306
INFO:root:current train perplexity3.26127552986145
INFO:root:current mean train loss 1498.9444305889424
INFO:root:current train perplexity3.262434482574463
INFO:root:current mean train loss 1499.6909294327445
INFO:root:current train perplexity3.2624032497406006
INFO:root:current mean train loss 1500.906267390839
INFO:root:current train perplexity3.2642292976379395
INFO:root:current mean train loss 1501.5264738484173
INFO:root:current train perplexity3.266514301300049

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.30s/it]
INFO:root:final mean train loss: 1501.1657533554255
INFO:root:final train perplexity: 3.267122983932495
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.40s/it]
INFO:root:eval mean loss: 2114.991629958998
INFO:root:eval perplexity: 5.531621932983398
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.23s/it]
INFO:root:eval mean loss: 2640.8871724013743
INFO:root:eval perplexity: 8.669317245483398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [13:44:43<21:00:48, 625.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1500.1484898158483
INFO:root:current train perplexity3.2512218952178955
INFO:root:current mean train loss 1501.1668881698392
INFO:root:current train perplexity3.244379758834839
INFO:root:current mean train loss 1498.5926856679364
INFO:root:current train perplexity3.2448222637176514
INFO:root:current mean train loss 1493.8076125474004
INFO:root:current train perplexity3.2395293712615967
INFO:root:current mean train loss 1492.51948623312
INFO:root:current train perplexity3.2442328929901123
INFO:root:current mean train loss 1490.4820739070428
INFO:root:current train perplexity3.238743543624878
INFO:root:current mean train loss 1491.915145160996
INFO:root:current train perplexity3.239469528198242
INFO:root:current mean train loss 1492.7168514940618
INFO:root:current train perplexity3.241816759109497
INFO:root:current mean train loss 1491.637181225412
INFO:root:current train perplexity3.2419071197509766
INFO:root:current mean train loss 1492.6131669548665
INFO:root:current train perplexity3.2461602687835693
INFO:root:current mean train loss 1494.2979964770648
INFO:root:current train perplexity3.2475838661193848
INFO:root:current mean train loss 1492.8912312896782
INFO:root:current train perplexity3.247478485107422
INFO:root:current mean train loss 1493.4842828084113
INFO:root:current train perplexity3.249321699142456
INFO:root:current mean train loss 1493.5089473354835
INFO:root:current train perplexity3.249919891357422
INFO:root:current mean train loss 1494.4465410758983
INFO:root:current train perplexity3.2523014545440674
INFO:root:current mean train loss 1495.4952228709417
INFO:root:current train perplexity3.253173828125
INFO:root:current mean train loss 1496.0089121736069
INFO:root:current train perplexity3.2535488605499268
INFO:root:current mean train loss 1495.9158050502072
INFO:root:current train perplexity3.2540066242218018
INFO:root:current mean train loss 1496.8016707992967
INFO:root:current train perplexity3.2547502517700195
INFO:root:current mean train loss 1496.801216856213
INFO:root:current train perplexity3.255387544631958

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.54s/it]
INFO:root:final mean train loss: 1496.8213493281762
INFO:root:final train perplexity: 3.2559475898742676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.52s/it]
INFO:root:eval mean loss: 2118.42083696947
INFO:root:eval perplexity: 5.546984672546387
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.55s/it]
INFO:root:eval mean loss: 2644.5645682693375
INFO:root:eval perplexity: 8.695428848266602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [13:55:09<20:50:50, 625.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1478.0117166810116
INFO:root:current train perplexity3.196732521057129
INFO:root:current mean train loss 1475.2074266349744
INFO:root:current train perplexity3.1971869468688965
INFO:root:current mean train loss 1476.6054654508023
INFO:root:current train perplexity3.209390878677368
INFO:root:current mean train loss 1482.1115168409426
INFO:root:current train perplexity3.2164812088012695
INFO:root:current mean train loss 1482.4013613366353
INFO:root:current train perplexity3.219482660293579
INFO:root:current mean train loss 1483.1611450413686
INFO:root:current train perplexity3.215459108352661
INFO:root:current mean train loss 1484.5007327923463
INFO:root:current train perplexity3.2181482315063477
INFO:root:current mean train loss 1484.443258855968
INFO:root:current train perplexity3.2205374240875244
INFO:root:current mean train loss 1483.9328846337494
INFO:root:current train perplexity3.222656726837158
INFO:root:current mean train loss 1485.6209438033595
INFO:root:current train perplexity3.2264809608459473
INFO:root:current mean train loss 1486.0669215042935
INFO:root:current train perplexity3.227203130722046
INFO:root:current mean train loss 1486.6108561689427
INFO:root:current train perplexity3.2287912368774414
INFO:root:current mean train loss 1488.278380426554
INFO:root:current train perplexity3.231334924697876
INFO:root:current mean train loss 1489.1785035347395
INFO:root:current train perplexity3.2334632873535156
INFO:root:current mean train loss 1490.0258379930335
INFO:root:current train perplexity3.2350382804870605
INFO:root:current mean train loss 1490.308250167876
INFO:root:current train perplexity3.2373476028442383
INFO:root:current mean train loss 1491.5013772091856
INFO:root:current train perplexity3.23899245262146
INFO:root:current mean train loss 1491.8298503621997
INFO:root:current train perplexity3.2415261268615723
INFO:root:current mean train loss 1491.6235002883984
INFO:root:current train perplexity3.2419676780700684
INFO:root:current mean train loss 1492.251716274945
INFO:root:current train perplexity3.244025230407715

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.27s/it]
INFO:root:final mean train loss: 1492.3453047497972
INFO:root:final train perplexity: 3.244473934173584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.16s/it]
INFO:root:eval mean loss: 2119.611338513963
INFO:root:eval perplexity: 5.5523271560668945
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it]
INFO:root:eval mean loss: 2645.98331056419
INFO:root:eval perplexity: 8.705524444580078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [14:05:38<20:42:27, 626.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1473.8027648925781
INFO:root:current train perplexity3.210362434387207
INFO:root:current mean train loss 1478.8491793545809
INFO:root:current train perplexity3.207279682159424
INFO:root:current mean train loss 1475.1499762051346
INFO:root:current train perplexity3.2010552883148193
INFO:root:current mean train loss 1476.349846048558
INFO:root:current train perplexity3.2053911685943604
INFO:root:current mean train loss 1478.0444148728827
INFO:root:current train perplexity3.2036728858947754
INFO:root:current mean train loss 1481.3487741682266
INFO:root:current train perplexity3.207230567932129
INFO:root:current mean train loss 1481.8895530926404
INFO:root:current train perplexity3.2105817794799805
INFO:root:current mean train loss 1482.573181466958
INFO:root:current train perplexity3.2121901512145996
INFO:root:current mean train loss 1483.3555890087669
INFO:root:current train perplexity3.21454119682312
INFO:root:current mean train loss 1484.3466329105565
INFO:root:current train perplexity3.2162139415740967
INFO:root:current mean train loss 1484.388254272007
INFO:root:current train perplexity3.218611240386963
INFO:root:current mean train loss 1484.9862042900656
INFO:root:current train perplexity3.220696449279785
INFO:root:current mean train loss 1486.6411437031618
INFO:root:current train perplexity3.224621534347534
INFO:root:current mean train loss 1486.2430956640908
INFO:root:current train perplexity3.2243077754974365
INFO:root:current mean train loss 1485.2901674182758
INFO:root:current train perplexity3.2247352600097656
INFO:root:current mean train loss 1485.9751163230935
INFO:root:current train perplexity3.2265217304229736
INFO:root:current mean train loss 1486.1310666614615
INFO:root:current train perplexity3.227860927581787
INFO:root:current mean train loss 1486.1245636811127
INFO:root:current train perplexity3.2287662029266357
INFO:root:current mean train loss 1487.2059838919242
INFO:root:current train perplexity3.230902671813965
INFO:root:current mean train loss 1487.9908264407263
INFO:root:current train perplexity3.232511520385742

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.83s/it]
INFO:root:final mean train loss: 1487.645306485744
INFO:root:final train perplexity: 3.2324697971343994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.13s/it]
INFO:root:eval mean loss: 2126.6167273555243
INFO:root:eval perplexity: 5.583874225616455
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it]
INFO:root:eval mean loss: 2654.285832398327
INFO:root:eval perplexity: 8.764837265014648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [14:15:59<20:28:53, 624.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1466.2053931451612
INFO:root:current train perplexity3.1781013011932373
INFO:root:current mean train loss 1470.353547881922
INFO:root:current train perplexity3.1926350593566895
INFO:root:current mean train loss 1472.4760221409717
INFO:root:current train perplexity3.1969099044799805
INFO:root:current mean train loss 1474.8894207592837
INFO:root:current train perplexity3.1974692344665527
INFO:root:current mean train loss 1475.886829925599
INFO:root:current train perplexity3.198726177215576
INFO:root:current mean train loss 1480.300313553963
INFO:root:current train perplexity3.2077677249908447
INFO:root:current mean train loss 1478.0567394438244
INFO:root:current train perplexity3.2084438800811768
INFO:root:current mean train loss 1478.7282353096923
INFO:root:current train perplexity3.2109415531158447
INFO:root:current mean train loss 1478.8152004468348
INFO:root:current train perplexity3.209750175476074
INFO:root:current mean train loss 1480.0553848865889
INFO:root:current train perplexity3.2107667922973633
INFO:root:current mean train loss 1481.1519501765497
INFO:root:current train perplexity3.21242356300354
INFO:root:current mean train loss 1481.1621344439243
INFO:root:current train perplexity3.2141635417938232
INFO:root:current mean train loss 1480.2099330869646
INFO:root:current train perplexity3.2133429050445557
INFO:root:current mean train loss 1480.3243466916053
INFO:root:current train perplexity3.214063882827759
INFO:root:current mean train loss 1480.3637379712095
INFO:root:current train perplexity3.2135918140411377
INFO:root:current mean train loss 1481.3899412376657
INFO:root:current train perplexity3.213386058807373
INFO:root:current mean train loss 1481.3214951327618
INFO:root:current train perplexity3.2152178287506104
INFO:root:current mean train loss 1481.5112983461072
INFO:root:current train perplexity3.2162044048309326
INFO:root:current mean train loss 1482.184872102813
INFO:root:current train perplexity3.2171578407287598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.18s/it]
INFO:root:final mean train loss: 1481.9875921960197
INFO:root:final train perplexity: 3.218079090118408
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2128.3839314778647
INFO:root:eval perplexity: 5.591859817504883
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.23s/it]
INFO:root:eval mean loss: 2659.346260666002
INFO:root:eval perplexity: 8.801185607910156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [14:26:33<20:23:44, 627.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1443.4132568359375
INFO:root:current train perplexity3.149664878845215
INFO:root:current mean train loss 1462.288447709517
INFO:root:current train perplexity3.171776533126831
INFO:root:current mean train loss 1465.988456217448
INFO:root:current train perplexity3.174330949783325
INFO:root:current mean train loss 1466.6047804309476
INFO:root:current train perplexity3.173447847366333
INFO:root:current mean train loss 1464.4458942692454
INFO:root:current train perplexity3.1720969676971436
INFO:root:current mean train loss 1466.0725444718903
INFO:root:current train perplexity3.175750732421875
INFO:root:current mean train loss 1465.8229864401897
INFO:root:current train perplexity3.179851531982422
INFO:root:current mean train loss 1466.2877644283672
INFO:root:current train perplexity3.1833150386810303
INFO:root:current mean train loss 1467.0658120237752
INFO:root:current train perplexity3.1861343383789062
INFO:root:current mean train loss 1467.8277133145175
INFO:root:current train perplexity3.188762903213501
INFO:root:current mean train loss 1470.215398746906
INFO:root:current train perplexity3.190124750137329
INFO:root:current mean train loss 1471.1823472031601
INFO:root:current train perplexity3.192718029022217
INFO:root:current mean train loss 1472.230412456418
INFO:root:current train perplexity3.1942965984344482
INFO:root:current mean train loss 1472.5390276493918
INFO:root:current train perplexity3.19647479057312
INFO:root:current mean train loss 1473.4366522606383
INFO:root:current train perplexity3.1994292736053467
INFO:root:current mean train loss 1474.0995091318296
INFO:root:current train perplexity3.1998305320739746
INFO:root:current mean train loss 1474.9214369874562
INFO:root:current train perplexity3.201662063598633
INFO:root:current mean train loss 1474.7659252501371
INFO:root:current train perplexity3.20255446434021
INFO:root:current mean train loss 1476.521031635769
INFO:root:current train perplexity3.205353021621704
INFO:root:current mean train loss 1477.6690794620215
INFO:root:current train perplexity3.2064602375030518

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.50s/it]
INFO:root:final mean train loss: 1478.4024712066728
INFO:root:final train perplexity: 3.2089931964874268
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it]
INFO:root:eval mean loss: 2130.042126378269
INFO:root:eval perplexity: 5.599364280700684
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.61s/it]
INFO:root:eval mean loss: 2659.169019766733
INFO:root:eval perplexity: 8.799907684326172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [14:36:53<20:09:11, 625.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1448.4637134693287
INFO:root:current train perplexity3.1628386974334717
INFO:root:current mean train loss 1460.6614442359744
INFO:root:current train perplexity3.1650543212890625
INFO:root:current mean train loss 1467.72417164269
INFO:root:current train perplexity3.173640012741089
INFO:root:current mean train loss 1464.0541335172975
INFO:root:current train perplexity3.1753194332122803
INFO:root:current mean train loss 1465.3922933895637
INFO:root:current train perplexity3.175873041152954
INFO:root:current mean train loss 1465.942879432519
INFO:root:current train perplexity3.174670696258545
INFO:root:current mean train loss 1467.0344662704347
INFO:root:current train perplexity3.176572799682617
INFO:root:current mean train loss 1469.2315569724144
INFO:root:current train perplexity3.1800765991210938
INFO:root:current mean train loss 1469.8219357251833
INFO:root:current train perplexity3.1818771362304688
INFO:root:current mean train loss 1468.4174516301325
INFO:root:current train perplexity3.1794395446777344
INFO:root:current mean train loss 1468.80610206552
INFO:root:current train perplexity3.1804277896881104
INFO:root:current mean train loss 1471.0719515132819
INFO:root:current train perplexity3.1830317974090576
INFO:root:current mean train loss 1471.3801848544467
INFO:root:current train perplexity3.1853153705596924
INFO:root:current mean train loss 1472.2728174895205
INFO:root:current train perplexity3.18816876411438
INFO:root:current mean train loss 1472.9697756643636
INFO:root:current train perplexity3.1893506050109863
INFO:root:current mean train loss 1473.0234815476374
INFO:root:current train perplexity3.191349506378174
INFO:root:current mean train loss 1472.470199237957
INFO:root:current train perplexity3.190706253051758
INFO:root:current mean train loss 1472.9045346541147
INFO:root:current train perplexity3.1930038928985596
INFO:root:current mean train loss 1473.6205609702124
INFO:root:current train perplexity3.1953959465026855
INFO:root:current mean train loss 1474.3786297388874
INFO:root:current train perplexity3.197087526321411

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.48s/it]
INFO:root:final mean train loss: 1473.9928795087837
INFO:root:final train perplexity: 3.197852611541748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it]
INFO:root:eval mean loss: 2134.822363887273
INFO:root:eval perplexity: 5.621052265167236
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.67s/it]
INFO:root:eval mean loss: 2665.910340221216
INFO:root:eval perplexity: 8.848562240600586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [14:47:28<20:04:08, 628.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1453.2038047096946
INFO:root:current train perplexity3.150824546813965
INFO:root:current mean train loss 1460.400127834744
INFO:root:current train perplexity3.1736581325531006
INFO:root:current mean train loss 1464.2548352851243
INFO:root:current train perplexity3.1823816299438477
INFO:root:current mean train loss 1464.1031302518622
INFO:root:current train perplexity3.1750569343566895
INFO:root:current mean train loss 1465.7320270710163
INFO:root:current train perplexity3.177210807800293
INFO:root:current mean train loss 1465.2217279322008
INFO:root:current train perplexity3.174901008605957
INFO:root:current mean train loss 1466.8598657454022
INFO:root:current train perplexity3.1763720512390137
INFO:root:current mean train loss 1469.44504785025
INFO:root:current train perplexity3.1797449588775635
INFO:root:current mean train loss 1469.7487875409602
INFO:root:current train perplexity3.1793267726898193
INFO:root:current mean train loss 1468.7220421483962
INFO:root:current train perplexity3.176405429840088
INFO:root:current mean train loss 1467.9607291386046
INFO:root:current train perplexity3.176628828048706
INFO:root:current mean train loss 1467.725249497207
INFO:root:current train perplexity3.177493095397949
INFO:root:current mean train loss 1468.8359959838474
INFO:root:current train perplexity3.1789815425872803
INFO:root:current mean train loss 1469.161502384004
INFO:root:current train perplexity3.1798207759857178
INFO:root:current mean train loss 1470.0329974483584
INFO:root:current train perplexity3.181654930114746
INFO:root:current mean train loss 1469.9522802323258
INFO:root:current train perplexity3.182004451751709
INFO:root:current mean train loss 1469.7127878602114
INFO:root:current train perplexity3.1828107833862305
INFO:root:current mean train loss 1469.57588167803
INFO:root:current train perplexity3.1842453479766846
INFO:root:current mean train loss 1469.3991091515134
INFO:root:current train perplexity3.185157060623169
INFO:root:current mean train loss 1469.6049531536337
INFO:root:current train perplexity3.186427593231201

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.05s/it]
INFO:root:final mean train loss: 1469.50257012487
INFO:root:final train perplexity: 3.1865475177764893
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.86s/it]
INFO:root:eval mean loss: 2139.344607955175
INFO:root:eval perplexity: 5.641648769378662
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it]
INFO:root:eval mean loss: 2674.749706511802
INFO:root:eval perplexity: 8.912758827209473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [14:57:53<19:51:56, 627.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1446.3669633709017
INFO:root:current train perplexity3.1249425411224365
INFO:root:current mean train loss 1448.1599143839771
INFO:root:current train perplexity3.142507314682007
INFO:root:current mean train loss 1451.3848226846862
INFO:root:current train perplexity3.149846315383911
INFO:root:current mean train loss 1452.9126605512033
INFO:root:current train perplexity3.151834011077881
INFO:root:current mean train loss 1453.4820829379066
INFO:root:current train perplexity3.152998208999634
INFO:root:current mean train loss 1453.7634784338095
INFO:root:current train perplexity3.157402992248535
INFO:root:current mean train loss 1454.9940035959958
INFO:root:current train perplexity3.156358480453491
INFO:root:current mean train loss 1457.721174723842
INFO:root:current train perplexity3.1623384952545166
INFO:root:current mean train loss 1458.4867415478004
INFO:root:current train perplexity3.163753032684326
INFO:root:current mean train loss 1458.4110550736536
INFO:root:current train perplexity3.164424180984497
INFO:root:current mean train loss 1458.9516598110936
INFO:root:current train perplexity3.1647534370422363
INFO:root:current mean train loss 1460.0829291659938
INFO:root:current train perplexity3.1655564308166504
INFO:root:current mean train loss 1460.4247053662148
INFO:root:current train perplexity3.166668653488159
INFO:root:current mean train loss 1461.7005678915434
INFO:root:current train perplexity3.167266845703125
INFO:root:current mean train loss 1462.7192515661095
INFO:root:current train perplexity3.1689443588256836
INFO:root:current mean train loss 1462.5606786421215
INFO:root:current train perplexity3.1676993370056152
INFO:root:current mean train loss 1462.4307063204349
INFO:root:current train perplexity3.1684634685516357
INFO:root:current mean train loss 1463.190732610422
INFO:root:current train perplexity3.1704187393188477
INFO:root:current mean train loss 1464.3864994038822
INFO:root:current train perplexity3.174595832824707
INFO:root:current mean train loss 1465.686976174564
INFO:root:current train perplexity3.1756129264831543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.55s/it]
INFO:root:final mean train loss: 1465.10192514055
INFO:root:final train perplexity: 3.1755075454711914
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.54s/it]
INFO:root:eval mean loss: 2140.420156492409
INFO:root:eval perplexity: 5.6465582847595215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.61s/it]
INFO:root:eval mean loss: 2675.6704837447364
INFO:root:eval perplexity: 8.919471740722656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [15:08:38<19:51:25, 632.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1441.676965958033
INFO:root:current train perplexity3.1108031272888184
INFO:root:current mean train loss 1457.9287321969364
INFO:root:current train perplexity3.141511917114258
INFO:root:current mean train loss 1452.042087033498
INFO:root:current train perplexity3.1343889236450195
INFO:root:current mean train loss 1453.5591847253224
INFO:root:current train perplexity3.136115312576294
INFO:root:current mean train loss 1454.4441362325117
INFO:root:current train perplexity3.141101121902466
INFO:root:current mean train loss 1454.4350547922525
INFO:root:current train perplexity3.140697717666626
INFO:root:current mean train loss 1455.1424574950452
INFO:root:current train perplexity3.1427738666534424
INFO:root:current mean train loss 1455.7513854509762
INFO:root:current train perplexity3.1451327800750732
INFO:root:current mean train loss 1455.5967717268472
INFO:root:current train perplexity3.14937162399292
INFO:root:current mean train loss 1457.108791234303
INFO:root:current train perplexity3.1537907123565674
INFO:root:current mean train loss 1456.3889574606476
INFO:root:current train perplexity3.1547458171844482
INFO:root:current mean train loss 1457.923533000849
INFO:root:current train perplexity3.1556906700134277
INFO:root:current mean train loss 1458.1737379572573
INFO:root:current train perplexity3.157182216644287
INFO:root:current mean train loss 1458.6884700957853
INFO:root:current train perplexity3.157780170440674
INFO:root:current mean train loss 1459.3922702236976
INFO:root:current train perplexity3.158048152923584
INFO:root:current mean train loss 1458.9877457806065
INFO:root:current train perplexity3.157902479171753
INFO:root:current mean train loss 1460.0788377073013
INFO:root:current train perplexity3.1603403091430664
INFO:root:current mean train loss 1460.2241287832185
INFO:root:current train perplexity3.160970687866211
INFO:root:current mean train loss 1460.5416503516249
INFO:root:current train perplexity3.1627025604248047
INFO:root:current mean train loss 1460.86004194331
INFO:root:current train perplexity3.1639840602874756

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.87s/it]
INFO:root:final mean train loss: 1460.5602620442708
INFO:root:final train perplexity: 3.164153814315796
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it]
INFO:root:eval mean loss: 2141.8218751731492
INFO:root:eval perplexity: 5.652962684631348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:39<00:00, 39.68s/it]
INFO:root:eval mean loss: 2675.2980216817655
INFO:root:eval perplexity: 8.916756629943848
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [15:19:05<19:37:22, 630.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1448.4385716488487
INFO:root:current train perplexity3.133230447769165
INFO:root:current mean train loss 1443.3161840194311
INFO:root:current train perplexity3.122776985168457
INFO:root:current mean train loss 1446.2562677932997
INFO:root:current train perplexity3.1250600814819336
INFO:root:current mean train loss 1442.9679245574564
INFO:root:current train perplexity3.12605357170105
INFO:root:current mean train loss 1446.3472197561553
INFO:root:current train perplexity3.1296792030334473
INFO:root:current mean train loss 1447.9174098936448
INFO:root:current train perplexity3.131946325302124
INFO:root:current mean train loss 1449.1003052636015
INFO:root:current train perplexity3.1361911296844482
INFO:root:current mean train loss 1450.0050968578028
INFO:root:current train perplexity3.1371185779571533
INFO:root:current mean train loss 1452.5532189736819
INFO:root:current train perplexity3.1378328800201416
INFO:root:current mean train loss 1452.2227813874058
INFO:root:current train perplexity3.1413588523864746
INFO:root:current mean train loss 1453.6421643122146
INFO:root:current train perplexity3.1425094604492188
INFO:root:current mean train loss 1453.7065938398928
INFO:root:current train perplexity3.1450750827789307
INFO:root:current mean train loss 1453.674919782366
INFO:root:current train perplexity3.144493341445923
INFO:root:current mean train loss 1454.689315828713
INFO:root:current train perplexity3.14559268951416
INFO:root:current mean train loss 1454.623484205163
INFO:root:current train perplexity3.1465890407562256
INFO:root:current mean train loss 1454.4102205378135
INFO:root:current train perplexity3.14841628074646
INFO:root:current mean train loss 1454.8005826246774
INFO:root:current train perplexity3.1484670639038086
INFO:root:current mean train loss 1454.9223586568594
INFO:root:current train perplexity3.1491761207580566
INFO:root:current mean train loss 1455.307921558068
INFO:root:current train perplexity3.150505781173706

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.24s/it]
INFO:root:final mean train loss: 1455.7081197564553
INFO:root:final train perplexity: 3.152068853378296
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:45<00:00, 45.32s/it]
INFO:root:eval mean loss: 2149.2722445007757
INFO:root:eval perplexity: 5.687127113342285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.49s/it]
INFO:root:eval mean loss: 2685.874493538065
INFO:root:eval perplexity: 8.994216918945312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [15:29:37<19:27:46, 631.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1477.0469462076824
INFO:root:current train perplexity3.11215877532959
INFO:root:current mean train loss 1437.6938291277204
INFO:root:current train perplexity3.095216989517212
INFO:root:current mean train loss 1439.7100162146226
INFO:root:current train perplexity3.100262403488159
INFO:root:current mean train loss 1439.863780486278
INFO:root:current train perplexity3.108891487121582
INFO:root:current mean train loss 1440.7340547135732
INFO:root:current train perplexity3.1134252548217773
INFO:root:current mean train loss 1443.4000499248505
INFO:root:current train perplexity3.1203689575195312
INFO:root:current mean train loss 1445.76944189134
INFO:root:current train perplexity3.1223080158233643
INFO:root:current mean train loss 1444.8398675811425
INFO:root:current train perplexity3.1228256225585938
INFO:root:current mean train loss 1444.7555868214574
INFO:root:current train perplexity3.1270647048950195
INFO:root:current mean train loss 1446.2720999466746
INFO:root:current train perplexity3.130995512008667
INFO:root:current mean train loss 1447.343979183393
INFO:root:current train perplexity3.131681442260742
INFO:root:current mean train loss 1447.9365842531054
INFO:root:current train perplexity3.131326198577881
INFO:root:current mean train loss 1448.9182008044554
INFO:root:current train perplexity3.13277006149292
INFO:root:current mean train loss 1449.2334890598204
INFO:root:current train perplexity3.1337411403656006
INFO:root:current mean train loss 1449.8132806621259
INFO:root:current train perplexity3.1351752281188965
INFO:root:current mean train loss 1450.4177617471685
INFO:root:current train perplexity3.1369175910949707
INFO:root:current mean train loss 1451.0428786360596
INFO:root:current train perplexity3.139859914779663
INFO:root:current mean train loss 1451.4459645636728
INFO:root:current train perplexity3.1410675048828125
INFO:root:current mean train loss 1451.5639368861453
INFO:root:current train perplexity3.140681505203247
INFO:root:current mean train loss 1451.7491763446121
INFO:root:current train perplexity3.141608953475952

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.14s/it]
INFO:root:final mean train loss: 1451.4962207142055
INFO:root:final train perplexity: 3.141615390777588
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.70s/it]
INFO:root:eval mean loss: 2149.308057419797
INFO:root:eval perplexity: 5.687291622161865
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.65s/it]
INFO:root:eval mean loss: 2687.4821924520725
INFO:root:eval perplexity: 9.006053924560547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [15:40:02<19:13:40, 629.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1420.1431505926723
INFO:root:current train perplexity3.1195342540740967
INFO:root:current mean train loss 1435.2665145046028
INFO:root:current train perplexity3.1093175411224365
INFO:root:current mean train loss 1437.7264159090134
INFO:root:current train perplexity3.1202392578125
INFO:root:current mean train loss 1436.4187931884023
INFO:root:current train perplexity3.1174893379211426
INFO:root:current mean train loss 1440.1882130727345
INFO:root:current train perplexity3.1168575286865234
INFO:root:current mean train loss 1440.215676089541
INFO:root:current train perplexity3.110194206237793
INFO:root:current mean train loss 1440.986806508657
INFO:root:current train perplexity3.1117453575134277
INFO:root:current mean train loss 1441.1815286016267
INFO:root:current train perplexity3.1137001514434814
INFO:root:current mean train loss 1441.101537172987
INFO:root:current train perplexity3.113391876220703
INFO:root:current mean train loss 1441.9963514247931
INFO:root:current train perplexity3.114649772644043
INFO:root:current mean train loss 1442.949982490206
INFO:root:current train perplexity3.1179635524749756
INFO:root:current mean train loss 1443.0954443878363
INFO:root:current train perplexity3.1199147701263428
INFO:root:current mean train loss 1444.2349408540035
INFO:root:current train perplexity3.122692346572876
INFO:root:current mean train loss 1444.5123855900817
INFO:root:current train perplexity3.124434471130371
INFO:root:current mean train loss 1445.6279201200578
INFO:root:current train perplexity3.126934289932251
INFO:root:current mean train loss 1446.4587238678516
INFO:root:current train perplexity3.1279101371765137
INFO:root:current mean train loss 1447.3639434570912
INFO:root:current train perplexity3.1287896633148193
INFO:root:current mean train loss 1447.3671757801203
INFO:root:current train perplexity3.128810405731201
INFO:root:current mean train loss 1447.2322149094236
INFO:root:current train perplexity3.1297550201416016
INFO:root:current mean train loss 1447.871651957479
INFO:root:current train perplexity3.1316299438476562

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.84s/it]
INFO:root:final mean train loss: 1447.5793455184498
INFO:root:final train perplexity: 3.1319260597229004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.58s/it]
INFO:root:eval mean loss: 2151.4451631586603
INFO:root:eval perplexity: 5.697129726409912
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 2689.648614978114
INFO:root:eval perplexity: 9.022019386291504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [15:50:39<19:07:43, 631.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1415.5422973632812
INFO:root:current train perplexity3.06063175201416
INFO:root:current mean train loss 1421.9363252822668
INFO:root:current train perplexity3.082561492919922
INFO:root:current mean train loss 1423.8727332634655
INFO:root:current train perplexity3.0797550678253174
INFO:root:current mean train loss 1424.5596832098988
INFO:root:current train perplexity3.0850539207458496
INFO:root:current mean train loss 1428.047277613071
INFO:root:current train perplexity3.088266611099243
INFO:root:current mean train loss 1431.493392553085
INFO:root:current train perplexity3.09881329536438
INFO:root:current mean train loss 1433.2487422600618
INFO:root:current train perplexity3.100271224975586
INFO:root:current mean train loss 1435.9650687455492
INFO:root:current train perplexity3.104376792907715
INFO:root:current mean train loss 1435.45226516904
INFO:root:current train perplexity3.1054861545562744
INFO:root:current mean train loss 1435.7070535736407
INFO:root:current train perplexity3.1066627502441406
INFO:root:current mean train loss 1438.0981141887248
INFO:root:current train perplexity3.108645439147949
INFO:root:current mean train loss 1437.8714909578493
INFO:root:current train perplexity3.1096296310424805
INFO:root:current mean train loss 1438.5060167144236
INFO:root:current train perplexity3.1099085807800293
INFO:root:current mean train loss 1439.4320203489215
INFO:root:current train perplexity3.1113345623016357
INFO:root:current mean train loss 1440.5722838595696
INFO:root:current train perplexity3.1128058433532715
INFO:root:current mean train loss 1440.980826591304
INFO:root:current train perplexity3.1143012046813965
INFO:root:current mean train loss 1441.7866144191885
INFO:root:current train perplexity3.1163840293884277
INFO:root:current mean train loss 1441.844976156438
INFO:root:current train perplexity3.117274522781372
INFO:root:current mean train loss 1442.0382166043134
INFO:root:current train perplexity3.1181397438049316
INFO:root:current mean train loss 1443.091713382921
INFO:root:current train perplexity3.120422124862671

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.62s/it]
INFO:root:final mean train loss: 1443.124428553348
INFO:root:final train perplexity: 3.120941400527954
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.56s/it]
INFO:root:eval mean loss: 2157.7007965737203
INFO:root:eval perplexity: 5.726025581359863
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.22s/it]
INFO:root:eval mean loss: 2696.420955576795
INFO:root:eval perplexity: 9.072132110595703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [16:01:10<18:56:36, 631.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1419.5115443638392
INFO:root:current train perplexity3.0861895084381104
INFO:root:current mean train loss 1425.5560362646186
INFO:root:current train perplexity3.1002984046936035
INFO:root:current mean train loss 1430.09590270764
INFO:root:current train perplexity3.1008851528167725
INFO:root:current mean train loss 1431.8376020951705
INFO:root:current train perplexity3.102285146713257
INFO:root:current mean train loss 1430.5851676397137
INFO:root:current train perplexity3.0964927673339844
INFO:root:current mean train loss 1430.0998105850356
INFO:root:current train perplexity3.0963332653045654
INFO:root:current mean train loss 1432.1124147901348
INFO:root:current train perplexity3.1002416610717773
INFO:root:current mean train loss 1433.177633103037
INFO:root:current train perplexity3.1002490520477295
INFO:root:current mean train loss 1434.308254414334
INFO:root:current train perplexity3.1011829376220703
INFO:root:current mean train loss 1435.0086453161507
INFO:root:current train perplexity3.1012771129608154
INFO:root:current mean train loss 1436.8129515338371
INFO:root:current train perplexity3.102982759475708
INFO:root:current mean train loss 1437.6934915216239
INFO:root:current train perplexity3.104863166809082
INFO:root:current mean train loss 1438.4679926421468
INFO:root:current train perplexity3.106806755065918
INFO:root:current mean train loss 1439.1036025877831
INFO:root:current train perplexity3.1076242923736572
INFO:root:current mean train loss 1438.9222734181424
INFO:root:current train perplexity3.1087749004364014
INFO:root:current mean train loss 1439.2552076304332
INFO:root:current train perplexity3.1089870929718018
INFO:root:current mean train loss 1439.214495082541
INFO:root:current train perplexity3.1106958389282227
INFO:root:current mean train loss 1439.7336161976434
INFO:root:current train perplexity3.1118924617767334
INFO:root:current mean train loss 1440.2805511916893
INFO:root:current train perplexity3.112826347351074
INFO:root:current mean train loss 1440.4736054508405
INFO:root:current train perplexity3.1131300926208496

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.81s/it]
INFO:root:final mean train loss: 1440.1135875646116
INFO:root:final train perplexity: 3.113539457321167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it]
INFO:root:eval mean loss: 2158.9355087821364
INFO:root:eval perplexity: 5.73174524307251
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it]
INFO:root:eval mean loss: 2698.8748428669383
INFO:root:eval perplexity: 9.090353965759277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [16:11:34<18:42:08, 629.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1417.0339447021483
INFO:root:current train perplexity3.0604782104492188
INFO:root:current mean train loss 1426.7867397732205
INFO:root:current train perplexity3.0703022480010986
INFO:root:current mean train loss 1428.223772757394
INFO:root:current train perplexity3.0764899253845215
INFO:root:current mean train loss 1429.077089008532
INFO:root:current train perplexity3.07704496383667
INFO:root:current mean train loss 1428.0428764343262
INFO:root:current train perplexity3.080929756164551
INFO:root:current mean train loss 1428.7468491126751
INFO:root:current train perplexity3.0836539268493652
INFO:root:current mean train loss 1429.7766452564913
INFO:root:current train perplexity3.0850653648376465
INFO:root:current mean train loss 1430.0020363832132
INFO:root:current train perplexity3.085075616836548
INFO:root:current mean train loss 1429.6532270951705
INFO:root:current train perplexity3.086944580078125
INFO:root:current mean train loss 1431.3620533223054
INFO:root:current train perplexity3.0898308753967285
INFO:root:current mean train loss 1431.7493943956163
INFO:root:current train perplexity3.091519832611084
INFO:root:current mean train loss 1432.6069610078457
INFO:root:current train perplexity3.0930423736572266
INFO:root:current mean train loss 1432.7561939239501
INFO:root:current train perplexity3.0943515300750732
INFO:root:current mean train loss 1433.2038705134737
INFO:root:current train perplexity3.094177007675171
INFO:root:current mean train loss 1433.5698558910474
INFO:root:current train perplexity3.096489191055298
INFO:root:current mean train loss 1434.6243873306469
INFO:root:current train perplexity3.0984079837799072
INFO:root:current mean train loss 1435.207190159389
INFO:root:current train perplexity3.099881410598755
INFO:root:current mean train loss 1435.628421123376
INFO:root:current train perplexity3.100992441177368
INFO:root:current mean train loss 1435.6747696247508
INFO:root:current train perplexity3.1013481616973877
INFO:root:current mean train loss 1435.8513165714764
INFO:root:current train perplexity3.1021676063537598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.61s/it]
INFO:root:final mean train loss: 1435.5259091006465
INFO:root:final train perplexity: 3.102294683456421
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2163.4038938698195
INFO:root:eval perplexity: 5.752497673034668
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it]
INFO:root:eval mean loss: 2704.032630866301
INFO:root:eval perplexity: 9.12878131866455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [16:21:58<18:28:43, 627.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1419.4153184399163
INFO:root:current train perplexity3.075840950012207
INFO:root:current mean train loss 1421.9793341777047
INFO:root:current train perplexity3.0760135650634766
INFO:root:current mean train loss 1420.0894717849064
INFO:root:current train perplexity3.0737805366516113
INFO:root:current mean train loss 1419.3693401807502
INFO:root:current train perplexity3.0695018768310547
INFO:root:current mean train loss 1418.7919138365348
INFO:root:current train perplexity3.072425127029419
INFO:root:current mean train loss 1419.832822560066
INFO:root:current train perplexity3.0751280784606934
INFO:root:current mean train loss 1421.0688886382488
INFO:root:current train perplexity3.0767550468444824
INFO:root:current mean train loss 1424.7905478674913
INFO:root:current train perplexity3.081690788269043
INFO:root:current mean train loss 1426.9040675678912
INFO:root:current train perplexity3.084266424179077
INFO:root:current mean train loss 1427.1721365267679
INFO:root:current train perplexity3.084886312484741
INFO:root:current mean train loss 1428.1104024158572
INFO:root:current train perplexity3.0854904651641846
INFO:root:current mean train loss 1428.9245509607351
INFO:root:current train perplexity3.086142063140869
INFO:root:current mean train loss 1429.3717899366627
INFO:root:current train perplexity3.087141275405884
INFO:root:current mean train loss 1429.5303296230484
INFO:root:current train perplexity3.0859925746917725
INFO:root:current mean train loss 1429.862770544344
INFO:root:current train perplexity3.0860705375671387
INFO:root:current mean train loss 1430.7011953412405
INFO:root:current train perplexity3.0875179767608643
INFO:root:current mean train loss 1430.3098871773948
INFO:root:current train perplexity3.0887961387634277
INFO:root:current mean train loss 1431.1611637206759
INFO:root:current train perplexity3.0909054279327393
INFO:root:current mean train loss 1431.361990985458
INFO:root:current train perplexity3.0910017490386963

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.28s/it]
INFO:root:final mean train loss: 1431.171544769949
INFO:root:final train perplexity: 3.0916595458984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 2168.1454225364305
INFO:root:eval perplexity: 5.774598598480225
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.68s/it]
INFO:root:eval mean loss: 2709.8264735877938
INFO:root:eval perplexity: 9.172139167785645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [16:32:20<18:15:31, 626.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1450.9818028041295
INFO:root:current train perplexity3.0851526260375977
INFO:root:current mean train loss 1416.6124877929688
INFO:root:current train perplexity3.0748963356018066
INFO:root:current mean train loss 1420.443381621459
INFO:root:current train perplexity3.0592167377471924
INFO:root:current mean train loss 1420.2607472413665
INFO:root:current train perplexity3.064253330230713
INFO:root:current mean train loss 1420.310490557537
INFO:root:current train perplexity3.0592825412750244
INFO:root:current mean train loss 1420.50129218788
INFO:root:current train perplexity3.060081720352173
INFO:root:current mean train loss 1419.7063942576854
INFO:root:current train perplexity3.0621116161346436
INFO:root:current mean train loss 1420.3811004382221
INFO:root:current train perplexity3.063331127166748
INFO:root:current mean train loss 1421.8571699362715
INFO:root:current train perplexity3.066152334213257
INFO:root:current mean train loss 1422.3619358054398
INFO:root:current train perplexity3.0675723552703857
INFO:root:current mean train loss 1422.6478715704743
INFO:root:current train perplexity3.0700109004974365
INFO:root:current mean train loss 1423.468719537211
INFO:root:current train perplexity3.069655179977417
INFO:root:current mean train loss 1423.5468484542318
INFO:root:current train perplexity3.0711722373962402
INFO:root:current mean train loss 1424.417905485249
INFO:root:current train perplexity3.0736217498779297
INFO:root:current mean train loss 1425.7995592519283
INFO:root:current train perplexity3.0745818614959717
INFO:root:current mean train loss 1425.995899759794
INFO:root:current train perplexity3.07650089263916
INFO:root:current mean train loss 1426.293947203304
INFO:root:current train perplexity3.0780115127563477
INFO:root:current mean train loss 1426.429023804993
INFO:root:current train perplexity3.079488515853882
INFO:root:current mean train loss 1426.915674514518
INFO:root:current train perplexity3.0804896354675293
INFO:root:current mean train loss 1427.5578034818484
INFO:root:current train perplexity3.0819666385650635

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.97s/it]
INFO:root:final mean train loss: 1427.355992981357
INFO:root:final train perplexity: 3.0823700428009033
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 2171.217441856438
INFO:root:eval perplexity: 5.788963317871094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.55s/it]
INFO:root:eval mean loss: 2717.473926300698
INFO:root:eval perplexity: 9.22968578338623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [16:42:50<18:06:57, 627.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1407.417007938508
INFO:root:current train perplexity2.9997236728668213
INFO:root:current mean train loss 1411.4235895753816
INFO:root:current train perplexity3.032165050506592
INFO:root:current mean train loss 1415.9361154795724
INFO:root:current train perplexity3.0439887046813965
INFO:root:current mean train loss 1415.8577836604277
INFO:root:current train perplexity3.0465784072875977
INFO:root:current mean train loss 1414.463713112674
INFO:root:current train perplexity3.0384175777435303
INFO:root:current mean train loss 1413.5403034332332
INFO:root:current train perplexity3.042949914932251
INFO:root:current mean train loss 1416.5346466886638
INFO:root:current train perplexity3.0497515201568604
INFO:root:current mean train loss 1418.6054234954793
INFO:root:current train perplexity3.0516624450683594
INFO:root:current mean train loss 1418.4908057992066
INFO:root:current train perplexity3.0519962310791016
INFO:root:current mean train loss 1418.698637113151
INFO:root:current train perplexity3.0559582710266113
INFO:root:current mean train loss 1419.9471228347024
INFO:root:current train perplexity3.0584542751312256
INFO:root:current mean train loss 1420.1996727090932
INFO:root:current train perplexity3.060723304748535
INFO:root:current mean train loss 1420.5081018591973
INFO:root:current train perplexity3.063523769378662
INFO:root:current mean train loss 1420.3728725281449
INFO:root:current train perplexity3.0647811889648438
INFO:root:current mean train loss 1420.827468242952
INFO:root:current train perplexity3.065561294555664
INFO:root:current mean train loss 1420.9614827899197
INFO:root:current train perplexity3.0662875175476074
INFO:root:current mean train loss 1422.12693463857
INFO:root:current train perplexity3.0681214332580566
INFO:root:current mean train loss 1422.9770115720682
INFO:root:current train perplexity3.069242477416992
INFO:root:current mean train loss 1423.1822252424606
INFO:root:current train perplexity3.070679187774658
INFO:root:current mean train loss 1423.5602905336655
INFO:root:current train perplexity3.0726726055145264

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.40s/it]
INFO:root:final mean train loss: 1423.4432355810522
INFO:root:final train perplexity: 3.072873115539551
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it]
INFO:root:eval mean loss: 2172.6738315879875
INFO:root:eval perplexity: 5.795785427093506
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.14s/it]
INFO:root:eval mean loss: 2718.243707318678
INFO:root:eval perplexity: 9.235496520996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [16:53:11<17:53:26, 625.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1423.3833592732747
INFO:root:current train perplexity3.04545259475708
INFO:root:current mean train loss 1418.3162982012775
INFO:root:current train perplexity3.037959337234497
INFO:root:current mean train loss 1419.0073936216293
INFO:root:current train perplexity3.051862955093384
INFO:root:current mean train loss 1417.2750756274695
INFO:root:current train perplexity3.0588185787200928
INFO:root:current mean train loss 1415.791256768363
INFO:root:current train perplexity3.057415008544922
INFO:root:current mean train loss 1415.6367174134637
INFO:root:current train perplexity3.053179979324341
INFO:root:current mean train loss 1413.6401018684294
INFO:root:current train perplexity3.0524251461029053
INFO:root:current mean train loss 1414.311282560787
INFO:root:current train perplexity3.054185390472412
INFO:root:current mean train loss 1416.1800142684074
INFO:root:current train perplexity3.0549166202545166
INFO:root:current mean train loss 1415.8336986429078
INFO:root:current train perplexity3.052932024002075
INFO:root:current mean train loss 1415.9714869142488
INFO:root:current train perplexity3.053995370864868
INFO:root:current mean train loss 1417.3112899301775
INFO:root:current train perplexity3.057201862335205
INFO:root:current mean train loss 1418.0562926072341
INFO:root:current train perplexity3.05781626701355
INFO:root:current mean train loss 1418.4978214795935
INFO:root:current train perplexity3.057656764984131
INFO:root:current mean train loss 1417.9597301167018
INFO:root:current train perplexity3.058812141418457
INFO:root:current mean train loss 1417.454101247073
INFO:root:current train perplexity3.059211254119873
INFO:root:current mean train loss 1417.6861007838572
INFO:root:current train perplexity3.060380697250366
INFO:root:current mean train loss 1418.398550003017
INFO:root:current train perplexity3.0606133937835693
INFO:root:current mean train loss 1419.1511970288825
INFO:root:current train perplexity3.0617692470550537
INFO:root:current mean train loss 1420.0024196616923
INFO:root:current train perplexity3.0627434253692627

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:16<00:00, 556.16s/it]
INFO:root:final mean train loss: 1419.3866929878084
INFO:root:final train perplexity: 3.0630578994750977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2177.1861996481603
INFO:root:eval perplexity: 5.816974639892578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.10s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.10s/it]
INFO:root:eval mean loss: 2718.8502032773713
INFO:root:eval perplexity: 9.24007797241211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [17:03:41<17:45:34, 626.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1386.51552734375
INFO:root:current train perplexity3.0050723552703857
INFO:root:current mean train loss 1401.5300470525567
INFO:root:current train perplexity3.0136473178863525
INFO:root:current mean train loss 1403.5508236291273
INFO:root:current train perplexity3.0194637775421143
INFO:root:current mean train loss 1404.1764808968321
INFO:root:current train perplexity3.01851224899292
INFO:root:current mean train loss 1406.9472611622143
INFO:root:current train perplexity3.0274364948272705
INFO:root:current mean train loss 1405.6356726182246
INFO:root:current train perplexity3.031928300857544
INFO:root:current mean train loss 1406.2559223375822
INFO:root:current train perplexity3.034796953201294
INFO:root:current mean train loss 1407.126007518893
INFO:root:current train perplexity3.0368306636810303
INFO:root:current mean train loss 1408.8699474180364
INFO:root:current train perplexity3.040369987487793
INFO:root:current mean train loss 1410.226894303554
INFO:root:current train perplexity3.0432522296905518
INFO:root:current mean train loss 1411.2132748312793
INFO:root:current train perplexity3.0437142848968506
INFO:root:current mean train loss 1411.4061099072894
INFO:root:current train perplexity3.0444841384887695
INFO:root:current mean train loss 1411.989406419837
INFO:root:current train perplexity3.0461924076080322
INFO:root:current mean train loss 1412.0242772364354
INFO:root:current train perplexity3.0478830337524414
INFO:root:current mean train loss 1413.368107568526
INFO:root:current train perplexity3.0493435859680176
INFO:root:current mean train loss 1413.756307486147
INFO:root:current train perplexity3.051292657852173
INFO:root:current mean train loss 1414.9290644648554
INFO:root:current train perplexity3.0525076389312744
INFO:root:current mean train loss 1415.6596775822193
INFO:root:current train perplexity3.0545060634613037
INFO:root:current mean train loss 1415.6906995512525
INFO:root:current train perplexity3.054128646850586
INFO:root:current mean train loss 1416.041688720082
INFO:root:current train perplexity3.0541839599609375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.56s/it]
INFO:root:final mean train loss: 1415.7671205921722
INFO:root:final train perplexity: 3.0543265342712402
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.24s/it]
INFO:root:eval mean loss: 2180.5280908895725
INFO:root:eval perplexity: 5.832716941833496
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.26s/it]
INFO:root:eval mean loss: 2725.771899500637
INFO:root:eval perplexity: 9.292534828186035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [17:14:02<17:32:20, 625.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1398.2340340963224
INFO:root:current train perplexity3.0289900302886963
INFO:root:current mean train loss 1399.2935019692222
INFO:root:current train perplexity3.030266046524048
INFO:root:current mean train loss 1404.2633372638243
INFO:root:current train perplexity3.0279464721679688
INFO:root:current mean train loss 1406.757978669012
INFO:root:current train perplexity3.0273303985595703
INFO:root:current mean train loss 1406.730289190142
INFO:root:current train perplexity3.0308876037597656
INFO:root:current mean train loss 1405.6031787780553
INFO:root:current train perplexity3.0295791625976562
INFO:root:current mean train loss 1405.2140302462312
INFO:root:current train perplexity3.0317468643188477
INFO:root:current mean train loss 1404.7467326678889
INFO:root:current train perplexity3.0312232971191406
INFO:root:current mean train loss 1407.2214170010452
INFO:root:current train perplexity3.034853935241699
INFO:root:current mean train loss 1408.1172364772945
INFO:root:current train perplexity3.0336337089538574
INFO:root:current mean train loss 1408.0019219869166
INFO:root:current train perplexity3.0342206954956055
INFO:root:current mean train loss 1408.4257092677599
INFO:root:current train perplexity3.033586263656616
INFO:root:current mean train loss 1409.6852991160663
INFO:root:current train perplexity3.035904884338379
INFO:root:current mean train loss 1410.4669576332988
INFO:root:current train perplexity3.037910223007202
INFO:root:current mean train loss 1411.1379544442161
INFO:root:current train perplexity3.0401928424835205
INFO:root:current mean train loss 1411.561171501536
INFO:root:current train perplexity3.042219877243042
INFO:root:current mean train loss 1412.4824964090137
INFO:root:current train perplexity3.0437963008880615
INFO:root:current mean train loss 1412.5009897833588
INFO:root:current train perplexity3.0439538955688477
INFO:root:current mean train loss 1412.6991143221555
INFO:root:current train perplexity3.0461909770965576
INFO:root:current mean train loss 1413.4080627626174
INFO:root:current train perplexity3.047590732574463

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:04<00:00, 544.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:04<00:00, 544.82s/it]
INFO:root:final mean train loss: 1412.9602823954788
INFO:root:final train perplexity: 3.047572612762451
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.55s/it]
INFO:root:eval mean loss: 2180.8244451428136
INFO:root:eval perplexity: 5.834115505218506
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it]
INFO:root:eval mean loss: 2727.8689384730997
INFO:root:eval perplexity: 9.30848217010498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [17:24:21<17:18:32, 623.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1390.4096913963858
INFO:root:current train perplexity3.0063300132751465
INFO:root:current mean train loss 1403.0932304343985
INFO:root:current train perplexity3.018178701400757
INFO:root:current mean train loss 1402.2379852601118
INFO:root:current train perplexity3.0169155597686768
INFO:root:current mean train loss 1402.522866002898
INFO:root:current train perplexity3.021442413330078
INFO:root:current mean train loss 1403.8587475243455
INFO:root:current train perplexity3.0187127590179443
INFO:root:current mean train loss 1402.4497924193317
INFO:root:current train perplexity3.0202476978302
INFO:root:current mean train loss 1401.7179257197783
INFO:root:current train perplexity3.021822214126587
INFO:root:current mean train loss 1401.8391321060506
INFO:root:current train perplexity3.0211799144744873
INFO:root:current mean train loss 1402.5969694517346
INFO:root:current train perplexity3.0247039794921875
INFO:root:current mean train loss 1403.2083327223709
INFO:root:current train perplexity3.025951385498047
INFO:root:current mean train loss 1404.4615831730905
INFO:root:current train perplexity3.0297863483428955
INFO:root:current mean train loss 1406.3030739116907
INFO:root:current train perplexity3.031963586807251
INFO:root:current mean train loss 1405.670263183218
INFO:root:current train perplexity3.031602382659912
INFO:root:current mean train loss 1405.8132902722089
INFO:root:current train perplexity3.0325963497161865
INFO:root:current mean train loss 1406.7069752230655
INFO:root:current train perplexity3.0332138538360596
INFO:root:current mean train loss 1407.7059272732713
INFO:root:current train perplexity3.033782958984375
INFO:root:current mean train loss 1408.10958709627
INFO:root:current train perplexity3.034876823425293
INFO:root:current mean train loss 1408.1017811951735
INFO:root:current train perplexity3.0344409942626953
INFO:root:current mean train loss 1408.537942461493
INFO:root:current train perplexity3.0349948406219482

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.34s/it]
INFO:root:final mean train loss: 1408.486834596758
INFO:root:final train perplexity: 3.036839723587036
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it]
INFO:root:eval mean loss: 2185.999535093916
INFO:root:eval perplexity: 5.858584403991699
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 2731.44170752992
INFO:root:eval perplexity: 9.335719108581543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [17:34:49<17:10:44, 624.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1375.4202346801758
INFO:root:current train perplexity2.9975976943969727
INFO:root:current mean train loss 1393.571338522023
INFO:root:current train perplexity2.9906797409057617
INFO:root:current mean train loss 1397.456022474501
INFO:root:current train perplexity2.99295973777771
INFO:root:current mean train loss 1395.0095767250543
INFO:root:current train perplexity2.995560884475708
INFO:root:current mean train loss 1393.2285702045147
INFO:root:current train perplexity3.0003390312194824
INFO:root:current mean train loss 1393.9199292086817
INFO:root:current train perplexity3.00339412689209
INFO:root:current mean train loss 1394.592812278054
INFO:root:current train perplexity3.002610206604004
INFO:root:current mean train loss 1393.6290883325332
INFO:root:current train perplexity3.004486322402954
INFO:root:current mean train loss 1395.6454325657264
INFO:root:current train perplexity3.0087056159973145
INFO:root:current mean train loss 1396.8107034608265
INFO:root:current train perplexity3.011124610900879
INFO:root:current mean train loss 1397.30256244329
INFO:root:current train perplexity3.013218402862549
INFO:root:current mean train loss 1398.7447212246584
INFO:root:current train perplexity3.0149106979370117
INFO:root:current mean train loss 1399.0547030599494
INFO:root:current train perplexity3.0160181522369385
INFO:root:current mean train loss 1400.1759550796091
INFO:root:current train perplexity3.017784833908081
INFO:root:current mean train loss 1401.3952708271263
INFO:root:current train perplexity3.0204646587371826
INFO:root:current mean train loss 1402.0521965731416
INFO:root:current train perplexity3.022857904434204
INFO:root:current mean train loss 1402.521801788028
INFO:root:current train perplexity3.023869514465332
INFO:root:current mean train loss 1403.526814138417
INFO:root:current train perplexity3.025913715362549
INFO:root:current mean train loss 1404.2855920329494
INFO:root:current train perplexity3.027202606201172
INFO:root:current mean train loss 1404.6675123879706
INFO:root:current train perplexity3.0276949405670166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.32s/it]
INFO:root:final mean train loss: 1404.8651718008837
INFO:root:final train perplexity: 3.0281782150268555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.79s/it]
INFO:root:eval mean loss: 2184.5824996190713
INFO:root:eval perplexity: 5.851874351501465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 2731.150067268534
INFO:root:eval perplexity: 9.333494186401367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [17:45:18<17:02:20, 625.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1404.3406686493845
INFO:root:current train perplexity3.015935182571411
INFO:root:current mean train loss 1400.1943937602796
INFO:root:current train perplexity3.0284528732299805
INFO:root:current mean train loss 1395.1158918781853
INFO:root:current train perplexity3.0113346576690674
INFO:root:current mean train loss 1396.5946968697212
INFO:root:current train perplexity3.0102431774139404
INFO:root:current mean train loss 1399.1946477383444
INFO:root:current train perplexity3.011528491973877
INFO:root:current mean train loss 1397.5531397492084
INFO:root:current train perplexity3.0094738006591797
INFO:root:current mean train loss 1396.1950859081876
INFO:root:current train perplexity3.009728193283081
INFO:root:current mean train loss 1398.267144800371
INFO:root:current train perplexity3.011641263961792
INFO:root:current mean train loss 1398.6306559733268
INFO:root:current train perplexity3.0136139392852783
INFO:root:current mean train loss 1399.608545890064
INFO:root:current train perplexity3.0121614933013916
INFO:root:current mean train loss 1399.736462957746
INFO:root:current train perplexity3.012382745742798
INFO:root:current mean train loss 1399.87584253296
INFO:root:current train perplexity3.0141477584838867
INFO:root:current mean train loss 1399.947549564705
INFO:root:current train perplexity3.0141098499298096
INFO:root:current mean train loss 1400.3819875867405
INFO:root:current train perplexity3.014741897583008
INFO:root:current mean train loss 1400.655573544765
INFO:root:current train perplexity3.0157253742218018
INFO:root:current mean train loss 1401.0987379219666
INFO:root:current train perplexity3.017672300338745
INFO:root:current mean train loss 1401.0695377683903
INFO:root:current train perplexity3.0178072452545166
INFO:root:current mean train loss 1400.8137768427898
INFO:root:current train perplexity3.0174455642700195
INFO:root:current mean train loss 1401.9834944022139
INFO:root:current train perplexity3.019719362258911
INFO:root:current mean train loss 1402.058510517371
INFO:root:current train perplexity3.0204272270202637

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.01s/it]
INFO:root:final mean train loss: 1401.7711803996076
INFO:root:final train perplexity: 3.0207982063293457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.76s/it]
INFO:root:eval mean loss: 2190.7877210251827
INFO:root:eval perplexity: 5.8813157081604
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it]
INFO:root:eval mean loss: 2740.5586452619405
INFO:root:eval perplexity: 9.405590057373047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [17:55:39<16:49:26, 624.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1395.8099975585938
INFO:root:current train perplexity2.974222421646118
INFO:root:current mean train loss 1385.9080444335937
INFO:root:current train perplexity2.9840128421783447
INFO:root:current mean train loss 1391.3353833007814
INFO:root:current train perplexity2.9889094829559326
INFO:root:current mean train loss 1389.293223702567
INFO:root:current train perplexity2.985013961791992
INFO:root:current mean train loss 1388.883656141493
INFO:root:current train perplexity2.988529682159424
INFO:root:current mean train loss 1391.7956070223722
INFO:root:current train perplexity2.995053768157959
INFO:root:current mean train loss 1392.6177649864783
INFO:root:current train perplexity2.999204158782959
INFO:root:current mean train loss 1394.6888929036459
INFO:root:current train perplexity3.0005314350128174
INFO:root:current mean train loss 1394.81389748966
INFO:root:current train perplexity2.999298095703125
INFO:root:current mean train loss 1396.5023078998765
INFO:root:current train perplexity3.0029067993164062
INFO:root:current mean train loss 1396.4779454985119
INFO:root:current train perplexity3.0048227310180664
INFO:root:current mean train loss 1396.219156759511
INFO:root:current train perplexity3.0044779777526855
INFO:root:current mean train loss 1396.5861884765625
INFO:root:current train perplexity3.0047404766082764
INFO:root:current mean train loss 1396.4076252350983
INFO:root:current train perplexity3.00557017326355
INFO:root:current mean train loss 1396.572810311153
INFO:root:current train perplexity3.006270170211792
INFO:root:current mean train loss 1396.9078836945564
INFO:root:current train perplexity3.008211612701416
INFO:root:current mean train loss 1397.8680772076232
INFO:root:current train perplexity3.0094666481018066
INFO:root:current mean train loss 1398.1419594726563
INFO:root:current train perplexity3.0097665786743164
INFO:root:current mean train loss 1398.1473318069045
INFO:root:current train perplexity3.0106968879699707
INFO:root:current mean train loss 1398.1896178886218
INFO:root:current train perplexity3.011054515838623

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.22s/it]
INFO:root:final mean train loss: 1397.98758948745
INFO:root:final train perplexity: 3.0117976665496826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.69s/it]
INFO:root:eval mean loss: 2192.8708933295934
INFO:root:eval perplexity: 5.891232013702393
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it]
INFO:root:eval mean loss: 2744.546218763852
INFO:root:eval perplexity: 9.436314582824707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [18:06:00<16:37:21, 623.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1398.6393842554803
INFO:root:current train perplexity2.957047939300537
INFO:root:current mean train loss 1386.6939682646425
INFO:root:current train perplexity2.968639850616455
INFO:root:current mean train loss 1388.0338303926733
INFO:root:current train perplexity2.974489688873291
INFO:root:current mean train loss 1386.9195317156634
INFO:root:current train perplexity2.976846218109131
INFO:root:current mean train loss 1386.3140401247992
INFO:root:current train perplexity2.975781202316284
INFO:root:current mean train loss 1387.2420103150491
INFO:root:current train perplexity2.9795501232147217
INFO:root:current mean train loss 1387.7892755282514
INFO:root:current train perplexity2.981518507003784
INFO:root:current mean train loss 1389.492945068041
INFO:root:current train perplexity2.9849321842193604
INFO:root:current mean train loss 1389.3913143382354
INFO:root:current train perplexity2.987786293029785
INFO:root:current mean train loss 1389.1279226182783
INFO:root:current train perplexity2.9885289669036865
INFO:root:current mean train loss 1389.3174869212014
INFO:root:current train perplexity2.992521286010742
INFO:root:current mean train loss 1390.8372842483063
INFO:root:current train perplexity2.995311737060547
INFO:root:current mean train loss 1390.962795531552
INFO:root:current train perplexity2.99627947807312
INFO:root:current mean train loss 1391.618200138876
INFO:root:current train perplexity2.9969449043273926
INFO:root:current mean train loss 1390.8754691427553
INFO:root:current train perplexity2.9975523948669434
INFO:root:current mean train loss 1391.1189199324695
INFO:root:current train perplexity2.9982681274414062
INFO:root:current mean train loss 1392.0222424997423
INFO:root:current train perplexity2.999647855758667
INFO:root:current mean train loss 1393.1648988923448
INFO:root:current train perplexity3.001671314239502
INFO:root:current mean train loss 1393.8805911079983
INFO:root:current train perplexity3.0025882720947266
INFO:root:current mean train loss 1394.878667260283
INFO:root:current train perplexity3.0033750534057617

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.87s/it]
INFO:root:final mean train loss: 1394.63225908996
INFO:root:final train perplexity: 3.003838539123535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it]
INFO:root:eval mean loss: 2197.1662251357493
INFO:root:eval perplexity: 5.911733150482178
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it]
INFO:root:eval mean loss: 2748.2028263173206
INFO:root:eval perplexity: 9.464573860168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [18:16:30<16:30:24, 625.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1373.5901794433594
INFO:root:current train perplexity2.9481921195983887
INFO:root:current mean train loss 1377.6905975341797
INFO:root:current train perplexity2.9621784687042236
INFO:root:current mean train loss 1378.5168779400033
INFO:root:current train perplexity2.9677343368530273
INFO:root:current mean train loss 1378.0304015477498
INFO:root:current train perplexity2.9661669731140137
INFO:root:current mean train loss 1380.5544738769531
INFO:root:current train perplexity2.971163034439087
INFO:root:current mean train loss 1384.145328260448
INFO:root:current train perplexity2.975271701812744
INFO:root:current mean train loss 1385.869811119392
INFO:root:current train perplexity2.9773356914520264
INFO:root:current mean train loss 1385.9000184973893
INFO:root:current train perplexity2.978019952774048
INFO:root:current mean train loss 1385.0995404687942
INFO:root:current train perplexity2.979897975921631
INFO:root:current mean train loss 1384.9696759479802
INFO:root:current train perplexity2.981112241744995
INFO:root:current mean train loss 1385.8847640484462
INFO:root:current train perplexity2.9824280738830566
INFO:root:current mean train loss 1387.129608051197
INFO:root:current train perplexity2.983351230621338
INFO:root:current mean train loss 1387.5757370232793
INFO:root:current train perplexity2.9859869480133057
INFO:root:current mean train loss 1388.4023440146034
INFO:root:current train perplexity2.988224744796753
INFO:root:current mean train loss 1389.263214358101
INFO:root:current train perplexity2.989943742752075
INFO:root:current mean train loss 1389.294894902393
INFO:root:current train perplexity2.9898552894592285
INFO:root:current mean train loss 1389.2767958108716
INFO:root:current train perplexity2.99070405960083
INFO:root:current mean train loss 1390.1131772439041
INFO:root:current train perplexity2.9935522079467773
INFO:root:current mean train loss 1391.0138486307392
INFO:root:current train perplexity2.9951417446136475

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.98s/it]
INFO:root:final mean train loss: 1390.9501521292805
INFO:root:final train perplexity: 2.9951281547546387
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.15s/it]
INFO:root:eval mean loss: 2200.163557769559
INFO:root:eval perplexity: 5.926079750061035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.86s/it]
INFO:root:eval mean loss: 2752.3014504723515
INFO:root:eval perplexity: 9.49635124206543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [18:26:52<16:17:57, 624.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1378.3021240234375
INFO:root:current train perplexity3.0638139247894287
INFO:root:current mean train loss 1375.8019596515317
INFO:root:current train perplexity2.966181993484497
INFO:root:current mean train loss 1381.634859758823
INFO:root:current train perplexity2.974942684173584
INFO:root:current mean train loss 1383.2044730455773
INFO:root:current train perplexity2.9749319553375244
INFO:root:current mean train loss 1381.7404776023807
INFO:root:current train perplexity2.97532320022583
INFO:root:current mean train loss 1381.307355503836
INFO:root:current train perplexity2.973980665206909
INFO:root:current mean train loss 1382.4180118097443
INFO:root:current train perplexity2.973501443862915
INFO:root:current mean train loss 1382.644199344129
INFO:root:current train perplexity2.9760117530822754
INFO:root:current mean train loss 1383.6302525285776
INFO:root:current train perplexity2.9769811630249023
INFO:root:current mean train loss 1382.7272781219651
INFO:root:current train perplexity2.976221799850464
INFO:root:current mean train loss 1382.4567193060846
INFO:root:current train perplexity2.976424217224121
INFO:root:current mean train loss 1382.4935079881216
INFO:root:current train perplexity2.9787204265594482
INFO:root:current mean train loss 1383.1136655529572
INFO:root:current train perplexity2.9807608127593994
INFO:root:current mean train loss 1383.8367895526578
INFO:root:current train perplexity2.981241226196289
INFO:root:current mean train loss 1383.7743661405357
INFO:root:current train perplexity2.9821367263793945
INFO:root:current mean train loss 1384.2146131094896
INFO:root:current train perplexity2.9828033447265625
INFO:root:current mean train loss 1384.9917564034686
INFO:root:current train perplexity2.9831175804138184
INFO:root:current mean train loss 1385.517608696401
INFO:root:current train perplexity2.9832730293273926
INFO:root:current mean train loss 1386.792600166791
INFO:root:current train perplexity2.985091209411621
INFO:root:current mean train loss 1387.1100823143042
INFO:root:current train perplexity2.9858052730560303

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.90s/it]
INFO:root:final mean train loss: 1387.4428064882063
INFO:root:final train perplexity: 2.9868545532226562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.05s/it]
INFO:root:eval mean loss: 2202.8863430158467
INFO:root:eval perplexity: 5.939145565032959
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.41s/it]
INFO:root:eval mean loss: 2755.4231827106883
INFO:root:eval perplexity: 9.520628929138184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [18:37:18<16:08:41, 624.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.3736029730903
INFO:root:current train perplexity2.964521646499634
INFO:root:current mean train loss 1377.9216453422935
INFO:root:current train perplexity2.962570905685425
INFO:root:current mean train loss 1379.3618231257167
INFO:root:current train perplexity2.9645466804504395
INFO:root:current mean train loss 1377.7341600334119
INFO:root:current train perplexity2.9600026607513428
INFO:root:current mean train loss 1379.3907806542502
INFO:root:current train perplexity2.9617085456848145
INFO:root:current mean train loss 1377.9814069004133
INFO:root:current train perplexity2.9656665325164795
INFO:root:current mean train loss 1377.3795426748331
INFO:root:current train perplexity2.964548110961914
INFO:root:current mean train loss 1376.779371851334
INFO:root:current train perplexity2.9643588066101074
INFO:root:current mean train loss 1377.3674453698045
INFO:root:current train perplexity2.9655141830444336
INFO:root:current mean train loss 1379.4379485219652
INFO:root:current train perplexity2.9672024250030518
INFO:root:current mean train loss 1380.5148327420877
INFO:root:current train perplexity2.9696221351623535
INFO:root:current mean train loss 1379.897789806714
INFO:root:current train perplexity2.9695425033569336
INFO:root:current mean train loss 1379.903669485709
INFO:root:current train perplexity2.969843626022339
INFO:root:current mean train loss 1381.025443139315
INFO:root:current train perplexity2.972337245941162
INFO:root:current mean train loss 1382.118568926167
INFO:root:current train perplexity2.9745302200317383
INFO:root:current mean train loss 1382.6151705253108
INFO:root:current train perplexity2.9752285480499268
INFO:root:current mean train loss 1383.5108112952912
INFO:root:current train perplexity2.975571870803833
INFO:root:current mean train loss 1383.828085423071
INFO:root:current train perplexity2.9770023822784424
INFO:root:current mean train loss 1384.339302625331
INFO:root:current train perplexity2.9770658016204834
INFO:root:current mean train loss 1384.33492497831
INFO:root:current train perplexity2.9777300357818604

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.59s/it]
INFO:root:final mean train loss: 1384.1679979286828
INFO:root:final train perplexity: 2.9791502952575684
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it]
INFO:root:eval mean loss: 2205.898866044714
INFO:root:eval perplexity: 5.953631401062012
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.30s/it]
INFO:root:eval mean loss: 2759.586030134918
INFO:root:eval perplexity: 9.553093910217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [18:47:43<15:58:11, 624.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.8517996651785
INFO:root:current train perplexity2.968484878540039
INFO:root:current mean train loss 1359.686062282986
INFO:root:current train perplexity2.961825132369995
INFO:root:current mean train loss 1365.1634069564495
INFO:root:current train perplexity2.9484167098999023
INFO:root:current mean train loss 1366.7979903947062
INFO:root:current train perplexity2.9454333782196045
INFO:root:current mean train loss 1367.6989513177982
INFO:root:current train perplexity2.9489879608154297
INFO:root:current mean train loss 1369.906115380403
INFO:root:current train perplexity2.9496917724609375
INFO:root:current mean train loss 1371.158114888349
INFO:root:current train perplexity2.949944257736206
INFO:root:current mean train loss 1372.6194978675064
INFO:root:current train perplexity2.9529025554656982
INFO:root:current mean train loss 1372.3243345340568
INFO:root:current train perplexity2.957615852355957
INFO:root:current mean train loss 1373.7207100444937
INFO:root:current train perplexity2.957303047180176
INFO:root:current mean train loss 1374.5158689047403
INFO:root:current train perplexity2.9597902297973633
INFO:root:current mean train loss 1375.9642549086248
INFO:root:current train perplexity2.962229013442993
INFO:root:current mean train loss 1376.523862620983
INFO:root:current train perplexity2.9642438888549805
INFO:root:current mean train loss 1377.6298202686096
INFO:root:current train perplexity2.965163230895996
INFO:root:current mean train loss 1377.6478097098213
INFO:root:current train perplexity2.9656479358673096
INFO:root:current mean train loss 1377.885315140218
INFO:root:current train perplexity2.9654407501220703
INFO:root:current mean train loss 1378.1204893712968
INFO:root:current train perplexity2.966038227081299
INFO:root:current mean train loss 1378.5782148465644
INFO:root:current train perplexity2.966982364654541
INFO:root:current mean train loss 1379.2512542974073
INFO:root:current train perplexity2.9678359031677246
INFO:root:current mean train loss 1380.5898263384206
INFO:root:current train perplexity2.969740629196167

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.01s/it]
INFO:root:final mean train loss: 1380.3969248253711
INFO:root:final train perplexity: 2.9703032970428467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.92s/it]
INFO:root:eval mean loss: 2207.7067113565213
INFO:root:eval perplexity: 5.962344646453857
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.98s/it]
INFO:root:eval mean loss: 2761.7604673128603
INFO:root:eval perplexity: 9.570099830627441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [18:58:16<15:51:32, 627.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1366.6745981069712
INFO:root:current train perplexity2.955115795135498
INFO:root:current mean train loss 1375.67368918971
INFO:root:current train perplexity2.955517053604126
INFO:root:current mean train loss 1373.8006984165736
INFO:root:current train perplexity2.943415641784668
INFO:root:current mean train loss 1370.3681030273438
INFO:root:current train perplexity2.9460582733154297
INFO:root:current mean train loss 1371.3959393796667
INFO:root:current train perplexity2.9518167972564697
INFO:root:current mean train loss 1373.5027835127237
INFO:root:current train perplexity2.9495813846588135
INFO:root:current mean train loss 1374.0039953688172
INFO:root:current train perplexity2.949692487716675
INFO:root:current mean train loss 1375.9135271437624
INFO:root:current train perplexity2.9498658180236816
INFO:root:current mean train loss 1376.0197865660762
INFO:root:current train perplexity2.9514803886413574
INFO:root:current mean train loss 1375.8995970397436
INFO:root:current train perplexity2.953519582748413
INFO:root:current mean train loss 1375.186141561646
INFO:root:current train perplexity2.9539880752563477
INFO:root:current mean train loss 1375.22261428833
INFO:root:current train perplexity2.9530539512634277
INFO:root:current mean train loss 1375.741851124139
INFO:root:current train perplexity2.954061269760132
INFO:root:current mean train loss 1375.7737905287884
INFO:root:current train perplexity2.955906391143799
INFO:root:current mean train loss 1376.3792213460958
INFO:root:current train perplexity2.95684552192688
INFO:root:current mean train loss 1376.5438815244695
INFO:root:current train perplexity2.9581351280212402
INFO:root:current mean train loss 1376.5649197557648
INFO:root:current train perplexity2.959493637084961
INFO:root:current mean train loss 1376.7763471908222
INFO:root:current train perplexity2.9613466262817383
INFO:root:current mean train loss 1376.8865122455243
INFO:root:current train perplexity2.9619743824005127
INFO:root:current mean train loss 1377.5145627006154
INFO:root:current train perplexity2.962846040725708

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.54s/it]
INFO:root:final mean train loss: 1376.9983419859825
INFO:root:final train perplexity: 2.9623525142669678
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it]
INFO:root:eval mean loss: 2213.4994342344025
INFO:root:eval perplexity: 5.990342140197754
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it]
INFO:root:eval mean loss: 2767.359229987395
INFO:root:eval perplexity: 9.614020347595215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [19:08:42<15:40:31, 627.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.7899506057518
INFO:root:current train perplexity2.915304660797119
INFO:root:current mean train loss 1358.0364231809356
INFO:root:current train perplexity2.921865940093994
INFO:root:current mean train loss 1359.0927965809421
INFO:root:current train perplexity2.9258081912994385
INFO:root:current mean train loss 1364.3260574134063
INFO:root:current train perplexity2.9278616905212402
INFO:root:current mean train loss 1364.584501026536
INFO:root:current train perplexity2.9274070262908936
INFO:root:current mean train loss 1366.1831726181485
INFO:root:current train perplexity2.930202007293701
INFO:root:current mean train loss 1368.676092720887
INFO:root:current train perplexity2.934760093688965
INFO:root:current mean train loss 1369.8218462364882
INFO:root:current train perplexity2.939549684524536
INFO:root:current mean train loss 1369.6596124822443
INFO:root:current train perplexity2.94290828704834
INFO:root:current mean train loss 1370.9077914368872
INFO:root:current train perplexity2.9436819553375244
INFO:root:current mean train loss 1370.36418473018
INFO:root:current train perplexity2.945323944091797
INFO:root:current mean train loss 1369.9665462601583
INFO:root:current train perplexity2.9467339515686035
INFO:root:current mean train loss 1370.7372767912111
INFO:root:current train perplexity2.9462411403656006
INFO:root:current mean train loss 1370.3940887116794
INFO:root:current train perplexity2.9477791786193848
INFO:root:current mean train loss 1372.1472226968017
INFO:root:current train perplexity2.949763536453247
INFO:root:current mean train loss 1372.6690954690985
INFO:root:current train perplexity2.950693368911743
INFO:root:current mean train loss 1373.1451153443772
INFO:root:current train perplexity2.9517953395843506
INFO:root:current mean train loss 1374.0319595121273
INFO:root:current train perplexity2.9534285068511963
INFO:root:current mean train loss 1374.0504128184148
INFO:root:current train perplexity2.954507827758789
INFO:root:current mean train loss 1374.141653391236
INFO:root:current train perplexity2.954822301864624

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.84s/it]
INFO:root:final mean train loss: 1373.9902409309698
INFO:root:final train perplexity: 2.9553329944610596
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.45s/it]
INFO:root:eval mean loss: 2214.279125024241
INFO:root:eval perplexity: 5.994119167327881
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it]
INFO:root:eval mean loss: 2769.080195866578
INFO:root:eval perplexity: 9.62756061553955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [19:19:11<15:30:49, 627.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.7915791356286
INFO:root:current train perplexity2.9252612590789795
INFO:root:current mean train loss 1360.5515438613072
INFO:root:current train perplexity2.926013946533203
INFO:root:current mean train loss 1362.0663682630845
INFO:root:current train perplexity2.9304280281066895
INFO:root:current mean train loss 1361.6232619211464
INFO:root:current train perplexity2.9284400939941406
INFO:root:current mean train loss 1364.1598297244727
INFO:root:current train perplexity2.9268829822540283
INFO:root:current mean train loss 1364.4534139275145
INFO:root:current train perplexity2.928633213043213
INFO:root:current mean train loss 1365.809932252756
INFO:root:current train perplexity2.9302144050598145
INFO:root:current mean train loss 1364.8954127653865
INFO:root:current train perplexity2.933302164077759
INFO:root:current mean train loss 1365.2581534977692
INFO:root:current train perplexity2.9327332973480225
INFO:root:current mean train loss 1365.91540118792
INFO:root:current train perplexity2.9342339038848877
INFO:root:current mean train loss 1366.586823802407
INFO:root:current train perplexity2.935500144958496
INFO:root:current mean train loss 1366.7749968298772
INFO:root:current train perplexity2.936882972717285
INFO:root:current mean train loss 1367.7107908447645
INFO:root:current train perplexity2.939138650894165
INFO:root:current mean train loss 1367.250517874053
INFO:root:current train perplexity2.9400033950805664
INFO:root:current mean train loss 1368.5702575437153
INFO:root:current train perplexity2.9414021968841553
INFO:root:current mean train loss 1368.6997565982622
INFO:root:current train perplexity2.9436025619506836
INFO:root:current mean train loss 1369.4812950776848
INFO:root:current train perplexity2.945854425430298
INFO:root:current mean train loss 1370.363393273092
INFO:root:current train perplexity2.9468994140625
INFO:root:current mean train loss 1370.4726083539072
INFO:root:current train perplexity2.947248935699463

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.45s/it]
INFO:root:final mean train loss: 1370.6762283794578
INFO:root:final train perplexity: 2.9476184844970703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.13s/it]
INFO:root:eval mean loss: 2219.5910406450853
INFO:root:eval perplexity: 6.019926071166992
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.12s/it]
INFO:root:eval mean loss: 2776.496740030059
INFO:root:eval perplexity: 9.686132431030273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [19:29:48<15:24:31, 630.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1353.102783203125
INFO:root:current train perplexity2.8940188884735107
INFO:root:current mean train loss 1360.869714236954
INFO:root:current train perplexity2.9204790592193604
INFO:root:current mean train loss 1360.9266213102294
INFO:root:current train perplexity2.933412790298462
INFO:root:current mean train loss 1357.0101733317863
INFO:root:current train perplexity2.9321157932281494
INFO:root:current mean train loss 1358.4512797088244
INFO:root:current train perplexity2.9351532459259033
INFO:root:current mean train loss 1358.9496376234779
INFO:root:current train perplexity2.9331881999969482
INFO:root:current mean train loss 1361.1933597798766
INFO:root:current train perplexity2.93456768989563
INFO:root:current mean train loss 1360.5621582725819
INFO:root:current train perplexity2.934706449508667
INFO:root:current mean train loss 1361.0309441401384
INFO:root:current train perplexity2.9345836639404297
INFO:root:current mean train loss 1361.7725417769761
INFO:root:current train perplexity2.9364283084869385
INFO:root:current mean train loss 1362.882531969521
INFO:root:current train perplexity2.9381048679351807
INFO:root:current mean train loss 1363.51855656891
INFO:root:current train perplexity2.936722755432129
INFO:root:current mean train loss 1364.59444873331
INFO:root:current train perplexity2.9368855953216553
INFO:root:current mean train loss 1364.5793783051731
INFO:root:current train perplexity2.9363160133361816
INFO:root:current mean train loss 1365.4000377260782
INFO:root:current train perplexity2.936398983001709
INFO:root:current mean train loss 1365.6412603666365
INFO:root:current train perplexity2.93692684173584
INFO:root:current mean train loss 1366.8220127269915
INFO:root:current train perplexity2.9385714530944824
INFO:root:current mean train loss 1367.8195577857778
INFO:root:current train perplexity2.937889337539673
INFO:root:current mean train loss 1368.1802702960345
INFO:root:current train perplexity2.9392290115356445
INFO:root:current mean train loss 1368.719794493378
INFO:root:current train perplexity2.940479040145874

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.31s/it]
INFO:root:final mean train loss: 1368.2550969126246
INFO:root:final train perplexity: 2.941995859146118
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.71s/it]
INFO:root:eval mean loss: 2222.39600864708
INFO:root:eval perplexity: 6.03359842300415
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.37s/it]
INFO:root:eval mean loss: 2780.37575190118
INFO:root:eval perplexity: 9.716907501220703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [19:40:19<15:14:10, 630.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1327.2745788574218
INFO:root:current train perplexity2.899519205093384
INFO:root:current mean train loss 1356.549267578125
INFO:root:current train perplexity2.9184670448303223
INFO:root:current mean train loss 1354.003379683061
INFO:root:current train perplexity2.9127297401428223
INFO:root:current mean train loss 1356.7458362579346
INFO:root:current train perplexity2.9140331745147705
INFO:root:current mean train loss 1355.0477934337798
INFO:root:current train perplexity2.917257308959961
INFO:root:current mean train loss 1356.063677274264
INFO:root:current train perplexity2.9192190170288086
INFO:root:current mean train loss 1357.5047274681829
INFO:root:current train perplexity2.9185829162597656
INFO:root:current mean train loss 1356.5034729003905
INFO:root:current train perplexity2.9193832874298096
INFO:root:current mean train loss 1355.7157222096514
INFO:root:current train perplexity2.918933629989624
INFO:root:current mean train loss 1356.7306042215098
INFO:root:current train perplexity2.920053243637085
INFO:root:current mean train loss 1358.939483762255
INFO:root:current train perplexity2.9211673736572266
INFO:root:current mean train loss 1359.6410403660366
INFO:root:current train perplexity2.92238450050354
INFO:root:current mean train loss 1360.8319410980725
INFO:root:current train perplexity2.9239273071289062
INFO:root:current mean train loss 1361.5774893096
INFO:root:current train perplexity2.9262325763702393
INFO:root:current mean train loss 1362.315371145329
INFO:root:current train perplexity2.926516532897949
INFO:root:current mean train loss 1362.66667448345
INFO:root:current train perplexity2.92865252494812
INFO:root:current mean train loss 1363.3787079234182
INFO:root:current train perplexity2.930772066116333
INFO:root:current mean train loss 1363.7437052881994
INFO:root:current train perplexity2.9305598735809326
INFO:root:current mean train loss 1364.0277424906637
INFO:root:current train perplexity2.931989908218384
INFO:root:current mean train loss 1364.8397391001383
INFO:root:current train perplexity2.932455062866211

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.09s/it]
INFO:root:final mean train loss: 1364.247003213841
INFO:root:final train perplexity: 2.932711362838745
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.81s/it]
INFO:root:eval mean loss: 2225.5898783798757
INFO:root:eval perplexity: 6.049202919006348
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.94s/it]
INFO:root:eval mean loss: 2785.5025422657636
INFO:root:eval perplexity: 9.757739067077637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [19:50:47<15:02:39, 629.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1345.7430551889781
INFO:root:current train perplexity2.8756844997406006
INFO:root:current mean train loss 1343.4565082188071
INFO:root:current train perplexity2.9057350158691406
INFO:root:current mean train loss 1343.9897852386603
INFO:root:current train perplexity2.8951945304870605
INFO:root:current mean train loss 1351.749134641019
INFO:root:current train perplexity2.9080772399902344
INFO:root:current mean train loss 1353.1556287542905
INFO:root:current train perplexity2.917073965072632
INFO:root:current mean train loss 1354.7547227799116
INFO:root:current train perplexity2.919381856918335
INFO:root:current mean train loss 1355.6894801452734
INFO:root:current train perplexity2.921328544616699
INFO:root:current mean train loss 1355.585411951694
INFO:root:current train perplexity2.918551206588745
INFO:root:current mean train loss 1355.8629869394974
INFO:root:current train perplexity2.9172704219818115
INFO:root:current mean train loss 1355.303161425677
INFO:root:current train perplexity2.916221857070923
INFO:root:current mean train loss 1356.449591435255
INFO:root:current train perplexity2.9169023036956787
INFO:root:current mean train loss 1356.379322598876
INFO:root:current train perplexity2.917069911956787
INFO:root:current mean train loss 1357.0794417212446
INFO:root:current train perplexity2.918403148651123
INFO:root:current mean train loss 1357.5839391806808
INFO:root:current train perplexity2.918898820877075
INFO:root:current mean train loss 1358.8823130905587
INFO:root:current train perplexity2.920758008956909
INFO:root:current mean train loss 1358.5719403159565
INFO:root:current train perplexity2.9208638668060303
INFO:root:current mean train loss 1359.2277023697598
INFO:root:current train perplexity2.922231435775757
INFO:root:current mean train loss 1359.4595875583577
INFO:root:current train perplexity2.9237701892852783
INFO:root:current mean train loss 1360.4400424116257
INFO:root:current train perplexity2.924259662628174
INFO:root:current mean train loss 1360.9599485855222
INFO:root:current train perplexity2.9244847297668457

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:24<00:00, 564.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:24<00:00, 564.38s/it]
INFO:root:final mean train loss: 1360.8188758499984
INFO:root:final train perplexity: 2.924792766571045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 2227.1268436080177
INFO:root:eval perplexity: 6.056725978851318
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.61s/it]
INFO:root:eval mean loss: 2785.290360254599
INFO:root:eval perplexity: 9.75604248046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [20:01:27<14:56:23, 632.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1356.8785807291667
INFO:root:current train perplexity2.891162633895874
INFO:root:current mean train loss 1358.5371513862115
INFO:root:current train perplexity2.8914432525634766
INFO:root:current mean train loss 1354.7153041569268
INFO:root:current train perplexity2.8976352214813232
INFO:root:current mean train loss 1349.5728042516332
INFO:root:current train perplexity2.89658784866333
INFO:root:current mean train loss 1349.7248263590136
INFO:root:current train perplexity2.9004130363464355
INFO:root:current mean train loss 1350.953600721669
INFO:root:current train perplexity2.9017343521118164
INFO:root:current mean train loss 1351.8335289444763
INFO:root:current train perplexity2.9040448665618896
INFO:root:current mean train loss 1352.6158986382522
INFO:root:current train perplexity2.90557861328125
INFO:root:current mean train loss 1352.472283606786
INFO:root:current train perplexity2.906310796737671
INFO:root:current mean train loss 1352.5069519938663
INFO:root:current train perplexity2.9088895320892334
INFO:root:current mean train loss 1353.60263136804
INFO:root:current train perplexity2.9098684787750244
INFO:root:current mean train loss 1354.496662529957
INFO:root:current train perplexity2.9100570678710938
INFO:root:current mean train loss 1354.6731866231185
INFO:root:current train perplexity2.9122395515441895
INFO:root:current mean train loss 1354.7584672079834
INFO:root:current train perplexity2.9114606380462646
INFO:root:current mean train loss 1354.880814878944
INFO:root:current train perplexity2.911949634552002
INFO:root:current mean train loss 1355.5041682220026
INFO:root:current train perplexity2.9131462574005127
INFO:root:current mean train loss 1356.2218595456327
INFO:root:current train perplexity2.9133031368255615
INFO:root:current mean train loss 1357.1172707360854
INFO:root:current train perplexity2.914724588394165
INFO:root:current mean train loss 1357.2927995371072
INFO:root:current train perplexity2.916109561920166
INFO:root:current mean train loss 1357.5745014983288
INFO:root:current train perplexity2.917313575744629

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.30s/it]
INFO:root:final mean train loss: 1357.5564107659245
INFO:root:final train perplexity: 2.9172770977020264
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.50s/it]
INFO:root:eval mean loss: 2229.536595121343
INFO:root:eval perplexity: 6.068541049957275
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2791.106130613503
INFO:root:eval perplexity: 9.802557945251465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [20:12:00<14:45:58, 632.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1363.4364976479974
INFO:root:current train perplexity2.8985562324523926
INFO:root:current mean train loss 1348.9285631681744
INFO:root:current train perplexity2.8906710147857666
INFO:root:current mean train loss 1351.697362020007
INFO:root:current train perplexity2.897127866744995
INFO:root:current mean train loss 1350.3740639082505
INFO:root:current train perplexity2.8974101543426514
INFO:root:current mean train loss 1350.323769862991
INFO:root:current train perplexity2.894827365875244
INFO:root:current mean train loss 1350.367810250999
INFO:root:current train perplexity2.896721124649048
INFO:root:current mean train loss 1350.130025985877
INFO:root:current train perplexity2.8992109298706055
INFO:root:current mean train loss 1351.598935059227
INFO:root:current train perplexity2.9008214473724365
INFO:root:current mean train loss 1353.589445584951
INFO:root:current train perplexity2.9040229320526123
INFO:root:current mean train loss 1352.6295299274668
INFO:root:current train perplexity2.9035017490386963
INFO:root:current mean train loss 1352.7580893522775
INFO:root:current train perplexity2.9032790660858154
INFO:root:current mean train loss 1352.3007936550957
INFO:root:current train perplexity2.904796600341797
INFO:root:current mean train loss 1351.894011370691
INFO:root:current train perplexity2.9047257900238037
INFO:root:current mean train loss 1352.8413808921407
INFO:root:current train perplexity2.906235694885254
INFO:root:current mean train loss 1353.1489739123047
INFO:root:current train perplexity2.907621145248413
INFO:root:current mean train loss 1353.0432197284274
INFO:root:current train perplexity2.9086575508117676
INFO:root:current mean train loss 1353.326992836204
INFO:root:current train perplexity2.9092767238616943
INFO:root:current mean train loss 1353.9557585756634
INFO:root:current train perplexity2.9099907875061035
INFO:root:current mean train loss 1354.629715659031
INFO:root:current train perplexity2.9103336334228516
INFO:root:current mean train loss 1355.314549245307
INFO:root:current train perplexity2.911376953125

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.63s/it]
INFO:root:final mean train loss: 1355.0484968792352
INFO:root:final train perplexity: 2.9115123748779297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2232.359594033965
INFO:root:eval perplexity: 6.082414150238037
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it]
INFO:root:eval mean loss: 2793.2259205486757
INFO:root:eval perplexity: 9.819565773010254
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [20:22:26<14:32:49, 630.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1350.08271373402
INFO:root:current train perplexity2.883626937866211
INFO:root:current mean train loss 1353.2009757833277
INFO:root:current train perplexity2.8857808113098145
INFO:root:current mean train loss 1350.3471069335938
INFO:root:current train perplexity2.8855936527252197
INFO:root:current mean train loss 1350.7158813476562
INFO:root:current train perplexity2.880275249481201
INFO:root:current mean train loss 1347.9615418481046
INFO:root:current train perplexity2.8827388286590576
INFO:root:current mean train loss 1348.1434020996094
INFO:root:current train perplexity2.8824920654296875
INFO:root:current mean train loss 1349.820003953091
INFO:root:current train perplexity2.8861565589904785
INFO:root:current mean train loss 1348.785179022
INFO:root:current train perplexity2.8877110481262207
INFO:root:current mean train loss 1349.166433248434
INFO:root:current train perplexity2.8900952339172363
INFO:root:current mean train loss 1348.4964405631247
INFO:root:current train perplexity2.8910958766937256
INFO:root:current mean train loss 1350.1750081006219
INFO:root:current train perplexity2.8929312229156494
INFO:root:current mean train loss 1349.8582005356297
INFO:root:current train perplexity2.8935658931732178
INFO:root:current mean train loss 1350.6046923524846
INFO:root:current train perplexity2.896395444869995
INFO:root:current mean train loss 1351.2403610185518
INFO:root:current train perplexity2.897008180618286
INFO:root:current mean train loss 1351.5748459190452
INFO:root:current train perplexity2.8971996307373047
INFO:root:current mean train loss 1351.4894775851849
INFO:root:current train perplexity2.899796485900879
INFO:root:current mean train loss 1352.7125469768216
INFO:root:current train perplexity2.902099847793579
INFO:root:current mean train loss 1352.9818259970987
INFO:root:current train perplexity2.9030842781066895
INFO:root:current mean train loss 1352.8197631189378
INFO:root:current train perplexity2.903803825378418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.68s/it]
INFO:root:final mean train loss: 1352.510604758366
INFO:root:final train perplexity: 2.905691146850586
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.78s/it]
INFO:root:eval mean loss: 2235.399014520307
INFO:root:eval perplexity: 6.097382068634033
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it]
INFO:root:eval mean loss: 2797.2875149774213
INFO:root:eval perplexity: 9.852234840393066
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [20:32:56<14:21:39, 630.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1294.5894287109375
INFO:root:current train perplexity2.794849157333374
INFO:root:current mean train loss 1339.1659272693453
INFO:root:current train perplexity2.871882200241089
INFO:root:current mean train loss 1334.547625881288
INFO:root:current train perplexity2.8681697845458984
INFO:root:current mean train loss 1338.856284419826
INFO:root:current train perplexity2.868199586868286
INFO:root:current mean train loss 1340.2853078583141
INFO:root:current train perplexity2.870934009552002
INFO:root:current mean train loss 1342.4097762608292
INFO:root:current train perplexity2.872358560562134
INFO:root:current mean train loss 1343.0002290079417
INFO:root:current train perplexity2.878664493560791
INFO:root:current mean train loss 1343.5792225939163
INFO:root:current train perplexity2.884937286376953
INFO:root:current mean train loss 1344.2355141207297
INFO:root:current train perplexity2.8877058029174805
INFO:root:current mean train loss 1345.7739588279092
INFO:root:current train perplexity2.8903417587280273
INFO:root:current mean train loss 1344.8398735084345
INFO:root:current train perplexity2.889857530593872
INFO:root:current mean train loss 1346.5304079910209
INFO:root:current train perplexity2.892763137817383
INFO:root:current mean train loss 1346.6216001645164
INFO:root:current train perplexity2.893073320388794
INFO:root:current mean train loss 1347.503051477191
INFO:root:current train perplexity2.8942835330963135
INFO:root:current mean train loss 1347.9099221008953
INFO:root:current train perplexity2.895071268081665
INFO:root:current mean train loss 1348.9066690945547
INFO:root:current train perplexity2.8967056274414062
INFO:root:current mean train loss 1348.9778169721087
INFO:root:current train perplexity2.897441864013672
INFO:root:current mean train loss 1348.4602347186583
INFO:root:current train perplexity2.8960812091827393
INFO:root:current mean train loss 1349.1125733774454
INFO:root:current train perplexity2.8962504863739014
INFO:root:current mean train loss 1349.2169884709235
INFO:root:current train perplexity2.8970532417297363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:23<00:00, 563.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:23<00:00, 563.56s/it]
INFO:root:final mean train loss: 1349.1225743527011
INFO:root:final train perplexity: 2.897937297821045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.22s/it]
INFO:root:eval mean loss: 2237.531482453042
INFO:root:eval perplexity: 6.107905387878418
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it]
INFO:root:eval mean loss: 2800.739577273105
INFO:root:eval perplexity: 9.88009262084961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [20:43:34<14:14:22, 632.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1342.9523814808238
INFO:root:current train perplexity2.8678395748138428
INFO:root:current mean train loss 1335.3242057425077
INFO:root:current train perplexity2.8752307891845703
INFO:root:current mean train loss 1335.323266381616
INFO:root:current train perplexity2.8642783164978027
INFO:root:current mean train loss 1336.2173943371506
INFO:root:current train perplexity2.8673298358917236
INFO:root:current mean train loss 1337.0862614780806
INFO:root:current train perplexity2.867743730545044
INFO:root:current mean train loss 1337.7874746505329
INFO:root:current train perplexity2.8709614276885986
INFO:root:current mean train loss 1337.9294141174512
INFO:root:current train perplexity2.872896432876587
INFO:root:current mean train loss 1338.6460114793433
INFO:root:current train perplexity2.874584913253784
INFO:root:current mean train loss 1340.4006145690769
INFO:root:current train perplexity2.8748080730438232
INFO:root:current mean train loss 1341.8237225249118
INFO:root:current train perplexity2.8783438205718994
INFO:root:current mean train loss 1342.0319785997126
INFO:root:current train perplexity2.878772735595703
INFO:root:current mean train loss 1342.8459896964823
INFO:root:current train perplexity2.8804984092712402
INFO:root:current mean train loss 1343.0549789903207
INFO:root:current train perplexity2.882901906967163
INFO:root:current mean train loss 1342.5637777677643
INFO:root:current train perplexity2.8836724758148193
INFO:root:current mean train loss 1343.0644116622989
INFO:root:current train perplexity2.8860340118408203
INFO:root:current mean train loss 1344.3657065352693
INFO:root:current train perplexity2.887632131576538
INFO:root:current mean train loss 1345.1547297655286
INFO:root:current train perplexity2.8890268802642822
INFO:root:current mean train loss 1345.358505408523
INFO:root:current train perplexity2.8903067111968994
INFO:root:current mean train loss 1346.270774665439
INFO:root:current train perplexity2.891035556793213
INFO:root:current mean train loss 1346.493883781949
INFO:root:current train perplexity2.891648054122925

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.62s/it]
INFO:root:final mean train loss: 1346.6814095655357
INFO:root:final train perplexity: 2.8923633098602295
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 2240.2771260146556
INFO:root:eval perplexity: 6.12148380279541
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2803.8824618725066
INFO:root:eval perplexity: 9.905518531799316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [20:54:04<14:02:35, 631.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1330.7764610877405
INFO:root:current train perplexity2.838644504547119
INFO:root:current mean train loss 1331.2510810687388
INFO:root:current train perplexity2.845893621444702
INFO:root:current mean train loss 1331.3041665304654
INFO:root:current train perplexity2.856198787689209
INFO:root:current mean train loss 1328.512252762606
INFO:root:current train perplexity2.8633739948272705
INFO:root:current mean train loss 1329.7748914992349
INFO:root:current train perplexity2.860405206680298
INFO:root:current mean train loss 1333.5373014262523
INFO:root:current train perplexity2.865661859512329
INFO:root:current mean train loss 1333.9308787839886
INFO:root:current train perplexity2.8638663291931152
INFO:root:current mean train loss 1334.7061554491922
INFO:root:current train perplexity2.8687455654144287
INFO:root:current mean train loss 1336.4513418131705
INFO:root:current train perplexity2.8681204319000244
INFO:root:current mean train loss 1336.8773406559922
INFO:root:current train perplexity2.8688228130340576
INFO:root:current mean train loss 1337.8461991604759
INFO:root:current train perplexity2.87119722366333
INFO:root:current mean train loss 1338.5695185606892
INFO:root:current train perplexity2.8721461296081543
INFO:root:current mean train loss 1338.680957366229
INFO:root:current train perplexity2.8728084564208984
INFO:root:current mean train loss 1339.6442282166029
INFO:root:current train perplexity2.8746285438537598
INFO:root:current mean train loss 1339.963712287993
INFO:root:current train perplexity2.8764851093292236
INFO:root:current mean train loss 1341.4614220533067
INFO:root:current train perplexity2.8787128925323486
INFO:root:current mean train loss 1342.1268808808247
INFO:root:current train perplexity2.8804972171783447
INFO:root:current mean train loss 1342.554981479568
INFO:root:current train perplexity2.881193161010742
INFO:root:current mean train loss 1342.8725413353045
INFO:root:current train perplexity2.882885217666626
INFO:root:current mean train loss 1343.3646449245946
INFO:root:current train perplexity2.8839454650878906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.90s/it]
INFO:root:final mean train loss: 1343.1402984018946
INFO:root:final train perplexity: 2.8842971324920654
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it]
INFO:root:eval mean loss: 2246.569428572418
INFO:root:eval perplexity: 6.152715682983398
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.62s/it]
INFO:root:eval mean loss: 2809.3165179313496
INFO:root:eval perplexity: 9.949640274047852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [21:04:34<13:51:29, 631.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1339.9000985281807
INFO:root:current train perplexity2.8745195865631104
INFO:root:current mean train loss 1336.5635548127004
INFO:root:current train perplexity2.8687000274658203
INFO:root:current mean train loss 1336.4194674491882
INFO:root:current train perplexity2.866464376449585
INFO:root:current mean train loss 1335.4783589223798
INFO:root:current train perplexity2.868222713470459
INFO:root:current mean train loss 1338.5728307355914
INFO:root:current train perplexity2.866903305053711
INFO:root:current mean train loss 1340.2154174365585
INFO:root:current train perplexity2.86698842048645
INFO:root:current mean train loss 1340.7265068612448
INFO:root:current train perplexity2.866722822189331
INFO:root:current mean train loss 1338.7693206948577
INFO:root:current train perplexity2.867832660675049
INFO:root:current mean train loss 1339.1955319698725
INFO:root:current train perplexity2.8714792728424072
INFO:root:current mean train loss 1339.1319800979422
INFO:root:current train perplexity2.8741660118103027
INFO:root:current mean train loss 1338.942360386704
INFO:root:current train perplexity2.8740854263305664
INFO:root:current mean train loss 1338.3168295889989
INFO:root:current train perplexity2.874664783477783
INFO:root:current mean train loss 1337.9354571688707
INFO:root:current train perplexity2.874711751937866
INFO:root:current mean train loss 1338.5836027702398
INFO:root:current train perplexity2.8747286796569824
INFO:root:current mean train loss 1338.6328928182413
INFO:root:current train perplexity2.8755722045898438
INFO:root:current mean train loss 1339.569509001195
INFO:root:current train perplexity2.8762927055358887
INFO:root:current mean train loss 1339.8519558376736
INFO:root:current train perplexity2.8767683506011963
INFO:root:current mean train loss 1340.2566061867126
INFO:root:current train perplexity2.8780128955841064
INFO:root:current mean train loss 1341.1071314976134
INFO:root:current train perplexity2.878720283508301
INFO:root:current mean train loss 1341.4162655071734
INFO:root:current train perplexity2.8796656131744385

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.82s/it]
INFO:root:final mean train loss: 1341.2093762865707
INFO:root:final train perplexity: 2.879908561706543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2247.730121152621
INFO:root:eval perplexity: 6.1584930419921875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.53s/it]
INFO:root:eval mean loss: 2813.3848734104886
INFO:root:eval perplexity: 9.98279857635498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [21:15:10<13:42:37, 632.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1325.0818473084332
INFO:root:current train perplexity2.860839366912842
INFO:root:current mean train loss 1326.7196002585351
INFO:root:current train perplexity2.8547019958496094
INFO:root:current mean train loss 1330.7420113252633
INFO:root:current train perplexity2.855156183242798
INFO:root:current mean train loss 1333.0533293450485
INFO:root:current train perplexity2.8583760261535645
INFO:root:current mean train loss 1331.2865328314945
INFO:root:current train perplexity2.8607873916625977
INFO:root:current mean train loss 1331.7587905537603
INFO:root:current train perplexity2.8626255989074707
INFO:root:current mean train loss 1332.2292587484328
INFO:root:current train perplexity2.8640761375427246
INFO:root:current mean train loss 1333.6054261122454
INFO:root:current train perplexity2.8645379543304443
INFO:root:current mean train loss 1335.0916950798253
INFO:root:current train perplexity2.864643096923828
INFO:root:current mean train loss 1336.4355566606982
INFO:root:current train perplexity2.8651938438415527
INFO:root:current mean train loss 1336.9541426318224
INFO:root:current train perplexity2.8658480644226074
INFO:root:current mean train loss 1337.7804085161579
INFO:root:current train perplexity2.8681321144104004
INFO:root:current mean train loss 1338.2047659970603
INFO:root:current train perplexity2.8675339221954346
INFO:root:current mean train loss 1337.7416576099326
INFO:root:current train perplexity2.868324041366577
INFO:root:current mean train loss 1337.4000370934637
INFO:root:current train perplexity2.8675413131713867
INFO:root:current mean train loss 1337.6430217066315
INFO:root:current train perplexity2.8683149814605713
INFO:root:current mean train loss 1338.3651572510787
INFO:root:current train perplexity2.8690929412841797
INFO:root:current mean train loss 1338.661471965551
INFO:root:current train perplexity2.869907855987549
INFO:root:current mean train loss 1338.035222075422
INFO:root:current train perplexity2.870518684387207
INFO:root:current mean train loss 1338.2212263011595
INFO:root:current train perplexity2.8720078468322754

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.61s/it]
INFO:root:final mean train loss: 1337.788363258104
INFO:root:final train perplexity: 2.8721489906311035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.61s/it]
INFO:root:eval mean loss: 2247.1303299707724
INFO:root:eval perplexity: 6.155506610870361
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.32s/it]
INFO:root:eval mean loss: 2813.227686672346
INFO:root:eval perplexity: 9.981513977050781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [21:25:35<13:29:00, 630.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1325.246253797743
INFO:root:current train perplexity2.859039783477783
INFO:root:current mean train loss 1322.665962942023
INFO:root:current train perplexity2.8467493057250977
INFO:root:current mean train loss 1328.5553664634967
INFO:root:current train perplexity2.853386402130127
INFO:root:current mean train loss 1331.9961541591547
INFO:root:current train perplexity2.858290910720825
INFO:root:current mean train loss 1331.8992917430644
INFO:root:current train perplexity2.8588879108428955
INFO:root:current mean train loss 1331.6084193342824
INFO:root:current train perplexity2.8577558994293213
INFO:root:current mean train loss 1331.0497746122055
INFO:root:current train perplexity2.8588554859161377
INFO:root:current mean train loss 1331.5269846469541
INFO:root:current train perplexity2.8589770793914795
INFO:root:current mean train loss 1331.6800799080495
INFO:root:current train perplexity2.85904860496521
INFO:root:current mean train loss 1331.6997607915089
INFO:root:current train perplexity2.8608481884002686
INFO:root:current mean train loss 1332.5915969708644
INFO:root:current train perplexity2.8617870807647705
INFO:root:current mean train loss 1332.8137757886357
INFO:root:current train perplexity2.863844633102417
INFO:root:current mean train loss 1332.654666587179
INFO:root:current train perplexity2.8642661571502686
INFO:root:current mean train loss 1333.0070674319918
INFO:root:current train perplexity2.8639256954193115
INFO:root:current mean train loss 1333.8631043709365
INFO:root:current train perplexity2.863877058029175
INFO:root:current mean train loss 1333.4037156207007
INFO:root:current train perplexity2.8626437187194824
INFO:root:current mean train loss 1333.3392700195313
INFO:root:current train perplexity2.8627777099609375
INFO:root:current mean train loss 1333.9101549542816
INFO:root:current train perplexity2.862793445587158
INFO:root:current mean train loss 1334.561246228092
INFO:root:current train perplexity2.86466121673584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.74s/it]
INFO:root:final mean train loss: 1334.8035373120254
INFO:root:final train perplexity: 2.8653955459594727
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2253.1031390250996
INFO:root:eval perplexity: 6.1853132247924805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it]
INFO:root:eval mean loss: 2819.3017976368574
INFO:root:eval perplexity: 10.031222343444824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [21:35:59<13:16:19, 628.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1327.424037388393
INFO:root:current train perplexity2.7687878608703613
INFO:root:current mean train loss 1327.109971661434
INFO:root:current train perplexity2.829458475112915
INFO:root:current mean train loss 1332.722388520909
INFO:root:current train perplexity2.8458871841430664
INFO:root:current mean train loss 1329.7126850538223
INFO:root:current train perplexity2.8463075160980225
INFO:root:current mean train loss 1324.6084068354576
INFO:root:current train perplexity2.8451919555664062
INFO:root:current mean train loss 1326.7848170052853
INFO:root:current train perplexity2.8484652042388916
INFO:root:current mean train loss 1327.6706583189612
INFO:root:current train perplexity2.8486580848693848
INFO:root:current mean train loss 1328.9605700804455
INFO:root:current train perplexity2.848773241043091
INFO:root:current mean train loss 1328.6910944942206
INFO:root:current train perplexity2.848037004470825
INFO:root:current mean train loss 1328.9903692310675
INFO:root:current train perplexity2.849464178085327
INFO:root:current mean train loss 1328.171994767099
INFO:root:current train perplexity2.8499882221221924
INFO:root:current mean train loss 1328.7103320003741
INFO:root:current train perplexity2.852036714553833
INFO:root:current mean train loss 1328.8555818192704
INFO:root:current train perplexity2.8514695167541504
INFO:root:current mean train loss 1329.0713308111071
INFO:root:current train perplexity2.8519551753997803
INFO:root:current mean train loss 1329.247603188022
INFO:root:current train perplexity2.852786064147949
INFO:root:current mean train loss 1330.7879253101412
INFO:root:current train perplexity2.854814052581787
INFO:root:current mean train loss 1330.5851827454107
INFO:root:current train perplexity2.8555877208709717
INFO:root:current mean train loss 1330.9446952118117
INFO:root:current train perplexity2.8563051223754883
INFO:root:current mean train loss 1331.2277650986182
INFO:root:current train perplexity2.857281446456909
INFO:root:current mean train loss 1331.9243212711392
INFO:root:current train perplexity2.8588578701019287

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.23s/it]
INFO:root:final mean train loss: 1332.1024804589006
INFO:root:final train perplexity: 2.8592982292175293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it]
INFO:root:eval mean loss: 2252.203133657469
INFO:root:eval perplexity: 6.180810928344727
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it]
INFO:root:eval mean loss: 2819.247968524906
INFO:root:eval perplexity: 10.030779838562012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [21:46:40<13:10:22, 632.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1310.1954345703125
INFO:root:current train perplexity2.8222625255584717
INFO:root:current mean train loss 1320.1968354255923
INFO:root:current train perplexity2.837252378463745
INFO:root:current mean train loss 1321.195528302874
INFO:root:current train perplexity2.827256441116333
INFO:root:current mean train loss 1324.2420194649403
INFO:root:current train perplexity2.8377511501312256
INFO:root:current mean train loss 1327.0798103764373
INFO:root:current train perplexity2.840277671813965
INFO:root:current mean train loss 1325.6097314266758
INFO:root:current train perplexity2.8389503955841064
INFO:root:current mean train loss 1326.2462375347432
INFO:root:current train perplexity2.8428897857666016
INFO:root:current mean train loss 1326.335447532696
INFO:root:current train perplexity2.843432903289795
INFO:root:current mean train loss 1325.4000421912924
INFO:root:current train perplexity2.8457589149475098
INFO:root:current mean train loss 1324.7654413660882
INFO:root:current train perplexity2.8464128971099854
INFO:root:current mean train loss 1325.7648210525513
INFO:root:current train perplexity2.848433256149292
INFO:root:current mean train loss 1326.3289663511664
INFO:root:current train perplexity2.8493945598602295
INFO:root:current mean train loss 1327.7411949805964
INFO:root:current train perplexity2.8502750396728516
INFO:root:current mean train loss 1328.0042100428095
INFO:root:current train perplexity2.850147008895874
INFO:root:current mean train loss 1328.4523529738522
INFO:root:current train perplexity2.8517093658447266
INFO:root:current mean train loss 1328.9383919783465
INFO:root:current train perplexity2.852388858795166
INFO:root:current mean train loss 1329.297552324868
INFO:root:current train perplexity2.852212429046631
INFO:root:current mean train loss 1329.5797677095418
INFO:root:current train perplexity2.852945566177368
INFO:root:current mean train loss 1329.910307834023
INFO:root:current train perplexity2.853628158569336
INFO:root:current mean train loss 1330.5823240537902
INFO:root:current train perplexity2.854719638824463

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.52s/it]
INFO:root:final mean train loss: 1330.338070714107
INFO:root:final train perplexity: 2.8553221225738525
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it]
INFO:root:eval mean loss: 2256.8311313061004
INFO:root:eval perplexity: 6.203989028930664
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it]
INFO:root:eval mean loss: 2824.9954777710827
INFO:root:eval perplexity: 10.078042030334473
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [21:57:04<12:56:29, 629.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1315.1128376286204
INFO:root:current train perplexity2.826711416244507
INFO:root:current mean train loss 1320.4148737048426
INFO:root:current train perplexity2.8271613121032715
INFO:root:current mean train loss 1320.3060495210386
INFO:root:current train perplexity2.8280208110809326
INFO:root:current mean train loss 1321.8977330003665
INFO:root:current train perplexity2.830765724182129
INFO:root:current mean train loss 1324.5427625314449
INFO:root:current train perplexity2.8322765827178955
INFO:root:current mean train loss 1324.1953129512765
INFO:root:current train perplexity2.829564332962036
INFO:root:current mean train loss 1324.0558910637676
INFO:root:current train perplexity2.8290395736694336
INFO:root:current mean train loss 1322.459714984765
INFO:root:current train perplexity2.827599048614502
INFO:root:current mean train loss 1322.1629969611604
INFO:root:current train perplexity2.827688455581665
INFO:root:current mean train loss 1322.5935116969563
INFO:root:current train perplexity2.8298659324645996
INFO:root:current mean train loss 1322.8190895688865
INFO:root:current train perplexity2.831831693649292
INFO:root:current mean train loss 1323.5586263805392
INFO:root:current train perplexity2.8342766761779785
INFO:root:current mean train loss 1324.1211314235936
INFO:root:current train perplexity2.8372445106506348
INFO:root:current mean train loss 1324.891282504748
INFO:root:current train perplexity2.839078187942505
INFO:root:current mean train loss 1325.423537138516
INFO:root:current train perplexity2.840071678161621
INFO:root:current mean train loss 1326.2214162184155
INFO:root:current train perplexity2.8422319889068604
INFO:root:current mean train loss 1326.5915207476387
INFO:root:current train perplexity2.8448657989501953
INFO:root:current mean train loss 1327.0037569049307
INFO:root:current train perplexity2.8455071449279785
INFO:root:current mean train loss 1327.3895473200494
INFO:root:current train perplexity2.846892833709717
INFO:root:current mean train loss 1327.5637257972494
INFO:root:current train perplexity2.847607374191284

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.98s/it]
INFO:root:final mean train loss: 1326.9773500474246
INFO:root:final train perplexity: 2.847764253616333
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.86s/it]
INFO:root:eval mean loss: 2256.322608893645
INFO:root:eval perplexity: 6.201437950134277
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 2825.8816935221353
INFO:root:eval perplexity: 10.085347175598145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [22:07:29<12:44:19, 628.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1301.6087393925109
INFO:root:current train perplexity2.80934739112854
INFO:root:current mean train loss 1307.2890771793413
INFO:root:current train perplexity2.824389934539795
INFO:root:current mean train loss 1311.8293220460878
INFO:root:current train perplexity2.8296473026275635
INFO:root:current mean train loss 1314.2770375512832
INFO:root:current train perplexity2.831310749053955
INFO:root:current mean train loss 1315.6056593182825
INFO:root:current train perplexity2.829331159591675
INFO:root:current mean train loss 1317.732217330659
INFO:root:current train perplexity2.829383134841919
INFO:root:current mean train loss 1317.9964408526666
INFO:root:current train perplexity2.8293445110321045
INFO:root:current mean train loss 1318.7044619759029
INFO:root:current train perplexity2.8306097984313965
INFO:root:current mean train loss 1319.392823972902
INFO:root:current train perplexity2.829484701156616
INFO:root:current mean train loss 1319.9489459394165
INFO:root:current train perplexity2.8308908939361572
INFO:root:current mean train loss 1321.2746637412865
INFO:root:current train perplexity2.832270860671997
INFO:root:current mean train loss 1321.2889413784205
INFO:root:current train perplexity2.833376884460449
INFO:root:current mean train loss 1321.377781029536
INFO:root:current train perplexity2.8340020179748535
INFO:root:current mean train loss 1321.910855322769
INFO:root:current train perplexity2.835245370864868
INFO:root:current mean train loss 1322.2908274123372
INFO:root:current train perplexity2.835981845855713
INFO:root:current mean train loss 1323.1623511651055
INFO:root:current train perplexity2.8365254402160645
INFO:root:current mean train loss 1323.6152506461333
INFO:root:current train perplexity2.837902307510376
INFO:root:current mean train loss 1324.29931453145
INFO:root:current train perplexity2.8391218185424805
INFO:root:current mean train loss 1324.645038255706
INFO:root:current train perplexity2.840006113052368
INFO:root:current mean train loss 1324.752526132274
INFO:root:current train perplexity2.841416835784912

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.63s/it]
INFO:root:final mean train loss: 1324.506766530401
INFO:root:final train perplexity: 2.8422207832336426
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it]
INFO:root:eval mean loss: 2262.9749664955953
INFO:root:eval perplexity: 6.23489236831665
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.05s/it]
INFO:root:eval mean loss: 2833.7644809154754
INFO:root:eval perplexity: 10.150577545166016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [22:17:57<12:33:46, 628.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1315.4753759765624
INFO:root:current train perplexity2.8320438861846924
INFO:root:current mean train loss 1314.5301764787946
INFO:root:current train perplexity2.812638998031616
INFO:root:current mean train loss 1317.7125244140625
INFO:root:current train perplexity2.826464891433716
INFO:root:current mean train loss 1317.6249095052083
INFO:root:current train perplexity2.826138734817505
INFO:root:current mean train loss 1315.2721949527138
INFO:root:current train perplexity2.8243279457092285
INFO:root:current mean train loss 1316.1773265540082
INFO:root:current train perplexity2.825786828994751
INFO:root:current mean train loss 1316.701951135706
INFO:root:current train perplexity2.82597279548645
INFO:root:current mean train loss 1317.783394342238
INFO:root:current train perplexity2.8270022869110107
INFO:root:current mean train loss 1319.1936625279018
INFO:root:current train perplexity2.8287229537963867
INFO:root:current mean train loss 1319.1212176983172
INFO:root:current train perplexity2.830176830291748
INFO:root:current mean train loss 1319.8751621547965
INFO:root:current train perplexity2.830751895904541
INFO:root:current mean train loss 1319.9359922498338
INFO:root:current train perplexity2.8303401470184326
INFO:root:current mean train loss 1319.8357668887868
INFO:root:current train perplexity2.8310461044311523
INFO:root:current mean train loss 1320.2558306107956
INFO:root:current train perplexity2.8330705165863037
INFO:root:current mean train loss 1321.4189218087924
INFO:root:current train perplexity2.8343989849090576
INFO:root:current mean train loss 1321.4854984344
INFO:root:current train perplexity2.836153507232666
INFO:root:current mean train loss 1321.6824317863807
INFO:root:current train perplexity2.8366925716400146
INFO:root:current mean train loss 1322.2741329913072
INFO:root:current train perplexity2.8369805812835693
INFO:root:current mean train loss 1322.6945596354167
INFO:root:current train perplexity2.8372116088867188
INFO:root:current mean train loss 1322.3267568853837
INFO:root:current train perplexity2.8369545936584473

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.52s/it]
INFO:root:final mean train loss: 1322.1098232990676
INFO:root:final train perplexity: 2.836852788925171
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.62s/it]
INFO:root:eval mean loss: 2266.336373403563
INFO:root:eval perplexity: 6.251863956451416
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.60s/it]
INFO:root:eval mean loss: 2840.187412559563
INFO:root:eval perplexity: 10.2040376663208
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [22:28:23<12:22:53, 627.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1309.1156417183254
INFO:root:current train perplexity2.7899956703186035
INFO:root:current mean train loss 1309.388053894043
INFO:root:current train perplexity2.8110005855560303
INFO:root:current mean train loss 1307.8366310433166
INFO:root:current train perplexity2.8174798488616943
INFO:root:current mean train loss 1310.989974975586
INFO:root:current train perplexity2.821810483932495
INFO:root:current mean train loss 1310.7023556096767
INFO:root:current train perplexity2.818026542663574
INFO:root:current mean train loss 1312.2929522540119
INFO:root:current train perplexity2.8199284076690674
INFO:root:current mean train loss 1313.3167442365877
INFO:root:current train perplexity2.822849750518799
INFO:root:current mean train loss 1313.0437830144708
INFO:root:current train perplexity2.823693037033081
INFO:root:current mean train loss 1313.1335412269216
INFO:root:current train perplexity2.824342966079712
INFO:root:current mean train loss 1314.3599980262018
INFO:root:current train perplexity2.8257806301116943
INFO:root:current mean train loss 1315.4896795810798
INFO:root:current train perplexity2.8283205032348633
INFO:root:current mean train loss 1315.4828765664324
INFO:root:current train perplexity2.828030586242676
INFO:root:current mean train loss 1316.2417266184332
INFO:root:current train perplexity2.828104019165039
INFO:root:current mean train loss 1316.9636923252851
INFO:root:current train perplexity2.827883005142212
INFO:root:current mean train loss 1317.7641423202392
INFO:root:current train perplexity2.8282535076141357
INFO:root:current mean train loss 1318.7220440581816
INFO:root:current train perplexity2.8295116424560547
INFO:root:current mean train loss 1319.642698896692
INFO:root:current train perplexity2.8294105529785156
INFO:root:current mean train loss 1319.7300474984306
INFO:root:current train perplexity2.8300936222076416
INFO:root:current mean train loss 1319.9489622862093
INFO:root:current train perplexity2.8312418460845947

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.43s/it]
INFO:root:final mean train loss: 1319.7358465843952
INFO:root:final train perplexity: 2.8315467834472656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.49s/it]
INFO:root:eval mean loss: 2266.0424562278367
INFO:root:eval perplexity: 6.250378131866455
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.59s/it]
INFO:root:eval mean loss: 2838.299197798925
INFO:root:eval perplexity: 10.188289642333984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [22:38:50<12:12:04, 627.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1298.7188449435764
INFO:root:current train perplexity2.7590553760528564
INFO:root:current mean train loss 1317.934633027523
INFO:root:current train perplexity2.825786828994751
INFO:root:current mean train loss 1312.0734676379336
INFO:root:current train perplexity2.8234574794769287
INFO:root:current mean train loss 1311.4844903544702
INFO:root:current train perplexity2.822064161300659
INFO:root:current mean train loss 1312.4837577838478
INFO:root:current train perplexity2.819272518157959
INFO:root:current mean train loss 1313.2362494627946
INFO:root:current train perplexity2.8203768730163574
INFO:root:current mean train loss 1312.9485284213363
INFO:root:current train perplexity2.819269895553589
INFO:root:current mean train loss 1313.756006065982
INFO:root:current train perplexity2.822544574737549
INFO:root:current mean train loss 1313.8831723735418
INFO:root:current train perplexity2.8237040042877197
INFO:root:current mean train loss 1313.4982054724028
INFO:root:current train perplexity2.823770761489868
INFO:root:current mean train loss 1314.0141630598055
INFO:root:current train perplexity2.8249826431274414
INFO:root:current mean train loss 1314.7569105666
INFO:root:current train perplexity2.824462652206421
INFO:root:current mean train loss 1314.4927388862502
INFO:root:current train perplexity2.8243155479431152
INFO:root:current mean train loss 1315.3804607114507
INFO:root:current train perplexity2.8246207237243652
INFO:root:current mean train loss 1315.4650585209756
INFO:root:current train perplexity2.825388193130493
INFO:root:current mean train loss 1316.516657460834
INFO:root:current train perplexity2.825742483139038
INFO:root:current mean train loss 1316.258382414349
INFO:root:current train perplexity2.82594633102417
INFO:root:current mean train loss 1316.6811945576499
INFO:root:current train perplexity2.8260841369628906
INFO:root:current mean train loss 1316.898118119796
INFO:root:current train perplexity2.825894355773926
INFO:root:current mean train loss 1317.4051232954916
INFO:root:current train perplexity2.8256795406341553

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.26s/it]
INFO:root:final mean train loss: 1317.265228902458
INFO:root:final train perplexity: 2.8260347843170166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.77s/it]
INFO:root:eval mean loss: 2269.0200121724015
INFO:root:eval perplexity: 6.265448093414307
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it]
INFO:root:eval mean loss: 2842.4928121363864
INFO:root:eval perplexity: 10.223294258117676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [22:49:27<12:04:52, 630.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1278.517582820012
INFO:root:current train perplexity2.825169801712036
INFO:root:current mean train loss 1301.419934469556
INFO:root:current train perplexity2.7953174114227295
INFO:root:current mean train loss 1304.597389963876
INFO:root:current train perplexity2.800083637237549
INFO:root:current mean train loss 1306.2796110375527
INFO:root:current train perplexity2.80307674407959
INFO:root:current mean train loss 1307.1414780594373
INFO:root:current train perplexity2.802649736404419
INFO:root:current mean train loss 1308.91700936089
INFO:root:current train perplexity2.8042399883270264
INFO:root:current mean train loss 1309.349313364242
INFO:root:current train perplexity2.8064332008361816
INFO:root:current mean train loss 1309.5223366813555
INFO:root:current train perplexity2.8075661659240723
INFO:root:current mean train loss 1310.3506664802608
INFO:root:current train perplexity2.808068037033081
INFO:root:current mean train loss 1311.1980157114774
INFO:root:current train perplexity2.810100555419922
INFO:root:current mean train loss 1311.7800847401163
INFO:root:current train perplexity2.810499906539917
INFO:root:current mean train loss 1312.155087296535
INFO:root:current train perplexity2.8119466304779053
INFO:root:current mean train loss 1312.3429394172806
INFO:root:current train perplexity2.8120899200439453
INFO:root:current mean train loss 1312.4855481085915
INFO:root:current train perplexity2.812509536743164
INFO:root:current mean train loss 1312.0150166173134
INFO:root:current train perplexity2.812608242034912
INFO:root:current mean train loss 1312.2438332093966
INFO:root:current train perplexity2.8133087158203125
INFO:root:current mean train loss 1313.1048614013973
INFO:root:current train perplexity2.8153672218322754
INFO:root:current mean train loss 1313.5426568554008
INFO:root:current train perplexity2.8175206184387207
INFO:root:current mean train loss 1313.97423253472
INFO:root:current train perplexity2.8173623085021973
INFO:root:current mean train loss 1314.250725703571
INFO:root:current train perplexity2.8184995651245117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.49s/it]
INFO:root:final mean train loss: 1314.1717253823024
INFO:root:final train perplexity: 2.819148540496826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.83s/it]
INFO:root:eval mean loss: 2270.8116130423036
INFO:root:eval perplexity: 6.274533271789551
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.13s/it]
INFO:root:eval mean loss: 2843.705758169188
INFO:root:eval perplexity: 10.233439445495605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [22:59:56<11:53:50, 629.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1301.1761644940043
INFO:root:current train perplexity2.7995238304138184
INFO:root:current mean train loss 1304.3644037846918
INFO:root:current train perplexity2.7975094318389893
INFO:root:current mean train loss 1303.7113680113491
INFO:root:current train perplexity2.7905213832855225
INFO:root:current mean train loss 1307.1581735861198
INFO:root:current train perplexity2.7953848838806152
INFO:root:current mean train loss 1307.332890702155
INFO:root:current train perplexity2.7992258071899414
INFO:root:current mean train loss 1308.5203218969411
INFO:root:current train perplexity2.8010764122009277
INFO:root:current mean train loss 1309.4346204315707
INFO:root:current train perplexity2.803097724914551
INFO:root:current mean train loss 1309.0228590214397
INFO:root:current train perplexity2.802854061126709
INFO:root:current mean train loss 1309.0897280510917
INFO:root:current train perplexity2.8021435737609863
INFO:root:current mean train loss 1308.3091442961709
INFO:root:current train perplexity2.803905725479126
INFO:root:current mean train loss 1308.6426423786927
INFO:root:current train perplexity2.805549383163452
INFO:root:current mean train loss 1309.1638055435942
INFO:root:current train perplexity2.806793451309204
INFO:root:current mean train loss 1310.2982581361875
INFO:root:current train perplexity2.8080408573150635
INFO:root:current mean train loss 1309.9637768391078
INFO:root:current train perplexity2.8085973262786865
INFO:root:current mean train loss 1310.4240852086311
INFO:root:current train perplexity2.810032367706299
INFO:root:current mean train loss 1310.908787053047
INFO:root:current train perplexity2.8115360736846924
INFO:root:current mean train loss 1311.5120077661386
INFO:root:current train perplexity2.811915159225464
INFO:root:current mean train loss 1311.3160051618304
INFO:root:current train perplexity2.8125815391540527
INFO:root:current mean train loss 1311.7690954265295
INFO:root:current train perplexity2.8125476837158203
INFO:root:current mean train loss 1312.0158639889185
INFO:root:current train perplexity2.8137662410736084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.15s/it]
INFO:root:final mean train loss: 1311.8103368992886
INFO:root:final train perplexity: 2.813903331756592
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.11s/it]
INFO:root:eval mean loss: 2272.901958925504
INFO:root:eval perplexity: 6.285149574279785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it]
INFO:root:eval mean loss: 2848.893297127798
INFO:root:eval perplexity: 10.276947975158691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [23:10:34<11:46:01, 632.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1295.8857991536458
INFO:root:current train perplexity2.800664186477661
INFO:root:current mean train loss 1309.6922248840333
INFO:root:current train perplexity2.7929232120513916
INFO:root:current mean train loss 1307.3919616699218
INFO:root:current train perplexity2.7899692058563232
INFO:root:current mean train loss 1307.8568617078993
INFO:root:current train perplexity2.7939414978027344
INFO:root:current mean train loss 1308.426537555197
INFO:root:current train perplexity2.796037197113037
INFO:root:current mean train loss 1309.310002354213
INFO:root:current train perplexity2.794590711593628
INFO:root:current mean train loss 1309.6387077562736
INFO:root:current train perplexity2.796870231628418
INFO:root:current mean train loss 1308.5855333830182
INFO:root:current train perplexity2.7995471954345703
INFO:root:current mean train loss 1309.03798828125
INFO:root:current train perplexity2.8003408908843994
INFO:root:current mean train loss 1308.2930620829263
INFO:root:current train perplexity2.8018064498901367
INFO:root:current mean train loss 1307.9303297510687
INFO:root:current train perplexity2.8013603687286377
INFO:root:current mean train loss 1308.6079622465988
INFO:root:current train perplexity2.8034298419952393
INFO:root:current mean train loss 1309.1148181733631
INFO:root:current train perplexity2.80322527885437
INFO:root:current mean train loss 1309.1927840288947
INFO:root:current train perplexity2.803959369659424
INFO:root:current mean train loss 1309.5661172109108
INFO:root:current train perplexity2.8038132190704346
INFO:root:current mean train loss 1309.4087086995444
INFO:root:current train perplexity2.8051655292510986
INFO:root:current mean train loss 1309.727513913074
INFO:root:current train perplexity2.8072259426116943
INFO:root:current mean train loss 1310.3962299693715
INFO:root:current train perplexity2.8084800243377686
INFO:root:current mean train loss 1310.461187875399
INFO:root:current train perplexity2.8096861839294434
INFO:root:current mean train loss 1310.8383384860292
INFO:root:current train perplexity2.810523748397827

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.70s/it]
INFO:root:final mean train loss: 1310.5328897003928
INFO:root:final train perplexity: 2.8110694885253906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 2273.4827647107713
INFO:root:eval perplexity: 6.288102149963379
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it]
INFO:root:eval mean loss: 2848.452002559148
INFO:root:eval perplexity: 10.273237228393555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [23:21:07<11:35:54, 632.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1293.173501547281
INFO:root:current train perplexity2.7744390964508057
INFO:root:current mean train loss 1299.01079667086
INFO:root:current train perplexity2.7908339500427246
INFO:root:current mean train loss 1297.7940294837263
INFO:root:current train perplexity2.7832257747650146
INFO:root:current mean train loss 1298.3003010635982
INFO:root:current train perplexity2.7851600646972656
INFO:root:current mean train loss 1297.7510766243286
INFO:root:current train perplexity2.7845542430877686
INFO:root:current mean train loss 1298.717279444121
INFO:root:current train perplexity2.7875711917877197
INFO:root:current mean train loss 1299.6019819025803
INFO:root:current train perplexity2.788681983947754
INFO:root:current mean train loss 1300.3274480360662
INFO:root:current train perplexity2.790971517562866
INFO:root:current mean train loss 1301.8657840393832
INFO:root:current train perplexity2.792344570159912
INFO:root:current mean train loss 1302.2018232082028
INFO:root:current train perplexity2.791154384613037
INFO:root:current mean train loss 1303.3613922770862
INFO:root:current train perplexity2.796571969985962
INFO:root:current mean train loss 1303.0492301169552
INFO:root:current train perplexity2.796865701675415
INFO:root:current mean train loss 1303.487487745173
INFO:root:current train perplexity2.797337055206299
INFO:root:current mean train loss 1303.9106115536492
INFO:root:current train perplexity2.7982022762298584
INFO:root:current mean train loss 1304.5571037814193
INFO:root:current train perplexity2.799516439437866
INFO:root:current mean train loss 1304.8771153693722
INFO:root:current train perplexity2.7993123531341553
INFO:root:current mean train loss 1305.2231951209144
INFO:root:current train perplexity2.800320863723755
INFO:root:current mean train loss 1306.2639838172
INFO:root:current train perplexity2.8015079498291016
INFO:root:current mean train loss 1306.7806265764434
INFO:root:current train perplexity2.8035378456115723
INFO:root:current mean train loss 1307.114775546223
INFO:root:current train perplexity2.8029046058654785

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.64s/it]
INFO:root:final mean train loss: 1306.837278796036
INFO:root:final train perplexity: 2.802888870239258
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.23s/it]
INFO:root:eval mean loss: 2279.908143388464
INFO:root:eval perplexity: 6.320864677429199
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it]
INFO:root:eval mean loss: 2855.4401665004434
INFO:root:eval perplexity: 10.332120895385742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [23:31:34<11:23:33, 630.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1291.2811214365859
INFO:root:current train perplexity2.7723729610443115
INFO:root:current mean train loss 1294.6959908082313
INFO:root:current train perplexity2.778691291809082
INFO:root:current mean train loss 1294.859543988494
INFO:root:current train perplexity2.777656078338623
INFO:root:current mean train loss 1297.3413683896138
INFO:root:current train perplexity2.775324583053589
INFO:root:current mean train loss 1297.7916623011292
INFO:root:current train perplexity2.779531478881836
INFO:root:current mean train loss 1299.71392061895
INFO:root:current train perplexity2.782710075378418
INFO:root:current mean train loss 1301.2429558042147
INFO:root:current train perplexity2.7863848209381104
INFO:root:current mean train loss 1301.28271807231
INFO:root:current train perplexity2.7884316444396973
INFO:root:current mean train loss 1302.694912153069
INFO:root:current train perplexity2.791036367416382
INFO:root:current mean train loss 1303.0949985803493
INFO:root:current train perplexity2.7920100688934326
INFO:root:current mean train loss 1303.4330191045617
INFO:root:current train perplexity2.7928757667541504
INFO:root:current mean train loss 1303.4379050607856
INFO:root:current train perplexity2.7938692569732666
INFO:root:current mean train loss 1303.6539452860861
INFO:root:current train perplexity2.793921709060669
INFO:root:current mean train loss 1304.2881697404334
INFO:root:current train perplexity2.7951271533966064
INFO:root:current mean train loss 1304.1590724878681
INFO:root:current train perplexity2.79472017288208
INFO:root:current mean train loss 1303.9808397855484
INFO:root:current train perplexity2.7954514026641846
INFO:root:current mean train loss 1304.9471990411932
INFO:root:current train perplexity2.796921968460083
INFO:root:current mean train loss 1305.1493505097287
INFO:root:current train perplexity2.7980282306671143
INFO:root:current mean train loss 1305.2495440087323
INFO:root:current train perplexity2.7984352111816406

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.70s/it]
INFO:root:final mean train loss: 1305.0714143030702
INFO:root:final train perplexity: 2.798987865447998
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.33s/it]
INFO:root:eval mean loss: 2281.565618853197
INFO:root:eval perplexity: 6.329343318939209
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it]
INFO:root:eval mean loss: 2858.0692974117633
INFO:root:eval perplexity: 10.354358673095703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [23:41:59<11:10:59, 629.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1306.1988303444602
INFO:root:current train perplexity2.80535626411438
INFO:root:current mean train loss 1291.5777158994933
INFO:root:current train perplexity2.7618937492370605
INFO:root:current mean train loss 1288.7066228062056
INFO:root:current train perplexity2.7700424194335938
INFO:root:current mean train loss 1292.6565608671624
INFO:root:current train perplexity2.7709403038024902
INFO:root:current mean train loss 1293.419059660603
INFO:root:current train perplexity2.7706408500671387
INFO:root:current mean train loss 1294.3756660118029
INFO:root:current train perplexity2.774136781692505
INFO:root:current mean train loss 1295.7747620927526
INFO:root:current train perplexity2.7786827087402344
INFO:root:current mean train loss 1296.1959286889614
INFO:root:current train perplexity2.780327796936035
INFO:root:current mean train loss 1297.5184410462102
INFO:root:current train perplexity2.7840194702148438
INFO:root:current mean train loss 1298.4634697555057
INFO:root:current train perplexity2.7852630615234375
INFO:root:current mean train loss 1298.7750439742906
INFO:root:current train perplexity2.7858619689941406
INFO:root:current mean train loss 1299.6548903059252
INFO:root:current train perplexity2.7866456508636475
INFO:root:current mean train loss 1299.37328012909
INFO:root:current train perplexity2.78572678565979
INFO:root:current mean train loss 1300.1355731885549
INFO:root:current train perplexity2.7870919704437256
INFO:root:current mean train loss 1300.7048424626817
INFO:root:current train perplexity2.7889084815979004
INFO:root:current mean train loss 1300.8072269825964
INFO:root:current train perplexity2.789994716644287
INFO:root:current mean train loss 1300.8644836160577
INFO:root:current train perplexity2.790414571762085
INFO:root:current mean train loss 1301.0456341064025
INFO:root:current train perplexity2.790249824523926
INFO:root:current mean train loss 1301.9751487491803
INFO:root:current train perplexity2.791240692138672
INFO:root:current mean train loss 1302.413265178367
INFO:root:current train perplexity2.7920775413513184

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.44s/it]
INFO:root:final mean train loss: 1302.041680917437
INFO:root:final train perplexity: 2.7923078536987305
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it]
INFO:root:eval mean loss: 2285.096431651014
INFO:root:eval perplexity: 6.34744119644165
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.43s/it]
INFO:root:eval mean loss: 2861.608049974374
INFO:root:eval perplexity: 10.384366989135742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [23:52:35<11:02:41, 631.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1283.3008510044642
INFO:root:current train perplexity2.7437052726745605
INFO:root:current mean train loss 1287.1491355895996
INFO:root:current train perplexity2.7571451663970947
INFO:root:current mean train loss 1291.653753447951
INFO:root:current train perplexity2.7626307010650635
INFO:root:current mean train loss 1294.7419013046638
INFO:root:current train perplexity2.768547773361206
INFO:root:current mean train loss 1292.679194940585
INFO:root:current train perplexity2.77164363861084
INFO:root:current mean train loss 1292.5929796623461
INFO:root:current train perplexity2.77116322517395
INFO:root:current mean train loss 1292.4490572206535
INFO:root:current train perplexity2.7720863819122314
INFO:root:current mean train loss 1292.5293993268695
INFO:root:current train perplexity2.7732505798339844
INFO:root:current mean train loss 1293.8049396017323
INFO:root:current train perplexity2.7756898403167725
INFO:root:current mean train loss 1294.5899642418171
INFO:root:current train perplexity2.775881052017212
INFO:root:current mean train loss 1295.1868523623693
INFO:root:current train perplexity2.777362585067749
INFO:root:current mean train loss 1295.7292571372175
INFO:root:current train perplexity2.7806904315948486
INFO:root:current mean train loss 1296.6326374464004
INFO:root:current train perplexity2.7818217277526855
INFO:root:current mean train loss 1297.2888304009496
INFO:root:current train perplexity2.784259796142578
INFO:root:current mean train loss 1297.3464077647661
INFO:root:current train perplexity2.785738945007324
INFO:root:current mean train loss 1297.696765120741
INFO:root:current train perplexity2.7860987186431885
INFO:root:current mean train loss 1297.8301246886758
INFO:root:current train perplexity2.7862064838409424
INFO:root:current mean train loss 1298.3118389270924
INFO:root:current train perplexity2.7857882976531982
INFO:root:current mean train loss 1299.6264980324509
INFO:root:current train perplexity2.7866101264953613
INFO:root:current mean train loss 1299.9407377757473
INFO:root:current train perplexity2.7871978282928467

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.23s/it]
INFO:root:final mean train loss: 1299.9738906806488
INFO:root:final train perplexity: 2.7877578735351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2284.601405799812
INFO:root:eval perplexity: 6.3449015617370605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it]
INFO:root:eval mean loss: 2860.889450614334
INFO:root:eval perplexity: 10.378268241882324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [24:03:10<10:53:19, 632.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1286.6908582899305
INFO:root:current train perplexity2.776778221130371
INFO:root:current mean train loss 1291.608134933998
INFO:root:current train perplexity2.7690746784210205
INFO:root:current mean train loss 1289.5783561862245
INFO:root:current train perplexity2.763859987258911
INFO:root:current mean train loss 1289.4636506453805
INFO:root:current train perplexity2.7642109394073486
INFO:root:current mean train loss 1289.0307392248947
INFO:root:current train perplexity2.766415596008301
INFO:root:current mean train loss 1290.9569297860521
INFO:root:current train perplexity2.7685275077819824
INFO:root:current mean train loss 1291.9865857028221
INFO:root:current train perplexity2.7692925930023193
INFO:root:current mean train loss 1292.0113884228188
INFO:root:current train perplexity2.7706503868103027
INFO:root:current mean train loss 1292.5875355376295
INFO:root:current train perplexity2.7717642784118652
INFO:root:current mean train loss 1293.3187431537285
INFO:root:current train perplexity2.773000717163086
INFO:root:current mean train loss 1294.221960390812
INFO:root:current train perplexity2.7748541831970215
INFO:root:current mean train loss 1295.5208467308612
INFO:root:current train perplexity2.775611162185669
INFO:root:current mean train loss 1296.1065845412902
INFO:root:current train perplexity2.778258800506592
INFO:root:current mean train loss 1296.7627248998026
INFO:root:current train perplexity2.7793188095092773
INFO:root:current mean train loss 1297.406660899654
INFO:root:current train perplexity2.7801501750946045
INFO:root:current mean train loss 1297.9078012015827
INFO:root:current train perplexity2.7807133197784424
INFO:root:current mean train loss 1298.3334197348736
INFO:root:current train perplexity2.782513380050659
INFO:root:current mean train loss 1298.640721187209
INFO:root:current train perplexity2.782566547393799
INFO:root:current mean train loss 1299.1447881600398
INFO:root:current train perplexity2.7837679386138916
INFO:root:current mean train loss 1299.0288325684849
INFO:root:current train perplexity2.7850561141967773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.16s/it]
INFO:root:final mean train loss: 1298.752455318449
INFO:root:final train perplexity: 2.7850735187530518
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.25s/it]
INFO:root:eval mean loss: 2288.066359066794
INFO:root:eval perplexity: 6.362705230712891
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2865.237645358904
INFO:root:eval perplexity: 10.415239334106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [24:13:34<10:40:20, 629.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1286.3674611737652
INFO:root:current train perplexity2.7697629928588867
INFO:root:current mean train loss 1290.615328565056
INFO:root:current train perplexity2.7652125358581543
INFO:root:current mean train loss 1292.0642625648557
INFO:root:current train perplexity2.763563871383667
INFO:root:current mean train loss 1293.0771403444405
INFO:root:current train perplexity2.761748790740967
INFO:root:current mean train loss 1294.505471497903
INFO:root:current train perplexity2.7664308547973633
INFO:root:current mean train loss 1293.6542884039286
INFO:root:current train perplexity2.7631101608276367
INFO:root:current mean train loss 1292.4743143410121
INFO:root:current train perplexity2.7646257877349854
INFO:root:current mean train loss 1292.7513788178212
INFO:root:current train perplexity2.7670505046844482
INFO:root:current mean train loss 1294.467227803029
INFO:root:current train perplexity2.7689478397369385
INFO:root:current mean train loss 1295.4925068877096
INFO:root:current train perplexity2.768375873565674
INFO:root:current mean train loss 1296.8185008708142
INFO:root:current train perplexity2.7706193923950195
INFO:root:current mean train loss 1296.772896692798
INFO:root:current train perplexity2.7726550102233887
INFO:root:current mean train loss 1295.807160923107
INFO:root:current train perplexity2.7712838649749756
INFO:root:current mean train loss 1295.535704939026
INFO:root:current train perplexity2.772003412246704
INFO:root:current mean train loss 1295.4436127836173
INFO:root:current train perplexity2.772583484649658
INFO:root:current mean train loss 1295.359433690656
INFO:root:current train perplexity2.7735214233398438
INFO:root:current mean train loss 1296.3243071077534
INFO:root:current train perplexity2.7750244140625
INFO:root:current mean train loss 1296.573120740702
INFO:root:current train perplexity2.776507616043091
INFO:root:current mean train loss 1296.2765893659582
INFO:root:current train perplexity2.777937173843384
INFO:root:current mean train loss 1296.3583334826549
INFO:root:current train perplexity2.778643846511841

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.43s/it]
INFO:root:final mean train loss: 1296.0322093261473
INFO:root:final train perplexity: 2.7791054248809814
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.32s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.32s/it]
INFO:root:eval mean loss: 2290.90399559508
INFO:root:eval perplexity: 6.37732458114624
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.08s/it]
INFO:root:eval mean loss: 2869.3579889392176
INFO:root:eval perplexity: 10.450395584106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [24:24:09<10:31:25, 631.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1284.6041367929192
INFO:root:current train perplexity2.7782933712005615
INFO:root:current mean train loss 1290.13957205831
INFO:root:current train perplexity2.7690541744232178
INFO:root:current mean train loss 1288.0662014868951
INFO:root:current train perplexity2.7679426670074463
INFO:root:current mean train loss 1287.600519587937
INFO:root:current train perplexity2.7635483741760254
INFO:root:current mean train loss 1290.0334462462488
INFO:root:current train perplexity2.768810272216797
INFO:root:current mean train loss 1288.6979868729086
INFO:root:current train perplexity2.7659058570861816
INFO:root:current mean train loss 1289.9731792287025
INFO:root:current train perplexity2.7665624618530273
INFO:root:current mean train loss 1290.5366985041921
INFO:root:current train perplexity2.7661406993865967
INFO:root:current mean train loss 1289.72985201023
INFO:root:current train perplexity2.7665421962738037
INFO:root:current mean train loss 1290.923701939958
INFO:root:current train perplexity2.7690482139587402
INFO:root:current mean train loss 1291.8384566284972
INFO:root:current train perplexity2.769289493560791
INFO:root:current mean train loss 1292.343824132607
INFO:root:current train perplexity2.770221710205078
INFO:root:current mean train loss 1292.4592410185264
INFO:root:current train perplexity2.770427942276001
INFO:root:current mean train loss 1292.3202611578815
INFO:root:current train perplexity2.769559383392334
INFO:root:current mean train loss 1293.6068697936475
INFO:root:current train perplexity2.7704737186431885
INFO:root:current mean train loss 1293.9579264838308
INFO:root:current train perplexity2.772451639175415
INFO:root:current mean train loss 1294.1294066001433
INFO:root:current train perplexity2.7732112407684326
INFO:root:current mean train loss 1294.0432130278598
INFO:root:current train perplexity2.773585557937622
INFO:root:current mean train loss 1294.3550345201072
INFO:root:current train perplexity2.774596691131592
INFO:root:current mean train loss 1294.166781355601
INFO:root:current train perplexity2.7741856575012207

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.55s/it]
INFO:root:final mean train loss: 1293.8719236803847
INFO:root:final train perplexity: 2.77437424659729
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.61s/it]
INFO:root:eval mean loss: 2292.5474554832945
INFO:root:eval perplexity: 6.385807514190674
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it]
INFO:root:eval mean loss: 2870.0932266560008
INFO:root:eval perplexity: 10.456679344177246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [24:34:37<10:19:46, 630.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1287.6414972941081
INFO:root:current train perplexity2.760485887527466
INFO:root:current mean train loss 1284.0921282087054
INFO:root:current train perplexity2.760319709777832
INFO:root:current mean train loss 1284.302578075512
INFO:root:current train perplexity2.7559263706207275
INFO:root:current mean train loss 1285.6289605034722
INFO:root:current train perplexity2.7582151889801025
INFO:root:current mean train loss 1286.4181447182932
INFO:root:current train perplexity2.7586679458618164
INFO:root:current mean train loss 1288.0970166097552
INFO:root:current train perplexity2.7594501972198486
INFO:root:current mean train loss 1287.694628134541
INFO:root:current train perplexity2.7587690353393555
INFO:root:current mean train loss 1287.4742957647122
INFO:root:current train perplexity2.760035753250122
INFO:root:current mean train loss 1287.999645778111
INFO:root:current train perplexity2.761200189590454
INFO:root:current mean train loss 1289.1334961427742
INFO:root:current train perplexity2.7627992630004883
INFO:root:current mean train loss 1289.934545029689
INFO:root:current train perplexity2.763546943664551
INFO:root:current mean train loss 1289.382886497472
INFO:root:current train perplexity2.7634894847869873
INFO:root:current mean train loss 1289.5983167106722
INFO:root:current train perplexity2.764174461364746
INFO:root:current mean train loss 1290.0871963282368
INFO:root:current train perplexity2.7640085220336914
INFO:root:current mean train loss 1290.6135307760799
INFO:root:current train perplexity2.764636754989624
INFO:root:current mean train loss 1291.824140964594
INFO:root:current train perplexity2.7667956352233887
INFO:root:current mean train loss 1291.994159122683
INFO:root:current train perplexity2.7685070037841797
INFO:root:current mean train loss 1292.874793921388
INFO:root:current train perplexity2.769256591796875
INFO:root:current mean train loss 1292.8045152752711
INFO:root:current train perplexity2.770423650741577

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.52s/it]
INFO:root:final mean train loss: 1292.038886104397
INFO:root:final train perplexity: 2.7703661918640137
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.12s/it]
INFO:root:eval mean loss: 2295.2072173855827
INFO:root:eval perplexity: 6.399557590484619
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.95s/it]
INFO:root:eval mean loss: 2875.2294753054357
INFO:root:eval perplexity: 10.500699043273926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [24:45:22<10:13:29, 634.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1310.749502328726
INFO:root:current train perplexity2.7454421520233154
INFO:root:current mean train loss 1297.7879055327019
INFO:root:current train perplexity2.7548537254333496
INFO:root:current mean train loss 1289.7934851131529
INFO:root:current train perplexity2.7401421070098877
INFO:root:current mean train loss 1288.2305685902556
INFO:root:current train perplexity2.7439420223236084
INFO:root:current mean train loss 1287.8174383796156
INFO:root:current train perplexity2.745224952697754
INFO:root:current mean train loss 1286.8569685729624
INFO:root:current train perplexity2.7466766834259033
INFO:root:current mean train loss 1287.5454402257724
INFO:root:current train perplexity2.7501540184020996
INFO:root:current mean train loss 1286.7054934722344
INFO:root:current train perplexity2.7532732486724854
INFO:root:current mean train loss 1287.2664386519355
INFO:root:current train perplexity2.754936695098877
INFO:root:current mean train loss 1287.056455848251
INFO:root:current train perplexity2.755237579345703
INFO:root:current mean train loss 1286.853861711809
INFO:root:current train perplexity2.7560203075408936
INFO:root:current mean train loss 1287.3535729859825
INFO:root:current train perplexity2.7571890354156494
INFO:root:current mean train loss 1287.7273589257652
INFO:root:current train perplexity2.7587015628814697
INFO:root:current mean train loss 1288.097707662706
INFO:root:current train perplexity2.7588982582092285
INFO:root:current mean train loss 1288.918261701472
INFO:root:current train perplexity2.7600107192993164
INFO:root:current mean train loss 1289.8144880598613
INFO:root:current train perplexity2.760859727859497
INFO:root:current mean train loss 1290.0394324948902
INFO:root:current train perplexity2.762800693511963
INFO:root:current mean train loss 1290.1122616457785
INFO:root:current train perplexity2.763465404510498
INFO:root:current mean train loss 1290.332029095422
INFO:root:current train perplexity2.764047622680664
INFO:root:current mean train loss 1290.3608469267633
INFO:root:current train perplexity2.7649283409118652

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.38s/it]
INFO:root:final mean train loss: 1290.1590040613771
INFO:root:final train perplexity: 2.7662620544433594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2296.635682018091
INFO:root:eval perplexity: 6.406954765319824
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.08s/it]
INFO:root:eval mean loss: 2875.472890001662
INFO:root:eval perplexity: 10.502787590026855
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [24:55:47<10:00:22, 631.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1288.190535481771
INFO:root:current train perplexity2.7380287647247314
INFO:root:current mean train loss 1282.8731276292067
INFO:root:current train perplexity2.736016273498535
INFO:root:current mean train loss 1287.085454526155
INFO:root:current train perplexity2.7359137535095215
INFO:root:current mean train loss 1286.0107114849668
INFO:root:current train perplexity2.7395212650299072
INFO:root:current mean train loss 1287.3087958757267
INFO:root:current train perplexity2.7419004440307617
INFO:root:current mean train loss 1286.0494529868072
INFO:root:current train perplexity2.7455830574035645
INFO:root:current mean train loss 1286.7176224965897
INFO:root:current train perplexity2.749906063079834
INFO:root:current mean train loss 1286.089105809878
INFO:root:current train perplexity2.7514023780822754
INFO:root:current mean train loss 1286.9107007130083
INFO:root:current train perplexity2.753169059753418
INFO:root:current mean train loss 1286.4106824649277
INFO:root:current train perplexity2.7527825832366943
INFO:root:current mean train loss 1286.8133036493098
INFO:root:current train perplexity2.7549731731414795
INFO:root:current mean train loss 1287.9425785571073
INFO:root:current train perplexity2.7563936710357666
INFO:root:current mean train loss 1287.5584828942772
INFO:root:current train perplexity2.756394624710083
INFO:root:current mean train loss 1287.9558280772733
INFO:root:current train perplexity2.7571330070495605
INFO:root:current mean train loss 1288.2641703145487
INFO:root:current train perplexity2.759036064147949
INFO:root:current mean train loss 1287.7801173789828
INFO:root:current train perplexity2.7595696449279785
INFO:root:current mean train loss 1288.0972503474884
INFO:root:current train perplexity2.7607929706573486
INFO:root:current mean train loss 1288.309284047033
INFO:root:current train perplexity2.7616255283355713
INFO:root:current mean train loss 1288.4703137006916
INFO:root:current train perplexity2.7616589069366455
INFO:root:current mean train loss 1288.786894645098
INFO:root:current train perplexity2.7617595195770264

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.51s/it]
INFO:root:final mean train loss: 1288.2452913670004
INFO:root:final train perplexity: 2.7620902061462402
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 44.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:44<00:00, 44.00s/it]
INFO:root:eval mean loss: 2299.3458966194316
INFO:root:eval perplexity: 6.421013832092285
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 2879.707780121066
INFO:root:eval perplexity: 10.539227485656738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [25:06:24<9:51:09, 633.38s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1266.116631690492
INFO:root:current train perplexity2.736762046813965
INFO:root:current mean train loss 1275.5083273543792
INFO:root:current train perplexity2.7356808185577393
INFO:root:current mean train loss 1275.807351301556
INFO:root:current train perplexity2.7338149547576904
INFO:root:current mean train loss 1277.558472031475
INFO:root:current train perplexity2.736313819885254
INFO:root:current mean train loss 1279.3417373418274
INFO:root:current train perplexity2.7402374744415283
INFO:root:current mean train loss 1279.1659759019367
INFO:root:current train perplexity2.738269329071045
INFO:root:current mean train loss 1280.4186645696484
INFO:root:current train perplexity2.743556499481201
INFO:root:current mean train loss 1282.2559948452665
INFO:root:current train perplexity2.745223045349121
INFO:root:current mean train loss 1283.4805245247474
INFO:root:current train perplexity2.746819257736206
INFO:root:current mean train loss 1284.3576513207827
INFO:root:current train perplexity2.748997449874878
INFO:root:current mean train loss 1283.971549339258
INFO:root:current train perplexity2.749202013015747
INFO:root:current mean train loss 1284.181725020604
INFO:root:current train perplexity2.750438928604126
INFO:root:current mean train loss 1284.7229784099025
INFO:root:current train perplexity2.7510836124420166
INFO:root:current mean train loss 1285.641020391814
INFO:root:current train perplexity2.751865863800049
INFO:root:current mean train loss 1285.72087581189
INFO:root:current train perplexity2.7525994777679443
INFO:root:current mean train loss 1285.6020293972456
INFO:root:current train perplexity2.752307415008545
INFO:root:current mean train loss 1286.086141172871
INFO:root:current train perplexity2.7535855770111084
INFO:root:current mean train loss 1286.0376168716684
INFO:root:current train perplexity2.7550363540649414
INFO:root:current mean train loss 1285.7118716980872
INFO:root:current train perplexity2.754661798477173
INFO:root:current mean train loss 1285.7364314490242
INFO:root:current train perplexity2.7550792694091797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:22<00:00, 562.38s/it]
INFO:root:final mean train loss: 1285.2355352219943
INFO:root:final train perplexity: 2.7555418014526367
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.62s/it]
INFO:root:eval mean loss: 2301.2713324364195
INFO:root:eval perplexity: 6.4310197830200195
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.42s/it]
INFO:root:eval mean loss: 2882.1840876586048
INFO:root:eval perplexity: 10.560591697692871
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [25:17:02<9:41:47, 634.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1280.17134475708
INFO:root:current train perplexity2.7351579666137695
INFO:root:current mean train loss 1278.172710139577
INFO:root:current train perplexity2.7353947162628174
INFO:root:current mean train loss 1271.2014030687737
INFO:root:current train perplexity2.7328743934631348
INFO:root:current mean train loss 1273.5348872928828
INFO:root:current train perplexity2.7341203689575195
INFO:root:current mean train loss 1274.0273732152477
INFO:root:current train perplexity2.7379655838012695
INFO:root:current mean train loss 1275.4193937693926
INFO:root:current train perplexity2.7402026653289795
INFO:root:current mean train loss 1276.059182776026
INFO:root:current train perplexity2.7417752742767334
INFO:root:current mean train loss 1277.505302069699
INFO:root:current train perplexity2.740802764892578
INFO:root:current mean train loss 1277.0703019036187
INFO:root:current train perplexity2.7414228916168213
INFO:root:current mean train loss 1277.9264903721473
INFO:root:current train perplexity2.7437632083892822
INFO:root:current mean train loss 1278.9792839566567
INFO:root:current train perplexity2.746425151824951
INFO:root:current mean train loss 1279.0491263792687
INFO:root:current train perplexity2.746699333190918
INFO:root:current mean train loss 1280.11105394967
INFO:root:current train perplexity2.7478420734405518
INFO:root:current mean train loss 1280.4852525817334
INFO:root:current train perplexity2.7484614849090576
INFO:root:current mean train loss 1281.252609086167
INFO:root:current train perplexity2.74894118309021
INFO:root:current mean train loss 1281.9946353063558
INFO:root:current train perplexity2.7499818801879883
INFO:root:current mean train loss 1282.2805632811326
INFO:root:current train perplexity2.7502992153167725
INFO:root:current mean train loss 1282.6729114212417
INFO:root:current train perplexity2.7513997554779053
INFO:root:current mean train loss 1283.5487406980326
INFO:root:current train perplexity2.7525386810302734
INFO:root:current mean train loss 1283.9334921283294
INFO:root:current train perplexity2.75227689743042

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.88s/it]
INFO:root:final mean train loss: 1283.7584199275384
INFO:root:final train perplexity: 2.752333879470825
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it]
INFO:root:eval mean loss: 2300.3048498275434
INFO:root:eval perplexity: 6.425994873046875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.69s/it]
INFO:root:eval mean loss: 2881.1325025626106
INFO:root:eval perplexity: 10.55151653289795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [25:27:26<9:28:24, 631.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1286.6318751205633
INFO:root:current train perplexity2.7400102615356445
INFO:root:current mean train loss 1282.37613100505
INFO:root:current train perplexity2.736224412918091
INFO:root:current mean train loss 1280.2629707309275
INFO:root:current train perplexity2.734981060028076
INFO:root:current mean train loss 1279.2307394833701
INFO:root:current train perplexity2.7396743297576904
INFO:root:current mean train loss 1280.1332124642672
INFO:root:current train perplexity2.74064564704895
INFO:root:current mean train loss 1280.851294197437
INFO:root:current train perplexity2.7435317039489746
INFO:root:current mean train loss 1281.1013825314335
INFO:root:current train perplexity2.7457876205444336
INFO:root:current mean train loss 1280.3513997916834
INFO:root:current train perplexity2.7447972297668457
INFO:root:current mean train loss 1281.5127895324915
INFO:root:current train perplexity2.745455741882324
INFO:root:current mean train loss 1280.597459518946
INFO:root:current train perplexity2.7457656860351562
INFO:root:current mean train loss 1280.9964250675735
INFO:root:current train perplexity2.74656343460083
INFO:root:current mean train loss 1281.3448090452343
INFO:root:current train perplexity2.7470669746398926
INFO:root:current mean train loss 1280.6510826426497
INFO:root:current train perplexity2.7475225925445557
INFO:root:current mean train loss 1280.6576550018951
INFO:root:current train perplexity2.7473676204681396
INFO:root:current mean train loss 1281.1631960562963
INFO:root:current train perplexity2.7473294734954834
INFO:root:current mean train loss 1281.178128459045
INFO:root:current train perplexity2.7472822666168213
INFO:root:current mean train loss 1281.5450254278053
INFO:root:current train perplexity2.747396230697632
INFO:root:current mean train loss 1282.3154440124272
INFO:root:current train perplexity2.74826979637146
INFO:root:current mean train loss 1282.2018549106401
INFO:root:current train perplexity2.748274087905884
INFO:root:current mean train loss 1282.2430927675218
INFO:root:current train perplexity2.7481613159179688

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.04s/it]
INFO:root:final mean train loss: 1281.8750564490554
INFO:root:final train perplexity: 2.7482481002807617
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it]
INFO:root:eval mean loss: 2305.9039375900375
INFO:root:eval perplexity: 6.455158710479736
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.01s/it]
INFO:root:eval mean loss: 2886.7238466519834
INFO:root:eval perplexity: 10.599872589111328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [25:37:47<9:15:06, 628.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1268.8802004444356
INFO:root:current train perplexity2.734142541885376
INFO:root:current mean train loss 1272.483799789891
INFO:root:current train perplexity2.72780179977417
INFO:root:current mean train loss 1271.298392276636
INFO:root:current train perplexity2.734103202819824
INFO:root:current mean train loss 1269.9022965167635
INFO:root:current train perplexity2.7296910285949707
INFO:root:current mean train loss 1273.380812311747
INFO:root:current train perplexity2.730393409729004
INFO:root:current mean train loss 1274.696647516461
INFO:root:current train perplexity2.731687545776367
INFO:root:current mean train loss 1275.3482214810172
INFO:root:current train perplexity2.732602596282959
INFO:root:current mean train loss 1275.8521307847254
INFO:root:current train perplexity2.7340471744537354
INFO:root:current mean train loss 1276.0629731923807
INFO:root:current train perplexity2.735288143157959
INFO:root:current mean train loss 1276.635321913358
INFO:root:current train perplexity2.7365715503692627
INFO:root:current mean train loss 1277.7076034684867
INFO:root:current train perplexity2.737983465194702
INFO:root:current mean train loss 1277.9441294980566
INFO:root:current train perplexity2.7389402389526367
INFO:root:current mean train loss 1278.18847026149
INFO:root:current train perplexity2.740083932876587
INFO:root:current mean train loss 1278.1044322874754
INFO:root:current train perplexity2.740386724472046
INFO:root:current mean train loss 1278.5751319141668
INFO:root:current train perplexity2.7400450706481934
INFO:root:current mean train loss 1279.09814048261
INFO:root:current train perplexity2.7407758235931396
INFO:root:current mean train loss 1279.4350023752668
INFO:root:current train perplexity2.7415549755096436
INFO:root:current mean train loss 1279.573603374409
INFO:root:current train perplexity2.7424588203430176
INFO:root:current mean train loss 1279.9705275444135
INFO:root:current train perplexity2.7430930137634277

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.28s/it]
INFO:root:final mean train loss: 1280.070695454823
INFO:root:final train perplexity: 2.7443408966064453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.74s/it]
INFO:root:eval mean loss: 2308.4480989063886
INFO:root:eval perplexity: 6.468454837799072
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.56s/it]
INFO:root:eval mean loss: 2890.3798087911405
INFO:root:eval perplexity: 10.631613731384277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [25:48:10<9:03:10, 626.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1284.0232747395833
INFO:root:current train perplexity2.73356032371521
INFO:root:current mean train loss 1283.2978494395381
INFO:root:current train perplexity2.739417314529419
INFO:root:current mean train loss 1270.099055800327
INFO:root:current train perplexity2.736107587814331
INFO:root:current mean train loss 1271.279679361979
INFO:root:current train perplexity2.7363057136535645
INFO:root:current mean train loss 1272.8312505882907
INFO:root:current train perplexity2.734653949737549
INFO:root:current mean train loss 1273.5510310793386
INFO:root:current train perplexity2.735163927078247
INFO:root:current mean train loss 1274.6102001159172
INFO:root:current train perplexity2.7347285747528076
INFO:root:current mean train loss 1275.7472994290865
INFO:root:current train perplexity2.7342615127563477
INFO:root:current mean train loss 1276.4981893153279
INFO:root:current train perplexity2.733030319213867
INFO:root:current mean train loss 1276.8844543790556
INFO:root:current train perplexity2.734311103820801
INFO:root:current mean train loss 1276.7099723628003
INFO:root:current train perplexity2.7339956760406494
INFO:root:current mean train loss 1276.7450604768078
INFO:root:current train perplexity2.7360379695892334
INFO:root:current mean train loss 1277.194805330504
INFO:root:current train perplexity2.735790729522705
INFO:root:current mean train loss 1277.92042732964
INFO:root:current train perplexity2.7364306449890137
INFO:root:current mean train loss 1277.4281604564653
INFO:root:current train perplexity2.7372872829437256
INFO:root:current mean train loss 1277.1863311868296
INFO:root:current train perplexity2.7375614643096924
INFO:root:current mean train loss 1277.4160079908813
INFO:root:current train perplexity2.7386112213134766
INFO:root:current mean train loss 1277.8680498929482
INFO:root:current train perplexity2.7392890453338623
INFO:root:current mean train loss 1277.8117297800447
INFO:root:current train perplexity2.7393252849578857
INFO:root:current mean train loss 1278.0425521173304
INFO:root:current train perplexity2.7400455474853516

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.49s/it]
INFO:root:final mean train loss: 1278.305376800214
INFO:root:final train perplexity: 2.7405223846435547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 2309.5031720966313
INFO:root:eval perplexity: 6.473976135253906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.39s/it]
INFO:root:eval mean loss: 2893.203874736813
INFO:root:eval perplexity: 10.656196594238281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [25:58:45<8:54:51, 629.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1272.4838371276855
INFO:root:current train perplexity2.750727415084839
INFO:root:current mean train loss 1269.0842322147255
INFO:root:current train perplexity2.7163872718811035
INFO:root:current mean train loss 1270.1177741741312
INFO:root:current train perplexity2.7275197505950928
INFO:root:current mean train loss 1269.0075510783367
INFO:root:current train perplexity2.7295355796813965
INFO:root:current mean train loss 1272.4409459431965
INFO:root:current train perplexity2.7296245098114014
INFO:root:current mean train loss 1273.0576646847833
INFO:root:current train perplexity2.7292230129241943
INFO:root:current mean train loss 1274.0989310349091
INFO:root:current train perplexity2.7308437824249268
INFO:root:current mean train loss 1273.2014398626943
INFO:root:current train perplexity2.7302441596984863
INFO:root:current mean train loss 1274.232035270104
INFO:root:current train perplexity2.7311856746673584
INFO:root:current mean train loss 1274.7190956475908
INFO:root:current train perplexity2.7327382564544678
INFO:root:current mean train loss 1274.9227179002391
INFO:root:current train perplexity2.7308852672576904
INFO:root:current mean train loss 1275.0728936616608
INFO:root:current train perplexity2.7322065830230713
INFO:root:current mean train loss 1275.1040680129806
INFO:root:current train perplexity2.7325408458709717
INFO:root:current mean train loss 1275.4227768723313
INFO:root:current train perplexity2.7330026626586914
INFO:root:current mean train loss 1275.9345674994272
INFO:root:current train perplexity2.7334539890289307
INFO:root:current mean train loss 1276.2834473453054
INFO:root:current train perplexity2.7356045246124268
INFO:root:current mean train loss 1276.8170485403023
INFO:root:current train perplexity2.736403465270996
INFO:root:current mean train loss 1277.3405336023202
INFO:root:current train perplexity2.7372398376464844
INFO:root:current mean train loss 1277.469828243339
INFO:root:current train perplexity2.736736536026001
INFO:root:current mean train loss 1277.2935393592093
INFO:root:current train perplexity2.7373158931732178

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.04s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.04s/it]
INFO:root:final mean train loss: 1277.0195218315644
INFO:root:final train perplexity: 2.7377445697784424
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it]
INFO:root:eval mean loss: 2313.8328762189717
INFO:root:eval perplexity: 6.496685981750488
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:38<00:00, 38.35s/it]
INFO:root:eval mean loss: 2898.384908473238
INFO:root:eval perplexity: 10.701444625854492
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [26:09:19<8:45:27, 630.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1273.5848737444196
INFO:root:current train perplexity2.716048240661621
INFO:root:current mean train loss 1268.564094287437
INFO:root:current train perplexity2.718858003616333
INFO:root:current mean train loss 1267.497332101845
INFO:root:current train perplexity2.71866774559021
INFO:root:current mean train loss 1266.900436095317
INFO:root:current train perplexity2.7168891429901123
INFO:root:current mean train loss 1267.9977581474457
INFO:root:current train perplexity2.7143797874450684
INFO:root:current mean train loss 1267.2536040759478
INFO:root:current train perplexity2.715595006942749
INFO:root:current mean train loss 1270.0577716092666
INFO:root:current train perplexity2.7218191623687744
INFO:root:current mean train loss 1270.514925173669
INFO:root:current train perplexity2.7225124835968018
INFO:root:current mean train loss 1271.8626202874245
INFO:root:current train perplexity2.721902847290039
INFO:root:current mean train loss 1271.524161689525
INFO:root:current train perplexity2.7239410877227783
INFO:root:current mean train loss 1272.310968593601
INFO:root:current train perplexity2.7249319553375244
INFO:root:current mean train loss 1272.9291293125136
INFO:root:current train perplexity2.725966691970825
INFO:root:current mean train loss 1272.927592269128
INFO:root:current train perplexity2.726778984069824
INFO:root:current mean train loss 1273.0157131367564
INFO:root:current train perplexity2.727142095565796
INFO:root:current mean train loss 1272.8821098164412
INFO:root:current train perplexity2.7271475791931152
INFO:root:current mean train loss 1273.6707844841935
INFO:root:current train perplexity2.728663682937622
INFO:root:current mean train loss 1273.6654622001024
INFO:root:current train perplexity2.72890305519104
INFO:root:current mean train loss 1274.200339996181
INFO:root:current train perplexity2.730102777481079
INFO:root:current mean train loss 1274.1091960867784
INFO:root:current train perplexity2.7303357124328613
INFO:root:current mean train loss 1274.4423723529094
INFO:root:current train perplexity2.730888843536377

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.72s/it]
INFO:root:final mean train loss: 1273.9923216973177
INFO:root:final train perplexity: 2.7312159538269043
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.57s/it]
INFO:root:eval mean loss: 2313.3357552602783
INFO:root:eval perplexity: 6.49407434463501
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.72s/it]
INFO:root:eval mean loss: 2897.535284380541
INFO:root:eval perplexity: 10.694009780883789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [26:19:53<8:35:51, 631.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1266.137636126894
INFO:root:current train perplexity2.7196192741394043
INFO:root:current mean train loss 1263.8214567253387
INFO:root:current train perplexity2.7113680839538574
INFO:root:current mean train loss 1264.3620766087581
INFO:root:current train perplexity2.7081971168518066
INFO:root:current mean train loss 1266.0470724470629
INFO:root:current train perplexity2.71212100982666
INFO:root:current mean train loss 1265.844185890558
INFO:root:current train perplexity2.715357542037964
INFO:root:current mean train loss 1266.1322769865974
INFO:root:current train perplexity2.717947483062744
INFO:root:current mean train loss 1266.9352515968117
INFO:root:current train perplexity2.7194066047668457
INFO:root:current mean train loss 1267.2878660197046
INFO:root:current train perplexity2.7183895111083984
INFO:root:current mean train loss 1267.5174592967396
INFO:root:current train perplexity2.719972610473633
INFO:root:current mean train loss 1267.9222443701071
INFO:root:current train perplexity2.7201051712036133
INFO:root:current mean train loss 1268.6707840395243
INFO:root:current train perplexity2.7214431762695312
INFO:root:current mean train loss 1268.9888622879369
INFO:root:current train perplexity2.722668409347534
INFO:root:current mean train loss 1270.1247790970885
INFO:root:current train perplexity2.7251901626586914
INFO:root:current mean train loss 1270.230404050947
INFO:root:current train perplexity2.7258622646331787
INFO:root:current mean train loss 1271.7928541737722
INFO:root:current train perplexity2.7273595333099365
INFO:root:current mean train loss 1272.3026595426245
INFO:root:current train perplexity2.727936267852783
INFO:root:current mean train loss 1272.361412900121
INFO:root:current train perplexity2.7284748554229736
INFO:root:current mean train loss 1272.8757722364276
INFO:root:current train perplexity2.729318380355835
INFO:root:current mean train loss 1273.1269736008885
INFO:root:current train perplexity2.729519844055176
INFO:root:current mean train loss 1273.3017085745764
INFO:root:current train perplexity2.7287449836730957

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.26s/it]
INFO:root:final mean train loss: 1272.9474248458084
INFO:root:final train perplexity: 2.72896671295166
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.81s/it]
INFO:root:eval mean loss: 2312.5454300684287
INFO:root:eval perplexity: 6.489924907684326
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.06s/it]
INFO:root:eval mean loss: 2897.4690222773993
INFO:root:eval perplexity: 10.69343376159668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [26:30:17<8:23:35, 629.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1260.190592938159
INFO:root:current train perplexity2.706554651260376
INFO:root:current mean train loss 1267.2016628244535
INFO:root:current train perplexity2.718397855758667
INFO:root:current mean train loss 1266.2007819401501
INFO:root:current train perplexity2.714986562728882
INFO:root:current mean train loss 1267.9263549485966
INFO:root:current train perplexity2.7157652378082275
INFO:root:current mean train loss 1267.6397732878818
INFO:root:current train perplexity2.7111575603485107
INFO:root:current mean train loss 1268.4293478807085
INFO:root:current train perplexity2.7141056060791016
INFO:root:current mean train loss 1268.1584933771046
INFO:root:current train perplexity2.7146835327148438
INFO:root:current mean train loss 1268.8653604987328
INFO:root:current train perplexity2.717517852783203
INFO:root:current mean train loss 1269.1787613969157
INFO:root:current train perplexity2.7173361778259277
INFO:root:current mean train loss 1269.8704866271537
INFO:root:current train perplexity2.719224214553833
INFO:root:current mean train loss 1269.6289428823652
INFO:root:current train perplexity2.720269203186035
INFO:root:current mean train loss 1269.888186379801
INFO:root:current train perplexity2.720289945602417
INFO:root:current mean train loss 1269.9293537333156
INFO:root:current train perplexity2.7212133407592773
INFO:root:current mean train loss 1270.0901187833288
INFO:root:current train perplexity2.7214157581329346
INFO:root:current mean train loss 1269.7310481518407
INFO:root:current train perplexity2.7206296920776367
INFO:root:current mean train loss 1269.9066033330207
INFO:root:current train perplexity2.721259117126465
INFO:root:current mean train loss 1269.8878293504902
INFO:root:current train perplexity2.721172571182251
INFO:root:current mean train loss 1269.88785873473
INFO:root:current train perplexity2.7222342491149902
INFO:root:current mean train loss 1270.4242185814483
INFO:root:current train perplexity2.7230584621429443
INFO:root:current mean train loss 1270.682987461292
INFO:root:current train perplexity2.72409725189209

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:26<00:00, 566.12s/it]
INFO:root:final mean train loss: 1270.682987461292
INFO:root:final train perplexity: 2.72409725189209
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.20s/it]
INFO:root:eval mean loss: 2316.50266606757
INFO:root:eval perplexity: 6.510728359222412
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.09s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.09s/it]
INFO:root:eval mean loss: 2899.8124861480496
INFO:root:eval perplexity: 10.713946342468262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [26:40:59<8:15:58, 633.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1264.1978332519532
INFO:root:current train perplexity2.718857765197754
INFO:root:current mean train loss 1262.2388824462892
INFO:root:current train perplexity2.7137789726257324
INFO:root:current mean train loss 1266.7612854003905
INFO:root:current train perplexity2.7137250900268555
INFO:root:current mean train loss 1266.5899749755858
INFO:root:current train perplexity2.7122018337249756
INFO:root:current mean train loss 1267.346793701172
INFO:root:current train perplexity2.713935613632202
INFO:root:current mean train loss 1266.7521431477865
INFO:root:current train perplexity2.712155818939209
INFO:root:current mean train loss 1267.0076708984375
INFO:root:current train perplexity2.7126827239990234
INFO:root:current mean train loss 1266.750330657959
INFO:root:current train perplexity2.711548089981079
INFO:root:current mean train loss 1267.101681315104
INFO:root:current train perplexity2.713836431503296
INFO:root:current mean train loss 1267.7955576171876
INFO:root:current train perplexity2.715204954147339
INFO:root:current mean train loss 1268.816504683061
INFO:root:current train perplexity2.715538501739502
INFO:root:current mean train loss 1267.7408169555665
INFO:root:current train perplexity2.7154486179351807
INFO:root:current mean train loss 1267.8681534517727
INFO:root:current train perplexity2.7154223918914795
INFO:root:current mean train loss 1268.1017953055245
INFO:root:current train perplexity2.716557264328003
INFO:root:current mean train loss 1268.558919921875
INFO:root:current train perplexity2.7159993648529053
INFO:root:current mean train loss 1269.115262069702
INFO:root:current train perplexity2.7167322635650635
INFO:root:current mean train loss 1268.3730437873392
INFO:root:current train perplexity2.7176353931427
INFO:root:current mean train loss 1268.715988023546
INFO:root:current train perplexity2.7183589935302734
INFO:root:current mean train loss 1268.7834450812088
INFO:root:current train perplexity2.718970537185669

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.40s/it]
INFO:root:final mean train loss: 1268.5056923670536
INFO:root:final train perplexity: 2.7194230556488037
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.88s/it]
INFO:root:eval mean loss: 2318.652128611896
INFO:root:eval perplexity: 6.522056579589844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.54s/it]
INFO:root:eval mean loss: 2904.597683521027
INFO:root:eval perplexity: 10.755956649780273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [26:51:32<8:05:24, 633.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1251.3139361213234
INFO:root:current train perplexity2.6755285263061523
INFO:root:current mean train loss 1260.9579076522436
INFO:root:current train perplexity2.7120401859283447
INFO:root:current mean train loss 1260.4401326684908
INFO:root:current train perplexity2.710479259490967
INFO:root:current mean train loss 1261.631411579505
INFO:root:current train perplexity2.709415912628174
INFO:root:current mean train loss 1264.419098412676
INFO:root:current train perplexity2.7111425399780273
INFO:root:current mean train loss 1264.2448175603693
INFO:root:current train perplexity2.707465648651123
INFO:root:current mean train loss 1264.358482521589
INFO:root:current train perplexity2.7080440521240234
INFO:root:current mean train loss 1265.5217707379925
INFO:root:current train perplexity2.70959734916687
INFO:root:current mean train loss 1265.7229438697693
INFO:root:current train perplexity2.7115843296051025
INFO:root:current mean train loss 1265.8614575168688
INFO:root:current train perplexity2.711678981781006
INFO:root:current mean train loss 1266.1059751557507
INFO:root:current train perplexity2.7115533351898193
INFO:root:current mean train loss 1266.8306031256993
INFO:root:current train perplexity2.7128899097442627
INFO:root:current mean train loss 1266.695851434913
INFO:root:current train perplexity2.7118420600891113
INFO:root:current mean train loss 1266.424769651374
INFO:root:current train perplexity2.7127275466918945
INFO:root:current mean train loss 1267.4974986354314
INFO:root:current train perplexity2.7133162021636963
INFO:root:current mean train loss 1267.7966245023845
INFO:root:current train perplexity2.714171886444092
INFO:root:current mean train loss 1267.9180150265006
INFO:root:current train perplexity2.7152130603790283
INFO:root:current mean train loss 1267.5460190859103
INFO:root:current train perplexity2.71549654006958
INFO:root:current mean train loss 1267.900504700614
INFO:root:current train perplexity2.7167060375213623
INFO:root:current mean train loss 1268.0911153953525
INFO:root:current train perplexity2.716850519180298

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.79s/it]
INFO:root:final mean train loss: 1267.565437320742
INFO:root:final train perplexity: 2.717407703399658
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.75s/it]
INFO:root:eval mean loss: 2320.829696763492
INFO:root:eval perplexity: 6.533552169799805
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.31s/it]
INFO:root:eval mean loss: 2906.2693996564717
INFO:root:eval perplexity: 10.770672798156738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [27:01:57<7:53:03, 630.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1264.0538258272059
INFO:root:current train perplexity2.6879236698150635
INFO:root:current mean train loss 1262.4856558273088
INFO:root:current train perplexity2.709902286529541
INFO:root:current mean train loss 1260.9671709109575
INFO:root:current train perplexity2.7091283798217773
INFO:root:current mean train loss 1261.7415756865175
INFO:root:current train perplexity2.706482172012329
INFO:root:current mean train loss 1261.9593708372336
INFO:root:current train perplexity2.707327127456665
INFO:root:current mean train loss 1262.8257165115872
INFO:root:current train perplexity2.7080206871032715
INFO:root:current mean train loss 1263.2130042235558
INFO:root:current train perplexity2.7071735858917236
INFO:root:current mean train loss 1264.251458690342
INFO:root:current train perplexity2.706953763961792
INFO:root:current mean train loss 1265.0316484117393
INFO:root:current train perplexity2.706571102142334
INFO:root:current mean train loss 1264.6131289888501
INFO:root:current train perplexity2.7057383060455322
INFO:root:current mean train loss 1263.4317988205694
INFO:root:current train perplexity2.706376552581787
INFO:root:current mean train loss 1263.4091115477224
INFO:root:current train perplexity2.707193613052368
INFO:root:current mean train loss 1263.4901478178497
INFO:root:current train perplexity2.7073192596435547
INFO:root:current mean train loss 1264.1610879740795
INFO:root:current train perplexity2.7089951038360596
INFO:root:current mean train loss 1264.2948555620478
INFO:root:current train perplexity2.710111141204834
INFO:root:current mean train loss 1264.5266454664327
INFO:root:current train perplexity2.710118055343628
INFO:root:current mean train loss 1265.0211662750057
INFO:root:current train perplexity2.7106049060821533
INFO:root:current mean train loss 1265.3739156580027
INFO:root:current train perplexity2.7116105556488037
INFO:root:current mean train loss 1265.666204055237
INFO:root:current train perplexity2.7118308544158936
INFO:root:current mean train loss 1265.802225264793
INFO:root:current train perplexity2.712697744369507

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:14<00:00, 554.91s/it]
INFO:root:final mean train loss: 1265.4769838980455
INFO:root:final train perplexity: 2.712935447692871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.76s/it]
INFO:root:eval mean loss: 2321.487087817902
INFO:root:eval perplexity: 6.537027359008789
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.94s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.94s/it]
INFO:root:eval mean loss: 2907.5385707557625
INFO:root:eval perplexity: 10.78185749053955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [27:12:25<7:41:56, 629.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1257.0595631318934
INFO:root:current train perplexity2.717435836791992
INFO:root:current mean train loss 1259.5374860953023
INFO:root:current train perplexity2.7080373764038086
INFO:root:current mean train loss 1260.2136497953497
INFO:root:current train perplexity2.705301523208618
INFO:root:current mean train loss 1259.4441780459847
INFO:root:current train perplexity2.6985087394714355
INFO:root:current mean train loss 1262.4321413568805
INFO:root:current train perplexity2.702075719833374
INFO:root:current mean train loss 1262.3436981588873
INFO:root:current train perplexity2.7039108276367188
INFO:root:current mean train loss 1263.1959322271625
INFO:root:current train perplexity2.7042489051818848
INFO:root:current mean train loss 1262.514775546667
INFO:root:current train perplexity2.7048661708831787
INFO:root:current mean train loss 1262.1100414379223
INFO:root:current train perplexity2.7051329612731934
INFO:root:current mean train loss 1262.965962021885
INFO:root:current train perplexity2.7049925327301025
INFO:root:current mean train loss 1263.6243146175889
INFO:root:current train perplexity2.70578932762146
INFO:root:current mean train loss 1264.1609614262054
INFO:root:current train perplexity2.7071568965911865
INFO:root:current mean train loss 1264.4483116631695
INFO:root:current train perplexity2.7077436447143555
INFO:root:current mean train loss 1264.8312497650757
INFO:root:current train perplexity2.7093935012817383
INFO:root:current mean train loss 1265.5298387460425
INFO:root:current train perplexity2.710132122039795
INFO:root:current mean train loss 1265.7080778592945
INFO:root:current train perplexity2.7103536128997803
INFO:root:current mean train loss 1265.6421769417682
INFO:root:current train perplexity2.7104909420013428
INFO:root:current mean train loss 1265.6364247634165
INFO:root:current train perplexity2.711110830307007
INFO:root:current mean train loss 1265.354447210756
INFO:root:current train perplexity2.7116031646728516
INFO:root:current mean train loss 1265.3178824185713
INFO:root:current train perplexity2.711031675338745

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.87s/it]
INFO:root:final mean train loss: 1264.8085129853757
INFO:root:final train perplexity: 2.711505651473999
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.46s/it]
INFO:root:eval mean loss: 2322.5991228252437
INFO:root:eval perplexity: 6.542908191680908
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.63s/it]
INFO:root:eval mean loss: 2909.038322286403
INFO:root:eval perplexity: 10.795090675354004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [27:22:58<7:32:08, 630.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1254.5424319996553
INFO:root:current train perplexity2.6846601963043213
INFO:root:current mean train loss 1252.9509800502233
INFO:root:current train perplexity2.690232515335083
INFO:root:current mean train loss 1254.0005306414703
INFO:root:current train perplexity2.6948976516723633
INFO:root:current mean train loss 1257.0636672973633
INFO:root:current train perplexity2.7012293338775635
INFO:root:current mean train loss 1257.3445893638154
INFO:root:current train perplexity2.7000865936279297
INFO:root:current mean train loss 1257.1222448214678
INFO:root:current train perplexity2.6995365619659424
INFO:root:current mean train loss 1256.1753130701488
INFO:root:current train perplexity2.700839042663574
INFO:root:current mean train loss 1257.3014877637227
INFO:root:current train perplexity2.7023725509643555
INFO:root:current mean train loss 1258.1935815767208
INFO:root:current train perplexity2.7034332752227783
INFO:root:current mean train loss 1258.6759788576237
INFO:root:current train perplexity2.7048757076263428
INFO:root:current mean train loss 1259.6191729713469
INFO:root:current train perplexity2.704826831817627
INFO:root:current mean train loss 1260.2953511068265
INFO:root:current train perplexity2.7056727409362793
INFO:root:current mean train loss 1261.2455798609399
INFO:root:current train perplexity2.7049083709716797
INFO:root:current mean train loss 1261.3121852763215
INFO:root:current train perplexity2.7051584720611572
INFO:root:current mean train loss 1260.7221712949165
INFO:root:current train perplexity2.7040810585021973
INFO:root:current mean train loss 1260.9844907643844
INFO:root:current train perplexity2.7039990425109863
INFO:root:current mean train loss 1261.2463877286843
INFO:root:current train perplexity2.705174684524536
INFO:root:current mean train loss 1261.5028634049775
INFO:root:current train perplexity2.7050974369049072
INFO:root:current mean train loss 1261.851103167973
INFO:root:current train perplexity2.7057104110717773
INFO:root:current mean train loss 1262.6280572162411
INFO:root:current train perplexity2.7061755657196045

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.31s/it]
INFO:root:final mean train loss: 1262.3111616587676
INFO:root:final train perplexity: 2.7061703205108643
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.16s/it]
INFO:root:eval mean loss: 2326.4296688864415
INFO:root:eval perplexity: 6.56320858001709
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.28s/it]
INFO:root:eval mean loss: 2913.3553960272607
INFO:root:eval perplexity: 10.833272933959961
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [27:33:21<7:19:57, 628.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1252.4980612362133
INFO:root:current train perplexity2.695474147796631
INFO:root:current mean train loss 1251.3354954075169
INFO:root:current train perplexity2.6948812007904053
INFO:root:current mean train loss 1255.3771702816612
INFO:root:current train perplexity2.6986703872680664
INFO:root:current mean train loss 1257.4470554104098
INFO:root:current train perplexity2.7016701698303223
INFO:root:current mean train loss 1258.4342063667848
INFO:root:current train perplexity2.7025492191314697
INFO:root:current mean train loss 1256.406732229901
INFO:root:current train perplexity2.7023863792419434
INFO:root:current mean train loss 1257.6774802549041
INFO:root:current train perplexity2.7023162841796875
INFO:root:current mean train loss 1257.9787708063793
INFO:root:current train perplexity2.702362298965454
INFO:root:current mean train loss 1258.9580806408899
INFO:root:current train perplexity2.7025034427642822
INFO:root:current mean train loss 1260.149108205108
INFO:root:current train perplexity2.7024714946746826
INFO:root:current mean train loss 1259.4853120724727
INFO:root:current train perplexity2.7023165225982666
INFO:root:current mean train loss 1259.379192625923
INFO:root:current train perplexity2.702362060546875
INFO:root:current mean train loss 1259.338614117218
INFO:root:current train perplexity2.702319860458374
INFO:root:current mean train loss 1259.8807092769912
INFO:root:current train perplexity2.7016959190368652
INFO:root:current mean train loss 1260.0823468243634
INFO:root:current train perplexity2.7013356685638428
INFO:root:current mean train loss 1260.356268021737
INFO:root:current train perplexity2.702510118484497
INFO:root:current mean train loss 1261.026413552485
INFO:root:current train perplexity2.702660322189331
INFO:root:current mean train loss 1261.7739989550507
INFO:root:current train perplexity2.7034549713134766
INFO:root:current mean train loss 1262.127726150634
INFO:root:current train perplexity2.7042081356048584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.29s/it]
INFO:root:final mean train loss: 1261.724912365458
INFO:root:final train perplexity: 2.7049195766448975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.11s/it]
INFO:root:eval mean loss: 2325.3233391511526
INFO:root:eval perplexity: 6.557338714599609
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it]
INFO:root:eval mean loss: 2913.1072517557345
INFO:root:eval perplexity: 10.831072807312012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [27:43:56<7:10:42, 630.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1176.8348388671875
INFO:root:current train perplexity2.6706795692443848
INFO:root:current mean train loss 1250.1460403741576
INFO:root:current train perplexity2.7000677585601807
INFO:root:current mean train loss 1249.0815925220452
INFO:root:current train perplexity2.688344717025757
INFO:root:current mean train loss 1254.80541385878
INFO:root:current train perplexity2.690816879272461
INFO:root:current mean train loss 1254.2008369407843
INFO:root:current train perplexity2.689157724380493
INFO:root:current mean train loss 1255.0167547583105
INFO:root:current train perplexity2.689608335494995
INFO:root:current mean train loss 1255.825598225641
INFO:root:current train perplexity2.690161943435669
INFO:root:current mean train loss 1256.077412923177
INFO:root:current train perplexity2.6902272701263428
INFO:root:current mean train loss 1256.694994386592
INFO:root:current train perplexity2.6916110515594482
INFO:root:current mean train loss 1256.9504428364485
INFO:root:current train perplexity2.6947858333587646
INFO:root:current mean train loss 1257.1983582883063
INFO:root:current train perplexity2.6968636512756348
INFO:root:current mean train loss 1258.3436675859234
INFO:root:current train perplexity2.6982710361480713
INFO:root:current mean train loss 1258.5750609539114
INFO:root:current train perplexity2.6982734203338623
INFO:root:current mean train loss 1258.9032990298879
INFO:root:current train perplexity2.6980056762695312
INFO:root:current mean train loss 1259.854590575127
INFO:root:current train perplexity2.698038101196289
INFO:root:current mean train loss 1259.9000023893923
INFO:root:current train perplexity2.699158191680908
INFO:root:current mean train loss 1260.4820254131798
INFO:root:current train perplexity2.7005271911621094
INFO:root:current mean train loss 1260.4320775535216
INFO:root:current train perplexity2.7012953758239746
INFO:root:current mean train loss 1260.620913218711
INFO:root:current train perplexity2.7020351886749268
INFO:root:current mean train loss 1260.8328133471757
INFO:root:current train perplexity2.7025039196014404

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.64s/it]
INFO:root:final mean train loss: 1260.5350830570592
INFO:root:final train perplexity: 2.7023823261260986
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.24s/it]
INFO:root:eval mean loss: 2328.8187264516846
INFO:root:eval perplexity: 6.575903415679932
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.87s/it]
INFO:root:eval mean loss: 2916.7919493330287
INFO:root:eval perplexity: 10.863759994506836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [27:54:20<6:58:56, 628.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1256.7006386204769
INFO:root:current train perplexity2.6989448070526123
INFO:root:current mean train loss 1255.9755346474528
INFO:root:current train perplexity2.680164098739624
INFO:root:current mean train loss 1256.294945285745
INFO:root:current train perplexity2.68034029006958
INFO:root:current mean train loss 1253.2789520933336
INFO:root:current train perplexity2.6861379146575928
INFO:root:current mean train loss 1253.7693660617729
INFO:root:current train perplexity2.6870405673980713
INFO:root:current mean train loss 1253.3169949628943
INFO:root:current train perplexity2.689281940460205
INFO:root:current mean train loss 1253.8013231711934
INFO:root:current train perplexity2.689962387084961
INFO:root:current mean train loss 1254.7054217554764
INFO:root:current train perplexity2.6900529861450195
INFO:root:current mean train loss 1254.5510720426491
INFO:root:current train perplexity2.690636157989502
INFO:root:current mean train loss 1254.7414589301807
INFO:root:current train perplexity2.6914496421813965
INFO:root:current mean train loss 1255.892205684763
INFO:root:current train perplexity2.6932156085968018
INFO:root:current mean train loss 1256.3408875111706
INFO:root:current train perplexity2.6950345039367676
INFO:root:current mean train loss 1257.2518799228683
INFO:root:current train perplexity2.695455551147461
INFO:root:current mean train loss 1258.517140189656
INFO:root:current train perplexity2.696563243865967
INFO:root:current mean train loss 1258.531960571375
INFO:root:current train perplexity2.695840835571289
INFO:root:current mean train loss 1258.1422428535427
INFO:root:current train perplexity2.6975021362304688
INFO:root:current mean train loss 1257.7734019118668
INFO:root:current train perplexity2.697767496109009
INFO:root:current mean train loss 1258.104024783895
INFO:root:current train perplexity2.6980271339416504
INFO:root:current mean train loss 1258.6001466454354
INFO:root:current train perplexity2.6976726055145264
INFO:root:current mean train loss 1258.9611050524768
INFO:root:current train perplexity2.698091983795166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:15<00:00, 555.55s/it]
INFO:root:final mean train loss: 1258.6188206444228
INFO:root:final train perplexity: 2.6983015537261963
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2330.254766802416
INFO:root:eval perplexity: 6.583543300628662
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.73s/it]
INFO:root:eval mean loss: 2918.8090448041335
INFO:root:eval perplexity: 10.881695747375488
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [28:04:50<6:48:49, 628.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1263.2452528211807
INFO:root:current train perplexity2.6930453777313232
INFO:root:current mean train loss 1254.076078527114
INFO:root:current train perplexity2.671264886856079
INFO:root:current mean train loss 1253.7293473583156
INFO:root:current train perplexity2.684103012084961
INFO:root:current mean train loss 1255.3554269699823
INFO:root:current train perplexity2.6876087188720703
INFO:root:current mean train loss 1256.069641393259
INFO:root:current train perplexity2.687889337539673
INFO:root:current mean train loss 1255.0952175766674
INFO:root:current train perplexity2.6891324520111084
INFO:root:current mean train loss 1255.7734309742286
INFO:root:current train perplexity2.68727445602417
INFO:root:current mean train loss 1255.4077035655146
INFO:root:current train perplexity2.690359354019165
INFO:root:current mean train loss 1255.6603197891745
INFO:root:current train perplexity2.689821720123291
INFO:root:current mean train loss 1255.3895345834585
INFO:root:current train perplexity2.689962148666382
INFO:root:current mean train loss 1255.3098218763196
INFO:root:current train perplexity2.6904425621032715
INFO:root:current mean train loss 1256.0798925480372
INFO:root:current train perplexity2.6915488243103027
INFO:root:current mean train loss 1255.9144705861904
INFO:root:current train perplexity2.6913466453552246
INFO:root:current mean train loss 1256.018701975931
INFO:root:current train perplexity2.6922173500061035
INFO:root:current mean train loss 1255.9487785828146
INFO:root:current train perplexity2.6918153762817383
INFO:root:current mean train loss 1256.6886818408966
INFO:root:current train perplexity2.692080020904541
INFO:root:current mean train loss 1257.0525892486198
INFO:root:current train perplexity2.693756103515625
INFO:root:current mean train loss 1256.5677154119114
INFO:root:current train perplexity2.693925380706787
INFO:root:current mean train loss 1256.7860721762663
INFO:root:current train perplexity2.6942298412323
INFO:root:current mean train loss 1256.9137575133773
INFO:root:current train perplexity2.693986177444458

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.42s/it]
INFO:root:final mean train loss: 1256.565086930314
INFO:root:final train perplexity: 2.693934679031372
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.93s/it]
INFO:root:eval mean loss: 2330.975517976369
INFO:root:eval perplexity: 6.58738374710083
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it]
INFO:root:eval mean loss: 2920.242884426252
INFO:root:eval perplexity: 10.894462585449219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [28:15:14<6:37:17, 627.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1256.9421985554245
INFO:root:current train perplexity2.686450242996216
INFO:root:current mean train loss 1256.9760678359885
INFO:root:current train perplexity2.678893566131592
INFO:root:current mean train loss 1256.621882140872
INFO:root:current train perplexity2.6762428283691406
INFO:root:current mean train loss 1255.7468019652974
INFO:root:current train perplexity2.6790499687194824
INFO:root:current mean train loss 1255.115932843543
INFO:root:current train perplexity2.6834728717803955
INFO:root:current mean train loss 1254.6489054729882
INFO:root:current train perplexity2.6850597858428955
INFO:root:current mean train loss 1254.436419313206
INFO:root:current train perplexity2.6852052211761475
INFO:root:current mean train loss 1253.9656611185467
INFO:root:current train perplexity2.685265064239502
INFO:root:current mean train loss 1253.9658756949277
INFO:root:current train perplexity2.684797525405884
INFO:root:current mean train loss 1254.2026517053466
INFO:root:current train perplexity2.6854872703552246
INFO:root:current mean train loss 1255.1449132269038
INFO:root:current train perplexity2.6869137287139893
INFO:root:current mean train loss 1255.827989378083
INFO:root:current train perplexity2.6871445178985596
INFO:root:current mean train loss 1255.71529004218
INFO:root:current train perplexity2.6873223781585693
INFO:root:current mean train loss 1256.6374964632992
INFO:root:current train perplexity2.68984317779541
INFO:root:current mean train loss 1255.7045010424283
INFO:root:current train perplexity2.689826726913452
INFO:root:current mean train loss 1255.7442586865643
INFO:root:current train perplexity2.6902105808258057
INFO:root:current mean train loss 1255.6752992458078
INFO:root:current train perplexity2.690498113632202
INFO:root:current mean train loss 1255.8097739255027
INFO:root:current train perplexity2.690786600112915
INFO:root:current mean train loss 1255.8793035549404
INFO:root:current train perplexity2.691227436065674
INFO:root:current mean train loss 1255.9383811813957
INFO:root:current train perplexity2.6918766498565674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.35s/it]
INFO:root:final mean train loss: 1255.685705726458
INFO:root:final train perplexity: 2.6920669078826904
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 2333.1853932049257
INFO:root:eval perplexity: 6.599165439605713
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it]
INFO:root:eval mean loss: 2923.7322643090647
INFO:root:eval perplexity: 10.92559814453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [28:25:48<6:28:06, 629.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1253.7503051757812
INFO:root:current train perplexity2.7042224407196045
INFO:root:current mean train loss 1252.7744406307445
INFO:root:current train perplexity2.692249298095703
INFO:root:current mean train loss 1250.5698802806712
INFO:root:current train perplexity2.6887965202331543
INFO:root:current mean train loss 1251.1530507680532
INFO:root:current train perplexity2.682523250579834
INFO:root:current mean train loss 1251.1286867831616
INFO:root:current train perplexity2.6793830394744873
INFO:root:current mean train loss 1252.469645824767
INFO:root:current train perplexity2.6787636280059814
INFO:root:current mean train loss 1252.165822863223
INFO:root:current train perplexity2.6800010204315186
INFO:root:current mean train loss 1252.873291015625
INFO:root:current train perplexity2.6818838119506836
INFO:root:current mean train loss 1252.969574886629
INFO:root:current train perplexity2.6819517612457275
INFO:root:current mean train loss 1253.9395439855832
INFO:root:current train perplexity2.6832046508789062
INFO:root:current mean train loss 1253.7210938640844
INFO:root:current train perplexity2.6844937801361084
INFO:root:current mean train loss 1253.1662723899906
INFO:root:current train perplexity2.6856510639190674
INFO:root:current mean train loss 1253.1455068513164
INFO:root:current train perplexity2.6857762336730957
INFO:root:current mean train loss 1253.0403304274064
INFO:root:current train perplexity2.6853556632995605
INFO:root:current mean train loss 1253.0937389555431
INFO:root:current train perplexity2.686302661895752
INFO:root:current mean train loss 1253.3574186871765
INFO:root:current train perplexity2.685739040374756
INFO:root:current mean train loss 1253.6246436570218
INFO:root:current train perplexity2.6863059997558594
INFO:root:current mean train loss 1254.1341035487287
INFO:root:current train perplexity2.68717622756958
INFO:root:current mean train loss 1253.741960070605
INFO:root:current train perplexity2.687452554702759
INFO:root:current mean train loss 1254.440226939245
INFO:root:current train perplexity2.6886191368103027

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.99s/it]
INFO:root:final mean train loss: 1254.096498213329
INFO:root:final train perplexity: 2.688694715499878
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.66s/it]
INFO:root:eval mean loss: 2333.172215671404
INFO:root:eval perplexity: 6.599094867706299
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it]
INFO:root:eval mean loss: 2922.34465124252
INFO:root:eval perplexity: 10.913206100463867
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [28:36:10<6:16:20, 627.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1241.1202673199532
INFO:root:current train perplexity2.672194242477417
INFO:root:current mean train loss 1247.9111876462232
INFO:root:current train perplexity2.675178289413452
INFO:root:current mean train loss 1246.1214603862695
INFO:root:current train perplexity2.669893741607666
INFO:root:current mean train loss 1248.5625570923166
INFO:root:current train perplexity2.6709988117218018
INFO:root:current mean train loss 1248.2586898020406
INFO:root:current train perplexity2.6730268001556396
INFO:root:current mean train loss 1247.426352921702
INFO:root:current train perplexity2.674741506576538
INFO:root:current mean train loss 1247.8570560194346
INFO:root:current train perplexity2.6763789653778076
INFO:root:current mean train loss 1248.3923446868548
INFO:root:current train perplexity2.6773927211761475
INFO:root:current mean train loss 1248.4176439631483
INFO:root:current train perplexity2.676976203918457
INFO:root:current mean train loss 1247.6129990165116
INFO:root:current train perplexity2.6773064136505127
INFO:root:current mean train loss 1248.4925497804306
INFO:root:current train perplexity2.6781318187713623
INFO:root:current mean train loss 1249.1791627107796
INFO:root:current train perplexity2.6787428855895996
INFO:root:current mean train loss 1249.7391778550225
INFO:root:current train perplexity2.6794755458831787
INFO:root:current mean train loss 1249.3611925891087
INFO:root:current train perplexity2.680765390396118
INFO:root:current mean train loss 1249.883924185388
INFO:root:current train perplexity2.6808276176452637
INFO:root:current mean train loss 1250.5649815579218
INFO:root:current train perplexity2.6820027828216553
INFO:root:current mean train loss 1251.1508810046728
INFO:root:current train perplexity2.6833155155181885
INFO:root:current mean train loss 1251.4364605238134
INFO:root:current train perplexity2.6834795475006104
INFO:root:current mean train loss 1251.6814417027897
INFO:root:current train perplexity2.683359384536743

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.24s/it]
INFO:root:final mean train loss: 1251.7708910135566
INFO:root:final train perplexity: 2.6837682723999023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.31s/it]
INFO:root:eval mean loss: 2334.8069116037786
INFO:root:eval perplexity: 6.607826232910156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.53s/it]
INFO:root:eval mean loss: 2925.5375920288952
INFO:root:eval perplexity: 10.941741943359375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [28:46:45<6:07:18, 629.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1225.3724975585938
INFO:root:current train perplexity2.6784121990203857
INFO:root:current mean train loss 1240.788805448092
INFO:root:current train perplexity2.665498971939087
INFO:root:current mean train loss 1244.34464338714
INFO:root:current train perplexity2.6726582050323486
INFO:root:current mean train loss 1245.074811031944
INFO:root:current train perplexity2.6728477478027344
INFO:root:current mean train loss 1247.7391874105624
INFO:root:current train perplexity2.673088312149048
INFO:root:current mean train loss 1248.1003866044302
INFO:root:current train perplexity2.674295663833618
INFO:root:current mean train loss 1247.5777280693812
INFO:root:current train perplexity2.6769542694091797
INFO:root:current mean train loss 1248.7181339263916
INFO:root:current train perplexity2.6782045364379883
INFO:root:current mean train loss 1248.738351394881
INFO:root:current train perplexity2.67600154876709
INFO:root:current mean train loss 1248.8942266143529
INFO:root:current train perplexity2.6772191524505615
INFO:root:current mean train loss 1249.7945043556244
INFO:root:current train perplexity2.678363084793091
INFO:root:current mean train loss 1249.9210575490758
INFO:root:current train perplexity2.678877592086792
INFO:root:current mean train loss 1250.580695370899
INFO:root:current train perplexity2.6794536113739014
INFO:root:current mean train loss 1250.8542796877996
INFO:root:current train perplexity2.680147647857666
INFO:root:current mean train loss 1250.2043583970465
INFO:root:current train perplexity2.6802475452423096
INFO:root:current mean train loss 1250.3950714760638
INFO:root:current train perplexity2.6818907260894775
INFO:root:current mean train loss 1250.2689020247233
INFO:root:current train perplexity2.6817967891693115
INFO:root:current mean train loss 1250.7983087530718
INFO:root:current train perplexity2.6813926696777344
INFO:root:current mean train loss 1251.1045903715485
INFO:root:current train perplexity2.682274580001831
INFO:root:current mean train loss 1251.2574734727875
INFO:root:current train perplexity2.682976722717285

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.63s/it]
INFO:root:final mean train loss: 1251.5437053886255
INFO:root:final train perplexity: 2.6832876205444336
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.68s/it]
INFO:root:eval mean loss: 2336.592817590592
INFO:root:eval perplexity: 6.617377281188965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.03s/it]
INFO:root:eval mean loss: 2927.071627136663
INFO:root:eval perplexity: 10.955477714538574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [28:57:08<5:55:36, 627.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1245.5313313802083
INFO:root:current train perplexity2.6602554321289062
INFO:root:current mean train loss 1247.1466064453125
INFO:root:current train perplexity2.6686792373657227
INFO:root:current mean train loss 1247.2274064974547
INFO:root:current train perplexity2.6626527309417725
INFO:root:current mean train loss 1246.2022180289866
INFO:root:current train perplexity2.663668394088745
INFO:root:current mean train loss 1247.2397426143111
INFO:root:current train perplexity2.6657145023345947
INFO:root:current mean train loss 1245.901101959873
INFO:root:current train perplexity2.6689765453338623
INFO:root:current mean train loss 1247.3605274931435
INFO:root:current train perplexity2.670912742614746
INFO:root:current mean train loss 1248.211610325828
INFO:root:current train perplexity2.6739938259124756
INFO:root:current mean train loss 1248.391714414348
INFO:root:current train perplexity2.672924757003784
INFO:root:current mean train loss 1249.1704102887911
INFO:root:current train perplexity2.6734325885772705
INFO:root:current mean train loss 1249.179177937148
INFO:root:current train perplexity2.6743669509887695
INFO:root:current mean train loss 1248.7194448534024
INFO:root:current train perplexity2.6767351627349854
INFO:root:current mean train loss 1249.4742190699221
INFO:root:current train perplexity2.676332473754883
INFO:root:current mean train loss 1249.255274250686
INFO:root:current train perplexity2.6763885021209717
INFO:root:current mean train loss 1249.1017678117853
INFO:root:current train perplexity2.6754069328308105
INFO:root:current mean train loss 1249.532887154704
INFO:root:current train perplexity2.6767022609710693
INFO:root:current mean train loss 1250.0687226490206
INFO:root:current train perplexity2.6774260997772217
INFO:root:current mean train loss 1249.9116296053348
INFO:root:current train perplexity2.6776630878448486
INFO:root:current mean train loss 1249.9786444658241
INFO:root:current train perplexity2.678635597229004
INFO:root:current mean train loss 1250.1330544165432
INFO:root:current train perplexity2.6795389652252197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.67s/it]
INFO:root:final mean train loss: 1249.7093658832005
INFO:root:final train perplexity: 2.679408311843872
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2339.4328184736537
INFO:root:eval perplexity: 6.632593631744385
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.99s/it]
INFO:root:eval mean loss: 2930.4182410273993
INFO:root:eval perplexity: 10.985505104064941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [29:07:33<5:44:42, 626.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1247.9292666786596
INFO:root:current train perplexity2.669168710708618
INFO:root:current mean train loss 1246.7446695963542
INFO:root:current train perplexity2.668137788772583
INFO:root:current mean train loss 1247.398652405298
INFO:root:current train perplexity2.664832592010498
INFO:root:current mean train loss 1246.9086065348788
INFO:root:current train perplexity2.6621899604797363
INFO:root:current mean train loss 1248.389339359928
INFO:root:current train perplexity2.666839599609375
INFO:root:current mean train loss 1246.580222204272
INFO:root:current train perplexity2.666091203689575
INFO:root:current mean train loss 1247.3690744238588
INFO:root:current train perplexity2.6659836769104004
INFO:root:current mean train loss 1248.7438319756732
INFO:root:current train perplexity2.6689398288726807
INFO:root:current mean train loss 1249.041978785986
INFO:root:current train perplexity2.6686131954193115
INFO:root:current mean train loss 1248.1922339335688
INFO:root:current train perplexity2.669799566268921
INFO:root:current mean train loss 1248.134788557284
INFO:root:current train perplexity2.6718482971191406
INFO:root:current mean train loss 1247.9277744930114
INFO:root:current train perplexity2.6733033657073975
INFO:root:current mean train loss 1247.8350957275784
INFO:root:current train perplexity2.6756398677825928
INFO:root:current mean train loss 1247.6629487224402
INFO:root:current train perplexity2.6750271320343018
INFO:root:current mean train loss 1248.2360576688
INFO:root:current train perplexity2.6756300926208496
INFO:root:current mean train loss 1248.0243378677667
INFO:root:current train perplexity2.6757755279541016
INFO:root:current mean train loss 1248.288779308799
INFO:root:current train perplexity2.67622709274292
INFO:root:current mean train loss 1248.6154390429463
INFO:root:current train perplexity2.676764488220215
INFO:root:current mean train loss 1248.5793669558454
INFO:root:current train perplexity2.6763687133789062
INFO:root:current mean train loss 1249.063695193075
INFO:root:current train perplexity2.6772279739379883

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.84s/it]
INFO:root:final mean train loss: 1248.5894814480212
INFO:root:final train perplexity: 2.6770429611206055
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.84s/it]
INFO:root:eval mean loss: 2340.9349759149213
INFO:root:eval perplexity: 6.6406569480896
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it]
INFO:root:eval mean loss: 2932.7135126641456
INFO:root:eval perplexity: 11.006144523620605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [29:18:05<5:35:08, 628.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1240.3214044744318
INFO:root:current train perplexity2.663776397705078
INFO:root:current mean train loss 1241.6952943863407
INFO:root:current train perplexity2.6553242206573486
INFO:root:current mean train loss 1244.576172353707
INFO:root:current train perplexity2.6615164279937744
INFO:root:current mean train loss 1244.864971322073
INFO:root:current train perplexity2.6619699001312256
INFO:root:current mean train loss 1244.968388081645
INFO:root:current train perplexity2.665170431137085
INFO:root:current mean train loss 1246.5600959846565
INFO:root:current train perplexity2.6685383319854736
INFO:root:current mean train loss 1247.0371964083374
INFO:root:current train perplexity2.6708896160125732
INFO:root:current mean train loss 1247.637468471906
INFO:root:current train perplexity2.671597480773926
INFO:root:current mean train loss 1246.8747410110564
INFO:root:current train perplexity2.6718456745147705
INFO:root:current mean train loss 1247.037495142752
INFO:root:current train perplexity2.6716177463531494
INFO:root:current mean train loss 1247.1375086779842
INFO:root:current train perplexity2.6712794303894043
INFO:root:current mean train loss 1247.3127293442235
INFO:root:current train perplexity2.6713953018188477
INFO:root:current mean train loss 1248.6838090995393
INFO:root:current train perplexity2.672017812728882
INFO:root:current mean train loss 1248.1979825516028
INFO:root:current train perplexity2.6718947887420654
INFO:root:current mean train loss 1248.1291006396316
INFO:root:current train perplexity2.6732373237609863
INFO:root:current mean train loss 1248.326529293735
INFO:root:current train perplexity2.6743686199188232
INFO:root:current mean train loss 1248.4176854436132
INFO:root:current train perplexity2.676374673843384
INFO:root:current mean train loss 1248.3261989321804
INFO:root:current train perplexity2.6769869327545166
INFO:root:current mean train loss 1248.5239303218707
INFO:root:current train perplexity2.6770987510681152
INFO:root:current mean train loss 1248.4424885859576
INFO:root:current train perplexity2.676382541656494

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.25s/it]
INFO:root:final mean train loss: 1248.2713953615498
INFO:root:final train perplexity: 2.6763715744018555
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.21s/it]
INFO:root:eval mean loss: 2341.853087513159
INFO:root:eval perplexity: 6.645588397979736
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it]
INFO:root:eval mean loss: 2934.064525414866
INFO:root:eval perplexity: 11.018311500549316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [29:28:27<5:23:45, 626.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1242.022708468967
INFO:root:current train perplexity2.6427979469299316
INFO:root:current mean train loss 1243.9305299270984
INFO:root:current train perplexity2.648158073425293
INFO:root:current mean train loss 1246.0416479671703
INFO:root:current train perplexity2.6550025939941406
INFO:root:current mean train loss 1246.3037201255881
INFO:root:current train perplexity2.6606435775756836
INFO:root:current mean train loss 1246.2373853780455
INFO:root:current train perplexity2.6604483127593994
INFO:root:current mean train loss 1245.0439875676082
INFO:root:current train perplexity2.664931058883667
INFO:root:current mean train loss 1244.604806627546
INFO:root:current train perplexity2.6650478839874268
INFO:root:current mean train loss 1245.0133002879088
INFO:root:current train perplexity2.665163516998291
INFO:root:current mean train loss 1244.9259020604125
INFO:root:current train perplexity2.6661148071289062
INFO:root:current mean train loss 1245.4501120484906
INFO:root:current train perplexity2.6674633026123047
INFO:root:current mean train loss 1244.2706892098954
INFO:root:current train perplexity2.666459321975708
INFO:root:current mean train loss 1244.7720299418063
INFO:root:current train perplexity2.667851209640503
INFO:root:current mean train loss 1245.5239225183643
INFO:root:current train perplexity2.6678826808929443
INFO:root:current mean train loss 1245.7730182614341
INFO:root:current train perplexity2.6693718433380127
INFO:root:current mean train loss 1246.0400713215704
INFO:root:current train perplexity2.6699211597442627
INFO:root:current mean train loss 1245.4441244887335
INFO:root:current train perplexity2.6696724891662598
INFO:root:current mean train loss 1245.4446572481731
INFO:root:current train perplexity2.6702067852020264
INFO:root:current mean train loss 1245.7837986655484
INFO:root:current train perplexity2.6707518100738525
INFO:root:current mean train loss 1246.0965868305957
INFO:root:current train perplexity2.6719179153442383
INFO:root:current mean train loss 1246.4651944235904
INFO:root:current train perplexity2.6719088554382324

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.67s/it]
INFO:root:final mean train loss: 1246.1145084475365
INFO:root:final train perplexity: 2.6718225479125977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it]
INFO:root:eval mean loss: 2341.372281987616
INFO:root:eval perplexity: 6.64300537109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.30s/it]
INFO:root:eval mean loss: 2934.3614038778537
INFO:root:eval perplexity: 11.02098560333252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [29:39:02<5:14:28, 628.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1236.1732836091116
INFO:root:current train perplexity2.662219762802124
INFO:root:current mean train loss 1240.06296890501
INFO:root:current train perplexity2.6690866947174072
INFO:root:current mean train loss 1243.4614139543685
INFO:root:current train perplexity2.664329767227173
INFO:root:current mean train loss 1243.3919941330937
INFO:root:current train perplexity2.6610162258148193
INFO:root:current mean train loss 1243.4526966305598
INFO:root:current train perplexity2.664593458175659
INFO:root:current mean train loss 1242.345068525175
INFO:root:current train perplexity2.6641368865966797
INFO:root:current mean train loss 1242.9017445601642
INFO:root:current train perplexity2.6651949882507324
INFO:root:current mean train loss 1243.4879213823867
INFO:root:current train perplexity2.6643381118774414
INFO:root:current mean train loss 1243.7900824530723
INFO:root:current train perplexity2.6650147438049316
INFO:root:current mean train loss 1244.3818696333497
INFO:root:current train perplexity2.6650800704956055
INFO:root:current mean train loss 1243.7114822766011
INFO:root:current train perplexity2.664015293121338
INFO:root:current mean train loss 1243.274689927033
INFO:root:current train perplexity2.663968324661255
INFO:root:current mean train loss 1243.5550204706894
INFO:root:current train perplexity2.66505765914917
INFO:root:current mean train loss 1243.5752029583728
INFO:root:current train perplexity2.6654844284057617
INFO:root:current mean train loss 1243.6950443388232
INFO:root:current train perplexity2.666139602661133
INFO:root:current mean train loss 1244.0082768127556
INFO:root:current train perplexity2.6674070358276367
INFO:root:current mean train loss 1244.0901561170162
INFO:root:current train perplexity2.6676247119903564
INFO:root:current mean train loss 1244.3056860337915
INFO:root:current train perplexity2.6684021949768066
INFO:root:current mean train loss 1244.7993120765989
INFO:root:current train perplexity2.6683084964752197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:07<00:00, 547.75s/it]
INFO:root:final mean train loss: 1245.0716873145861
INFO:root:final train perplexity: 2.669625997543335
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it]
INFO:root:eval mean loss: 2343.8454572528813
INFO:root:eval perplexity: 6.656304359436035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.81s/it]
INFO:root:eval mean loss: 2935.8251347102173
INFO:root:eval perplexity: 11.034189224243164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [29:49:23<5:02:50, 626.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1240.7540690104167
INFO:root:current train perplexity2.6830246448516846
INFO:root:current mean train loss 1251.3286017651828
INFO:root:current train perplexity2.6559088230133057
INFO:root:current mean train loss 1241.0609012344507
INFO:root:current train perplexity2.6455063819885254
INFO:root:current mean train loss 1239.4377608953737
INFO:root:current train perplexity2.645958423614502
INFO:root:current mean train loss 1241.007318205434
INFO:root:current train perplexity2.6519439220428467
INFO:root:current mean train loss 1243.2222980001698
INFO:root:current train perplexity2.6562440395355225
INFO:root:current mean train loss 1242.0661689582043
INFO:root:current train perplexity2.6553590297698975
INFO:root:current mean train loss 1241.1724220686526
INFO:root:current train perplexity2.6556084156036377
INFO:root:current mean train loss 1240.7823345477764
INFO:root:current train perplexity2.658177137374878
INFO:root:current mean train loss 1241.257447366946
INFO:root:current train perplexity2.6597447395324707
INFO:root:current mean train loss 1240.96452413449
INFO:root:current train perplexity2.6602234840393066
INFO:root:current mean train loss 1241.1803977373065
INFO:root:current train perplexity2.6605701446533203
INFO:root:current mean train loss 1242.1338693292976
INFO:root:current train perplexity2.6617395877838135
INFO:root:current mean train loss 1242.8872110504103
INFO:root:current train perplexity2.662534236907959
INFO:root:current mean train loss 1243.4066056187767
INFO:root:current train perplexity2.6627626419067383
INFO:root:current mean train loss 1243.1172434286293
INFO:root:current train perplexity2.663545608520508
INFO:root:current mean train loss 1243.9908583321578
INFO:root:current train perplexity2.6643662452697754
INFO:root:current mean train loss 1244.341795086162
INFO:root:current train perplexity2.665595769882202
INFO:root:current mean train loss 1244.3961545959
INFO:root:current train perplexity2.6668100357055664
INFO:root:current mean train loss 1244.2363676409407
INFO:root:current train perplexity2.666527032852173

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.29s/it]
INFO:root:final mean train loss: 1243.9730299402834
INFO:root:final train perplexity: 2.667314291000366
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.22s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.22s/it]
INFO:root:eval mean loss: 2344.0886987997287
INFO:root:eval perplexity: 6.657614707946777
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.41s/it]
INFO:root:eval mean loss: 2937.4811102684507
INFO:root:eval perplexity: 11.049144744873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [29:59:55<4:53:11, 628.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1227.5521187160325
INFO:root:current train perplexity2.637847900390625
INFO:root:current mean train loss 1234.4390224053607
INFO:root:current train perplexity2.6556477546691895
INFO:root:current mean train loss 1234.719831663397
INFO:root:current train perplexity2.660836696624756
INFO:root:current mean train loss 1235.0901249274382
INFO:root:current train perplexity2.6585848331451416
INFO:root:current mean train loss 1235.5448353003103
INFO:root:current train perplexity2.6557042598724365
INFO:root:current mean train loss 1236.2069271517985
INFO:root:current train perplexity2.659778118133545
INFO:root:current mean train loss 1237.4834197949062
INFO:root:current train perplexity2.659712553024292
INFO:root:current mean train loss 1237.5438497498487
INFO:root:current train perplexity2.659329891204834
INFO:root:current mean train loss 1238.6949117296667
INFO:root:current train perplexity2.659679651260376
INFO:root:current mean train loss 1240.1547192938278
INFO:root:current train perplexity2.6604623794555664
INFO:root:current mean train loss 1239.9481057980893
INFO:root:current train perplexity2.659311294555664
INFO:root:current mean train loss 1239.686739968277
INFO:root:current train perplexity2.658710479736328
INFO:root:current mean train loss 1240.1979603178659
INFO:root:current train perplexity2.6594252586364746
INFO:root:current mean train loss 1241.5176503706953
INFO:root:current train perplexity2.6618611812591553
INFO:root:current mean train loss 1242.3944378314686
INFO:root:current train perplexity2.6611239910125732
INFO:root:current mean train loss 1242.690338715862
INFO:root:current train perplexity2.6622231006622314
INFO:root:current mean train loss 1242.5700268419305
INFO:root:current train perplexity2.6624882221221924
INFO:root:current mean train loss 1242.4776054466456
INFO:root:current train perplexity2.6631622314453125
INFO:root:current mean train loss 1242.3344181498132
INFO:root:current train perplexity2.6638901233673096
INFO:root:current mean train loss 1242.4572470154665
INFO:root:current train perplexity2.663755416870117

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.13s/it]
INFO:root:final mean train loss: 1242.3211100198857
INFO:root:final train perplexity: 2.6638412475585938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.71s/it]
INFO:root:eval mean loss: 2347.0983462502772
INFO:root:eval perplexity: 6.673840045928955
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.72s/it]
INFO:root:eval mean loss: 2940.854896924174
INFO:root:eval perplexity: 11.079668998718262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [30:10:30<4:43:35, 630.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1230.9023040771485
INFO:root:current train perplexity2.651247024536133
INFO:root:current mean train loss 1235.1017342703683
INFO:root:current train perplexity2.65256667137146
INFO:root:current mean train loss 1239.944893391927
INFO:root:current train perplexity2.6506779193878174
INFO:root:current mean train loss 1239.2991225298713
INFO:root:current train perplexity2.650801420211792
INFO:root:current mean train loss 1238.5553638805043
INFO:root:current train perplexity2.6535651683807373
INFO:root:current mean train loss 1238.7244599518951
INFO:root:current train perplexity2.6537587642669678
INFO:root:current mean train loss 1239.1755834579467
INFO:root:current train perplexity2.6541295051574707
INFO:root:current mean train loss 1239.7143501900339
INFO:root:current train perplexity2.653228521347046
INFO:root:current mean train loss 1239.433496965681
INFO:root:current train perplexity2.655519962310791
INFO:root:current mean train loss 1240.0171685401428
INFO:root:current train perplexity2.6575469970703125
INFO:root:current mean train loss 1239.825690166767
INFO:root:current train perplexity2.6575069427490234
INFO:root:current mean train loss 1239.3091092293723
INFO:root:current train perplexity2.658257007598877
INFO:root:current mean train loss 1239.3129298056326
INFO:root:current train perplexity2.657416343688965
INFO:root:current mean train loss 1239.058670453883
INFO:root:current train perplexity2.658884048461914
INFO:root:current mean train loss 1239.8652625189886
INFO:root:current train perplexity2.6604392528533936
INFO:root:current mean train loss 1240.1190302068537
INFO:root:current train perplexity2.6607587337493896
INFO:root:current mean train loss 1240.4493682861328
INFO:root:current train perplexity2.661348581314087
INFO:root:current mean train loss 1241.0138817798131
INFO:root:current train perplexity2.661928176879883
INFO:root:current mean train loss 1241.083074022376
INFO:root:current train perplexity2.661970615386963
INFO:root:current mean train loss 1241.6087038649725
INFO:root:current train perplexity2.662249803543091

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.13s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.13s/it]
INFO:root:final mean train loss: 1241.5293933677963
INFO:root:final train perplexity: 2.6621785163879395
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2346.5552476382427
INFO:root:eval perplexity: 6.670909404754639
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it]
INFO:root:eval mean loss: 2939.8700193581008
INFO:root:eval perplexity: 11.070752143859863
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [30:20:54<4:32:19, 628.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1239.9694502981085
INFO:root:current train perplexity2.663358688354492
INFO:root:current mean train loss 1234.9104322688595
INFO:root:current train perplexity2.6560587882995605
INFO:root:current mean train loss 1234.8745107687864
INFO:root:current train perplexity2.6506688594818115
INFO:root:current mean train loss 1237.5943066679797
INFO:root:current train perplexity2.6518685817718506
INFO:root:current mean train loss 1237.3837174764087
INFO:root:current train perplexity2.6490566730499268
INFO:root:current mean train loss 1237.502505838336
INFO:root:current train perplexity2.6533308029174805
INFO:root:current mean train loss 1237.0011736958356
INFO:root:current train perplexity2.6535816192626953
INFO:root:current mean train loss 1237.1202263573832
INFO:root:current train perplexity2.653252363204956
INFO:root:current mean train loss 1237.591726082765
INFO:root:current train perplexity2.6552693843841553
INFO:root:current mean train loss 1238.8476484691337
INFO:root:current train perplexity2.656456232070923
INFO:root:current mean train loss 1238.8901039202933
INFO:root:current train perplexity2.6572060585021973
INFO:root:current mean train loss 1239.016011362562
INFO:root:current train perplexity2.657972812652588
INFO:root:current mean train loss 1240.0463430181608
INFO:root:current train perplexity2.65830397605896
INFO:root:current mean train loss 1240.3266436043432
INFO:root:current train perplexity2.6583290100097656
INFO:root:current mean train loss 1240.2169075509662
INFO:root:current train perplexity2.6584718227386475
INFO:root:current mean train loss 1240.625702315902
INFO:root:current train perplexity2.6587109565734863
INFO:root:current mean train loss 1240.6280603771406
INFO:root:current train perplexity2.6588873863220215
INFO:root:current mean train loss 1241.0053326732134
INFO:root:current train perplexity2.6599314212799072
INFO:root:current mean train loss 1241.17793177788
INFO:root:current train perplexity2.6596837043762207
INFO:root:current mean train loss 1241.2815707386544
INFO:root:current train perplexity2.6606338024139404

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.26s/it]
INFO:root:final mean train loss: 1240.8883986172505
INFO:root:final train perplexity: 2.6608331203460693
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 2346.8179407863754
INFO:root:eval perplexity: 6.672327041625977
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.19s/it]
INFO:root:eval mean loss: 2940.645839393562
INFO:root:eval perplexity: 11.077775001525879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [30:31:16<4:21:03, 626.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1235.4715048300254
INFO:root:current train perplexity2.640404224395752
INFO:root:current mean train loss 1235.6887978740122
INFO:root:current train perplexity2.64229154586792
INFO:root:current mean train loss 1232.1577424654995
INFO:root:current train perplexity2.6504130363464355
INFO:root:current mean train loss 1232.1240257222385
INFO:root:current train perplexity2.6501786708831787
INFO:root:current mean train loss 1233.1953001384493
INFO:root:current train perplexity2.6507043838500977
INFO:root:current mean train loss 1235.1599082813862
INFO:root:current train perplexity2.653031349182129
INFO:root:current mean train loss 1236.7778438036096
INFO:root:current train perplexity2.6554598808288574
INFO:root:current mean train loss 1237.6125087688752
INFO:root:current train perplexity2.6552677154541016
INFO:root:current mean train loss 1237.9735139545642
INFO:root:current train perplexity2.6558644771575928
INFO:root:current mean train loss 1238.2863314587478
INFO:root:current train perplexity2.6564412117004395
INFO:root:current mean train loss 1237.8583948003957
INFO:root:current train perplexity2.6561362743377686
INFO:root:current mean train loss 1238.485392321923
INFO:root:current train perplexity2.656153678894043
INFO:root:current mean train loss 1239.1828953430079
INFO:root:current train perplexity2.656912326812744
INFO:root:current mean train loss 1238.7366170425082
INFO:root:current train perplexity2.6558289527893066
INFO:root:current mean train loss 1239.255017222349
INFO:root:current train perplexity2.656920909881592
INFO:root:current mean train loss 1239.5820822806643
INFO:root:current train perplexity2.6582181453704834
INFO:root:current mean train loss 1240.0684871331766
INFO:root:current train perplexity2.6593141555786133
INFO:root:current mean train loss 1240.1741007532853
INFO:root:current train perplexity2.6593570709228516
INFO:root:current mean train loss 1240.4194214127742
INFO:root:current train perplexity2.659602642059326
INFO:root:current mean train loss 1240.5712006326385
INFO:root:current train perplexity2.6591761112213135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.67s/it]
INFO:root:final mean train loss: 1240.206844143235
INFO:root:final train perplexity: 2.659403085708618
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.44s/it]
INFO:root:eval mean loss: 2348.416393956394
INFO:root:eval perplexity: 6.680957794189453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.38s/it]
INFO:root:eval mean loss: 2943.2216125921154
INFO:root:eval perplexity: 11.101137161254883
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [30:41:51<4:11:35, 628.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1232.5787205958104
INFO:root:current train perplexity2.6550557613372803
INFO:root:current mean train loss 1236.3584124979548
INFO:root:current train perplexity2.657642126083374
INFO:root:current mean train loss 1236.7832392007624
INFO:root:current train perplexity2.657057046890259
INFO:root:current mean train loss 1236.983238903153
INFO:root:current train perplexity2.6523666381835938
INFO:root:current mean train loss 1236.49702108659
INFO:root:current train perplexity2.6525721549987793
INFO:root:current mean train loss 1237.0426043980012
INFO:root:current train perplexity2.6526546478271484
INFO:root:current mean train loss 1238.3324990389833
INFO:root:current train perplexity2.6543967723846436
INFO:root:current mean train loss 1237.9478708838692
INFO:root:current train perplexity2.655677318572998
INFO:root:current mean train loss 1237.5165426509013
INFO:root:current train perplexity2.6542601585388184
INFO:root:current mean train loss 1237.6058219039717
INFO:root:current train perplexity2.6524574756622314
INFO:root:current mean train loss 1236.9056558722646
INFO:root:current train perplexity2.653110980987549
INFO:root:current mean train loss 1237.657479415112
INFO:root:current train perplexity2.653355360031128
INFO:root:current mean train loss 1237.580798443854
INFO:root:current train perplexity2.6537892818450928
INFO:root:current mean train loss 1237.60494334744
INFO:root:current train perplexity2.6538145542144775
INFO:root:current mean train loss 1237.65870450568
INFO:root:current train perplexity2.654107093811035
INFO:root:current mean train loss 1238.0064839207848
INFO:root:current train perplexity2.654595136642456
INFO:root:current mean train loss 1238.5332530070732
INFO:root:current train perplexity2.655604839324951
INFO:root:current mean train loss 1238.5553642370926
INFO:root:current train perplexity2.6557793617248535
INFO:root:current mean train loss 1238.3631086989976
INFO:root:current train perplexity2.6556787490844727

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:08<00:00, 548.20s/it]
INFO:root:final mean train loss: 1238.5682686071352
INFO:root:final train perplexity: 2.6559689044952393
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it]
INFO:root:eval mean loss: 2349.5545411887742
INFO:root:eval perplexity: 6.68710994720459
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.79s/it]
INFO:root:eval mean loss: 2943.925873019171
INFO:root:eval perplexity: 11.10753345489502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [30:52:13<4:00:18, 626.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1206.0564270019531
INFO:root:current train perplexity2.6286966800689697
INFO:root:current mean train loss 1245.8008795844185
INFO:root:current train perplexity2.665242910385132
INFO:root:current mean train loss 1241.8243989210862
INFO:root:current train perplexity2.6623573303222656
INFO:root:current mean train loss 1235.6661892184964
INFO:root:current train perplexity2.6562130451202393
INFO:root:current mean train loss 1236.4777027204925
INFO:root:current train perplexity2.6540462970733643
INFO:root:current mean train loss 1235.1508935642994
INFO:root:current train perplexity2.6527888774871826
INFO:root:current mean train loss 1235.7131446035285
INFO:root:current train perplexity2.6549694538116455
INFO:root:current mean train loss 1237.0419382213872
INFO:root:current train perplexity2.654233694076538
INFO:root:current mean train loss 1236.7184486011467
INFO:root:current train perplexity2.652820110321045
INFO:root:current mean train loss 1237.1943360719388
INFO:root:current train perplexity2.6544349193573
INFO:root:current mean train loss 1237.4016395447745
INFO:root:current train perplexity2.653503656387329
INFO:root:current mean train loss 1236.9221798452659
INFO:root:current train perplexity2.653627634048462
INFO:root:current mean train loss 1236.6505918187022
INFO:root:current train perplexity2.6531100273132324
INFO:root:current mean train loss 1236.4882691176294
INFO:root:current train perplexity2.653210163116455
INFO:root:current mean train loss 1236.8177777203646
INFO:root:current train perplexity2.652757406234741
INFO:root:current mean train loss 1236.46600495599
INFO:root:current train perplexity2.6527657508850098
INFO:root:current mean train loss 1236.7701901108471
INFO:root:current train perplexity2.653975486755371
INFO:root:current mean train loss 1237.241859096554
INFO:root:current train perplexity2.65464448928833
INFO:root:current mean train loss 1237.5489667369202
INFO:root:current train perplexity2.65429425239563
INFO:root:current mean train loss 1238.2037546729641
INFO:root:current train perplexity2.654362916946411

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.36s/it]
INFO:root:final mean train loss: 1237.7637861273954
INFO:root:final train perplexity: 2.6542840003967285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 2351.3228266289893
INFO:root:eval perplexity: 6.696679592132568
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.44s/it]
INFO:root:eval mean loss: 2946.7894105302526
INFO:root:eval perplexity: 11.133574485778809
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [31:02:48<3:50:45, 629.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1233.2356884765625
INFO:root:current train perplexity2.7002902030944824
INFO:root:current mean train loss 1228.2863974609375
INFO:root:current train perplexity2.6473608016967773
INFO:root:current mean train loss 1235.1207807074652
INFO:root:current train perplexity2.645939826965332
INFO:root:current mean train loss 1233.557247596154
INFO:root:current train perplexity2.6483852863311768
INFO:root:current mean train loss 1233.1347397748161
INFO:root:current train perplexity2.647890090942383
INFO:root:current mean train loss 1233.368474469866
INFO:root:current train perplexity2.6466891765594482
INFO:root:current mean train loss 1233.49545078125
INFO:root:current train perplexity2.6483612060546875
INFO:root:current mean train loss 1233.3143896484376
INFO:root:current train perplexity2.64867901802063
INFO:root:current mean train loss 1233.7484536280776
INFO:root:current train perplexity2.6508848667144775
INFO:root:current mean train loss 1234.1797239231419
INFO:root:current train perplexity2.650228261947632
INFO:root:current mean train loss 1234.8808031631097
INFO:root:current train perplexity2.649599552154541
INFO:root:current mean train loss 1234.7193077256945
INFO:root:current train perplexity2.6494219303131104
INFO:root:current mean train loss 1234.508798030931
INFO:root:current train perplexity2.6501519680023193
INFO:root:current mean train loss 1235.478806014151
INFO:root:current train perplexity2.650233268737793
INFO:root:current mean train loss 1235.9273304721764
INFO:root:current train perplexity2.6499722003936768
INFO:root:current mean train loss 1235.9567940733862
INFO:root:current train perplexity2.6511354446411133
INFO:root:current mean train loss 1236.4280536358174
INFO:root:current train perplexity2.6507513523101807
INFO:root:current mean train loss 1236.4275877490943
INFO:root:current train perplexity2.651172637939453
INFO:root:current mean train loss 1236.3805073442852
INFO:root:current train perplexity2.652055025100708
INFO:root:current mean train loss 1236.8006296291599
INFO:root:current train perplexity2.651996612548828

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.11s/it]
INFO:root:final mean train loss: 1236.777004501643
INFO:root:final train perplexity: 2.652219295501709
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.55s/it]
INFO:root:eval mean loss: 2350.336559106272
INFO:root:eval perplexity: 6.69133996963501
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.17s/it]
INFO:root:eval mean loss: 2945.2982623594025
INFO:root:eval perplexity: 11.120007514953613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [31:13:14<3:39:55, 628.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1233.6315307617188
INFO:root:current train perplexity2.638326406478882
INFO:root:current mean train loss 1229.4136154819541
INFO:root:current train perplexity2.633859872817993
INFO:root:current mean train loss 1232.2792136452415
INFO:root:current train perplexity2.6350247859954834
INFO:root:current mean train loss 1232.4305473461486
INFO:root:current train perplexity2.6407110691070557
INFO:root:current mean train loss 1233.4672724521
INFO:root:current train perplexity2.6464455127716064
INFO:root:current mean train loss 1234.5636107947994
INFO:root:current train perplexity2.646695613861084
INFO:root:current mean train loss 1233.545186360677
INFO:root:current train perplexity2.646251916885376
INFO:root:current mean train loss 1233.6271822947376
INFO:root:current train perplexity2.647674560546875
INFO:root:current mean train loss 1233.1965462510207
INFO:root:current train perplexity2.647517204284668
INFO:root:current mean train loss 1232.1260312479267
INFO:root:current train perplexity2.64572811126709
INFO:root:current mean train loss 1232.6581017902397
INFO:root:current train perplexity2.646527051925659
INFO:root:current mean train loss 1234.141724487946
INFO:root:current train perplexity2.647733449935913
INFO:root:current mean train loss 1234.4844699435764
INFO:root:current train perplexity2.649020195007324
INFO:root:current mean train loss 1235.1615862555013
INFO:root:current train perplexity2.649038076400757
INFO:root:current mean train loss 1235.9175356628164
INFO:root:current train perplexity2.649726390838623
INFO:root:current mean train loss 1236.4503889467312
INFO:root:current train perplexity2.6498687267303467
INFO:root:current mean train loss 1236.3165266847784
INFO:root:current train perplexity2.6508257389068604
INFO:root:current mean train loss 1236.0561528342737
INFO:root:current train perplexity2.650510311126709
INFO:root:current mean train loss 1236.295552240262
INFO:root:current train perplexity2.650630235671997
INFO:root:current mean train loss 1236.5803663291106
INFO:root:current train perplexity2.6513185501098633

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.20s/it]
INFO:root:final mean train loss: 1236.021472155657
INFO:root:final train perplexity: 2.650639295578003
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.33s/it]
INFO:root:eval mean loss: 2353.2871059120125
INFO:root:eval perplexity: 6.707326889038086
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2949.4357633117243
INFO:root:eval perplexity: 11.157697677612305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [31:23:39<3:29:06, 627.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1232.1327235335011
INFO:root:current train perplexity2.6185719966888428
INFO:root:current mean train loss 1240.2071947781544
INFO:root:current train perplexity2.639162302017212
INFO:root:current mean train loss 1237.6421016265988
INFO:root:current train perplexity2.644576072692871
INFO:root:current mean train loss 1235.1686399667044
INFO:root:current train perplexity2.642117738723755
INFO:root:current mean train loss 1235.3982125076593
INFO:root:current train perplexity2.642972230911255
INFO:root:current mean train loss 1234.1392145310404
INFO:root:current train perplexity2.645171642303467
INFO:root:current mean train loss 1235.7703209096999
INFO:root:current train perplexity2.645423650741577
INFO:root:current mean train loss 1235.5791108906662
INFO:root:current train perplexity2.6437013149261475
INFO:root:current mean train loss 1234.3728450823996
INFO:root:current train perplexity2.6425867080688477
INFO:root:current mean train loss 1233.085772024081
INFO:root:current train perplexity2.6432759761810303
INFO:root:current mean train loss 1232.3010061406324
INFO:root:current train perplexity2.6435749530792236
INFO:root:current mean train loss 1232.570344307795
INFO:root:current train perplexity2.645376205444336
INFO:root:current mean train loss 1233.0921529053317
INFO:root:current train perplexity2.6456453800201416
INFO:root:current mean train loss 1233.090738842468
INFO:root:current train perplexity2.645401954650879
INFO:root:current mean train loss 1233.1881196386116
INFO:root:current train perplexity2.644582509994507
INFO:root:current mean train loss 1233.5682513204579
INFO:root:current train perplexity2.6460959911346436
INFO:root:current mean train loss 1234.2080716069308
INFO:root:current train perplexity2.6462435722351074
INFO:root:current mean train loss 1234.6632072444393
INFO:root:current train perplexity2.646644115447998
INFO:root:current mean train loss 1235.097864275148
INFO:root:current train perplexity2.6476497650146484
INFO:root:current mean train loss 1235.0708333084083
INFO:root:current train perplexity2.6478116512298584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:18<00:00, 558.24s/it]
INFO:root:final mean train loss: 1234.7797617024985
INFO:root:final train perplexity: 2.648045063018799
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.01s/it]
INFO:root:eval mean loss: 2353.209913754294
INFO:root:eval perplexity: 6.706908226013184
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.12s/it]
INFO:root:eval mean loss: 2948.9743383962214
INFO:root:eval perplexity: 11.15349006652832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [31:34:12<3:19:09, 628.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1223.9606676603619
INFO:root:current train perplexity2.6281795501708984
INFO:root:current mean train loss 1229.0025960748847
INFO:root:current train perplexity2.6328680515289307
INFO:root:current mean train loss 1230.1514428180196
INFO:root:current train perplexity2.6337547302246094
INFO:root:current mean train loss 1228.821867273209
INFO:root:current train perplexity2.6368215084075928
INFO:root:current mean train loss 1230.1832613904937
INFO:root:current train perplexity2.6413190364837646
INFO:root:current mean train loss 1230.6415708329941
INFO:root:current train perplexity2.6382131576538086
INFO:root:current mean train loss 1230.3027394311669
INFO:root:current train perplexity2.6389145851135254
INFO:root:current mean train loss 1231.424950196571
INFO:root:current train perplexity2.6408817768096924
INFO:root:current mean train loss 1231.6995258766767
INFO:root:current train perplexity2.643446207046509
INFO:root:current mean train loss 1230.975251244717
INFO:root:current train perplexity2.6433916091918945
INFO:root:current mean train loss 1231.3912228722554
INFO:root:current train perplexity2.6439361572265625
INFO:root:current mean train loss 1232.1282508486793
INFO:root:current train perplexity2.644376039505005
INFO:root:current mean train loss 1232.6375829044928
INFO:root:current train perplexity2.644566297531128
INFO:root:current mean train loss 1232.5987351883289
INFO:root:current train perplexity2.6442885398864746
INFO:root:current mean train loss 1233.0801372579765
INFO:root:current train perplexity2.6449644565582275
INFO:root:current mean train loss 1234.0651199418276
INFO:root:current train perplexity2.646134853363037
INFO:root:current mean train loss 1234.278007343447
INFO:root:current train perplexity2.646725654602051
INFO:root:current mean train loss 1234.427277779794
INFO:root:current train perplexity2.6454379558563232
INFO:root:current mean train loss 1234.1338745637743
INFO:root:current train perplexity2.6449363231658936
INFO:root:current mean train loss 1234.1777058960456
INFO:root:current train perplexity2.6461918354034424

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.11s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:17<00:00, 557.11s/it]
INFO:root:final mean train loss: 1233.8506743969247
INFO:root:final train perplexity: 2.6461050510406494
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.35s/it]
INFO:root:eval mean loss: 2354.5188213375445
INFO:root:eval perplexity: 6.714009761810303
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.58s/it]
INFO:root:eval mean loss: 2950.4003546965037
INFO:root:eval perplexity: 11.16650104522705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [31:44:44<3:08:58, 629.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1236.189461000504
INFO:root:current train perplexity2.6384034156799316
INFO:root:current mean train loss 1235.0106979132934
INFO:root:current train perplexity2.637902021408081
INFO:root:current mean train loss 1232.541795958431
INFO:root:current train perplexity2.634568452835083
INFO:root:current mean train loss 1231.8716363882593
INFO:root:current train perplexity2.6379573345184326
INFO:root:current mean train loss 1233.210588126347
INFO:root:current train perplexity2.6384716033935547
INFO:root:current mean train loss 1232.3039176130376
INFO:root:current train perplexity2.6403284072875977
INFO:root:current mean train loss 1232.5063453663306
INFO:root:current train perplexity2.643899440765381
INFO:root:current mean train loss 1233.0461096360734
INFO:root:current train perplexity2.6435139179229736
INFO:root:current mean train loss 1232.6992328297786
INFO:root:current train perplexity2.6427669525146484
INFO:root:current mean train loss 1232.666863970646
INFO:root:current train perplexity2.6444761753082275
INFO:root:current mean train loss 1232.7149960866022
INFO:root:current train perplexity2.6453583240509033
INFO:root:current mean train loss 1232.6851793338747
INFO:root:current train perplexity2.644604206085205
INFO:root:current mean train loss 1232.5414242253964
INFO:root:current train perplexity2.6447317600250244
INFO:root:current mean train loss 1233.021681457651
INFO:root:current train perplexity2.6445400714874268
INFO:root:current mean train loss 1232.8985616961183
INFO:root:current train perplexity2.644258975982666
INFO:root:current mean train loss 1233.1993011263878
INFO:root:current train perplexity2.644660234451294
INFO:root:current mean train loss 1233.276365457029
INFO:root:current train perplexity2.6445653438568115
INFO:root:current mean train loss 1233.3485035255376
INFO:root:current train perplexity2.644691228866577
INFO:root:current mean train loss 1233.6936369189118
INFO:root:current train perplexity2.644425630569458

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.68s/it]
INFO:root:final mean train loss: 1233.2921203520943
INFO:root:final train perplexity: 2.644940137863159
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.74s/it]
INFO:root:eval mean loss: 2354.2857484208776
INFO:root:eval perplexity: 6.712745666503906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.92s/it]
INFO:root:eval mean loss: 2949.6874272772607
INFO:root:eval perplexity: 11.159994125366211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [31:55:10<2:58:06, 628.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1253.7607666015624
INFO:root:current train perplexity2.6287624835968018
INFO:root:current mean train loss 1233.1884099786932
INFO:root:current train perplexity2.6292262077331543
INFO:root:current mean train loss 1231.6034545898438
INFO:root:current train perplexity2.634016752243042
INFO:root:current mean train loss 1230.3235875283517
INFO:root:current train perplexity2.6384992599487305
INFO:root:current mean train loss 1229.7684954387385
INFO:root:current train perplexity2.640977621078491
INFO:root:current mean train loss 1230.17722455193
INFO:root:current train perplexity2.643705129623413
INFO:root:current mean train loss 1231.567955142162
INFO:root:current train perplexity2.644681692123413
INFO:root:current mean train loss 1231.432137502751
INFO:root:current train perplexity2.643442392349243
INFO:root:current mean train loss 1231.6853316695601
INFO:root:current train perplexity2.643930435180664
INFO:root:current mean train loss 1231.7676340627147
INFO:root:current train perplexity2.64343523979187
INFO:root:current mean train loss 1232.0232059289913
INFO:root:current train perplexity2.6444880962371826
INFO:root:current mean train loss 1232.240207541526
INFO:root:current train perplexity2.6450531482696533
INFO:root:current mean train loss 1233.1868725989475
INFO:root:current train perplexity2.643481731414795
INFO:root:current mean train loss 1233.315925889343
INFO:root:current train perplexity2.6434311866760254
INFO:root:current mean train loss 1233.6148924915503
INFO:root:current train perplexity2.643380880355835
INFO:root:current mean train loss 1233.2990263477857
INFO:root:current train perplexity2.642401933670044
INFO:root:current mean train loss 1232.7448332413383
INFO:root:current train perplexity2.6418490409851074
INFO:root:current mean train loss 1232.7963032683433
INFO:root:current train perplexity2.6420514583587646
INFO:root:current mean train loss 1232.2536594116884
INFO:root:current train perplexity2.6417930126190186
INFO:root:current mean train loss 1232.3133370444414
INFO:root:current train perplexity2.642677068710327

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:12<00:00, 552.05s/it]
INFO:root:final mean train loss: 1232.4426500067468
INFO:root:final train perplexity: 2.6431686878204346
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 2355.7873682333225
INFO:root:eval perplexity: 6.72090482711792
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it]
INFO:root:eval mean loss: 2952.163101520944
INFO:root:eval perplexity: 11.182611465454102
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [32:05:36<2:47:29, 628.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1229.931667751736
INFO:root:current train perplexity2.642306327819824
INFO:root:current mean train loss 1228.2930225762796
INFO:root:current train perplexity2.640601634979248
INFO:root:current mean train loss 1228.0316974118944
INFO:root:current train perplexity2.6346006393432617
INFO:root:current mean train loss 1226.8938085340214
INFO:root:current train perplexity2.6270651817321777
INFO:root:current mean train loss 1226.981885851965
INFO:root:current train perplexity2.628587007522583
INFO:root:current mean train loss 1225.8203838428012
INFO:root:current train perplexity2.629932403564453
INFO:root:current mean train loss 1227.402893942509
INFO:root:current train perplexity2.629453182220459
INFO:root:current mean train loss 1228.3650338908828
INFO:root:current train perplexity2.6306533813476562
INFO:root:current mean train loss 1229.3023618169966
INFO:root:current train perplexity2.6337344646453857
INFO:root:current mean train loss 1229.8863454281704
INFO:root:current train perplexity2.63555645942688
INFO:root:current mean train loss 1229.744228344465
INFO:root:current train perplexity2.6384308338165283
INFO:root:current mean train loss 1230.0003316586485
INFO:root:current train perplexity2.637718677520752
INFO:root:current mean train loss 1230.5755367512224
INFO:root:current train perplexity2.638333559036255
INFO:root:current mean train loss 1230.792608518392
INFO:root:current train perplexity2.639509677886963
INFO:root:current mean train loss 1230.6621762698735
INFO:root:current train perplexity2.6395134925842285
INFO:root:current mean train loss 1231.2702276983055
INFO:root:current train perplexity2.639631509780884
INFO:root:current mean train loss 1231.4642264958752
INFO:root:current train perplexity2.6398470401763916
INFO:root:current mean train loss 1230.9910796783395
INFO:root:current train perplexity2.6398000717163086
INFO:root:current mean train loss 1231.1641475550125
INFO:root:current train perplexity2.6400294303894043
INFO:root:current mean train loss 1231.4511919561048
INFO:root:current train perplexity2.6396725177764893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.16s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.16s/it]
INFO:root:final mean train loss: 1230.8854650054025
INFO:root:final train perplexity: 2.6399247646331787
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.54s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.54s/it]
INFO:root:eval mean loss: 2356.9341097351507
INFO:root:eval perplexity: 6.727137565612793
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2953.1665437306074
INFO:root:eval perplexity: 11.191792488098145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [32:16:12<2:37:35, 630.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1222.8284246271307
INFO:root:current train perplexity2.6427788734436035
INFO:root:current mean train loss 1227.8197038438584
INFO:root:current train perplexity2.634739398956299
INFO:root:current mean train loss 1228.739097720287
INFO:root:current train perplexity2.6359007358551025
INFO:root:current mean train loss 1228.3398618476335
INFO:root:current train perplexity2.6343069076538086
INFO:root:current mean train loss 1228.7960108851528
INFO:root:current train perplexity2.635791778564453
INFO:root:current mean train loss 1228.4465388129738
INFO:root:current train perplexity2.6391940116882324
INFO:root:current mean train loss 1228.1316530215813
INFO:root:current train perplexity2.638296365737915
INFO:root:current mean train loss 1228.8279843894384
INFO:root:current train perplexity2.6378839015960693
INFO:root:current mean train loss 1230.9843700824756
INFO:root:current train perplexity2.6378543376922607
INFO:root:current mean train loss 1230.3744345196224
INFO:root:current train perplexity2.6383683681488037
INFO:root:current mean train loss 1230.562405757977
INFO:root:current train perplexity2.637375593185425
INFO:root:current mean train loss 1231.3124466475906
INFO:root:current train perplexity2.6369619369506836
INFO:root:current mean train loss 1231.3313694797528
INFO:root:current train perplexity2.6373965740203857
INFO:root:current mean train loss 1230.867385955084
INFO:root:current train perplexity2.636523723602295
INFO:root:current mean train loss 1231.2460374488726
INFO:root:current train perplexity2.6376616954803467
INFO:root:current mean train loss 1231.1372419762488
INFO:root:current train perplexity2.6381454467773438
INFO:root:current mean train loss 1230.6460421299992
INFO:root:current train perplexity2.6380765438079834
INFO:root:current mean train loss 1230.6763710231955
INFO:root:current train perplexity2.63871169090271
INFO:root:current mean train loss 1230.973481813377
INFO:root:current train perplexity2.6395061016082764
INFO:root:current mean train loss 1230.866683771581
INFO:root:current train perplexity2.6396679878234863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.50s/it]
INFO:root:final mean train loss: 1230.7960800963463
INFO:root:final train perplexity: 2.6397385597229004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.52s/it]
INFO:root:eval mean loss: 2356.6013845890125
INFO:root:eval perplexity: 6.725329399108887
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.18s/it]
INFO:root:eval mean loss: 2953.0946486452794
INFO:root:eval perplexity: 11.191136360168457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [32:26:36<2:26:40, 628.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1233.723096503586
INFO:root:current train perplexity2.6252634525299072
INFO:root:current mean train loss 1237.0318611097632
INFO:root:current train perplexity2.633962869644165
INFO:root:current mean train loss 1237.4485953027718
INFO:root:current train perplexity2.6392745971679688
INFO:root:current mean train loss 1235.034446821979
INFO:root:current train perplexity2.640556573867798
INFO:root:current mean train loss 1233.3004833560703
INFO:root:current train perplexity2.638895034790039
INFO:root:current mean train loss 1232.917937634038
INFO:root:current train perplexity2.6373047828674316
INFO:root:current mean train loss 1231.9309142974068
INFO:root:current train perplexity2.637944459915161
INFO:root:current mean train loss 1230.456971417902
INFO:root:current train perplexity2.6388933658599854
INFO:root:current mean train loss 1230.8165607873293
INFO:root:current train perplexity2.6393702030181885
INFO:root:current mean train loss 1229.8161290830678
INFO:root:current train perplexity2.638313055038452
INFO:root:current mean train loss 1229.8108792606106
INFO:root:current train perplexity2.6381170749664307
INFO:root:current mean train loss 1229.2301808701416
INFO:root:current train perplexity2.6374218463897705
INFO:root:current mean train loss 1229.1850233569587
INFO:root:current train perplexity2.6380221843719482
INFO:root:current mean train loss 1230.3076909140223
INFO:root:current train perplexity2.6384291648864746
INFO:root:current mean train loss 1230.9684794567613
INFO:root:current train perplexity2.637192487716675
INFO:root:current mean train loss 1230.9108883590748
INFO:root:current train perplexity2.637824296951294
INFO:root:current mean train loss 1230.8360150341239
INFO:root:current train perplexity2.6388821601867676
INFO:root:current mean train loss 1230.6724102100413
INFO:root:current train perplexity2.6387054920196533
INFO:root:current mean train loss 1230.628111513887
INFO:root:current train perplexity2.6384775638580322
INFO:root:current mean train loss 1230.3092436296852
INFO:root:current train perplexity2.638265371322632

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.24s/it]
INFO:root:final mean train loss: 1229.944838900429
INFO:root:final train perplexity: 2.637967109680176
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.45s/it]
INFO:root:eval mean loss: 2358.2516496807125
INFO:root:eval perplexity: 6.734310150146484
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.06s/it]
INFO:root:eval mean loss: 2955.0889477019614
INFO:root:eval perplexity: 11.209403991699219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [32:37:12<2:16:36, 630.53s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1219.0054665589944
INFO:root:current train perplexity2.6433417797088623
INFO:root:current mean train loss 1227.5887396308813
INFO:root:current train perplexity2.641665458679199
INFO:root:current mean train loss 1225.9717038381013
INFO:root:current train perplexity2.628326892852783
INFO:root:current mean train loss 1226.217869995763
INFO:root:current train perplexity2.627892017364502
INFO:root:current mean train loss 1225.3828048386833
INFO:root:current train perplexity2.628573417663574
INFO:root:current mean train loss 1225.6922413123107
INFO:root:current train perplexity2.631087303161621
INFO:root:current mean train loss 1225.6947604829231
INFO:root:current train perplexity2.63165283203125
INFO:root:current mean train loss 1226.1506416693444
INFO:root:current train perplexity2.6304852962493896
INFO:root:current mean train loss 1226.107501401445
INFO:root:current train perplexity2.631986379623413
INFO:root:current mean train loss 1226.9570850458126
INFO:root:current train perplexity2.6324856281280518
INFO:root:current mean train loss 1227.3328944614955
INFO:root:current train perplexity2.633816957473755
INFO:root:current mean train loss 1227.6311200956334
INFO:root:current train perplexity2.6343696117401123
INFO:root:current mean train loss 1228.4305175399184
INFO:root:current train perplexity2.6340842247009277
INFO:root:current mean train loss 1229.17782464325
INFO:root:current train perplexity2.6338143348693848
INFO:root:current mean train loss 1229.489939110205
INFO:root:current train perplexity2.634605646133423
INFO:root:current mean train loss 1229.5061499301926
INFO:root:current train perplexity2.633899450302124
INFO:root:current mean train loss 1229.2141904774098
INFO:root:current train perplexity2.6337103843688965
INFO:root:current mean train loss 1229.389369282331
INFO:root:current train perplexity2.6343624591827393
INFO:root:current mean train loss 1229.7945986291725
INFO:root:current train perplexity2.635859966278076
INFO:root:current mean train loss 1229.721602853557
INFO:root:current train perplexity2.636698007583618

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.83s/it]
INFO:root:final mean train loss: 1229.4185835029882
INFO:root:final train perplexity: 2.6368720531463623
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 2358.1752964317375
INFO:root:eval perplexity: 6.733894348144531
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.38s/it]
INFO:root:eval mean loss: 2955.059718788093
INFO:root:eval perplexity: 11.209134101867676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [32:47:46<2:06:22, 631.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1226.841654245477
INFO:root:current train perplexity2.6335177421569824
INFO:root:current mean train loss 1227.8957419370993
INFO:root:current train perplexity2.6252779960632324
INFO:root:current mean train loss 1228.977998377913
INFO:root:current train perplexity2.6235342025756836
INFO:root:current mean train loss 1228.1142738825158
INFO:root:current train perplexity2.6229941844940186
INFO:root:current mean train loss 1229.4822236032196
INFO:root:current train perplexity2.628143787384033
INFO:root:current mean train loss 1228.102511981355
INFO:root:current train perplexity2.6278464794158936
INFO:root:current mean train loss 1227.9828934703798
INFO:root:current train perplexity2.628657817840576
INFO:root:current mean train loss 1227.398377769998
INFO:root:current train perplexity2.628743886947632
INFO:root:current mean train loss 1227.2360805745898
INFO:root:current train perplexity2.6283621788024902
INFO:root:current mean train loss 1228.130405322511
INFO:root:current train perplexity2.6293997764587402
INFO:root:current mean train loss 1228.0508346487943
INFO:root:current train perplexity2.631152868270874
INFO:root:current mean train loss 1228.525211963095
INFO:root:current train perplexity2.632167339324951
INFO:root:current mean train loss 1228.4014212943412
INFO:root:current train perplexity2.633403778076172
INFO:root:current mean train loss 1228.2999607099855
INFO:root:current train perplexity2.634396553039551
INFO:root:current mean train loss 1228.7610145798494
INFO:root:current train perplexity2.6347644329071045
INFO:root:current mean train loss 1228.7946258449256
INFO:root:current train perplexity2.6357080936431885
INFO:root:current mean train loss 1229.390160772723
INFO:root:current train perplexity2.636021852493286
INFO:root:current mean train loss 1229.3997548392888
INFO:root:current train perplexity2.6366658210754395
INFO:root:current mean train loss 1229.2425271066952
INFO:root:current train perplexity2.6368885040283203

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.21s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:21<00:00, 561.21s/it]
INFO:root:final mean train loss: 1229.106445004708
INFO:root:final train perplexity: 2.636223077774048
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 2358.630194914256
INFO:root:eval perplexity: 6.736373424530029
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.67s/it]
INFO:root:eval mean loss: 2955.7230241924312
INFO:root:eval perplexity: 11.215215682983398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [32:58:22<1:56:04, 633.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1220.0032653808594
INFO:root:current train perplexity2.618305206298828
INFO:root:current mean train loss 1227.0641359601702
INFO:root:current train perplexity2.622793197631836
INFO:root:current mean train loss 1224.9701975696491
INFO:root:current train perplexity2.6306777000427246
INFO:root:current mean train loss 1225.539301163111
INFO:root:current train perplexity2.628988265991211
INFO:root:current mean train loss 1224.6008386704527
INFO:root:current train perplexity2.6274359226226807
INFO:root:current mean train loss 1225.036608695984
INFO:root:current train perplexity2.631319046020508
INFO:root:current mean train loss 1226.737985648361
INFO:root:current train perplexity2.629709243774414
INFO:root:current mean train loss 1226.869009982334
INFO:root:current train perplexity2.6296963691711426
INFO:root:current mean train loss 1227.4407216339862
INFO:root:current train perplexity2.632448673248291
INFO:root:current mean train loss 1226.7467346191406
INFO:root:current train perplexity2.633984327316284
INFO:root:current mean train loss 1226.9101321254323
INFO:root:current train perplexity2.632904052734375
INFO:root:current mean train loss 1227.337584571015
INFO:root:current train perplexity2.6325178146362305
INFO:root:current mean train loss 1228.1176881695737
INFO:root:current train perplexity2.632986545562744
INFO:root:current mean train loss 1227.7556973899284
INFO:root:current train perplexity2.633262872695923
INFO:root:current mean train loss 1228.174598931591
INFO:root:current train perplexity2.6341443061828613
INFO:root:current mean train loss 1228.5610367709367
INFO:root:current train perplexity2.6341135501861572
INFO:root:current mean train loss 1228.3323274901131
INFO:root:current train perplexity2.634678840637207
INFO:root:current mean train loss 1228.4007363720475
INFO:root:current train perplexity2.6345510482788086
INFO:root:current mean train loss 1228.6802995357555
INFO:root:current train perplexity2.6348633766174316
INFO:root:current mean train loss 1228.7292099956687
INFO:root:current train perplexity2.635122060775757

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.69s/it]
INFO:root:final mean train loss: 1228.351535752874
INFO:root:final train perplexity: 2.6346540451049805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.64s/it]
INFO:root:eval mean loss: 2358.5349856978614
INFO:root:eval perplexity: 6.73585319519043
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.85s/it]
INFO:root:eval mean loss: 2955.1872398430573
INFO:root:eval perplexity: 11.210304260253906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [33:08:48<1:45:07, 630.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1231.7405879579742
INFO:root:current train perplexity2.623491048812866
INFO:root:current mean train loss 1221.023810334908
INFO:root:current train perplexity2.626324415206909
INFO:root:current mean train loss 1221.94454436323
INFO:root:current train perplexity2.623647451400757
INFO:root:current mean train loss 1224.1090043366498
INFO:root:current train perplexity2.627631187438965
INFO:root:current mean train loss 1226.9539473384689
INFO:root:current train perplexity2.6298508644104004
INFO:root:current mean train loss 1226.5506695637405
INFO:root:current train perplexity2.6295571327209473
INFO:root:current mean train loss 1225.241464199436
INFO:root:current train perplexity2.6292617321014404
INFO:root:current mean train loss 1226.688218021262
INFO:root:current train perplexity2.629441022872925
INFO:root:current mean train loss 1226.2242044372927
INFO:root:current train perplexity2.6293277740478516
INFO:root:current mean train loss 1226.4411598755803
INFO:root:current train perplexity2.6304285526275635
INFO:root:current mean train loss 1225.6916374599505
INFO:root:current train perplexity2.6301729679107666
INFO:root:current mean train loss 1224.8190965542653
INFO:root:current train perplexity2.6297049522399902
INFO:root:current mean train loss 1225.511193519884
INFO:root:current train perplexity2.6290953159332275
INFO:root:current mean train loss 1226.2585376656261
INFO:root:current train perplexity2.630009412765503
INFO:root:current mean train loss 1226.515320379472
INFO:root:current train perplexity2.6309773921966553
INFO:root:current mean train loss 1226.9322326939584
INFO:root:current train perplexity2.6309192180633545
INFO:root:current mean train loss 1227.1837732210856
INFO:root:current train perplexity2.632413387298584
INFO:root:current mean train loss 1227.0171741421614
INFO:root:current train perplexity2.6326403617858887
INFO:root:current mean train loss 1227.3302024645254
INFO:root:current train perplexity2.6329550743103027
INFO:root:current mean train loss 1227.3645779543926
INFO:root:current train perplexity2.6324682235717773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:13<00:00, 553.85s/it]
INFO:root:final mean train loss: 1227.4188472499645
INFO:root:final train perplexity: 2.6327168941497803
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.82s/it]
INFO:root:eval mean loss: 2359.1690816676364
INFO:root:eval perplexity: 6.7393107414245605
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.59s/it]
INFO:root:eval mean loss: 2955.8255732110206
INFO:root:eval perplexity: 11.21615982055664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [33:19:17<1:34:33, 630.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1222.6182542883832
INFO:root:current train perplexity2.6126723289489746
INFO:root:current mean train loss 1220.7884580011237
INFO:root:current train perplexity2.6206095218658447
INFO:root:current mean train loss 1224.273987312627
INFO:root:current train perplexity2.6300525665283203
INFO:root:current mean train loss 1224.650207166727
INFO:root:current train perplexity2.6317052841186523
INFO:root:current mean train loss 1224.1224816839792
INFO:root:current train perplexity2.6287107467651367
INFO:root:current mean train loss 1224.260967324505
INFO:root:current train perplexity2.6286566257476807
INFO:root:current mean train loss 1224.6730732164885
INFO:root:current train perplexity2.6297359466552734
INFO:root:current mean train loss 1225.0730312643998
INFO:root:current train perplexity2.6293928623199463
INFO:root:current mean train loss 1225.6149326622062
INFO:root:current train perplexity2.6276350021362305
INFO:root:current mean train loss 1225.72775404045
INFO:root:current train perplexity2.6277213096618652
INFO:root:current mean train loss 1226.4742505162897
INFO:root:current train perplexity2.6280746459960938
INFO:root:current mean train loss 1227.2121347690336
INFO:root:current train perplexity2.628633499145508
INFO:root:current mean train loss 1227.381008289025
INFO:root:current train perplexity2.6306235790252686
INFO:root:current mean train loss 1227.2893900764998
INFO:root:current train perplexity2.6307613849639893
INFO:root:current mean train loss 1227.2816027882682
INFO:root:current train perplexity2.631558656692505
INFO:root:current mean train loss 1227.1634977866267
INFO:root:current train perplexity2.632197856903076
INFO:root:current mean train loss 1227.2965007795888
INFO:root:current train perplexity2.632768154144287
INFO:root:current mean train loss 1227.761025200458
INFO:root:current train perplexity2.632908582687378
INFO:root:current mean train loss 1228.1368461765937
INFO:root:current train perplexity2.633240222930908
INFO:root:current mean train loss 1228.0053956834533
INFO:root:current train perplexity2.633068084716797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.64s/it]
INFO:root:final mean train loss: 1227.7811183265767
INFO:root:final train perplexity: 2.633469581604004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.66s/it]
INFO:root:eval mean loss: 2359.3130021332004
INFO:root:eval perplexity: 6.740093231201172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.08s/it]
INFO:root:eval mean loss: 2955.941152586159
INFO:root:eval perplexity: 11.217215538024902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [33:29:52<1:24:13, 631.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1243.3842250279017
INFO:root:current train perplexity2.6140992641448975
INFO:root:current mean train loss 1227.019721469996
INFO:root:current train perplexity2.631304979324341
INFO:root:current mean train loss 1224.8260052467026
INFO:root:current train perplexity2.629769802093506
INFO:root:current mean train loss 1226.3368716909865
INFO:root:current train perplexity2.625248908996582
INFO:root:current mean train loss 1227.3472945211258
INFO:root:current train perplexity2.6251416206359863
INFO:root:current mean train loss 1227.1631058850467
INFO:root:current train perplexity2.6279568672180176
INFO:root:current mean train loss 1228.1495458910729
INFO:root:current train perplexity2.6306703090667725
INFO:root:current mean train loss 1228.802364804329
INFO:root:current train perplexity2.6308298110961914
INFO:root:current mean train loss 1227.775141533667
INFO:root:current train perplexity2.630495309829712
INFO:root:current mean train loss 1227.4040329597449
INFO:root:current train perplexity2.6302857398986816
INFO:root:current mean train loss 1227.602402408058
INFO:root:current train perplexity2.6319689750671387
INFO:root:current mean train loss 1227.2491007941812
INFO:root:current train perplexity2.6313889026641846
INFO:root:current mean train loss 1227.4538926995187
INFO:root:current train perplexity2.631544828414917
INFO:root:current mean train loss 1227.0083484271827
INFO:root:current train perplexity2.6310231685638428
INFO:root:current mean train loss 1226.9202953450742
INFO:root:current train perplexity2.6322505474090576
INFO:root:current mean train loss 1227.031925018369
INFO:root:current train perplexity2.631960153579712
INFO:root:current mean train loss 1227.0943932364091
INFO:root:current train perplexity2.631669521331787
INFO:root:current mean train loss 1227.3601963261751
INFO:root:current train perplexity2.6305341720581055
INFO:root:current mean train loss 1227.3235508687894
INFO:root:current train perplexity2.6312386989593506
INFO:root:current mean train loss 1226.732590397948
INFO:root:current train perplexity2.631110668182373

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.45s/it]
INFO:root:final mean train loss: 1226.6516019712478
INFO:root:final train perplexity: 2.631124258041382
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.25s/it]
INFO:root:eval mean loss: 2359.7146385679853
INFO:root:eval perplexity: 6.742283344268799
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.89s/it]
INFO:root:eval mean loss: 2956.1601662060893
INFO:root:eval perplexity: 11.219226837158203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [33:40:17<1:13:27, 629.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1221.5119720458983
INFO:root:current train perplexity2.620452404022217
INFO:root:current mean train loss 1226.7724663628471
INFO:root:current train perplexity2.6345064640045166
INFO:root:current mean train loss 1225.7174573625837
INFO:root:current train perplexity2.6354482173919678
INFO:root:current mean train loss 1225.229791902241
INFO:root:current train perplexity2.6336209774017334
INFO:root:current mean train loss 1226.393561299642
INFO:root:current train perplexity2.6310620307922363
INFO:root:current mean train loss 1225.8499574858567
INFO:root:current train perplexity2.629425048828125
INFO:root:current mean train loss 1225.2863536161535
INFO:root:current train perplexity2.627617835998535
INFO:root:current mean train loss 1225.313662484976
INFO:root:current train perplexity2.6279056072235107
INFO:root:current mean train loss 1225.2629022771662
INFO:root:current train perplexity2.6269760131835938
INFO:root:current mean train loss 1225.3891590351961
INFO:root:current train perplexity2.6268250942230225
INFO:root:current mean train loss 1224.9453216552733
INFO:root:current train perplexity2.626610040664673
INFO:root:current mean train loss 1225.4968022750595
INFO:root:current train perplexity2.62717604637146
INFO:root:current mean train loss 1225.3149166107178
INFO:root:current train perplexity2.626678705215454
INFO:root:current mean train loss 1225.6642141148664
INFO:root:current train perplexity2.6268270015716553
INFO:root:current mean train loss 1225.3634691393054
INFO:root:current train perplexity2.6273481845855713
INFO:root:current mean train loss 1225.1191797956635
INFO:root:current train perplexity2.6282074451446533
INFO:root:current mean train loss 1225.3107235136486
INFO:root:current train perplexity2.628812313079834
INFO:root:current mean train loss 1225.7710602149534
INFO:root:current train perplexity2.6290087699890137
INFO:root:current mean train loss 1225.9987900754238
INFO:root:current train perplexity2.6297762393951416
INFO:root:current mean train loss 1226.0895302512429
INFO:root:current train perplexity2.629335880279541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:19<00:00, 559.87s/it]
INFO:root:final mean train loss: 1225.6950840259885
INFO:root:final train perplexity: 2.6291403770446777
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.18s/it]
INFO:root:eval mean loss: 2360.4478201358875
INFO:root:eval perplexity: 6.746283054351807
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2957.867171050809
INFO:root:eval perplexity: 11.234902381896973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [33:50:52<1:03:07, 631.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1215.8975439956507
INFO:root:current train perplexity2.6275017261505127
INFO:root:current mean train loss 1217.1270578452172
INFO:root:current train perplexity2.620481491088867
INFO:root:current mean train loss 1219.7346585976957
INFO:root:current train perplexity2.624951124191284
INFO:root:current mean train loss 1221.8339828375906
INFO:root:current train perplexity2.6207873821258545
INFO:root:current mean train loss 1222.6470630423164
INFO:root:current train perplexity2.6201581954956055
INFO:root:current mean train loss 1221.9728036749502
INFO:root:current train perplexity2.62192440032959
INFO:root:current mean train loss 1221.762008601316
INFO:root:current train perplexity2.624690055847168
INFO:root:current mean train loss 1222.6186713358688
INFO:root:current train perplexity2.624985933303833
INFO:root:current mean train loss 1223.821082345884
INFO:root:current train perplexity2.6250436305999756
INFO:root:current mean train loss 1223.9373195269402
INFO:root:current train perplexity2.627027988433838
INFO:root:current mean train loss 1224.4074347608178
INFO:root:current train perplexity2.627978563308716
INFO:root:current mean train loss 1224.7611742776537
INFO:root:current train perplexity2.628615379333496
INFO:root:current mean train loss 1224.9120482927851
INFO:root:current train perplexity2.627176523208618
INFO:root:current mean train loss 1224.9052165529092
INFO:root:current train perplexity2.627013921737671
INFO:root:current mean train loss 1224.6040615573595
INFO:root:current train perplexity2.627305507659912
INFO:root:current mean train loss 1225.2029341361488
INFO:root:current train perplexity2.6269984245300293
INFO:root:current mean train loss 1225.4188021658165
INFO:root:current train perplexity2.6277410984039307
INFO:root:current mean train loss 1225.6237581706273
INFO:root:current train perplexity2.627835512161255
INFO:root:current mean train loss 1225.7472438621219
INFO:root:current train perplexity2.628000497817993

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.87s/it]
INFO:root:final mean train loss: 1225.2201145958913
INFO:root:final train perplexity: 2.6281557083129883
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.36s/it]
INFO:root:eval mean loss: 2361.3081154248393
INFO:root:eval perplexity: 6.750977516174316
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.85s/it]
INFO:root:eval mean loss: 2958.5853665399213
INFO:root:eval perplexity: 11.241503715515137
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [34:01:15<52:24, 628.90s/it]  
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1202.558314732143
INFO:root:current train perplexity2.63419508934021
INFO:root:current mean train loss 1223.4820042660363
INFO:root:current train perplexity2.6284351348876953
INFO:root:current mean train loss 1225.504748763325
INFO:root:current train perplexity2.627372980117798
INFO:root:current mean train loss 1225.217231118755
INFO:root:current train perplexity2.6255617141723633
INFO:root:current mean train loss 1225.730346089976
INFO:root:current train perplexity2.627394437789917
INFO:root:current mean train loss 1225.6649393163302
INFO:root:current train perplexity2.6292970180511475
INFO:root:current mean train loss 1224.7189623307715
INFO:root:current train perplexity2.628730297088623
INFO:root:current mean train loss 1224.8521126712403
INFO:root:current train perplexity2.627920150756836
INFO:root:current mean train loss 1225.852151106851
INFO:root:current train perplexity2.6279802322387695
INFO:root:current mean train loss 1225.9499573154574
INFO:root:current train perplexity2.6284444332122803
INFO:root:current mean train loss 1225.3959971772142
INFO:root:current train perplexity2.628612756729126
INFO:root:current mean train loss 1224.9472398740813
INFO:root:current train perplexity2.6282501220703125
INFO:root:current mean train loss 1225.024987712527
INFO:root:current train perplexity2.627753496170044
INFO:root:current mean train loss 1224.8457059119935
INFO:root:current train perplexity2.627382278442383
INFO:root:current mean train loss 1224.7726487911068
INFO:root:current train perplexity2.627054452896118
INFO:root:current mean train loss 1225.8330804580426
INFO:root:current train perplexity2.627136707305908
INFO:root:current mean train loss 1225.895002967806
INFO:root:current train perplexity2.6271331310272217
INFO:root:current mean train loss 1225.9014206733837
INFO:root:current train perplexity2.627753496170044
INFO:root:current mean train loss 1225.6164973384095
INFO:root:current train perplexity2.627387523651123
INFO:root:current mean train loss 1225.394526530458
INFO:root:current train perplexity2.6270341873168945

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:09<00:00, 549.96s/it]
INFO:root:final mean train loss: 1225.1117281315005
INFO:root:final train perplexity: 2.6279308795928955
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.07s/it]
INFO:root:eval mean loss: 2361.6042411209
INFO:root:eval perplexity: 6.752595901489258
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.29s/it]
INFO:root:eval mean loss: 2958.620347043301
INFO:root:eval perplexity: 11.241823196411133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [34:11:40<41:50, 627.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1218.0984674269152
INFO:root:current train perplexity2.6477222442626953
INFO:root:current mean train loss 1225.7666863594347
INFO:root:current train perplexity2.6308586597442627
INFO:root:current mean train loss 1227.042427624459
INFO:root:current train perplexity2.6239538192749023
INFO:root:current mean train loss 1225.761975060777
INFO:root:current train perplexity2.623924970626831
INFO:root:current mean train loss 1226.8935184345999
INFO:root:current train perplexity2.628141164779663
INFO:root:current mean train loss 1227.639850738583
INFO:root:current train perplexity2.6284849643707275
INFO:root:current mean train loss 1227.8604442276026
INFO:root:current train perplexity2.62772274017334
INFO:root:current mean train loss 1227.3380868392505
INFO:root:current train perplexity2.628034830093384
INFO:root:current mean train loss 1227.171697109328
INFO:root:current train perplexity2.628014087677002
INFO:root:current mean train loss 1226.561691005555
INFO:root:current train perplexity2.6279208660125732
INFO:root:current mean train loss 1225.9460056131031
INFO:root:current train perplexity2.627912759780884
INFO:root:current mean train loss 1224.9144779276153
INFO:root:current train perplexity2.627645492553711
INFO:root:current mean train loss 1225.2976418316218
INFO:root:current train perplexity2.6271181106567383
INFO:root:current mean train loss 1225.431806259098
INFO:root:current train perplexity2.6276729106903076
INFO:root:current mean train loss 1225.2278630478577
INFO:root:current train perplexity2.6280858516693115
INFO:root:current mean train loss 1224.771629966372
INFO:root:current train perplexity2.6275243759155273
INFO:root:current mean train loss 1224.3507463278615
INFO:root:current train perplexity2.6267471313476562
INFO:root:current mean train loss 1224.3478827605973
INFO:root:current train perplexity2.6263086795806885
INFO:root:current mean train loss 1224.4970754459866
INFO:root:current train perplexity2.6269118785858154
INFO:root:current mean train loss 1224.6730434234003
INFO:root:current train perplexity2.6266162395477295

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:10<00:00, 550.50s/it]
INFO:root:final mean train loss: 1224.258087081255
INFO:root:final train perplexity: 2.626162528991699
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2361.8541095273713
INFO:root:eval perplexity: 6.753959655761719
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.95s/it]
INFO:root:eval mean loss: 2958.9673310408357
INFO:root:eval perplexity: 11.245013236999512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [34:22:04<31:19, 626.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1226.829668680827
INFO:root:current train perplexity2.6416232585906982
INFO:root:current mean train loss 1222.5046708390519
INFO:root:current train perplexity2.6301534175872803
INFO:root:current mean train loss 1220.1134220246345
INFO:root:current train perplexity2.62152099609375
INFO:root:current mean train loss 1221.703365983634
INFO:root:current train perplexity2.6206917762756348
INFO:root:current mean train loss 1220.7301916394915
INFO:root:current train perplexity2.620647430419922
INFO:root:current mean train loss 1222.4638689695485
INFO:root:current train perplexity2.618544101715088
INFO:root:current mean train loss 1221.9738910816334
INFO:root:current train perplexity2.619361639022827
INFO:root:current mean train loss 1221.6724093024106
INFO:root:current train perplexity2.620030164718628
INFO:root:current mean train loss 1222.4192991436653
INFO:root:current train perplexity2.620173692703247
INFO:root:current mean train loss 1223.1880435219293
INFO:root:current train perplexity2.624382495880127
INFO:root:current mean train loss 1223.9031517669446
INFO:root:current train perplexity2.6254348754882812
INFO:root:current mean train loss 1223.9051177659517
INFO:root:current train perplexity2.62630295753479
INFO:root:current mean train loss 1224.3658674191206
INFO:root:current train perplexity2.6270711421966553
INFO:root:current mean train loss 1224.773502066864
INFO:root:current train perplexity2.6269657611846924
INFO:root:current mean train loss 1224.8895622801385
INFO:root:current train perplexity2.625748872756958
INFO:root:current mean train loss 1224.9055823195504
INFO:root:current train perplexity2.625833034515381
INFO:root:current mean train loss 1224.751584247478
INFO:root:current train perplexity2.6258060932159424
INFO:root:current mean train loss 1224.5990491644468
INFO:root:current train perplexity2.625967264175415
INFO:root:current mean train loss 1224.7725831399232
INFO:root:current train perplexity2.6259496212005615
INFO:root:current mean train loss 1224.5567259112913
INFO:root:current train perplexity2.6259164810180664

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.00s/it]
INFO:root:final mean train loss: 1224.1346955715378
INFO:root:final train perplexity: 2.6259069442749023
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.15s/it]
INFO:root:eval mean loss: 2361.6838227400544
INFO:root:eval perplexity: 6.753028869628906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.93s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:35<00:00, 35.93s/it]
INFO:root:eval mean loss: 2959.13768275917
INFO:root:eval perplexity: 11.24657917022705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [34:32:39<20:57, 628.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1209.6349740835337
INFO:root:current train perplexity2.631309986114502
INFO:root:current mean train loss 1218.120137162642
INFO:root:current train perplexity2.6238467693328857
INFO:root:current mean train loss 1221.0867708026237
INFO:root:current train perplexity2.6264309883117676
INFO:root:current mean train loss 1221.4032055998503
INFO:root:current train perplexity2.631483793258667
INFO:root:current mean train loss 1222.4436145413306
INFO:root:current train perplexity2.629648208618164
INFO:root:current mean train loss 1222.2630392699116
INFO:root:current train perplexity2.6290228366851807
INFO:root:current mean train loss 1222.78078393004
INFO:root:current train perplexity2.630462408065796
INFO:root:current mean train loss 1223.4028661790237
INFO:root:current train perplexity2.6316282749176025
INFO:root:current mean train loss 1223.1787127720827
INFO:root:current train perplexity2.6303699016571045
INFO:root:current mean train loss 1222.8143385180538
INFO:root:current train perplexity2.6305370330810547
INFO:root:current mean train loss 1223.6360114299075
INFO:root:current train perplexity2.6298153400421143
INFO:root:current mean train loss 1223.6664327596902
INFO:root:current train perplexity2.6284027099609375
INFO:root:current mean train loss 1223.810775768898
INFO:root:current train perplexity2.6286888122558594
INFO:root:current mean train loss 1224.1039658990098
INFO:root:current train perplexity2.6278984546661377
INFO:root:current mean train loss 1223.86083609415
INFO:root:current train perplexity2.628152847290039
INFO:root:current mean train loss 1224.2609683100789
INFO:root:current train perplexity2.627314329147339
INFO:root:current mean train loss 1224.02481561151
INFO:root:current train perplexity2.6270196437835693
INFO:root:current mean train loss 1224.2732911539483
INFO:root:current train perplexity2.626166582107544
INFO:root:current mean train loss 1224.348654477526
INFO:root:current train perplexity2.6261003017425537
INFO:root:current mean train loss 1224.7138853272106
INFO:root:current train perplexity2.626450538635254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.17s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:11<00:00, 551.17s/it]
INFO:root:final mean train loss: 1224.3433324801338
INFO:root:final train perplexity: 2.6263391971588135
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.35s/it]
INFO:root:eval mean loss: 2361.852083679632
INFO:root:eval perplexity: 6.753947734832764
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.07s/it]
INFO:root:eval mean loss: 2959.26210660461
INFO:root:eval perplexity: 11.247726440429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [34:43:03<10:27, 627.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1207.847724728468
INFO:root:current train perplexity2.630899429321289
INFO:root:current mean train loss 1215.7924905294901
INFO:root:current train perplexity2.620410442352295
INFO:root:current mean train loss 1218.4956028715094
INFO:root:current train perplexity2.621256113052368
INFO:root:current mean train loss 1218.0537281935128
INFO:root:current train perplexity2.6201488971710205
INFO:root:current mean train loss 1221.449493534832
INFO:root:current train perplexity2.622805118560791
INFO:root:current mean train loss 1222.306114799788
INFO:root:current train perplexity2.6231935024261475
INFO:root:current mean train loss 1222.8763911003941
INFO:root:current train perplexity2.623805046081543
INFO:root:current mean train loss 1223.8214308014306
INFO:root:current train perplexity2.6232266426086426
INFO:root:current mean train loss 1222.9335674536742
INFO:root:current train perplexity2.6232516765594482
INFO:root:current mean train loss 1222.8486666242363
INFO:root:current train perplexity2.6250829696655273
INFO:root:current mean train loss 1223.7572390402972
INFO:root:current train perplexity2.6252214908599854
INFO:root:current mean train loss 1223.600010079579
INFO:root:current train perplexity2.6252176761627197
INFO:root:current mean train loss 1224.1214969057746
INFO:root:current train perplexity2.6258883476257324
INFO:root:current mean train loss 1224.356446372445
INFO:root:current train perplexity2.62630558013916
INFO:root:current mean train loss 1224.0211610781198
INFO:root:current train perplexity2.6258811950683594
INFO:root:current mean train loss 1224.0063865459072
INFO:root:current train perplexity2.6258678436279297
INFO:root:current mean train loss 1224.4584333168057
INFO:root:current train perplexity2.625652313232422
INFO:root:current mean train loss 1224.592243849629
INFO:root:current train perplexity2.626100540161133
INFO:root:current mean train loss 1224.4917561675993
INFO:root:current train perplexity2.6263792514801025
INFO:root:current mean train loss 1224.5215992393455
INFO:root:current train perplexity2.6260533332824707

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [09:20<00:00, 560.97s/it]
INFO:root:final mean train loss: 1224.208180545378
INFO:root:final train perplexity: 2.62605881690979
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.37s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:37<00:00, 37.37s/it]
INFO:root:eval mean loss: 2361.7929328215037
INFO:root:eval perplexity: 6.753625392913818
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:36<00:00, 36.80s/it]
INFO:root:eval mean loss: 2959.162736608627
INFO:root:eval perplexity: 11.246809005737305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: multiqal6_allminilml12/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [34:53:40<00:00, 630.33s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [34:53:40<00:00, 628.10s/it]
INFO:root:evaluating final model
INFO:root:start evaluating on validation
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:50<00:00, 50.81s/it]
INFO:root:eval mean loss: 2361.7929328215037
INFO:root:eval perplexity: 6.753625392913818
INFO:root:evalaution complete
INFO:root:start evaluating on test
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.57s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:43<00:00, 43.57s/it]
INFO:root:eval mean loss: 2959.162736608627
INFO:root:eval perplexity: 11.246809005737305
INFO:root:evalaution complete
INFO:root:save model final: multiqal6_allminilml12/final
