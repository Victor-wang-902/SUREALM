INFO:root:Output: small_val_134
INFO:root:Steps per epochs:992
INFO:root:Total steps:198400
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.3.crossattention.self.key.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'cls.predictions.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24458.517183554293
INFO:root:current train perplexity15522.7294921875
INFO:root:current mean train loss 20545.55944272142
INFO:root:current train perplexity3285.405029296875
INFO:root:current mean train loss 17750.34676460598
INFO:root:current train perplexity1094.645263671875
INFO:root:current mean train loss 15856.34895588581
INFO:root:current train perplexity514.4224853515625
INFO:root:current mean train loss 14483.150621555611
INFO:root:current train perplexity299.39068603515625
INFO:root:current mean train loss 13439.507763590358
INFO:root:current train perplexity199.0478515625
INFO:root:current mean train loss 12629.41237412263
INFO:root:current train perplexity144.619873046875
INFO:root:current mean train loss 11978.35749581997
INFO:root:current train perplexity112.15164947509766
INFO:root:current mean train loss 11445.485691023881
INFO:root:current train perplexity90.9472885131836


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 263.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 263.00s/it]
INFO:root:final mean train loss: 11015.58750644807
INFO:root:final train perplexity: 77.16671752929688
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it]
INFO:root:eval mean loss: 6411.489434424867
INFO:root:eval perplexity: 13.364714622497559
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/1

  0%|          | 1/200 [04:42<15:37:56, 282.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6832.314801897322
INFO:root:current train perplexity14.560154914855957
INFO:root:current mean train loss 6731.6765113901865
INFO:root:current train perplexity14.401796340942383
INFO:root:current mean train loss 6702.994256208484
INFO:root:current train perplexity14.163097381591797
INFO:root:current mean train loss 6631.374697806393
INFO:root:current train perplexity13.743603706359863
INFO:root:current mean train loss 6577.927174111256
INFO:root:current train perplexity13.444099426269531
INFO:root:current mean train loss 6527.4802713187255
INFO:root:current train perplexity13.171587944030762
INFO:root:current mean train loss 6484.859466703562
INFO:root:current train perplexity12.901396751403809
INFO:root:current mean train loss 6437.137644895907
INFO:root:current train perplexity12.659504890441895
INFO:root:current mean train loss 6392.923984229787
INFO:root:current train perplexity12.43344783782959
INFO:root:current mean train loss 6348.796798554645
INFO:root:current train perplexity12.231148719787598


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.19s/it]
INFO:root:final mean train loss: 6311.676459281675
INFO:root:final train perplexity: 12.062952995300293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.67s/it]
INFO:root:eval mean loss: 5541.929015680408
INFO:root:eval perplexity: 9.402667999267578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/2

  1%|          | 2/200 [09:28<15:39:21, 284.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5997.637272135416
INFO:root:current train perplexity10.681870460510254
INFO:root:current mean train loss 5885.592777683424
INFO:root:current train perplexity10.25619125366211
INFO:root:current mean train loss 5862.4988349382265
INFO:root:current train perplexity10.111995697021484
INFO:root:current mean train loss 5842.158252728174
INFO:root:current train perplexity9.998424530029297
INFO:root:current mean train loss 5817.679116858058
INFO:root:current train perplexity9.90808391571045
INFO:root:current mean train loss 5791.838344773969
INFO:root:current train perplexity9.819902420043945
INFO:root:current mean train loss 5767.546763052592
INFO:root:current train perplexity9.741592407226562
INFO:root:current mean train loss 5749.236949573864
INFO:root:current train perplexity9.66556167602539
INFO:root:current mean train loss 5736.237928968559
INFO:root:current train perplexity9.595240592956543
INFO:root:current mean train loss 5717.41656367401
INFO:root:current train perplexity9.511542320251465


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.81s/it]
INFO:root:final mean train loss: 5694.102915240872
INFO:root:final train perplexity: 9.454479217529297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it]
INFO:root:eval mean loss: 5181.369473071809
INFO:root:eval perplexity: 8.12701416015625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/3

  2%|â–         | 3/200 [14:08<15:27:49, 282.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5556.154190726902
INFO:root:current train perplexity8.79135799407959
INFO:root:current mean train loss 5485.37366218877
INFO:root:current train perplexity8.717336654663086
INFO:root:current mean train loss 5485.408012629625
INFO:root:current train perplexity8.660653114318848
INFO:root:current mean train loss 5459.638821533959
INFO:root:current train perplexity8.583788871765137
INFO:root:current mean train loss 5446.652025155142
INFO:root:current train perplexity8.55742073059082
INFO:root:current mean train loss 5431.052627009142
INFO:root:current train perplexity8.506153106689453
INFO:root:current mean train loss 5421.189467232644
INFO:root:current train perplexity8.473349571228027
INFO:root:current mean train loss 5410.871580005187
INFO:root:current train perplexity8.436800956726074
INFO:root:current mean train loss 5400.107265245291
INFO:root:current train perplexity8.400633811950684
INFO:root:current mean train loss 5389.450209066901
INFO:root:current train perplexity8.365748405456543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.25s/it]
INFO:root:final mean train loss: 5378.565294573384
INFO:root:final train perplexity: 8.347814559936523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it]
INFO:root:eval mean loss: 4960.6020646332
INFO:root:eval perplexity: 7.43294095993042
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/4

  2%|â–         | 4/200 [19:18<15:57:32, 293.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5316.184349798387
INFO:root:current train perplexity7.955507755279541
INFO:root:current mean train loss 5242.955025942271
INFO:root:current train perplexity7.892007350921631
INFO:root:current mean train loss 5245.302911931818
INFO:root:current train perplexity7.913947105407715
INFO:root:current mean train loss 5233.76343289747
INFO:root:current train perplexity7.864986896514893
INFO:root:current mean train loss 5226.8903157174445
INFO:root:current train perplexity7.831855297088623
INFO:root:current mean train loss 5216.15790978843
INFO:root:current train perplexity7.801182270050049
INFO:root:current mean train loss 5208.2887846981475
INFO:root:current train perplexity7.7760114669799805
INFO:root:current mean train loss 5200.65149076073
INFO:root:current train perplexity7.760884761810303
INFO:root:current mean train loss 5187.360493757521
INFO:root:current train perplexity7.7361907958984375
INFO:root:current mean train loss 5179.436827629901
INFO:root:current train perplexity7.70700740814209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.61s/it]
INFO:root:final mean train loss: 5169.966733378748
INFO:root:final train perplexity: 7.6883134841918945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it]
INFO:root:eval mean loss: 4819.5713444703015
INFO:root:eval perplexity: 7.0209126472473145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/5

  2%|â–Ž         | 5/200 [24:06<15:46:28, 291.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5076.447791466346
INFO:root:current train perplexity7.327725410461426
INFO:root:current mean train loss 5069.538110527204
INFO:root:current train perplexity7.391945838928223
INFO:root:current mean train loss 5076.994737186193
INFO:root:current train perplexity7.401645183563232
INFO:root:current mean train loss 5060.739607819413
INFO:root:current train perplexity7.3419880867004395
INFO:root:current mean train loss 5059.00878350121
INFO:root:current train perplexity7.333547115325928
INFO:root:current mean train loss 5045.90359661265
INFO:root:current train perplexity7.307410717010498
INFO:root:current mean train loss 5037.7972364351035
INFO:root:current train perplexity7.286426067352295
INFO:root:current mean train loss 5037.583644097809
INFO:root:current train perplexity7.281081676483154
INFO:root:current mean train loss 5033.755129572035
INFO:root:current train perplexity7.2648844718933105
INFO:root:current mean train loss 5025.905508998103
INFO:root:current train perplexity7.249846458435059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.25s/it]
INFO:root:final mean train loss: 5017.224813153667
INFO:root:final train perplexity: 7.238690376281738
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.50s/it]
INFO:root:eval mean loss: 4703.864226645612
INFO:root:eval perplexity: 6.699982166290283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/6

  3%|â–Ž         | 6/200 [28:49<15:33:35, 288.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4888.001225897607
INFO:root:current train perplexity6.922650337219238
INFO:root:current mean train loss 4943.779901413691
INFO:root:current train perplexity7.004716873168945
INFO:root:current mean train loss 4925.503141210147
INFO:root:current train perplexity6.987415313720703
INFO:root:current mean train loss 4924.342635536744
INFO:root:current train perplexity6.975874423980713
INFO:root:current mean train loss 4921.823566615982
INFO:root:current train perplexity6.966258525848389
INFO:root:current mean train loss 4912.6183345592435
INFO:root:current train perplexity6.9436469078063965
INFO:root:current mean train loss 4912.316117960298
INFO:root:current train perplexity6.94079065322876
INFO:root:current mean train loss 4909.026346924155
INFO:root:current train perplexity6.928163528442383
INFO:root:current mean train loss 4905.401144664994
INFO:root:current train perplexity6.919374465942383
INFO:root:current mean train loss 4903.335035185124
INFO:root:current train perplexity6.9089202880859375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.67s/it]
INFO:root:final mean train loss: 4898.1999978096255
INFO:root:final train perplexity: 6.906628608703613
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.29s/it]
INFO:root:eval mean loss: 4615.899531804078
INFO:root:eval perplexity: 6.465847015380859
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/7

  4%|â–Ž         | 7/200 [34:12<16:04:22, 299.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4793.689146839489
INFO:root:current train perplexity6.715846061706543
INFO:root:current mean train loss 4843.124510143649
INFO:root:current train perplexity6.77014684677124
INFO:root:current mean train loss 4835.5370126761645
INFO:root:current train perplexity6.724762916564941
INFO:root:current mean train loss 4834.517695725132
INFO:root:current train perplexity6.7167887687683105
INFO:root:current mean train loss 4828.001574841174
INFO:root:current train perplexity6.69796085357666
INFO:root:current mean train loss 4822.38175279772
INFO:root:current train perplexity6.681208610534668
INFO:root:current mean train loss 4821.294275927362
INFO:root:current train perplexity6.680020332336426
INFO:root:current mean train loss 4821.898628608754
INFO:root:current train perplexity6.672234535217285
INFO:root:current mean train loss 4812.849692468476
INFO:root:current train perplexity6.655544281005859
INFO:root:current mean train loss 4805.271938144225
INFO:root:current train perplexity6.646611213684082


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.68s/it]
INFO:root:final mean train loss: 4801.0259093007735
INFO:root:final train perplexity: 6.646854400634766
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4544.43340674867
INFO:root:eval perplexity: 6.281667709350586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/8

  4%|â–         | 8/200 [39:30<16:17:47, 305.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4744.786783854167
INFO:root:current train perplexity6.443689823150635
INFO:root:current mean train loss 4733.421987334644
INFO:root:current train perplexity6.464957237243652
INFO:root:current mean train loss 4727.139240917598
INFO:root:current train perplexity6.462920665740967
INFO:root:current mean train loss 4738.101821437027
INFO:root:current train perplexity6.463621139526367
INFO:root:current mean train loss 4742.811539783848
INFO:root:current train perplexity6.47415828704834
INFO:root:current mean train loss 4735.046837273118
INFO:root:current train perplexity6.461654186248779
INFO:root:current mean train loss 4732.235540467689
INFO:root:current train perplexity6.458899021148682
INFO:root:current mean train loss 4729.978686811414
INFO:root:current train perplexity6.450743198394775
INFO:root:current mean train loss 4725.004805015661
INFO:root:current train perplexity6.442073822021484
INFO:root:current mean train loss 4721.552385783765
INFO:root:current train perplexity6.4329915046691895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.78s/it]
INFO:root:final mean train loss: 4719.392849276142
INFO:root:final train perplexity: 6.436192512512207
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it]
INFO:root:eval mean loss: 4486.297745941379
INFO:root:eval perplexity: 6.135718822479248
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/9

  4%|â–         | 9/200 [44:43<16:19:42, 307.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4643.151587257923
INFO:root:current train perplexity6.21678352355957
INFO:root:current mean train loss 4653.668828239218
INFO:root:current train perplexity6.265774250030518
INFO:root:current mean train loss 4676.572281840982
INFO:root:current train perplexity6.303302764892578
INFO:root:current mean train loss 4670.944907134434
INFO:root:current train perplexity6.291752815246582
INFO:root:current mean train loss 4664.998247992967
INFO:root:current train perplexity6.289331436157227
INFO:root:current mean train loss 4664.617182369198
INFO:root:current train perplexity6.282885551452637
INFO:root:current mean train loss 4666.099739995692
INFO:root:current train perplexity6.28801155090332
INFO:root:current mean train loss 4656.63507689638
INFO:root:current train perplexity6.27482795715332
INFO:root:current mean train loss 4658.837439062949
INFO:root:current train perplexity6.271572589874268
INFO:root:current mean train loss 4654.745051060842
INFO:root:current train perplexity6.264941692352295


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.04s/it]
INFO:root:final mean train loss: 4650.421834391932
INFO:root:final train perplexity: 6.263416290283203
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it]
INFO:root:eval mean loss: 4440.714201365802
INFO:root:eval perplexity: 6.023656368255615
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/10

  5%|â–Œ         | 10/200 [49:22<15:46:49, 299.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4631.997045589399
INFO:root:current train perplexity6.130322456359863
INFO:root:current mean train loss 4596.988330350908
INFO:root:current train perplexity6.10365104675293
INFO:root:current mean train loss 4606.518409428203
INFO:root:current train perplexity6.116256237030029
INFO:root:current mean train loss 4601.538678574373
INFO:root:current train perplexity6.120849132537842
INFO:root:current mean train loss 4605.256765600535
INFO:root:current train perplexity6.119520664215088
INFO:root:current mean train loss 4599.025535254075
INFO:root:current train perplexity6.119762897491455
INFO:root:current mean train loss 4598.344736630154
INFO:root:current train perplexity6.11664342880249
INFO:root:current mean train loss 4596.47256222922
INFO:root:current train perplexity6.113539218902588
INFO:root:current mean train loss 4591.390238096826
INFO:root:current train perplexity6.110463619232178
INFO:root:current mean train loss 4590.666637073864
INFO:root:current train perplexity6.110545635223389


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.92s/it]
INFO:root:final mean train loss: 4587.778538303991
INFO:root:final train perplexity: 6.110517978668213
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.67s/it]
INFO:root:eval mean loss: 4395.041464081893
INFO:root:eval perplexity: 5.913427352905273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/11

  6%|â–Œ         | 11/200 [54:02<15:24:02, 293.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4538.229576373922
INFO:root:current train perplexity5.974287509918213
INFO:root:current mean train loss 4559.179919890542
INFO:root:current train perplexity6.006707191467285
INFO:root:current mean train loss 4545.779063793009
INFO:root:current train perplexity5.979340553283691
INFO:root:current mean train loss 4546.832352985707
INFO:root:current train perplexity5.994357109069824
INFO:root:current mean train loss 4544.3000383005
INFO:root:current train perplexity5.993463516235352
INFO:root:current mean train loss 4540.737283891876
INFO:root:current train perplexity5.98698091506958
INFO:root:current mean train loss 4539.387058841089
INFO:root:current train perplexity5.986424922943115
INFO:root:current mean train loss 4536.396245197844
INFO:root:current train perplexity5.979893207550049
INFO:root:current mean train loss 4537.581895830397
INFO:root:current train perplexity5.9819159507751465
INFO:root:current mean train loss 4537.325702887538
INFO:root:current train perplexity5.982062816619873


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.42s/it]
INFO:root:final mean train loss: 4533.888069768106
INFO:root:final train perplexity: 5.98197078704834
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.86s/it]
INFO:root:eval mean loss: 4357.24412677305
INFO:root:eval perplexity: 5.823734283447266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/12

  6%|â–Œ         | 12/200 [58:45<15:08:34, 289.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4501.038127055921
INFO:root:current train perplexity5.890717506408691
INFO:root:current mean train loss 4485.296807391827
INFO:root:current train perplexity5.883645057678223
INFO:root:current mean train loss 4489.574888274629
INFO:root:current train perplexity5.892749309539795
INFO:root:current mean train loss 4482.982300731804
INFO:root:current train perplexity5.877628803253174
INFO:root:current mean train loss 4488.795636541193
INFO:root:current train perplexity5.878473281860352
INFO:root:current mean train loss 4483.513863904937
INFO:root:current train perplexity5.8750762939453125
INFO:root:current mean train loss 4480.56785915018
INFO:root:current train perplexity5.870061874389648
INFO:root:current mean train loss 4484.582062880798
INFO:root:current train perplexity5.86456823348999
INFO:root:current mean train loss 4485.644224096543
INFO:root:current train perplexity5.865556240081787


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.39s/it]
INFO:root:final mean train loss: 4484.545262982769
INFO:root:final train perplexity: 5.866645336151123
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.45s/it]
INFO:root:eval mean loss: 4322.752280377327
INFO:root:eval perplexity: 5.743072032928467
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/13

  6%|â–‹         | 13/200 [1:03:23<14:53:06, 286.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4702.91015625
INFO:root:current train perplexity6.030516624450684
INFO:root:current mean train loss 4458.422770972391
INFO:root:current train perplexity5.762020587921143
INFO:root:current mean train loss 4455.885654393088
INFO:root:current train perplexity5.760703086853027
INFO:root:current mean train loss 4454.864114389955
INFO:root:current train perplexity5.7684502601623535
INFO:root:current mean train loss 4457.029544650473
INFO:root:current train perplexity5.773984432220459
INFO:root:current mean train loss 4453.324566759598
INFO:root:current train perplexity5.7764997482299805
INFO:root:current mean train loss 4449.653326790526
INFO:root:current train perplexity5.777878284454346
INFO:root:current mean train loss 4446.203398659762
INFO:root:current train perplexity5.7713236808776855
INFO:root:current mean train loss 4444.4345019044795
INFO:root:current train perplexity5.768021583557129
INFO:root:current mean train loss 4444.599978154416
INFO:root:current train perplexity5.76718807220459


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.65s/it]
INFO:root:final mean train loss: 4440.914852019279
INFO:root:final train perplexity: 5.766523361206055
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it]
INFO:root:eval mean loss: 4296.74487997285
INFO:root:eval perplexity: 5.682990074157715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/14

  7%|â–‹         | 14/200 [1:08:03<14:41:35, 284.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4390.943403764205
INFO:root:current train perplexity5.595365047454834
INFO:root:current mean train loss 4398.603742169904
INFO:root:current train perplexity5.691611289978027
INFO:root:current mean train loss 4388.115448431946
INFO:root:current train perplexity5.6840009689331055
INFO:root:current mean train loss 4391.630831114348
INFO:root:current train perplexity5.679732322692871
INFO:root:current mean train loss 4388.82572339283
INFO:root:current train perplexity5.67389440536499
INFO:root:current mean train loss 4392.470778612708
INFO:root:current train perplexity5.6786789894104
INFO:root:current mean train loss 4400.214950436656
INFO:root:current train perplexity5.684433460235596
INFO:root:current mean train loss 4400.994355235254
INFO:root:current train perplexity5.6802778244018555
INFO:root:current mean train loss 4403.682644882861
INFO:root:current train perplexity5.685186386108398
INFO:root:current mean train loss 4405.828034418736
INFO:root:current train perplexity5.6812262535095215


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.25s/it]
INFO:root:final mean train loss: 4402.324271602015
INFO:root:final train perplexity: 5.679391860961914
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it]
INFO:root:eval mean loss: 4272.395088791001
INFO:root:eval perplexity: 5.6273088455200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/15

  8%|â–Š         | 15/200 [1:12:41<14:31:37, 282.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4416.676051089638
INFO:root:current train perplexity5.653327941894531
INFO:root:current mean train loss 4365.268095128677
INFO:root:current train perplexity5.577244281768799
INFO:root:current mean train loss 4362.302036511844
INFO:root:current train perplexity5.589941024780273
INFO:root:current mean train loss 4360.457374883669
INFO:root:current train perplexity5.589922904968262
INFO:root:current mean train loss 4365.188593680079
INFO:root:current train perplexity5.595102310180664
INFO:root:current mean train loss 4363.027397376264
INFO:root:current train perplexity5.591134071350098
INFO:root:current mean train loss 4362.100002208704
INFO:root:current train perplexity5.59122371673584
INFO:root:current mean train loss 4365.471173070237
INFO:root:current train perplexity5.595264434814453
INFO:root:current mean train loss 4365.146842686393
INFO:root:current train perplexity5.595942974090576
INFO:root:current mean train loss 4364.6829519178455
INFO:root:current train perplexity5.592251777648926


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.72s/it]
INFO:root:final mean train loss: 4365.052623625725
INFO:root:final train perplexity: 5.596489429473877
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it]
INFO:root:eval mean loss: 4248.549252340979
INFO:root:eval perplexity: 5.573307991027832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/16

  8%|â–Š         | 16/200 [1:17:24<14:26:51, 282.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4356.33923791956
INFO:root:current train perplexity5.5838823318481445
INFO:root:current mean train loss 4312.327167661171
INFO:root:current train perplexity5.528467178344727
INFO:root:current mean train loss 4325.605548337693
INFO:root:current train perplexity5.526209831237793
INFO:root:current mean train loss 4321.959382316752
INFO:root:current train perplexity5.514781475067139
INFO:root:current mean train loss 4328.951950037507
INFO:root:current train perplexity5.519003868103027
INFO:root:current mean train loss 4329.149893078451
INFO:root:current train perplexity5.51438045501709
INFO:root:current mean train loss 4329.555463142942
INFO:root:current train perplexity5.51258659362793
INFO:root:current mean train loss 4328.70128101627
INFO:root:current train perplexity5.518001079559326
INFO:root:current mean train loss 4329.135545576066
INFO:root:current train perplexity5.5153703689575195
INFO:root:current mean train loss 4331.346584612241
INFO:root:current train perplexity5.520495414733887


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.04s/it]
INFO:root:final mean train loss: 4331.649612057594
INFO:root:final train perplexity: 5.523220539093018
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.51s/it]
INFO:root:eval mean loss: 4227.6538051307625
INFO:root:eval perplexity: 5.5264129638671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/17

  8%|â–Š         | 17/200 [1:22:09<14:24:37, 283.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4262.633649553572
INFO:root:current train perplexity5.422844886779785
INFO:root:current mean train loss 4300.860013382523
INFO:root:current train perplexity5.450702667236328
INFO:root:current mean train loss 4294.700960979056
INFO:root:current train perplexity5.452118396759033
INFO:root:current mean train loss 4297.4333401352615
INFO:root:current train perplexity5.4505109786987305
INFO:root:current mean train loss 4296.927560389727
INFO:root:current train perplexity5.453505992889404
INFO:root:current mean train loss 4308.348299229702
INFO:root:current train perplexity5.457111358642578
INFO:root:current mean train loss 4303.752516378568
INFO:root:current train perplexity5.450589656829834
INFO:root:current mean train loss 4302.243286298894
INFO:root:current train perplexity5.44680643081665
INFO:root:current mean train loss 4303.1882958691995
INFO:root:current train perplexity5.454225540161133
INFO:root:current mean train loss 4302.3645714091745
INFO:root:current train perplexity5.456971168518066


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.78s/it]
INFO:root:final mean train loss: 4301.062409308649
INFO:root:final train perplexity: 5.456969261169434
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it]
INFO:root:eval mean loss: 4208.77827009918
INFO:root:eval perplexity: 5.484393119812012
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/18

  9%|â–‰         | 18/200 [1:26:54<14:20:37, 283.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4266.498364825581
INFO:root:current train perplexity5.400979518890381
INFO:root:current mean train loss 4284.7014262592875
INFO:root:current train perplexity5.4115777015686035
INFO:root:current mean train loss 4275.46220542374
INFO:root:current train perplexity5.396581172943115
INFO:root:current mean train loss 4281.4122246834
INFO:root:current train perplexity5.409492015838623
INFO:root:current mean train loss 4281.552794445718
INFO:root:current train perplexity5.404146194458008
INFO:root:current mean train loss 4282.311578740073
INFO:root:current train perplexity5.402763843536377
INFO:root:current mean train loss 4278.029015145072
INFO:root:current train perplexity5.396294116973877
INFO:root:current mean train loss 4274.702668920339
INFO:root:current train perplexity5.391825199127197
INFO:root:current mean train loss 4271.2387492586
INFO:root:current train perplexity5.389529705047607
INFO:root:current mean train loss 4270.350420501806
INFO:root:current train perplexity5.386855602264404


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.98s/it]
INFO:root:final mean train loss: 4269.6311602438645
INFO:root:final train perplexity: 5.3897175788879395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.91s/it]
INFO:root:eval mean loss: 4195.128677692819
INFO:root:eval perplexity: 5.454204559326172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/19

 10%|â–‰         | 19/200 [1:31:34<14:12:23, 282.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4211.540316712622
INFO:root:current train perplexity5.2477803230285645
INFO:root:current mean train loss 4210.408888658941
INFO:root:current train perplexity5.260633945465088
INFO:root:current mean train loss 4241.589412856387
INFO:root:current train perplexity5.292852401733398
INFO:root:current mean train loss 4234.0282966635505
INFO:root:current train perplexity5.304788112640381
INFO:root:current mean train loss 4240.016107326601
INFO:root:current train perplexity5.309173107147217
INFO:root:current mean train loss 4238.785620604583
INFO:root:current train perplexity5.315255641937256
INFO:root:current mean train loss 4245.868274319557
INFO:root:current train perplexity5.327103137969971
INFO:root:current mean train loss 4246.5670941349035
INFO:root:current train perplexity5.327287197113037
INFO:root:current mean train loss 4246.728328001065
INFO:root:current train perplexity5.330893516540527
INFO:root:current mean train loss 4248.9318056958955
INFO:root:current train perplexity5.333824157714844


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.08s/it]
INFO:root:final mean train loss: 4242.221535405805
INFO:root:final train perplexity: 5.331747055053711
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it]
INFO:root:eval mean loss: 4176.887108336104
INFO:root:eval perplexity: 5.414120674133301
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/20

 10%|â–ˆ         | 20/200 [1:36:37<14:26:25, 288.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4250.5297230866
INFO:root:current train perplexity5.28627872467041
INFO:root:current mean train loss 4237.607272933864
INFO:root:current train perplexity5.27323055267334
INFO:root:current mean train loss 4229.789983447454
INFO:root:current train perplexity5.273989200592041
INFO:root:current mean train loss 4228.386429725583
INFO:root:current train perplexity5.284526348114014
INFO:root:current mean train loss 4224.040212992749
INFO:root:current train perplexity5.279017925262451
INFO:root:current mean train loss 4221.003579564513
INFO:root:current train perplexity5.276250839233398
INFO:root:current mean train loss 4222.50019375652
INFO:root:current train perplexity5.276333332061768
INFO:root:current mean train loss 4226.428108144969
INFO:root:current train perplexity5.286112308502197
INFO:root:current mean train loss 4224.314683339094
INFO:root:current train perplexity5.284733772277832
INFO:root:current mean train loss 4221.6686998989835
INFO:root:current train perplexity5.280997276306152


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.79s/it]
INFO:root:final mean train loss: 4216.979137174545
INFO:root:final train perplexity: 5.2789130210876465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.75s/it]
INFO:root:eval mean loss: 4168.006773603724
INFO:root:eval perplexity: 5.39471435546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/21

 10%|â–ˆ         | 21/200 [1:41:46<14:39:41, 294.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4169.965455923508
INFO:root:current train perplexity5.217550754547119
INFO:root:current mean train loss 4184.718929816149
INFO:root:current train perplexity5.240118980407715
INFO:root:current mean train loss 4188.865256320225
INFO:root:current train perplexity5.227418422698975
INFO:root:current mean train loss 4185.568187744806
INFO:root:current train perplexity5.216604709625244
INFO:root:current mean train loss 4190.816755470423
INFO:root:current train perplexity5.223276615142822
INFO:root:current mean train loss 4190.35936251309
INFO:root:current train perplexity5.218215465545654
INFO:root:current mean train loss 4192.646074789754
INFO:root:current train perplexity5.220326900482178
INFO:root:current mean train loss 4193.299167757395
INFO:root:current train perplexity5.219857692718506
INFO:root:current mean train loss 4194.849673578071
INFO:root:current train perplexity5.220931529998779
INFO:root:current mean train loss 4193.3512249446585
INFO:root:current train perplexity5.223231792449951


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.39s/it]
INFO:root:final mean train loss: 4191.646745312599
INFO:root:final train perplexity: 5.22641658782959
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it]
INFO:root:eval mean loss: 4147.52978515625
INFO:root:eval perplexity: 5.350227355957031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/22

 11%|â–ˆ         | 22/200 [1:46:54<14:46:52, 298.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4181.679208984375
INFO:root:current train perplexity5.174361705780029
INFO:root:current mean train loss 4173.547784598214
INFO:root:current train perplexity5.184149742126465
INFO:root:current mean train loss 4164.633832563921
INFO:root:current train perplexity5.1616315841674805
INFO:root:current mean train loss 4164.511158203125
INFO:root:current train perplexity5.168060779571533
INFO:root:current mean train loss 4165.722295435855
INFO:root:current train perplexity5.1717400550842285
INFO:root:current mean train loss 4170.465451766305
INFO:root:current train perplexity5.176543235778809
INFO:root:current mean train loss 4168.2941540075235
INFO:root:current train perplexity5.178452014923096
INFO:root:current mean train loss 4172.7323743069555
INFO:root:current train perplexity5.181338787078857
INFO:root:current mean train loss 4173.0200658482145
INFO:root:current train perplexity5.181307315826416
INFO:root:current mean train loss 4174.570462740385
INFO:root:current train perplexity5.181865692138672


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.57s/it]
INFO:root:final mean train loss: 4169.434621072584
INFO:root:final train perplexity: 5.180815696716309
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it]
INFO:root:eval mean loss: 4138.600120165669
INFO:root:eval perplexity: 5.330944538116455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/23

 12%|â–ˆâ–        | 23/200 [1:52:04<14:51:41, 302.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4155.796939711973
INFO:root:current train perplexity5.101681709289551
INFO:root:current mean train loss 4149.618673689379
INFO:root:current train perplexity5.116613864898682
INFO:root:current mean train loss 4150.360664718142
INFO:root:current train perplexity5.120131492614746
INFO:root:current mean train loss 4156.2368623021375
INFO:root:current train perplexity5.130006313323975
INFO:root:current mean train loss 4157.721174725834
INFO:root:current train perplexity5.134799003601074
INFO:root:current mean train loss 4151.677937476549
INFO:root:current train perplexity5.129516124725342
INFO:root:current mean train loss 4146.600653853633
INFO:root:current train perplexity5.123647212982178
INFO:root:current mean train loss 4151.579115905372
INFO:root:current train perplexity5.134061336517334
INFO:root:current mean train loss 4151.227317317561
INFO:root:current train perplexity5.137825012207031
INFO:root:current mean train loss 4151.859023566649
INFO:root:current train perplexity5.138722896575928


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.06s/it]
INFO:root:final mean train loss: 4148.60066986084
INFO:root:final train perplexity: 5.138406276702881
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.45s/it]
INFO:root:eval mean loss: 4124.317723916777
INFO:root:eval perplexity: 5.30024528503418
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/24

 12%|â–ˆâ–        | 24/200 [1:57:21<14:59:21, 306.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4098.4228113195395
INFO:root:current train perplexity5.057861804962158
INFO:root:current mean train loss 4114.451782865674
INFO:root:current train perplexity5.082625865936279
INFO:root:current mean train loss 4112.182732965528
INFO:root:current train perplexity5.072386741638184
INFO:root:current mean train loss 4121.508960148258
INFO:root:current train perplexity5.088981628417969
INFO:root:current mean train loss 4121.713842325929
INFO:root:current train perplexity5.091942310333252
INFO:root:current mean train loss 4127.014157677665
INFO:root:current train perplexity5.094564437866211
INFO:root:current mean train loss 4129.296443602456
INFO:root:current train perplexity5.0973429679870605
INFO:root:current mean train loss 4133.36424330604
INFO:root:current train perplexity5.098433971405029
INFO:root:current mean train loss 4131.769183808572
INFO:root:current train perplexity5.100247383117676
INFO:root:current mean train loss 4130.579657592158
INFO:root:current train perplexity5.095413684844971


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.84s/it]
INFO:root:final mean train loss: 4127.260360410137
INFO:root:final train perplexity: 5.095325946807861
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.51s/it]
INFO:root:eval mean loss: 4115.928667650155
INFO:root:eval perplexity: 5.2822957038879395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/25

 12%|â–ˆâ–Ž        | 25/200 [2:02:28<14:54:01, 306.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4082.8728471235795
INFO:root:current train perplexity5.067503452301025
INFO:root:current mean train loss 4091.55883543695
INFO:root:current train perplexity5.035504341125488
INFO:root:current mean train loss 4104.335554550324
INFO:root:current train perplexity5.04459810256958
INFO:root:current mean train loss 4112.090519878799
INFO:root:current train perplexity5.057278156280518
INFO:root:current mean train loss 4111.49698469204
INFO:root:current train perplexity5.054096221923828
INFO:root:current mean train loss 4109.69963570469
INFO:root:current train perplexity5.048424243927002
INFO:root:current mean train loss 4110.1233367701625
INFO:root:current train perplexity5.050037384033203
INFO:root:current mean train loss 4113.273116053269
INFO:root:current train perplexity5.053041458129883
INFO:root:current mean train loss 4112.872354645352
INFO:root:current train perplexity5.0568528175354


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.65s/it]
INFO:root:final mean train loss: 4107.130020818403
INFO:root:final train perplexity: 5.055019378662109
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.37s/it]
INFO:root:eval mean loss: 4111.906932208555
INFO:root:eval perplexity: 5.273712158203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/26

 13%|â–ˆâ–Ž        | 26/200 [2:07:38<14:52:41, 307.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4144.792724609375
INFO:root:current train perplexity5.110298156738281
INFO:root:current mean train loss 4076.717682169977
INFO:root:current train perplexity5.013930797576904
INFO:root:current mean train loss 4088.0621202256943
INFO:root:current train perplexity5.025963306427002
INFO:root:current mean train loss 4091.2804301010283
INFO:root:current train perplexity5.02467679977417
INFO:root:current mean train loss 4098.741524061348
INFO:root:current train perplexity5.036691665649414
INFO:root:current mean train loss 4089.847919170673
INFO:root:current train perplexity5.024682521820068
INFO:root:current mean train loss 4092.1828396088604
INFO:root:current train perplexity5.017498970031738
INFO:root:current mean train loss 4093.6268008392635
INFO:root:current train perplexity5.021484375
INFO:root:current mean train loss 4091.663701885843
INFO:root:current train perplexity5.018756866455078
INFO:root:current mean train loss 4089.8989392399394
INFO:root:current train perplexity5.01642370223999


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.49s/it]
INFO:root:final mean train loss: 4088.354679415303
INFO:root:final train perplexity: 5.0177130699157715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it]
INFO:root:eval mean loss: 4100.305622506649
INFO:root:eval perplexity: 5.249029636383057
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/27

 14%|â–ˆâ–Ž        | 27/200 [2:12:55<14:55:31, 310.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4093.790283203125
INFO:root:current train perplexity4.936838150024414
INFO:root:current mean train loss 4047.1526006283966
INFO:root:current train perplexity4.953934192657471
INFO:root:current mean train loss 4047.8560069949126
INFO:root:current train perplexity4.95646858215332
INFO:root:current mean train loss 4061.0441003224205
INFO:root:current train perplexity4.970137596130371
INFO:root:current mean train loss 4071.309266166227
INFO:root:current train perplexity4.983354568481445
INFO:root:current mean train loss 4066.122805578732
INFO:root:current train perplexity4.973259925842285
INFO:root:current mean train loss 4069.4086747332317
INFO:root:current train perplexity4.975294589996338
INFO:root:current mean train loss 4070.3599175726617
INFO:root:current train perplexity4.974230766296387
INFO:root:current mean train loss 4073.5872588549655
INFO:root:current train perplexity4.977571487426758
INFO:root:current mean train loss 4070.7154782488046
INFO:root:current train perplexity4.97666597366333


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.13s/it]
INFO:root:final mean train loss: 4069.4669130386847
INFO:root:final train perplexity: 4.980461120605469
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it]
INFO:root:eval mean loss: 4090.1817220052085
INFO:root:eval perplexity: 5.2275848388671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/28

 14%|â–ˆâ–        | 28/200 [2:18:08<14:51:43, 311.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4091.5481593919835
INFO:root:current train perplexity4.972657680511475
INFO:root:current mean train loss 4057.220202934451
INFO:root:current train perplexity4.935136795043945
INFO:root:current mean train loss 4063.388431018778
INFO:root:current train perplexity4.95487642288208
INFO:root:current mean train loss 4052.848415882595
INFO:root:current train perplexity4.943769931793213
INFO:root:current mean train loss 4063.904667414672
INFO:root:current train perplexity4.951615810394287
INFO:root:current mean train loss 4064.7959665914796
INFO:root:current train perplexity4.9510722160339355
INFO:root:current mean train loss 4057.142182327197
INFO:root:current train perplexity4.945778846740723
INFO:root:current mean train loss 4054.131309836402
INFO:root:current train perplexity4.94454288482666
INFO:root:current mean train loss 4057.0832086426376
INFO:root:current train perplexity4.946102142333984
INFO:root:current mean train loss 4055.513143653084
INFO:root:current train perplexity4.946244716644287


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.93s/it]
INFO:root:final mean train loss: 4051.3136461934737
INFO:root:final train perplexity: 4.944918632507324
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it]
INFO:root:eval mean loss: 4084.21684708832
INFO:root:eval perplexity: 5.214991092681885
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/29

 14%|â–ˆâ–        | 29/200 [2:23:18<14:45:36, 310.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4055.6403887348793
INFO:root:current train perplexity4.898158550262451
INFO:root:current mean train loss 4035.780713263359
INFO:root:current train perplexity4.891722679138184
INFO:root:current mean train loss 4054.163496009199
INFO:root:current train perplexity4.9117608070373535
INFO:root:current mean train loss 4057.3355333034365
INFO:root:current train perplexity4.913818836212158
INFO:root:current mean train loss 4054.419379780851
INFO:root:current train perplexity4.909539222717285
INFO:root:current mean train loss 4050.293727379061
INFO:root:current train perplexity4.910863876342773
INFO:root:current mean train loss 4045.14790201565
INFO:root:current train perplexity4.907707691192627
INFO:root:current mean train loss 4044.9733873359482
INFO:root:current train perplexity4.910800457000732
INFO:root:current mean train loss 4041.985578957017
INFO:root:current train perplexity4.907814025878906
INFO:root:current mean train loss 4036.96642319037
INFO:root:current train perplexity4.910030364990234


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.63s/it]
INFO:root:final mean train loss: 4033.814728213895
INFO:root:final train perplexity: 4.9108967781066895
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it]
INFO:root:eval mean loss: 4072.6459251025044
INFO:root:eval perplexity: 5.190647602081299
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/30

 15%|â–ˆâ–Œ        | 30/200 [2:27:56<14:12:33, 300.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4018.6190467247598
INFO:root:current train perplexity4.900896072387695
INFO:root:current mean train loss 4012.2502845379945
INFO:root:current train perplexity4.883295059204102
INFO:root:current mean train loss 4016.940974151739
INFO:root:current train perplexity4.869140148162842
INFO:root:current mean train loss 4013.8929130081583
INFO:root:current train perplexity4.867584705352783
INFO:root:current mean train loss 4020.5626429251497
INFO:root:current train perplexity4.871165752410889
INFO:root:current mean train loss 4018.228476671208
INFO:root:current train perplexity4.872544765472412
INFO:root:current mean train loss 4018.4227522251563
INFO:root:current train perplexity4.875664710998535
INFO:root:current mean train loss 4022.9364438192447
INFO:root:current train perplexity4.879305362701416
INFO:root:current mean train loss 4022.718896076989
INFO:root:current train perplexity4.882439136505127
INFO:root:current mean train loss 4021.704854264427
INFO:root:current train perplexity4.8818888664245605


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.01s/it]
INFO:root:final mean train loss: 4018.319003320509
INFO:root:final train perplexity: 4.880965709686279
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it]
INFO:root:eval mean loss: 4066.762688386525
INFO:root:eval perplexity: 5.178313255310059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/31

 16%|â–ˆâ–Œ        | 31/200 [2:33:04<14:14:08, 303.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3990.6942476313166
INFO:root:current train perplexity4.8192973136901855
INFO:root:current mean train loss 3991.7553760762116
INFO:root:current train perplexity4.820246696472168
INFO:root:current mean train loss 3983.665757646445
INFO:root:current train perplexity4.806534767150879
INFO:root:current mean train loss 3995.7781950760987
INFO:root:current train perplexity4.822371482849121
INFO:root:current mean train loss 4000.272999466932
INFO:root:current train perplexity4.830859184265137
INFO:root:current mean train loss 4002.503338076297
INFO:root:current train perplexity4.8415398597717285
INFO:root:current mean train loss 4006.485975687065
INFO:root:current train perplexity4.8472161293029785
INFO:root:current mean train loss 4001.8655164276897
INFO:root:current train perplexity4.847210884094238
INFO:root:current mean train loss 4004.179269837939
INFO:root:current train perplexity4.850754737854004
INFO:root:current mean train loss 4004.253916819974
INFO:root:current train perplexity4.849946975708008


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.62s/it]
INFO:root:final mean train loss: 4003.0190418612574
INFO:root:final train perplexity: 4.85159158706665
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.34s/it]
INFO:root:eval mean loss: 4060.626407704455
INFO:root:eval perplexity: 5.165480136871338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/32

 16%|â–ˆâ–Œ        | 32/200 [2:38:19<14:18:27, 306.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3981.509432705966
INFO:root:current train perplexity4.7495198249816895
INFO:root:current mean train loss 4003.807908581149
INFO:root:current train perplexity4.812755107879639
INFO:root:current mean train loss 3982.90423560049
INFO:root:current train perplexity4.801252365112305
INFO:root:current mean train loss 3976.662218722491
INFO:root:current train perplexity4.808181285858154
INFO:root:current mean train loss 3985.5383590530564
INFO:root:current train perplexity4.818293571472168
INFO:root:current mean train loss 3986.787060546875
INFO:root:current train perplexity4.819086074829102
INFO:root:current mean train loss 3990.775696639432
INFO:root:current train perplexity4.8206610679626465
INFO:root:current mean train loss 3992.8600906068914
INFO:root:current train perplexity4.8218512535095215
INFO:root:current mean train loss 3993.7386870088635
INFO:root:current train perplexity4.8228349685668945
INFO:root:current mean train loss 3993.0620853444043
INFO:root:current train perplexity4.826437950134277


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.35s/it]
INFO:root:final mean train loss: 3988.9874017776983
INFO:root:final train perplexity: 4.824808120727539
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.54s/it]
INFO:root:eval mean loss: 4054.372475482048
INFO:root:eval perplexity: 5.152433395385742
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/33

 16%|â–ˆâ–‹        | 33/200 [2:43:07<13:57:48, 301.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3924.0286070808534
INFO:root:current train perplexity4.7457170486450195
INFO:root:current mean train loss 3949.3929720451497
INFO:root:current train perplexity4.779756546020508
INFO:root:current mean train loss 3949.0380915072483
INFO:root:current train perplexity4.780108451843262
INFO:root:current mean train loss 3959.4131753884726
INFO:root:current train perplexity4.782401084899902
INFO:root:current mean train loss 3968.5771758571814
INFO:root:current train perplexity4.784366607666016
INFO:root:current mean train loss 3965.8118174469914
INFO:root:current train perplexity4.779453754425049
INFO:root:current mean train loss 3968.496956895739
INFO:root:current train perplexity4.7857842445373535
INFO:root:current mean train loss 3967.042831160919
INFO:root:current train perplexity4.782673358917236
INFO:root:current mean train loss 3969.0409884668306
INFO:root:current train perplexity4.784251689910889
INFO:root:current mean train loss 3976.399673921421
INFO:root:current train perplexity4.795004844665527


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.62s/it]
INFO:root:final mean train loss: 3972.917253863427
INFO:root:final train perplexity: 4.794314861297607
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.76s/it]
INFO:root:eval mean loss: 4050.9274711879434
INFO:root:eval perplexity: 5.145260810852051
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/34

 17%|â–ˆâ–‹        | 34/200 [2:48:22<14:04:52, 305.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3992.401349994498
INFO:root:current train perplexity4.774587631225586
INFO:root:current mean train loss 3954.1769805372805
INFO:root:current train perplexity4.746944904327393
INFO:root:current mean train loss 3950.360489398351
INFO:root:current train perplexity4.750211715698242
INFO:root:current mean train loss 3952.5968172222456
INFO:root:current train perplexity4.74562406539917
INFO:root:current mean train loss 3952.580579883227
INFO:root:current train perplexity4.746511459350586
INFO:root:current mean train loss 3956.8757944190565
INFO:root:current train perplexity4.756639003753662
INFO:root:current mean train loss 3959.066180301672
INFO:root:current train perplexity4.760190963745117
INFO:root:current mean train loss 3958.0192934424654
INFO:root:current train perplexity4.759819030761719
INFO:root:current mean train loss 3961.581095611187
INFO:root:current train perplexity4.763672351837158
INFO:root:current mean train loss 3960.3747055729436
INFO:root:current train perplexity4.765298843383789


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.86s/it]
INFO:root:final mean train loss: 3957.646281027025
INFO:root:final train perplexity: 4.76551628112793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it]
INFO:root:eval mean loss: 4045.146451476618
INFO:root:eval perplexity: 5.133246898651123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/35

 18%|â–ˆâ–Š        | 35/200 [2:53:38<14:08:22, 308.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3969.4232100474683
INFO:root:current train perplexity4.758956432342529
INFO:root:current mean train loss 3948.3220678574544
INFO:root:current train perplexity4.726589202880859
INFO:root:current mean train loss 3951.575618839606
INFO:root:current train perplexity4.737825393676758
INFO:root:current mean train loss 3949.3727782558954
INFO:root:current train perplexity4.734598636627197
INFO:root:current mean train loss 3948.076602051801
INFO:root:current train perplexity4.7381815910339355
INFO:root:current mean train loss 3950.0860197235265
INFO:root:current train perplexity4.735019683837891
INFO:root:current mean train loss 3950.6776984190906
INFO:root:current train perplexity4.738769054412842
INFO:root:current mean train loss 3948.629452510731
INFO:root:current train perplexity4.739019870758057
INFO:root:current mean train loss 3946.7823259963384
INFO:root:current train perplexity4.73756217956543
INFO:root:current mean train loss 3946.634157891902
INFO:root:current train perplexity4.739621639251709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.07s/it]
INFO:root:final mean train loss: 3943.5065889050884
INFO:root:final train perplexity: 4.739006042480469
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.47s/it]
INFO:root:eval mean loss: 4042.7616044714096
INFO:root:eval perplexity: 5.128299713134766
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/36

 18%|â–ˆâ–Š        | 36/200 [2:58:57<14:11:35, 311.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3898.2268318965516
INFO:root:current train perplexity4.68770170211792
INFO:root:current mean train loss 3907.1555136614306
INFO:root:current train perplexity4.682829856872559
INFO:root:current mean train loss 3919.912409659462
INFO:root:current train perplexity4.693997859954834
INFO:root:current mean train loss 3924.6440770348836
INFO:root:current train perplexity4.702035427093506
INFO:root:current mean train loss 3920.7402614460343
INFO:root:current train perplexity4.7047953605651855
INFO:root:current mean train loss 3921.5018038124203
INFO:root:current train perplexity4.706630229949951
INFO:root:current mean train loss 3926.345244339633
INFO:root:current train perplexity4.708750247955322
INFO:root:current mean train loss 3929.2697440587276
INFO:root:current train perplexity4.711061000823975
INFO:root:current mean train loss 3930.762201251145
INFO:root:current train perplexity4.709813594818115
INFO:root:current mean train loss 3932.849293748417
INFO:root:current train perplexity4.714084148406982


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.64s/it]
INFO:root:final mean train loss: 3929.9933599041356
INFO:root:final train perplexity: 4.713808059692383
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4037.7039180241577
INFO:root:eval perplexity: 5.11782169342041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/37

 18%|â–ˆâ–Š        | 37/200 [3:04:11<14:09:01, 312.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3892.9307668585525
INFO:root:current train perplexity4.644598007202148
INFO:root:current mean train loss 3894.6923753004808
INFO:root:current train perplexity4.65382719039917
INFO:root:current mean train loss 3904.5637140823624
INFO:root:current train perplexity4.6712965965271
INFO:root:current mean train loss 3897.244784661788
INFO:root:current train perplexity4.671479225158691
INFO:root:current mean train loss 3904.1523210621845
INFO:root:current train perplexity4.677372932434082
INFO:root:current mean train loss 3908.0689342338496
INFO:root:current train perplexity4.6800618171691895
INFO:root:current mean train loss 3909.5382580654227
INFO:root:current train perplexity4.681509971618652
INFO:root:current mean train loss 3916.0515710986633
INFO:root:current train perplexity4.688193321228027
INFO:root:current mean train loss 3921.5659340629363
INFO:root:current train perplexity4.692229747772217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.31s/it]
INFO:root:final mean train loss: 3917.9342480321084
INFO:root:final train perplexity: 4.691434860229492
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it]
INFO:root:eval mean loss: 4033.7237973044103
INFO:root:eval perplexity: 5.109591484069824
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/38

 19%|â–ˆâ–‰        | 38/200 [3:09:26<14:05:31, 313.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3830.983154296875
INFO:root:current train perplexity4.490453243255615
INFO:root:current mean train loss 3913.2366007091928
INFO:root:current train perplexity4.662052154541016
INFO:root:current mean train loss 3903.779248768473
INFO:root:current train perplexity4.655093669891357
INFO:root:current mean train loss 3906.0634330522894
INFO:root:current train perplexity4.654759407043457
INFO:root:current mean train loss 3900.223492265043
INFO:root:current train perplexity4.648864269256592
INFO:root:current mean train loss 3902.098781820794
INFO:root:current train perplexity4.6478166580200195
INFO:root:current mean train loss 3907.7515199069753
INFO:root:current train perplexity4.657480716705322
INFO:root:current mean train loss 3905.335293635535
INFO:root:current train perplexity4.661286354064941
INFO:root:current mean train loss 3907.502273274537
INFO:root:current train perplexity4.662692070007324
INFO:root:current mean train loss 3909.701525243546
INFO:root:current train perplexity4.666112899780273


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.18s/it]
INFO:root:final mean train loss: 3906.39594053453
INFO:root:final train perplexity: 4.670125961303711
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it]
INFO:root:eval mean loss: 4027.9257275736923
INFO:root:eval perplexity: 5.097625732421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/39

 20%|â–ˆâ–‰        | 39/200 [3:14:39<14:00:24, 313.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3846.2744140625
INFO:root:current train perplexity4.577446460723877
INFO:root:current mean train loss 3871.31188634924
INFO:root:current train perplexity4.604209899902344
INFO:root:current mean train loss 3882.342714427207
INFO:root:current train perplexity4.61372184753418
INFO:root:current mean train loss 3881.4383996307274
INFO:root:current train perplexity4.626084804534912
INFO:root:current mean train loss 3884.734939315313
INFO:root:current train perplexity4.633991241455078
INFO:root:current mean train loss 3889.222250145242
INFO:root:current train perplexity4.6360297203063965
INFO:root:current mean train loss 3888.2180359585977
INFO:root:current train perplexity4.633139133453369
INFO:root:current mean train loss 3890.6706257966334
INFO:root:current train perplexity4.638494491577148
INFO:root:current mean train loss 3892.379563713779
INFO:root:current train perplexity4.639604568481445
INFO:root:current mean train loss 3893.826846142632
INFO:root:current train perplexity4.641850471496582


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.28s/it]
INFO:root:final mean train loss: 3891.6689546031334
INFO:root:final train perplexity: 4.643070697784424
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it]
INFO:root:eval mean loss: 4025.6601216201243
INFO:root:eval perplexity: 5.092957973480225
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/40

 20%|â–ˆâ–ˆ        | 40/200 [3:19:53<13:55:44, 313.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3888.764275801809
INFO:root:current train perplexity4.632397174835205
INFO:root:current mean train loss 3876.7915695575107
INFO:root:current train perplexity4.619166851043701
INFO:root:current mean train loss 3876.5087199450627
INFO:root:current train perplexity4.619449138641357
INFO:root:current mean train loss 3884.26314226587
INFO:root:current train perplexity4.619173049926758
INFO:root:current mean train loss 3883.6217644083754
INFO:root:current train perplexity4.61944580078125
INFO:root:current mean train loss 3886.5417466356575
INFO:root:current train perplexity4.621883869171143
INFO:root:current mean train loss 3884.5119688067953
INFO:root:current train perplexity4.6191935539245605
INFO:root:current mean train loss 3890.082203065238
INFO:root:current train perplexity4.628046035766602
INFO:root:current mean train loss 3886.446314698756
INFO:root:current train perplexity4.623930931091309
INFO:root:current mean train loss 3883.5905392452732
INFO:root:current train perplexity4.623660087585449


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.86s/it]
INFO:root:final mean train loss: 3880.473082327074
INFO:root:final train perplexity: 4.6226067543029785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.62s/it]
INFO:root:eval mean loss: 4020.945826753657
INFO:root:eval perplexity: 5.083257675170898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/41

 20%|â–ˆâ–ˆ        | 41/200 [3:25:02<13:47:03, 312.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3830.916087962963
INFO:root:current train perplexity4.5870041847229
INFO:root:current mean train loss 3850.251376414862
INFO:root:current train perplexity4.571352958679199
INFO:root:current mean train loss 3855.537582599119
INFO:root:current train perplexity4.58320426940918
INFO:root:current mean train loss 3858.3947925625957
INFO:root:current train perplexity4.58048677444458
INFO:root:current mean train loss 3860.8014313387366
INFO:root:current train perplexity4.5889458656311035
INFO:root:current mean train loss 3861.380476718157
INFO:root:current train perplexity4.584953784942627
INFO:root:current mean train loss 3864.281253115032
INFO:root:current train perplexity4.585445404052734
INFO:root:current mean train loss 3868.9044876203575
INFO:root:current train perplexity4.593024730682373
INFO:root:current mean train loss 3873.9597454324744
INFO:root:current train perplexity4.600736618041992
INFO:root:current mean train loss 3873.844005465379
INFO:root:current train perplexity4.601924896240234


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.96s/it]
INFO:root:final mean train loss: 3869.6391213324764
INFO:root:final train perplexity: 4.602891445159912
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it]
INFO:root:eval mean loss: 4017.3020573609265
INFO:root:eval perplexity: 5.0757737159729
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/42

 21%|â–ˆâ–ˆ        | 42/200 [3:30:16<13:42:53, 312.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3798.152197265625
INFO:root:current train perplexity4.543546199798584
INFO:root:current mean train loss 3823.153141276042
INFO:root:current train perplexity4.557469367980957
INFO:root:current mean train loss 3839.3979918134974
INFO:root:current train perplexity4.558098316192627
INFO:root:current mean train loss 3852.3743433710356
INFO:root:current train perplexity4.573044300079346
INFO:root:current mean train loss 3855.334073051365
INFO:root:current train perplexity4.573105812072754
INFO:root:current mean train loss 3857.61159234448
INFO:root:current train perplexity4.575128078460693
INFO:root:current mean train loss 3857.7892670398624
INFO:root:current train perplexity4.578411102294922
INFO:root:current mean train loss 3861.574471526892
INFO:root:current train perplexity4.5793633460998535
INFO:root:current mean train loss 3858.6471121234094
INFO:root:current train perplexity4.576838493347168
INFO:root:current mean train loss 3858.9030479716744
INFO:root:current train perplexity4.577834606170654


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.79s/it]
INFO:root:final mean train loss: 3857.928845620924
INFO:root:final train perplexity: 4.581675052642822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.32s/it]
INFO:root:eval mean loss: 4013.1391359153367
INFO:root:eval perplexity: 5.06723690032959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/43

 22%|â–ˆâ–ˆâ–       | 43/200 [3:35:32<13:40:16, 313.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3890.6254655704943
INFO:root:current train perplexity4.572019577026367
INFO:root:current mean train loss 3865.248173213505
INFO:root:current train perplexity4.570649147033691
INFO:root:current mean train loss 3855.3099068849665
INFO:root:current train perplexity4.550830841064453
INFO:root:current mean train loss 3853.183656386662
INFO:root:current train perplexity4.553122043609619
INFO:root:current mean train loss 3852.4593613325337
INFO:root:current train perplexity4.549975872039795
INFO:root:current mean train loss 3852.3374477548055
INFO:root:current train perplexity4.553623199462891
INFO:root:current mean train loss 3850.8824639446443
INFO:root:current train perplexity4.553452491760254
INFO:root:current mean train loss 3848.1783682205796
INFO:root:current train perplexity4.554754734039307
INFO:root:current mean train loss 3849.261543536384
INFO:root:current train perplexity4.559119701385498
INFO:root:current mean train loss 3848.4416876719083
INFO:root:current train perplexity4.559080600738525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.02s/it]
INFO:root:final mean train loss: 3845.4968739786455
INFO:root:final train perplexity: 4.559256553649902
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it]
INFO:root:eval mean loss: 4011.4486006067154
INFO:root:eval perplexity: 5.063774108886719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/44

 22%|â–ˆâ–ˆâ–       | 44/200 [3:40:39<13:30:05, 311.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3786.058526731005
INFO:root:current train perplexity4.490429401397705
INFO:root:current mean train loss 3822.5699406301737
INFO:root:current train perplexity4.508115291595459
INFO:root:current mean train loss 3825.7287714376866
INFO:root:current train perplexity4.519128799438477
INFO:root:current mean train loss 3825.6968142082887
INFO:root:current train perplexity4.520097732543945
INFO:root:current mean train loss 3832.4929177565477
INFO:root:current train perplexity4.521117687225342
INFO:root:current mean train loss 3835.443317281789
INFO:root:current train perplexity4.529540061950684
INFO:root:current mean train loss 3831.6739766345045
INFO:root:current train perplexity4.528914928436279
INFO:root:current mean train loss 3834.624621923373
INFO:root:current train perplexity4.533222675323486
INFO:root:current mean train loss 3833.417328418772
INFO:root:current train perplexity4.534626483917236
INFO:root:current mean train loss 3836.430016871632
INFO:root:current train perplexity4.53707218170166


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.52s/it]
INFO:root:final mean train loss: 3834.697056924143
INFO:root:final train perplexity: 4.539872646331787
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it]
INFO:root:eval mean loss: 4007.6604454094636
INFO:root:eval perplexity: 5.056023120880127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [3:45:36<13:13:56, 307.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3817.88526632018
INFO:root:current train perplexity4.4846978187561035
INFO:root:current mean train loss 3829.366355272209
INFO:root:current train perplexity4.5206828117370605
INFO:root:current mean train loss 3823.6564168451378
INFO:root:current train perplexity4.518189430236816
INFO:root:current mean train loss 3821.1236895292914
INFO:root:current train perplexity4.51496696472168
INFO:root:current mean train loss 3820.7797739864177
INFO:root:current train perplexity4.515836715698242
INFO:root:current mean train loss 3822.320386746702
INFO:root:current train perplexity4.516660690307617
INFO:root:current mean train loss 3827.1791469822883
INFO:root:current train perplexity4.519029140472412
INFO:root:current mean train loss 3830.0933221909995
INFO:root:current train perplexity4.517561912536621
INFO:root:current mean train loss 3827.2845608197213
INFO:root:current train perplexity4.518674373626709
INFO:root:current mean train loss 3826.7675638686133
INFO:root:current train perplexity4.519299030303955


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.03s/it]
INFO:root:final mean train loss: 3823.174038979315
INFO:root:final train perplexity: 4.519280910491943
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.49s/it]
INFO:root:eval mean loss: 4005.9493918993794
INFO:root:eval perplexity: 5.052526473999023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [3:50:15<12:46:36, 298.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3832.18457395639
INFO:root:current train perplexity4.504281520843506
INFO:root:current mean train loss 3818.7841197487837
INFO:root:current train perplexity4.479460716247559
INFO:root:current mean train loss 3812.1795430272705
INFO:root:current train perplexity4.484474182128906
INFO:root:current mean train loss 3818.2381521947377
INFO:root:current train perplexity4.4938788414001465
INFO:root:current mean train loss 3822.733144886744
INFO:root:current train perplexity4.496984958648682
INFO:root:current mean train loss 3820.2083540013227
INFO:root:current train perplexity4.49629545211792
INFO:root:current mean train loss 3822.2617421757873
INFO:root:current train perplexity4.5043044090271
INFO:root:current mean train loss 3818.468562517825
INFO:root:current train perplexity4.501430034637451
INFO:root:current mean train loss 3817.7463714001224
INFO:root:current train perplexity4.501514911651611
INFO:root:current mean train loss 3817.034700790137
INFO:root:current train perplexity4.501866340637207


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.50s/it]
INFO:root:final mean train loss: 3813.1635133681757
INFO:root:final train perplexity: 4.501467227935791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.31s/it]
INFO:root:eval mean loss: 4006.6545531360816
INFO:root:eval perplexity: 5.053966045379639
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [3:55:22<12:48:32, 301.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3803.3736067708332
INFO:root:current train perplexity4.431095600128174
INFO:root:current mean train loss 3794.8918303571427
INFO:root:current train perplexity4.465921878814697
INFO:root:current mean train loss 3795.751569602273
INFO:root:current train perplexity4.4657368659973145
INFO:root:current mean train loss 3803.7265390625
INFO:root:current train perplexity4.477301597595215
INFO:root:current mean train loss 3803.374124691612
INFO:root:current train perplexity4.479032516479492
INFO:root:current mean train loss 3805.0317170516305
INFO:root:current train perplexity4.480134010314941
INFO:root:current mean train loss 3804.33880859375
INFO:root:current train perplexity4.481734275817871
INFO:root:current mean train loss 3804.5575384324598
INFO:root:current train perplexity4.48568058013916
INFO:root:current mean train loss 3805.598923828125
INFO:root:current train perplexity4.486982822418213
INFO:root:current mean train loss 3806.5925736177883
INFO:root:current train perplexity4.485650062561035


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.41s/it]
INFO:root:final mean train loss: 3804.080089322982
INFO:root:final train perplexity: 4.485363483428955
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.38s/it]
INFO:root:eval mean loss: 4003.2847995622783
INFO:root:eval perplexity: 5.047084808349609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/48

 24%|â–ˆâ–ˆâ–       | 48/200 [4:00:33<12:50:44, 304.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3797.749829395708
INFO:root:current train perplexity4.4424943923950195
INFO:root:current mean train loss 3803.281734278945
INFO:root:current train perplexity4.447078227996826
INFO:root:current mean train loss 3789.6418758971954
INFO:root:current train perplexity4.440796852111816
INFO:root:current mean train loss 3787.2485619288514
INFO:root:current train perplexity4.450978755950928
INFO:root:current mean train loss 3787.0724521423717
INFO:root:current train perplexity4.454072952270508
INFO:root:current mean train loss 3791.489488133844
INFO:root:current train perplexity4.449474811553955
INFO:root:current mean train loss 3792.6230054104135
INFO:root:current train perplexity4.453034400939941
INFO:root:current mean train loss 3791.776759122067
INFO:root:current train perplexity4.456660270690918
INFO:root:current mean train loss 3792.4993184522755
INFO:root:current train perplexity4.458685874938965
INFO:root:current mean train loss 3795.342785358914
INFO:root:current train perplexity4.465802192687988


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.94s/it]
INFO:root:final mean train loss: 3792.8732831401207
INFO:root:final train perplexity: 4.465575695037842
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it]
INFO:root:eval mean loss: 3999.3041645888743
INFO:root:eval perplexity: 5.038967609405518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/49

 24%|â–ˆâ–ˆâ–       | 49/200 [4:05:50<12:55:28, 308.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3782.780026614011
INFO:root:current train perplexity4.394871234893799
INFO:root:current mean train loss 3779.1526313502127
INFO:root:current train perplexity4.413736343383789
INFO:root:current mean train loss 3771.7903838796715
INFO:root:current train perplexity4.423780918121338
INFO:root:current mean train loss 3776.4436715752877
INFO:root:current train perplexity4.4307661056518555
INFO:root:current mean train loss 3776.747709254869
INFO:root:current train perplexity4.433101654052734
INFO:root:current mean train loss 3779.7747461928934
INFO:root:current train perplexity4.436883926391602
INFO:root:current mean train loss 3778.476342384791
INFO:root:current train perplexity4.436209201812744
INFO:root:current mean train loss 3781.4541595883375
INFO:root:current train perplexity4.43961238861084
INFO:root:current mean train loss 3781.418426342417
INFO:root:current train perplexity4.442012310028076
INFO:root:current mean train loss 3785.8187419687342
INFO:root:current train perplexity4.448009490966797


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.57s/it]
INFO:root:final mean train loss: 3782.9340832617977
INFO:root:final train perplexity: 4.448099613189697
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it]
INFO:root:eval mean loss: 4002.46011157746
INFO:root:eval perplexity: 5.0454020500183105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [4:10:34<12:31:37, 300.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3763.6568492542615
INFO:root:current train perplexity4.3880228996276855
INFO:root:current mean train loss 3767.424607166693
INFO:root:current train perplexity4.401833534240723
INFO:root:current mean train loss 3769.276673383936
INFO:root:current train perplexity4.408788681030273
INFO:root:current mean train loss 3771.5921866433664
INFO:root:current train perplexity4.411896705627441
INFO:root:current mean train loss 3775.581495999812
INFO:root:current train perplexity4.423418045043945
INFO:root:current mean train loss 3774.1096590834986
INFO:root:current train perplexity4.427644729614258
INFO:root:current mean train loss 3769.347570678536
INFO:root:current train perplexity4.4268293380737305
INFO:root:current mean train loss 3769.6173497511536
INFO:root:current train perplexity4.427301406860352
INFO:root:current mean train loss 3771.185842885324
INFO:root:current train perplexity4.4283857345581055


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.55s/it]
INFO:root:final mean train loss: 3772.8515603465416
INFO:root:final train perplexity: 4.430440425872803
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.33s/it]
INFO:root:eval mean loss: 4000.046733017509
INFO:root:eval perplexity: 5.0404815673828125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [4:15:19<12:15:34, 296.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3710.2357003348216
INFO:root:current train perplexity4.378704071044922
INFO:root:current mean train loss 3757.5902042567172
INFO:root:current train perplexity4.385841369628906
INFO:root:current mean train loss 3757.650088692633
INFO:root:current train perplexity4.395932197570801
INFO:root:current mean train loss 3768.0290757965186
INFO:root:current train perplexity4.398380279541016
INFO:root:current mean train loss 3770.547883954622
INFO:root:current train perplexity4.407130718231201
INFO:root:current mean train loss 3767.6955889037845
INFO:root:current train perplexity4.406146049499512
INFO:root:current mean train loss 3764.6245141320014
INFO:root:current train perplexity4.406951427459717
INFO:root:current mean train loss 3761.0792358916415
INFO:root:current train perplexity4.4079790115356445
INFO:root:current mean train loss 3764.670194755847
INFO:root:current train perplexity4.412323474884033
INFO:root:current mean train loss 3765.447384599814
INFO:root:current train perplexity4.411988735198975


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.67s/it]
INFO:root:final mean train loss: 3763.1492605516987
INFO:root:final train perplexity: 4.413514614105225
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.40s/it]
INFO:root:eval mean loss: 3997.2926795905364
INFO:root:eval perplexity: 5.034870147705078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [4:20:28<12:19:58, 299.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3777.710302734375
INFO:root:current train perplexity4.480462074279785
INFO:root:current mean train loss 3735.460710343071
INFO:root:current train perplexity4.371242523193359
INFO:root:current mean train loss 3739.453665515988
INFO:root:current train perplexity4.377017021179199
INFO:root:current mean train loss 3739.5701706659224
INFO:root:current train perplexity4.382413387298584
INFO:root:current mean train loss 3745.490259083208
INFO:root:current train perplexity4.3824005126953125
INFO:root:current mean train loss 3747.470672311135
INFO:root:current train perplexity4.3843674659729
INFO:root:current mean train loss 3745.7214823901168
INFO:root:current train perplexity4.389728546142578
INFO:root:current mean train loss 3750.1534036276225
INFO:root:current train perplexity4.394032001495361
INFO:root:current mean train loss 3755.8705476538535
INFO:root:current train perplexity4.396197319030762
INFO:root:current mean train loss 3756.565468910092
INFO:root:current train perplexity4.396729469299316


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.86s/it]
INFO:root:final mean train loss: 3754.6537766610422
INFO:root:final train perplexity: 4.398746013641357
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it]
INFO:root:eval mean loss: 3994.199701836769
INFO:root:eval perplexity: 5.02857780456543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [4:25:43<12:26:00, 304.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3741.4078316066575
INFO:root:current train perplexity4.425030708312988
INFO:root:current mean train loss 3747.017651565676
INFO:root:current train perplexity4.388181686401367
INFO:root:current mean train loss 3737.5116891903726
INFO:root:current train perplexity4.3769683837890625
INFO:root:current mean train loss 3746.220344094669
INFO:root:current train perplexity4.380749702453613
INFO:root:current mean train loss 3740.6804799469933
INFO:root:current train perplexity4.377912521362305
INFO:root:current mean train loss 3747.068497083383
INFO:root:current train perplexity4.384632587432861
INFO:root:current mean train loss 3749.5637305001005
INFO:root:current train perplexity4.384548664093018
INFO:root:current mean train loss 3744.711264033865
INFO:root:current train perplexity4.377877235412598
INFO:root:current mean train loss 3747.2645713092347
INFO:root:current train perplexity4.38120174407959
INFO:root:current mean train loss 3749.7587168518926
INFO:root:current train perplexity4.383279323577881


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.61s/it]
INFO:root:final mean train loss: 3744.1641727570563
INFO:root:final train perplexity: 4.380580902099609
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it]
INFO:root:eval mean loss: 3995.521680033799
INFO:root:eval perplexity: 5.031265735626221
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [4:30:29<12:07:18, 298.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3717.248598160282
INFO:root:current train perplexity4.370807647705078
INFO:root:current mean train loss 3723.6573756560115
INFO:root:current train perplexity4.3450422286987305
INFO:root:current mean train loss 3735.7838192894346
INFO:root:current train perplexity4.360725402832031
INFO:root:current mean train loss 3727.6890083612634
INFO:root:current train perplexity4.353436470031738
INFO:root:current mean train loss 3730.8393939874563
INFO:root:current train perplexity4.359791278839111
INFO:root:current mean train loss 3733.248368717632
INFO:root:current train perplexity4.3651652336120605
INFO:root:current mean train loss 3732.015135558018
INFO:root:current train perplexity4.367305755615234
INFO:root:current mean train loss 3733.3344957009876
INFO:root:current train perplexity4.366786956787109
INFO:root:current mean train loss 3734.5024464007033
INFO:root:current train perplexity4.366373538970947
INFO:root:current mean train loss 3736.506519157828
INFO:root:current train perplexity4.365388870239258


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.24s/it]
INFO:root:final mean train loss: 3735.4260821803923
INFO:root:final train perplexity: 4.365504264831543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 3994.4975724457004
INFO:root:eval perplexity: 5.0291829109191895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [4:35:10<11:49:10, 293.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3687.3431865985576
INFO:root:current train perplexity4.303696155548096
INFO:root:current mean train loss 3726.8263650798112
INFO:root:current train perplexity4.333763122558594
INFO:root:current mean train loss 3728.7890665860355
INFO:root:current train perplexity4.3379364013671875
INFO:root:current mean train loss 3722.79746842736
INFO:root:current train perplexity4.329832077026367
INFO:root:current mean train loss 3718.6372576389876
INFO:root:current train perplexity4.3288750648498535
INFO:root:current mean train loss 3717.0030601374074
INFO:root:current train perplexity4.335430145263672
INFO:root:current mean train loss 3718.752447901384
INFO:root:current train perplexity4.338579177856445
INFO:root:current mean train loss 3722.4244340166188
INFO:root:current train perplexity4.345410346984863
INFO:root:current mean train loss 3725.3544726911687
INFO:root:current train perplexity4.347914695739746
INFO:root:current mean train loss 3729.324987831969
INFO:root:current train perplexity4.350862979888916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.87s/it]
INFO:root:final mean train loss: 3727.0679259761687
INFO:root:final train perplexity: 4.351132869720459
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.42s/it]
INFO:root:eval mean loss: 3990.2762269365026
INFO:root:eval perplexity: 5.020604133605957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [4:39:48<11:33:19, 288.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3722.4574727809177
INFO:root:current train perplexity4.335180759429932
INFO:root:current mean train loss 3705.730339205995
INFO:root:current train perplexity4.313933849334717
INFO:root:current mean train loss 3712.816082047065
INFO:root:current train perplexity4.31843376159668
INFO:root:current mean train loss 3715.931257176468
INFO:root:current train perplexity4.322556018829346
INFO:root:current mean train loss 3714.5077840988533
INFO:root:current train perplexity4.328368663787842
INFO:root:current mean train loss 3717.064932479719
INFO:root:current train perplexity4.329940319061279
INFO:root:current mean train loss 3722.322778810858
INFO:root:current train perplexity4.334455966949463
INFO:root:current mean train loss 3722.4434665746317
INFO:root:current train perplexity4.336151599884033
INFO:root:current mean train loss 3723.2702074647655
INFO:root:current train perplexity4.336907863616943
INFO:root:current mean train loss 3723.341536234903
INFO:root:current train perplexity4.337360382080078


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.02s/it]
INFO:root:final mean train loss: 3719.094507832681
INFO:root:final train perplexity: 4.337466239929199
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it]
INFO:root:eval mean loss: 3992.0010562112147
INFO:root:eval perplexity: 5.024107933044434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [4:44:27<11:21:05, 285.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3669.9884499289774
INFO:root:current train perplexity4.248848915100098
INFO:root:current mean train loss 3688.049713331653
INFO:root:current train perplexity4.289281845092773
INFO:root:current mean train loss 3696.04568493413
INFO:root:current train perplexity4.291576862335205
INFO:root:current mean train loss 3694.138531580106
INFO:root:current train perplexity4.291600704193115
INFO:root:current mean train loss 3705.946438229739
INFO:root:current train perplexity4.305356979370117
INFO:root:current mean train loss 3706.004469752956
INFO:root:current train perplexity4.309476852416992
INFO:root:current mean train loss 3707.0668885675095
INFO:root:current train perplexity4.3144121170043945
INFO:root:current mean train loss 3707.853159923427
INFO:root:current train perplexity4.319502353668213
INFO:root:current mean train loss 3709.4757112915754
INFO:root:current train perplexity4.319162845611572
INFO:root:current mean train loss 3711.7283584035504
INFO:root:current train perplexity4.320414066314697


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.13s/it]
INFO:root:final mean train loss: 3709.70543067686
INFO:root:final train perplexity: 4.3214287757873535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.56s/it]
INFO:root:eval mean loss: 3992.6393592780364
INFO:root:eval perplexity: 5.0254058837890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [4:49:05<11:11:15, 283.63s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3699.8653816344245
INFO:root:current train perplexity4.309220314025879
INFO:root:current mean train loss 3681.162377480349
INFO:root:current train perplexity4.2682366371154785
INFO:root:current mean train loss 3689.338416037904
INFO:root:current train perplexity4.275039196014404
INFO:root:current mean train loss 3681.8076548510676
INFO:root:current train perplexity4.276412487030029
INFO:root:current mean train loss 3688.025662712608
INFO:root:current train perplexity4.284786701202393
INFO:root:current mean train loss 3691.1774377636543
INFO:root:current train perplexity4.287707805633545
INFO:root:current mean train loss 3694.034335819664
INFO:root:current train perplexity4.291902542114258
INFO:root:current mean train loss 3695.5848271881146
INFO:root:current train perplexity4.297726154327393
INFO:root:current mean train loss 3697.8116945905454
INFO:root:current train perplexity4.302334308624268
INFO:root:current mean train loss 3702.7085224599236
INFO:root:current train perplexity4.306443691253662


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.14s/it]
INFO:root:final mean train loss: 3701.7871652418567
INFO:root:final train perplexity: 4.307950496673584
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it]
INFO:root:eval mean loss: 3989.104997783688
INFO:root:eval perplexity: 5.018228054046631
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [4:53:44<11:03:11, 282.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3688.180017605634
INFO:root:current train perplexity4.269961357116699
INFO:root:current mean train loss 3673.2304430509867
INFO:root:current train perplexity4.250967502593994
INFO:root:current mean train loss 3687.535429219038
INFO:root:current train perplexity4.2668914794921875
INFO:root:current mean train loss 3692.3306749863123
INFO:root:current train perplexity4.279443740844727
INFO:root:current mean train loss 3691.5340055234874
INFO:root:current train perplexity4.288015365600586
INFO:root:current mean train loss 3692.8325259447515
INFO:root:current train perplexity4.291295528411865
INFO:root:current mean train loss 3693.000945271749
INFO:root:current train perplexity4.29180908203125
INFO:root:current mean train loss 3695.3349805700796
INFO:root:current train perplexity4.2915802001953125
INFO:root:current mean train loss 3695.5400544789572
INFO:root:current train perplexity4.2926435470581055
INFO:root:current mean train loss 3695.3379155167836
INFO:root:current train perplexity4.293259620666504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.41s/it]
INFO:root:final mean train loss: 3693.7554008729994
INFO:root:final train perplexity: 4.294320583343506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it]
INFO:root:eval mean loss: 3990.0633293855276
INFO:root:eval perplexity: 5.0201735496521
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [4:58:22<10:55:40, 281.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3679.2055416831486
INFO:root:current train perplexity4.2749104499816895
INFO:root:current mean train loss 3692.581826662884
INFO:root:current train perplexity4.28400993347168
INFO:root:current mean train loss 3687.7894527749777
INFO:root:current train perplexity4.283779621124268
INFO:root:current mean train loss 3678.09056973017
INFO:root:current train perplexity4.276978015899658
INFO:root:current mean train loss 3678.2407685281837
INFO:root:current train perplexity4.272432327270508
INFO:root:current mean train loss 3681.9071249426543
INFO:root:current train perplexity4.277435302734375
INFO:root:current mean train loss 3680.313271254257
INFO:root:current train perplexity4.273921489715576
INFO:root:current mean train loss 3682.990961155628
INFO:root:current train perplexity4.276548385620117
INFO:root:current mean train loss 3684.700425287969
INFO:root:current train perplexity4.2764387130737305
INFO:root:current mean train loss 3689.365230883714
INFO:root:current train perplexity4.281210899353027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.45s/it]
INFO:root:final mean train loss: 3686.221800035046
INFO:root:final train perplexity: 4.281576633453369
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.51s/it]
INFO:root:eval mean loss: 3991.299946669991
INFO:root:eval perplexity: 5.022684574127197
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [5:03:03<10:50:53, 280.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3665.132127783764
INFO:root:current train perplexity4.225452423095703
INFO:root:current mean train loss 3674.2641614618146
INFO:root:current train perplexity4.254270553588867
INFO:root:current mean train loss 3679.2683717946975
INFO:root:current train perplexity4.260836124420166
INFO:root:current mean train loss 3681.712815553335
INFO:root:current train perplexity4.264262676239014
INFO:root:current mean train loss 3682.439648638026
INFO:root:current train perplexity4.267082214355469
INFO:root:current mean train loss 3682.443021238155
INFO:root:current train perplexity4.265224456787109
INFO:root:current mean train loss 3681.4676561647107
INFO:root:current train perplexity4.2651567459106445
INFO:root:current mean train loss 3679.4724080765563
INFO:root:current train perplexity4.266201019287109
INFO:root:current mean train loss 3678.088314499366
INFO:root:current train perplexity4.264451503753662
INFO:root:current mean train loss 3679.861837431532
INFO:root:current train perplexity4.2659993171691895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.56s/it]
INFO:root:final mean train loss: 3676.9613496103593
INFO:root:final train perplexity: 4.265961647033691
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 3990.40776505707
INFO:root:eval perplexity: 5.020873069763184
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [5:07:41<10:44:12, 280.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3642.4168739720394
INFO:root:current train perplexity4.214443206787109
INFO:root:current mean train loss 3656.5397448417466
INFO:root:current train perplexity4.233327388763428
INFO:root:current mean train loss 3662.8296676377117
INFO:root:current train perplexity4.238992691040039
INFO:root:current mean train loss 3664.6795045490508
INFO:root:current train perplexity4.2405595779418945
INFO:root:current mean train loss 3669.349622198548
INFO:root:current train perplexity4.241918087005615
INFO:root:current mean train loss 3670.833316373424
INFO:root:current train perplexity4.248378753662109
INFO:root:current mean train loss 3670.6997354850496
INFO:root:current train perplexity4.248159408569336
INFO:root:current mean train loss 3672.787782527516
INFO:root:current train perplexity4.254649639129639
INFO:root:current mean train loss 3672.587143472853
INFO:root:current train perplexity4.253847599029541


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.86s/it]
INFO:root:final mean train loss: 3669.423462283227
INFO:root:final train perplexity: 4.253294467926025
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.60s/it]
INFO:root:eval mean loss: 3991.1535506011746
INFO:root:eval perplexity: 5.0223870277404785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [5:12:23<10:40:27, 280.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3672.28857421875
INFO:root:current train perplexity4.281792163848877
INFO:root:current mean train loss 3669.5812703845572
INFO:root:current train perplexity4.237166404724121
INFO:root:current mean train loss 3655.594706117226
INFO:root:current train perplexity4.225376129150391
INFO:root:current mean train loss 3656.2717510764746
INFO:root:current train perplexity4.235296249389648
INFO:root:current mean train loss 3663.9559002064593
INFO:root:current train perplexity4.23979377746582
INFO:root:current mean train loss 3667.7534407810945
INFO:root:current train perplexity4.236577033996582
INFO:root:current mean train loss 3668.404769770937
INFO:root:current train perplexity4.236910343170166
INFO:root:current mean train loss 3665.3280222039475
INFO:root:current train perplexity4.234987258911133
INFO:root:current mean train loss 3666.2487668314134
INFO:root:current train perplexity4.23738956451416
INFO:root:current mean train loss 3666.8557031574437
INFO:root:current train perplexity4.239418983459473


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.17s/it]
INFO:root:final mean train loss: 3663.1387494610203
INFO:root:final train perplexity: 4.242761135101318
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.55s/it]
INFO:root:eval mean loss: 3987.754877618019
INFO:root:eval perplexity: 5.01548957824707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [5:17:01<10:34:31, 279.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3637.878750887784
INFO:root:current train perplexity4.209001064300537
INFO:root:current mean train loss 3669.6007706925675
INFO:root:current train perplexity4.225753307342529
INFO:root:current mean train loss 3645.6949792654027
INFO:root:current train perplexity4.215147495269775
INFO:root:current mean train loss 3634.343822221664
INFO:root:current train perplexity4.208052635192871
INFO:root:current mean train loss 3641.1043721962437
INFO:root:current train perplexity4.209437370300293
INFO:root:current mean train loss 3647.6586402848275
INFO:root:current train perplexity4.215249538421631
INFO:root:current mean train loss 3651.997028756649
INFO:root:current train perplexity4.221028804779053
INFO:root:current mean train loss 3654.199698790216
INFO:root:current train perplexity4.2248077392578125
INFO:root:current mean train loss 3658.3108238286068
INFO:root:current train perplexity4.228294372558594
INFO:root:current mean train loss 3656.458847699129
INFO:root:current train perplexity4.227197170257568


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.04s/it]
INFO:root:final mean train loss: 3654.4204452883814
INFO:root:final train perplexity: 4.228193283081055
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.32s/it]
INFO:root:eval mean loss: 3985.72972940215
INFO:root:eval perplexity: 5.011384010314941
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/65
################best#########
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [5:21:40<10:28:44, 279.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3586.973748458059
INFO:root:current train perplexity4.186627388000488
INFO:root:current mean train loss 3638.8605936515232
INFO:root:current train perplexity4.19378662109375
INFO:root:current mean train loss 3639.8214005422374
INFO:root:current train perplexity4.19401216506958
INFO:root:current mean train loss 3641.920885426871
INFO:root:current train perplexity4.194908142089844
INFO:root:current mean train loss 3644.6623406967856
INFO:root:current train perplexity4.201620101928711
INFO:root:current mean train loss 3640.965997655497
INFO:root:current train perplexity4.196043968200684
INFO:root:current mean train loss 3646.238297026454
INFO:root:current train perplexity4.203454971313477
INFO:root:current mean train loss 3645.442305393776
INFO:root:current train perplexity4.207418441772461
INFO:root:current mean train loss 3644.3839187342605
INFO:root:current train perplexity4.204718112945557
INFO:root:current mean train loss 3645.113287094498
INFO:root:current train perplexity4.209766864776611


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.23s/it]
INFO:root:final mean train loss: 3645.0238697913387
INFO:root:final train perplexity: 4.212546348571777
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.24s/it]
INFO:root:eval mean loss: 3987.973866564162
INFO:root:eval perplexity: 5.015933513641357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [5:26:18<10:23:22, 279.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3601.9872685185187
INFO:root:current train perplexity4.104031562805176
INFO:root:current mean train loss 3620.565479669045
INFO:root:current train perplexity4.16394567489624
INFO:root:current mean train loss 3636.356551787927
INFO:root:current train perplexity4.194450855255127
INFO:root:current mean train loss 3638.800248172305
INFO:root:current train perplexity4.193384647369385
INFO:root:current mean train loss 3641.7700544084823
INFO:root:current train perplexity4.192754745483398
INFO:root:current mean train loss 3637.086010232596
INFO:root:current train perplexity4.190138816833496
INFO:root:current mean train loss 3636.071624707187
INFO:root:current train perplexity4.191911220550537
INFO:root:current mean train loss 3636.1673310291653
INFO:root:current train perplexity4.195801258087158
INFO:root:current mean train loss 3637.189115697268
INFO:root:current train perplexity4.198251724243164
INFO:root:current mean train loss 3640.145684531334
INFO:root:current train perplexity4.200438976287842


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.27s/it]
INFO:root:final mean train loss: 3638.715468991187
INFO:root:final train perplexity: 4.202075481414795
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it]
INFO:root:eval mean loss: 3988.270788314495
INFO:root:eval perplexity: 5.016536712646484
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [5:30:59<10:19:49, 279.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3605.368582589286
INFO:root:current train perplexity4.147712230682373
INFO:root:current mean train loss 3603.116440610532
INFO:root:current train perplexity4.15480375289917
INFO:root:current mean train loss 3614.152118309508
INFO:root:current train perplexity4.166409015655518
INFO:root:current mean train loss 3623.4185350104945
INFO:root:current train perplexity4.172752380371094
INFO:root:current mean train loss 3623.571953573994
INFO:root:current train perplexity4.173737049102783
INFO:root:current mean train loss 3631.624046254381
INFO:root:current train perplexity4.1807169914245605
INFO:root:current mean train loss 3632.3252318374753
INFO:root:current train perplexity4.178953647613525
INFO:root:current mean train loss 3633.7228914221937
INFO:root:current train perplexity4.18440580368042
INFO:root:current mean train loss 3635.2873628719126
INFO:root:current train perplexity4.188290119171143
INFO:root:current mean train loss 3637.1098264643215
INFO:root:current train perplexity4.190628528594971


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.26s/it]
INFO:root:final mean train loss: 3632.1978152490433
INFO:root:final train perplexity: 4.191284656524658
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.28s/it]
INFO:root:eval mean loss: 3987.3869334552305
INFO:root:eval perplexity: 5.014743804931641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [5:35:55<10:26:25, 284.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3619.9757903343025
INFO:root:current train perplexity4.161243915557861
INFO:root:current mean train loss 3630.984200857736
INFO:root:current train perplexity4.175959587097168
INFO:root:current mean train loss 3632.0916783211164
INFO:root:current train perplexity4.1797027587890625
INFO:root:current mean train loss 3628.594965008883
INFO:root:current train perplexity4.172348499298096
INFO:root:current mean train loss 3621.5882488448788
INFO:root:current train perplexity4.166125297546387
INFO:root:current mean train loss 3624.8858316607675
INFO:root:current train perplexity4.1704864501953125
INFO:root:current mean train loss 3628.2590783862265
INFO:root:current train perplexity4.173165798187256
INFO:root:current mean train loss 3629.1426750583573
INFO:root:current train perplexity4.1763224601745605
INFO:root:current mean train loss 3631.1683044071583
INFO:root:current train perplexity4.180179119110107
INFO:root:current mean train loss 3628.329972494698
INFO:root:current train perplexity4.1782026290893555


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:35<00:00, 275.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:35<00:00, 275.43s/it]
INFO:root:final mean train loss: 3625.1851860784714
INFO:root:final train perplexity: 4.179704189300537
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.55s/it]
INFO:root:eval mean loss: 3987.881439425421
INFO:root:eval perplexity: 5.015746593475342
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [5:41:20<10:47:46, 296.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3602.613223805147
INFO:root:current train perplexity4.145341396331787
INFO:root:current mean train loss 3597.059306769971
INFO:root:current train perplexity4.147150993347168
INFO:root:current mean train loss 3603.810162669634
INFO:root:current train perplexity4.14802360534668
INFO:root:current mean train loss 3613.998370309161
INFO:root:current train perplexity4.161121368408203
INFO:root:current mean train loss 3606.378132686911
INFO:root:current train perplexity4.157760143280029
INFO:root:current mean train loss 3607.810999709335
INFO:root:current train perplexity4.161494255065918
INFO:root:current mean train loss 3611.2594761064706
INFO:root:current train perplexity4.161980152130127
INFO:root:current mean train loss 3613.5303755149384
INFO:root:current train perplexity4.164590835571289
INFO:root:current mean train loss 3616.972029689336
INFO:root:current train perplexity4.164980411529541
INFO:root:current mean train loss 3620.485184694565
INFO:root:current train perplexity4.1676506996154785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:35<00:00, 275.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:35<00:00, 275.11s/it]
INFO:root:final mean train loss: 3618.331605726673
INFO:root:final train perplexity: 4.168417453765869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.49s/it]
INFO:root:eval mean loss: 3987.091415946365
INFO:root:eval perplexity: 5.014143943786621
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [5:46:46<11:01:56, 305.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3580.65188029661
INFO:root:current train perplexity4.153050422668457
INFO:root:current mean train loss 3584.6519783068
INFO:root:current train perplexity4.138540744781494
INFO:root:current mean train loss 3600.9375263935813
INFO:root:current train perplexity4.147821426391602
INFO:root:current mean train loss 3599.6886969011143
INFO:root:current train perplexity4.14773416519165
INFO:root:current mean train loss 3602.3175679125816
INFO:root:current train perplexity4.144659996032715
INFO:root:current mean train loss 3603.3063414544667
INFO:root:current train perplexity4.146358966827393
INFO:root:current mean train loss 3604.8438526205664
INFO:root:current train perplexity4.147497177124023
INFO:root:current mean train loss 3605.5279458992095
INFO:root:current train perplexity4.150376319885254
INFO:root:current mean train loss 3607.924788487249
INFO:root:current train perplexity4.152210235595703
INFO:root:current mean train loss 3613.038963978184
INFO:root:current train perplexity4.154529571533203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.80s/it]
INFO:root:final mean train loss: 3611.2148943255024
INFO:root:final train perplexity: 4.1567301750183105
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it]
INFO:root:eval mean loss: 3988.599692486702
INFO:root:eval perplexity: 5.017202854156494
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [5:52:07<11:07:05, 310.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3581.7004503847947
INFO:root:current train perplexity4.093727111816406
INFO:root:current mean train loss 3581.2129593352356
INFO:root:current train perplexity4.1013946533203125
INFO:root:current mean train loss 3582.5184824511352
INFO:root:current train perplexity4.11349630355835
INFO:root:current mean train loss 3594.818997333745
INFO:root:current train perplexity4.124825477600098
INFO:root:current mean train loss 3594.929223266863
INFO:root:current train perplexity4.124432563781738
INFO:root:current mean train loss 3591.145317064181
INFO:root:current train perplexity4.126811504364014
INFO:root:current mean train loss 3596.4838570704883
INFO:root:current train perplexity4.129266262054443
INFO:root:current mean train loss 3599.5184919812377
INFO:root:current train perplexity4.1347527503967285
INFO:root:current mean train loss 3599.230708666739
INFO:root:current train perplexity4.13609504699707
INFO:root:current mean train loss 3605.329180333829
INFO:root:current train perplexity4.143082141876221


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.18s/it]
INFO:root:final mean train loss: 3602.7419410213347
INFO:root:final train perplexity: 4.142858028411865
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.65s/it]
INFO:root:eval mean loss: 3991.3748164616577
INFO:root:eval perplexity: 5.022836685180664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [5:57:10<10:57:13, 308.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3613.466031901042
INFO:root:current train perplexity4.117778301239014
INFO:root:current mean train loss 3591.076556919643
INFO:root:current train perplexity4.1078925132751465
INFO:root:current mean train loss 3595.196854580966
INFO:root:current train perplexity4.114455223083496
INFO:root:current mean train loss 3592.389576171875
INFO:root:current train perplexity4.121705055236816
INFO:root:current mean train loss 3596.2825853207237
INFO:root:current train perplexity4.128298759460449
INFO:root:current mean train loss 3596.779154636549
INFO:root:current train perplexity4.126986980438232
INFO:root:current mean train loss 3596.469443359375
INFO:root:current train perplexity4.127284049987793
INFO:root:current mean train loss 3598.2852551663304
INFO:root:current train perplexity4.131633281707764
INFO:root:current mean train loss 3600.0841593191963
INFO:root:current train perplexity4.132689952850342
INFO:root:current mean train loss 3600.383842648237
INFO:root:current train perplexity4.134174823760986


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.90s/it]
INFO:root:final mean train loss: 3596.941748280679
INFO:root:final train perplexity: 4.133388519287109
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it]
INFO:root:eval mean loss: 3990.096762799202
INFO:root:eval perplexity: 5.0202412605285645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [6:02:15<10:50:10, 307.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3556.2001894295936
INFO:root:current train perplexity4.084452152252197
INFO:root:current mean train loss 3575.0413531527493
INFO:root:current train perplexity4.102261543273926
INFO:root:current mean train loss 3584.6952728163646
INFO:root:current train perplexity4.112753391265869
INFO:root:current mean train loss 3584.1414262657067
INFO:root:current train perplexity4.109018802642822
INFO:root:current mean train loss 3585.8526836261
INFO:root:current train perplexity4.115001678466797
INFO:root:current mean train loss 3586.0809960602487
INFO:root:current train perplexity4.117938041687012
INFO:root:current mean train loss 3588.5806725698894
INFO:root:current train perplexity4.119248867034912
INFO:root:current mean train loss 3588.2083277209053
INFO:root:current train perplexity4.1172990798950195
INFO:root:current mean train loss 3590.575774558943
INFO:root:current train perplexity4.118870735168457
INFO:root:current mean train loss 3591.408479801151
INFO:root:current train perplexity4.120710849761963


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.85s/it]
INFO:root:final mean train loss: 3589.3290573858444
INFO:root:final train perplexity: 4.120993137359619
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.95s/it]
INFO:root:eval mean loss: 3989.820386954233
INFO:root:eval perplexity: 5.019680500030518
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [6:07:36<10:53:23, 311.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3594.1185772235576
INFO:root:current train perplexity4.112062454223633
INFO:root:current mean train loss 3575.0185738608475
INFO:root:current train perplexity4.089961528778076
INFO:root:current mean train loss 3581.3780530162694
INFO:root:current train perplexity4.097240924835205
INFO:root:current mean train loss 3576.8067673783166
INFO:root:current train perplexity4.0964460372924805
INFO:root:current mean train loss 3576.5806270684825
INFO:root:current train perplexity4.096302032470703
INFO:root:current mean train loss 3578.083116870241
INFO:root:current train perplexity4.094322681427002
INFO:root:current mean train loss 3579.5138750310916
INFO:root:current train perplexity4.1004180908203125
INFO:root:current mean train loss 3581.594178094876
INFO:root:current train perplexity4.105635166168213
INFO:root:current mean train loss 3582.815275969329
INFO:root:current train perplexity4.107766628265381
INFO:root:current mean train loss 3586.1069180732056
INFO:root:current train perplexity4.111238956451416


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.08s/it]
INFO:root:final mean train loss: 3583.317693894909
INFO:root:final train perplexity: 4.111230373382568
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it]
INFO:root:eval mean loss: 3987.60570596465
INFO:root:eval perplexity: 5.015186786651611
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [6:12:50<10:50:03, 312.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3563.0311686197915
INFO:root:current train perplexity4.080587387084961
INFO:root:current mean train loss 3574.6116802273086
INFO:root:current train perplexity4.083341121673584
INFO:root:current mean train loss 3573.847517440949
INFO:root:current train perplexity4.0896992683410645
INFO:root:current mean train loss 3573.3333106937266
INFO:root:current train perplexity4.091701984405518
INFO:root:current mean train loss 3576.770977012619
INFO:root:current train perplexity4.093223571777344
INFO:root:current mean train loss 3576.7227292068815
INFO:root:current train perplexity4.090362548828125
INFO:root:current mean train loss 3580.17388540549
INFO:root:current train perplexity4.098241329193115
INFO:root:current mean train loss 3578.0634062842223
INFO:root:current train perplexity4.097939491271973
INFO:root:current mean train loss 3578.444816071677
INFO:root:current train perplexity4.099968910217285


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.32s/it]
INFO:root:final mean train loss: 3576.880263420843
INFO:root:final train perplexity: 4.100803375244141
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.35s/it]
INFO:root:eval mean loss: 3990.5554060699246
INFO:root:eval perplexity: 5.021173477172852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [6:18:06<10:47:37, 313.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3553.922607421875
INFO:root:current train perplexity4.066295146942139
INFO:root:current mean train loss 3568.2255973459405
INFO:root:current train perplexity4.08891487121582
INFO:root:current mean train loss 3562.4460083597523
INFO:root:current train perplexity4.078546524047852
INFO:root:current mean train loss 3561.812223254275
INFO:root:current train perplexity4.072652339935303
INFO:root:current mean train loss 3564.7292204535856
INFO:root:current train perplexity4.075658321380615
INFO:root:current mean train loss 3565.030306182199
INFO:root:current train perplexity4.076724529266357
INFO:root:current mean train loss 3570.276633851807
INFO:root:current train perplexity4.082756042480469
INFO:root:current mean train loss 3571.0631992712383
INFO:root:current train perplexity4.085341930389404
INFO:root:current mean train loss 3571.3402113223165
INFO:root:current train perplexity4.085829257965088
INFO:root:current mean train loss 3571.1356412473297
INFO:root:current train perplexity4.08678674697876


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.08s/it]
INFO:root:final mean train loss: 3569.705037824569
INFO:root:final train perplexity: 4.0892109870910645
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it]
INFO:root:eval mean loss: 3989.5627908909573
INFO:root:eval perplexity: 5.019157886505127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [6:22:54<10:26:47, 305.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3508.5352376302085
INFO:root:current train perplexity3.9875011444091797
INFO:root:current mean train loss 3551.3958092730977
INFO:root:current train perplexity4.04371452331543
INFO:root:current mean train loss 3557.636579078852
INFO:root:current train perplexity4.052317142486572
INFO:root:current mean train loss 3554.2291806175595
INFO:root:current train perplexity4.053969383239746
INFO:root:current mean train loss 3559.313872482116
INFO:root:current train perplexity4.064390182495117
INFO:root:current mean train loss 3561.451450621966
INFO:root:current train perplexity4.069021701812744
INFO:root:current mean train loss 3557.377242917937
INFO:root:current train perplexity4.064949989318848
INFO:root:current mean train loss 3561.925876174607
INFO:root:current train perplexity4.071830749511719
INFO:root:current mean train loss 3562.996490965299
INFO:root:current train perplexity4.074400901794434
INFO:root:current mean train loss 3564.031612608863
INFO:root:current train perplexity4.075323581695557


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.63s/it]
INFO:root:final mean train loss: 3563.0597248692666
INFO:root:final train perplexity: 4.078503608703613
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.32s/it]
INFO:root:eval mean loss: 3991.3120359596633
INFO:root:eval perplexity: 5.022708892822266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [6:28:21<10:34:28, 312.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3560.8569972826085
INFO:root:current train perplexity4.075193881988525
INFO:root:current mean train loss 3554.4443121189024
INFO:root:current train perplexity4.071813106536865
INFO:root:current mean train loss 3551.0327630149945
INFO:root:current train perplexity4.0705647468566895
INFO:root:current mean train loss 3554.343353932856
INFO:root:current train perplexity4.0662360191345215
INFO:root:current mean train loss 3552.4171814974884
INFO:root:current train perplexity4.063272476196289
INFO:root:current mean train loss 3550.760784200227
INFO:root:current train perplexity4.063794136047363
INFO:root:current mean train loss 3555.7898760408307
INFO:root:current train perplexity4.0663323402404785
INFO:root:current mean train loss 3556.1368163387147
INFO:root:current train perplexity4.065338611602783
INFO:root:current mean train loss 3558.2341569643263
INFO:root:current train perplexity4.065918445587158
INFO:root:current mean train loss 3559.164073873832
INFO:root:current train perplexity4.070178031921387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.56s/it]
INFO:root:final mean train loss: 3558.511360476094
INFO:root:final train perplexity: 4.071191787719727
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.53s/it]
INFO:root:eval mean loss: 3991.853861923759
INFO:root:eval perplexity: 5.02380895614624
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [6:33:54<10:42:08, 318.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3544.2415732106856
INFO:root:current train perplexity4.033326625823975
INFO:root:current mean train loss 3545.411813051646
INFO:root:current train perplexity4.042741775512695
INFO:root:current mean train loss 3551.489668941085
INFO:root:current train perplexity4.045788288116455
INFO:root:current mean train loss 3556.092088221063
INFO:root:current train perplexity4.0492939949035645
INFO:root:current mean train loss 3555.279542148528
INFO:root:current train perplexity4.056228160858154
INFO:root:current mean train loss 3554.4159241297375
INFO:root:current train perplexity4.058758735656738
INFO:root:current mean train loss 3554.140635059677
INFO:root:current train perplexity4.058250427246094
INFO:root:current mean train loss 3553.1848912689165
INFO:root:current train perplexity4.05730676651001
INFO:root:current mean train loss 3556.6679273254176
INFO:root:current train perplexity4.0621185302734375
INFO:root:current mean train loss 3553.1602899897625
INFO:root:current train perplexity4.060182571411133


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:33<00:00, 273.92s/it]
INFO:root:final mean train loss: 3551.099342592301
INFO:root:final train perplexity: 4.059303283691406
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.74s/it]
INFO:root:eval mean loss: 3992.094591505984
INFO:root:eval perplexity: 5.024299144744873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [6:38:49<10:22:33, 311.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3506.16744916867
INFO:root:current train perplexity4.048344612121582
INFO:root:current mean train loss 3526.149324485724
INFO:root:current train perplexity4.024484634399414
INFO:root:current mean train loss 3529.3784588291055
INFO:root:current train perplexity4.025570392608643
INFO:root:current mean train loss 3533.520082186809
INFO:root:current train perplexity4.023824691772461
INFO:root:current mean train loss 3535.5775021355353
INFO:root:current train perplexity4.03470516204834
INFO:root:current mean train loss 3535.5839590097403
INFO:root:current train perplexity4.036341190338135
INFO:root:current mean train loss 3542.469780051839
INFO:root:current train perplexity4.042771816253662
INFO:root:current mean train loss 3545.340358460546
INFO:root:current train perplexity4.048491954803467
INFO:root:current mean train loss 3546.4814735385316
INFO:root:current train perplexity4.049422740936279
INFO:root:current mean train loss 3546.652853871306
INFO:root:current train perplexity4.0493245124816895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.70s/it]
INFO:root:final mean train loss: 3546.120181114443
INFO:root:final train perplexity: 4.051337242126465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.07s/it]
INFO:root:eval mean loss: 3991.495572570368
INFO:root:eval perplexity: 5.023081302642822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [6:43:34<10:01:35, 303.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3547.630791846742
INFO:root:current train perplexity4.077612400054932
INFO:root:current mean train loss 3555.756370907738
INFO:root:current train perplexity4.053367614746094
INFO:root:current mean train loss 3537.483924278846
INFO:root:current train perplexity4.0337371826171875
INFO:root:current mean train loss 3534.805467764995
INFO:root:current train perplexity4.029452323913574
INFO:root:current mean train loss 3541.87520754684
INFO:root:current train perplexity4.031939506530762
INFO:root:current mean train loss 3536.2166143274394
INFO:root:current train perplexity4.031919002532959
INFO:root:current mean train loss 3539.040722807187
INFO:root:current train perplexity4.035271644592285
INFO:root:current mean train loss 3541.896771330112
INFO:root:current train perplexity4.035098075866699
INFO:root:current mean train loss 3544.382084113507
INFO:root:current train perplexity4.038304805755615
INFO:root:current mean train loss 3541.7199722499504
INFO:root:current train perplexity4.0404229164123535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.28s/it]
INFO:root:final mean train loss: 3538.4704610147783
INFO:root:final train perplexity: 4.039128303527832
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.88s/it]
INFO:root:eval mean loss: 3993.239538314495
INFO:root:eval perplexity: 5.026625633239746
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [6:48:19<9:45:47, 297.86s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3539.140318714489
INFO:root:current train perplexity4.028894424438477
INFO:root:current mean train loss 3530.406568170363
INFO:root:current train perplexity4.022999286651611
INFO:root:current mean train loss 3525.558145680147
INFO:root:current train perplexity4.0192484855651855
INFO:root:current mean train loss 3529.9959582691463
INFO:root:current train perplexity4.027568340301514
INFO:root:current mean train loss 3526.9850526914493
INFO:root:current train perplexity4.030663013458252
INFO:root:current mean train loss 3529.8937935494087
INFO:root:current train perplexity4.031148910522461
INFO:root:current mean train loss 3531.238306595897
INFO:root:current train perplexity4.030917644500732
INFO:root:current mean train loss 3534.135517125414
INFO:root:current train perplexity4.031681537628174
INFO:root:current mean train loss 3534.9490182976974
INFO:root:current train perplexity4.030521869659424
INFO:root:current mean train loss 3537.9994715825424
INFO:root:current train perplexity4.032852649688721


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.00s/it]
INFO:root:final mean train loss: 3533.419181454566
INFO:root:final train perplexity: 4.0310869216918945
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.00s/it]
INFO:root:eval mean loss: 3994.0338905280364
INFO:root:eval perplexity: 5.028239727020264
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [6:53:10<9:36:55, 295.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3524.8563329303074
INFO:root:current train perplexity3.984966516494751
INFO:root:current mean train loss 3525.042150953796
INFO:root:current train perplexity4.004047870635986
INFO:root:current mean train loss 3516.6307071352185
INFO:root:current train perplexity4.011372089385986
INFO:root:current mean train loss 3517.1470876646435
INFO:root:current train perplexity4.013597011566162
INFO:root:current mean train loss 3521.7165764629453
INFO:root:current train perplexity4.013860702514648
INFO:root:current mean train loss 3524.2251124000886
INFO:root:current train perplexity4.017714023590088
INFO:root:current mean train loss 3528.801572589673
INFO:root:current train perplexity4.0219316482543945
INFO:root:current mean train loss 3531.8266844743202
INFO:root:current train perplexity4.025269985198975
INFO:root:current mean train loss 3531.2334932081944
INFO:root:current train perplexity4.023760795593262
INFO:root:current mean train loss 3531.0020590967356
INFO:root:current train perplexity4.023664474487305


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.06s/it]
INFO:root:final mean train loss: 3528.5996312787456
INFO:root:final train perplexity: 4.023428916931152
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:20<00:00, 20.06s/it]
INFO:root:eval mean loss: 3994.5361103030805
INFO:root:eval perplexity: 5.029260635375977
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [6:58:38<9:50:19, 305.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3502.325184996699
INFO:root:current train perplexity4.000524044036865
INFO:root:current mean train loss 3510.6961191634687
INFO:root:current train perplexity3.9960460662841797
INFO:root:current mean train loss 3505.905559018969
INFO:root:current train perplexity3.9933431148529053
INFO:root:current mean train loss 3510.265019583895
INFO:root:current train perplexity4.001906871795654
INFO:root:current mean train loss 3512.521557461684
INFO:root:current train perplexity4.001020908355713
INFO:root:current mean train loss 3517.021075621169
INFO:root:current train perplexity4.0032830238342285
INFO:root:current mean train loss 3521.36316118084
INFO:root:current train perplexity4.007803916931152
INFO:root:current mean train loss 3525.001724500446
INFO:root:current train perplexity4.009324550628662
INFO:root:current mean train loss 3525.6261991200845
INFO:root:current train perplexity4.011807441711426
INFO:root:current mean train loss 3524.333204181015
INFO:root:current train perplexity4.012386322021484


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.95s/it]
INFO:root:final mean train loss: 3521.454672105851
INFO:root:final train perplexity: 4.01210355758667
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it]
INFO:root:eval mean loss: 3995.478016954787
INFO:root:eval perplexity: 5.031177043914795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [7:04:00<9:55:13, 310.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3470.023023388054
INFO:root:current train perplexity3.993403673171997
INFO:root:current mean train loss 3499.850763246334
INFO:root:current train perplexity3.9954164028167725
INFO:root:current mean train loss 3513.9986314124103
INFO:root:current train perplexity3.9993581771850586
INFO:root:current mean train loss 3516.3043138811017
INFO:root:current train perplexity3.995432138442993
INFO:root:current mean train loss 3516.839023661763
INFO:root:current train perplexity3.998009204864502
INFO:root:current mean train loss 3515.4981451215726
INFO:root:current train perplexity3.999809741973877
INFO:root:current mean train loss 3513.803474347616
INFO:root:current train perplexity4.00017786026001
INFO:root:current mean train loss 3516.9421020037707
INFO:root:current train perplexity4.0022101402282715
INFO:root:current mean train loss 3518.4897122084753
INFO:root:current train perplexity4.002326488494873
INFO:root:current mean train loss 3519.3684772807073
INFO:root:current train perplexity4.003057479858398


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.14s/it]
INFO:root:final mean train loss: 3516.156082584012
INFO:root:final train perplexity: 4.003725051879883
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:22<00:00, 22.30s/it]
INFO:root:eval mean loss: 3996.755656790226
INFO:root:eval perplexity: 5.033776760101318
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [7:09:27<9:59:02, 315.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3484.8536671605602
INFO:root:current train perplexity3.9730379581451416
INFO:root:current mean train loss 3490.830068986046
INFO:root:current train perplexity3.974412202835083
INFO:root:current mean train loss 3493.5756597751524
INFO:root:current train perplexity3.987722158432007
INFO:root:current mean train loss 3498.8224870548693
INFO:root:current train perplexity3.9884002208709717
INFO:root:current mean train loss 3501.320182659298
INFO:root:current train perplexity3.9867303371429443
INFO:root:current mean train loss 3507.2413694001543
INFO:root:current train perplexity3.990222454071045
INFO:root:current mean train loss 3507.7418936072827
INFO:root:current train perplexity3.989898681640625
INFO:root:current mean train loss 3510.52345301084
INFO:root:current train perplexity3.9936487674713135
INFO:root:current mean train loss 3511.950501382821
INFO:root:current train perplexity3.994370222091675
INFO:root:current mean train loss 3513.981249653701
INFO:root:current train perplexity3.9958364963531494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.73s/it]
INFO:root:final mean train loss: 3511.353418719384
INFO:root:final train perplexity: 3.9961462020874023
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it]
INFO:root:eval mean loss: 3996.9161610704787
INFO:root:eval perplexity: 5.034104347229004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [7:14:20<9:41:27, 308.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3495.430892783717
INFO:root:current train perplexity3.9542551040649414
INFO:root:current mean train loss 3513.8130020532853
INFO:root:current train perplexity3.9762840270996094
INFO:root:current mean train loss 3511.5011991856463
INFO:root:current train perplexity3.981565237045288
INFO:root:current mean train loss 3502.4557332871836
INFO:root:current train perplexity3.9766011238098145
INFO:root:current mean train loss 3502.0133498066602
INFO:root:current train perplexity3.9761598110198975
INFO:root:current mean train loss 3503.7526395909927
INFO:root:current train perplexity3.974353313446045
INFO:root:current mean train loss 3503.1782138742133
INFO:root:current train perplexity3.9767580032348633
INFO:root:current mean train loss 3504.4404023560337
INFO:root:current train perplexity3.9805092811584473
INFO:root:current mean train loss 3506.1553168099686
INFO:root:current train perplexity3.983743906021118


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.96s/it]
INFO:root:final mean train loss: 3504.738855731103
INFO:root:final train perplexity: 3.985731840133667
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it]
INFO:root:eval mean loss: 3998.236435477615
INFO:root:eval perplexity: 5.036791801452637
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [7:19:45<9:45:37, 313.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3696.15234375
INFO:root:current train perplexity4.079987525939941
INFO:root:current mean train loss 3509.6738944933254
INFO:root:current train perplexity3.9729115962982178
INFO:root:current mean train loss 3497.0543327143628
INFO:root:current train perplexity3.9624500274658203
INFO:root:current mean train loss 3492.8436331670277
INFO:root:current train perplexity3.9579670429229736
INFO:root:current mean train loss 3499.577863896751
INFO:root:current train perplexity3.9706459045410156
INFO:root:current mean train loss 3499.394466210549
INFO:root:current train perplexity3.9730706214904785
INFO:root:current mean train loss 3499.6994082322763
INFO:root:current train perplexity3.9712414741516113
INFO:root:current mean train loss 3496.681667713149
INFO:root:current train perplexity3.9709200859069824
INFO:root:current mean train loss 3499.3221157354255
INFO:root:current train perplexity3.9718942642211914
INFO:root:current mean train loss 3500.5913367118287
INFO:root:current train perplexity3.977118968963623


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.75s/it]
INFO:root:final mean train loss: 3499.935356509301
INFO:root:final train perplexity: 3.9781851768493652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.46s/it]
INFO:root:eval mean loss: 3999.843945658799
INFO:root:eval perplexity: 5.040066719055176
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [7:25:15<9:49:14, 318.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3493.464599609375
INFO:root:current train perplexity3.9167609214782715
INFO:root:current mean train loss 3488.710649370073
INFO:root:current train perplexity3.960357904434204
INFO:root:current mean train loss 3485.9676068202016
INFO:root:current train perplexity3.9590725898742676
INFO:root:current mean train loss 3483.85219836465
INFO:root:current train perplexity3.9592580795288086
INFO:root:current mean train loss 3481.553805386063
INFO:root:current train perplexity3.951777219772339
INFO:root:current mean train loss 3483.6539031922703
INFO:root:current train perplexity3.95320200920105
INFO:root:current mean train loss 3484.870761702767
INFO:root:current train perplexity3.9581661224365234
INFO:root:current mean train loss 3491.0109224601133
INFO:root:current train perplexity3.959937334060669
INFO:root:current mean train loss 3491.2058141593134
INFO:root:current train perplexity3.9619252681732178
INFO:root:current mean train loss 3493.8563212322483
INFO:root:current train perplexity3.965559959411621


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.04s/it]
INFO:root:final mean train loss: 3493.8590941890593
INFO:root:final train perplexity: 3.9686598777770996
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it]
INFO:root:eval mean loss: 4001.9118998642507
INFO:root:eval perplexity: 5.044283390045166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [7:30:39<9:46:54, 320.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3448.7298134251646
INFO:root:current train perplexity3.991915702819824
INFO:root:current mean train loss 3483.172629989496
INFO:root:current train perplexity3.9355459213256836
INFO:root:current mean train loss 3498.257862665882
INFO:root:current train perplexity3.9455313682556152
INFO:root:current mean train loss 3495.0203884208463
INFO:root:current train perplexity3.952153205871582
INFO:root:current mean train loss 3495.4965540628727
INFO:root:current train perplexity3.9510138034820557
INFO:root:current mean train loss 3493.5588016693764
INFO:root:current train perplexity3.951841354370117
INFO:root:current mean train loss 3489.981009093548
INFO:root:current train perplexity3.952296018600464
INFO:root:current mean train loss 3486.242139962187
INFO:root:current train perplexity3.9547119140625
INFO:root:current mean train loss 3487.674791869372
INFO:root:current train perplexity3.9563097953796387
INFO:root:current mean train loss 3491.8307372249897
INFO:root:current train perplexity3.9601686000823975


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:34<00:00, 274.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:34<00:00, 274.64s/it]
INFO:root:final mean train loss: 3488.0663388775242
INFO:root:final train perplexity: 3.9596009254455566
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.69s/it]
INFO:root:eval mean loss: 4001.03919928801
INFO:root:eval perplexity: 5.042503833770752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [7:36:13<9:49:18, 324.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3467.2953830295137
INFO:root:current train perplexity3.961805820465088
INFO:root:current mean train loss 3473.862287386196
INFO:root:current train perplexity3.9372360706329346
INFO:root:current mean train loss 3473.857392836247
INFO:root:current train perplexity3.9415388107299805
INFO:root:current mean train loss 3477.2520046409118
INFO:root:current train perplexity3.943392753601074
INFO:root:current mean train loss 3471.8220609356704
INFO:root:current train perplexity3.9411725997924805
INFO:root:current mean train loss 3477.9471859434298
INFO:root:current train perplexity3.946277379989624
INFO:root:current mean train loss 3480.228845818381
INFO:root:current train perplexity3.9455182552337646
INFO:root:current mean train loss 3482.837391933352
INFO:root:current train perplexity3.947996139526367
INFO:root:current mean train loss 3483.5120281325576
INFO:root:current train perplexity3.949904680252075
INFO:root:current mean train loss 3483.931591375489
INFO:root:current train perplexity3.950230121612549


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.71s/it]
INFO:root:final mean train loss: 3481.9368819575157
INFO:root:final train perplexity: 3.9500370025634766
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.81s/it]
INFO:root:eval mean loss: 4004.1376416361923
INFO:root:eval perplexity: 5.048825740814209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [7:41:27<9:38:09, 321.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3461.9472028459822
INFO:root:current train perplexity3.9236230850219727
INFO:root:current mean train loss 3470.8215024594906
INFO:root:current train perplexity3.9287943840026855
INFO:root:current mean train loss 3477.2943141206783
INFO:root:current train perplexity3.9385101795196533
INFO:root:current mean train loss 3480.055231168377
INFO:root:current train perplexity3.9398324489593506
INFO:root:current mean train loss 3483.7167379445045
INFO:root:current train perplexity3.944425344467163
INFO:root:current mean train loss 3484.5786433995327
INFO:root:current train perplexity3.94625186920166
INFO:root:current mean train loss 3480.0267939530017
INFO:root:current train perplexity3.940382480621338
INFO:root:current mean train loss 3476.661259034864
INFO:root:current train perplexity3.9385335445404053
INFO:root:current mean train loss 3478.681542091598
INFO:root:current train perplexity3.940953254699707
INFO:root:current mean train loss 3477.5429812834223
INFO:root:current train perplexity3.9392008781433105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.88s/it]
INFO:root:final mean train loss: 3477.208036976476
INFO:root:final train perplexity: 3.942674160003662
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.30s/it]
INFO:root:eval mean loss: 4003.2229973542776
INFO:root:eval perplexity: 5.046958923339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [7:46:18<9:16:44, 312.19s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3437.0172772074857
INFO:root:current train perplexity3.919644594192505
INFO:root:current mean train loss 3472.5005992542615
INFO:root:current train perplexity3.910761833190918
INFO:root:current mean train loss 3465.9247233072915
INFO:root:current train perplexity3.925374746322632
INFO:root:current mean train loss 3467.427682415042
INFO:root:current train perplexity3.9264848232269287
INFO:root:current mean train loss 3465.768221267459
INFO:root:current train perplexity3.92516827583313
INFO:root:current mean train loss 3466.3366627280443
INFO:root:current train perplexity3.925830364227295
INFO:root:current mean train loss 3467.81841367066
INFO:root:current train perplexity3.9295430183410645
INFO:root:current mean train loss 3473.0387575706595
INFO:root:current train perplexity3.9315686225891113
INFO:root:current mean train loss 3472.957808561314
INFO:root:current train perplexity3.931872606277466
INFO:root:current mean train loss 3473.6428875078705
INFO:root:current train perplexity3.9328744411468506


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.35s/it]
INFO:root:final mean train loss: 3471.576804561
INFO:root:final train perplexity: 3.933924436569214
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.85s/it]
INFO:root:eval mean loss: 4003.3656603224736
INFO:root:eval perplexity: 5.047250270843506
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [7:51:35<9:13:59, 313.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3462.3786238128064
INFO:root:current train perplexity3.9406492710113525
INFO:root:current mean train loss 3469.5042360823677
INFO:root:current train perplexity3.923078775405884
INFO:root:current mean train loss 3471.189397682707
INFO:root:current train perplexity3.926950454711914
INFO:root:current mean train loss 3469.704262931802
INFO:root:current train perplexity3.9157931804656982
INFO:root:current mean train loss 3467.456447153028
INFO:root:current train perplexity3.917327880859375
INFO:root:current mean train loss 3469.581840279747
INFO:root:current train perplexity3.922837257385254
INFO:root:current mean train loss 3469.981900591638
INFO:root:current train perplexity3.927114486694336
INFO:root:current mean train loss 3470.1112075175806
INFO:root:current train perplexity3.926055669784546
INFO:root:current mean train loss 3470.1035646826344
INFO:root:current train perplexity3.9276700019836426
INFO:root:current mean train loss 3470.9306494294656
INFO:root:current train perplexity3.9299466609954834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.09s/it]
INFO:root:final mean train loss: 3467.292537873791
INFO:root:final train perplexity: 3.9272801876068115
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it]
INFO:root:eval mean loss: 4006.7919540946364
INFO:root:eval perplexity: 5.054248332977295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [7:56:52<9:10:31, 314.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3435.84328654661
INFO:root:current train perplexity3.8767952919006348
INFO:root:current mean train loss 3443.579486966883
INFO:root:current train perplexity3.8785629272460938
INFO:root:current mean train loss 3446.7418113914696
INFO:root:current train perplexity3.8868534564971924
INFO:root:current mean train loss 3454.880807690634
INFO:root:current train perplexity3.896721839904785
INFO:root:current mean train loss 3456.0951238893995
INFO:root:current train perplexity3.90051531791687
INFO:root:current mean train loss 3460.165500265541
INFO:root:current train perplexity3.904181957244873
INFO:root:current mean train loss 3459.835785236272
INFO:root:current train perplexity3.9072461128234863
INFO:root:current mean train loss 3462.640990728446
INFO:root:current train perplexity3.9111204147338867
INFO:root:current mean train loss 3465.417421920474
INFO:root:current train perplexity3.915713310241699
INFO:root:current mean train loss 3465.841096784574
INFO:root:current train perplexity3.917628526687622


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.65s/it]
INFO:root:final mean train loss: 3461.389109150056
INFO:root:final train perplexity: 3.9181442260742188
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it]
INFO:root:eval mean loss: 4005.585508089539
INFO:root:eval perplexity: 5.051782131195068
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [8:02:13<9:08:29, 316.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3461.838222218983
INFO:root:current train perplexity3.9369301795959473
INFO:root:current mean train loss 3461.7787369596745
INFO:root:current train perplexity3.916593313217163
INFO:root:current mean train loss 3466.624338900105
INFO:root:current train perplexity3.917079448699951
INFO:root:current mean train loss 3462.6406921885646
INFO:root:current train perplexity3.9089694023132324
INFO:root:current mean train loss 3459.5244673865764
INFO:root:current train perplexity3.90736985206604
INFO:root:current mean train loss 3457.3602150676534
INFO:root:current train perplexity3.9076242446899414
INFO:root:current mean train loss 3458.4263584760356
INFO:root:current train perplexity3.9076931476593018
INFO:root:current mean train loss 3456.5234642376954
INFO:root:current train perplexity3.9099271297454834
INFO:root:current mean train loss 3459.89728381587
INFO:root:current train perplexity3.9116063117980957
INFO:root:current mean train loss 3460.2019580734554
INFO:root:current train perplexity3.912924289703369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.64s/it]
INFO:root:final mean train loss: 3457.6329614577753
INFO:root:final train perplexity: 3.9123425483703613
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it]
INFO:root:eval mean loss: 4006.799162303302
INFO:root:eval perplexity: 5.054261684417725
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [8:07:36<9:06:56, 318.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3441.4615592447917
INFO:root:current train perplexity3.9083430767059326
INFO:root:current mean train loss 3446.1325864955356
INFO:root:current train perplexity3.901282548904419
INFO:root:current mean train loss 3455.780363103693
INFO:root:current train perplexity3.8967554569244385
INFO:root:current mean train loss 3458.9063040364586
INFO:root:current train perplexity3.8970630168914795
INFO:root:current mean train loss 3456.2937510279608
INFO:root:current train perplexity3.9000978469848633
INFO:root:current mean train loss 3453.063737262228
INFO:root:current train perplexity3.9029879570007324
INFO:root:current mean train loss 3456.9140321180557
INFO:root:current train perplexity3.903829574584961
INFO:root:current mean train loss 3455.123564138105
INFO:root:current train perplexity3.9031529426574707
INFO:root:current mean train loss 3456.150699497768
INFO:root:current train perplexity3.9037351608276367
INFO:root:current mean train loss 3454.9260326522435
INFO:root:current train perplexity3.9052817821502686


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:32<00:00, 272.61s/it]
INFO:root:final mean train loss: 3453.1318512578164
INFO:root:final train perplexity: 3.905400514602661
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it]
INFO:root:eval mean loss: 4006.649853861924
INFO:root:eval perplexity: 5.053957462310791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [8:13:08<9:08:21, 322.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3429.479086266943
INFO:root:current train perplexity3.8707070350646973
INFO:root:current mean train loss 3432.721238099812
INFO:root:current train perplexity3.882554769515991
INFO:root:current mean train loss 3447.2197878133284
INFO:root:current train perplexity3.8846957683563232
INFO:root:current mean train loss 3448.972269322169
INFO:root:current train perplexity3.8838768005371094
INFO:root:current mean train loss 3448.5155148081653
INFO:root:current train perplexity3.8862786293029785
INFO:root:current mean train loss 3447.739034191413
INFO:root:current train perplexity3.8875606060028076
INFO:root:current mean train loss 3449.9039590816024
INFO:root:current train perplexity3.892209768295288
INFO:root:current mean train loss 3446.9880904274423
INFO:root:current train perplexity3.890794515609741
INFO:root:current mean train loss 3448.442506127017
INFO:root:current train perplexity3.891878128051758
INFO:root:current mean train loss 3448.947384342415
INFO:root:current train perplexity3.8943703174591064


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:31<00:00, 271.78s/it]
INFO:root:final mean train loss: 3446.0602111201133
INFO:root:final train perplexity: 3.8945207595825195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.95s/it]
INFO:root:eval mean loss: 4010.6682267425754
INFO:root:eval perplexity: 5.062176704406738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [8:18:39<9:07:21, 325.16s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3416.19920533568
INFO:root:current train perplexity3.8467464447021484
INFO:root:current mean train loss 3431.9711977973657
INFO:root:current train perplexity3.859951972961426
INFO:root:current mean train loss 3435.5723671405176
INFO:root:current train perplexity3.8708841800689697
INFO:root:current mean train loss 3439.668397713195
INFO:root:current train perplexity3.876765251159668
INFO:root:current mean train loss 3439.983035955798
INFO:root:current train perplexity3.8782236576080322
INFO:root:current mean train loss 3440.2359394002483
INFO:root:current train perplexity3.877946138381958
INFO:root:current mean train loss 3441.396375553998
INFO:root:current train perplexity3.8800876140594482
INFO:root:current mean train loss 3442.351111565167
INFO:root:current train perplexity3.882554769515991
INFO:root:current mean train loss 3441.0727640445252
INFO:root:current train perplexity3.8838562965393066
INFO:root:current mean train loss 3445.268634261084
INFO:root:current train perplexity3.8891937732696533


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.03s/it]
INFO:root:final mean train loss: 3442.529319886238
INFO:root:final train perplexity: 3.889098644256592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it]
INFO:root:eval mean loss: 4010.8732979416
INFO:root:eval perplexity: 5.062595844268799
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [8:24:05<9:02:14, 325.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3416.949988162879
INFO:root:current train perplexity3.8467047214508057
INFO:root:current mean train loss 3423.496854389133
INFO:root:current train perplexity3.8683278560638428
INFO:root:current mean train loss 3427.2322283588524
INFO:root:current train perplexity3.8707258701324463
INFO:root:current mean train loss 3425.4178010945334
INFO:root:current train perplexity3.8678367137908936
INFO:root:current mean train loss 3430.322767116264
INFO:root:current train perplexity3.8694393634796143
INFO:root:current mean train loss 3432.713467351184
INFO:root:current train perplexity3.8732802867889404
INFO:root:current mean train loss 3438.290973363175
INFO:root:current train perplexity3.878302574157715
INFO:root:current mean train loss 3441.629399725731
INFO:root:current train perplexity3.8807036876678467
INFO:root:current mean train loss 3440.982901737608
INFO:root:current train perplexity3.881685972213745


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.61s/it]
INFO:root:final mean train loss: 3437.736209377166
INFO:root:final train perplexity: 3.881751775741577
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it]
INFO:root:eval mean loss: 4011.6099498559397
INFO:root:eval perplexity: 5.064104080200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [8:29:22<8:52:20, 322.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3387.6042131696427
INFO:root:current train perplexity3.84601092338562
INFO:root:current mean train loss 3452.6439380110983
INFO:root:current train perplexity3.869844675064087
INFO:root:current mean train loss 3438.6565601883303
INFO:root:current train perplexity3.861189842224121
INFO:root:current mean train loss 3431.0411563836014
INFO:root:current train perplexity3.8678739070892334
INFO:root:current mean train loss 3437.157537886786
INFO:root:current train perplexity3.8715579509735107
INFO:root:current mean train loss 3436.658209866556
INFO:root:current train perplexity3.8740081787109375
INFO:root:current mean train loss 3434.7735054732548
INFO:root:current train perplexity3.8703835010528564
INFO:root:current mean train loss 3432.9516960694395
INFO:root:current train perplexity3.8697800636291504
INFO:root:current mean train loss 3434.46100496389
INFO:root:current train perplexity3.8723490238189697
INFO:root:current mean train loss 3434.197938290294
INFO:root:current train perplexity3.8740265369415283


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.00s/it]
INFO:root:final mean train loss: 3433.2080661404516
INFO:root:final train perplexity: 3.8748228549957275
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.20s/it]
INFO:root:eval mean loss: 4016.024794991135
INFO:root:eval perplexity: 5.073153018951416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [8:34:47<8:48:11, 323.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3408.830533854167
INFO:root:current train perplexity3.8136284351348877
INFO:root:current mean train loss 3410.833345363451
INFO:root:current train perplexity3.8229777812957764
INFO:root:current mean train loss 3409.659749727471
INFO:root:current train perplexity3.833387613296509
INFO:root:current mean train loss 3427.5579830109127
INFO:root:current train perplexity3.8526113033294678
INFO:root:current mean train loss 3432.3626858998496
INFO:root:current train perplexity3.85674786567688
INFO:root:current mean train loss 3428.044771598149
INFO:root:current train perplexity3.854384422302246
INFO:root:current mean train loss 3430.263048621697
INFO:root:current train perplexity3.858513355255127
INFO:root:current mean train loss 3428.6386606069714
INFO:root:current train perplexity3.8611912727355957
INFO:root:current mean train loss 3428.7337300493673
INFO:root:current train perplexity3.863784074783325
INFO:root:current mean train loss 3429.4755808679133
INFO:root:current train perplexity3.865722179412842


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.74s/it]
INFO:root:final mean train loss: 3429.065486108103
INFO:root:final train perplexity: 3.868495225906372
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it]
INFO:root:eval mean loss: 4012.501921958112
INFO:root:eval perplexity: 5.06593132019043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [8:40:11<8:43:28, 323.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3353.304337211277
INFO:root:current train perplexity3.7937991619110107
INFO:root:current mean train loss 3408.874079014228
INFO:root:current train perplexity3.8217625617980957
INFO:root:current mean train loss 3414.208877084501
INFO:root:current train perplexity3.835137367248535
INFO:root:current mean train loss 3411.7241755151895
INFO:root:current train perplexity3.8367702960968018
INFO:root:current mean train loss 3411.8115857712764
INFO:root:current train perplexity3.840625286102295
INFO:root:current mean train loss 3416.8809167923937
INFO:root:current train perplexity3.849867343902588
INFO:root:current mean train loss 3420.8319556173506
INFO:root:current train perplexity3.8550145626068115
INFO:root:current mean train loss 3423.0218075320927
INFO:root:current train perplexity3.8589894771575928
INFO:root:current mean train loss 3423.8096248955803
INFO:root:current train perplexity3.8590478897094727
INFO:root:current mean train loss 3424.515578711149
INFO:root:current train perplexity3.857752561569214


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.47s/it]
INFO:root:final mean train loss: 3423.784203683176
INFO:root:final train perplexity: 3.860442876815796
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.14s/it]
INFO:root:eval mean loss: 4013.6940052221853
INFO:root:eval perplexity: 5.068374156951904
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [8:45:38<8:39:16, 324.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3362.961898311492
INFO:root:current train perplexity3.7952773571014404
INFO:root:current mean train loss 3391.0391258647423
INFO:root:current train perplexity3.8191518783569336
INFO:root:current mean train loss 3388.883222571699
INFO:root:current train perplexity3.8279075622558594
INFO:root:current mean train loss 3398.7662873513027
INFO:root:current train perplexity3.8374221324920654
INFO:root:current mean train loss 3399.9411938306625
INFO:root:current train perplexity3.8368124961853027
INFO:root:current mean train loss 3404.8701323600812
INFO:root:current train perplexity3.8394296169281006
INFO:root:current mean train loss 3411.3899146320323
INFO:root:current train perplexity3.8442025184631348
INFO:root:current mean train loss 3413.764775684529
INFO:root:current train perplexity3.846372127532959
INFO:root:current mean train loss 3419.885889083183
INFO:root:current train perplexity3.852344512939453
INFO:root:current mean train loss 3419.912184374161
INFO:root:current train perplexity3.8508660793304443


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.80s/it]
INFO:root:final mean train loss: 3418.804605853173
INFO:root:final train perplexity: 3.852865695953369
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it]
INFO:root:eval mean loss: 4014.959749695257
INFO:root:eval perplexity: 5.0709686279296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [8:51:00<8:32:38, 323.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3403.844883062901
INFO:root:current train perplexity3.847674608230591
INFO:root:current mean train loss 3419.4179476731115
INFO:root:current train perplexity3.8442351818084717
INFO:root:current mean train loss 3412.3023165778636
INFO:root:current train perplexity3.842531681060791
INFO:root:current mean train loss 3422.6173171321902
INFO:root:current train perplexity3.8457586765289307
INFO:root:current mean train loss 3418.9605643374503
INFO:root:current train perplexity3.8427748680114746
INFO:root:current mean train loss 3417.576918338358
INFO:root:current train perplexity3.8422348499298096
INFO:root:current mean train loss 3419.6542311595267
INFO:root:current train perplexity3.8427131175994873
INFO:root:current mean train loss 3420.79476957618
INFO:root:current train perplexity3.8447558879852295
INFO:root:current mean train loss 3418.318470824177
INFO:root:current train perplexity3.8440732955932617
INFO:root:current mean train loss 3416.961112480448
INFO:root:current train perplexity3.84440016746521


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.46s/it]
INFO:root:final mean train loss: 3413.528178676482
INFO:root:final train perplexity: 3.8448538780212402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.94s/it]
INFO:root:eval mean loss: 4018.2284359762853
INFO:root:eval perplexity: 5.077675819396973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [8:56:24<8:27:27, 323.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3350.3440720578456
INFO:root:current train perplexity3.8042678833007812
INFO:root:current mean train loss 3393.926995309843
INFO:root:current train perplexity3.8298277854919434
INFO:root:current mean train loss 3394.0226517032515
INFO:root:current train perplexity3.8236937522888184
INFO:root:current mean train loss 3396.874542676063
INFO:root:current train perplexity3.828101873397827
INFO:root:current mean train loss 3403.3505892145554
INFO:root:current train perplexity3.8279125690460205
INFO:root:current mean train loss 3409.780825543447
INFO:root:current train perplexity3.83300518989563
INFO:root:current mean train loss 3411.9924584319456
INFO:root:current train perplexity3.835291862487793
INFO:root:current mean train loss 3410.84804582915
INFO:root:current train perplexity3.8359837532043457
INFO:root:current mean train loss 3413.4999169864227
INFO:root:current train perplexity3.8406472206115723
INFO:root:current mean train loss 3413.3429234795735
INFO:root:current train perplexity3.84146785736084


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.50s/it]
INFO:root:final mean train loss: 3411.7983659928846
INFO:root:final train perplexity: 3.842230796813965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.81s/it]
INFO:root:eval mean loss: 4017.547252465647
INFO:root:eval perplexity: 5.076276779174805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [9:01:45<8:20:49, 323.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3390.262704190341
INFO:root:current train perplexity3.8126420974731445
INFO:root:current mean train loss 3397.9799127394153
INFO:root:current train perplexity3.814232349395752
INFO:root:current mean train loss 3390.2025055530025
INFO:root:current train perplexity3.814453125
INFO:root:current mean train loss 3399.831066378741
INFO:root:current train perplexity3.820456027984619
INFO:root:current mean train loss 3403.5497038118133
INFO:root:current train perplexity3.8196322917938232
INFO:root:current mean train loss 3402.464821755349
INFO:root:current train perplexity3.823789596557617
INFO:root:current mean train loss 3403.6437712458255
INFO:root:current train perplexity3.8278892040252686
INFO:root:current mean train loss 3406.7939931705296
INFO:root:current train perplexity3.827723503112793
INFO:root:current mean train loss 3405.4871230811405
INFO:root:current train perplexity3.8267905712127686
INFO:root:current mean train loss 3408.3424418664104
INFO:root:current train perplexity3.8317551612854004


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.12s/it]
INFO:root:final mean train loss: 3404.813107459776
INFO:root:final train perplexity: 3.8316562175750732
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.48s/it]
INFO:root:eval mean loss: 4018.166689176086
INFO:root:eval perplexity: 5.077548980712891
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [9:07:09<8:15:47, 323.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3349.8694118923613
INFO:root:current train perplexity3.8061931133270264
INFO:root:current mean train loss 3375.205120063267
INFO:root:current train perplexity3.820141077041626
INFO:root:current mean train loss 3381.969426724394
INFO:root:current train perplexity3.8140063285827637
INFO:root:current mean train loss 3391.294944742166
INFO:root:current train perplexity3.8223981857299805
INFO:root:current mean train loss 3395.0855810441417
INFO:root:current train perplexity3.8164072036743164
INFO:root:current mean train loss 3398.8169986054063
INFO:root:current train perplexity3.820019483566284
INFO:root:current mean train loss 3401.102391031533
INFO:root:current train perplexity3.820373773574829
INFO:root:current mean train loss 3401.662291440551
INFO:root:current train perplexity3.820808172225952
INFO:root:current mean train loss 3401.6573142607544
INFO:root:current train perplexity3.823129177093506
INFO:root:current mean train loss 3404.1149192485236
INFO:root:current train perplexity3.8271286487579346


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.09s/it]
INFO:root:final mean train loss: 3401.890914424773
INFO:root:final train perplexity: 3.8272416591644287
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.68s/it]
INFO:root:eval mean loss: 4019.743965744127
INFO:root:eval perplexity: 5.080787658691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [9:12:33<8:10:44, 323.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3378.324198118398
INFO:root:current train perplexity3.7880477905273438
INFO:root:current mean train loss 3377.9877015944808
INFO:root:current train perplexity3.803083658218384
INFO:root:current mean train loss 3380.8292538125575
INFO:root:current train perplexity3.805799722671509
INFO:root:current mean train loss 3385.670331846993
INFO:root:current train perplexity3.8019309043884277
INFO:root:current mean train loss 3388.9216775104496
INFO:root:current train perplexity3.809380531311035
INFO:root:current mean train loss 3393.3155600953646
INFO:root:current train perplexity3.8165032863616943
INFO:root:current mean train loss 3394.7456258441225
INFO:root:current train perplexity3.81782603263855
INFO:root:current mean train loss 3396.176144136065
INFO:root:current train perplexity3.8179843425750732
INFO:root:current mean train loss 3397.589094790471
INFO:root:current train perplexity3.819101572036743
INFO:root:current mean train loss 3400.215536194162
INFO:root:current train perplexity3.822394609451294


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:30<00:00, 270.60s/it]
INFO:root:final mean train loss: 3398.4012767422582
INFO:root:final train perplexity: 3.8219761848449707
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it]
INFO:root:eval mean loss: 4021.149893686281
INFO:root:eval perplexity: 5.083677768707275
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [9:18:03<8:08:00, 325.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3386.7171275464793
INFO:root:current train perplexity3.7919797897338867
INFO:root:current mean train loss 3390.4304376527584
INFO:root:current train perplexity3.793994188308716
INFO:root:current mean train loss 3397.458731483815
INFO:root:current train perplexity3.8074424266815186
INFO:root:current mean train loss 3399.449672246042
INFO:root:current train perplexity3.812309741973877
INFO:root:current mean train loss 3399.9046908639416
INFO:root:current train perplexity3.8080852031707764
INFO:root:current mean train loss 3395.235248677677
INFO:root:current train perplexity3.803227663040161
INFO:root:current mean train loss 3396.2031066624863
INFO:root:current train perplexity3.80511212348938
INFO:root:current mean train loss 3393.522601028462
INFO:root:current train perplexity3.807220935821533
INFO:root:current mean train loss 3393.3086168030964
INFO:root:current train perplexity3.810818672180176
INFO:root:current mean train loss 3394.7167913886938
INFO:root:current train perplexity3.812995195388794


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.63s/it]
INFO:root:final mean train loss: 3392.3169585812475
INFO:root:final train perplexity: 3.8128130435943604
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it]
INFO:root:eval mean loss: 4021.5292501246677
INFO:root:eval perplexity: 5.084457874298096
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [9:23:23<8:00:08, 323.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3394.7029257588
INFO:root:current train perplexity3.7907333374023438
INFO:root:current mean train loss 3390.400139956551
INFO:root:current train perplexity3.79362154006958
INFO:root:current mean train loss 3389.4093957562063
INFO:root:current train perplexity3.7997002601623535
INFO:root:current mean train loss 3385.7446945151
INFO:root:current train perplexity3.7944576740264893
INFO:root:current mean train loss 3385.695516535389
INFO:root:current train perplexity3.7990779876708984
INFO:root:current mean train loss 3385.89043035296
INFO:root:current train perplexity3.8021109104156494
INFO:root:current mean train loss 3390.0706497481124
INFO:root:current train perplexity3.8060920238494873
INFO:root:current mean train loss 3387.817882261555
INFO:root:current train perplexity3.806002140045166
INFO:root:current mean train loss 3390.682523329605
INFO:root:current train perplexity3.8081319332122803
INFO:root:current mean train loss 3391.862174330753
INFO:root:current train perplexity3.8078722953796387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:29<00:00, 269.88s/it]
INFO:root:final mean train loss: 3388.963198200349
INFO:root:final train perplexity: 3.8077714443206787
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.07s/it]
INFO:root:eval mean loss: 4025.924231563054
INFO:root:eval perplexity: 5.093501567840576
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [9:28:50<7:56:27, 324.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3365.4393580386513
INFO:root:current train perplexity3.773042917251587
INFO:root:current mean train loss 3358.307921424279
INFO:root:current train perplexity3.7845046520233154
INFO:root:current mean train loss 3366.0099741790254
INFO:root:current train perplexity3.7846930027008057
INFO:root:current mean train loss 3376.2464466722704
INFO:root:current train perplexity3.7938551902770996
INFO:root:current mean train loss 3377.8979457662563
INFO:root:current train perplexity3.795349359512329
INFO:root:current mean train loss 3376.448972147453
INFO:root:current train perplexity3.7936089038848877
INFO:root:current mean train loss 3380.36903102518
INFO:root:current train perplexity3.797114849090576
INFO:root:current mean train loss 3383.734374692905
INFO:root:current train perplexity3.799776792526245
INFO:root:current mean train loss 3385.333058822888
INFO:root:current train perplexity3.801988363265991


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.71s/it]
INFO:root:final mean train loss: 3384.9333788964054
INFO:root:final train perplexity: 3.801722288131714
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.83s/it]
INFO:root:eval mean loss: 4024.9309220550754
INFO:root:eval perplexity: 5.091455936431885
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [9:34:15<7:51:09, 324.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3292.1459147135415
INFO:root:current train perplexity3.7904810905456543
INFO:root:current mean train loss 3358.3271034018508
INFO:root:current train perplexity3.7668213844299316
INFO:root:current mean train loss 3376.7458844866073
INFO:root:current train perplexity3.78798770904541
INFO:root:current mean train loss 3375.322828840501
INFO:root:current train perplexity3.7881267070770264
INFO:root:current mean train loss 3374.868907994727
INFO:root:current train perplexity3.7891674041748047
INFO:root:current mean train loss 3379.7483778966825
INFO:root:current train perplexity3.7934699058532715
INFO:root:current mean train loss 3380.273335471082
INFO:root:current train perplexity3.7929584980010986
INFO:root:current mean train loss 3381.5532289073612
INFO:root:current train perplexity3.794607400894165
INFO:root:current mean train loss 3384.6295346916836
INFO:root:current train perplexity3.7973380088806152
INFO:root:current mean train loss 3382.9664289066827
INFO:root:current train perplexity3.7940430641174316


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.48s/it]
INFO:root:final mean train loss: 3380.418958110194
INFO:root:final train perplexity: 3.794957160949707
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it]
INFO:root:eval mean loss: 4024.7425563081783
INFO:root:eval perplexity: 5.091069221496582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [9:39:36<7:43:50, 323.61s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3342.505437677557
INFO:root:current train perplexity3.7726593017578125
INFO:root:current mean train loss 3365.791970192849
INFO:root:current train perplexity3.7913081645965576
INFO:root:current mean train loss 3364.1456628591527
INFO:root:current train perplexity3.788025140762329
INFO:root:current mean train loss 3372.6235398663584
INFO:root:current train perplexity3.786806583404541
INFO:root:current mean train loss 3374.9989610658076
INFO:root:current train perplexity3.7878174781799316
INFO:root:current mean train loss 3373.938495673312
INFO:root:current train perplexity3.787546396255493
INFO:root:current mean train loss 3379.75108644576
INFO:root:current train perplexity3.78718900680542
INFO:root:current mean train loss 3378.868604957806
INFO:root:current train perplexity3.788090944290161
INFO:root:current mean train loss 3376.6991266328223
INFO:root:current train perplexity3.786454916000366
INFO:root:current mean train loss 3378.5157273729073
INFO:root:current train perplexity3.7881994247436523


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.05s/it]
INFO:root:final mean train loss: 3377.8694956379554
INFO:root:final train perplexity: 3.7911417484283447
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.62s/it]
INFO:root:eval mean loss: 4026.3115459469195
INFO:root:eval perplexity: 5.094299793243408
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [9:44:59<7:38:14, 323.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3313.8609811883225
INFO:root:current train perplexity3.7217752933502197
INFO:root:current mean train loss 3341.079946822479
INFO:root:current train perplexity3.7636778354644775
INFO:root:current mean train loss 3356.9789749215183
INFO:root:current train perplexity3.7671401500701904
INFO:root:current mean train loss 3364.3963665140086
INFO:root:current train perplexity3.772047758102417
INFO:root:current mean train loss 3365.491134024463
INFO:root:current train perplexity3.7704904079437256
INFO:root:current mean train loss 3368.664946392552
INFO:root:current train perplexity3.7714266777038574
INFO:root:current mean train loss 3370.320088868765
INFO:root:current train perplexity3.775097131729126
INFO:root:current mean train loss 3374.866382752738
INFO:root:current train perplexity3.7787320613861084
INFO:root:current mean train loss 3375.868137531956
INFO:root:current train perplexity3.7805378437042236
INFO:root:current mean train loss 3373.876362565033
INFO:root:current train perplexity3.7822251319885254


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.64s/it]
INFO:root:final mean train loss: 3373.611475052372
INFO:root:final train perplexity: 3.7847788333892822
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.04s/it]
INFO:root:eval mean loss: 4027.1261601008423
INFO:root:eval perplexity: 5.095977306365967
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [9:50:16<7:30:16, 321.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3388.8959689670137
INFO:root:current train perplexity3.8000309467315674
INFO:root:current mean train loss 3375.9766797643947
INFO:root:current train perplexity3.772545099258423
INFO:root:current mean train loss 3373.9917551452368
INFO:root:current train perplexity3.7687437534332275
INFO:root:current mean train loss 3369.49043371918
INFO:root:current train perplexity3.7701120376586914
INFO:root:current mean train loss 3372.063897948075
INFO:root:current train perplexity3.773717164993286
INFO:root:current mean train loss 3371.0376527847784
INFO:root:current train perplexity3.7718143463134766
INFO:root:current mean train loss 3371.5706680030153
INFO:root:current train perplexity3.772313356399536
INFO:root:current mean train loss 3372.1241728770847
INFO:root:current train perplexity3.774925708770752
INFO:root:current mean train loss 3371.346671716766
INFO:root:current train perplexity3.774782657623291
INFO:root:current mean train loss 3371.6393711127125
INFO:root:current train perplexity3.7777605056762695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.21s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.21s/it]
INFO:root:final mean train loss: 3368.666005596038
INFO:root:final train perplexity: 3.7774007320404053
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it]
INFO:root:eval mean loss: 4027.458750623338
INFO:root:eval perplexity: 5.0966644287109375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [9:55:44<7:27:35, 323.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3358.0841099330355
INFO:root:current train perplexity3.7411539554595947
INFO:root:current mean train loss 3360.2054777922453
INFO:root:current train perplexity3.747819185256958
INFO:root:current mean train loss 3357.7609094498007
INFO:root:current train perplexity3.7557990550994873
INFO:root:current mean train loss 3366.093399457789
INFO:root:current train perplexity3.7643659114837646
INFO:root:current mean train loss 3368.584619140625
INFO:root:current train perplexity3.7658801078796387
INFO:root:current mean train loss 3368.364353187062
INFO:root:current train perplexity3.7669668197631836
INFO:root:current mean train loss 3367.745194851132
INFO:root:current train perplexity3.7700729370117188
INFO:root:current mean train loss 3369.027651666135
INFO:root:current train perplexity3.7699856758117676
INFO:root:current mean train loss 3368.454351550805
INFO:root:current train perplexity3.7709555625915527
INFO:root:current mean train loss 3368.7122195646725
INFO:root:current train perplexity3.7734375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.46s/it]
INFO:root:final mean train loss: 3365.7875424661943
INFO:root:final train perplexity: 3.773113489151001
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.49s/it]
INFO:root:eval mean loss: 4031.051513671875
INFO:root:eval perplexity: 5.104074478149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [10:01:10<7:23:10, 324.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3354.6939952761627
INFO:root:current train perplexity3.746938705444336
INFO:root:current mean train loss 3346.1910886964597
INFO:root:current train perplexity3.738142490386963
INFO:root:current mean train loss 3348.3137327594523
INFO:root:current train perplexity3.7472028732299805
INFO:root:current mean train loss 3353.2801737882655
INFO:root:current train perplexity3.750844717025757
INFO:root:current mean train loss 3360.1711646224253
INFO:root:current train perplexity3.7538068294525146
INFO:root:current mean train loss 3360.124020290199
INFO:root:current train perplexity3.754227876663208
INFO:root:current mean train loss 3362.225721866495
INFO:root:current train perplexity3.7588467597961426
INFO:root:current mean train loss 3363.4851560528473
INFO:root:current train perplexity3.761855363845825
INFO:root:current mean train loss 3362.639686665925
INFO:root:current train perplexity3.766404628753662
INFO:root:current mean train loss 3363.3848963683886
INFO:root:current train perplexity3.7660279273986816


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.14s/it]
INFO:root:final mean train loss: 3361.338952956661
INFO:root:final train perplexity: 3.7664976119995117
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.65s/it]
INFO:root:eval mean loss: 4031.2506510416665
INFO:root:eval perplexity: 5.104483604431152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [10:06:35<7:18:01, 324.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3370.042767693015
INFO:root:current train perplexity3.768958330154419
INFO:root:current mean train loss 3349.3619538364032
INFO:root:current train perplexity3.7460639476776123
INFO:root:current mean train loss 3347.0953763072707
INFO:root:current train perplexity3.7502403259277344
INFO:root:current mean train loss 3343.774478053775
INFO:root:current train perplexity3.7516133785247803
INFO:root:current mean train loss 3345.4620060888997
INFO:root:current train perplexity3.7513136863708496
INFO:root:current mean train loss 3349.656401092474
INFO:root:current train perplexity3.7521750926971436
INFO:root:current mean train loss 3353.6380185831895
INFO:root:current train perplexity3.75477933883667
INFO:root:current mean train loss 3357.3877687822487
INFO:root:current train perplexity3.7578673362731934
INFO:root:current mean train loss 3357.5623126629516
INFO:root:current train perplexity3.759397506713867
INFO:root:current mean train loss 3359.6635757590693
INFO:root:current train perplexity3.760730504989624


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.99s/it]
INFO:root:final mean train loss: 3357.6306553502236
INFO:root:final train perplexity: 3.760990858078003
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it]
INFO:root:eval mean loss: 4033.82369410738
INFO:root:eval perplexity: 5.109797477722168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [10:11:51<7:09:08, 321.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3341.8243470272773
INFO:root:current train perplexity3.7304248809814453
INFO:root:current mean train loss 3338.863989104265
INFO:root:current train perplexity3.7328479290008545
INFO:root:current mean train loss 3339.6304135120054
INFO:root:current train perplexity3.7296106815338135
INFO:root:current mean train loss 3347.297306156424
INFO:root:current train perplexity3.742327928543091
INFO:root:current mean train loss 3348.2396258850763
INFO:root:current train perplexity3.745086431503296
INFO:root:current mean train loss 3346.282244032312
INFO:root:current train perplexity3.7478487491607666
INFO:root:current mean train loss 3351.8691472934843
INFO:root:current train perplexity3.751885414123535
INFO:root:current mean train loss 3354.582569710351
INFO:root:current train perplexity3.754859447479248
INFO:root:current mean train loss 3356.3160214229847
INFO:root:current train perplexity3.7553603649139404
INFO:root:current mean train loss 3356.224832385623
INFO:root:current train perplexity3.7553913593292236


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:27<00:00, 267.59s/it]
INFO:root:final mean train loss: 3354.3620332287205
INFO:root:final train perplexity: 3.756143808364868
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it]
INFO:root:eval mean loss: 4034.9072248310063
INFO:root:eval perplexity: 5.112037658691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [10:17:13<7:04:01, 322.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3343.0032904326026
INFO:root:current train perplexity3.7507119178771973
INFO:root:current mean train loss 3347.7212031016093
INFO:root:current train perplexity3.745002508163452
INFO:root:current mean train loss 3340.4976033985836
INFO:root:current train perplexity3.7335565090179443
INFO:root:current mean train loss 3341.0827583500086
INFO:root:current train perplexity3.737900733947754
INFO:root:current mean train loss 3345.2023559831705
INFO:root:current train perplexity3.7373597621917725
INFO:root:current mean train loss 3344.837539699763
INFO:root:current train perplexity3.738250494003296
INFO:root:current mean train loss 3348.3721943959663
INFO:root:current train perplexity3.741105318069458
INFO:root:current mean train loss 3348.3132406978284
INFO:root:current train perplexity3.742311954498291
INFO:root:current mean train loss 3350.0480379766796
INFO:root:current train perplexity3.7438066005706787
INFO:root:current mean train loss 3352.5676405866243
INFO:root:current train perplexity3.748568058013916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:28<00:00, 268.47s/it]
INFO:root:final mean train loss: 3350.4906387944375
INFO:root:final train perplexity: 3.7504115104675293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.08s/it]
INFO:root:eval mean loss: 4033.3585456144724
INFO:root:eval perplexity: 5.108836650848389
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [10:22:02<6:45:36, 312.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3343.883756510417
INFO:root:current train perplexity3.73270320892334
INFO:root:current mean train loss 3333.867993861607
INFO:root:current train perplexity3.742851734161377
INFO:root:current mean train loss 3343.5621147017046
INFO:root:current train perplexity3.746513605117798
INFO:root:current mean train loss 3343.9231315104166
INFO:root:current train perplexity3.7423243522644043
INFO:root:current mean train loss 3348.1206614925986
INFO:root:current train perplexity3.7456507682800293
INFO:root:current mean train loss 3349.000177904212
INFO:root:current train perplexity3.743258237838745
INFO:root:current mean train loss 3350.325275607639
INFO:root:current train perplexity3.74458909034729
INFO:root:current mean train loss 3351.0479945816533
INFO:root:current train perplexity3.745626211166382
INFO:root:current mean train loss 3349.6796964285713
INFO:root:current train perplexity3.7449119091033936
INFO:root:current mean train loss 3349.35126176883
INFO:root:current train perplexity3.745077133178711


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.39s/it]
INFO:root:final mean train loss: 3347.231369080082
INFO:root:final train perplexity: 3.745591878890991
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.04s/it]
INFO:root:eval mean loss: 4036.6473483904033
INFO:root:eval perplexity: 5.115635395050049
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [10:26:47<6:30:11, 304.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3321.632559535015
INFO:root:current train perplexity3.7225184440612793
INFO:root:current mean train loss 3314.803317377476
INFO:root:current train perplexity3.7186331748962402
INFO:root:current mean train loss 3323.0212799180104
INFO:root:current train perplexity3.720411539077759
INFO:root:current mean train loss 3332.9629945281904
INFO:root:current train perplexity3.7298424243927
INFO:root:current mean train loss 3336.782952413302
INFO:root:current train perplexity3.7365944385528564
INFO:root:current mean train loss 3345.8076712083243
INFO:root:current train perplexity3.738562822341919
INFO:root:current mean train loss 3347.50149486983
INFO:root:current train perplexity3.739189386367798
INFO:root:current mean train loss 3345.712995702127
INFO:root:current train perplexity3.7362287044525146
INFO:root:current mean train loss 3345.843072323135
INFO:root:current train perplexity3.7383484840393066
INFO:root:current mean train loss 3345.740725884966
INFO:root:current train perplexity3.7393717765808105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.86s/it]
INFO:root:final mean train loss: 3342.823719639932
INFO:root:final train perplexity: 3.739084243774414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.36s/it]
INFO:root:eval mean loss: 4036.3058839622117
INFO:root:eval perplexity: 5.114929676055908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [10:31:32<6:17:35, 298.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3311.5731992616757
INFO:root:current train perplexity3.6813666820526123
INFO:root:current mean train loss 3336.0655754049412
INFO:root:current train perplexity3.7197256088256836
INFO:root:current mean train loss 3337.7911876141
INFO:root:current train perplexity3.72147274017334
INFO:root:current mean train loss 3339.180916944733
INFO:root:current train perplexity3.7210590839385986
INFO:root:current mean train loss 3338.0153579867297
INFO:root:current train perplexity3.7220985889434814
INFO:root:current mean train loss 3341.0631547595444
INFO:root:current train perplexity3.7287025451660156
INFO:root:current mean train loss 3342.4350497608766
INFO:root:current train perplexity3.730062484741211
INFO:root:current mean train loss 3344.789931961619
INFO:root:current train perplexity3.730766773223877
INFO:root:current mean train loss 3342.6607138551312
INFO:root:current train perplexity3.73295521736145
INFO:root:current mean train loss 3342.3067519787464
INFO:root:current train perplexity3.734398603439331


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.73s/it]
INFO:root:final mean train loss: 3339.608131039527
INFO:root:final train perplexity: 3.7343432903289795
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it]
INFO:root:eval mean loss: 4036.9009568234706
INFO:root:eval perplexity: 5.116159439086914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [10:36:16<6:07:38, 294.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3345.96956626815
INFO:root:current train perplexity3.732469320297241
INFO:root:current mean train loss 3332.403806140075
INFO:root:current train perplexity3.7276904582977295
INFO:root:current mean train loss 3334.9725721480454
INFO:root:current train perplexity3.727797508239746
INFO:root:current mean train loss 3334.3989453614504
INFO:root:current train perplexity3.7262439727783203
INFO:root:current mean train loss 3334.550459806331
INFO:root:current train perplexity3.724546432495117
INFO:root:current mean train loss 3336.392378003052
INFO:root:current train perplexity3.7241909503936768
INFO:root:current mean train loss 3335.4566910597728
INFO:root:current train perplexity3.725109577178955
INFO:root:current mean train loss 3338.3267267678348
INFO:root:current train perplexity3.7271714210510254
INFO:root:current mean train loss 3339.5264853200606
INFO:root:current train perplexity3.7303154468536377


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.91s/it]
INFO:root:final mean train loss: 3336.8629014415123
INFO:root:final train perplexity: 3.7303011417388916
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.13s/it]
INFO:root:eval mean loss: 4039.1952345827794
INFO:root:eval perplexity: 5.120909690856934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [10:41:02<5:59:44, 291.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3344.123360770089
INFO:root:current train perplexity3.782371997833252
INFO:root:current mean train loss 3325.0652539975176
INFO:root:current train perplexity3.712862014770508
INFO:root:current mean train loss 3329.011773003472
INFO:root:current train perplexity3.712831735610962
INFO:root:current mean train loss 3329.584664310617
INFO:root:current train perplexity3.715517997741699
INFO:root:current mean train loss 3322.7382884482495
INFO:root:current train perplexity3.7155566215515137
INFO:root:current mean train loss 3327.2890023075383
INFO:root:current train perplexity3.718794584274292
INFO:root:current mean train loss 3328.2656881467515
INFO:root:current train perplexity3.716562271118164
INFO:root:current mean train loss 3332.602074953589
INFO:root:current train perplexity3.7212769985198975
INFO:root:current mean train loss 3334.206508783012
INFO:root:current train perplexity3.7228925228118896
INFO:root:current mean train loss 3334.1730771301336
INFO:root:current train perplexity3.7213284969329834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.67s/it]
INFO:root:final mean train loss: 3332.2205458610288
INFO:root:final train perplexity: 3.723475217819214
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.09s/it]
INFO:root:eval mean loss: 4040.4861186142507
INFO:root:eval perplexity: 5.1235833168029785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [10:45:48<5:52:43, 289.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3293.582063802083
INFO:root:current train perplexity3.7298943996429443
INFO:root:current mean train loss 3310.4194527004074
INFO:root:current train perplexity3.7248587608337402
INFO:root:current mean train loss 3316.4690577307415
INFO:root:current train perplexity3.722318410873413
INFO:root:current mean train loss 3324.20574234251
INFO:root:current train perplexity3.71826171875
INFO:root:current mean train loss 3326.451007741905
INFO:root:current train perplexity3.716337203979492
INFO:root:current mean train loss 3326.3210330703882
INFO:root:current train perplexity3.7180235385894775
INFO:root:current mean train loss 3328.369730532266
INFO:root:current train perplexity3.7169179916381836
INFO:root:current mean train loss 3329.337802529502
INFO:root:current train perplexity3.7178196907043457
INFO:root:current mean train loss 3329.1579134513995
INFO:root:current train perplexity3.7168705463409424
INFO:root:current mean train loss 3331.6834632748464
INFO:root:current train perplexity3.7203238010406494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.30s/it]
INFO:root:final mean train loss: 3330.416530301494
INFO:root:final train perplexity: 3.7208263874053955
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it]
INFO:root:eval mean loss: 4039.921757258422
INFO:root:eval perplexity: 5.122413158416748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [10:50:34<5:46:18, 288.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3278.266803243886
INFO:root:current train perplexity3.6900134086608887
INFO:root:current mean train loss 3328.4415789348323
INFO:root:current train perplexity3.7072582244873047
INFO:root:current mean train loss 3327.4265914027465
INFO:root:current train perplexity3.708425760269165
INFO:root:current mean train loss 3327.7872182178794
INFO:root:current train perplexity3.71557354927063
INFO:root:current mean train loss 3325.56553761728
INFO:root:current train perplexity3.709317445755005
INFO:root:current mean train loss 3323.9837592801446
INFO:root:current train perplexity3.71089768409729
INFO:root:current mean train loss 3326.850405673154
INFO:root:current train perplexity3.7143521308898926
INFO:root:current mean train loss 3328.3269586629062
INFO:root:current train perplexity3.7156920433044434
INFO:root:current mean train loss 3326.850174191221
INFO:root:current train perplexity3.7140145301818848
INFO:root:current mean train loss 3327.371934355532
INFO:root:current train perplexity3.7133169174194336


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.75s/it]
INFO:root:final mean train loss: 3326.1202722364856
INFO:root:final train perplexity: 3.714524269104004
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it]
INFO:root:eval mean loss: 4042.0900636497117
INFO:root:eval perplexity: 5.126908302307129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [10:55:18<5:40:05, 287.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3318.0014254662296
INFO:root:current train perplexity3.669391393661499
INFO:root:current mean train loss 3311.995085505129
INFO:root:current train perplexity3.6929891109466553
INFO:root:current mean train loss 3326.07250342431
INFO:root:current train perplexity3.707423686981201
INFO:root:current mean train loss 3327.784041021526
INFO:root:current train perplexity3.71053409576416
INFO:root:current mean train loss 3324.799073398528
INFO:root:current train perplexity3.711564302444458
INFO:root:current mean train loss 3326.619416490113
INFO:root:current train perplexity3.714506149291992
INFO:root:current mean train loss 3326.561897580106
INFO:root:current train perplexity3.711857795715332
INFO:root:current mean train loss 3327.8796203028814
INFO:root:current train perplexity3.711899757385254
INFO:root:current mean train loss 3323.8342634767973
INFO:root:current train perplexity3.7082200050354004
INFO:root:current mean train loss 3323.7305240815485
INFO:root:current train perplexity3.707728624343872


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.43s/it]
INFO:root:final mean train loss: 3322.429395121913
INFO:root:final train perplexity: 3.7091197967529297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.22s/it]
INFO:root:eval mean loss: 4043.758645348515
INFO:root:eval perplexity: 5.130367755889893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [11:00:02<5:33:57, 286.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3323.916015625
INFO:root:current train perplexity3.7055001258850098
INFO:root:current mean train loss 3323.9741421706385
INFO:root:current train perplexity3.7081780433654785
INFO:root:current mean train loss 3321.8639431877614
INFO:root:current train perplexity3.7015187740325928
INFO:root:current mean train loss 3324.093636931923
INFO:root:current train perplexity3.704725503921509
INFO:root:current mean train loss 3323.0176932437003
INFO:root:current train perplexity3.7057790756225586
INFO:root:current mean train loss 3316.4402091003303
INFO:root:current train perplexity3.7024729251861572
INFO:root:current mean train loss 3316.1060277135907
INFO:root:current train perplexity3.7024271488189697
INFO:root:current mean train loss 3316.4698550749536
INFO:root:current train perplexity3.700434446334839
INFO:root:current mean train loss 3320.7907784681356
INFO:root:current train perplexity3.703601837158203
INFO:root:current mean train loss 3320.959750336961
INFO:root:current train perplexity3.704465389251709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.23s/it]
INFO:root:final mean train loss: 3319.08692156884
INFO:root:final train perplexity: 3.7042312622070312
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.89s/it]
INFO:root:eval mean loss: 4045.427542179189
INFO:root:eval perplexity: 5.133830547332764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [11:04:46<5:28:25, 285.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3284.770741564162
INFO:root:current train perplexity3.697652816772461
INFO:root:current mean train loss 3316.2278081154336
INFO:root:current train perplexity3.692981719970703
INFO:root:current mean train loss 3314.7721851673205
INFO:root:current train perplexity3.700289249420166
INFO:root:current mean train loss 3313.251167231403
INFO:root:current train perplexity3.692831516265869
INFO:root:current mean train loss 3315.6468093496574
INFO:root:current train perplexity3.69407320022583
INFO:root:current mean train loss 3316.354449340151
INFO:root:current train perplexity3.6949901580810547
INFO:root:current mean train loss 3317.049339801488
INFO:root:current train perplexity3.6981871128082275
INFO:root:current mean train loss 3318.1933289799827
INFO:root:current train perplexity3.7000722885131836
INFO:root:current mean train loss 3317.40943564603
INFO:root:current train perplexity3.6993300914764404
INFO:root:current mean train loss 3318.6944238487495
INFO:root:current train perplexity3.7000114917755127


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.79s/it]
INFO:root:final mean train loss: 3317.378673922631
INFO:root:final train perplexity: 3.701735496520996
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it]
INFO:root:eval mean loss: 4042.852970204455
INFO:root:eval perplexity: 5.128489017486572
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [11:09:30<5:22:59, 285.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3282.4252485795455
INFO:root:current train perplexity3.670941114425659
INFO:root:current mean train loss 3289.76792937248
INFO:root:current train perplexity3.6826939582824707
INFO:root:current mean train loss 3290.4613022748163
INFO:root:current train perplexity3.6836659908294678
INFO:root:current mean train loss 3293.4129752145686
INFO:root:current train perplexity3.6798582077026367
INFO:root:current mean train loss 3298.8414974673765
INFO:root:current train perplexity3.6809380054473877
INFO:root:current mean train loss 3302.6930307749153
INFO:root:current train perplexity3.683551073074341
INFO:root:current mean train loss 3305.3680641698475
INFO:root:current train perplexity3.6879193782806396
INFO:root:current mean train loss 3307.3477616670116
INFO:root:current train perplexity3.6889429092407227
INFO:root:current mean train loss 3310.8110591419954
INFO:root:current train perplexity3.6907806396484375
INFO:root:current mean train loss 3313.128896535504
INFO:root:current train perplexity3.6936938762664795


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.68s/it]
INFO:root:final mean train loss: 3312.893886812272
INFO:root:final train perplexity: 3.6951913833618164
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.62s/it]
INFO:root:eval mean loss: 4045.942524794991
INFO:root:eval perplexity: 5.1349005699157715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [11:14:14<5:17:59, 284.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3299.6206015935018
INFO:root:current train perplexity3.6893835067749023
INFO:root:current mean train loss 3298.025225867523
INFO:root:current train perplexity3.6830267906188965
INFO:root:current mean train loss 3304.5682062069272
INFO:root:current train perplexity3.689974308013916
INFO:root:current mean train loss 3308.0735219740013
INFO:root:current train perplexity3.692549228668213
INFO:root:current mean train loss 3310.330760453226
INFO:root:current train perplexity3.6915903091430664
INFO:root:current mean train loss 3317.244764202653
INFO:root:current train perplexity3.694136142730713
INFO:root:current mean train loss 3314.1650652072726
INFO:root:current train perplexity3.6906790733337402
INFO:root:current mean train loss 3312.26887210231
INFO:root:current train perplexity3.6894657611846924
INFO:root:current mean train loss 3312.5256992662767
INFO:root:current train perplexity3.6907942295074463
INFO:root:current mean train loss 3314.2708979811623
INFO:root:current train perplexity3.692948579788208


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.29s/it]
INFO:root:final mean train loss: 3311.6842663672664
INFO:root:final train perplexity: 3.6934285163879395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.12s/it]
INFO:root:eval mean loss: 4048.9803025265956
INFO:root:eval perplexity: 5.141211986541748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [11:19:00<5:13:46, 285.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3301.6680615922096
INFO:root:current train perplexity3.676586151123047
INFO:root:current mean train loss 3289.2614446271928
INFO:root:current train perplexity3.6611762046813965
INFO:root:current mean train loss 3291.938658541859
INFO:root:current train perplexity3.666210889816284
INFO:root:current mean train loss 3296.7489076187667
INFO:root:current train perplexity3.6720900535583496
INFO:root:current mean train loss 3300.249575475219
INFO:root:current train perplexity3.6754324436187744
INFO:root:current mean train loss 3306.4005022199267
INFO:root:current train perplexity3.6784462928771973
INFO:root:current mean train loss 3310.6201965059145
INFO:root:current train perplexity3.685331106185913
INFO:root:current mean train loss 3308.9170650180367
INFO:root:current train perplexity3.684530258178711
INFO:root:current mean train loss 3309.593824839893
INFO:root:current train perplexity3.685905933380127
INFO:root:current mean train loss 3308.236391737336
INFO:root:current train perplexity3.6846532821655273


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.83s/it]
INFO:root:final mean train loss: 3306.2801028220883
INFO:root:final train perplexity: 3.6855626106262207
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.14s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.14s/it]
INFO:root:eval mean loss: 4049.054718666888
INFO:root:eval perplexity: 5.1413655281066895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [11:23:47<5:09:35, 285.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3308.056189428402
INFO:root:current train perplexity3.674241781234741
INFO:root:current mean train loss 3301.5524684117495
INFO:root:current train perplexity3.6879403591156006
INFO:root:current mean train loss 3299.8968413978496
INFO:root:current train perplexity3.68050479888916
INFO:root:current mean train loss 3303.714942952259
INFO:root:current train perplexity3.6810519695281982
INFO:root:current mean train loss 3307.0731101782685
INFO:root:current train perplexity3.683320999145508
INFO:root:current mean train loss 3308.631142308263
INFO:root:current train perplexity3.6833858489990234
INFO:root:current mean train loss 3311.791695191688
INFO:root:current train perplexity3.684316396713257
INFO:root:current mean train loss 3311.4059246881015
INFO:root:current train perplexity3.6827006340026855
INFO:root:current mean train loss 3310.721336390785
INFO:root:current train perplexity3.683790683746338
INFO:root:current mean train loss 3307.720065715973
INFO:root:current train perplexity3.6833529472351074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.67s/it]
INFO:root:final mean train loss: 3305.1325310737857
INFO:root:final train perplexity: 3.683893918991089
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.05s/it]
INFO:root:eval mean loss: 4050.141894184951
INFO:root:eval perplexity: 5.143627166748047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [11:28:32<5:04:27, 285.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3280.620327653556
INFO:root:current train perplexity3.6636626720428467
INFO:root:current mean train loss 3297.3036965762867
INFO:root:current train perplexity3.673107862472534
INFO:root:current mean train loss 3295.2595538096148
INFO:root:current train perplexity3.669151782989502
INFO:root:current mean train loss 3302.3226857739824
INFO:root:current train perplexity3.673767328262329
INFO:root:current mean train loss 3306.0906568836626
INFO:root:current train perplexity3.676328420639038
INFO:root:current mean train loss 3305.6332151032793
INFO:root:current train perplexity3.6749496459960938
INFO:root:current mean train loss 3304.4008256004367
INFO:root:current train perplexity3.6742048263549805
INFO:root:current mean train loss 3302.056448290581
INFO:root:current train perplexity3.67336368560791
INFO:root:current mean train loss 3302.311675096445
INFO:root:current train perplexity3.674640655517578
INFO:root:current mean train loss 3302.8042469585075
INFO:root:current train perplexity3.6769979000091553


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.43s/it]
INFO:root:final mean train loss: 3300.5425850037605
INFO:root:final train perplexity: 3.677229166030884
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.35s/it]
INFO:root:eval mean loss: 4051.0029002521055
INFO:root:eval perplexity: 5.145419597625732
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [11:33:17<4:59:30, 285.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3281.2931614925988
INFO:root:current train perplexity3.6557891368865967
INFO:root:current mean train loss 3294.7801094250804
INFO:root:current train perplexity3.662623643875122
INFO:root:current mean train loss 3297.756806971663
INFO:root:current train perplexity3.669188976287842
INFO:root:current mean train loss 3298.48251953125
INFO:root:current train perplexity3.673501968383789
INFO:root:current mean train loss 3297.0317437065974
INFO:root:current train perplexity3.6738181114196777
INFO:root:current mean train loss 3299.773873670562
INFO:root:current train perplexity3.676487684249878
INFO:root:current mean train loss 3297.8325012646133
INFO:root:current train perplexity3.674598217010498
INFO:root:current mean train loss 3301.7250675609275
INFO:root:current train perplexity3.6781327724456787
INFO:root:current mean train loss 3301.2798819941513
INFO:root:current train perplexity3.6761279106140137


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.25s/it]
INFO:root:final mean train loss: 3299.468313032581
INFO:root:final train perplexity: 3.675671100616455
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.20s/it]
INFO:root:eval mean loss: 4054.4550175227173
INFO:root:eval perplexity: 5.1526055335998535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [11:38:03<4:55:06, 285.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3371.3509114583335
INFO:root:current train perplexity3.555084466934204
INFO:root:current mean train loss 3273.9440680939015
INFO:root:current train perplexity3.638389825820923
INFO:root:current mean train loss 3279.207246526709
INFO:root:current train perplexity3.64755916595459
INFO:root:current mean train loss 3285.732959306673
INFO:root:current train perplexity3.6578776836395264
INFO:root:current mean train loss 3292.143523185484
INFO:root:current train perplexity3.658374786376953
INFO:root:current mean train loss 3293.646960036655
INFO:root:current train perplexity3.6594600677490234
INFO:root:current mean train loss 3294.7488392186206
INFO:root:current train perplexity3.660771131515503
INFO:root:current mean train loss 3295.0820378483954
INFO:root:current train perplexity3.6633870601654053
INFO:root:current mean train loss 3297.6830057450575
INFO:root:current train perplexity3.666593313217163
INFO:root:current mean train loss 3298.1098294854824
INFO:root:current train perplexity3.669034004211426


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.81s/it]
INFO:root:final mean train loss: 3296.7036276786557
INFO:root:final train perplexity: 3.671663999557495
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.11s/it]
INFO:root:eval mean loss: 4053.974986840647
INFO:root:eval perplexity: 5.151605129241943
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [11:42:47<4:49:50, 285.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3337.958185369318
INFO:root:current train perplexity3.5972137451171875
INFO:root:current mean train loss 3290.411313168637
INFO:root:current train perplexity3.661937713623047
INFO:root:current mean train loss 3287.963996778732
INFO:root:current train perplexity3.653993606567383
INFO:root:current mean train loss 3277.610306816469
INFO:root:current train perplexity3.642889976501465
INFO:root:current mean train loss 3282.2343381710007
INFO:root:current train perplexity3.6478426456451416
INFO:root:current mean train loss 3284.6981867661448
INFO:root:current train perplexity3.6564128398895264
INFO:root:current mean train loss 3286.8713486791635
INFO:root:current train perplexity3.656562328338623
INFO:root:current mean train loss 3289.9302887520876
INFO:root:current train perplexity3.65773868560791
INFO:root:current mean train loss 3292.0285801070245
INFO:root:current train perplexity3.6622979640960693
INFO:root:current mean train loss 3294.332023478235
INFO:root:current train perplexity3.663513660430908


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.07s/it]
INFO:root:final mean train loss: 3291.9541746570217
INFO:root:final train perplexity: 3.6647908687591553
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.00s/it]
INFO:root:eval mean loss: 4054.7627074329566
INFO:root:eval perplexity: 5.153247356414795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [11:47:31<4:44:45, 284.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3276.1977796052633
INFO:root:current train perplexity3.6474363803863525
INFO:root:current mean train loss 3263.18835756959
INFO:root:current train perplexity3.634091854095459
INFO:root:current mean train loss 3265.7531047106877
INFO:root:current train perplexity3.6476778984069824
INFO:root:current mean train loss 3274.6018280698963
INFO:root:current train perplexity3.6498849391937256
INFO:root:current mean train loss 3279.2409061987246
INFO:root:current train perplexity3.6523749828338623
INFO:root:current mean train loss 3283.0219350237835
INFO:root:current train perplexity3.654982566833496
INFO:root:current mean train loss 3281.7050059477233
INFO:root:current train perplexity3.653900384902954
INFO:root:current mean train loss 3284.9584268243652
INFO:root:current train perplexity3.655590295791626
INFO:root:current mean train loss 3286.959173665961
INFO:root:current train perplexity3.6568522453308105
INFO:root:current mean train loss 3288.685116241754
INFO:root:current train perplexity3.6570637226104736


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:23<00:00, 263.53s/it]
INFO:root:final mean train loss: 3289.019182882001
INFO:root:final train perplexity: 3.6605489253997803
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.32s/it]
INFO:root:eval mean loss: 4054.302542179189
INFO:root:eval perplexity: 5.152288436889648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [11:52:15<4:39:43, 284.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3240.423828125
INFO:root:current train perplexity3.6601908206939697
INFO:root:current mean train loss 3280.276976577879
INFO:root:current train perplexity3.6552958488464355
INFO:root:current mean train loss 3281.360289182957
INFO:root:current train perplexity3.6554195880889893
INFO:root:current mean train loss 3283.3848425255637
INFO:root:current train perplexity3.6600425243377686
INFO:root:current mean train loss 3287.83356985052
INFO:root:current train perplexity3.6596951484680176
INFO:root:current mean train loss 3287.4505557326256
INFO:root:current train perplexity3.656851291656494
INFO:root:current mean train loss 3287.9005888189045
INFO:root:current train perplexity3.658053398132324
INFO:root:current mean train loss 3288.3744291072044
INFO:root:current train perplexity3.6588034629821777
INFO:root:current mean train loss 3289.162522377097
INFO:root:current train perplexity3.660121440887451
INFO:root:current mean train loss 3289.2642894691376
INFO:root:current train perplexity3.6584320068359375


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.37s/it]
INFO:root:final mean train loss: 3287.641415934409
INFO:root:final train perplexity: 3.658560037612915
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.47s/it]
INFO:root:eval mean loss: 4056.792244985594
INFO:root:eval perplexity: 5.157477378845215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [11:56:54<4:33:20, 282.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3344.1060128348213
INFO:root:current train perplexity3.6545181274414062
INFO:root:current mean train loss 3311.1402000144676
INFO:root:current train perplexity3.643451452255249
INFO:root:current mean train loss 3297.817691364694
INFO:root:current train perplexity3.642242431640625
INFO:root:current mean train loss 3296.859337103545
INFO:root:current train perplexity3.645857572555542
INFO:root:current mean train loss 3289.845447198276
INFO:root:current train perplexity3.647646903991699
INFO:root:current mean train loss 3294.416851635514
INFO:root:current train perplexity3.652378797531128
INFO:root:current mean train loss 3290.3690172090305
INFO:root:current train perplexity3.6485984325408936
INFO:root:current mean train loss 3287.6430763711733
INFO:root:current train perplexity3.6494932174682617
INFO:root:current mean train loss 3285.9123181371633
INFO:root:current train perplexity3.651573419570923
INFO:root:current mean train loss 3286.239525975518
INFO:root:current train perplexity3.653636932373047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.67s/it]
INFO:root:final mean train loss: 3285.0187809851864
INFO:root:final train perplexity: 3.6547765731811523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.52s/it]
INFO:root:eval mean loss: 4057.0090989998894
INFO:root:eval perplexity: 5.157930850982666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [12:01:33<4:27:35, 281.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3282.819091796875
INFO:root:current train perplexity3.636014699935913
INFO:root:current mean train loss 3271.883911986451
INFO:root:current train perplexity3.6294984817504883
INFO:root:current mean train loss 3269.687498995306
INFO:root:current train perplexity3.631561040878296
INFO:root:current mean train loss 3275.413585607234
INFO:root:current train perplexity3.633925437927246
INFO:root:current mean train loss 3276.296665579148
INFO:root:current train perplexity3.6369569301605225
INFO:root:current mean train loss 3277.6608702376843
INFO:root:current train perplexity3.639240026473999
INFO:root:current mean train loss 3280.9445702061867
INFO:root:current train perplexity3.6429429054260254
INFO:root:current mean train loss 3278.530199176691
INFO:root:current train perplexity3.6439502239227295
INFO:root:current mean train loss 3280.703480929808
INFO:root:current train perplexity3.646488666534424
INFO:root:current mean train loss 3281.417640726488
INFO:root:current train perplexity3.6487581729888916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.45s/it]
INFO:root:final mean train loss: 3281.2008673144924
INFO:root:final train perplexity: 3.649275779724121
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.84s/it]
INFO:root:eval mean loss: 4059.3080015791224
INFO:root:eval perplexity: 5.162728309631348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [12:06:12<4:22:12, 280.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3276.677801393995
INFO:root:current train perplexity3.6156375408172607
INFO:root:current mean train loss 3271.579384506933
INFO:root:current train perplexity3.6298320293426514
INFO:root:current mean train loss 3273.402582054594
INFO:root:current train perplexity3.630521774291992
INFO:root:current mean train loss 3269.6714868790064
INFO:root:current train perplexity3.6318061351776123
INFO:root:current mean train loss 3273.562505954649
INFO:root:current train perplexity3.6339051723480225
INFO:root:current mean train loss 3274.6985186734346
INFO:root:current train perplexity3.6377665996551514
INFO:root:current mean train loss 3279.27390703005
INFO:root:current train perplexity3.641314744949341
INFO:root:current mean train loss 3281.468428488578
INFO:root:current train perplexity3.644338369369507
INFO:root:current mean train loss 3280.5052890441393
INFO:root:current train perplexity3.6438746452331543
INFO:root:current mean train loss 3279.1654315872274
INFO:root:current train perplexity3.6441025733947754


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.95s/it]
INFO:root:final mean train loss: 3278.4278849324874
INFO:root:final train perplexity: 3.6452853679656982
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.37s/it]
INFO:root:eval mean loss: 4059.5246201102614
INFO:root:eval perplexity: 5.16317892074585
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [12:10:50<4:16:46, 280.13s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3285.0252871755824
INFO:root:current train perplexity3.633514881134033
INFO:root:current mean train loss 3274.458724879619
INFO:root:current train perplexity3.6297719478607178
INFO:root:current mean train loss 3269.352640866313
INFO:root:current train perplexity3.63197922706604
INFO:root:current mean train loss 3271.050966905684
INFO:root:current train perplexity3.6357791423797607
INFO:root:current mean train loss 3273.8499524484273
INFO:root:current train perplexity3.637054920196533
INFO:root:current mean train loss 3274.998765757771
INFO:root:current train perplexity3.63849139213562
INFO:root:current mean train loss 3279.8397459455614
INFO:root:current train perplexity3.6409571170806885
INFO:root:current mean train loss 3279.57558741714
INFO:root:current train perplexity3.64269757270813
INFO:root:current mean train loss 3280.2784234825194
INFO:root:current train perplexity3.6436305046081543
INFO:root:current mean train loss 3279.314347984147
INFO:root:current train perplexity3.6434175968170166


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.92s/it]
INFO:root:final mean train loss: 3277.548230940296
INFO:root:final train perplexity: 3.6440210342407227
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it]
INFO:root:eval mean loss: 4061.4479911209
INFO:root:eval perplexity: 5.167197227478027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [12:15:33<4:12:44, 280.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3271.055529238573
INFO:root:current train perplexity3.6312713623046875
INFO:root:current mean train loss 3286.5159860942176
INFO:root:current train perplexity3.6299004554748535
INFO:root:current mean train loss 3281.004843493972
INFO:root:current train perplexity3.6330676078796387
INFO:root:current mean train loss 3281.870571541851
INFO:root:current train perplexity3.6306378841400146
INFO:root:current mean train loss 3277.7192037774357
INFO:root:current train perplexity3.63085675239563
INFO:root:current mean train loss 3275.5398974006557
INFO:root:current train perplexity3.6334474086761475
INFO:root:current mean train loss 3277.1638029862024
INFO:root:current train perplexity3.6353344917297363
INFO:root:current mean train loss 3277.0196553893006
INFO:root:current train perplexity3.6352787017822266
INFO:root:current mean train loss 3278.076852202278
INFO:root:current train perplexity3.6388561725616455
INFO:root:current mean train loss 3278.5317193458345
INFO:root:current train perplexity3.6403863430023193


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.17s/it]
INFO:root:final mean train loss: 3275.334361660865
INFO:root:final train perplexity: 3.640839099884033
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.46s/it]
INFO:root:eval mean loss: 4062.803181100399
INFO:root:eval perplexity: 5.170030117034912
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [12:20:11<4:07:27, 280.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3250.895817057292
INFO:root:current train perplexity3.610976457595825
INFO:root:current mean train loss 3268.2190931919645
INFO:root:current train perplexity3.630452871322632
INFO:root:current mean train loss 3274.389814453125
INFO:root:current train perplexity3.6369948387145996
INFO:root:current mean train loss 3267.9842786458335
INFO:root:current train perplexity3.630176067352295
INFO:root:current mean train loss 3267.6655437911186
INFO:root:current train perplexity3.6276280879974365
INFO:root:current mean train loss 3269.1777500849184
INFO:root:current train perplexity3.6279056072235107
INFO:root:current mean train loss 3271.91646484375
INFO:root:current train perplexity3.6296563148498535
INFO:root:current mean train loss 3272.9088549017138
INFO:root:current train perplexity3.6297428607940674
INFO:root:current mean train loss 3276.4950772879465
INFO:root:current train perplexity3.634463310241699
INFO:root:current mean train loss 3274.7559825721155
INFO:root:current train perplexity3.6352882385253906


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.91s/it]
INFO:root:final mean train loss: 3271.9212698782644
INFO:root:final train perplexity: 3.6359400749206543
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it]
INFO:root:eval mean loss: 4061.935254252549
INFO:root:eval perplexity: 5.168214797973633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [12:24:49<4:02:17, 279.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3273.458093114646
INFO:root:current train perplexity3.6339406967163086
INFO:root:current mean train loss 3268.33584011057
INFO:root:current train perplexity3.63425874710083
INFO:root:current mean train loss 3268.7298527909675
INFO:root:current train perplexity3.629897356033325
INFO:root:current mean train loss 3272.234285120553
INFO:root:current train perplexity3.631049871444702
INFO:root:current mean train loss 3272.5305458842845
INFO:root:current train perplexity3.6302733421325684
INFO:root:current mean train loss 3270.224301581931
INFO:root:current train perplexity3.631187915802002
INFO:root:current mean train loss 3270.989969859535
INFO:root:current train perplexity3.6315693855285645
INFO:root:current mean train loss 3270.6784256390683
INFO:root:current train perplexity3.6313228607177734
INFO:root:current mean train loss 3270.983583685766
INFO:root:current train perplexity3.6319382190704346
INFO:root:current mean train loss 3272.594271810227
INFO:root:current train perplexity3.633148431777954


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.69s/it]
INFO:root:final mean train loss: 3270.5052103227185
INFO:root:final train perplexity: 3.633908748626709
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.26s/it]
INFO:root:eval mean loss: 4062.739108904034
INFO:root:eval perplexity: 5.169894695281982
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [12:29:28<3:57:29, 279.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3265.7109026227677
INFO:root:current train perplexity3.6175477504730225
INFO:root:current mean train loss 3261.0433534951735
INFO:root:current train perplexity3.6161768436431885
INFO:root:current mean train loss 3263.598847589132
INFO:root:current train perplexity3.6213948726654053
INFO:root:current mean train loss 3264.5894990808824
INFO:root:current train perplexity3.6211111545562744
INFO:root:current mean train loss 3264.3324161071155
INFO:root:current train perplexity3.6218667030334473
INFO:root:current mean train loss 3264.2316762340047
INFO:root:current train perplexity3.623461961746216
INFO:root:current mean train loss 3266.971854225081
INFO:root:current train perplexity3.62481951713562
INFO:root:current mean train loss 3266.9865926363977
INFO:root:current train perplexity3.627952814102173
INFO:root:current mean train loss 3268.4998975212193
INFO:root:current train perplexity3.628467559814453
INFO:root:current mean train loss 3270.3152358038756
INFO:root:current train perplexity3.630021095275879


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.43s/it]
INFO:root:final mean train loss: 3267.782552965226
INFO:root:final train perplexity: 3.630007266998291
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it]
INFO:root:eval mean loss: 4064.865118364916
INFO:root:eval perplexity: 5.174341678619385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [12:34:06<3:52:28, 278.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3263.6260875355115
INFO:root:current train perplexity3.6380980014801025
INFO:root:current mean train loss 3263.293902373194
INFO:root:current train perplexity3.6204164028167725
INFO:root:current mean train loss 3262.4528465653743
INFO:root:current train perplexity3.6188671588897705
INFO:root:current mean train loss 3260.4493998668545
INFO:root:current train perplexity3.6171460151672363
INFO:root:current mean train loss 3262.8500438376755
INFO:root:current train perplexity3.6189165115356445
INFO:root:current mean train loss 3264.812710719037
INFO:root:current train perplexity3.6240365505218506
INFO:root:current mean train loss 3263.087122228183
INFO:root:current train perplexity3.6244356632232666
INFO:root:current mean train loss 3261.0275558070634
INFO:root:current train perplexity3.6235721111297607
INFO:root:current mean train loss 3264.5085470944277
INFO:root:current train perplexity3.6239945888519287


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.54s/it]
INFO:root:final mean train loss: 3264.3893075143137
INFO:root:final train perplexity: 3.6251513957977295
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.54s/it]
INFO:root:eval mean loss: 4065.4535786513743
INFO:root:eval perplexity: 5.175573825836182
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [12:38:50<3:49:04, 280.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3245.775704520089
INFO:root:current train perplexity3.606879472732544
INFO:root:current mean train loss 3262.862822630695
INFO:root:current train perplexity3.6192679405212402
INFO:root:current mean train loss 3260.0371801403985
INFO:root:current train perplexity3.6199822425842285
INFO:root:current mean train loss 3257.663547975621
INFO:root:current train perplexity3.622297763824463
INFO:root:current mean train loss 3258.1354800512518
INFO:root:current train perplexity3.621350049972534
INFO:root:current mean train loss 3260.1819642196747
INFO:root:current train perplexity3.6187944412231445
INFO:root:current mean train loss 3259.861350648682
INFO:root:current train perplexity3.6191797256469727
INFO:root:current mean train loss 3264.757935088291
INFO:root:current train perplexity3.6199347972869873
INFO:root:current mean train loss 3265.5018729549065
INFO:root:current train perplexity3.620978355407715
INFO:root:current mean train loss 3268.0430201621934
INFO:root:current train perplexity3.6229989528656006


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.04s/it]
INFO:root:final mean train loss: 3262.522225656817
INFO:root:final train perplexity: 3.6224820613861084
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.43s/it]
INFO:root:eval mean loss: 4065.02402620789
INFO:root:eval perplexity: 5.174673557281494
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [12:43:29<3:43:53, 279.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3273.1423828125
INFO:root:current train perplexity3.609194278717041
INFO:root:current mean train loss 3259.4873726222827
INFO:root:current train perplexity3.612267255783081
INFO:root:current mean train loss 3258.261626771439
INFO:root:current train perplexity3.614238739013672
INFO:root:current mean train loss 3252.3247248573907
INFO:root:current train perplexity3.6105809211730957
INFO:root:current mean train loss 3257.772429169804
INFO:root:current train perplexity3.612577199935913
INFO:root:current mean train loss 3260.908337283829
INFO:root:current train perplexity3.6168694496154785
INFO:root:current mean train loss 3262.3542281980435
INFO:root:current train perplexity3.614835023880005
INFO:root:current mean train loss 3265.143902972028
INFO:root:current train perplexity3.6191136837005615
INFO:root:current mean train loss 3263.9419334739264
INFO:root:current train perplexity3.620655059814453
INFO:root:current mean train loss 3264.146200478142
INFO:root:current train perplexity3.621553421020508


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.82s/it]
INFO:root:final mean train loss: 3261.713677990821
INFO:root:final train perplexity: 3.6213271617889404
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.75s/it]
INFO:root:eval mean loss: 4066.84517848238
INFO:root:eval perplexity: 5.1784868240356445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [12:48:07<3:38:55, 279.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.2592136548915
INFO:root:current train perplexity3.5734212398529053
INFO:root:current mean train loss 3250.0506323837653
INFO:root:current train perplexity3.6052980422973633
INFO:root:current mean train loss 3250.200304792601
INFO:root:current train perplexity3.600766658782959
INFO:root:current mean train loss 3249.9833273872873
INFO:root:current train perplexity3.5996687412261963
INFO:root:current mean train loss 3251.099115899269
INFO:root:current train perplexity3.6032567024230957
INFO:root:current mean train loss 3253.678877587984
INFO:root:current train perplexity3.6046302318573
INFO:root:current mean train loss 3253.63739748445
INFO:root:current train perplexity3.6077046394348145
INFO:root:current mean train loss 3257.3128397032765
INFO:root:current train perplexity3.6116549968719482
INFO:root:current mean train loss 3259.6371201136276
INFO:root:current train perplexity3.61198091506958
INFO:root:current mean train loss 3260.2011366954735
INFO:root:current train perplexity3.6140120029449463


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.22s/it]
INFO:root:final mean train loss: 3257.6361339938258
INFO:root:final train perplexity: 3.6155056953430176
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.93s/it]
INFO:root:eval mean loss: 4069.108507521609
INFO:root:eval perplexity: 5.183228015899658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [12:52:49<3:34:51, 280.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3283.3231398059474
INFO:root:current train perplexity3.6446292400360107
INFO:root:current mean train loss 3256.8158564676764
INFO:root:current train perplexity3.618812084197998
INFO:root:current mean train loss 3251.3383398014744
INFO:root:current train perplexity3.6041440963745117
INFO:root:current mean train loss 3253.978199201048
INFO:root:current train perplexity3.609088897705078
INFO:root:current mean train loss 3251.2830167624347
INFO:root:current train perplexity3.6123619079589844
INFO:root:current mean train loss 3250.1022710135653
INFO:root:current train perplexity3.6108386516571045
INFO:root:current mean train loss 3254.6159621539473
INFO:root:current train perplexity3.6147468090057373
INFO:root:current mean train loss 3257.292867553544
INFO:root:current train perplexity3.6170623302459717
INFO:root:current mean train loss 3260.064314455475
INFO:root:current train perplexity3.614781141281128
INFO:root:current mean train loss 3258.2976551486136
INFO:root:current train perplexity3.613898754119873


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.99s/it]
INFO:root:final mean train loss: 3256.541509689823
INFO:root:final train perplexity: 3.6139445304870605
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.39s/it]
INFO:root:eval mean loss: 4068.7457699606603
INFO:root:eval perplexity: 5.182468414306641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [12:57:28<3:29:47, 279.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3292.3824994991987
INFO:root:current train perplexity3.610511541366577
INFO:root:current mean train loss 3253.2971683200317
INFO:root:current train perplexity3.5961015224456787
INFO:root:current mean train loss 3256.509624656773
INFO:root:current train perplexity3.603694438934326
INFO:root:current mean train loss 3253.437518004471
INFO:root:current train perplexity3.6065244674682617
INFO:root:current mean train loss 3250.2389497170416
INFO:root:current train perplexity3.609957218170166
INFO:root:current mean train loss 3251.9715546730054
INFO:root:current train perplexity3.607964038848877
INFO:root:current mean train loss 3251.7553363256798
INFO:root:current train perplexity3.6059792041778564
INFO:root:current mean train loss 3254.036978549983
INFO:root:current train perplexity3.6066782474517822
INFO:root:current mean train loss 3254.2063058118856
INFO:root:current train perplexity3.6049463748931885
INFO:root:current mean train loss 3255.8630763694755
INFO:root:current train perplexity3.6092844009399414


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:18<00:00, 258.80s/it]
INFO:root:final mean train loss: 3253.3844475899973
INFO:root:final train perplexity: 3.6094460487365723
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.40s/it]
INFO:root:eval mean loss: 4069.024086810173
INFO:root:eval perplexity: 5.183050632476807
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [13:02:06<3:24:47, 279.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3248.6314463513963
INFO:root:current train perplexity3.564692735671997
INFO:root:current mean train loss 3258.5388715056333
INFO:root:current train perplexity3.5923259258270264
INFO:root:current mean train loss 3248.5405451353745
INFO:root:current train perplexity3.592912197113037
INFO:root:current mean train loss 3250.885568404404
INFO:root:current train perplexity3.5968785285949707
INFO:root:current mean train loss 3252.8627012111997
INFO:root:current train perplexity3.6014328002929688
INFO:root:current mean train loss 3248.3270591721894
INFO:root:current train perplexity3.600569486618042
INFO:root:current mean train loss 3248.57905703608
INFO:root:current train perplexity3.6022701263427734
INFO:root:current mean train loss 3251.26585770164
INFO:root:current train perplexity3.603868246078491
INFO:root:current mean train loss 3254.3951261806374
INFO:root:current train perplexity3.6058411598205566
INFO:root:current mean train loss 3255.586689257194
INFO:root:current train perplexity3.6077024936676025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.64s/it]
INFO:root:final mean train loss: 3252.838146578881
INFO:root:final train perplexity: 3.608668088912964
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it]
INFO:root:eval mean loss: 4070.755085397274
INFO:root:eval perplexity: 5.186680793762207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [13:06:46<3:20:21, 279.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3227.097722833807
INFO:root:current train perplexity3.616579055786133
INFO:root:current mean train loss 3224.1286447832663
INFO:root:current train perplexity3.595612049102783
INFO:root:current mean train loss 3238.8886460248164
INFO:root:current train perplexity3.600600481033325
INFO:root:current mean train loss 3243.6329190966107
INFO:root:current train perplexity3.603243350982666
INFO:root:current mean train loss 3244.9421638907966
INFO:root:current train perplexity3.602733612060547
INFO:root:current mean train loss 3248.1388069221566
INFO:root:current train perplexity3.6020100116729736
INFO:root:current mean train loss 3252.211511510019
INFO:root:current train perplexity3.6054325103759766
INFO:root:current mean train loss 3252.4390990402526
INFO:root:current train perplexity3.6052086353302
INFO:root:current mean train loss 3254.45146484375
INFO:root:current train perplexity3.6074135303497314
INFO:root:current mean train loss 3253.7350506687662
INFO:root:current train perplexity3.60599422454834


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.96s/it]
INFO:root:final mean train loss: 3251.049460687945
INFO:root:final train perplexity: 3.6061224937438965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.65s/it]
INFO:root:eval mean loss: 4071.6489292442375
INFO:root:eval perplexity: 5.1885552406311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [13:11:29<3:16:20, 280.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3264.3954031808034
INFO:root:current train perplexity3.599205255508423
INFO:root:current mean train loss 3255.4231481259585
INFO:root:current train perplexity3.590355396270752
INFO:root:current mean train loss 3251.221248960314
INFO:root:current train perplexity3.5901384353637695
INFO:root:current mean train loss 3242.63906397964
INFO:root:current train perplexity3.5912742614746094
INFO:root:current mean train loss 3246.398493393966
INFO:root:current train perplexity3.5988497734069824
INFO:root:current mean train loss 3248.278294727603
INFO:root:current train perplexity3.59909987449646
INFO:root:current mean train loss 3246.600383775806
INFO:root:current train perplexity3.60129714012146
INFO:root:current mean train loss 3250.7469771998894
INFO:root:current train perplexity3.6012444496154785
INFO:root:current mean train loss 3248.9806220804967
INFO:root:current train perplexity3.600228786468506
INFO:root:current mean train loss 3248.9121859383113
INFO:root:current train perplexity3.599321126937866


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.29s/it]
INFO:root:final mean train loss: 3246.4309190934705
INFO:root:final train perplexity: 3.599557638168335
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.82s/it]
INFO:root:eval mean loss: 4072.488374750665
INFO:root:eval perplexity: 5.190317153930664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [13:16:09<3:11:34, 280.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3252.7440374669895
INFO:root:current train perplexity3.606771469116211
INFO:root:current mean train loss 3237.2907572071454
INFO:root:current train perplexity3.583467960357666
INFO:root:current mean train loss 3239.492423532634
INFO:root:current train perplexity3.5857832431793213
INFO:root:current mean train loss 3236.0018471771814
INFO:root:current train perplexity3.590747356414795
INFO:root:current mean train loss 3242.085150651871
INFO:root:current train perplexity3.59468412399292
INFO:root:current mean train loss 3242.501844095474
INFO:root:current train perplexity3.5969414710998535
INFO:root:current mean train loss 3244.038496719565
INFO:root:current train perplexity3.5940749645233154
INFO:root:current mean train loss 3245.707314655784
INFO:root:current train perplexity3.5946497917175293
INFO:root:current mean train loss 3247.4149572151264
INFO:root:current train perplexity3.598757028579712
INFO:root:current mean train loss 3247.4850191691876
INFO:root:current train perplexity3.5984609127044678


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.01s/it]
INFO:root:final mean train loss: 3245.6600474080733
INFO:root:final train perplexity: 3.5984630584716797
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it]
INFO:root:eval mean loss: 4073.0597001745346
INFO:root:eval perplexity: 5.191515922546387
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [13:20:48<3:06:33, 279.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3238.2692562054986
INFO:root:current train perplexity3.576967477798462
INFO:root:current mean train loss 3234.737325146212
INFO:root:current train perplexity3.5916194915771484
INFO:root:current mean train loss 3241.0091968385977
INFO:root:current train perplexity3.5943329334259033
INFO:root:current mean train loss 3246.495546849233
INFO:root:current train perplexity3.59250807762146
INFO:root:current mean train loss 3245.3215484937696
INFO:root:current train perplexity3.5924878120422363
INFO:root:current mean train loss 3247.3107897506475
INFO:root:current train perplexity3.5933079719543457
INFO:root:current mean train loss 3242.8213149507546
INFO:root:current train perplexity3.593200922012329
INFO:root:current mean train loss 3244.6907305539953
INFO:root:current train perplexity3.5945632457733154
INFO:root:current mean train loss 3243.440971018647
INFO:root:current train perplexity3.5952444076538086
INFO:root:current mean train loss 3245.306393741222
INFO:root:current train perplexity3.595533609390259


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.88s/it]
INFO:root:final mean train loss: 3243.441977654734
INFO:root:final train perplexity: 3.595315456390381
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.34s/it]
INFO:root:eval mean loss: 4073.6779404227614
INFO:root:eval perplexity: 5.192814826965332
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [13:25:27<3:01:46, 279.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3235.045236170977
INFO:root:current train perplexity3.568307399749756
INFO:root:current mean train loss 3233.039821033172
INFO:root:current train perplexity3.5741493701934814
INFO:root:current mean train loss 3239.1056550454596
INFO:root:current train perplexity3.577871322631836
INFO:root:current mean train loss 3244.6158765847063
INFO:root:current train perplexity3.5868115425109863
INFO:root:current mean train loss 3242.34768382235
INFO:root:current train perplexity3.5874781608581543
INFO:root:current mean train loss 3243.829678433108
INFO:root:current train perplexity3.587512254714966
INFO:root:current mean train loss 3242.5372827965793
INFO:root:current train perplexity3.5876946449279785
INFO:root:current mean train loss 3243.2483223475224
INFO:root:current train perplexity3.5893354415893555
INFO:root:current mean train loss 3243.6747457854776
INFO:root:current train perplexity3.591590404510498
INFO:root:current mean train loss 3245.973758469478
INFO:root:current train perplexity3.5954208374023438


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.18s/it]
INFO:root:final mean train loss: 3243.151579149308
INFO:root:final train perplexity: 3.5949032306671143
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.55s/it]
INFO:root:eval mean loss: 4074.2620511968084
INFO:root:eval perplexity: 5.1940412521362305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [13:30:06<2:56:55, 279.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3239.590825452303
INFO:root:current train perplexity3.593243360519409
INFO:root:current mean train loss 3248.0403896233975
INFO:root:current train perplexity3.60068941116333
INFO:root:current mean train loss 3243.2076850503177
INFO:root:current train perplexity3.589452028274536
INFO:root:current mean train loss 3246.4562771954115
INFO:root:current train perplexity3.5868778228759766
INFO:root:current mean train loss 3248.7772288312817
INFO:root:current train perplexity3.591048240661621
INFO:root:current mean train loss 3247.991700449711
INFO:root:current train perplexity3.590851068496704
INFO:root:current mean train loss 3247.2225561347796
INFO:root:current train perplexity3.589949131011963
INFO:root:current mean train loss 3244.8374591563484
INFO:root:current train perplexity3.5907657146453857
INFO:root:current mean train loss 3245.012238128492
INFO:root:current train perplexity3.591905117034912


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.24s/it]
INFO:root:final mean train loss: 3241.438542642901
INFO:root:final train perplexity: 3.5924744606018066
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.48s/it]
INFO:root:eval mean loss: 4074.830093708444
INFO:root:eval perplexity: 5.195233345031738
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [13:34:45<2:52:19, 279.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3243.3270670572915
INFO:root:current train perplexity3.5735437870025635
INFO:root:current mean train loss 3211.245930199484
INFO:root:current train perplexity3.582669734954834
INFO:root:current mean train loss 3236.0113351004466
INFO:root:current train perplexity3.596264123916626
INFO:root:current mean train loss 3234.4675832817657
INFO:root:current train perplexity3.5932531356811523
INFO:root:current mean train loss 3233.957575871394
INFO:root:current train perplexity3.594594717025757
INFO:root:current mean train loss 3235.9408009948124
INFO:root:current train perplexity3.593289613723755
INFO:root:current mean train loss 3237.907737111966
INFO:root:current train perplexity3.590202569961548
INFO:root:current mean train loss 3237.172117404205
INFO:root:current train perplexity3.586972236633301
INFO:root:current mean train loss 3238.1609935641736
INFO:root:current train perplexity3.5885415077209473
INFO:root:current mean train loss 3242.259294647183
INFO:root:current train perplexity3.590054750442505


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 258.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:17<00:00, 258.00s/it]
INFO:root:final mean train loss: 3240.0704984972554
INFO:root:final train perplexity: 3.590535879135132
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.79s/it]
INFO:root:eval mean loss: 4076.335416320368
INFO:root:eval perplexity: 5.198398113250732
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [13:39:23<2:47:21, 278.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3254.097745028409
INFO:root:current train perplexity3.5729546546936035
INFO:root:current mean train loss 3236.616549655124
INFO:root:current train perplexity3.567974805831909
INFO:root:current mean train loss 3240.201926281102
INFO:root:current train perplexity3.5765304565429688
INFO:root:current mean train loss 3240.705991886053
INFO:root:current train perplexity3.584822177886963
INFO:root:current mean train loss 3244.312138244183
INFO:root:current train perplexity3.589587926864624
INFO:root:current mean train loss 3240.969846482846
INFO:root:current train perplexity3.5898990631103516
INFO:root:current mean train loss 3237.936745601473
INFO:root:current train perplexity3.5859782695770264
INFO:root:current mean train loss 3236.9757648366076
INFO:root:current train perplexity3.5850961208343506
INFO:root:current mean train loss 3238.2225620255663
INFO:root:current train perplexity3.584780693054199
INFO:root:current mean train loss 3238.552885790426
INFO:root:current train perplexity3.585019588470459


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.08s/it]
INFO:root:final mean train loss: 3235.4888651447914
INFO:root:final train perplexity: 3.584052085876465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it]
INFO:root:eval mean loss: 4075.07925220246
INFO:root:eval perplexity: 5.195756912231445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [13:44:05<2:43:10, 279.73s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3254.6058670847037
INFO:root:current train perplexity3.599076986312866
INFO:root:current mean train loss 3238.647444524685
INFO:root:current train perplexity3.5860533714294434
INFO:root:current mean train loss 3243.357138716467
INFO:root:current train perplexity3.59413743019104
INFO:root:current mean train loss 3241.823188614322
INFO:root:current train perplexity3.5876150131225586
INFO:root:current mean train loss 3237.9076798832784
INFO:root:current train perplexity3.5859599113464355
INFO:root:current mean train loss 3235.4407909591764
INFO:root:current train perplexity3.5848045349121094
INFO:root:current mean train loss 3237.6421150860765
INFO:root:current train perplexity3.5849459171295166
INFO:root:current mean train loss 3240.0022661954536
INFO:root:current train perplexity3.58516263961792
INFO:root:current mean train loss 3239.8995824867407
INFO:root:current train perplexity3.5864291191101074
INFO:root:current mean train loss 3237.7873123384793
INFO:root:current train perplexity3.583021640777588


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:19<00:00, 259.76s/it]
INFO:root:final mean train loss: 3234.621513305172
INFO:root:final train perplexity: 3.5828256607055664
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.96s/it]
INFO:root:eval mean loss: 4076.9453990746897
INFO:root:eval perplexity: 5.199678897857666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [13:48:44<2:38:30, 279.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3221.941333912037
INFO:root:current train perplexity3.5765860080718994
INFO:root:current mean train loss 3226.989242433563
INFO:root:current train perplexity3.5642471313476562
INFO:root:current mean train loss 3234.134241851941
INFO:root:current train perplexity3.576331853866577
INFO:root:current mean train loss 3229.891193914851
INFO:root:current train perplexity3.574110507965088
INFO:root:current mean train loss 3232.9337761407714
INFO:root:current train perplexity3.57405161857605
INFO:root:current mean train loss 3236.096809864949
INFO:root:current train perplexity3.5739362239837646
INFO:root:current mean train loss 3236.660706442509
INFO:root:current train perplexity3.5767147541046143
INFO:root:current mean train loss 3237.938221004019
INFO:root:current train perplexity3.5789031982421875
INFO:root:current mean train loss 3235.3923301466143
INFO:root:current train perplexity3.5798141956329346
INFO:root:current mean train loss 3236.0164996397148
INFO:root:current train perplexity3.581054449081421


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:26<00:00, 266.58s/it]
INFO:root:final mean train loss: 3233.456628368747
INFO:root:final train perplexity: 3.581179141998291
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.11s/it]
INFO:root:eval mean loss: 4077.067997492797
INFO:root:eval perplexity: 5.199937343597412
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [13:53:31<2:34:58, 281.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3271.1699776785713
INFO:root:current train perplexity3.5856709480285645
INFO:root:current mean train loss 3235.290700954861
INFO:root:current train perplexity3.5802102088928223
INFO:root:current mean train loss 3223.332463430851
INFO:root:current train perplexity3.5712289810180664
INFO:root:current mean train loss 3229.915475600513
INFO:root:current train perplexity3.5730113983154297
INFO:root:current mean train loss 3232.6392387302444
INFO:root:current train perplexity3.5757439136505127
INFO:root:current mean train loss 3236.133926420123
INFO:root:current train perplexity3.576908826828003
INFO:root:current mean train loss 3235.9075541338584
INFO:root:current train perplexity3.5794804096221924
INFO:root:current mean train loss 3236.4674249973427
INFO:root:current train perplexity3.5779714584350586
INFO:root:current mean train loss 3235.706460516467
INFO:root:current train perplexity3.577159881591797
INFO:root:current mean train loss 3234.977409550468
INFO:root:current train perplexity3.5785276889801025


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.46s/it]
INFO:root:final mean train loss: 3232.276814183881
INFO:root:final train perplexity: 3.57951283454895
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.58s/it]
INFO:root:eval mean loss: 4077.122693650266
INFO:root:eval perplexity: 5.200052738189697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [13:58:13<2:30:18, 281.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3243.202994413154
INFO:root:current train perplexity3.5614802837371826
INFO:root:current mean train loss 3251.574481670673
INFO:root:current train perplexity3.5885496139526367
INFO:root:current mean train loss 3232.860982510288
INFO:root:current train perplexity3.5793848037719727
INFO:root:current mean train loss 3228.014455545053
INFO:root:current train perplexity3.5732431411743164
INFO:root:current mean train loss 3229.160669331088
INFO:root:current train perplexity3.5739924907684326
INFO:root:current mean train loss 3227.6645359439744
INFO:root:current train perplexity3.570310354232788
INFO:root:current mean train loss 3232.5409423448436
INFO:root:current train perplexity3.5718212127685547
INFO:root:current mean train loss 3233.9808633837692
INFO:root:current train perplexity3.576406955718994
INFO:root:current mean train loss 3235.1723647292965
INFO:root:current train perplexity3.5772554874420166
INFO:root:current mean train loss 3233.3653345684484
INFO:root:current train perplexity3.576697587966919


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:20<00:00, 260.02s/it]
INFO:root:final mean train loss: 3230.0351371765137
INFO:root:final train perplexity: 3.5763487815856934
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.52s/it]
INFO:root:eval mean loss: 4077.3338978003103
INFO:root:eval perplexity: 5.200496673583984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [14:02:53<2:25:24, 281.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3240.7025457643995
INFO:root:current train perplexity3.565532684326172
INFO:root:current mean train loss 3222.969765366308
INFO:root:current train perplexity3.559631824493408
INFO:root:current mean train loss 3227.0298668606824
INFO:root:current train perplexity3.572887897491455
INFO:root:current mean train loss 3226.6680800391737
INFO:root:current train perplexity3.571399688720703
INFO:root:current mean train loss 3220.542967126005
INFO:root:current train perplexity3.5655789375305176
INFO:root:current mean train loss 3220.540363844856
INFO:root:current train perplexity3.5662293434143066
INFO:root:current mean train loss 3225.3764614685338
INFO:root:current train perplexity3.5696821212768555
INFO:root:current mean train loss 3228.116332845269
INFO:root:current train perplexity3.572509288787842
INFO:root:current mean train loss 3229.223980519242
INFO:root:current train perplexity3.5714354515075684
INFO:root:current mean train loss 3230.3349837855712
INFO:root:current train perplexity3.573101282119751


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.91s/it]
INFO:root:final mean train loss: 3228.5972382945397
INFO:root:final train perplexity: 3.5743205547332764
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.46s/it]
INFO:root:eval mean loss: 4080.279203374335
INFO:root:eval perplexity: 5.20669412612915
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [14:07:36<2:20:51, 281.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3220.1168647378177
INFO:root:current train perplexity3.569950580596924
INFO:root:current mean train loss 3212.1835323309747
INFO:root:current train perplexity3.5529391765594482
INFO:root:current mean train loss 3209.973518754525
INFO:root:current train perplexity3.5586326122283936
INFO:root:current mean train loss 3220.2626830714657
INFO:root:current train perplexity3.569347620010376
INFO:root:current mean train loss 3220.2362451491013
INFO:root:current train perplexity3.5680055618286133
INFO:root:current mean train loss 3222.7261838418212
INFO:root:current train perplexity3.570957660675049
INFO:root:current mean train loss 3226.2281429308136
INFO:root:current train perplexity3.57250714302063
INFO:root:current mean train loss 3228.091461061018
INFO:root:current train perplexity3.572737455368042
INFO:root:current mean train loss 3228.3585317343022
INFO:root:current train perplexity3.572577476501465
INFO:root:current mean train loss 3228.2303748105937
INFO:root:current train perplexity3.5715537071228027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.31s/it]
INFO:root:final mean train loss: 3226.663765876524
INFO:root:final train perplexity: 3.5715949535369873
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.86s/it]
INFO:root:eval mean loss: 4079.387930795656
INFO:root:eval perplexity: 5.2048187255859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/171

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [14:12:18<2:16:13, 281.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3255.417389371502
INFO:root:current train perplexity3.564134359359741
INFO:root:current mean train loss 3243.094790886976
INFO:root:current train perplexity3.578441619873047
INFO:root:current mean train loss 3233.4112626550796
INFO:root:current train perplexity3.580173969268799
INFO:root:current mean train loss 3232.106678809392
INFO:root:current train perplexity3.5794851779937744
INFO:root:current mean train loss 3223.7042730360013
INFO:root:current train perplexity3.5731961727142334
INFO:root:current mean train loss 3221.8821928909006
INFO:root:current train perplexity3.5687601566314697
INFO:root:current mean train loss 3221.946924853003
INFO:root:current train perplexity3.5686635971069336
INFO:root:current mean train loss 3224.513113248146
INFO:root:current train perplexity3.5694429874420166
INFO:root:current mean train loss 3227.2977194956566
INFO:root:current train perplexity3.5697970390319824
INFO:root:current mean train loss 3227.6746706247577
INFO:root:current train perplexity3.570077419281006


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.47s/it]
INFO:root:final mean train loss: 3225.4870998628676
INFO:root:final train perplexity: 3.569936990737915
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.66s/it]
INFO:root:eval mean loss: 4080.654702044548
INFO:root:eval perplexity: 5.207485198974609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/172

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [14:17:00<2:11:33, 281.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.308414713542
INFO:root:current train perplexity3.572510004043579
INFO:root:current mean train loss 3216.054188058036
INFO:root:current train perplexity3.561649799346924
INFO:root:current mean train loss 3226.5741654829544
INFO:root:current train perplexity3.5662918090820312
INFO:root:current mean train loss 3221.8811907552085
INFO:root:current train perplexity3.5620651245117188
INFO:root:current mean train loss 3222.783798314145
INFO:root:current train perplexity3.563424587249756
INFO:root:current mean train loss 3224.2554037873642
INFO:root:current train perplexity3.563636541366577
INFO:root:current mean train loss 3222.258480541088
INFO:root:current train perplexity3.564328193664551
INFO:root:current mean train loss 3224.2284390751006
INFO:root:current train perplexity3.565666913986206
INFO:root:current mean train loss 3226.3150753348214
INFO:root:current train perplexity3.5664563179016113
INFO:root:current mean train loss 3225.8205714142628
INFO:root:current train perplexity3.5661778450012207


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.02s/it]
INFO:root:final mean train loss: 3222.8994534400204
INFO:root:final train perplexity: 3.5662944316864014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.19s/it]
INFO:root:eval mean loss: 4081.6265375664893
INFO:root:eval perplexity: 5.209531307220459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/173

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [14:21:42<2:06:53, 281.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3224.4761742281626
INFO:root:current train perplexity3.582970380783081
INFO:root:current mean train loss 3226.6577655396177
INFO:root:current train perplexity3.5627758502960205
INFO:root:current mean train loss 3224.5070964691918
INFO:root:current train perplexity3.565441131591797
INFO:root:current mean train loss 3225.4032559307684
INFO:root:current train perplexity3.5612311363220215
INFO:root:current mean train loss 3232.73569881842
INFO:root:current train perplexity3.566088914871216
INFO:root:current mean train loss 3226.0768695392903
INFO:root:current train perplexity3.5637171268463135
INFO:root:current mean train loss 3224.269987002997
INFO:root:current train perplexity3.562739372253418
INFO:root:current mean train loss 3225.3250175856083
INFO:root:current train perplexity3.564056158065796
INFO:root:current mean train loss 3226.9835643314696
INFO:root:current train perplexity3.5662429332733154
INFO:root:current mean train loss 3224.5160325633424
INFO:root:current train perplexity3.5649447441101074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.65s/it]
INFO:root:final mean train loss: 3222.036639305853
INFO:root:final train perplexity: 3.5650808811187744
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.98s/it]
INFO:root:eval mean loss: 4083.041259765625
INFO:root:eval perplexity: 5.212512493133545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/174

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [14:26:27<2:02:31, 282.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3196.570755172562
INFO:root:current train perplexity3.5431501865386963
INFO:root:current mean train loss 3200.1071266054482
INFO:root:current train perplexity3.5409908294677734
INFO:root:current mean train loss 3210.9140625
INFO:root:current train perplexity3.5497725009918213
INFO:root:current mean train loss 3218.1455384081282
INFO:root:current train perplexity3.556438446044922
INFO:root:current mean train loss 3214.5117729482245
INFO:root:current train perplexity3.551741123199463
INFO:root:current mean train loss 3216.572551488473
INFO:root:current train perplexity3.552719831466675
INFO:root:current mean train loss 3218.5021880794366
INFO:root:current train perplexity3.5547540187835693
INFO:root:current mean train loss 3219.195843683332
INFO:root:current train perplexity3.5562384128570557
INFO:root:current mean train loss 3222.837786776182
INFO:root:current train perplexity3.5609641075134277
INFO:root:current mean train loss 3223.364869272673
INFO:root:current train perplexity3.5634186267852783


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.52s/it]
INFO:root:final mean train loss: 3220.8402924691477
INFO:root:final train perplexity: 3.5633981227874756
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4083.389101285461
INFO:root:eval perplexity: 5.213245391845703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/175

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [14:31:09<1:57:43, 282.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3216.4539560645517
INFO:root:current train perplexity3.564743757247925
INFO:root:current mean train loss 3216.3193776499684
INFO:root:current train perplexity3.555662155151367
INFO:root:current mean train loss 3216.4282202066784
INFO:root:current train perplexity3.555372714996338
INFO:root:current mean train loss 3218.8438020099074
INFO:root:current train perplexity3.5580716133117676
INFO:root:current mean train loss 3218.969947218656
INFO:root:current train perplexity3.5579357147216797
INFO:root:current mean train loss 3220.837155757643
INFO:root:current train perplexity3.5620086193084717
INFO:root:current mean train loss 3221.3927656836777
INFO:root:current train perplexity3.5598766803741455
INFO:root:current mean train loss 3219.992773559723
INFO:root:current train perplexity3.559276819229126
INFO:root:current mean train loss 3220.2180417477753
INFO:root:current train perplexity3.5598206520080566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.10s/it]
INFO:root:final mean train loss: 3219.9137464338733
INFO:root:final train perplexity: 3.562096118927002
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.80s/it]
INFO:root:eval mean loss: 4082.079044423205
INFO:root:eval perplexity: 5.210485458374023
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/176

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [14:35:53<1:53:17, 283.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3194.2762625558034
INFO:root:current train perplexity3.4578027725219727
INFO:root:current mean train loss 3231.9655373831774
INFO:root:current train perplexity3.5563669204711914
INFO:root:current mean train loss 3222.607483205012
INFO:root:current train perplexity3.5505998134613037
INFO:root:current mean train loss 3217.7480381272903
INFO:root:current train perplexity3.550112724304199
INFO:root:current mean train loss 3218.015206301827
INFO:root:current train perplexity3.5472216606140137
INFO:root:current mean train loss 3216.5814143976518
INFO:root:current train perplexity3.5495197772979736
INFO:root:current mean train loss 3215.9141063407383
INFO:root:current train perplexity3.552006959915161
INFO:root:current mean train loss 3218.2398990701245
INFO:root:current train perplexity3.557145357131958
INFO:root:current mean train loss 3219.769180014231
INFO:root:current train perplexity3.558945894241333
INFO:root:current mean train loss 3222.628936666638
INFO:root:current train perplexity3.5629708766937256


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.49s/it]
INFO:root:final mean train loss: 3219.7783292339695
INFO:root:final train perplexity: 3.561905860900879
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it]
INFO:root:eval mean loss: 4084.35365241301
INFO:root:eval perplexity: 5.215279579162598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/177

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [14:40:36<1:48:26, 282.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3229.5403645833335
INFO:root:current train perplexity3.530388355255127
INFO:root:current mean train loss 3208.1633449388587
INFO:root:current train perplexity3.5279810428619385
INFO:root:current mean train loss 3212.7710585483287
INFO:root:current train perplexity3.553788900375366
INFO:root:current mean train loss 3209.860255456349
INFO:root:current train perplexity3.5538294315338135
INFO:root:current mean train loss 3214.1170839608435
INFO:root:current train perplexity3.5528388023376465
INFO:root:current mean train loss 3213.211021882585
INFO:root:current train perplexity3.5504589080810547
INFO:root:current mean train loss 3217.2932482215447
INFO:root:current train perplexity3.5532469749450684
INFO:root:current mean train loss 3219.2611536412805
INFO:root:current train perplexity3.555021047592163
INFO:root:current mean train loss 3219.184668567868
INFO:root:current train perplexity3.55488920211792
INFO:root:current mean train loss 3218.355163774334
INFO:root:current train perplexity3.5567924976348877


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.25s/it]
INFO:root:final mean train loss: 3216.636279444541
INFO:root:final train perplexity: 3.55749249458313
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.52s/it]
INFO:root:eval mean loss: 4084.7527825105276
INFO:root:eval perplexity: 5.216121196746826
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/178

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [14:45:17<1:43:35, 282.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3243.810589334239
INFO:root:current train perplexity3.565955877304077
INFO:root:current mean train loss 3232.687365027947
INFO:root:current train perplexity3.553046703338623
INFO:root:current mean train loss 3224.2209768252524
INFO:root:current train perplexity3.545527219772339
INFO:root:current mean train loss 3224.6906723164184
INFO:root:current train perplexity3.5489706993103027
INFO:root:current mean train loss 3227.288090554817
INFO:root:current train perplexity3.5530648231506348
INFO:root:current mean train loss 3222.2605293230163
INFO:root:current train perplexity3.554044723510742
INFO:root:current mean train loss 3221.0063750877807
INFO:root:current train perplexity3.553645610809326
INFO:root:current mean train loss 3217.871978464298
INFO:root:current train perplexity3.554354429244995
INFO:root:current mean train loss 3215.980969193784
INFO:root:current train perplexity3.5524520874023438
INFO:root:current mean train loss 3216.843501098236
INFO:root:current train perplexity3.5559377670288086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.69s/it]
INFO:root:final mean train loss: 3215.652203406057
INFO:root:final train perplexity: 3.556112051010132
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.06s/it]
INFO:root:eval mean loss: 4084.6770088791
INFO:root:eval perplexity: 5.21596097946167
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/179

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [14:50:03<1:39:13, 283.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3188.729019657258
INFO:root:current train perplexity3.5377554893493652
INFO:root:current mean train loss 3209.6066093153627
INFO:root:current train perplexity3.5510480403900146
INFO:root:current mean train loss 3208.378685360863
INFO:root:current train perplexity3.5576188564300537
INFO:root:current mean train loss 3210.404662717145
INFO:root:current train perplexity3.5589754581451416
INFO:root:current mean train loss 3211.8594554361225
INFO:root:current train perplexity3.556097984313965
INFO:root:current mean train loss 3213.2375699777836
INFO:root:current train perplexity3.552612066268921
INFO:root:current mean train loss 3215.374492760128
INFO:root:current train perplexity3.555772542953491
INFO:root:current mean train loss 3217.3598191956653
INFO:root:current train perplexity3.557018995285034
INFO:root:current mean train loss 3217.830985058946
INFO:root:current train perplexity3.555663585662842
INFO:root:current mean train loss 3215.985133383123
INFO:root:current train perplexity3.5536184310913086


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.66s/it]
INFO:root:final mean train loss: 3215.1305898850965
INFO:root:final train perplexity: 3.555380344390869
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4085.2942500554077
INFO:root:eval perplexity: 5.217264175415039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/180

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [14:54:44<1:34:15, 282.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3230.148218399439
INFO:root:current train perplexity3.581916570663452
INFO:root:current mean train loss 3212.007933692109
INFO:root:current train perplexity3.557440757751465
INFO:root:current mean train loss 3207.2574641654683
INFO:root:current train perplexity3.549774169921875
INFO:root:current mean train loss 3211.8855484593932
INFO:root:current train perplexity3.547544240951538
INFO:root:current mean train loss 3211.303427311717
INFO:root:current train perplexity3.5493874549865723
INFO:root:current mean train loss 3210.9682635305544
INFO:root:current train perplexity3.5480306148529053
INFO:root:current mean train loss 3210.2396543977407
INFO:root:current train perplexity3.546910047531128
INFO:root:current mean train loss 3212.6401050035943
INFO:root:current train perplexity3.548326253890991
INFO:root:current mean train loss 3214.8487521184074
INFO:root:current train perplexity3.5519070625305176
INFO:root:current mean train loss 3215.0355899311103
INFO:root:current train perplexity3.551182270050049


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.77s/it]
INFO:root:final mean train loss: 3212.939259929042
INFO:root:final train perplexity: 3.5523080825805664
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.53s/it]
INFO:root:eval mean loss: 4085.7079610621677
INFO:root:eval perplexity: 5.218137264251709
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/181

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [14:59:25<1:29:24, 282.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3221.687120802859
INFO:root:current train perplexity3.533690929412842
INFO:root:current mean train loss 3211.1098649420705
INFO:root:current train perplexity3.5533008575439453
INFO:root:current mean train loss 3214.247292707806
INFO:root:current train perplexity3.5505688190460205
INFO:root:current mean train loss 3221.7819001035664
INFO:root:current train perplexity3.5570266246795654
INFO:root:current mean train loss 3217.7106600426455
INFO:root:current train perplexity3.5525076389312744
INFO:root:current mean train loss 3215.779865048703
INFO:root:current train perplexity3.553191900253296
INFO:root:current mean train loss 3218.756397842808
INFO:root:current train perplexity3.5543859004974365
INFO:root:current mean train loss 3216.5331279545267
INFO:root:current train perplexity3.5532522201538086
INFO:root:current mean train loss 3216.2942949495464
INFO:root:current train perplexity3.552992820739746
INFO:root:current mean train loss 3215.7975334320554
INFO:root:current train perplexity3.5525410175323486


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.26s/it]
INFO:root:final mean train loss: 3212.50003703948
INFO:root:final train perplexity: 3.551692485809326
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.57s/it]
INFO:root:eval mean loss: 4086.5862959192154
INFO:root:eval perplexity: 5.21998929977417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/182

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [15:04:07<1:24:39, 282.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.6624644886365
INFO:root:current train perplexity3.5778417587280273
INFO:root:current mean train loss 3205.2613659274193
INFO:root:current train perplexity3.5610294342041016
INFO:root:current mean train loss 3203.5660213694855
INFO:root:current train perplexity3.5457122325897217
INFO:root:current mean train loss 3210.0091047260125
INFO:root:current train perplexity3.5475659370422363
INFO:root:current mean train loss 3212.3841346153845
INFO:root:current train perplexity3.5463063716888428
INFO:root:current mean train loss 3210.8787756017737
INFO:root:current train perplexity3.550025463104248
INFO:root:current mean train loss 3212.2851733957536
INFO:root:current train perplexity3.5484447479248047
INFO:root:current mean train loss 3213.799320933361
INFO:root:current train perplexity3.551649332046509
INFO:root:current mean train loss 3215.3630668060123
INFO:root:current train perplexity3.551941394805908
INFO:root:current mean train loss 3215.688702041067
INFO:root:current train perplexity3.5515565872192383


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.45s/it]
INFO:root:final mean train loss: 3212.2905584150744
INFO:root:final train perplexity: 3.551398515701294
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.68s/it]
INFO:root:eval mean loss: 4085.7514942791445
INFO:root:eval perplexity: 5.218228816986084
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/183

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [15:08:49<1:19:56, 282.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3202.2014043898807
INFO:root:current train perplexity3.5550103187561035
INFO:root:current mean train loss 3219.6154395729486
INFO:root:current train perplexity3.550030469894409
INFO:root:current mean train loss 3212.973624457878
INFO:root:current train perplexity3.5495946407318115
INFO:root:current mean train loss 3212.7196613238207
INFO:root:current train perplexity3.55120587348938
INFO:root:current mean train loss 3212.214140329711
INFO:root:current train perplexity3.550032615661621
INFO:root:current mean train loss 3211.1753385011934
INFO:root:current train perplexity3.549654722213745
INFO:root:current mean train loss 3213.667765115361
INFO:root:current train perplexity3.5506324768066406
INFO:root:current mean train loss 3215.141263029366
INFO:root:current train perplexity3.549696207046509
INFO:root:current mean train loss 3212.159192417892
INFO:root:current train perplexity3.5485775470733643
INFO:root:current mean train loss 3212.308757778021
INFO:root:current train perplexity3.549468517303467


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.60s/it]
INFO:root:final mean train loss: 3211.0276074563303
INFO:root:final train perplexity: 3.5496299266815186
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.97s/it]
INFO:root:eval mean loss: 4086.7996401955897
INFO:root:eval perplexity: 5.220440864562988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/184

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [15:13:31<1:15:11, 281.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3212.4917817451583
INFO:root:current train perplexity3.5438852310180664
INFO:root:current mean train loss 3219.0654011330407
INFO:root:current train perplexity3.5442404747009277
INFO:root:current mean train loss 3215.112635313365
INFO:root:current train perplexity3.5413832664489746
INFO:root:current mean train loss 3212.032889229911
INFO:root:current train perplexity3.5405969619750977
INFO:root:current mean train loss 3210.7265293259024
INFO:root:current train perplexity3.539977788925171
INFO:root:current mean train loss 3210.608326178716
INFO:root:current train perplexity3.540943145751953
INFO:root:current mean train loss 3210.604610801276
INFO:root:current train perplexity3.543198823928833
INFO:root:current mean train loss 3209.2942046525413
INFO:root:current train perplexity3.5438432693481445
INFO:root:current mean train loss 3212.980752693115
INFO:root:current train perplexity3.5454299449920654
INFO:root:current mean train loss 3212.3092235875547
INFO:root:current train perplexity3.5470807552337646


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.50s/it]
INFO:root:final mean train loss: 3208.9515618970318
INFO:root:final train perplexity: 3.5467236042022705
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.66s/it]
INFO:root:eval mean loss: 4087.2382102587544
INFO:root:eval perplexity: 5.221365928649902
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/185

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [15:18:13<1:10:30, 282.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3204.90234375
INFO:root:current train perplexity3.536860227584839
INFO:root:current mean train loss 3198.734670969361
INFO:root:current train perplexity3.5432932376861572
INFO:root:current mean train loss 3202.2525936659945
INFO:root:current train perplexity3.5513265132904053
INFO:root:current mean train loss 3205.623799266161
INFO:root:current train perplexity3.546349287033081
INFO:root:current mean train loss 3208.392173432607
INFO:root:current train perplexity3.546710968017578
INFO:root:current mean train loss 3210.995773289076
INFO:root:current train perplexity3.546276569366455
INFO:root:current mean train loss 3212.94541569346
INFO:root:current train perplexity3.54848575592041
INFO:root:current mean train loss 3213.1967309601655
INFO:root:current train perplexity3.5496463775634766
INFO:root:current mean train loss 3214.595455095901
INFO:root:current train perplexity3.550004720687866
INFO:root:current mean train loss 3213.4118936634163
INFO:root:current train perplexity3.5488550662994385


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.76s/it]
INFO:root:final mean train loss: 3210.3980615062096
INFO:root:final train perplexity: 3.54874849319458
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it]
INFO:root:eval mean loss: 4088.288160391733
INFO:root:eval perplexity: 5.223583698272705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/186

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [15:22:55<1:05:49, 282.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3213.075784617457
INFO:root:current train perplexity3.5581295490264893
INFO:root:current mean train loss 3210.1934507645387
INFO:root:current train perplexity3.5516517162323
INFO:root:current mean train loss 3214.3571666757402
INFO:root:current train perplexity3.5468668937683105
INFO:root:current mean train loss 3214.397785827479
INFO:root:current train perplexity3.5446207523345947
INFO:root:current mean train loss 3214.829815937019
INFO:root:current train perplexity3.5438930988311768
INFO:root:current mean train loss 3212.9736386352747
INFO:root:current train perplexity3.5451245307922363
INFO:root:current mean train loss 3214.1387571643013
INFO:root:current train perplexity3.5459280014038086
INFO:root:current mean train loss 3212.5458823062263
INFO:root:current train perplexity3.546119451522827
INFO:root:current mean train loss 3211.507295868711
INFO:root:current train perplexity3.546088218688965
INFO:root:current mean train loss 3211.0563786747243
INFO:root:current train perplexity3.5461649894714355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.22s/it]
INFO:root:final mean train loss: 3208.165576504123
INFO:root:final train perplexity: 3.545624017715454
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.63s/it]
INFO:root:eval mean loss: 4087.7955296293217
INFO:root:eval perplexity: 5.222543716430664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/187

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [15:27:37<1:01:06, 282.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3214.4853772615133
INFO:root:current train perplexity3.533952474594116
INFO:root:current mean train loss 3211.2916228465547
INFO:root:current train perplexity3.543321132659912
INFO:root:current mean train loss 3208.3971646583686
INFO:root:current train perplexity3.5416159629821777
INFO:root:current mean train loss 3206.822223595728
INFO:root:current train perplexity3.541449546813965
INFO:root:current mean train loss 3205.2352479876895
INFO:root:current train perplexity3.5398643016815186
INFO:root:current mean train loss 3209.084940421481
INFO:root:current train perplexity3.544965982437134
INFO:root:current mean train loss 3212.6469290973473
INFO:root:current train perplexity3.547607660293579
INFO:root:current mean train loss 3211.5502088246853
INFO:root:current train perplexity3.5460736751556396
INFO:root:current mean train loss 3210.785291823062
INFO:root:current train perplexity3.5463781356811523


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.01s/it]
INFO:root:final mean train loss: 3208.0167852832424
INFO:root:final train perplexity: 3.5454156398773193
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4087.2094016650044
INFO:root:eval perplexity: 5.221306324005127
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/188

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [15:32:19<56:23, 281.92s/it]  

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3187.853515625
INFO:root:current train perplexity3.5017096996307373
INFO:root:current mean train loss 3201.5400840981492
INFO:root:current train perplexity3.5185513496398926
INFO:root:current mean train loss 3200.3805514932264
INFO:root:current train perplexity3.5284645557403564
INFO:root:current mean train loss 3206.718430925124
INFO:root:current train perplexity3.538813352584839
INFO:root:current mean train loss 3203.7027596977746
INFO:root:current train perplexity3.538442611694336
INFO:root:current mean train loss 3205.2606824871086
INFO:root:current train perplexity3.5420167446136475
INFO:root:current mean train loss 3201.209805464863
INFO:root:current train perplexity3.540039539337158
INFO:root:current mean train loss 3203.061397373422
INFO:root:current train perplexity3.5372793674468994
INFO:root:current mean train loss 3203.4032628497625
INFO:root:current train perplexity3.5377023220062256
INFO:root:current mean train loss 3208.312757929298
INFO:root:current train perplexity3.5405113697052


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.22s/it]
INFO:root:final mean train loss: 3204.9613082639635
INFO:root:final train perplexity: 3.541144371032715
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.25s/it]
INFO:root:eval mean loss: 4087.9194041583555
INFO:root:eval perplexity: 5.222804546356201
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/189

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [15:37:03<51:49, 282.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3177.729669744318
INFO:root:current train perplexity3.5334644317626953
INFO:root:current mean train loss 3192.7422952737893
INFO:root:current train perplexity3.5325052738189697
INFO:root:current mean train loss 3206.5634511070793
INFO:root:current train perplexity3.538222312927246
INFO:root:current mean train loss 3207.291990617464
INFO:root:current train perplexity3.539154529571533
INFO:root:current mean train loss 3205.2052189068204
INFO:root:current train perplexity3.5429844856262207
INFO:root:current mean train loss 3203.8304694188782
INFO:root:current train perplexity3.5416996479034424
INFO:root:current mean train loss 3202.0411898399143
INFO:root:current train perplexity3.5392487049102783
INFO:root:current mean train loss 3203.7529623082587
INFO:root:current train perplexity3.5406765937805176
INFO:root:current mean train loss 3203.154190308069
INFO:root:current train perplexity3.5383965969085693
INFO:root:current mean train loss 3206.5668152056464
INFO:root:current train perplexity3.5406363010406494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.78s/it]
INFO:root:final mean train loss: 3204.8827769987047
INFO:root:final train perplexity: 3.5410351753234863
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it]
INFO:root:eval mean loss: 4088.1780166084886
INFO:root:eval perplexity: 5.22335147857666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/190

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [15:41:45<47:02, 282.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3187.6143606085525
INFO:root:current train perplexity3.545830249786377
INFO:root:current mean train loss 3220.203781512605
INFO:root:current train perplexity3.543215274810791
INFO:root:current mean train loss 3218.438425281821
INFO:root:current train perplexity3.5427730083465576
INFO:root:current mean train loss 3217.9579588313086
INFO:root:current train perplexity3.5437393188476562
INFO:root:current mean train loss 3209.409173860755
INFO:root:current train perplexity3.540668249130249
INFO:root:current mean train loss 3209.706771115577
INFO:root:current train perplexity3.540297269821167
INFO:root:current mean train loss 3207.1855910490713
INFO:root:current train perplexity3.5400631427764893
INFO:root:current mean train loss 3207.12441698268
INFO:root:current train perplexity3.5403997898101807
INFO:root:current mean train loss 3205.8221043550634
INFO:root:current train perplexity3.5410735607147217
INFO:root:current mean train loss 3206.31744417973
INFO:root:current train perplexity3.5404579639434814


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.65s/it]
INFO:root:final mean train loss: 3204.99291020055
INFO:root:final train perplexity: 3.5411887168884277
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.64s/it]
INFO:root:eval mean loss: 4088.11371931793
INFO:root:eval perplexity: 5.223215103149414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/191

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [15:46:27<42:20, 282.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3253.379466869213
INFO:root:current train perplexity3.5479888916015625
INFO:root:current mean train loss 3222.362881397638
INFO:root:current train perplexity3.5369133949279785
INFO:root:current mean train loss 3215.513058834664
INFO:root:current train perplexity3.5410358905792236
INFO:root:current mean train loss 3212.253956272697
INFO:root:current train perplexity3.5377554893493652
INFO:root:current mean train loss 3209.357201748207
INFO:root:current train perplexity3.538255214691162
INFO:root:current mean train loss 3212.955876793762
INFO:root:current train perplexity3.5400824546813965
INFO:root:current mean train loss 3207.8341568698916
INFO:root:current train perplexity3.537834405899048
INFO:root:current mean train loss 3206.1880692136992
INFO:root:current train perplexity3.5374631881713867
INFO:root:current mean train loss 3205.366343192639
INFO:root:current train perplexity3.53857421875
INFO:root:current mean train loss 3205.8072950904293
INFO:root:current train perplexity3.539255380630493


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:25<00:00, 265.86s/it]
INFO:root:final mean train loss: 3204.376127735261
INFO:root:final train perplexity: 3.5403268337249756
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.72s/it]
INFO:root:eval mean loss: 4087.8286409851507
INFO:root:eval perplexity: 5.2226128578186035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/192

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [15:51:12<37:46, 283.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3185.1720982142856
INFO:root:current train perplexity3.5335793495178223
INFO:root:current mean train loss 3203.7053023726853
INFO:root:current train perplexity3.5357632637023926
INFO:root:current mean train loss 3199.0442590591756
INFO:root:current train perplexity3.5355255603790283
INFO:root:current mean train loss 3204.263719974347
INFO:root:current train perplexity3.540823221206665
INFO:root:current mean train loss 3208.7409084276223
INFO:root:current train perplexity3.5411887168884277
INFO:root:current mean train loss 3203.9399669611566
INFO:root:current train perplexity3.5372962951660156
INFO:root:current mean train loss 3205.4574268731544
INFO:root:current train perplexity3.5395145416259766
INFO:root:current mean train loss 3204.5725386639033
INFO:root:current train perplexity3.540323495864868
INFO:root:current mean train loss 3203.7362535670845
INFO:root:current train perplexity3.537992000579834
INFO:root:current mean train loss 3204.854378864472
INFO:root:current train perplexity3.538729667663574


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.15s/it]
INFO:root:final mean train loss: 3203.99889029226
INFO:root:final train perplexity: 3.5398001670837402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.71s/it]
INFO:root:eval mean loss: 4089.0977947695037
INFO:root:eval perplexity: 5.225294589996338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/193

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [15:55:53<32:57, 282.52s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3210.5594113372094
INFO:root:current train perplexity3.565728187561035
INFO:root:current mean train loss 3208.0468271962413
INFO:root:current train perplexity3.5548694133758545
INFO:root:current mean train loss 3208.13769028903
INFO:root:current train perplexity3.545536994934082
INFO:root:current mean train loss 3203.5196216460913
INFO:root:current train perplexity3.5452373027801514
INFO:root:current mean train loss 3207.517012137592
INFO:root:current train perplexity3.5422024726867676
INFO:root:current mean train loss 3206.734230673774
INFO:root:current train perplexity3.5392181873321533
INFO:root:current mean train loss 3205.8750167063567
INFO:root:current train perplexity3.5395946502685547
INFO:root:current mean train loss 3204.7240734485404
INFO:root:current train perplexity3.537639856338501
INFO:root:current mean train loss 3207.1990406402915
INFO:root:current train perplexity3.5387701988220215
INFO:root:current mean train loss 3206.9900049915495
INFO:root:current train perplexity3.5385284423828125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.79s/it]
INFO:root:final mean train loss: 3203.5037122541858
INFO:root:final train perplexity: 3.5391085147857666
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.92s/it]
INFO:root:eval mean loss: 4088.452891248338
INFO:root:eval perplexity: 5.223931789398193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/194

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [16:00:35<28:13, 282.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3176.1460774739585
INFO:root:current train perplexity3.5078885555267334
INFO:root:current mean train loss 3192.921519298427
INFO:root:current train perplexity3.525639533996582
INFO:root:current mean train loss 3197.397752739044
INFO:root:current train perplexity3.535114288330078
INFO:root:current mean train loss 3198.6684932002313
INFO:root:current train perplexity3.536065101623535
INFO:root:current mean train loss 3201.144854966394
INFO:root:current train perplexity3.536400318145752
INFO:root:current mean train loss 3203.841186301894
INFO:root:current train perplexity3.5396926403045654
INFO:root:current mean train loss 3206.8165723856328
INFO:root:current train perplexity3.5415546894073486
INFO:root:current mean train loss 3204.307807038532
INFO:root:current train perplexity3.5394599437713623
INFO:root:current mean train loss 3204.8404516630253
INFO:root:current train perplexity3.5394127368927
INFO:root:current mean train loss 3205.6750144276584
INFO:root:current train perplexity3.539857864379883


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.53s/it]
INFO:root:final mean train loss: 3203.213419883482
INFO:root:final train perplexity: 3.538703680038452
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.17s/it]
INFO:root:eval mean loss: 4088.622738669105
INFO:root:eval perplexity: 5.224289894104004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/195

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [16:05:38<24:02, 288.46s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3215.4126721398306
INFO:root:current train perplexity3.5533177852630615
INFO:root:current mean train loss 3219.5433771865173
INFO:root:current train perplexity3.552159070968628
INFO:root:current mean train loss 3219.398373401303
INFO:root:current train perplexity3.5476737022399902
INFO:root:current mean train loss 3214.873960192157
INFO:root:current train perplexity3.543571710586548
INFO:root:current mean train loss 3210.9468060661766
INFO:root:current train perplexity3.5417227745056152
INFO:root:current mean train loss 3204.6248956178724
INFO:root:current train perplexity3.538472890853882
INFO:root:current mean train loss 3203.2013793389606
INFO:root:current train perplexity3.538344621658325
INFO:root:current mean train loss 3203.9091545979495
INFO:root:current train perplexity3.538740634918213
INFO:root:current mean train loss 3204.993518762733
INFO:root:current train perplexity3.5374274253845215
INFO:root:current mean train loss 3205.099876427675
INFO:root:current train perplexity3.537928342819214


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:21<00:00, 261.85s/it]
INFO:root:final mean train loss: 3202.8752472785213
INFO:root:final train perplexity: 3.538231134414673
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.69s/it]
INFO:root:eval mean loss: 4088.825572778147
INFO:root:eval perplexity: 5.2247185707092285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/196

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [16:10:39<19:28, 292.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3213.660105235541
INFO:root:current train perplexity3.541531562805176
INFO:root:current mean train loss 3204.339551366018
INFO:root:current train perplexity3.5395002365112305
INFO:root:current mean train loss 3209.2573854825023
INFO:root:current train perplexity3.5400984287261963
INFO:root:current mean train loss 3208.7132944216196
INFO:root:current train perplexity3.5382206439971924
INFO:root:current mean train loss 3210.0367554495115
INFO:root:current train perplexity3.5399954319000244
INFO:root:current mean train loss 3209.6355200066137
INFO:root:current train perplexity3.5407180786132812
INFO:root:current mean train loss 3206.28614159717
INFO:root:current train perplexity3.5383636951446533
INFO:root:current mean train loss 3207.113971973802
INFO:root:current train perplexity3.540379047393799
INFO:root:current mean train loss 3205.6495706278833
INFO:root:current train perplexity3.5391013622283936
INFO:root:current mean train loss 3204.8146308654345
INFO:root:current train perplexity3.538295269012451


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.26s/it]
INFO:root:final mean train loss: 3202.186085977862
INFO:root:final train perplexity: 3.5372695922851562
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.12s/it]
INFO:root:eval mean loss: 4088.788001094304
INFO:root:eval perplexity: 5.224639892578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/197

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [16:15:44<14:48, 296.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3201.9438248697916
INFO:root:current train perplexity3.5332958698272705
INFO:root:current mean train loss 3204.130567801339
INFO:root:current train perplexity3.533390998840332
INFO:root:current mean train loss 3202.374968927557
INFO:root:current train perplexity3.5357439517974854
INFO:root:current mean train loss 3198.8544973958333
INFO:root:current train perplexity3.533686876296997
INFO:root:current mean train loss 3202.256060855263
INFO:root:current train perplexity3.5326743125915527
INFO:root:current mean train loss 3202.9904407269023
INFO:root:current train perplexity3.5316784381866455
INFO:root:current mean train loss 3205.193454861111
INFO:root:current train perplexity3.535029172897339
INFO:root:current mean train loss 3203.3622275075604
INFO:root:current train perplexity3.5346925258636475
INFO:root:current mean train loss 3205.076720424107
INFO:root:current train perplexity3.535217523574829
INFO:root:current mean train loss 3203.9858836638623
INFO:root:current train perplexity3.5365679264068604


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:24<00:00, 264.86s/it]
INFO:root:final mean train loss: 3201.562095149871
INFO:root:final train perplexity: 3.5363986492156982
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.87s/it]
INFO:root:eval mean loss: 4089.0237959192154
INFO:root:eval perplexity: 5.225138187408447
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/198

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [16:20:58<10:03, 301.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3194.7849238751883
INFO:root:current train perplexity3.535763740539551
INFO:root:current mean train loss 3210.099054388661
INFO:root:current train perplexity3.541017770767212
INFO:root:current mean train loss 3213.6589122543064
INFO:root:current train perplexity3.548959255218506
INFO:root:current mean train loss 3209.8017029924117
INFO:root:current train perplexity3.5455644130706787
INFO:root:current mean train loss 3201.79826756195
INFO:root:current train perplexity3.5420327186584473
INFO:root:current mean train loss 3205.1646265779104
INFO:root:current train perplexity3.5444703102111816
INFO:root:current mean train loss 3205.216116283858
INFO:root:current train perplexity3.539813280105591
INFO:root:current mean train loss 3204.5280378202824
INFO:root:current train perplexity3.5368592739105225
INFO:root:current mean train loss 3204.21576584398
INFO:root:current train perplexity3.534558057785034
INFO:root:current mean train loss 3203.871423079063
INFO:root:current train perplexity3.5355303287506104


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.50s/it]
INFO:root:final mean train loss: 3200.833761399792
INFO:root:final train perplexity: 3.5353827476501465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.94s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.94s/it]
INFO:root:eval mean loss: 4089.049512065049
INFO:root:eval perplexity: 5.225192546844482
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [16:26:05<05:03, 303.22s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3198.4801172948146
INFO:root:current train perplexity3.541346311569214
INFO:root:current mean train loss 3195.3640775830336
INFO:root:current train perplexity3.524675130844116
INFO:root:current mean train loss 3194.851094354059
INFO:root:current train perplexity3.529778003692627
INFO:root:current mean train loss 3197.8117825637387
INFO:root:current train perplexity3.529463768005371
INFO:root:current mean train loss 3199.0828198590248
INFO:root:current train perplexity3.530282735824585
INFO:root:current mean train loss 3199.2008223945113
INFO:root:current train perplexity3.5332870483398438
INFO:root:current mean train loss 3198.562981568266
INFO:root:current train perplexity3.53216814994812
INFO:root:current mean train loss 3200.306015304006
INFO:root:current train perplexity3.5320820808410645
INFO:root:current mean train loss 3201.3112984773957
INFO:root:current train perplexity3.5328755378723145
INFO:root:current mean train loss 3201.2295628922016
INFO:root:current train perplexity3.5323967933654785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.10s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [04:22<00:00, 262.10s/it]
INFO:root:final mean train loss: 3198.6823389607093
INFO:root:final train perplexity: 3.5323832035064697
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:18<00:00, 18.59s/it]
INFO:root:eval mean loss: 4089.087880236037
INFO:root:eval perplexity: 5.225274085998535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_134/200

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [16:30:52<00:00, 298.23s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [16:30:52<00:00, 297.26s/it]
INFO:root:evaluating final model
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.65s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:23<00:00, 23.65s/it]
INFO:root:eval mean loss: 4089.087880236037
INFO:root:eval perplexity: 5.225274085998535
INFO:root:evalaution complete
INFO:root:save model final: small_val_134/final
Fatal error condition occurred in /opt/vcpkg/buildtrees/aws-c-io/src/9e6648842a-364b708815.clean/source/event_loop.c:72: aws_thread_launch(&cleanup_thread, s_event_loop_destroy_async_thread_fn, el_group, &thread_options) == AWS_OP_SUCCESS
Exiting Application
################################################################################
Stack trace:
################################################################################
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200af06) [0x1475cd34ff06]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x20028e5) [0x1475cd3478e5]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f27e09) [0x1475cd26ce09]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x1475cd350a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f25948) [0x1475cd26a948]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x1475cd350a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1ee0b46) [0x1475cd225b46]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x194546a) [0x1475ccc8a46a]
/lib/x86_64-linux-gnu/libc.so.6(+0x49a27) [0x1476c94a6a27]
/lib/x86_64-linux-gnu/libc.so.6(on_exit+0) [0x1476c94a6be0]
python(+0x24a989) [0x5579f7862989]
python(+0x24a9bd) [0x5579f78629bd]
python(+0x24aa14) [0x5579f7862a14]
python(+0x108f75) [0x5579f7720f75]
python(Py_RunMain+0x313) [0x5579f7865983]
python(Py_BytesMain+0x39) [0x5579f7865bc9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x1476c94840b3]
python(+0x1d6e13) [0x5579f77eee13]
/opt/slurm/data/slurmd/job26146193/slurm_script: line 130: 306934 Aborted                 singularity exec --nv --overlay /scratch/zw2374/overlay-50G-10M.ext3:ro /scratch/work/public/singularity/cuda11.3.0-cudnn8-devel-ubuntu20.04.sif /bin/bash -c "
source /ext3/env.sh
conda activate rblm
python train_script.py --model_path sentence-transformers/multi-qa-MiniLM-L6-cos-v1 --data_config data_config.json --data_folder fast_processed_data_134_final  --output small_val_134 --batch_size 128 --epochs 200 --save_head  --save_epochs 1 --external_embedding
"
