INFO:root:Output: distilroberta_roberta_not_concat_1e-5
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]Downloading:  91%|█████████ | 815k/899k [00:00<00:00, 8.10MB/s]Downloading: 100%|██████████| 899k/899k [00:00<00:00, 8.31MB/s]
Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]Downloading: 100%|██████████| 456k/456k [00:00<00:00, 7.49MB/s]
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 11520.395438762625
INFO:root:current train perplexity8868.5380859375
INFO:root:current mean train loss 9862.777265232413
INFO:root:current train perplexity2417.907958984375
INFO:root:current mean train loss 9109.55387750836
INFO:root:current train perplexity1339.3175048828125
INFO:root:current mean train loss 8675.575260171914
INFO:root:current train perplexity942.3643798828125
INFO:root:current mean train loss 8356.025853464742
INFO:root:current train perplexity741.002197265625
INFO:root:current mean train loss 8105.8934943656095
INFO:root:current train perplexity603.0139770507812
INFO:root:current mean train loss 7861.092045556152
INFO:root:current train perplexity496.3470458984375
INFO:root:current mean train loss 7616.517720514901
INFO:root:current train perplexity409.7740478515625
INFO:root:current mean train loss 7378.608012809372
INFO:root:current train perplexity340.50897216796875
INFO:root:current mean train loss 7166.022681372779
INFO:root:current train perplexity285.9527587890625
INFO:root:current mean train loss 6959.200517427064
INFO:root:current train perplexity243.41183471679688
INFO:root:current mean train loss 6771.660221204846
INFO:root:current train perplexity209.5511474609375
INFO:root:current mean train loss 6600.489105577007
INFO:root:current train perplexity182.8381805419922
INFO:root:current mean train loss 6438.620787832548
INFO:root:current train perplexity160.94309997558594
INFO:root:current mean train loss 6292.516242110626
INFO:root:current train perplexity143.1800994873047
INFO:root:current mean train loss 6156.1212502504
INFO:root:current train perplexity128.51690673828125
INFO:root:current mean train loss 6029.57247513473
INFO:root:current train perplexity116.18106079101562
INFO:root:current mean train loss 5910.432094707563
INFO:root:current train perplexity105.78683471679688
INFO:root:current mean train loss 5799.599880513799
INFO:root:current train perplexity96.9795913696289

100%|██████████| 1/1 [07:32<00:00, 452.05s/it][A100%|██████████| 1/1 [07:32<00:00, 452.05s/it]
INFO:root:final mean train loss: 5708.820214683698
INFO:root:final train perplexity: 90.50347900390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.35s/it][A100%|██████████| 1/1 [00:37<00:00, 37.35s/it]
INFO:root:eval mean loss: 3305.9491650736923
INFO:root:eval perplexity: 14.515584945678711
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.74s/it][A100%|██████████| 1/1 [00:35<00:00, 35.74s/it]
INFO:root:eval mean loss: 3575.89082585328
INFO:root:eval perplexity: 18.912866592407227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/1
  0%|          | 1/200 [08:46<29:07:34, 526.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3660.959930419922
INFO:root:current train perplexity18.495649337768555
INFO:root:current mean train loss 3626.987658270474
INFO:root:current train perplexity17.677846908569336
INFO:root:current mean train loss 3591.7504803692855
INFO:root:current train perplexity17.138460159301758
INFO:root:current mean train loss 3579.3080629759197
INFO:root:current train perplexity16.822919845581055
INFO:root:current mean train loss 3551.7153326181265
INFO:root:current train perplexity16.43822479248047
INFO:root:current mean train loss 3518.2744301492853
INFO:root:current train perplexity16.085819244384766
INFO:root:current mean train loss 3488.330035717456
INFO:root:current train perplexity15.738875389099121
INFO:root:current mean train loss 3459.0311286116444
INFO:root:current train perplexity15.390271186828613
INFO:root:current mean train loss 3432.4699255251417
INFO:root:current train perplexity15.077120780944824
INFO:root:current mean train loss 3408.3413752260167
INFO:root:current train perplexity14.774328231811523
INFO:root:current mean train loss 3384.296113262026
INFO:root:current train perplexity14.49919319152832
INFO:root:current mean train loss 3365.509475543935
INFO:root:current train perplexity14.240398406982422
INFO:root:current mean train loss 3344.7817808452405
INFO:root:current train perplexity13.999593734741211
INFO:root:current mean train loss 3323.361754443508
INFO:root:current train perplexity13.768640518188477
INFO:root:current mean train loss 3303.237716588597
INFO:root:current train perplexity13.559134483337402
INFO:root:current mean train loss 3284.808076320034
INFO:root:current train perplexity13.356947898864746
INFO:root:current mean train loss 3267.755001256962
INFO:root:current train perplexity13.166872024536133
INFO:root:current mean train loss 3250.976368012684
INFO:root:current train perplexity12.983325004577637
INFO:root:current mean train loss 3232.939975284795
INFO:root:current train perplexity12.812417030334473
INFO:root:current mean train loss 3217.141492106969
INFO:root:current train perplexity12.64599895477295

100%|██████████| 1/1 [07:53<00:00, 473.84s/it][A100%|██████████| 1/1 [07:53<00:00, 473.84s/it]
INFO:root:final mean train loss: 3204.434062147886
INFO:root:final train perplexity: 12.54011344909668
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.87s/it][A100%|██████████| 1/1 [00:39<00:00, 39.87s/it]
INFO:root:eval mean loss: 2620.599169575576
INFO:root:eval perplexity: 8.336365699768066
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.14s/it][A100%|██████████| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 2948.6424859229555
INFO:root:eval perplexity: 11.2927827835083
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/2
  1%|          | 2/200 [18:00<29:50:23, 542.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2860.264522668087
INFO:root:current train perplexity9.787226676940918
INFO:root:current mean train loss 2851.7295949835525
INFO:root:current train perplexity9.529051780700684
INFO:root:current mean train loss 2840.6134059398473
INFO:root:current train perplexity9.425878524780273
INFO:root:current mean train loss 2834.500806470533
INFO:root:current train perplexity9.347550392150879
INFO:root:current mean train loss 2831.0150053451575
INFO:root:current train perplexity9.310264587402344
INFO:root:current mean train loss 2819.62441415411
INFO:root:current train perplexity9.251884460449219
INFO:root:current mean train loss 2812.3396813752715
INFO:root:current train perplexity9.205568313598633
INFO:root:current mean train loss 2805.7261395005544
INFO:root:current train perplexity9.162574768066406
INFO:root:current mean train loss 2800.4451958517784
INFO:root:current train perplexity9.100630760192871
INFO:root:current mean train loss 2792.6471639389906
INFO:root:current train perplexity9.038196563720703
INFO:root:current mean train loss 2784.02928813037
INFO:root:current train perplexity8.982032775878906
INFO:root:current mean train loss 2776.583778805577
INFO:root:current train perplexity8.920794486999512
INFO:root:current mean train loss 2770.107060317189
INFO:root:current train perplexity8.872701644897461
INFO:root:current mean train loss 2761.4222557714625
INFO:root:current train perplexity8.829833030700684
INFO:root:current mean train loss 2753.312590125883
INFO:root:current train perplexity8.78071117401123
INFO:root:current mean train loss 2746.234220043165
INFO:root:current train perplexity8.730477333068848
INFO:root:current mean train loss 2740.3128925984574
INFO:root:current train perplexity8.684501647949219
INFO:root:current mean train loss 2732.0179606072834
INFO:root:current train perplexity8.633522033691406
INFO:root:current mean train loss 2725.3367064164363
INFO:root:current train perplexity8.586947441101074
INFO:root:current mean train loss 2719.376423164285
INFO:root:current train perplexity8.547249794006348

100%|██████████| 1/1 [07:46<00:00, 466.97s/it][A100%|██████████| 1/1 [07:46<00:00, 466.97s/it]
INFO:root:final mean train loss: 2715.3538126442927
INFO:root:final train perplexity: 8.524559020996094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.01s/it][A100%|██████████| 1/1 [00:39<00:00, 39.01s/it]
INFO:root:eval mean loss: 2374.392546958112
INFO:root:eval perplexity: 6.830469608306885
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.55s/it][A100%|██████████| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 2720.5496960362643
INFO:root:eval perplexity: 9.36184024810791
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/3
  2%|▏         | 3/200 [27:07<29:48:21, 544.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2563.3922412109373
INFO:root:current train perplexity7.635380744934082
INFO:root:current mean train loss 2568.33990234375
INFO:root:current train perplexity7.570906162261963
INFO:root:current mean train loss 2557.74257421875
INFO:root:current train perplexity7.5095391273498535
INFO:root:current mean train loss 2551.099794921875
INFO:root:current train perplexity7.498035430908203
INFO:root:current mean train loss 2559.1271940104166
INFO:root:current train perplexity7.506980895996094
INFO:root:current mean train loss 2555.4835506924715
INFO:root:current train perplexity7.476678371429443
INFO:root:current mean train loss 2548.884277719351
INFO:root:current train perplexity7.451616287231445
INFO:root:current mean train loss 2542.6184350585936
INFO:root:current train perplexity7.420809745788574
INFO:root:current mean train loss 2533.66634090648
INFO:root:current train perplexity7.381518840789795
INFO:root:current mean train loss 2530.5077568616366
INFO:root:current train perplexity7.364110469818115
INFO:root:current mean train loss 2527.146070498512
INFO:root:current train perplexity7.344644069671631
INFO:root:current mean train loss 2519.674176715353
INFO:root:current train perplexity7.32019567489624
INFO:root:current mean train loss 2513.706169921875
INFO:root:current train perplexity7.294435501098633
INFO:root:current mean train loss 2509.4582644314237
INFO:root:current train perplexity7.264301300048828
INFO:root:current mean train loss 2506.1442521720096
INFO:root:current train perplexity7.249155044555664
INFO:root:current mean train loss 2503.513047426285
INFO:root:current train perplexity7.225105285644531
INFO:root:current mean train loss 2499.919961159446
INFO:root:current train perplexity7.2004899978637695
INFO:root:current mean train loss 2498.798448939732
INFO:root:current train perplexity7.186142444610596
INFO:root:current mean train loss 2496.0035428763726
INFO:root:current train perplexity7.166428565979004
INFO:root:current mean train loss 2492.3556708859173
INFO:root:current train perplexity7.145376205444336

100%|██████████| 1/1 [07:33<00:00, 453.99s/it][A100%|██████████| 1/1 [07:33<00:00, 453.99s/it]
INFO:root:final mean train loss: 2490.7259014858723
INFO:root:final train perplexity: 7.139733791351318
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.68s/it][A100%|██████████| 1/1 [00:38<00:00, 38.68s/it]
INFO:root:eval mean loss: 2236.024667726341
INFO:root:eval perplexity: 6.106929302215576
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.46s/it][A100%|██████████| 1/1 [00:37<00:00, 37.46s/it]
INFO:root:eval mean loss: 2594.25941543038
INFO:root:eval perplexity: 8.438587188720703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/4
  2%|▏         | 4/200 [36:00<29:23:52, 539.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2379.535052399137
INFO:root:current train perplexity6.51044225692749
INFO:root:current mean train loss 2411.7944226293507
INFO:root:current train perplexity6.600500106811523
INFO:root:current mean train loss 2407.6166704156426
INFO:root:current train perplexity6.598895072937012
INFO:root:current mean train loss 2397.1542433237187
INFO:root:current train perplexity6.568841934204102
INFO:root:current mean train loss 2391.882029629366
INFO:root:current train perplexity6.559850692749023
INFO:root:current mean train loss 2384.281094128913
INFO:root:current train perplexity6.53762674331665
INFO:root:current mean train loss 2378.740104618101
INFO:root:current train perplexity6.518466472625732
INFO:root:current mean train loss 2375.672518773682
INFO:root:current train perplexity6.510862827301025
INFO:root:current mean train loss 2373.130431213731
INFO:root:current train perplexity6.5046772956848145
INFO:root:current mean train loss 2369.8910844994184
INFO:root:current train perplexity6.486097812652588
INFO:root:current mean train loss 2367.426106618293
INFO:root:current train perplexity6.471075057983398
INFO:root:current mean train loss 2366.2272991059476
INFO:root:current train perplexity6.461223602294922
INFO:root:current mean train loss 2362.582114011167
INFO:root:current train perplexity6.448702812194824
INFO:root:current mean train loss 2361.203010341418
INFO:root:current train perplexity6.438597679138184
INFO:root:current mean train loss 2359.161182239743
INFO:root:current train perplexity6.431517124176025
INFO:root:current mean train loss 2357.709201639902
INFO:root:current train perplexity6.425720691680908
INFO:root:current mean train loss 2356.2152576320673
INFO:root:current train perplexity6.415590286254883
INFO:root:current mean train loss 2355.8956677404985
INFO:root:current train perplexity6.409589767456055
INFO:root:current mean train loss 2353.2404817193988
INFO:root:current train perplexity6.400153160095215
INFO:root:current mean train loss 2350.333661791632
INFO:root:current train perplexity6.388942241668701

100%|██████████| 1/1 [07:31<00:00, 451.61s/it][A100%|██████████| 1/1 [07:31<00:00, 451.61s/it]
INFO:root:final mean train loss: 2349.437174048258
INFO:root:final train perplexity: 6.386399745941162
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.22s/it][A100%|██████████| 1/1 [00:40<00:00, 40.22s/it]
INFO:root:eval mean loss: 2145.773703717171
INFO:root:eval perplexity: 5.67682409286499
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.53s/it][A100%|██████████| 1/1 [00:36<00:00, 36.53s/it]
INFO:root:eval mean loss: 2508.5835774739585
INFO:root:eval perplexity: 7.86465311050415
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/5
  2%|▎         | 5/200 [44:51<29:04:26, 536.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2251.0167236328125
INFO:root:current train perplexity5.990497589111328
INFO:root:current mean train loss 2278.744045091712
INFO:root:current train perplexity6.045828819274902
INFO:root:current mean train loss 2275.5988417074714
INFO:root:current train perplexity6.032529830932617
INFO:root:current mean train loss 2268.887840270996
INFO:root:current train perplexity6.015574932098389
INFO:root:current mean train loss 2272.459710996013
INFO:root:current train perplexity6.023255348205566
INFO:root:current mean train loss 2270.493542814908
INFO:root:current train perplexity6.02323579788208
INFO:root:current mean train loss 2270.4087660047744
INFO:root:current train perplexity6.021906852722168
INFO:root:current mean train loss 2269.9695584044166
INFO:root:current train perplexity6.006501197814941
INFO:root:current mean train loss 2269.983767134032
INFO:root:current train perplexity6.007443428039551
INFO:root:current mean train loss 2266.3040430332585
INFO:root:current train perplexity5.988454818725586
INFO:root:current mean train loss 2264.755829307866
INFO:root:current train perplexity5.9734787940979
INFO:root:current mean train loss 2266.1010975193335
INFO:root:current train perplexity5.977819442749023
INFO:root:current mean train loss 2260.993960181501
INFO:root:current train perplexity5.962130069732666
INFO:root:current mean train loss 2258.0184550202653
INFO:root:current train perplexity5.952073097229004
INFO:root:current mean train loss 2258.5981308764844
INFO:root:current train perplexity5.9468302726745605
INFO:root:current mean train loss 2257.3888075857453
INFO:root:current train perplexity5.93791389465332
INFO:root:current mean train loss 2255.961877020974
INFO:root:current train perplexity5.9315290451049805
INFO:root:current mean train loss 2254.704793750438
INFO:root:current train perplexity5.926027297973633
INFO:root:current mean train loss 2252.8574805776025
INFO:root:current train perplexity5.915573596954346

100%|██████████| 1/1 [07:36<00:00, 456.69s/it][A100%|██████████| 1/1 [07:36<00:00, 456.69s/it]
INFO:root:final mean train loss: 2249.5069061756376
INFO:root:final train perplexity: 5.902087688446045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.84s/it][A100%|██████████| 1/1 [00:38<00:00, 38.84s/it]
INFO:root:eval mean loss: 2075.487752278646
INFO:root:eval perplexity: 5.362955570220947
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.16s/it][A100%|██████████| 1/1 [00:38<00:00, 38.16s/it]
INFO:root:eval mean loss: 2447.1966756184897
INFO:root:eval perplexity: 7.477591037750244
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/6
  3%|▎         | 6/200 [53:48<28:55:32, 536.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2121.076171875
INFO:root:current train perplexity5.74666690826416
INFO:root:current mean train loss 2187.2905998607675
INFO:root:current train perplexity5.62126350402832
INFO:root:current mean train loss 2192.1560283300296
INFO:root:current train perplexity5.647351264953613
INFO:root:current mean train loss 2196.8045151415854
INFO:root:current train perplexity5.637606620788574
INFO:root:current mean train loss 2195.2219865375623
INFO:root:current train perplexity5.6460041999816895
INFO:root:current mean train loss 2196.7056353601392
INFO:root:current train perplexity5.639531135559082
INFO:root:current mean train loss 2194.83933820423
INFO:root:current train perplexity5.629271507263184
INFO:root:current mean train loss 2194.422718173257
INFO:root:current train perplexity5.621455669403076
INFO:root:current mean train loss 2193.869959760992
INFO:root:current train perplexity5.617446422576904
INFO:root:current mean train loss 2191.7463093036818
INFO:root:current train perplexity5.613692283630371
INFO:root:current mean train loss 2190.225019609297
INFO:root:current train perplexity5.606216907501221
INFO:root:current mean train loss 2187.4816368996935
INFO:root:current train perplexity5.5994744300842285
INFO:root:current mean train loss 2181.5779471290202
INFO:root:current train perplexity5.586379528045654
INFO:root:current mean train loss 2180.085325271876
INFO:root:current train perplexity5.582021236419678
INFO:root:current mean train loss 2179.3985607901445
INFO:root:current train perplexity5.575083255767822
INFO:root:current mean train loss 2176.4877266067415
INFO:root:current train perplexity5.565809726715088
INFO:root:current mean train loss 2176.1442270272973
INFO:root:current train perplexity5.563485622406006
INFO:root:current mean train loss 2175.6902366427375
INFO:root:current train perplexity5.561084270477295
INFO:root:current mean train loss 2175.3415044755952
INFO:root:current train perplexity5.557004451751709
INFO:root:current mean train loss 2173.3554211676214
INFO:root:current train perplexity5.551349639892578

100%|██████████| 1/1 [07:32<00:00, 452.03s/it][A100%|██████████| 1/1 [07:32<00:00, 452.03s/it]
INFO:root:final mean train loss: 2171.8016607964573
INFO:root:final train perplexity: 5.551016330718994
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.56s/it][A100%|██████████| 1/1 [00:38<00:00, 38.56s/it]
INFO:root:eval mean loss: 2023.7646480046265
INFO:root:eval perplexity: 5.143121242523193
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.46s/it][A100%|██████████| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2396.600817524795
INFO:root:eval perplexity: 7.1729302406311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/7
  4%|▎         | 7/200 [1:02:37<28:39:02, 534.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2094.0050523546006
INFO:root:current train perplexity5.2394304275512695
INFO:root:current mean train loss 2109.5364876440017
INFO:root:current train perplexity5.326138973236084
INFO:root:current mean train loss 2110.1535487743695
INFO:root:current train perplexity5.323148250579834
INFO:root:current mean train loss 2112.9715253721993
INFO:root:current train perplexity5.321563243865967
INFO:root:current mean train loss 2122.9475126859675
INFO:root:current train perplexity5.328643798828125
INFO:root:current mean train loss 2120.2278699248914
INFO:root:current train perplexity5.318103790283203
INFO:root:current mean train loss 2121.038320201886
INFO:root:current train perplexity5.3154730796813965
INFO:root:current mean train loss 2119.006643481241
INFO:root:current train perplexity5.308752059936523
INFO:root:current mean train loss 2117.9632144545576
INFO:root:current train perplexity5.3026909828186035
INFO:root:current mean train loss 2118.556103675194
INFO:root:current train perplexity5.301469326019287
INFO:root:current mean train loss 2119.0698586334647
INFO:root:current train perplexity5.302541255950928
INFO:root:current mean train loss 2118.250908321046
INFO:root:current train perplexity5.301976680755615
INFO:root:current mean train loss 2116.6682520774
INFO:root:current train perplexity5.299156188964844
INFO:root:current mean train loss 2115.7193815610476
INFO:root:current train perplexity5.296785354614258
INFO:root:current mean train loss 2114.0127221714124
INFO:root:current train perplexity5.292165756225586
INFO:root:current mean train loss 2112.8599325187593
INFO:root:current train perplexity5.29030179977417
INFO:root:current mean train loss 2111.9659337820613
INFO:root:current train perplexity5.28483247756958
INFO:root:current mean train loss 2110.216048537089
INFO:root:current train perplexity5.281010627746582
INFO:root:current mean train loss 2108.4095978689666
INFO:root:current train perplexity5.274271488189697
INFO:root:current mean train loss 2107.9731207281757
INFO:root:current train perplexity5.273244380950928

100%|██████████| 1/1 [07:36<00:00, 456.07s/it][A100%|██████████| 1/1 [07:36<00:00, 456.07s/it]
INFO:root:final mean train loss: 2106.3321266655244
INFO:root:final train perplexity: 5.271488189697266
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.19s/it][A100%|██████████| 1/1 [00:39<00:00, 39.20s/it]
INFO:root:eval mean loss: 1983.9211741778868
INFO:root:eval perplexity: 4.9799418449401855
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.46s/it][A100%|██████████| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2359.380748559397
INFO:root:eval perplexity: 6.9567646980285645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/8
  4%|▍         | 8/200 [1:11:31<28:29:46, 534.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2073.919203404018
INFO:root:current train perplexity5.095484733581543
INFO:root:current mean train loss 2069.1591182002317
INFO:root:current train perplexity5.079781532287598
INFO:root:current mean train loss 2059.023472822473
INFO:root:current train perplexity5.061038970947266
INFO:root:current mean train loss 2055.1938400040813
INFO:root:current train perplexity5.058485507965088
INFO:root:current mean train loss 2056.3800020765984
INFO:root:current train perplexity5.070786952972412
INFO:root:current mean train loss 2054.999729619962
INFO:root:current train perplexity5.076970100402832
INFO:root:current mean train loss 2058.3335199311023
INFO:root:current train perplexity5.084813594818115
INFO:root:current mean train loss 2055.1857940051023
INFO:root:current train perplexity5.074793338775635
INFO:root:current mean train loss 2057.1056861374905
INFO:root:current train perplexity5.088362693786621
INFO:root:current mean train loss 2059.441052833598
INFO:root:current train perplexity5.0914082527160645
INFO:root:current mean train loss 2058.771561391342
INFO:root:current train perplexity5.089838027954102
INFO:root:current mean train loss 2063.137113354385
INFO:root:current train perplexity5.097165107727051
INFO:root:current mean train loss 2061.0385823238234
INFO:root:current train perplexity5.091264724731445
INFO:root:current mean train loss 2060.9808111869443
INFO:root:current train perplexity5.087718486785889
INFO:root:current mean train loss 2060.7391957140135
INFO:root:current train perplexity5.086348056793213
INFO:root:current mean train loss 2059.880654758118
INFO:root:current train perplexity5.083449840545654
INFO:root:current mean train loss 2057.5198171259794
INFO:root:current train perplexity5.077033042907715
INFO:root:current mean train loss 2056.921132446641
INFO:root:current train perplexity5.0736541748046875
INFO:root:current mean train loss 2056.993736961427
INFO:root:current train perplexity5.0699143409729
INFO:root:current mean train loss 2056.52524918746
INFO:root:current train perplexity5.0670671463012695

100%|██████████| 1/1 [07:31<00:00, 451.33s/it][A100%|██████████| 1/1 [07:31<00:00, 451.33s/it]
INFO:root:final mean train loss: 2055.2878763926974
INFO:root:final train perplexity: 5.063352108001709
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.74s/it][A100%|██████████| 1/1 [00:39<00:00, 39.74s/it]
INFO:root:eval mean loss: 1946.6515256191822
INFO:root:eval perplexity: 4.831993579864502
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.82s/it][A100%|██████████| 1/1 [00:37<00:00, 37.82s/it]
INFO:root:eval mean loss: 2331.518170728751
INFO:root:eval perplexity: 6.799220085144043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/9
  4%|▍         | 9/200 [1:20:23<28:18:00, 533.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2022.7524977463943
INFO:root:current train perplexity4.915200710296631
INFO:root:current mean train loss 2030.7287180047285
INFO:root:current train perplexity4.921555042266846
INFO:root:current mean train loss 2026.1907518174912
INFO:root:current train perplexity4.919982433319092
INFO:root:current mean train loss 2025.3508928472345
INFO:root:current train perplexity4.923393249511719
INFO:root:current mean train loss 2022.976314848503
INFO:root:current train perplexity4.925743103027344
INFO:root:current mean train loss 2022.1849137458248
INFO:root:current train perplexity4.923525333404541
INFO:root:current mean train loss 2020.3988928297547
INFO:root:current train perplexity4.922117710113525
INFO:root:current mean train loss 2021.0006721983564
INFO:root:current train perplexity4.921327590942383
INFO:root:current mean train loss 2020.6164794348774
INFO:root:current train perplexity4.924066543579102
INFO:root:current mean train loss 2019.6821183917903
INFO:root:current train perplexity4.920959949493408
INFO:root:current mean train loss 2018.815349622371
INFO:root:current train perplexity4.919351577758789
INFO:root:current mean train loss 2020.2143363952637
INFO:root:current train perplexity4.922427177429199
INFO:root:current mean train loss 2020.6140185468876
INFO:root:current train perplexity4.928339004516602
INFO:root:current mean train loss 2019.1899457401073
INFO:root:current train perplexity4.92506742477417
INFO:root:current mean train loss 2019.6570111739734
INFO:root:current train perplexity4.927163600921631
INFO:root:current mean train loss 2019.247997008648
INFO:root:current train perplexity4.925928592681885
INFO:root:current mean train loss 2020.7451038129682
INFO:root:current train perplexity4.928482532501221
INFO:root:current mean train loss 2020.2405424074495
INFO:root:current train perplexity4.927504539489746
INFO:root:current mean train loss 2022.4187906813158
INFO:root:current train perplexity4.934699058532715
INFO:root:current mean train loss 2024.249185968618
INFO:root:current train perplexity4.939936637878418

100%|██████████| 1/1 [07:28<00:00, 448.08s/it][A100%|██████████| 1/1 [07:28<00:00, 448.08s/it]
INFO:root:final mean train loss: 2023.674103906644
INFO:root:final train perplexity: 4.938586711883545
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.91s/it][A100%|██████████| 1/1 [00:37<00:00, 37.91s/it]
INFO:root:eval mean loss: 1935.0885451296542
INFO:root:eval perplexity: 4.786992073059082
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.54s/it][A100%|██████████| 1/1 [00:35<00:00, 35.54s/it]
INFO:root:eval mean loss: 2321.3153807728004
INFO:root:eval perplexity: 6.742427825927734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/10
  5%|▌         | 10/200 [1:29:07<27:59:53, 530.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2051.17309393399
INFO:root:current train perplexity4.999096393585205
INFO:root:current mean train loss 2037.4099236663276
INFO:root:current train perplexity4.978680610656738
INFO:root:current mean train loss 2029.8719632173559
INFO:root:current train perplexity4.951807498931885
INFO:root:current mean train loss 2024.175605587843
INFO:root:current train perplexity4.938085556030273
INFO:root:current mean train loss 2032.638200772088
INFO:root:current train perplexity4.95101261138916
INFO:root:current mean train loss 2030.0414603556815
INFO:root:current train perplexity4.947184085845947
INFO:root:current mean train loss 2031.4469428411692
INFO:root:current train perplexity4.9563727378845215
INFO:root:current mean train loss 2029.951149175319
INFO:root:current train perplexity4.956099033355713
INFO:root:current mean train loss 2030.2864574436762
INFO:root:current train perplexity4.953258037567139
INFO:root:current mean train loss 2029.8511024372742
INFO:root:current train perplexity4.948116779327393
INFO:root:current mean train loss 2027.2429513244344
INFO:root:current train perplexity4.939764499664307
INFO:root:current mean train loss 2026.01255465492
INFO:root:current train perplexity4.942694664001465
INFO:root:current mean train loss 2025.0180532276584
INFO:root:current train perplexity4.944343090057373
INFO:root:current mean train loss 2023.097777874475
INFO:root:current train perplexity4.940893173217773
INFO:root:current mean train loss 2023.703164637535
INFO:root:current train perplexity4.941035270690918
INFO:root:current mean train loss 2025.589190607697
INFO:root:current train perplexity4.942317008972168
INFO:root:current mean train loss 2027.4255787259165
INFO:root:current train perplexity4.948111057281494
INFO:root:current mean train loss 2026.8326255923412
INFO:root:current train perplexity4.948517322540283
INFO:root:current mean train loss 2027.5555093356031
INFO:root:current train perplexity4.953775882720947
INFO:root:current mean train loss 2030.2890235044558
INFO:root:current train perplexity4.963711261749268

100%|██████████| 1/1 [07:38<00:00, 458.25s/it][A100%|██████████| 1/1 [07:38<00:00, 458.26s/it]
INFO:root:final mean train loss: 2030.5104416901574
INFO:root:final train perplexity: 4.965303421020508
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.78s/it][A100%|██████████| 1/1 [00:39<00:00, 39.78s/it]
INFO:root:eval mean loss: 1958.8475956477173
INFO:root:eval perplexity: 4.879916667938232
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.97s/it][A100%|██████████| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 2351.0839692244294
INFO:root:eval perplexity: 6.909473419189453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/11
  6%|▌         | 11/200 [1:38:04<27:57:59, 532.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2041.9734851925873
INFO:root:current train perplexity5.037141799926758
INFO:root:current mean train loss 2064.4689331054688
INFO:root:current train perplexity5.1099019050598145
INFO:root:current mean train loss 2059.6980569479347
INFO:root:current train perplexity5.089574813842773
INFO:root:current mean train loss 2052.5097137609296
INFO:root:current train perplexity5.068909168243408
INFO:root:current mean train loss 2057.044839741271
INFO:root:current train perplexity5.076107501983643
INFO:root:current mean train loss 2061.820644756226
INFO:root:current train perplexity5.091791152954102
INFO:root:current mean train loss 2064.1220905982373
INFO:root:current train perplexity5.0938801765441895
INFO:root:current mean train loss 2061.245338342875
INFO:root:current train perplexity5.077975749969482
INFO:root:current mean train loss 2058.732372964209
INFO:root:current train perplexity5.070597171783447
INFO:root:current mean train loss 2056.4907719300677
INFO:root:current train perplexity5.066416263580322
INFO:root:current mean train loss 2056.8381074515496
INFO:root:current train perplexity5.066094875335693
INFO:root:current mean train loss 2056.0302058150755
INFO:root:current train perplexity5.05955696105957
INFO:root:current mean train loss 2055.9428755551066
INFO:root:current train perplexity5.062253952026367
INFO:root:current mean train loss 2053.7218418314
INFO:root:current train perplexity5.060062408447266
INFO:root:current mean train loss 2052.305363322652
INFO:root:current train perplexity5.055964946746826
INFO:root:current mean train loss 2051.7627256376595
INFO:root:current train perplexity5.051497459411621
INFO:root:current mean train loss 2053.485592372618
INFO:root:current train perplexity5.053454875946045
INFO:root:current mean train loss 2052.5517750363065
INFO:root:current train perplexity5.051806449890137
INFO:root:current mean train loss 2051.9603035240134
INFO:root:current train perplexity5.049468517303467

100%|██████████| 1/1 [07:33<00:00, 453.99s/it][A100%|██████████| 1/1 [07:33<00:00, 453.99s/it]
INFO:root:final mean train loss: 2050.950126982673
INFO:root:final train perplexity: 5.046048164367676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.81s/it][A100%|██████████| 1/1 [00:40<00:00, 40.81s/it]
INFO:root:eval mean loss: 1942.6495950036015
INFO:root:eval perplexity: 4.816370487213135
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.14s/it][A100%|██████████| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 2336.922465439384
INFO:root:eval perplexity: 6.829496383666992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/12
  6%|▌         | 12/200 [1:47:00<27:51:53, 533.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1880.2495524088542
INFO:root:current train perplexity4.239706039428711
INFO:root:current mean train loss 2006.3077594053398
INFO:root:current train perplexity4.855715751647949
INFO:root:current mean train loss 2002.0244772023168
INFO:root:current train perplexity4.867311954498291
INFO:root:current mean train loss 2006.5320952261243
INFO:root:current train perplexity4.862244129180908
INFO:root:current mean train loss 2000.92184925316
INFO:root:current train perplexity4.8521270751953125
INFO:root:current mean train loss 2004.6652419467569
INFO:root:current train perplexity4.862964153289795
INFO:root:current mean train loss 2007.1581083838619
INFO:root:current train perplexity4.87028169631958
INFO:root:current mean train loss 2010.4818259357219
INFO:root:current train perplexity4.877967357635498
INFO:root:current mean train loss 2009.8820730853051
INFO:root:current train perplexity4.8738884925842285
INFO:root:current mean train loss 2010.7601553577917
INFO:root:current train perplexity4.879510402679443
INFO:root:current mean train loss 2009.6933423362725
INFO:root:current train perplexity4.877262115478516
INFO:root:current mean train loss 2008.2877913308166
INFO:root:current train perplexity4.874408721923828
INFO:root:current mean train loss 2005.9895922628325
INFO:root:current train perplexity4.869672775268555
INFO:root:current mean train loss 2006.0485392034009
INFO:root:current train perplexity4.870573997497559
INFO:root:current mean train loss 2006.2088128849173
INFO:root:current train perplexity4.872594356536865
INFO:root:current mean train loss 2006.4925208177394
INFO:root:current train perplexity4.872345924377441
INFO:root:current mean train loss 2008.8772073937296
INFO:root:current train perplexity4.8756818771362305
INFO:root:current mean train loss 2007.884809922976
INFO:root:current train perplexity4.874652862548828
INFO:root:current mean train loss 2007.517115368158
INFO:root:current train perplexity4.871417999267578
INFO:root:current mean train loss 2006.7719639965064
INFO:root:current train perplexity4.869263172149658

100%|██████████| 1/1 [07:40<00:00, 460.43s/it][A100%|██████████| 1/1 [07:40<00:00, 460.43s/it]
INFO:root:final mean train loss: 2005.1786372828713
INFO:root:final train perplexity: 4.867023468017578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.83s/it][A100%|██████████| 1/1 [00:38<00:00, 38.83s/it]
INFO:root:eval mean loss: 1924.8032512258976
INFO:root:eval perplexity: 4.747314453125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.68s/it][A100%|██████████| 1/1 [00:36<00:00, 36.68s/it]
INFO:root:eval mean loss: 2320.5471091845357
INFO:root:eval perplexity: 6.738171577453613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/13
  6%|▋         | 13/200 [1:55:59<27:47:39, 535.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1966.706854248047
INFO:root:current train perplexity4.709991455078125
INFO:root:current mean train loss 1991.9108154296875
INFO:root:current train perplexity4.798582077026367
INFO:root:current mean train loss 1988.8454656427557
INFO:root:current train perplexity4.781339645385742
INFO:root:current mean train loss 1987.0829082489013
INFO:root:current train perplexity4.776025772094727
INFO:root:current mean train loss 1990.8870826357886
INFO:root:current train perplexity4.788898468017578
INFO:root:current mean train loss 1988.6213554969202
INFO:root:current train perplexity4.780885696411133
INFO:root:current mean train loss 1985.6950366604713
INFO:root:current train perplexity4.777430534362793
INFO:root:current mean train loss 1982.8128145005967
INFO:root:current train perplexity4.7679948806762695
INFO:root:current mean train loss 1979.2685320598323
INFO:root:current train perplexity4.759154319763184
INFO:root:current mean train loss 1980.103929071841
INFO:root:current train perplexity4.75989294052124
INFO:root:current mean train loss 1977.6147097120097
INFO:root:current train perplexity4.753017902374268
INFO:root:current mean train loss 1975.9313613891602
INFO:root:current train perplexity4.753147602081299
INFO:root:current mean train loss 1976.1939198978612
INFO:root:current train perplexity4.75448751449585
INFO:root:current mean train loss 1974.9582390987512
INFO:root:current train perplexity4.751607418060303
INFO:root:current mean train loss 1976.3223746286312
INFO:root:current train perplexity4.750543594360352
INFO:root:current mean train loss 1974.802194213867
INFO:root:current train perplexity4.749564170837402
INFO:root:current mean train loss 1976.0213503990644
INFO:root:current train perplexity4.755517482757568
INFO:root:current mean train loss 1975.3156609113826
INFO:root:current train perplexity4.751002788543701
INFO:root:current mean train loss 1974.6840868604052
INFO:root:current train perplexity4.750746250152588
INFO:root:current mean train loss 1974.7936475753784
INFO:root:current train perplexity4.750414848327637

100%|██████████| 1/1 [07:36<00:00, 456.68s/it][A100%|██████████| 1/1 [07:36<00:00, 456.68s/it]
INFO:root:final mean train loss: 1973.5821780667904
INFO:root:final train perplexity: 4.747160911560059
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.61s/it][A100%|██████████| 1/1 [00:38<00:00, 38.61s/it]
INFO:root:eval mean loss: 1908.3159435082835
INFO:root:eval perplexity: 4.6843976974487305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.16s/it][A100%|██████████| 1/1 [00:36<00:00, 36.16s/it]
INFO:root:eval mean loss: 2308.2513514309067
INFO:root:eval perplexity: 6.670398235321045
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/14
  7%|▋         | 14/200 [2:04:52<27:37:39, 534.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1929.8997439822635
INFO:root:current train perplexity4.652190685272217
INFO:root:current mean train loss 1946.1840633197423
INFO:root:current train perplexity4.633485794067383
INFO:root:current mean train loss 1952.7631377529997
INFO:root:current train perplexity4.651358604431152
INFO:root:current mean train loss 1949.940141717475
INFO:root:current train perplexity4.652868747711182
INFO:root:current mean train loss 1948.5336218513123
INFO:root:current train perplexity4.649331092834473
INFO:root:current mean train loss 1952.0115355308717
INFO:root:current train perplexity4.6607208251953125
INFO:root:current mean train loss 1951.7825257018371
INFO:root:current train perplexity4.659535884857178
INFO:root:current mean train loss 1951.1928515492496
INFO:root:current train perplexity4.665307521820068
INFO:root:current mean train loss 1950.1739637586807
INFO:root:current train perplexity4.662200927734375
INFO:root:current mean train loss 1946.3729748313683
INFO:root:current train perplexity4.650501251220703
INFO:root:current mean train loss 1947.3841045383317
INFO:root:current train perplexity4.6513214111328125
INFO:root:current mean train loss 1949.5142870578413
INFO:root:current train perplexity4.660358905792236
INFO:root:current mean train loss 1949.0330877651008
INFO:root:current train perplexity4.6580729484558105
INFO:root:current mean train loss 1948.738707354823
INFO:root:current train perplexity4.657107830047607
INFO:root:current mean train loss 1950.2307892589
INFO:root:current train perplexity4.662051200866699
INFO:root:current mean train loss 1950.2306788189503
INFO:root:current train perplexity4.661004543304443
INFO:root:current mean train loss 1950.8231711227427
INFO:root:current train perplexity4.663536548614502
INFO:root:current mean train loss 1953.6603507051266
INFO:root:current train perplexity4.6686482429504395
INFO:root:current mean train loss 1953.187018031586
INFO:root:current train perplexity4.667281150817871
INFO:root:current mean train loss 1952.3198956837653
INFO:root:current train perplexity4.6661696434021

100%|██████████| 1/1 [07:36<00:00, 456.76s/it][A100%|██████████| 1/1 [07:36<00:00, 456.76s/it]
INFO:root:final mean train loss: 1951.473257337023
INFO:root:final train perplexity: 4.6650495529174805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.97s/it][A100%|██████████| 1/1 [00:40<00:00, 40.97s/it]
INFO:root:eval mean loss: 1905.7721670164285
INFO:root:eval perplexity: 4.674765110015869
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.17s/it][A100%|██████████| 1/1 [00:37<00:00, 37.17s/it]
INFO:root:eval mean loss: 2311.5295907960717
INFO:root:eval perplexity: 6.688401222229004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/15
  8%|▊         | 15/200 [2:13:50<27:31:19, 535.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1923.7498530635128
INFO:root:current train perplexity4.574539661407471
INFO:root:current mean train loss 1940.1104427189023
INFO:root:current train perplexity4.575049877166748
INFO:root:current mean train loss 1937.693973090705
INFO:root:current train perplexity4.589922904968262
INFO:root:current mean train loss 1947.4759342172051
INFO:root:current train perplexity4.611608982086182
INFO:root:current mean train loss 1939.079326612834
INFO:root:current train perplexity4.589052677154541
INFO:root:current mean train loss 1942.5800168695002
INFO:root:current train perplexity4.601816654205322
INFO:root:current mean train loss 1938.314413368155
INFO:root:current train perplexity4.601196765899658
INFO:root:current mean train loss 1939.2591024950266
INFO:root:current train perplexity4.605413436889648
INFO:root:current mean train loss 1941.4107317243304
INFO:root:current train perplexity4.617252349853516
INFO:root:current mean train loss 1941.6669822069084
INFO:root:current train perplexity4.617630958557129
INFO:root:current mean train loss 1941.6368592350939
INFO:root:current train perplexity4.618847846984863
INFO:root:current mean train loss 1940.21628320651
INFO:root:current train perplexity4.615845680236816
INFO:root:current mean train loss 1942.0965447676808
INFO:root:current train perplexity4.619622230529785
INFO:root:current mean train loss 1942.0182301283235
INFO:root:current train perplexity4.623812198638916
INFO:root:current mean train loss 1942.7524273018398
INFO:root:current train perplexity4.6288743019104
INFO:root:current mean train loss 1941.6771315644608
INFO:root:current train perplexity4.628013610839844
INFO:root:current mean train loss 1941.3334161650025
INFO:root:current train perplexity4.629596710205078
INFO:root:current mean train loss 1941.5071180772075
INFO:root:current train perplexity4.629354476928711
INFO:root:current mean train loss 1942.3542539067766
INFO:root:current train perplexity4.631088733673096
INFO:root:current mean train loss 1943.3341303096213
INFO:root:current train perplexity4.63356351852417

100%|██████████| 1/1 [07:34<00:00, 454.30s/it][A100%|██████████| 1/1 [07:34<00:00, 454.30s/it]
INFO:root:final mean train loss: 1943.11434297855
INFO:root:final train perplexity: 4.634376049041748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 1895.1247004515735
INFO:root:eval perplexity: 4.634659290313721
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.04s/it][A100%|██████████| 1/1 [00:37<00:00, 37.04s/it]
INFO:root:eval mean loss: 2299.341077872202
INFO:root:eval perplexity: 6.621712684631348
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/16
  8%|▊         | 16/200 [2:22:42<27:19:28, 534.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1921.7174106651628
INFO:root:current train perplexity4.530433654785156
INFO:root:current mean train loss 1923.5917276304367
INFO:root:current train perplexity4.55257511138916
INFO:root:current mean train loss 1934.6627656718463
INFO:root:current train perplexity4.567234992980957
INFO:root:current mean train loss 1933.811725462222
INFO:root:current train perplexity4.573369979858398
INFO:root:current mean train loss 1930.330165466179
INFO:root:current train perplexity4.567319869995117
INFO:root:current mean train loss 1928.4108350122456
INFO:root:current train perplexity4.564104080200195
INFO:root:current mean train loss 1925.9518475369086
INFO:root:current train perplexity4.565357685089111
INFO:root:current mean train loss 1928.6357011807413
INFO:root:current train perplexity4.573063850402832
INFO:root:current mean train loss 1928.0050591207125
INFO:root:current train perplexity4.573695182800293
INFO:root:current mean train loss 1928.564302894286
INFO:root:current train perplexity4.573434829711914
INFO:root:current mean train loss 1928.6955732813958
INFO:root:current train perplexity4.573320388793945
INFO:root:current mean train loss 1927.855405682076
INFO:root:current train perplexity4.574588298797607
INFO:root:current mean train loss 1927.7272757133285
INFO:root:current train perplexity4.577643394470215
INFO:root:current mean train loss 1929.7103703315897
INFO:root:current train perplexity4.585115909576416
INFO:root:current mean train loss 1929.9057073638521
INFO:root:current train perplexity4.584006309509277
INFO:root:current mean train loss 1929.1992698004108
INFO:root:current train perplexity4.581875801086426
INFO:root:current mean train loss 1928.8333125864938
INFO:root:current train perplexity4.579112529754639
INFO:root:current mean train loss 1929.2015101703664
INFO:root:current train perplexity4.582102298736572
INFO:root:current mean train loss 1929.6082552283412
INFO:root:current train perplexity4.581171035766602
INFO:root:current mean train loss 1928.7811499779023
INFO:root:current train perplexity4.579780101776123

100%|██████████| 1/1 [07:40<00:00, 460.51s/it][A100%|██████████| 1/1 [07:40<00:00, 460.51s/it]
INFO:root:final mean train loss: 1928.0263328686906
INFO:root:final train perplexity: 4.579520225524902
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 1889.5386733467697
INFO:root:eval perplexity: 4.613757610321045
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.68s/it][A100%|██████████| 1/1 [00:36<00:00, 36.68s/it]
INFO:root:eval mean loss: 2296.152890902039
INFO:root:eval perplexity: 6.604382038116455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/17
  8%|▊         | 17/200 [2:31:41<27:13:53, 535.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1887.5387490012429
INFO:root:current train perplexity4.461255073547363
INFO:root:current mean train loss 1906.475691125748
INFO:root:current train perplexity4.477921009063721
INFO:root:current mean train loss 1907.9064793056912
INFO:root:current train perplexity4.483691215515137
INFO:root:current mean train loss 1909.1297535060608
INFO:root:current train perplexity4.481583595275879
INFO:root:current mean train loss 1905.7917012699315
INFO:root:current train perplexity4.47581672668457
INFO:root:current mean train loss 1905.0001459446082
INFO:root:current train perplexity4.47943115234375
INFO:root:current mean train loss 1907.3548805769099
INFO:root:current train perplexity4.483704566955566
INFO:root:current mean train loss 1907.7113495647604
INFO:root:current train perplexity4.486298561096191
INFO:root:current mean train loss 1908.4321132350613
INFO:root:current train perplexity4.493337631225586
INFO:root:current mean train loss 1907.5472428171258
INFO:root:current train perplexity4.495105266571045
INFO:root:current mean train loss 1906.5219493192785
INFO:root:current train perplexity4.497175693511963
INFO:root:current mean train loss 1904.7961249046455
INFO:root:current train perplexity4.492573261260986
INFO:root:current mean train loss 1904.3071761990186
INFO:root:current train perplexity4.4937357902526855
INFO:root:current mean train loss 1903.9867346156227
INFO:root:current train perplexity4.495418548583984
INFO:root:current mean train loss 1906.0012535177252
INFO:root:current train perplexity4.500462055206299
INFO:root:current mean train loss 1905.3736154090248
INFO:root:current train perplexity4.500481605529785
INFO:root:current mean train loss 1906.1809989603776
INFO:root:current train perplexity4.502047538757324
INFO:root:current mean train loss 1906.4707662083158
INFO:root:current train perplexity4.501429557800293
INFO:root:current mean train loss 1907.739436779992
INFO:root:current train perplexity4.503591537475586

100%|██████████| 1/1 [07:34<00:00, 454.76s/it][A100%|██████████| 1/1 [07:34<00:00, 454.76s/it]
INFO:root:final mean train loss: 1906.5005257703172
INFO:root:final train perplexity: 4.502379417419434
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.66s/it][A100%|██████████| 1/1 [00:40<00:00, 40.66s/it]
INFO:root:eval mean loss: 1876.1247437389184
INFO:root:eval perplexity: 4.563946723937988
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.89s/it][A100%|██████████| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 2285.3377018921765
INFO:root:eval perplexity: 6.545918941497803
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/18
  9%|▉         | 18/200 [2:40:35<27:04:04, 535.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1878.3076416015624
INFO:root:current train perplexity4.479570388793945
INFO:root:current mean train loss 1892.488488188244
INFO:root:current train perplexity4.415912628173828
INFO:root:current mean train loss 1882.814052972561
INFO:root:current train perplexity4.40804386138916
INFO:root:current mean train loss 1887.5535460425206
INFO:root:current train perplexity4.411490440368652
INFO:root:current mean train loss 1888.2193271966628
INFO:root:current train perplexity4.414111137390137
INFO:root:current mean train loss 1887.825672474474
INFO:root:current train perplexity4.432651996612549
INFO:root:current mean train loss 1885.3335927411545
INFO:root:current train perplexity4.4249091148376465
INFO:root:current mean train loss 1886.9931422456782
INFO:root:current train perplexity4.430428981781006
INFO:root:current mean train loss 1884.7149159307064
INFO:root:current train perplexity4.4202375411987305
INFO:root:current mean train loss 1884.859169301407
INFO:root:current train perplexity4.426168918609619
INFO:root:current mean train loss 1886.0291134658737
INFO:root:current train perplexity4.433027267456055
INFO:root:current mean train loss 1884.675921768948
INFO:root:current train perplexity4.432620525360107
INFO:root:current mean train loss 1888.408955402295
INFO:root:current train perplexity4.442210674285889
INFO:root:current mean train loss 1888.9641265752216
INFO:root:current train perplexity4.4412455558776855
INFO:root:current mean train loss 1888.3107804159254
INFO:root:current train perplexity4.440489292144775
INFO:root:current mean train loss 1889.3434471358491
INFO:root:current train perplexity4.4405412673950195
INFO:root:current mean train loss 1889.3262827650408
INFO:root:current train perplexity4.441482067108154
INFO:root:current mean train loss 1890.1549012841367
INFO:root:current train perplexity4.440823078155518
INFO:root:current mean train loss 1889.7385322211521
INFO:root:current train perplexity4.439740180969238
INFO:root:current mean train loss 1889.1009361287115
INFO:root:current train perplexity4.439267635345459

100%|██████████| 1/1 [07:29<00:00, 449.89s/it][A100%|██████████| 1/1 [07:29<00:00, 449.89s/it]
INFO:root:final mean train loss: 1887.9637139070774
INFO:root:final train perplexity: 4.43699312210083
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.20s/it][A100%|██████████| 1/1 [00:40<00:00, 40.20s/it]
INFO:root:eval mean loss: 1865.922324755513
INFO:root:eval perplexity: 4.526421546936035
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.57s/it][A100%|██████████| 1/1 [00:38<00:00, 38.57s/it]
INFO:root:eval mean loss: 2277.0641851763353
INFO:root:eval perplexity: 6.501543998718262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/19
 10%|▉         | 19/200 [2:49:27<26:51:24, 534.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1891.2715010209517
INFO:root:current train perplexity4.468321800231934
INFO:root:current mean train loss 1862.2061047163165
INFO:root:current train perplexity4.386104583740234
INFO:root:current mean train loss 1863.8969149202915
INFO:root:current train perplexity4.377764701843262
INFO:root:current mean train loss 1872.4611266710745
INFO:root:current train perplexity4.395513534545898
INFO:root:current mean train loss 1875.5675031472156
INFO:root:current train perplexity4.398024559020996
INFO:root:current mean train loss 1868.629003532088
INFO:root:current train perplexity4.3890485763549805
INFO:root:current mean train loss 1868.2538740642585
INFO:root:current train perplexity4.38606595993042
INFO:root:current mean train loss 1865.8667585631817
INFO:root:current train perplexity4.380000591278076
INFO:root:current mean train loss 1867.5194311582839
INFO:root:current train perplexity4.380373001098633
INFO:root:current mean train loss 1870.9360743458515
INFO:root:current train perplexity4.383107662200928
INFO:root:current mean train loss 1870.156827265969
INFO:root:current train perplexity4.378856658935547
INFO:root:current mean train loss 1870.0223727030764
INFO:root:current train perplexity4.377209186553955
INFO:root:current mean train loss 1871.4592250193396
INFO:root:current train perplexity4.379986763000488
INFO:root:current mean train loss 1871.1686899066874
INFO:root:current train perplexity4.377194881439209
INFO:root:current mean train loss 1869.6574385974309
INFO:root:current train perplexity4.373425006866455
INFO:root:current mean train loss 1870.4468202528283
INFO:root:current train perplexity4.374423980712891
INFO:root:current mean train loss 1870.863844564605
INFO:root:current train perplexity4.375195503234863
INFO:root:current mean train loss 1870.490571167134
INFO:root:current train perplexity4.374284744262695
INFO:root:current mean train loss 1871.2258755697508
INFO:root:current train perplexity4.377758026123047
INFO:root:current mean train loss 1871.6665238490586
INFO:root:current train perplexity4.379001617431641

100%|██████████| 1/1 [07:41<00:00, 461.60s/it][A100%|██████████| 1/1 [07:41<00:00, 461.60s/it]
INFO:root:final mean train loss: 1871.5847980231874
INFO:root:final train perplexity: 4.380008697509766
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.78s/it][A100%|██████████| 1/1 [00:38<00:00, 38.78s/it]
INFO:root:eval mean loss: 1879.9926861702127
INFO:root:eval perplexity: 4.578254222869873
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.93s/it][A100%|██████████| 1/1 [00:36<00:00, 36.93s/it]
INFO:root:eval mean loss: 2291.908315239223
INFO:root:eval perplexity: 6.581373691558838
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/20
 10%|█         | 20/200 [2:58:26<26:47:21, 535.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1831.1083076672676
INFO:root:current train perplexity4.279441833496094
INFO:root:current mean train loss 1836.304836794627
INFO:root:current train perplexity4.3061137199401855
INFO:root:current mean train loss 1850.306593635591
INFO:root:current train perplexity4.314294815063477
INFO:root:current mean train loss 1854.6195655305125
INFO:root:current train perplexity4.321639060974121
INFO:root:current mean train loss 1850.5100459140092
INFO:root:current train perplexity4.320234298706055
INFO:root:current mean train loss 1851.9973672219244
INFO:root:current train perplexity4.320664405822754
INFO:root:current mean train loss 1851.4937144295898
INFO:root:current train perplexity4.325384616851807
INFO:root:current mean train loss 1853.8881005066496
INFO:root:current train perplexity4.328442096710205
INFO:root:current mean train loss 1854.9175692207054
INFO:root:current train perplexity4.327837944030762
INFO:root:current mean train loss 1853.0080080205005
INFO:root:current train perplexity4.318235397338867
INFO:root:current mean train loss 1854.1530913278618
INFO:root:current train perplexity4.321874141693115
INFO:root:current mean train loss 1854.325211817178
INFO:root:current train perplexity4.320903778076172
INFO:root:current mean train loss 1855.624253686346
INFO:root:current train perplexity4.318681716918945
INFO:root:current mean train loss 1854.0624164926016
INFO:root:current train perplexity4.316575050354004
INFO:root:current mean train loss 1852.9970392647347
INFO:root:current train perplexity4.318017482757568
INFO:root:current mean train loss 1853.3816638968842
INFO:root:current train perplexity4.315356254577637
INFO:root:current mean train loss 1852.5594293991192
INFO:root:current train perplexity4.313317775726318
INFO:root:current mean train loss 1852.6649270301718
INFO:root:current train perplexity4.314224720001221
INFO:root:current mean train loss 1852.4951971737569
INFO:root:current train perplexity4.314032077789307
INFO:root:current mean train loss 1854.0082369445831
INFO:root:current train perplexity4.318511962890625

100%|██████████| 1/1 [07:30<00:00, 450.71s/it][A100%|██████████| 1/1 [07:30<00:00, 450.71s/it]
INFO:root:final mean train loss: 1853.2074568955752
INFO:root:final train perplexity: 4.316941738128662
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.56s/it][A100%|██████████| 1/1 [00:40<00:00, 40.56s/it]
INFO:root:eval mean loss: 1846.5242781402371
INFO:root:eval perplexity: 4.455924987792969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.77s/it][A100%|██████████| 1/1 [00:38<00:00, 38.77s/it]
INFO:root:eval mean loss: 2260.051850447418
INFO:root:eval perplexity: 6.411244869232178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/21
 10%|█         | 21/200 [3:07:19<26:35:34, 534.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1850.238305228097
INFO:root:current train perplexity4.285877704620361
INFO:root:current mean train loss 1828.8591997195513
INFO:root:current train perplexity4.235556602478027
INFO:root:current mean train loss 1829.4347548484802
INFO:root:current train perplexity4.226832866668701
INFO:root:current mean train loss 1824.1387325672622
INFO:root:current train perplexity4.2322516441345215
INFO:root:current mean train loss 1823.2455626370613
INFO:root:current train perplexity4.229763031005859
INFO:root:current mean train loss 1826.1824580130817
INFO:root:current train perplexity4.235334873199463
INFO:root:current mean train loss 1825.5159288732018
INFO:root:current train perplexity4.235240459442139
INFO:root:current mean train loss 1827.406684835121
INFO:root:current train perplexity4.238238334655762
INFO:root:current mean train loss 1828.9885677444602
INFO:root:current train perplexity4.233707904815674
INFO:root:current mean train loss 1828.6650550235763
INFO:root:current train perplexity4.234549045562744
INFO:root:current mean train loss 1829.233500509551
INFO:root:current train perplexity4.235742092132568
INFO:root:current mean train loss 1832.2258107538455
INFO:root:current train perplexity4.242637634277344
INFO:root:current mean train loss 1832.0257032843913
INFO:root:current train perplexity4.243401527404785
INFO:root:current mean train loss 1831.9255019106345
INFO:root:current train perplexity4.245901107788086
INFO:root:current mean train loss 1832.2111875093901
INFO:root:current train perplexity4.247407913208008
INFO:root:current mean train loss 1832.7354814779483
INFO:root:current train perplexity4.248473644256592
INFO:root:current mean train loss 1831.7513047370358
INFO:root:current train perplexity4.247419834136963
INFO:root:current mean train loss 1832.1838173833687
INFO:root:current train perplexity4.249155521392822
INFO:root:current mean train loss 1833.0853646377038
INFO:root:current train perplexity4.250200271606445
INFO:root:current mean train loss 1834.1089472546412
INFO:root:current train perplexity4.251061916351318

100%|██████████| 1/1 [07:35<00:00, 455.79s/it][A100%|██████████| 1/1 [07:35<00:00, 455.79s/it]
INFO:root:final mean train loss: 1833.784815647358
INFO:root:final train perplexity: 4.251275539398193
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.41s/it][A100%|██████████| 1/1 [00:40<00:00, 40.41s/it]
INFO:root:eval mean loss: 1841.1681354062778
INFO:root:eval perplexity: 4.4366536140441895
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.02s/it][A100%|██████████| 1/1 [00:38<00:00, 38.03s/it]
INFO:root:eval mean loss: 2257.689971274518
INFO:root:eval perplexity: 6.398808479309082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/22
 11%|█         | 22/200 [3:16:16<26:28:38, 535.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1806.7733438570206
INFO:root:current train perplexity4.164560794830322
INFO:root:current mean train loss 1799.6518350060965
INFO:root:current train perplexity4.185164451599121
INFO:root:current mean train loss 1799.2513526106056
INFO:root:current train perplexity4.163809776306152
INFO:root:current mean train loss 1804.2413120627723
INFO:root:current train perplexity4.175398826599121
INFO:root:current mean train loss 1806.4742645844344
INFO:root:current train perplexity4.173606872558594
INFO:root:current mean train loss 1806.8983976620445
INFO:root:current train perplexity4.175301551818848
INFO:root:current mean train loss 1810.5971315109005
INFO:root:current train perplexity4.180174350738525
INFO:root:current mean train loss 1810.3962757658373
INFO:root:current train perplexity4.17659854888916
INFO:root:current mean train loss 1810.800489148187
INFO:root:current train perplexity4.181488990783691
INFO:root:current mean train loss 1811.4160131158467
INFO:root:current train perplexity4.181736469268799
INFO:root:current mean train loss 1813.2988050306164
INFO:root:current train perplexity4.18548059463501
INFO:root:current mean train loss 1813.426307723752
INFO:root:current train perplexity4.185307025909424
INFO:root:current mean train loss 1813.1236951038393
INFO:root:current train perplexity4.184497356414795
INFO:root:current mean train loss 1812.7090897306593
INFO:root:current train perplexity4.186252117156982
INFO:root:current mean train loss 1811.9189938754348
INFO:root:current train perplexity4.183769226074219
INFO:root:current mean train loss 1812.0876059753457
INFO:root:current train perplexity4.182437419891357
INFO:root:current mean train loss 1813.058519179851
INFO:root:current train perplexity4.183281898498535
INFO:root:current mean train loss 1814.6483312650919
INFO:root:current train perplexity4.187277793884277
INFO:root:current mean train loss 1813.7161504172159
INFO:root:current train perplexity4.184629440307617
INFO:root:current mean train loss 1814.1515776706237
INFO:root:current train perplexity4.184531211853027

100%|██████████| 1/1 [07:39<00:00, 459.75s/it][A100%|██████████| 1/1 [07:39<00:00, 459.75s/it]
INFO:root:final mean train loss: 1813.5839306037353
INFO:root:final train perplexity: 4.1840362548828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.60s/it][A100%|██████████| 1/1 [00:39<00:00, 39.60s/it]
INFO:root:eval mean loss: 1838.8002319335938
INFO:root:eval perplexity: 4.428159713745117
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.56s/it][A100%|██████████| 1/1 [00:37<00:00, 37.56s/it]
INFO:root:eval mean loss: 2256.9259950894834
INFO:root:eval perplexity: 6.39478874206543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/23
 12%|█▏        | 23/200 [3:25:16<26:23:31, 536.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1774.5276909722222
INFO:root:current train perplexity4.079501628875732
INFO:root:current mean train loss 1760.4063900596218
INFO:root:current train perplexity4.070676803588867
INFO:root:current mean train loss 1772.9502113079202
INFO:root:current train perplexity4.101211071014404
INFO:root:current mean train loss 1775.0899263822116
INFO:root:current train perplexity4.095335483551025
INFO:root:current mean train loss 1779.062307676977
INFO:root:current train perplexity4.096307277679443
INFO:root:current mean train loss 1785.16311966201
INFO:root:current train perplexity4.105656623840332
INFO:root:current mean train loss 1787.9113091952559
INFO:root:current train perplexity4.1083526611328125
INFO:root:current mean train loss 1789.500782022597
INFO:root:current train perplexity4.112790107727051
INFO:root:current mean train loss 1786.755243262816
INFO:root:current train perplexity4.107491493225098
INFO:root:current mean train loss 1789.649432558002
INFO:root:current train perplexity4.11082649230957
INFO:root:current mean train loss 1788.5246007516844
INFO:root:current train perplexity4.106726169586182
INFO:root:current mean train loss 1790.036148096934
INFO:root:current train perplexity4.108967304229736
INFO:root:current mean train loss 1790.9970217682594
INFO:root:current train perplexity4.110545635223389
INFO:root:current mean train loss 1789.3889383219987
INFO:root:current train perplexity4.1084675788879395
INFO:root:current mean train loss 1790.5422127333263
INFO:root:current train perplexity4.110089302062988
INFO:root:current mean train loss 1790.9485138131388
INFO:root:current train perplexity4.111414432525635
INFO:root:current mean train loss 1809.7172577807185
INFO:root:current train perplexity4.1712799072265625
INFO:root:current mean train loss 1810.5762550055647
INFO:root:current train perplexity4.171336650848389
INFO:root:current mean train loss 1810.3996820359002
INFO:root:current train perplexity4.171377182006836

100%|██████████| 1/1 [07:29<00:00, 449.45s/it][A100%|██████████| 1/1 [07:29<00:00, 449.45s/it]
INFO:root:final mean train loss: 1809.7341578835137
INFO:root:final train perplexity: 4.171343803405762
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.88s/it][A100%|██████████| 1/1 [00:38<00:00, 38.88s/it]
INFO:root:eval mean loss: 1831.0337974602448
INFO:root:eval perplexity: 4.400417804718018
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.01s/it][A100%|██████████| 1/1 [00:38<00:00, 38.01s/it]
INFO:root:eval mean loss: 2251.0855163141346
INFO:root:eval perplexity: 6.364158630371094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/24
 12%|█▏        | 24/200 [3:34:04<26:07:34, 534.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1603.0916050502233
INFO:root:current train perplexity3.7222087383270264
INFO:root:current mean train loss 1755.9309618227949
INFO:root:current train perplexity4.030855178833008
INFO:root:current mean train loss 1757.0022002141832
INFO:root:current train perplexity4.025963306427002
INFO:root:current mean train loss 1767.4648334117976
INFO:root:current train perplexity4.039278030395508
INFO:root:current mean train loss 1771.6993684136019
INFO:root:current train perplexity4.044337749481201
INFO:root:current mean train loss 1789.6313575278136
INFO:root:current train perplexity4.100176811218262
INFO:root:current mean train loss 1790.1893224072023
INFO:root:current train perplexity4.104084491729736
INFO:root:current mean train loss 1792.8296349078964
INFO:root:current train perplexity4.110270023345947
INFO:root:current mean train loss 1790.6326385460231
INFO:root:current train perplexity4.10650634765625
INFO:root:current mean train loss 1791.214492074447
INFO:root:current train perplexity4.106180667877197
INFO:root:current mean train loss 1789.640493110725
INFO:root:current train perplexity4.103583335876465
INFO:root:current mean train loss 1788.141157389764
INFO:root:current train perplexity4.09871768951416
INFO:root:current mean train loss 1789.6321865736006
INFO:root:current train perplexity4.099615573883057
INFO:root:current mean train loss 1787.6474580421827
INFO:root:current train perplexity4.093739032745361
INFO:root:current mean train loss 1787.2565354900996
INFO:root:current train perplexity4.090510368347168
INFO:root:current mean train loss 1786.3959947167127
INFO:root:current train perplexity4.08864164352417
INFO:root:current mean train loss 1784.5775043936196
INFO:root:current train perplexity4.087054252624512
INFO:root:current mean train loss 1784.5974602366862
INFO:root:current train perplexity4.087601184844971
INFO:root:current mean train loss 1783.5907216159164
INFO:root:current train perplexity4.085819244384766
INFO:root:current mean train loss 1782.796882297334
INFO:root:current train perplexity4.082579135894775

100%|██████████| 1/1 [07:34<00:00, 454.90s/it][A100%|██████████| 1/1 [07:34<00:00, 454.90s/it]
INFO:root:final mean train loss: 1782.3030409358453
INFO:root:final train perplexity: 4.082009792327881
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.85s/it][A100%|██████████| 1/1 [00:38<00:00, 38.85s/it]
INFO:root:eval mean loss: 1816.1611652780086
INFO:root:eval perplexity: 4.347775459289551
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.00s/it][A100%|██████████| 1/1 [00:37<00:00, 37.00s/it]
INFO:root:eval mean loss: 2237.506713434314
INFO:root:eval perplexity: 6.293506622314453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/25
 12%|█▎        | 25/200 [3:42:58<25:57:49, 534.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1822.6493886311848
INFO:root:current train perplexity4.0161285400390625
INFO:root:current mean train loss 1765.2721596994709
INFO:root:current train perplexity3.9803285598754883
INFO:root:current mean train loss 1762.4839684622627
INFO:root:current train perplexity3.9847936630249023
INFO:root:current mean train loss 1762.8130553445699
INFO:root:current train perplexity3.9919800758361816
INFO:root:current mean train loss 1767.3752136230469
INFO:root:current train perplexity4.011922836303711
INFO:root:current mean train loss 1766.6795365428197
INFO:root:current train perplexity4.0182390213012695
INFO:root:current mean train loss 1765.7722643338716
INFO:root:current train perplexity4.020018577575684
INFO:root:current mean train loss 1765.7726410080716
INFO:root:current train perplexity4.0193047523498535
INFO:root:current mean train loss 1767.4131235659702
INFO:root:current train perplexity4.022075176239014
INFO:root:current mean train loss 1767.507310215013
INFO:root:current train perplexity4.025061130523682
INFO:root:current mean train loss 1765.685386300087
INFO:root:current train perplexity4.021767616271973
INFO:root:current mean train loss 1763.804491470717
INFO:root:current train perplexity4.018947124481201
INFO:root:current mean train loss 1764.1164731293723
INFO:root:current train perplexity4.018272876739502
INFO:root:current mean train loss 1762.685619803714
INFO:root:current train perplexity4.01818323135376
INFO:root:current mean train loss 1762.4503026383647
INFO:root:current train perplexity4.016212463378906
INFO:root:current mean train loss 1761.9248800603111
INFO:root:current train perplexity4.015793800354004
INFO:root:current mean train loss 1761.7078780752097
INFO:root:current train perplexity4.014162540435791
INFO:root:current mean train loss 1760.4621332084496
INFO:root:current train perplexity4.011874198913574
INFO:root:current mean train loss 1761.682538350423
INFO:root:current train perplexity4.015048027038574
INFO:root:current mean train loss 1760.5616272987793
INFO:root:current train perplexity4.010756492614746

100%|██████████| 1/1 [07:31<00:00, 451.87s/it][A100%|██████████| 1/1 [07:31<00:00, 451.87s/it]
INFO:root:final mean train loss: 1760.4246727986222
INFO:root:final train perplexity: 4.012134552001953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.23s/it][A100%|██████████| 1/1 [00:39<00:00, 39.23s/it]
INFO:root:eval mean loss: 1814.0989981576906
INFO:root:eval perplexity: 4.340526103973389
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 2237.377633169188
INFO:root:eval perplexity: 6.292839050292969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/26
 13%|█▎        | 26/200 [3:51:50<25:47:10, 533.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1726.3222805116234
INFO:root:current train perplexity3.9327080249786377
INFO:root:current mean train loss 1725.8098923703458
INFO:root:current train perplexity3.9159176349639893
INFO:root:current mean train loss 1737.1756495558868
INFO:root:current train perplexity3.951090097427368
INFO:root:current mean train loss 1741.6729117027126
INFO:root:current train perplexity3.9579858779907227
INFO:root:current mean train loss 1738.8287821313422
INFO:root:current train perplexity3.9464199542999268
INFO:root:current mean train loss 1741.0772233494108
INFO:root:current train perplexity3.951054573059082
INFO:root:current mean train loss 1742.0985132178726
INFO:root:current train perplexity3.9546122550964355
INFO:root:current mean train loss 1741.0377854567307
INFO:root:current train perplexity3.947582960128784
INFO:root:current mean train loss 1742.292447955373
INFO:root:current train perplexity3.9508328437805176
INFO:root:current mean train loss 1741.150870474188
INFO:root:current train perplexity3.9498519897460938
INFO:root:current mean train loss 1740.8563998146315
INFO:root:current train perplexity3.9524857997894287
INFO:root:current mean train loss 1741.8016003300286
INFO:root:current train perplexity3.951268434524536
INFO:root:current mean train loss 1742.9101346098157
INFO:root:current train perplexity3.9555699825286865
INFO:root:current mean train loss 1743.4604959167891
INFO:root:current train perplexity3.957728862762451
INFO:root:current mean train loss 1743.8492241885247
INFO:root:current train perplexity3.9587197303771973
INFO:root:current mean train loss 1745.050485302766
INFO:root:current train perplexity3.960852861404419
INFO:root:current mean train loss 1745.7356826624152
INFO:root:current train perplexity3.9629266262054443
INFO:root:current mean train loss 1745.8212710429307
INFO:root:current train perplexity3.9628968238830566
INFO:root:current mean train loss 1746.581006946802
INFO:root:current train perplexity3.9658401012420654
INFO:root:current mean train loss 1746.7214071832939
INFO:root:current train perplexity3.9676432609558105

100%|██████████| 1/1 [07:31<00:00, 451.87s/it][A100%|██████████| 1/1 [07:31<00:00, 451.87s/it]
INFO:root:final mean train loss: 1746.185856575324
INFO:root:final train perplexity: 3.967301368713379
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.81s/it][A100%|██████████| 1/1 [00:39<00:00, 39.81s/it]
INFO:root:eval mean loss: 1807.3314468708445
INFO:root:eval perplexity: 4.3168206214904785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.98s/it][A100%|██████████| 1/1 [00:36<00:00, 36.98s/it]
INFO:root:eval mean loss: 2230.9786337994515
INFO:root:eval perplexity: 6.25982141494751
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/27
 14%|█▎        | 27/200 [4:00:41<25:36:07, 532.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1732.4628990436422
INFO:root:current train perplexity3.9666860103607178
INFO:root:current mean train loss 1726.029877095283
INFO:root:current train perplexity3.929845094680786
INFO:root:current mean train loss 1730.1192182200823
INFO:root:current train perplexity3.934309959411621
INFO:root:current mean train loss 1728.0609246792073
INFO:root:current train perplexity3.9240992069244385
INFO:root:current mean train loss 1723.8343255322052
INFO:root:current train perplexity3.908357620239258
INFO:root:current mean train loss 1727.038332484529
INFO:root:current train perplexity3.912388801574707
INFO:root:current mean train loss 1726.5450953335749
INFO:root:current train perplexity3.9167332649230957
INFO:root:current mean train loss 1727.8475399772221
INFO:root:current train perplexity3.9138381481170654
INFO:root:current mean train loss 1725.8978829194893
INFO:root:current train perplexity3.9140541553497314
INFO:root:current mean train loss 1726.6795187932216
INFO:root:current train perplexity3.913937568664551
INFO:root:current mean train loss 1728.6979747404448
INFO:root:current train perplexity3.918458938598633
INFO:root:current mean train loss 1729.8447717854397
INFO:root:current train perplexity3.9174818992614746
INFO:root:current mean train loss 1730.6254141463385
INFO:root:current train perplexity3.9196577072143555
INFO:root:current mean train loss 1731.147610603973
INFO:root:current train perplexity3.920746088027954
INFO:root:current mean train loss 1731.584013343675
INFO:root:current train perplexity3.92313289642334
INFO:root:current mean train loss 1731.6397723098773
INFO:root:current train perplexity3.9244651794433594
INFO:root:current mean train loss 1730.9956528096493
INFO:root:current train perplexity3.9245779514312744
INFO:root:current mean train loss 1732.2695781200005
INFO:root:current train perplexity3.925863265991211
INFO:root:current mean train loss 1733.3880111316562
INFO:root:current train perplexity3.928105115890503
INFO:root:current mean train loss 1734.1983219259728
INFO:root:current train perplexity3.929638147354126

100%|██████████| 1/1 [07:32<00:00, 452.66s/it][A100%|██████████| 1/1 [07:32<00:00, 452.66s/it]
INFO:root:final mean train loss: 1734.29931138924
INFO:root:final train perplexity: 3.9302587509155273
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.93s/it][A100%|██████████| 1/1 [00:39<00:00, 39.93s/it]
INFO:root:eval mean loss: 1820.5931799056682
INFO:root:eval perplexity: 4.363397121429443
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.43s/it][A100%|██████████| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 2246.6828037559562
INFO:root:eval perplexity: 6.3411641120910645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/28
 14%|█▍        | 28/200 [4:09:34<25:27:03, 532.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.7959684244793
INFO:root:current train perplexity3.9169869422912598
INFO:root:current mean train loss 1723.7631389508929
INFO:root:current train perplexity3.897453546524048
INFO:root:current mean train loss 1728.2922128018465
INFO:root:current train perplexity3.9059243202209473
INFO:root:current mean train loss 1724.0786917317707
INFO:root:current train perplexity3.893223762512207
INFO:root:current mean train loss 1722.0205825966284
INFO:root:current train perplexity3.8845629692077637
INFO:root:current mean train loss 1721.2202802309782
INFO:root:current train perplexity3.8765461444854736
INFO:root:current mean train loss 1721.8764440465857
INFO:root:current train perplexity3.879220962524414
INFO:root:current mean train loss 1722.2346594632056
INFO:root:current train perplexity3.8819422721862793
INFO:root:current mean train loss 1719.6028809988838
INFO:root:current train perplexity3.8807857036590576
INFO:root:current mean train loss 1722.9862762920673
INFO:root:current train perplexity3.886678457260132
INFO:root:current mean train loss 1723.639689998183
INFO:root:current train perplexity3.8907105922698975
INFO:root:current mean train loss 1722.30777624252
INFO:root:current train perplexity3.889474630355835
INFO:root:current mean train loss 1720.2589840877758
INFO:root:current train perplexity3.8860630989074707
INFO:root:current mean train loss 1722.2702840909092
INFO:root:current train perplexity3.889761209487915
INFO:root:current mean train loss 1720.8651123046875
INFO:root:current train perplexity3.8877735137939453
INFO:root:current mean train loss 1721.0051557849702
INFO:root:current train perplexity3.88932728767395
INFO:root:current mean train loss 1721.6526571974114
INFO:root:current train perplexity3.889725923538208
INFO:root:current mean train loss 1722.947451722051
INFO:root:current train perplexity3.893409490585327
INFO:root:current mean train loss 1724.0905336588542
INFO:root:current train perplexity3.89730167388916
INFO:root:current mean train loss 1723.3008124011076
INFO:root:current train perplexity3.894915819168091

100%|██████████| 1/1 [07:37<00:00, 457.24s/it][A100%|██████████| 1/1 [07:37<00:00, 457.24s/it]
INFO:root:final mean train loss: 1722.9448582605467
INFO:root:final train perplexity: 3.895197629928589
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.11s/it][A100%|██████████| 1/1 [00:39<00:00, 39.11s/it]
INFO:root:eval mean loss: 1820.903812489611
INFO:root:eval perplexity: 4.364493370056152
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.54s/it][A100%|██████████| 1/1 [00:39<00:00, 39.54s/it]
INFO:root:eval mean loss: 2248.297771480912
INFO:root:eval perplexity: 6.3495869636535645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/29
 14%|█▍        | 29/200 [4:18:32<25:23:11, 534.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1719.544826341712
INFO:root:current train perplexity3.902498960494995
INFO:root:current mean train loss 1711.152043024699
INFO:root:current train perplexity3.8828351497650146
INFO:root:current mean train loss 1708.654973278307
INFO:root:current train perplexity3.8755767345428467
INFO:root:current mean train loss 1713.059251434949
INFO:root:current train perplexity3.8823583126068115
INFO:root:current mean train loss 1713.8800934582223
INFO:root:current train perplexity3.877868413925171
INFO:root:current mean train loss 1712.632769198031
INFO:root:current train perplexity3.872140407562256
INFO:root:current mean train loss 1711.8034768517996
INFO:root:current train perplexity3.8695669174194336
INFO:root:current mean train loss 1712.5890937573981
INFO:root:current train perplexity3.871943473815918
INFO:root:current mean train loss 1712.126515888847
INFO:root:current train perplexity3.8705759048461914
INFO:root:current mean train loss 1714.5045696381599
INFO:root:current train perplexity3.873652458190918
INFO:root:current mean train loss 1714.510250999814
INFO:root:current train perplexity3.8731751441955566
INFO:root:current mean train loss 1713.775270500439
INFO:root:current train perplexity3.8706552982330322
INFO:root:current mean train loss 1713.3730938323881
INFO:root:current train perplexity3.8694369792938232
INFO:root:current mean train loss 1713.661987041605
INFO:root:current train perplexity3.871211051940918
INFO:root:current mean train loss 1715.1522896692516
INFO:root:current train perplexity3.872997283935547
INFO:root:current mean train loss 1716.1166000749597
INFO:root:current train perplexity3.8743369579315186
INFO:root:current mean train loss 1716.4439918031085
INFO:root:current train perplexity3.8752899169921875
INFO:root:current mean train loss 1718.2769002233233
INFO:root:current train perplexity3.8789350986480713
INFO:root:current mean train loss 1718.0926290435468
INFO:root:current train perplexity3.8783512115478516

100%|██████████| 1/1 [07:32<00:00, 452.52s/it][A100%|██████████| 1/1 [07:32<00:00, 452.52s/it]
INFO:root:final mean train loss: 1717.467440344983
INFO:root:final train perplexity: 3.8783955574035645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.48s/it][A100%|██████████| 1/1 [00:39<00:00, 39.48s/it]
INFO:root:eval mean loss: 1805.0891667359265
INFO:root:eval perplexity: 4.308994770050049
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.11s/it][A100%|██████████| 1/1 [00:37<00:00, 37.11s/it]
INFO:root:eval mean loss: 2234.62341092157
INFO:root:eval perplexity: 6.278605937957764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/30
 15%|█▌        | 30/200 [4:27:24<25:11:50, 533.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1670.4955512152778
INFO:root:current train perplexity3.7940471172332764
INFO:root:current mean train loss 1715.9531473982224
INFO:root:current train perplexity3.8587539196014404
INFO:root:current mean train loss 1714.594971871262
INFO:root:current train perplexity3.8757920265197754
INFO:root:current mean train loss 1711.1521580767092
INFO:root:current train perplexity3.858046531677246
INFO:root:current mean train loss 1706.7610128911026
INFO:root:current train perplexity3.850806474685669
INFO:root:current mean train loss 1709.1946697722249
INFO:root:current train perplexity3.8500466346740723
INFO:root:current mean train loss 1707.2599640644244
INFO:root:current train perplexity3.8523757457733154
INFO:root:current mean train loss 1704.9523837973268
INFO:root:current train perplexity3.846967935562134
INFO:root:current mean train loss 1705.6569059204555
INFO:root:current train perplexity3.8479719161987305
INFO:root:current mean train loss 1710.0595085387445
INFO:root:current train perplexity3.8553922176361084
INFO:root:current mean train loss 1708.627020027758
INFO:root:current train perplexity3.854539394378662
INFO:root:current mean train loss 1710.1888036977289
INFO:root:current train perplexity3.8593969345092773
INFO:root:current mean train loss 1710.3748081607991
INFO:root:current train perplexity3.8608925342559814
INFO:root:current mean train loss 1711.5583370199997
INFO:root:current train perplexity3.8625378608703613
INFO:root:current mean train loss 1711.4794829174336
INFO:root:current train perplexity3.861344575881958
INFO:root:current mean train loss 1713.7238780047578
INFO:root:current train perplexity3.866821050643921
INFO:root:current mean train loss 1714.2118101244464
INFO:root:current train perplexity3.8683462142944336
INFO:root:current mean train loss 1713.9290565771971
INFO:root:current train perplexity3.8675179481506348
INFO:root:current mean train loss 1714.8104659671478
INFO:root:current train perplexity3.86946702003479
INFO:root:current mean train loss 1715.5127634774833
INFO:root:current train perplexity3.8715569972991943

100%|██████████| 1/1 [07:36<00:00, 456.33s/it][A100%|██████████| 1/1 [07:36<00:00, 456.33s/it]
INFO:root:final mean train loss: 1715.7415802879161
INFO:root:final train perplexity: 3.873116970062256
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.87s/it][A100%|██████████| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 1812.3108433933123
INFO:root:eval perplexity: 4.334249973297119
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.50s/it][A100%|██████████| 1/1 [00:36<00:00, 36.50s/it]
INFO:root:eval mean loss: 2241.7969516186004
INFO:root:eval perplexity: 6.315744400024414
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/31
 16%|█▌        | 31/200 [4:36:18<25:03:22, 533.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1666.3941838191106
INFO:root:current train perplexity3.8413665294647217
INFO:root:current mean train loss 1707.8683452303446
INFO:root:current train perplexity3.8441545963287354
INFO:root:current mean train loss 1708.097883106333
INFO:root:current train perplexity3.835261821746826
INFO:root:current mean train loss 1705.0649477718798
INFO:root:current train perplexity3.837839365005493
INFO:root:current mean train loss 1707.8085559253961
INFO:root:current train perplexity3.8397605419158936
INFO:root:current mean train loss 1705.9560356575273
INFO:root:current train perplexity3.830298900604248
INFO:root:current mean train loss 1705.6834981997554
INFO:root:current train perplexity3.8319571018218994
INFO:root:current mean train loss 1704.4454572693376
INFO:root:current train perplexity3.8323755264282227
INFO:root:current mean train loss 1703.1479908940867
INFO:root:current train perplexity3.829780101776123
INFO:root:current mean train loss 1704.1859275867305
INFO:root:current train perplexity3.829629421234131
INFO:root:current mean train loss 1705.8526409067374
INFO:root:current train perplexity3.8340957164764404
INFO:root:current mean train loss 1706.4229547693717
INFO:root:current train perplexity3.8383750915527344
INFO:root:current mean train loss 1707.2953855629653
INFO:root:current train perplexity3.8377950191497803
INFO:root:current mean train loss 1706.4049272954373
INFO:root:current train perplexity3.8387224674224854
INFO:root:current mean train loss 1706.1623194455096
INFO:root:current train perplexity3.838439702987671
INFO:root:current mean train loss 1707.15420136258
INFO:root:current train perplexity3.84173846244812
INFO:root:current mean train loss 1706.9430794240805
INFO:root:current train perplexity3.8442649841308594
INFO:root:current mean train loss 1707.1914056842047
INFO:root:current train perplexity3.845219612121582
INFO:root:current mean train loss 1707.714081378782
INFO:root:current train perplexity3.847970962524414
INFO:root:current mean train loss 1708.0089923862604
INFO:root:current train perplexity3.849112033843994

100%|██████████| 1/1 [07:33<00:00, 453.11s/it][A100%|██████████| 1/1 [07:33<00:00, 453.11s/it]
INFO:root:final mean train loss: 1708.5185894987767
INFO:root:final train perplexity: 3.8511016368865967
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.65s/it][A100%|██████████| 1/1 [00:39<00:00, 39.65s/it]
INFO:root:eval mean loss: 1806.0974532323526
INFO:root:eval perplexity: 4.312512397766113
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.10s/it][A100%|██████████| 1/1 [00:38<00:00, 38.10s/it]
INFO:root:eval mean loss: 2237.3065021054963
INFO:root:eval perplexity: 6.292470932006836
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/32
 16%|█▌        | 32/200 [4:45:11<24:54:05, 533.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1688.3424781976744
INFO:root:current train perplexity3.7727298736572266
INFO:root:current mean train loss 1699.0867902849104
INFO:root:current train perplexity3.800401449203491
INFO:root:current mean train loss 1706.8540395728844
INFO:root:current train perplexity3.8265433311462402
INFO:root:current mean train loss 1707.7932805097485
INFO:root:current train perplexity3.828674554824829
INFO:root:current mean train loss 1700.0560512155228
INFO:root:current train perplexity3.816373586654663
INFO:root:current mean train loss 1705.0053389463196
INFO:root:current train perplexity3.8279716968536377
INFO:root:current mean train loss 1704.544406256075
INFO:root:current train perplexity3.8279595375061035
INFO:root:current mean train loss 1705.5503672295592
INFO:root:current train perplexity3.831472635269165
INFO:root:current mean train loss 1704.3351729314948
INFO:root:current train perplexity3.837181568145752
INFO:root:current mean train loss 1706.746614264026
INFO:root:current train perplexity3.8426883220672607
INFO:root:current mean train loss 1706.9735016132474
INFO:root:current train perplexity3.8439836502075195
INFO:root:current mean train loss 1705.6164736610071
INFO:root:current train perplexity3.8415987491607666
INFO:root:current mean train loss 1705.8701135538704
INFO:root:current train perplexity3.8442530632019043
INFO:root:current mean train loss 1709.6894757575449
INFO:root:current train perplexity3.857497453689575
INFO:root:current mean train loss 1710.238432674712
INFO:root:current train perplexity3.8593831062316895
INFO:root:current mean train loss 1710.8185970284146
INFO:root:current train perplexity3.859571933746338
INFO:root:current mean train loss 1709.3810725336893
INFO:root:current train perplexity3.855257511138916
INFO:root:current mean train loss 1710.301725386479
INFO:root:current train perplexity3.8568544387817383
INFO:root:current mean train loss 1711.2892783584637
INFO:root:current train perplexity3.857964515686035
INFO:root:current mean train loss 1712.0217517611297
INFO:root:current train perplexity3.8597333431243896

100%|██████████| 1/1 [07:31<00:00, 451.72s/it][A100%|██████████| 1/1 [07:31<00:00, 451.72s/it]
INFO:root:final mean train loss: 1711.3399494457774
INFO:root:final train perplexity: 3.8596854209899902
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.49s/it][A100%|██████████| 1/1 [00:39<00:00, 39.49s/it]
INFO:root:eval mean loss: 1808.176143132203
INFO:root:eval perplexity: 4.319772720336914
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.77s/it][A100%|██████████| 1/1 [00:37<00:00, 37.77s/it]
INFO:root:eval mean loss: 2238.7666781811004
INFO:root:eval perplexity: 6.3000288009643555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/33
 16%|█▋        | 33/200 [4:54:03<24:43:24, 532.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1685.1493530273438
INFO:root:current train perplexity3.8433420658111572
INFO:root:current mean train loss 1693.258180999756
INFO:root:current train perplexity3.8181846141815186
INFO:root:current mean train loss 1697.7255432128907
INFO:root:current train perplexity3.8321239948272705
INFO:root:current mean train loss 1699.9565056694878
INFO:root:current train perplexity3.8342652320861816
INFO:root:current mean train loss 1700.018835714589
INFO:root:current train perplexity3.8348522186279297
INFO:root:current mean train loss 1699.7004462105888
INFO:root:current train perplexity3.836458444595337
INFO:root:current mean train loss 1698.0384120131985
INFO:root:current train perplexity3.8351423740386963
INFO:root:current mean train loss 1698.38674075478
INFO:root:current train perplexity3.832641124725342
INFO:root:current mean train loss 1699.5693069812864
INFO:root:current train perplexity3.8316283226013184
INFO:root:current mean train loss 1701.8381782531737
INFO:root:current train perplexity3.8348746299743652
INFO:root:current mean train loss 1702.7307052900205
INFO:root:current train perplexity3.8335390090942383
INFO:root:current mean train loss 1703.2578552246093
INFO:root:current train perplexity3.836871862411499
INFO:root:current mean train loss 1704.2414792015438
INFO:root:current train perplexity3.838472604751587
INFO:root:current mean train loss 1705.817024679745
INFO:root:current train perplexity3.838967800140381
INFO:root:current mean train loss 1707.0604713753478
INFO:root:current train perplexity3.841992139816284
INFO:root:current mean train loss 1706.957277033879
INFO:root:current train perplexity3.842101573944092
INFO:root:current mean train loss 1707.0388573336313
INFO:root:current train perplexity3.842334747314453
INFO:root:current mean train loss 1706.339830710671
INFO:root:current train perplexity3.8416032791137695
INFO:root:current mean train loss 1706.3522888839886
INFO:root:current train perplexity3.841688871383667
INFO:root:current mean train loss 1706.3796545534717
INFO:root:current train perplexity3.843325138092041

100%|██████████| 1/1 [07:33<00:00, 453.92s/it][A100%|██████████| 1/1 [07:33<00:00, 453.92s/it]
INFO:root:final mean train loss: 1705.599198903571
INFO:root:final train perplexity: 3.842238187789917
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 1804.3233162088598
INFO:root:eval perplexity: 4.3063249588012695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.58s/it][A100%|██████████| 1/1 [00:36<00:00, 36.58s/it]
INFO:root:eval mean loss: 2236.3566786312886
INFO:root:eval perplexity: 6.287559986114502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/34
 17%|█▋        | 34/200 [5:02:55<24:34:20, 532.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1714.6029750279017
INFO:root:current train perplexity3.854492425918579
INFO:root:current mean train loss 1695.9490677138506
INFO:root:current train perplexity3.8134829998016357
INFO:root:current mean train loss 1694.6446352521434
INFO:root:current train perplexity3.8103160858154297
INFO:root:current mean train loss 1698.064945615571
INFO:root:current train perplexity3.8199455738067627
INFO:root:current mean train loss 1700.028735402745
INFO:root:current train perplexity3.8231117725372314
INFO:root:current mean train loss 1701.0309516999296
INFO:root:current train perplexity3.826810121536255
INFO:root:current mean train loss 1700.0367837339595
INFO:root:current train perplexity3.8259243965148926
INFO:root:current mean train loss 1698.908129600024
INFO:root:current train perplexity3.8239097595214844
INFO:root:current mean train loss 1698.2302067929554
INFO:root:current train perplexity3.8201982975006104
INFO:root:current mean train loss 1698.1973838220476
INFO:root:current train perplexity3.8159689903259277
INFO:root:current mean train loss 1696.7769259680392
INFO:root:current train perplexity3.8114964962005615
INFO:root:current mean train loss 1696.7936575086621
INFO:root:current train perplexity3.8112471103668213
INFO:root:current mean train loss 1697.3088742153852
INFO:root:current train perplexity3.8159236907958984
INFO:root:current mean train loss 1696.0057490064169
INFO:root:current train perplexity3.812002658843994
INFO:root:current mean train loss 1697.9937863152982
INFO:root:current train perplexity3.817875623703003
INFO:root:current mean train loss 1698.056330379082
INFO:root:current train perplexity3.8171584606170654
INFO:root:current mean train loss 1699.0777706539766
INFO:root:current train perplexity3.819927453994751
INFO:root:current mean train loss 1697.6673186242615
INFO:root:current train perplexity3.8179497718811035
INFO:root:current mean train loss 1698.337835670597
INFO:root:current train perplexity3.8201286792755127
INFO:root:current mean train loss 1699.0064620701544
INFO:root:current train perplexity3.8206841945648193

100%|██████████| 1/1 [07:36<00:00, 456.11s/it][A100%|██████████| 1/1 [07:36<00:00, 456.11s/it]
INFO:root:final mean train loss: 1698.3388063677983
INFO:root:final train perplexity: 3.820286273956299
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.51s/it][A100%|██████████| 1/1 [00:39<00:00, 39.51s/it]
INFO:root:eval mean loss: 1800.3404934930463
INFO:root:eval perplexity: 4.292468547821045
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.37s/it][A100%|██████████| 1/1 [00:36<00:00, 36.37s/it]
INFO:root:eval mean loss: 2233.848982141373
INFO:root:eval perplexity: 6.274608612060547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/35
 18%|█▊        | 35/200 [5:11:50<24:27:01, 533.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1679.335066125748
INFO:root:current train perplexity3.770902633666992
INFO:root:current mean train loss 1689.0049834890465
INFO:root:current train perplexity3.794660806655884
INFO:root:current mean train loss 1686.8731427873884
INFO:root:current train perplexity3.789438247680664
INFO:root:current mean train loss 1686.7482541466727
INFO:root:current train perplexity3.7880170345306396
INFO:root:current mean train loss 1689.455768044661
INFO:root:current train perplexity3.795142650604248
INFO:root:current mean train loss 1691.8201928957544
INFO:root:current train perplexity3.7992138862609863
INFO:root:current mean train loss 1690.5658235137676
INFO:root:current train perplexity3.7954018115997314
INFO:root:current mean train loss 1689.639286531309
INFO:root:current train perplexity3.7945897579193115
INFO:root:current mean train loss 1690.9441010818653
INFO:root:current train perplexity3.7956275939941406
INFO:root:current mean train loss 1693.0037241269886
INFO:root:current train perplexity3.801499843597412
INFO:root:current mean train loss 1694.259857903015
INFO:root:current train perplexity3.803807258605957
INFO:root:current mean train loss 1696.4610308418721
INFO:root:current train perplexity3.8081765174865723
INFO:root:current mean train loss 1697.0383540393764
INFO:root:current train perplexity3.807628631591797
INFO:root:current mean train loss 1697.5546405633518
INFO:root:current train perplexity3.810621500015259
INFO:root:current mean train loss 1697.3171598339975
INFO:root:current train perplexity3.8124325275421143
INFO:root:current mean train loss 1698.72925042089
INFO:root:current train perplexity3.8160665035247803
INFO:root:current mean train loss 1699.78106667835
INFO:root:current train perplexity3.8173933029174805
INFO:root:current mean train loss 1699.217240723745
INFO:root:current train perplexity3.8183672428131104
INFO:root:current mean train loss 1698.3866194309126
INFO:root:current train perplexity3.817563056945801

100%|██████████| 1/1 [07:29<00:00, 449.94s/it][A100%|██████████| 1/1 [07:29<00:00, 449.94s/it]
INFO:root:final mean train loss: 1697.5349006287329
INFO:root:final train perplexity: 3.8178632259368896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.40s/it][A100%|██████████| 1/1 [00:39<00:00, 39.40s/it]
INFO:root:eval mean loss: 1823.1869753573803
INFO:root:eval perplexity: 4.372565269470215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.84s/it][A100%|██████████| 1/1 [00:37<00:00, 37.84s/it]
INFO:root:eval mean loss: 2258.703883394282
INFO:root:eval perplexity: 6.404143810272217
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/36
 18%|█▊        | 36/200 [5:20:40<24:14:56, 532.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1702.3248291015625
INFO:root:current train perplexity3.7808218002319336
INFO:root:current mean train loss 1700.2191162109375
INFO:root:current train perplexity3.7638816833496094
INFO:root:current mean train loss 1688.792248477303
INFO:root:current train perplexity3.7573037147521973
INFO:root:current mean train loss 1686.237024828552
INFO:root:current train perplexity3.763010025024414
INFO:root:current mean train loss 1690.126201397601
INFO:root:current train perplexity3.7784478664398193
INFO:root:current mean train loss 1693.4847913768194
INFO:root:current train perplexity3.7931923866271973
INFO:root:current mean train loss 1690.6226123366534
INFO:root:current train perplexity3.790717840194702
INFO:root:current mean train loss 1687.8507588275206
INFO:root:current train perplexity3.7823550701141357
INFO:root:current mean train loss 1686.7645296785893
INFO:root:current train perplexity3.779869556427002
INFO:root:current mean train loss 1691.1327850308298
INFO:root:current train perplexity3.790285348892212
INFO:root:current mean train loss 1696.039535326255
INFO:root:current train perplexity3.800377130508423
INFO:root:current mean train loss 1696.728808879423
INFO:root:current train perplexity3.8061277866363525
INFO:root:current mean train loss 1699.192794585602
INFO:root:current train perplexity3.812863349914551
INFO:root:current mean train loss 1702.5402790130686
INFO:root:current train perplexity3.818467378616333
INFO:root:current mean train loss 1705.8337041583152
INFO:root:current train perplexity3.832418918609619
INFO:root:current mean train loss 1706.1170090398277
INFO:root:current train perplexity3.8385074138641357
INFO:root:current mean train loss 1707.8840361582722
INFO:root:current train perplexity3.8448543548583984
INFO:root:current mean train loss 1710.5902655382451
INFO:root:current train perplexity3.8523147106170654
INFO:root:current mean train loss 1710.1362927508974
INFO:root:current train perplexity3.8529152870178223
INFO:root:current mean train loss 1710.5135066233524
INFO:root:current train perplexity3.8553812503814697

100%|██████████| 1/1 [07:35<00:00, 455.91s/it][A100%|██████████| 1/1 [07:35<00:00, 455.91s/it]
INFO:root:final mean train loss: 1710.2387581429455
INFO:root:final train perplexity: 3.856332778930664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.38s/it][A100%|██████████| 1/1 [00:39<00:00, 39.38s/it]
INFO:root:eval mean loss: 1803.5442587128769
INFO:root:eval perplexity: 4.303611755371094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.85s/it][A100%|██████████| 1/1 [00:36<00:00, 36.85s/it]
INFO:root:eval mean loss: 2239.8255926903257
INFO:root:eval perplexity: 6.305516242980957
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/37
 18%|█▊        | 37/200 [5:29:34<24:07:54, 532.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1723.285169328962
INFO:root:current train perplexity3.878875494003296
INFO:root:current mean train loss 1710.9966592788696
INFO:root:current train perplexity3.8471503257751465
INFO:root:current mean train loss 1711.2326472767613
INFO:root:current train perplexity3.868495225906372
INFO:root:current mean train loss 1721.3557891845703
INFO:root:current train perplexity3.9008398056030273
INFO:root:current mean train loss 1716.4235061217692
INFO:root:current train perplexity3.8896493911743164
INFO:root:current mean train loss 1711.8047432176995
INFO:root:current train perplexity3.8706960678100586
INFO:root:current mean train loss 1713.747717790543
INFO:root:current train perplexity3.8712830543518066
INFO:root:current mean train loss 1714.217061807821
INFO:root:current train perplexity3.8714218139648438
INFO:root:current mean train loss 1713.6669761178573
INFO:root:current train perplexity3.8661305904388428
INFO:root:current mean train loss 1713.3813930379933
INFO:root:current train perplexity3.8631350994110107
INFO:root:current mean train loss 1712.8518219587868
INFO:root:current train perplexity3.863128662109375
INFO:root:current mean train loss 1710.5906557123712
INFO:root:current train perplexity3.861309051513672
INFO:root:current mean train loss 1710.2636436437551
INFO:root:current train perplexity3.858598470687866
INFO:root:current mean train loss 1710.420485714832
INFO:root:current train perplexity3.860793113708496
INFO:root:current mean train loss 1709.9737347087273
INFO:root:current train perplexity3.857984781265259
INFO:root:current mean train loss 1708.0309685512364
INFO:root:current train perplexity3.854973554611206
INFO:root:current mean train loss 1708.6502469599393
INFO:root:current train perplexity3.854177713394165
INFO:root:current mean train loss 1707.7344621022542
INFO:root:current train perplexity3.851177215576172
INFO:root:current mean train loss 1707.245124666644
INFO:root:current train perplexity3.8476264476776123
INFO:root:current mean train loss 1707.1845144058163
INFO:root:current train perplexity3.84661602973938

100%|██████████| 1/1 [07:30<00:00, 450.96s/it][A100%|██████████| 1/1 [07:30<00:00, 450.96s/it]
INFO:root:final mean train loss: 1706.8785166160903
INFO:root:final train perplexity: 3.8461196422576904
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.44s/it][A100%|██████████| 1/1 [00:39<00:00, 39.44s/it]
INFO:root:eval mean loss: 1801.7538361245013
INFO:root:eval perplexity: 4.2973809242248535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.04s/it][A100%|██████████| 1/1 [00:37<00:00, 37.04s/it]
INFO:root:eval mean loss: 2237.7430199156415
INFO:root:eval perplexity: 6.294729709625244
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/38
 19%|█▉        | 38/200 [5:38:24<23:56:31, 532.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1672.880992296007
INFO:root:current train perplexity3.7751238346099854
INFO:root:current mean train loss 1682.5016879377695
INFO:root:current train perplexity3.7898313999176025
INFO:root:current mean train loss 1681.4600316884566
INFO:root:current train perplexity3.795982837677002
INFO:root:current mean train loss 1687.8867835003396
INFO:root:current train perplexity3.809065341949463
INFO:root:current mean train loss 1688.045157237535
INFO:root:current train perplexity3.8066256046295166
INFO:root:current mean train loss 1689.577444542001
INFO:root:current train perplexity3.8076391220092773
INFO:root:current mean train loss 1688.9478462633238
INFO:root:current train perplexity3.8084588050842285
INFO:root:current mean train loss 1690.4608239500315
INFO:root:current train perplexity3.8104965686798096
INFO:root:current mean train loss 1691.11433654424
INFO:root:current train perplexity3.808568239212036
INFO:root:current mean train loss 1689.1833543888476
INFO:root:current train perplexity3.8065097332000732
INFO:root:current mean train loss 1689.1173068835974
INFO:root:current train perplexity3.8065896034240723
INFO:root:current mean train loss 1690.3008572640897
INFO:root:current train perplexity3.806854724884033
INFO:root:current mean train loss 1691.6750884396963
INFO:root:current train perplexity3.80594539642334
INFO:root:current mean train loss 1692.6621489457482
INFO:root:current train perplexity3.8065106868743896
INFO:root:current mean train loss 1692.5689414265246
INFO:root:current train perplexity3.8063881397247314
INFO:root:current mean train loss 1692.3751348699182
INFO:root:current train perplexity3.8053781986236572
INFO:root:current mean train loss 1692.9137577323565
INFO:root:current train perplexity3.8050036430358887
INFO:root:current mean train loss 1693.3283327643715
INFO:root:current train perplexity3.8052186965942383
INFO:root:current mean train loss 1693.2896468495935
INFO:root:current train perplexity3.80507755279541
INFO:root:current mean train loss 1694.3415749517994
INFO:root:current train perplexity3.807560920715332

100%|██████████| 1/1 [07:32<00:00, 452.31s/it][A100%|██████████| 1/1 [07:32<00:00, 452.31s/it]
INFO:root:final mean train loss: 1694.0598995268374
INFO:root:final train perplexity: 3.8074071407318115
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.15s/it][A100%|██████████| 1/1 [00:40<00:00, 40.15s/it]
INFO:root:eval mean loss: 1802.6022983848625
INFO:root:eval perplexity: 4.300332546234131
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.30s/it][A100%|██████████| 1/1 [00:38<00:00, 38.30s/it]
INFO:root:eval mean loss: 2241.1853109589706
INFO:root:eval perplexity: 6.312568664550781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/39
 20%|█▉        | 39/200 [5:47:18<23:48:52, 532.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1689.6729047221522
INFO:root:current train perplexity3.8065197467803955
INFO:root:current mean train loss 1677.4118524245273
INFO:root:current train perplexity3.7890472412109375
INFO:root:current mean train loss 1669.4688725362298
INFO:root:current train perplexity3.772394895553589
INFO:root:current mean train loss 1669.3915152365332
INFO:root:current train perplexity3.772223472595215
INFO:root:current mean train loss 1672.3172311493845
INFO:root:current train perplexity3.7706499099731445
INFO:root:current mean train loss 1674.9883713908891
INFO:root:current train perplexity3.773651599884033
INFO:root:current mean train loss 1678.0305258759558
INFO:root:current train perplexity3.7764339447021484
INFO:root:current mean train loss 1678.1986450836102
INFO:root:current train perplexity3.7780563831329346
INFO:root:current mean train loss 1678.871618425763
INFO:root:current train perplexity3.777423143386841
INFO:root:current mean train loss 1681.1852526119494
INFO:root:current train perplexity3.7803165912628174
INFO:root:current mean train loss 1687.0110336849693
INFO:root:current train perplexity3.7913520336151123
INFO:root:current mean train loss 1689.8795965460615
INFO:root:current train perplexity3.8029494285583496
INFO:root:current mean train loss 1697.2564772713204
INFO:root:current train perplexity3.8222036361694336
INFO:root:current mean train loss 1698.4915114527407
INFO:root:current train perplexity3.827134609222412
INFO:root:current mean train loss 1703.8584380978273
INFO:root:current train perplexity3.839341163635254
INFO:root:current mean train loss 1708.4541718193573
INFO:root:current train perplexity3.852546215057373
INFO:root:current mean train loss 1716.722443177812
INFO:root:current train perplexity3.8750481605529785
INFO:root:current mean train loss 1717.837113795026
INFO:root:current train perplexity3.8790695667266846
INFO:root:current mean train loss 1719.0910909388426
INFO:root:current train perplexity3.8837499618530273
INFO:root:current mean train loss 1720.9534970593622
INFO:root:current train perplexity3.8883495330810547

100%|██████████| 1/1 [07:30<00:00, 450.57s/it][A100%|██████████| 1/1 [07:30<00:00, 450.57s/it]
INFO:root:final mean train loss: 1721.6590941213203
INFO:root:final train perplexity: 3.8912463188171387
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.70s/it][A100%|██████████| 1/1 [00:39<00:00, 39.70s/it]
INFO:root:eval mean loss: 1819.5637185387577
INFO:root:eval perplexity: 4.3597636222839355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.81s/it][A100%|██████████| 1/1 [00:36<00:00, 36.81s/it]
INFO:root:eval mean loss: 2262.5857669478614
INFO:root:eval perplexity: 6.424614429473877
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/40
 20%|██        | 40/200 [5:56:07<23:37:51, 531.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1758.7961533944817
INFO:root:current train perplexity3.9961624145507812
INFO:root:current mean train loss 1755.4605965214735
INFO:root:current train perplexity3.9880850315093994
INFO:root:current mean train loss 1737.1968250343023
INFO:root:current train perplexity3.932800054550171
INFO:root:current mean train loss 1736.3112470883493
INFO:root:current train perplexity3.925349473953247
INFO:root:current mean train loss 1729.8194842567523
INFO:root:current train perplexity3.9069764614105225
INFO:root:current mean train loss 1725.8862030609077
INFO:root:current train perplexity3.898421049118042
INFO:root:current mean train loss 1720.9890706619915
INFO:root:current train perplexity3.892251491546631
INFO:root:current mean train loss 1718.943045658998
INFO:root:current train perplexity3.8875977993011475
INFO:root:current mean train loss 1718.190879639505
INFO:root:current train perplexity3.8849687576293945
INFO:root:current mean train loss 1717.2370967066204
INFO:root:current train perplexity3.878220319747925
INFO:root:current mean train loss 1716.5917533188644
INFO:root:current train perplexity3.8770997524261475
INFO:root:current mean train loss 1713.8779424225709
INFO:root:current train perplexity3.869993209838867
INFO:root:current mean train loss 1714.88143756261
INFO:root:current train perplexity3.8708624839782715
INFO:root:current mean train loss 1713.7016729917796
INFO:root:current train perplexity3.868239164352417
INFO:root:current mean train loss 1712.376224417232
INFO:root:current train perplexity3.8629848957061768
INFO:root:current mean train loss 1712.2704098624772
INFO:root:current train perplexity3.8624448776245117
INFO:root:current mean train loss 1712.5145049921596
INFO:root:current train perplexity3.8627781867980957
INFO:root:current mean train loss 1713.6399802025146
INFO:root:current train perplexity3.8642280101776123
INFO:root:current mean train loss 1713.7179832113366
INFO:root:current train perplexity3.8649771213531494
INFO:root:current mean train loss 1714.0424022549266
INFO:root:current train perplexity3.8662545680999756

100%|██████████| 1/1 [07:34<00:00, 454.14s/it][A100%|██████████| 1/1 [07:34<00:00, 454.14s/it]
INFO:root:final mean train loss: 1713.4336803318934
INFO:root:final train perplexity: 3.8660683631896973
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.15s/it][A100%|██████████| 1/1 [00:39<00:00, 39.15s/it]
INFO:root:eval mean loss: 1805.4420646505152
INFO:root:eval perplexity: 4.310225963592529
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.29s/it][A100%|██████████| 1/1 [00:37<00:00, 37.29s/it]
INFO:root:eval mean loss: 2245.674463150349
INFO:root:eval perplexity: 6.335910320281982
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/41
 20%|██        | 41/200 [6:05:00<23:30:02, 532.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1700.3001874287922
INFO:root:current train perplexity3.8288867473602295
INFO:root:current mean train loss 1705.548729098573
INFO:root:current train perplexity3.858675718307495
INFO:root:current mean train loss 1702.6610581681534
INFO:root:current train perplexity3.847750425338745
INFO:root:current mean train loss 1701.4969143337673
INFO:root:current train perplexity3.846308946609497
INFO:root:current mean train loss 1700.9344366750408
INFO:root:current train perplexity3.8424644470214844
INFO:root:current mean train loss 1701.0728614346292
INFO:root:current train perplexity3.8342533111572266
INFO:root:current mean train loss 1700.3321122794316
INFO:root:current train perplexity3.839046001434326
INFO:root:current mean train loss 1701.7671182143629
INFO:root:current train perplexity3.839901924133301
INFO:root:current mean train loss 1704.8841408320836
INFO:root:current train perplexity3.8480916023254395
INFO:root:current mean train loss 1706.545463592652
INFO:root:current train perplexity3.8483829498291016
INFO:root:current mean train loss 1706.0087652276031
INFO:root:current train perplexity3.847487449645996
INFO:root:current mean train loss 1706.0862891155741
INFO:root:current train perplexity3.8488874435424805
INFO:root:current mean train loss 1705.1625207971645
INFO:root:current train perplexity3.846254825592041
INFO:root:current mean train loss 1706.6235723194898
INFO:root:current train perplexity3.848651647567749
INFO:root:current mean train loss 1706.9113646318567
INFO:root:current train perplexity3.849234104156494
INFO:root:current mean train loss 1707.737095271137
INFO:root:current train perplexity3.8508412837982178
INFO:root:current mean train loss 1707.8482258634747
INFO:root:current train perplexity3.850351095199585
INFO:root:current mean train loss 1709.0907411978876
INFO:root:current train perplexity3.852590322494507
INFO:root:current mean train loss 1708.2844041912867
INFO:root:current train perplexity3.8513503074645996

100%|██████████| 1/1 [07:32<00:00, 452.66s/it][A100%|██████████| 1/1 [07:32<00:00, 452.66s/it]
INFO:root:final mean train loss: 1709.5701271168703
INFO:root:final train perplexity: 3.8542985916137695
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.29s/it][A100%|██████████| 1/1 [00:39<00:00, 39.29s/it]
INFO:root:eval mean loss: 1818.0786907655972
INFO:root:eval perplexity: 4.354527473449707
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.09s/it][A100%|██████████| 1/1 [00:37<00:00, 37.09s/it]
INFO:root:eval mean loss: 2253.635453893783
INFO:root:eval perplexity: 6.377514362335205
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/42
 21%|██        | 42/200 [6:13:52<23:20:49, 531.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1638.0865854116587
INFO:root:current train perplexity3.733247995376587
INFO:root:current mean train loss 1696.1408151272124
INFO:root:current train perplexity3.8108999729156494
INFO:root:current mean train loss 1715.8288620066755
INFO:root:current train perplexity3.8594210147857666
INFO:root:current mean train loss 1707.8594771802616
INFO:root:current train perplexity3.840675115585327
INFO:root:current mean train loss 1709.2724189665935
INFO:root:current train perplexity3.838980197906494
INFO:root:current mean train loss 1707.405362194277
INFO:root:current train perplexity3.8433237075805664
INFO:root:current mean train loss 1704.5421988905741
INFO:root:current train perplexity3.8394675254821777
INFO:root:current mean train loss 1708.144886674921
INFO:root:current train perplexity3.85137939453125
INFO:root:current mean train loss 1708.2854272671145
INFO:root:current train perplexity3.855424404144287
INFO:root:current mean train loss 1710.1020142804884
INFO:root:current train perplexity3.8555338382720947
INFO:root:current mean train loss 1710.597697582791
INFO:root:current train perplexity3.85711669921875
INFO:root:current mean train loss 1708.5115994216083
INFO:root:current train perplexity3.853795051574707
INFO:root:current mean train loss 1710.6868472810825
INFO:root:current train perplexity3.858726739883423
INFO:root:current mean train loss 1711.501846952649
INFO:root:current train perplexity3.8589401245117188
INFO:root:current mean train loss 1712.426533628168
INFO:root:current train perplexity3.8595736026763916
INFO:root:current mean train loss 1711.4210925559216
INFO:root:current train perplexity3.8582956790924072
INFO:root:current mean train loss 1710.762551295262
INFO:root:current train perplexity3.8569376468658447
INFO:root:current mean train loss 1710.1834838653403
INFO:root:current train perplexity3.8556432723999023
INFO:root:current mean train loss 1710.968010777738
INFO:root:current train perplexity3.8559415340423584
INFO:root:current mean train loss 1710.7300761213367
INFO:root:current train perplexity3.8552916049957275

100%|██████████| 1/1 [07:21<00:00, 441.71s/it][A100%|██████████| 1/1 [07:21<00:00, 441.71s/it]
INFO:root:final mean train loss: 1709.488104208034
INFO:root:final train perplexity: 3.8540496826171875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.33s/it][A100%|██████████| 1/1 [00:39<00:00, 39.33s/it]
INFO:root:eval mean loss: 1809.6250623337767
INFO:root:eval perplexity: 4.324840068817139
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.09s/it][A100%|██████████| 1/1 [00:38<00:00, 38.09s/it]
INFO:root:eval mean loss: 2250.9543500318596
INFO:root:eval perplexity: 6.363471984863281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/43
 22%|██▏       | 43/200 [6:22:34<23:03:49, 528.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1769.80078125
INFO:root:current train perplexity3.9089651107788086
INFO:root:current mean train loss 1713.9518235426683
INFO:root:current train perplexity3.837026834487915
INFO:root:current mean train loss 1705.3572711447011
INFO:root:current train perplexity3.825185537338257
INFO:root:current mean train loss 1705.6419022993607
INFO:root:current train perplexity3.838576555252075
INFO:root:current mean train loss 1706.8948815634085
INFO:root:current train perplexity3.8412516117095947
INFO:root:current mean train loss 1704.757088830336
INFO:root:current train perplexity3.842010974884033
INFO:root:current mean train loss 1705.424183485243
INFO:root:current train perplexity3.8413124084472656
INFO:root:current mean train loss 1706.1753672142552
INFO:root:current train perplexity3.841284990310669
INFO:root:current mean train loss 1704.076606033509
INFO:root:current train perplexity3.833653688430786
INFO:root:current mean train loss 1706.4068561512936
INFO:root:current train perplexity3.8372802734375
INFO:root:current mean train loss 1707.3627875170662
INFO:root:current train perplexity3.8434040546417236
INFO:root:current mean train loss 1707.2905238868916
INFO:root:current train perplexity3.845679521560669
INFO:root:current mean train loss 1706.7104971536776
INFO:root:current train perplexity3.844180107116699
INFO:root:current mean train loss 1706.4204251167469
INFO:root:current train perplexity3.841953754425049
INFO:root:current mean train loss 1711.4272389231862
INFO:root:current train perplexity3.858497142791748
INFO:root:current mean train loss 1712.467990451389
INFO:root:current train perplexity3.8592538833618164
INFO:root:current mean train loss 1713.1354178399397
INFO:root:current train perplexity3.8622303009033203
INFO:root:current mean train loss 1713.5302827515354
INFO:root:current train perplexity3.864109754562378
INFO:root:current mean train loss 1715.5121220489668
INFO:root:current train perplexity3.869046211242676
INFO:root:current mean train loss 1717.8161252985346
INFO:root:current train perplexity3.8773252964019775

100%|██████████| 1/1 [07:37<00:00, 457.73s/it][A100%|██████████| 1/1 [07:37<00:00, 457.73s/it]
INFO:root:final mean train loss: 1717.985132045236
INFO:root:final train perplexity: 3.8799803256988525
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.73s/it][A100%|██████████| 1/1 [00:39<00:00, 39.73s/it]
INFO:root:eval mean loss: 1849.8930218202847
INFO:root:eval perplexity: 4.468088150024414
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.94s/it][A100%|██████████| 1/1 [00:36<00:00, 36.94s/it]
INFO:root:eval mean loss: 2293.888288781998
INFO:root:eval perplexity: 6.592095375061035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/44
 22%|██▏       | 44/200 [6:31:30<23:01:08, 531.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1711.878771193484
INFO:root:current train perplexity3.8994667530059814
INFO:root:current mean train loss 1723.52829207855
INFO:root:current train perplexity3.906212568283081
INFO:root:current mean train loss 1716.4022068533338
INFO:root:current train perplexity3.89266037940979
INFO:root:current mean train loss 1715.47258905856
INFO:root:current train perplexity3.885164499282837
INFO:root:current mean train loss 1716.9443233754544
INFO:root:current train perplexity3.8903377056121826
INFO:root:current mean train loss 1722.61291146845
INFO:root:current train perplexity3.898513078689575
INFO:root:current mean train loss 1721.8554647879032
INFO:root:current train perplexity3.8984615802764893
INFO:root:current mean train loss 1724.472519635835
INFO:root:current train perplexity3.904038667678833
INFO:root:current mean train loss 1724.351940240601
INFO:root:current train perplexity3.901705265045166
INFO:root:current mean train loss 1724.9697065826706
INFO:root:current train perplexity3.8969480991363525
INFO:root:current mean train loss 1725.738813952252
INFO:root:current train perplexity3.8985090255737305
INFO:root:current mean train loss 1726.274904685116
INFO:root:current train perplexity3.897099256515503
INFO:root:current mean train loss 1726.6845736408004
INFO:root:current train perplexity3.897966146469116
INFO:root:current mean train loss 1729.219123188973
INFO:root:current train perplexity3.9082088470458984
INFO:root:current mean train loss 1730.199666791071
INFO:root:current train perplexity3.913490056991577
INFO:root:current mean train loss 1730.7813034994647
INFO:root:current train perplexity3.915627956390381
INFO:root:current mean train loss 1731.743134638144
INFO:root:current train perplexity3.9182851314544678
INFO:root:current mean train loss 1731.3866124712677
INFO:root:current train perplexity3.916884183883667
INFO:root:current mean train loss 1730.6893438102752
INFO:root:current train perplexity3.9162487983703613
INFO:root:current mean train loss 1731.0003570572314
INFO:root:current train perplexity3.918375253677368

100%|██████████| 1/1 [07:32<00:00, 452.33s/it][A100%|██████████| 1/1 [07:32<00:00, 452.33s/it]
INFO:root:final mean train loss: 1731.2512217803971
INFO:root:final train perplexity: 3.9208152294158936
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.23s/it][A100%|██████████| 1/1 [00:39<00:00, 39.23s/it]
INFO:root:eval mean loss: 1815.6227378033577
INFO:root:eval perplexity: 4.345881462097168
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.71s/it][A100%|██████████| 1/1 [00:36<00:00, 36.71s/it]
INFO:root:eval mean loss: 2258.269236030308
INFO:root:eval perplexity: 6.40185546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/45
 22%|██▎       | 45/200 [6:40:21<22:51:51, 531.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1721.4547672271729
INFO:root:current train perplexity3.9012796878814697
INFO:root:current mean train loss 1744.130516238329
INFO:root:current train perplexity3.948768138885498
INFO:root:current mean train loss 1733.1393876509233
INFO:root:current train perplexity3.9099385738372803
INFO:root:current mean train loss 1729.8730911422563
INFO:root:current train perplexity3.904655694961548
INFO:root:current mean train loss 1727.8772025272765
INFO:root:current train perplexity3.9040870666503906
INFO:root:current mean train loss 1727.0742079281638
INFO:root:current train perplexity3.9041428565979004
INFO:root:current mean train loss 1724.577562263213
INFO:root:current train perplexity3.9021239280700684
INFO:root:current mean train loss 1721.6554157996052
INFO:root:current train perplexity3.8958933353424072
INFO:root:current mean train loss 1722.2197060761628
INFO:root:current train perplexity3.895249605178833
INFO:root:current mean train loss 1721.1431217431034
INFO:root:current train perplexity3.891213893890381
INFO:root:current mean train loss 1719.5782199945666
INFO:root:current train perplexity3.887540340423584
INFO:root:current mean train loss 1719.9676857650074
INFO:root:current train perplexity3.8866686820983887
INFO:root:current mean train loss 1719.3169978660874
INFO:root:current train perplexity3.8857245445251465
INFO:root:current mean train loss 1719.2839199748557
INFO:root:current train perplexity3.8864166736602783
INFO:root:current mean train loss 1719.9164398693647
INFO:root:current train perplexity3.887498140335083
INFO:root:current mean train loss 1717.9325038587956
INFO:root:current train perplexity3.882333517074585
INFO:root:current mean train loss 1717.3024173149695
INFO:root:current train perplexity3.8789212703704834
INFO:root:current mean train loss 1716.3684578893406
INFO:root:current train perplexity3.8772332668304443
INFO:root:current mean train loss 1715.6300239399268
INFO:root:current train perplexity3.8730268478393555
INFO:root:current mean train loss 1716.0502145926482
INFO:root:current train perplexity3.8727664947509766

100%|██████████| 1/1 [07:32<00:00, 452.49s/it][A100%|██████████| 1/1 [07:32<00:00, 452.49s/it]
INFO:root:final mean train loss: 1715.301245061785
INFO:root:final train perplexity: 3.8717713356018066
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.87s/it][A100%|██████████| 1/1 [00:38<00:00, 38.88s/it]
INFO:root:eval mean loss: 1796.9708723785184
INFO:root:eval perplexity: 4.28078031539917
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.97s/it][A100%|██████████| 1/1 [00:35<00:00, 35.97s/it]
INFO:root:eval mean loss: 2235.983237841451
INFO:root:eval perplexity: 6.285628795623779
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/46
 23%|██▎       | 46/200 [6:49:11<22:42:06, 530.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1680.7781108338156
INFO:root:current train perplexity3.770966053009033
INFO:root:current mean train loss 1671.4366765312068
INFO:root:current train perplexity3.781003475189209
INFO:root:current mean train loss 1692.5296626515235
INFO:root:current train perplexity3.817026376724243
INFO:root:current mean train loss 1700.7422438894357
INFO:root:current train perplexity3.8332712650299072
INFO:root:current mean train loss 1704.5341012681101
INFO:root:current train perplexity3.8496525287628174
INFO:root:current mean train loss 1705.6306200667625
INFO:root:current train perplexity3.8528060913085938
INFO:root:current mean train loss 1711.9051967178323
INFO:root:current train perplexity3.8660850524902344
INFO:root:current mean train loss 1717.9313641302717
INFO:root:current train perplexity3.8840763568878174
INFO:root:current mean train loss 1725.5181826570924
INFO:root:current train perplexity3.900700807571411
INFO:root:current mean train loss 1732.4979318974579
INFO:root:current train perplexity3.915898561477661
INFO:root:current mean train loss 1736.4441907178682
INFO:root:current train perplexity3.9264163970947266
INFO:root:current mean train loss 1736.883322280509
INFO:root:current train perplexity3.9303548336029053
INFO:root:current mean train loss 1736.1251448453356
INFO:root:current train perplexity3.9313182830810547
INFO:root:current mean train loss 1737.7634463852337
INFO:root:current train perplexity3.9372849464416504
INFO:root:current mean train loss 1743.752985323868
INFO:root:current train perplexity3.9562277793884277
INFO:root:current mean train loss 1746.1821944582393
INFO:root:current train perplexity3.965437889099121
INFO:root:current mean train loss 1746.4892692860926
INFO:root:current train perplexity3.9668240547180176
INFO:root:current mean train loss 1748.0611685357155
INFO:root:current train perplexity3.9715652465820312
INFO:root:current mean train loss 1747.6183397373197
INFO:root:current train perplexity3.970010757446289
INFO:root:current mean train loss 1747.8789297890507
INFO:root:current train perplexity3.970905303955078

100%|██████████| 1/1 [07:34<00:00, 454.90s/it][A100%|██████████| 1/1 [07:34<00:00, 454.90s/it]
INFO:root:final mean train loss: 1747.3004893708337
INFO:root:final train perplexity: 3.970792770385742
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.48s/it][A100%|██████████| 1/1 [00:39<00:00, 39.48s/it]
INFO:root:eval mean loss: 1823.5495371717088
INFO:root:eval perplexity: 4.373847484588623
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.10s/it][A100%|██████████| 1/1 [00:38<00:00, 38.10s/it]
INFO:root:eval mean loss: 2260.8196341873063
INFO:root:eval perplexity: 6.415292739868164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/47
 24%|██▎       | 47/200 [6:58:06<22:36:41, 532.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1738.4974975585938
INFO:root:current train perplexity3.9264309406280518
INFO:root:current mean train loss 1726.265472103851
INFO:root:current train perplexity3.9088504314422607
INFO:root:current mean train loss 1722.4554992266148
INFO:root:current train perplexity3.889166831970215
INFO:root:current mean train loss 1723.2138217945194
INFO:root:current train perplexity3.880577564239502
INFO:root:current mean train loss 1727.7252405618567
INFO:root:current train perplexity3.889584541320801
INFO:root:current mean train loss 1727.1407264530858
INFO:root:current train perplexity3.8885507583618164
INFO:root:current mean train loss 1725.361231937791
INFO:root:current train perplexity3.8908543586730957
INFO:root:current mean train loss 1726.3014858312774
INFO:root:current train perplexity3.8920443058013916
INFO:root:current mean train loss 1725.370958357983
INFO:root:current train perplexity3.8924362659454346
INFO:root:current mean train loss 1724.1085074201137
INFO:root:current train perplexity3.8918707370758057
INFO:root:current mean train loss 1721.672628322803
INFO:root:current train perplexity3.8853979110717773
INFO:root:current mean train loss 1721.1840852918926
INFO:root:current train perplexity3.888035297393799
INFO:root:current mean train loss 1719.5513136383197
INFO:root:current train perplexity3.8862571716308594
INFO:root:current mean train loss 1721.6228257862795
INFO:root:current train perplexity3.891362190246582
INFO:root:current mean train loss 1720.3061168146069
INFO:root:current train perplexity3.8891775608062744
INFO:root:current mean train loss 1719.5922093779334
INFO:root:current train perplexity3.8856568336486816
INFO:root:current mean train loss 1719.2133773965465
INFO:root:current train perplexity3.8847620487213135
INFO:root:current mean train loss 1719.523934403569
INFO:root:current train perplexity3.8834481239318848
INFO:root:current mean train loss 1718.853115326989
INFO:root:current train perplexity3.8818438053131104

100%|██████████| 1/1 [07:32<00:00, 452.22s/it][A100%|██████████| 1/1 [07:32<00:00, 452.22s/it]
INFO:root:final mean train loss: 1718.0830386532598
INFO:root:final train perplexity: 3.8802809715270996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.63s/it][A100%|██████████| 1/1 [00:39<00:00, 39.63s/it]
INFO:root:eval mean loss: 1809.923836782469
INFO:root:eval perplexity: 4.325885772705078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.73s/it][A100%|██████████| 1/1 [00:36<00:00, 36.73s/it]
INFO:root:eval mean loss: 2250.992090969221
INFO:root:eval perplexity: 6.363669395446777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/48
 24%|██▍       | 48/200 [7:06:57<22:27:00, 531.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1726.5795084635417
INFO:root:current train perplexity3.8626506328582764
INFO:root:current mean train loss 1697.2374225118886
INFO:root:current train perplexity3.836087465286255
INFO:root:current mean train loss 1693.6116330168968
INFO:root:current train perplexity3.8261067867279053
INFO:root:current mean train loss 1698.3767775762649
INFO:root:current train perplexity3.826396942138672
INFO:root:current mean train loss 1697.4706454725151
INFO:root:current train perplexity3.824183940887451
INFO:root:current mean train loss 1694.803033506523
INFO:root:current train perplexity3.8144845962524414
INFO:root:current mean train loss 1696.5586112169715
INFO:root:current train perplexity3.814439535140991
INFO:root:current mean train loss 1694.4316904774912
INFO:root:current train perplexity3.8153204917907715
INFO:root:current mean train loss 1695.8046066190568
INFO:root:current train perplexity3.814354181289673
INFO:root:current mean train loss 1695.999966247225
INFO:root:current train perplexity3.8132948875427246
INFO:root:current mean train loss 1694.8539125038485
INFO:root:current train perplexity3.8119068145751953
INFO:root:current mean train loss 1693.8732902492643
INFO:root:current train perplexity3.811854124069214
INFO:root:current mean train loss 1693.7843987107767
INFO:root:current train perplexity3.808152914047241
INFO:root:current mean train loss 1693.5554798894962
INFO:root:current train perplexity3.8076438903808594
INFO:root:current mean train loss 1694.556050891674
INFO:root:current train perplexity3.809173107147217
INFO:root:current mean train loss 1694.5369876269854
INFO:root:current train perplexity3.808800220489502
INFO:root:current mean train loss 1694.8454802994388
INFO:root:current train perplexity3.8081846237182617
INFO:root:current mean train loss 1694.9201245900147
INFO:root:current train perplexity3.80795955657959
INFO:root:current mean train loss 1694.4974262332128
INFO:root:current train perplexity3.806558847427368
INFO:root:current mean train loss 1694.2617232121002
INFO:root:current train perplexity3.806135892868042

100%|██████████| 1/1 [07:33<00:00, 453.04s/it][A100%|██████████| 1/1 [07:33<00:00, 453.04s/it]
INFO:root:final mean train loss: 1693.148646490777
INFO:root:final train perplexity: 3.8046698570251465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.87s/it][A100%|██████████| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 1801.4937570991247
INFO:root:eval perplexity: 4.296475887298584
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.69s/it][A100%|██████████| 1/1 [00:37<00:00, 37.69s/it]
INFO:root:eval mean loss: 2240.406047415226
INFO:root:eval perplexity: 6.308525562286377
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/49
 24%|██▍       | 49/200 [7:15:49<22:18:25, 531.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1716.582176208496
INFO:root:current train perplexity3.8315794467926025
INFO:root:current mean train loss 1682.4675866329308
INFO:root:current train perplexity3.7696049213409424
INFO:root:current mean train loss 1686.547078099744
INFO:root:current train perplexity3.760192632675171
INFO:root:current mean train loss 1688.5078161768167
INFO:root:current train perplexity3.7633845806121826
INFO:root:current mean train loss 1684.2203925803856
INFO:root:current train perplexity3.7645511627197266
INFO:root:current mean train loss 1683.704436108582
INFO:root:current train perplexity3.766662359237671
INFO:root:current mean train loss 1682.1502127345605
INFO:root:current train perplexity3.7633652687072754
INFO:root:current mean train loss 1679.4923297486018
INFO:root:current train perplexity3.7621490955352783
INFO:root:current mean train loss 1677.7785735497107
INFO:root:current train perplexity3.7554686069488525
INFO:root:current mean train loss 1678.3932766239018
INFO:root:current train perplexity3.758004903793335
INFO:root:current mean train loss 1680.2286864288094
INFO:root:current train perplexity3.7618017196655273
INFO:root:current mean train loss 1682.9168402466253
INFO:root:current train perplexity3.770617961883545
INFO:root:current mean train loss 1685.9499309589337
INFO:root:current train perplexity3.779183864593506
INFO:root:current mean train loss 1686.4652070283173
INFO:root:current train perplexity3.7846059799194336
INFO:root:current mean train loss 1687.1665931573793
INFO:root:current train perplexity3.785722017288208
INFO:root:current mean train loss 1685.759282602654
INFO:root:current train perplexity3.7822656631469727
INFO:root:current mean train loss 1687.7111482059254
INFO:root:current train perplexity3.786996603012085
INFO:root:current mean train loss 1688.10703381545
INFO:root:current train perplexity3.78715181350708
INFO:root:current mean train loss 1688.3555260537494
INFO:root:current train perplexity3.787630081176758
INFO:root:current mean train loss 1688.7972442437404
INFO:root:current train perplexity3.788684844970703

100%|██████████| 1/1 [07:32<00:00, 452.55s/it][A100%|██████████| 1/1 [07:32<00:00, 452.55s/it]
INFO:root:final mean train loss: 1687.8056649243176
INFO:root:final train perplexity: 3.7886605262756348
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.71s/it][A100%|██████████| 1/1 [00:38<00:00, 38.71s/it]
INFO:root:eval mean loss: 1802.2596011330895
INFO:root:eval perplexity: 4.299139976501465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.96s/it][A100%|██████████| 1/1 [00:36<00:00, 36.96s/it]
INFO:root:eval mean loss: 2244.471335985982
INFO:root:eval perplexity: 6.32964563369751
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/50
 25%|██▌       | 50/200 [7:24:40<22:08:40, 531.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.9043566645407
INFO:root:current train perplexity3.7967870235443115
INFO:root:current mean train loss 1696.4671409658138
INFO:root:current train perplexity3.825786590576172
INFO:root:current mean train loss 1705.9188453030874
INFO:root:current train perplexity3.839592456817627
INFO:root:current mean train loss 1702.5775650155579
INFO:root:current train perplexity3.8229668140411377
INFO:root:current mean train loss 1698.3361976810447
INFO:root:current train perplexity3.8082668781280518
INFO:root:current mean train loss 1697.226405520691
INFO:root:current train perplexity3.806854248046875
INFO:root:current mean train loss 1696.9506676061128
INFO:root:current train perplexity3.8101789951324463
INFO:root:current mean train loss 1694.4288312150575
INFO:root:current train perplexity3.8077166080474854
INFO:root:current mean train loss 1695.7560531921747
INFO:root:current train perplexity3.8090622425079346
INFO:root:current mean train loss 1695.6318548461786
INFO:root:current train perplexity3.807291269302368
INFO:root:current mean train loss 1696.5778993619294
INFO:root:current train perplexity3.809739828109741
INFO:root:current mean train loss 1695.1701040774246
INFO:root:current train perplexity3.809861421585083
INFO:root:current mean train loss 1695.8113698966986
INFO:root:current train perplexity3.8090014457702637
INFO:root:current mean train loss 1697.8809588229417
INFO:root:current train perplexity3.816171169281006
INFO:root:current mean train loss 1699.5060148903879
INFO:root:current train perplexity3.81829571723938
INFO:root:current mean train loss 1699.3014143764626
INFO:root:current train perplexity3.8183963298797607
INFO:root:current mean train loss 1699.1843898349853
INFO:root:current train perplexity3.8206746578216553
INFO:root:current mean train loss 1698.0734112294078
INFO:root:current train perplexity3.817600727081299
INFO:root:current mean train loss 1698.0181606162748
INFO:root:current train perplexity3.8174054622650146
INFO:root:current mean train loss 1698.897628854641
INFO:root:current train perplexity3.819782257080078

100%|██████████| 1/1 [07:30<00:00, 450.46s/it][A100%|██████████| 1/1 [07:30<00:00, 450.46s/it]
INFO:root:final mean train loss: 1698.333122188012
INFO:root:final train perplexity: 3.820268392562866
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.42s/it][A100%|██████████| 1/1 [00:39<00:00, 39.42s/it]
INFO:root:eval mean loss: 1804.6987672629932
INFO:root:eval perplexity: 4.307633876800537
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.85s/it][A100%|██████████| 1/1 [00:36<00:00, 36.85s/it]
INFO:root:eval mean loss: 2248.5169686391846
INFO:root:eval perplexity: 6.350733757019043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/51
 26%|██▌       | 51/200 [7:33:29<21:58:13, 530.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1733.2302616003788
INFO:root:current train perplexity3.8550734519958496
INFO:root:current mean train loss 1715.5388911603445
INFO:root:current train perplexity3.8439254760742188
INFO:root:current mean train loss 1717.2799361379523
INFO:root:current train perplexity3.8604719638824463
INFO:root:current mean train loss 1719.3660211615224
INFO:root:current train perplexity3.874220132827759
INFO:root:current mean train loss 1714.0798633231625
INFO:root:current train perplexity3.866729259490967
INFO:root:current mean train loss 1715.7506325657714
INFO:root:current train perplexity3.8693525791168213
INFO:root:current mean train loss 1721.3443568690761
INFO:root:current train perplexity3.8864519596099854
INFO:root:current mean train loss 1725.3184098923487
INFO:root:current train perplexity3.8966472148895264
INFO:root:current mean train loss 1724.2205062055698
INFO:root:current train perplexity3.894099235534668
INFO:root:current mean train loss 1724.8245139428054
INFO:root:current train perplexity3.896728515625
INFO:root:current mean train loss 1725.879823266006
INFO:root:current train perplexity3.899648666381836
INFO:root:current mean train loss 1726.4170787673872
INFO:root:current train perplexity3.9023189544677734
INFO:root:current mean train loss 1728.6299141496656
INFO:root:current train perplexity3.909008502960205
INFO:root:current mean train loss 1728.2988073033434
INFO:root:current train perplexity3.9070498943328857
INFO:root:current mean train loss 1726.6313992821665
INFO:root:current train perplexity3.9040768146514893
INFO:root:current mean train loss 1725.3984567537466
INFO:root:current train perplexity3.9011449813842773
INFO:root:current mean train loss 1723.9413268969697
INFO:root:current train perplexity3.8980743885040283
INFO:root:current mean train loss 1723.4911832193827
INFO:root:current train perplexity3.8960344791412354
INFO:root:current mean train loss 1723.142400187567
INFO:root:current train perplexity3.8939552307128906
INFO:root:current mean train loss 1721.3131427008161
INFO:root:current train perplexity3.889296054840088

100%|██████████| 1/1 [07:30<00:00, 450.39s/it][A100%|██████████| 1/1 [07:30<00:00, 450.40s/it]
INFO:root:final mean train loss: 1721.020182476342
INFO:root:final train perplexity: 3.889285087585449
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.86s/it][A100%|██████████| 1/1 [00:39<00:00, 39.86s/it]
INFO:root:eval mean loss: 1801.7624377527982
INFO:root:eval perplexity: 4.29741096496582
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.07s/it][A100%|██████████| 1/1 [00:37<00:00, 37.07s/it]
INFO:root:eval mean loss: 2246.301763006981
INFO:root:eval perplexity: 6.339178562164307
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/52
 26%|██▌       | 52/200 [7:42:19<21:48:39, 530.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1690.4097341514496
INFO:root:current train perplexity3.7897794246673584
INFO:root:current mean train loss 1697.1120505411118
INFO:root:current train perplexity3.8049724102020264
INFO:root:current mean train loss 1700.8564500572825
INFO:root:current train perplexity3.8181564807891846
INFO:root:current mean train loss 1698.9312737766197
INFO:root:current train perplexity3.813542604446411
INFO:root:current mean train loss 1693.279441944067
INFO:root:current train perplexity3.8034090995788574
INFO:root:current mean train loss 1694.0117424102837
INFO:root:current train perplexity3.80645489692688
INFO:root:current mean train loss 1695.715275911077
INFO:root:current train perplexity3.8119404315948486
INFO:root:current mean train loss 1694.3498583485493
INFO:root:current train perplexity3.8100900650024414
INFO:root:current mean train loss 1694.6398042395863
INFO:root:current train perplexity3.8114218711853027
INFO:root:current mean train loss 1692.8603135629928
INFO:root:current train perplexity3.8103771209716797
INFO:root:current mean train loss 1692.8966055435928
INFO:root:current train perplexity3.812211275100708
INFO:root:current mean train loss 1692.9540345940868
INFO:root:current train perplexity3.8135626316070557
INFO:root:current mean train loss 1693.6514632833812
INFO:root:current train perplexity3.814351797103882
INFO:root:current mean train loss 1695.6096329099444
INFO:root:current train perplexity3.8160400390625
INFO:root:current mean train loss 1695.105075540369
INFO:root:current train perplexity3.8146300315856934
INFO:root:current mean train loss 1696.678269155554
INFO:root:current train perplexity3.814079999923706
INFO:root:current mean train loss 1696.8425140652853
INFO:root:current train perplexity3.8143222332000732
INFO:root:current mean train loss 1697.4357275500167
INFO:root:current train perplexity3.8145477771759033
INFO:root:current mean train loss 1698.8775814337992
INFO:root:current train perplexity3.8188729286193848
INFO:root:current mean train loss 1699.5644401669563
INFO:root:current train perplexity3.8239827156066895

100%|██████████| 1/1 [07:30<00:00, 450.40s/it][A100%|██████████| 1/1 [07:30<00:00, 450.40s/it]
INFO:root:final mean train loss: 1699.5644401669563
INFO:root:final train perplexity: 3.8239827156066895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.49s/it][A100%|██████████| 1/1 [00:39<00:00, 39.49s/it]
INFO:root:eval mean loss: 1803.5014535890402
INFO:root:eval perplexity: 4.303462505340576
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.50s/it][A100%|██████████| 1/1 [00:37<00:00, 37.50s/it]
INFO:root:eval mean loss: 2246.4850844795824
INFO:root:eval perplexity: 6.3401336669921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/53
 26%|██▋       | 53/200 [7:51:09<21:39:21, 530.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1706.9980895996093
INFO:root:current train perplexity3.807234525680542
INFO:root:current mean train loss 1698.6090716552735
INFO:root:current train perplexity3.810343027114868
INFO:root:current mean train loss 1717.332255045573
INFO:root:current train perplexity3.8779382705688477
INFO:root:current mean train loss 1729.2102444458008
INFO:root:current train perplexity3.8995649814605713
INFO:root:current mean train loss 1732.7130329589843
INFO:root:current train perplexity3.911588668823242
INFO:root:current mean train loss 1735.2790696207683
INFO:root:current train perplexity3.9125940799713135
INFO:root:current mean train loss 1737.2526021902902
INFO:root:current train perplexity3.921846628189087
INFO:root:current mean train loss 1739.8729385375977
INFO:root:current train perplexity3.9299747943878174
INFO:root:current mean train loss 1739.5336519368489
INFO:root:current train perplexity3.93127179145813
INFO:root:current mean train loss 1736.7359045410155
INFO:root:current train perplexity3.9259769916534424
INFO:root:current mean train loss 1735.886631303267
INFO:root:current train perplexity3.9233601093292236
INFO:root:current mean train loss 1735.3043704223633
INFO:root:current train perplexity3.923348903656006
INFO:root:current mean train loss 1733.4518524639423
INFO:root:current train perplexity3.9187257289886475
INFO:root:current mean train loss 1733.1190771484376
INFO:root:current train perplexity3.920177459716797
INFO:root:current mean train loss 1731.957686279297
INFO:root:current train perplexity3.918410301208496
INFO:root:current mean train loss 1730.1327304840088
INFO:root:current train perplexity3.9149997234344482
INFO:root:current mean train loss 1731.0271610753678
INFO:root:current train perplexity3.9192423820495605
INFO:root:current mean train loss 1731.260392388238
INFO:root:current train perplexity3.920585870742798
INFO:root:current mean train loss 1731.417599712171
INFO:root:current train perplexity3.919769048690796

100%|██████████| 1/1 [07:36<00:00, 456.64s/it][A100%|██████████| 1/1 [07:36<00:00, 456.64s/it]
INFO:root:final mean train loss: 1731.1779317620183
INFO:root:final train perplexity: 3.920588731765747
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.49s/it][A100%|██████████| 1/1 [00:39<00:00, 39.49s/it]
INFO:root:eval mean loss: 1806.0250971368018
INFO:root:eval perplexity: 4.312259674072266
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.83s/it][A100%|██████████| 1/1 [00:36<00:00, 36.83s/it]
INFO:root:eval mean loss: 2252.138774898881
INFO:root:eval perplexity: 6.36967134475708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/54
 27%|██▋       | 54/200 [8:00:04<21:34:06, 531.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1679.3014131433824
INFO:root:current train perplexity3.907153367996216
INFO:root:current mean train loss 1733.4454909104568
INFO:root:current train perplexity3.9230079650878906
INFO:root:current mean train loss 1750.1319355063724
INFO:root:current train perplexity3.9679388999938965
INFO:root:current mean train loss 1744.2672854643138
INFO:root:current train perplexity3.9545750617980957
INFO:root:current mean train loss 1741.0980813005845
INFO:root:current train perplexity3.9534664154052734
INFO:root:current mean train loss 1737.8901903163533
INFO:root:current train perplexity3.946707248687744
INFO:root:current mean train loss 1732.47821331797
INFO:root:current train perplexity3.9300906658172607
INFO:root:current mean train loss 1733.3524721536676
INFO:root:current train perplexity3.934396266937256
INFO:root:current mean train loss 1730.2905458709456
INFO:root:current train perplexity3.923945903778076
INFO:root:current mean train loss 1729.1527648060505
INFO:root:current train perplexity3.9188451766967773
INFO:root:current mean train loss 1727.576119902094
INFO:root:current train perplexity3.9106285572052
INFO:root:current mean train loss 1728.7503948433653
INFO:root:current train perplexity3.911781072616577
INFO:root:current mean train loss 1726.250571533805
INFO:root:current train perplexity3.906947374343872
INFO:root:current mean train loss 1725.4228164336857
INFO:root:current train perplexity3.9058637619018555
INFO:root:current mean train loss 1725.2044612262648
INFO:root:current train perplexity3.9056546688079834
INFO:root:current mean train loss 1726.0923417254191
INFO:root:current train perplexity3.906421184539795
INFO:root:current mean train loss 1725.2080912309882
INFO:root:current train perplexity3.9031152725219727
INFO:root:current mean train loss 1725.3028229595134
INFO:root:current train perplexity3.9022443294525146
INFO:root:current mean train loss 1723.8747785670062
INFO:root:current train perplexity3.8978099822998047
INFO:root:current mean train loss 1724.8477180174507
INFO:root:current train perplexity3.9002182483673096

100%|██████████| 1/1 [07:28<00:00, 448.06s/it][A100%|██████████| 1/1 [07:28<00:00, 448.06s/it]
INFO:root:final mean train loss: 1724.6476702052898
INFO:root:final train perplexity: 3.90043568611145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.77s/it][A100%|██████████| 1/1 [00:38<00:00, 38.77s/it]
INFO:root:eval mean loss: 1804.2431467475622
INFO:root:eval perplexity: 4.306046009063721
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.73s/it][A100%|██████████| 1/1 [00:37<00:00, 37.73s/it]
INFO:root:eval mean loss: 2248.966262276291
INFO:root:eval perplexity: 6.353079319000244
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/55
 28%|██▊       | 55/200 [8:08:51<21:21:36, 530.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1746.1827033547795
INFO:root:current train perplexity3.9384539127349854
INFO:root:current mean train loss 1734.652120561742
INFO:root:current train perplexity3.93178653717041
INFO:root:current mean train loss 1724.6906039246128
INFO:root:current train perplexity3.90655255317688
INFO:root:current mean train loss 1721.6877708206634
INFO:root:current train perplexity3.900576591491699
INFO:root:current mean train loss 1729.8380374468966
INFO:root:current train perplexity3.920354127883911
INFO:root:current mean train loss 1730.515708208977
INFO:root:current train perplexity3.9255824089050293
INFO:root:current mean train loss 1735.5889871398733
INFO:root:current train perplexity3.9435808658599854
INFO:root:current mean train loss 1750.4564732855606
INFO:root:current train perplexity3.9750192165374756
INFO:root:current mean train loss 1753.2430743393566
INFO:root:current train perplexity3.983027458190918
INFO:root:current mean train loss 1753.6307295936078
INFO:root:current train perplexity3.980079412460327
INFO:root:current mean train loss 1751.8701390279332
INFO:root:current train perplexity3.9738075733184814
INFO:root:current mean train loss 1748.9866982111855
INFO:root:current train perplexity3.96920108795166
INFO:root:current mean train loss 1748.0268315295152
INFO:root:current train perplexity3.968564748764038
INFO:root:current mean train loss 1748.8782076857078
INFO:root:current train perplexity3.9689948558807373
INFO:root:current mean train loss 1747.8943027725113
INFO:root:current train perplexity3.9673755168914795
INFO:root:current mean train loss 1748.4867164581976
INFO:root:current train perplexity3.969940662384033
INFO:root:current mean train loss 1747.9216778497218
INFO:root:current train perplexity3.9681220054626465
INFO:root:current mean train loss 1747.4887470038566
INFO:root:current train perplexity3.9685282707214355
INFO:root:current mean train loss 1747.7844999057515
INFO:root:current train perplexity3.9693028926849365
INFO:root:current mean train loss 1747.8277746948115
INFO:root:current train perplexity3.9705114364624023

100%|██████████| 1/1 [07:32<00:00, 452.26s/it][A100%|██████████| 1/1 [07:32<00:00, 452.26s/it]
INFO:root:final mean train loss: 1747.6929408394203
INFO:root:final train perplexity: 3.972022533416748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.82s/it][A100%|██████████| 1/1 [00:38<00:00, 38.82s/it]
INFO:root:eval mean loss: 1815.3870369119848
INFO:root:eval perplexity: 4.345052719116211
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.43s/it][A100%|██████████| 1/1 [00:37<00:00, 37.43s/it]
INFO:root:eval mean loss: 2260.825138173205
INFO:root:eval perplexity: 6.4153218269348145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/56
 28%|██▊       | 56/200 [8:17:42<21:13:14, 530.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1718.1876316444548
INFO:root:current train perplexity3.9166409969329834
INFO:root:current mean train loss 1735.8046980093648
INFO:root:current train perplexity3.952252149581909
INFO:root:current mean train loss 1743.2589908919012
INFO:root:current train perplexity3.9730000495910645
INFO:root:current mean train loss 1739.7282259253695
INFO:root:current train perplexity3.956892728805542
INFO:root:current mean train loss 1742.235766493296
INFO:root:current train perplexity3.9622998237609863
INFO:root:current mean train loss 1743.216889701608
INFO:root:current train perplexity3.9628689289093018
INFO:root:current mean train loss 1741.0768763575868
INFO:root:current train perplexity3.9599251747131348
INFO:root:current mean train loss 1743.8940104600117
INFO:root:current train perplexity3.962855815887451
INFO:root:current mean train loss 1745.300309751625
INFO:root:current train perplexity3.961782693862915
INFO:root:current mean train loss 1744.0665998168047
INFO:root:current train perplexity3.960545063018799
INFO:root:current mean train loss 1742.7729442244365
INFO:root:current train perplexity3.9562172889709473
INFO:root:current mean train loss 1741.3566633633798
INFO:root:current train perplexity3.952969789505005
INFO:root:current mean train loss 1741.9271277314085
INFO:root:current train perplexity3.952720880508423
INFO:root:current mean train loss 1742.4116048297558
INFO:root:current train perplexity3.954455852508545
INFO:root:current mean train loss 1740.9201999193713
INFO:root:current train perplexity3.9518606662750244
INFO:root:current mean train loss 1740.9806349734042
INFO:root:current train perplexity3.951328754425049
INFO:root:current mean train loss 1741.3470274880754
INFO:root:current train perplexity3.9511544704437256
INFO:root:current mean train loss 1741.1036444576314
INFO:root:current train perplexity3.9515888690948486
INFO:root:current mean train loss 1740.2584469226938
INFO:root:current train perplexity3.9477791786193848
INFO:root:current mean train loss 1740.150228448552
INFO:root:current train perplexity3.9479124546051025

100%|██████████| 1/1 [07:26<00:00, 446.56s/it][A100%|██████████| 1/1 [07:26<00:00, 446.56s/it]
INFO:root:final mean train loss: 1739.8183401995575
INFO:root:final train perplexity: 3.9474146366119385
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.55s/it][A100%|██████████| 1/1 [00:39<00:00, 39.55s/it]
INFO:root:eval mean loss: 1828.3081149919658
INFO:root:eval perplexity: 4.390722274780273
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.68s/it][A100%|██████████| 1/1 [00:36<00:00, 36.68s/it]
INFO:root:eval mean loss: 2273.245457858904
INFO:root:eval perplexity: 6.481165885925293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/57
 28%|██▊       | 57/200 [8:26:27<21:00:34, 528.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1709.4716545553767
INFO:root:current train perplexity3.8919575214385986
INFO:root:current mean train loss 1711.037574404762
INFO:root:current train perplexity3.898513078689575
INFO:root:current mean train loss 1709.614980669164
INFO:root:current train perplexity3.883406400680542
INFO:root:current mean train loss 1716.0194144870925
INFO:root:current train perplexity3.8870983123779297
INFO:root:current mean train loss 1719.4749732384314
INFO:root:current train perplexity3.8948843479156494
INFO:root:current mean train loss 1721.8362867328483
INFO:root:current train perplexity3.901536464691162
INFO:root:current mean train loss 1724.6682989611597
INFO:root:current train perplexity3.904123306274414
INFO:root:current mean train loss 1722.9794541994731
INFO:root:current train perplexity3.90252685546875
INFO:root:current mean train loss 1724.8601791452154
INFO:root:current train perplexity3.9053685665130615
INFO:root:current mean train loss 1725.9409318403764
INFO:root:current train perplexity3.906357526779175
INFO:root:current mean train loss 1729.2733515478699
INFO:root:current train perplexity3.9141433238983154
INFO:root:current mean train loss 1728.5673565799243
INFO:root:current train perplexity3.9144787788391113
INFO:root:current mean train loss 1730.4101388251368
INFO:root:current train perplexity3.9165685176849365
INFO:root:current mean train loss 1730.3905526322928
INFO:root:current train perplexity3.9180541038513184
INFO:root:current mean train loss 1730.463608744359
INFO:root:current train perplexity3.9193596839904785
INFO:root:current mean train loss 1731.1364247069066
INFO:root:current train perplexity3.919890880584717
INFO:root:current mean train loss 1731.2207645260744
INFO:root:current train perplexity3.9203147888183594
INFO:root:current mean train loss 1730.8221909190734
INFO:root:current train perplexity3.9177405834198
INFO:root:current mean train loss 1731.809408706567
INFO:root:current train perplexity3.919299364089966
INFO:root:current mean train loss 1730.9533864463249
INFO:root:current train perplexity3.9186136722564697

100%|██████████| 1/1 [07:33<00:00, 453.64s/it][A100%|██████████| 1/1 [07:33<00:00, 453.64s/it]
INFO:root:final mean train loss: 1730.9164021502104
INFO:root:final train perplexity: 3.9197802543640137
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.28s/it][A100%|██████████| 1/1 [00:39<00:00, 39.28s/it]
INFO:root:eval mean loss: 1805.8038208873559
INFO:root:eval perplexity: 4.311488151550293
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.11s/it][A100%|██████████| 1/1 [00:37<00:00, 37.11s/it]
INFO:root:eval mean loss: 2248.4523237512467
INFO:root:eval perplexity: 6.350396156311035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/58
 29%|██▉       | 58/200 [8:35:19<20:54:10, 529.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1719.096728515625
INFO:root:current train perplexity3.9092941284179688
INFO:root:current mean train loss 1701.7927780563766
INFO:root:current train perplexity3.8814046382904053
INFO:root:current mean train loss 1710.258942828262
INFO:root:current train perplexity3.896191120147705
INFO:root:current mean train loss 1713.3228474406453
INFO:root:current train perplexity3.892739772796631
INFO:root:current mean train loss 1712.8126804627093
INFO:root:current train perplexity3.8924176692962646
INFO:root:current mean train loss 1715.739983974359
INFO:root:current train perplexity3.8919737339019775
INFO:root:current mean train loss 1717.608134694343
INFO:root:current train perplexity3.891446590423584
INFO:root:current mean train loss 1714.656803903762
INFO:root:current train perplexity3.885340929031372
INFO:root:current mean train loss 1715.879371220648
INFO:root:current train perplexity3.8856945037841797
INFO:root:current mean train loss 1715.3769924105727
INFO:root:current train perplexity3.8835554122924805
INFO:root:current mean train loss 1714.111491485455
INFO:root:current train perplexity3.8808271884918213
INFO:root:current mean train loss 1715.3096906315927
INFO:root:current train perplexity3.88093638420105
INFO:root:current mean train loss 1715.618297342382
INFO:root:current train perplexity3.880819797515869
INFO:root:current mean train loss 1716.9823696976534
INFO:root:current train perplexity3.8854434490203857
INFO:root:current mean train loss 1717.6323615385627
INFO:root:current train perplexity3.8858444690704346
INFO:root:current mean train loss 1718.7931689915222
INFO:root:current train perplexity3.8874337673187256
INFO:root:current mean train loss 1720.923504584338
INFO:root:current train perplexity3.889214038848877
INFO:root:current mean train loss 1721.5205676508886
INFO:root:current train perplexity3.8900556564331055
INFO:root:current mean train loss 1722.3321317556365
INFO:root:current train perplexity3.893037796020508

100%|██████████| 1/1 [07:33<00:00, 453.24s/it][A100%|██████████| 1/1 [07:33<00:00, 453.24s/it]
INFO:root:final mean train loss: 1723.2462710689788
INFO:root:final train perplexity: 3.8961236476898193
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.17s/it][A100%|██████████| 1/1 [00:39<00:00, 39.17s/it]
INFO:root:eval mean loss: 1824.1995589885305
INFO:root:eval perplexity: 4.3761491775512695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.46s/it][A100%|██████████| 1/1 [00:36<00:00, 36.46s/it]
INFO:root:eval mean loss: 2268.5672702654033
INFO:root:eval perplexity: 6.45628547668457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/59
 30%|██▉       | 59/200 [8:44:11<20:46:18, 530.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1784.3004760742188
INFO:root:current train perplexity4.095778942108154
INFO:root:current mean train loss 1758.0688069661458
INFO:root:current train perplexity3.9930365085601807
INFO:root:current mean train loss 1744.5091576906714
INFO:root:current train perplexity3.9601616859436035
INFO:root:current mean train loss 1732.638903889435
INFO:root:current train perplexity3.91418194770813
INFO:root:current mean train loss 1731.7378734379859
INFO:root:current train perplexity3.909116506576538
INFO:root:current mean train loss 1734.260674829977
INFO:root:current train perplexity3.9181580543518066
INFO:root:current mean train loss 1735.3429176102445
INFO:root:current train perplexity3.916642665863037
INFO:root:current mean train loss 1738.639326742232
INFO:root:current train perplexity3.9247350692749023
INFO:root:current mean train loss 1745.523365505913
INFO:root:current train perplexity3.944058418273926
INFO:root:current mean train loss 1746.3667981471297
INFO:root:current train perplexity3.952134847640991
INFO:root:current mean train loss 1744.9160448633982
INFO:root:current train perplexity3.952897310256958
INFO:root:current mean train loss 1743.8388454762649
INFO:root:current train perplexity3.952310562133789
INFO:root:current mean train loss 1743.2784921452528
INFO:root:current train perplexity3.9532129764556885
INFO:root:current mean train loss 1743.531978296611
INFO:root:current train perplexity3.9539053440093994
INFO:root:current mean train loss 1742.746476155715
INFO:root:current train perplexity3.9514808654785156
INFO:root:current mean train loss 1741.9775556419565
INFO:root:current train perplexity3.951758861541748
INFO:root:current mean train loss 1746.325853593043
INFO:root:current train perplexity3.966827392578125
INFO:root:current mean train loss 1751.2359127129848
INFO:root:current train perplexity3.9816372394561768
INFO:root:current mean train loss 1757.553199082183
INFO:root:current train perplexity4.000072002410889
INFO:root:current mean train loss 1760.0307685218274
INFO:root:current train perplexity4.010065078735352

100%|██████████| 1/1 [07:34<00:00, 454.38s/it][A100%|██████████| 1/1 [07:34<00:00, 454.38s/it]
INFO:root:final mean train loss: 1759.9863567188781
INFO:root:final train perplexity: 4.010746479034424
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.30s/it][A100%|██████████| 1/1 [00:39<00:00, 39.30s/it]
INFO:root:eval mean loss: 1822.802714029948
INFO:root:eval perplexity: 4.3712053298950195
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.77s/it][A100%|██████████| 1/1 [00:36<00:00, 36.77s/it]
INFO:root:eval mean loss: 2266.370785976978
INFO:root:eval perplexity: 6.444637298583984
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/60
 30%|███       | 60/200 [8:53:04<20:39:11, 531.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1755.0598337273848
INFO:root:current train perplexity3.9789626598358154
INFO:root:current mean train loss 1747.9161417985163
INFO:root:current train perplexity3.961242914199829
INFO:root:current mean train loss 1744.130396176691
INFO:root:current train perplexity3.953519344329834
INFO:root:current mean train loss 1739.1059206780221
INFO:root:current train perplexity3.935605525970459
INFO:root:current mean train loss 1740.3479662328461
INFO:root:current train perplexity3.933720350265503
INFO:root:current mean train loss 1737.6316477751686
INFO:root:current train perplexity3.926706075668335
INFO:root:current mean train loss 1738.5393951859728
INFO:root:current train perplexity3.925806999206543
INFO:root:current mean train loss 1736.4401122028207
INFO:root:current train perplexity3.9136693477630615
INFO:root:current mean train loss 1733.8440443698012
INFO:root:current train perplexity3.90820050239563
INFO:root:current mean train loss 1731.5751423135287
INFO:root:current train perplexity3.9068286418914795
INFO:root:current mean train loss 1731.6499917102399
INFO:root:current train perplexity3.9107348918914795
INFO:root:current mean train loss 1728.9726432684386
INFO:root:current train perplexity3.9071319103240967
INFO:root:current mean train loss 1728.1841701541991
INFO:root:current train perplexity3.9059457778930664
INFO:root:current mean train loss 1727.349903861531
INFO:root:current train perplexity3.9019012451171875
INFO:root:current mean train loss 1725.2478340476898
INFO:root:current train perplexity3.895596504211426
INFO:root:current mean train loss 1724.1869318386377
INFO:root:current train perplexity3.892786741256714
INFO:root:current mean train loss 1722.7908016438098
INFO:root:current train perplexity3.891498565673828
INFO:root:current mean train loss 1722.6749233492174
INFO:root:current train perplexity3.890873908996582
INFO:root:current mean train loss 1722.299937763602
INFO:root:current train perplexity3.8913891315460205
INFO:root:current mean train loss 1722.3520695466186
INFO:root:current train perplexity3.8919074535369873

100%|██████████| 1/1 [07:31<00:00, 451.64s/it][A100%|██████████| 1/1 [07:31<00:00, 451.64s/it]
INFO:root:final mean train loss: 1721.3902607897105
INFO:root:final train perplexity: 3.8904216289520264
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.62s/it][A100%|██████████| 1/1 [00:38<00:00, 38.62s/it]
INFO:root:eval mean loss: 1836.309140902039
INFO:root:eval perplexity: 4.419242858886719
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.80s/it][A100%|██████████| 1/1 [00:36<00:00, 36.80s/it]
INFO:root:eval mean loss: 2279.055165392287
INFO:root:eval perplexity: 6.5121941566467285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/61
 30%|███       | 61/200 [9:01:53<20:29:18, 530.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1696.3676452636719
INFO:root:current train perplexity3.8345110416412354
INFO:root:current mean train loss 1707.1435780244715
INFO:root:current train perplexity3.8653767108917236
INFO:root:current mean train loss 1721.5617598194187
INFO:root:current train perplexity3.89689564704895
INFO:root:current mean train loss 1719.923477536156
INFO:root:current train perplexity3.8876399993896484
INFO:root:current mean train loss 1717.5277354389157
INFO:root:current train perplexity3.880119562149048
INFO:root:current mean train loss 1720.799880525959
INFO:root:current train perplexity3.8831887245178223
INFO:root:current mean train loss 1721.3492314560608
INFO:root:current train perplexity3.8834738731384277
INFO:root:current mean train loss 1717.5802285567574
INFO:root:current train perplexity3.873706102371216
INFO:root:current mean train loss 1718.0984380256616
INFO:root:current train perplexity3.8773226737976074
INFO:root:current mean train loss 1717.2972496880425
INFO:root:current train perplexity3.8750343322753906
INFO:root:current mean train loss 1716.634200873062
INFO:root:current train perplexity3.875107765197754
INFO:root:current mean train loss 1715.8824282364108
INFO:root:current train perplexity3.872382879257202
INFO:root:current mean train loss 1716.7108156272122
INFO:root:current train perplexity3.8730595111846924
INFO:root:current mean train loss 1715.5178036261461
INFO:root:current train perplexity3.8715786933898926
INFO:root:current mean train loss 1715.8255612684159
INFO:root:current train perplexity3.872765064239502
INFO:root:current mean train loss 1717.4973117510478
INFO:root:current train perplexity3.8746731281280518
INFO:root:current mean train loss 1718.244583689496
INFO:root:current train perplexity3.8758604526519775
INFO:root:current mean train loss 1716.89187769736
INFO:root:current train perplexity3.873807191848755
INFO:root:current mean train loss 1716.5728351534842
INFO:root:current train perplexity3.8721542358398438
INFO:root:current mean train loss 1716.1250064944434
INFO:root:current train perplexity3.8708999156951904

100%|██████████| 1/1 [07:30<00:00, 450.40s/it][A100%|██████████| 1/1 [07:30<00:00, 450.40s/it]
INFO:root:final mean train loss: 1714.71093596104
INFO:root:final train perplexity: 3.869967460632324
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.70s/it][A100%|██████████| 1/1 [00:39<00:00, 39.70s/it]
INFO:root:eval mean loss: 1808.3114156520112
INFO:root:eval perplexity: 4.320245265960693
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.22s/it][A100%|██████████| 1/1 [00:38<00:00, 38.22s/it]
INFO:root:eval mean loss: 2255.0085639683066
INFO:root:eval perplexity: 6.384716510772705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/62
 31%|███       | 62/200 [9:10:44<20:20:39, 530.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1668.8536699403007
INFO:root:current train perplexity3.7742679119110107
INFO:root:current mean train loss 1688.592013888889
INFO:root:current train perplexity3.7923879623413086
INFO:root:current mean train loss 1692.0275082795517
INFO:root:current train perplexity3.7924585342407227
INFO:root:current mean train loss 1691.3275578744688
INFO:root:current train perplexity3.7965734004974365
INFO:root:current mean train loss 1694.1963478071536
INFO:root:current train perplexity3.8028981685638428
INFO:root:current mean train loss 1695.5089261432668
INFO:root:current train perplexity3.801675796508789
INFO:root:current mean train loss 1695.1461609727937
INFO:root:current train perplexity3.8045568466186523
INFO:root:current mean train loss 1695.8955653622488
INFO:root:current train perplexity3.805448532104492
INFO:root:current mean train loss 1696.7167462151049
INFO:root:current train perplexity3.8084115982055664
INFO:root:current mean train loss 1698.0817671272462
INFO:root:current train perplexity3.8108808994293213
INFO:root:current mean train loss 1697.9593234592014
INFO:root:current train perplexity3.8085851669311523
INFO:root:current mean train loss 1701.830440312805
INFO:root:current train perplexity3.818380355834961
INFO:root:current mean train loss 1703.8999542699084
INFO:root:current train perplexity3.827422857284546
INFO:root:current mean train loss 1702.8065623123384
INFO:root:current train perplexity3.8266472816467285
INFO:root:current mean train loss 1703.1521296858868
INFO:root:current train perplexity3.8278791904449463
INFO:root:current mean train loss 1704.6739754268451
INFO:root:current train perplexity3.833730459213257
INFO:root:current mean train loss 1705.3570640827047
INFO:root:current train perplexity3.8355414867401123
INFO:root:current mean train loss 1704.7812364211575
INFO:root:current train perplexity3.8356893062591553
INFO:root:current mean train loss 1703.1920427547811
INFO:root:current train perplexity3.8336832523345947
INFO:root:current mean train loss 1703.3248999185948
INFO:root:current train perplexity3.834071397781372

100%|██████████| 1/1 [07:38<00:00, 458.50s/it][A100%|██████████| 1/1 [07:38<00:00, 458.50s/it]
INFO:root:final mean train loss: 1702.943894440636
INFO:root:final train perplexity: 3.834195137023926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.54s/it][A100%|██████████| 1/1 [00:39<00:00, 39.54s/it]
INFO:root:eval mean loss: 1806.9016589442044
INFO:root:eval perplexity: 4.315320014953613
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.58s/it][A100%|██████████| 1/1 [00:37<00:00, 37.58s/it]
INFO:root:eval mean loss: 2249.802522699884
INFO:root:eval perplexity: 6.357450008392334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/63
 32%|███▏      | 63/200 [9:19:42<20:16:54, 532.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1686.7800415039062
INFO:root:current train perplexity3.7892730236053467
INFO:root:current mean train loss 1694.7926707548254
INFO:root:current train perplexity3.8023080825805664
INFO:root:current mean train loss 1685.7975405092593
INFO:root:current train perplexity3.773357391357422
INFO:root:current mean train loss 1683.7328712257179
INFO:root:current train perplexity3.7720513343811035
INFO:root:current mean train loss 1682.1562967503326
INFO:root:current train perplexity3.7686357498168945
INFO:root:current mean train loss 1687.9314054790295
INFO:root:current train perplexity3.7825539112091064
INFO:root:current mean train loss 1690.6237444977262
INFO:root:current train perplexity3.792135715484619
INFO:root:current mean train loss 1694.4764838676947
INFO:root:current train perplexity3.79988431930542
INFO:root:current mean train loss 1695.591814413838
INFO:root:current train perplexity3.8079798221588135
INFO:root:current mean train loss 1693.50825472173
INFO:root:current train perplexity3.803079605102539
INFO:root:current mean train loss 1693.058520393728
INFO:root:current train perplexity3.8010213375091553
INFO:root:current mean train loss 1693.3180623372396
INFO:root:current train perplexity3.80354380607605
INFO:root:current mean train loss 1693.3122623954232
INFO:root:current train perplexity3.8028786182403564
INFO:root:current mean train loss 1694.7188050652942
INFO:root:current train perplexity3.8031511306762695
INFO:root:current mean train loss 1693.4569830862033
INFO:root:current train perplexity3.8033578395843506
INFO:root:current mean train loss 1692.3283139368532
INFO:root:current train perplexity3.801288366317749
INFO:root:current mean train loss 1691.6604606948213
INFO:root:current train perplexity3.79774808883667
INFO:root:current mean train loss 1690.7428400589247
INFO:root:current train perplexity3.796661615371704
INFO:root:current mean train loss 1690.363915493399
INFO:root:current train perplexity3.7954108715057373
INFO:root:current mean train loss 1691.391855307642
INFO:root:current train perplexity3.79748272895813

100%|██████████| 1/1 [07:32<00:00, 452.07s/it][A100%|██████████| 1/1 [07:32<00:00, 452.07s/it]
INFO:root:final mean train loss: 1690.853016017004
INFO:root:final train perplexity: 3.797783374786377
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.38s/it][A100%|██████████| 1/1 [00:39<00:00, 39.38s/it]
INFO:root:eval mean loss: 1806.5623593161292
INFO:root:eval perplexity: 4.3141350746154785
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.14s/it][A100%|██████████| 1/1 [00:37<00:00, 37.14s/it]
INFO:root:eval mean loss: 2254.4875808607603
INFO:root:eval perplexity: 6.381983280181885
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/64
 32%|███▏      | 64/200 [9:28:33<20:06:41, 532.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1667.4914943651222
INFO:root:current train perplexity3.7726070880889893
INFO:root:current mean train loss 1668.9671363218583
INFO:root:current train perplexity3.7619266510009766
INFO:root:current mean train loss 1679.056011558825
INFO:root:current train perplexity3.780552387237549
INFO:root:current mean train loss 1680.4491834221576
INFO:root:current train perplexity3.7769644260406494
INFO:root:current mean train loss 1679.9784647414817
INFO:root:current train perplexity3.777059316635132
INFO:root:current mean train loss 1678.4945588249973
INFO:root:current train perplexity3.7727460861206055
INFO:root:current mean train loss 1680.8424020736672
INFO:root:current train perplexity3.7711691856384277
INFO:root:current mean train loss 1680.3331819992356
INFO:root:current train perplexity3.7702057361602783
INFO:root:current mean train loss 1679.450041176367
INFO:root:current train perplexity3.7676570415496826
INFO:root:current mean train loss 1679.504070370876
INFO:root:current train perplexity3.767354726791382
INFO:root:current mean train loss 1679.9939071528936
INFO:root:current train perplexity3.765556573867798
INFO:root:current mean train loss 1680.5524177326308
INFO:root:current train perplexity3.763915777206421
INFO:root:current mean train loss 1679.4408484446326
INFO:root:current train perplexity3.760740280151367
INFO:root:current mean train loss 1678.3783151726973
INFO:root:current train perplexity3.757917881011963
INFO:root:current mean train loss 1676.316728377711
INFO:root:current train perplexity3.7547900676727295
INFO:root:current mean train loss 1675.3655296667011
INFO:root:current train perplexity3.752079963684082
INFO:root:current mean train loss 1676.2379031721205
INFO:root:current train perplexity3.7538297176361084
INFO:root:current mean train loss 1677.4360299646755
INFO:root:current train perplexity3.756291627883911
INFO:root:current mean train loss 1677.594106507415
INFO:root:current train perplexity3.757925033569336

100%|██████████| 1/1 [07:30<00:00, 450.04s/it][A100%|██████████| 1/1 [07:30<00:00, 450.04s/it]
INFO:root:final mean train loss: 1678.2436796756808
INFO:root:final train perplexity: 3.7601778507232666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.66s/it][A100%|██████████| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 1803.6702504259474
INFO:root:eval perplexity: 4.304050445556641
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.05s/it][A100%|██████████| 1/1 [00:37<00:00, 37.05s/it]
INFO:root:eval mean loss: 2250.1294434459496
INFO:root:eval perplexity: 6.359158039093018
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/65
 32%|███▎      | 65/200 [9:37:22<19:55:41, 531.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1712.8684997558594
INFO:root:current train perplexity3.7972207069396973
INFO:root:current mean train loss 1675.452840951773
INFO:root:current train perplexity3.769375801086426
INFO:root:current mean train loss 1681.1617258109297
INFO:root:current train perplexity3.767364978790283
INFO:root:current mean train loss 1687.1875477841027
INFO:root:current train perplexity3.791588306427002
INFO:root:current mean train loss 1695.4156028823097
INFO:root:current train perplexity3.808091163635254
INFO:root:current mean train loss 1700.5305655343193
INFO:root:current train perplexity3.8313469886779785
INFO:root:current mean train loss 1700.1318405858729
INFO:root:current train perplexity3.8294036388397217
INFO:root:current mean train loss 1699.3108530911531
INFO:root:current train perplexity3.8268494606018066
INFO:root:current mean train loss 1699.6741384629586
INFO:root:current train perplexity3.8246617317199707
INFO:root:current mean train loss 1698.4250416713478
INFO:root:current train perplexity3.818713665008545
INFO:root:current mean train loss 1696.95453817056
INFO:root:current train perplexity3.814523696899414
INFO:root:current mean train loss 1696.5188901597176
INFO:root:current train perplexity3.8133351802825928
INFO:root:current mean train loss 1696.1751656262977
INFO:root:current train perplexity3.8121416568756104
INFO:root:current mean train loss 1696.5958106854212
INFO:root:current train perplexity3.8127479553222656
INFO:root:current mean train loss 1696.6362151664887
INFO:root:current train perplexity3.812467098236084
INFO:root:current mean train loss 1695.8244451969229
INFO:root:current train perplexity3.8088529109954834
INFO:root:current mean train loss 1694.9129610513512
INFO:root:current train perplexity3.8061487674713135
INFO:root:current mean train loss 1692.8180405880923
INFO:root:current train perplexity3.8030667304992676
INFO:root:current mean train loss 1692.9141201518328
INFO:root:current train perplexity3.8024227619171143
INFO:root:current mean train loss 1692.273427177878
INFO:root:current train perplexity3.799921751022339

100%|██████████| 1/1 [07:36<00:00, 456.73s/it][A100%|██████████| 1/1 [07:36<00:00, 456.73s/it]
INFO:root:final mean train loss: 1691.2618013921556
INFO:root:final train perplexity: 3.7990081310272217
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.02s/it][A100%|██████████| 1/1 [00:40<00:00, 40.02s/it]
INFO:root:eval mean loss: 1795.5060944252825
INFO:root:eval perplexity: 4.2757086753845215
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.82s/it][A100%|██████████| 1/1 [00:36<00:00, 36.82s/it]
INFO:root:eval mean loss: 2242.2316773326684
INFO:root:eval perplexity: 6.318002700805664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/66
 33%|███▎      | 66/200 [9:46:19<19:50:04, 532.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1689.2125999813989
INFO:root:current train perplexity3.7318525314331055
INFO:root:current mean train loss 1655.64597793453
INFO:root:current train perplexity3.7073192596435547
INFO:root:current mean train loss 1665.3627653510323
INFO:root:current train perplexity3.7183990478515625
INFO:root:current mean train loss 1665.7406370929468
INFO:root:current train perplexity3.7267889976501465
INFO:root:current mean train loss 1667.8215427715818
INFO:root:current train perplexity3.7251229286193848
INFO:root:current mean train loss 1668.6701039261186
INFO:root:current train perplexity3.736931085586548
INFO:root:current mean train loss 1668.551154734048
INFO:root:current train perplexity3.736992120742798
INFO:root:current mean train loss 1671.0030813865292
INFO:root:current train perplexity3.73995041847229
INFO:root:current mean train loss 1669.9309142992063
INFO:root:current train perplexity3.7384510040283203
INFO:root:current mean train loss 1670.3802205271104
INFO:root:current train perplexity3.7365381717681885
INFO:root:current mean train loss 1670.0118259949268
INFO:root:current train perplexity3.7361395359039307
INFO:root:current mean train loss 1669.2914472813059
INFO:root:current train perplexity3.7355895042419434
INFO:root:current mean train loss 1670.466881254479
INFO:root:current train perplexity3.7379817962646484
INFO:root:current mean train loss 1670.7686992128358
INFO:root:current train perplexity3.738905191421509
INFO:root:current mean train loss 1671.760432673541
INFO:root:current train perplexity3.740227222442627
INFO:root:current mean train loss 1673.6781546788966
INFO:root:current train perplexity3.7439355850219727
INFO:root:current mean train loss 1673.8700762212754
INFO:root:current train perplexity3.74731183052063
INFO:root:current mean train loss 1673.3584838370678
INFO:root:current train perplexity3.7459449768066406
INFO:root:current mean train loss 1674.0866712625705
INFO:root:current train perplexity3.7457330226898193
INFO:root:current mean train loss 1674.6782656127994
INFO:root:current train perplexity3.74713134765625

100%|██████████| 1/1 [07:31<00:00, 451.29s/it][A100%|██████████| 1/1 [07:31<00:00, 451.29s/it]
INFO:root:final mean train loss: 1673.6633153348876
INFO:root:final train perplexity: 3.746610164642334
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.82s/it][A100%|██████████| 1/1 [00:39<00:00, 39.82s/it]
INFO:root:eval mean loss: 1801.5231253982436
INFO:root:eval perplexity: 4.296578884124756
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.35s/it][A100%|██████████| 1/1 [00:37<00:00, 37.35s/it]
INFO:root:eval mean loss: 2249.211881164118
INFO:root:eval perplexity: 6.354362487792969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/67
 34%|███▎      | 67/200 [9:55:10<19:39:59, 532.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1644.8995007966694
INFO:root:current train perplexity3.6523993015289307
INFO:root:current mean train loss 1664.2668943543365
INFO:root:current train perplexity3.7021005153656006
INFO:root:current mean train loss 1674.638739064962
INFO:root:current train perplexity3.740828514099121
INFO:root:current mean train loss 1668.2954000439165
INFO:root:current train perplexity3.7280900478363037
INFO:root:current mean train loss 1672.5928333578588
INFO:root:current train perplexity3.734577178955078
INFO:root:current mean train loss 1674.488123783835
INFO:root:current train perplexity3.7425005435943604
INFO:root:current mean train loss 1671.6547641096445
INFO:root:current train perplexity3.7376623153686523
INFO:root:current mean train loss 1669.9308569269774
INFO:root:current train perplexity3.737016201019287
INFO:root:current mean train loss 1668.3160640743865
INFO:root:current train perplexity3.7322659492492676
INFO:root:current mean train loss 1668.9278712811501
INFO:root:current train perplexity3.7314374446868896
INFO:root:current mean train loss 1669.3025873731785
INFO:root:current train perplexity3.7321560382843018
INFO:root:current mean train loss 1672.3649526907816
INFO:root:current train perplexity3.7387683391571045
INFO:root:current mean train loss 1672.621946171528
INFO:root:current train perplexity3.7387657165527344
INFO:root:current mean train loss 1673.3717827447683
INFO:root:current train perplexity3.739777088165283
INFO:root:current mean train loss 1674.2655201621446
INFO:root:current train perplexity3.742588996887207
INFO:root:current mean train loss 1675.4271461675319
INFO:root:current train perplexity3.7469780445098877
INFO:root:current mean train loss 1673.8259778890272
INFO:root:current train perplexity3.744365930557251
INFO:root:current mean train loss 1673.1372705246692
INFO:root:current train perplexity3.7440946102142334
INFO:root:current mean train loss 1672.7002373530374
INFO:root:current train perplexity3.7426867485046387
INFO:root:current mean train loss 1672.4479766940185
INFO:root:current train perplexity3.741687297821045

100%|██████████| 1/1 [07:31<00:00, 451.89s/it][A100%|██████████| 1/1 [07:31<00:00, 451.89s/it]
INFO:root:final mean train loss: 1672.532247184565
INFO:root:final train perplexity: 3.743267059326172
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.99s/it][A100%|██████████| 1/1 [00:38<00:00, 39.00s/it]
INFO:root:eval mean loss: 1791.837192833001
INFO:root:eval perplexity: 4.263033866882324
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.49s/it][A100%|██████████| 1/1 [00:38<00:00, 38.49s/it]
INFO:root:eval mean loss: 2237.958377486425
INFO:root:eval perplexity: 6.295844078063965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/68
 34%|███▍      | 68/200 [10:04:02<19:30:47, 532.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1674.2700106534091
INFO:root:current train perplexity3.7243661880493164
INFO:root:current mean train loss 1657.798660376764
INFO:root:current train perplexity3.6901910305023193
INFO:root:current mean train loss 1655.8817636527267
INFO:root:current train perplexity3.693174362182617
INFO:root:current mean train loss 1660.275361740757
INFO:root:current train perplexity3.706637382507324
INFO:root:current mean train loss 1663.4099791809754
INFO:root:current train perplexity3.7110745906829834
INFO:root:current mean train loss 1660.139768968187
INFO:root:current train perplexity3.705260753631592
INFO:root:current mean train loss 1661.6323456509422
INFO:root:current train perplexity3.710516929626465
INFO:root:current mean train loss 1664.2929019751139
INFO:root:current train perplexity3.7176716327667236
INFO:root:current mean train loss 1667.1010523745888
INFO:root:current train perplexity3.717927932739258
INFO:root:current mean train loss 1671.9830672498772
INFO:root:current train perplexity3.7255537509918213
INFO:root:current mean train loss 1669.8689435769031
INFO:root:current train perplexity3.71977162361145
INFO:root:current mean train loss 1668.2674844849162
INFO:root:current train perplexity3.7172462940216064
INFO:root:current mean train loss 1668.7427983378984
INFO:root:current train perplexity3.719848871231079
INFO:root:current mean train loss 1668.0348718396851
INFO:root:current train perplexity3.720637798309326
INFO:root:current mean train loss 1666.37102436708
INFO:root:current train perplexity3.7213549613952637
INFO:root:current mean train loss 1666.018317533536
INFO:root:current train perplexity3.721846103668213
INFO:root:current mean train loss 1665.782568654409
INFO:root:current train perplexity3.7208714485168457
INFO:root:current mean train loss 1665.5574000344996
INFO:root:current train perplexity3.7212939262390137
INFO:root:current mean train loss 1665.8062209795107
INFO:root:current train perplexity3.722313404083252
INFO:root:current mean train loss 1667.2818442420275
INFO:root:current train perplexity3.7277114391326904

100%|██████████| 1/1 [07:34<00:00, 454.17s/it][A100%|██████████| 1/1 [07:34<00:00, 454.17s/it]
INFO:root:final mean train loss: 1667.6609291157456
INFO:root:final train perplexity: 3.7289042472839355
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.66s/it][A100%|██████████| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 1814.9409071469138
INFO:root:eval perplexity: 4.343484401702881
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.47s/it][A100%|██████████| 1/1 [00:37<00:00, 37.47s/it]
INFO:root:eval mean loss: 2266.3527238994625
INFO:root:eval perplexity: 6.44454288482666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/69
 34%|███▍      | 69/200 [10:12:55<19:22:58, 532.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1651.618943956163
INFO:root:current train perplexity3.7053182125091553
INFO:root:current mean train loss 1648.1593195005905
INFO:root:current train perplexity3.68184494972229
INFO:root:current mean train loss 1648.002883462345
INFO:root:current train perplexity3.6751866340637207
INFO:root:current mean train loss 1653.5919534006428
INFO:root:current train perplexity3.6866655349731445
INFO:root:current mean train loss 1658.6322298211567
INFO:root:current train perplexity3.6983470916748047
INFO:root:current mean train loss 1659.4395719941679
INFO:root:current train perplexity3.700247287750244
INFO:root:current mean train loss 1658.827696119036
INFO:root:current train perplexity3.7006871700286865
INFO:root:current mean train loss 1658.1057306003077
INFO:root:current train perplexity3.6947271823883057
INFO:root:current mean train loss 1660.6122974080777
INFO:root:current train perplexity3.6981518268585205
INFO:root:current mean train loss 1661.5767666538065
INFO:root:current train perplexity3.7057785987854004
INFO:root:current mean train loss 1663.0263365560504
INFO:root:current train perplexity3.709473133087158
INFO:root:current mean train loss 1663.614414983235
INFO:root:current train perplexity3.7101240158081055
INFO:root:current mean train loss 1665.0784989842828
INFO:root:current train perplexity3.7142364978790283
INFO:root:current mean train loss 1665.268895007431
INFO:root:current train perplexity3.7173855304718018
INFO:root:current mean train loss 1666.3341155674148
INFO:root:current train perplexity3.7205567359924316
INFO:root:current mean train loss 1667.9230608369862
INFO:root:current train perplexity3.726010322570801
INFO:root:current mean train loss 1669.5083051617637
INFO:root:current train perplexity3.7303919792175293
INFO:root:current mean train loss 1669.6541998111907
INFO:root:current train perplexity3.730010509490967
INFO:root:current mean train loss 1669.7710517818093
INFO:root:current train perplexity3.7309348583221436
INFO:root:current mean train loss 1669.8021563361672
INFO:root:current train perplexity3.7340428829193115

100%|██████████| 1/1 [07:33<00:00, 453.82s/it][A100%|██████████| 1/1 [07:33<00:00, 453.82s/it]
INFO:root:final mean train loss: 1669.5746829619145
INFO:root:final train perplexity: 3.7345407009124756
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.11s/it][A100%|██████████| 1/1 [00:40<00:00, 40.12s/it]
INFO:root:eval mean loss: 1794.6733355150154
INFO:root:eval perplexity: 4.272829055786133
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.64s/it][A100%|██████████| 1/1 [00:37<00:00, 37.64s/it]
INFO:root:eval mean loss: 2242.0551173433346
INFO:root:eval perplexity: 6.317085266113281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/70
 35%|███▌      | 70/200 [10:21:50<19:15:17, 533.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1677.4768121269312
INFO:root:current train perplexity3.7724313735961914
INFO:root:current mean train loss 1662.0377378110531
INFO:root:current train perplexity3.715684175491333
INFO:root:current mean train loss 1666.124564517328
INFO:root:current train perplexity3.7159128189086914
INFO:root:current mean train loss 1668.2920747183282
INFO:root:current train perplexity3.725764274597168
INFO:root:current mean train loss 1670.746126951128
INFO:root:current train perplexity3.734098196029663
INFO:root:current mean train loss 1669.517841747135
INFO:root:current train perplexity3.7321488857269287
INFO:root:current mean train loss 1666.574751146646
INFO:root:current train perplexity3.7291953563690186
INFO:root:current mean train loss 1666.28663826714
INFO:root:current train perplexity3.7270407676696777
INFO:root:current mean train loss 1666.627617440154
INFO:root:current train perplexity3.7295031547546387
INFO:root:current mean train loss 1668.4685558477233
INFO:root:current train perplexity3.734099864959717
INFO:root:current mean train loss 1668.279830666394
INFO:root:current train perplexity3.7373013496398926
INFO:root:current mean train loss 1669.662608538885
INFO:root:current train perplexity3.7367284297943115
INFO:root:current mean train loss 1669.090742373115
INFO:root:current train perplexity3.735011577606201
INFO:root:current mean train loss 1669.585338485421
INFO:root:current train perplexity3.7356791496276855
INFO:root:current mean train loss 1669.1321337759455
INFO:root:current train perplexity3.7349965572357178
INFO:root:current mean train loss 1668.4835607625914
INFO:root:current train perplexity3.733205795288086
INFO:root:current mean train loss 1668.6484584593786
INFO:root:current train perplexity3.7323641777038574
INFO:root:current mean train loss 1669.3302740925446
INFO:root:current train perplexity3.733773946762085
INFO:root:current mean train loss 1670.768425121075
INFO:root:current train perplexity3.735354423522949

100%|██████████| 1/1 [07:46<00:00, 466.50s/it][A100%|██████████| 1/1 [07:46<00:00, 466.50s/it]
INFO:root:final mean train loss: 1669.983654520455
INFO:root:final train perplexity: 3.735745429992676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.92s/it][A100%|██████████| 1/1 [00:41<00:00, 41.92s/it]
INFO:root:eval mean loss: 1796.9257721596575
INFO:root:eval perplexity: 4.280623912811279
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.52s/it][A100%|██████████| 1/1 [00:40<00:00, 40.52s/it]
INFO:root:eval mean loss: 2246.419249622534
INFO:root:eval perplexity: 6.339788913726807
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/71
 36%|███▌      | 71/200 [10:31:01<19:18:13, 538.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1614.7217407226562
INFO:root:current train perplexity3.5185937881469727
INFO:root:current mean train loss 1671.0704000221108
INFO:root:current train perplexity3.7067983150482178
INFO:root:current mean train loss 1662.7347915797557
INFO:root:current train perplexity3.7040252685546875
INFO:root:current mean train loss 1658.0702829797283
INFO:root:current train perplexity3.7019424438476562
INFO:root:current mean train loss 1653.6831511699506
INFO:root:current train perplexity3.6967642307281494
INFO:root:current mean train loss 1655.13018750579
INFO:root:current train perplexity3.6968986988067627
INFO:root:current mean train loss 1653.0189946240719
INFO:root:current train perplexity3.6922905445098877
INFO:root:current mean train loss 1653.6195842969857
INFO:root:current train perplexity3.6892058849334717
INFO:root:current mean train loss 1653.741909585579
INFO:root:current train perplexity3.6894078254699707
INFO:root:current mean train loss 1654.824006137469
INFO:root:current train perplexity3.691868305206299
INFO:root:current mean train loss 1655.607092430767
INFO:root:current train perplexity3.6952593326568604
INFO:root:current mean train loss 1655.3006976991621
INFO:root:current train perplexity3.6941051483154297
INFO:root:current mean train loss 1655.3288032696219
INFO:root:current train perplexity3.6932969093322754
INFO:root:current mean train loss 1656.1195219778906
INFO:root:current train perplexity3.7003588676452637
INFO:root:current mean train loss 1656.8046310663563
INFO:root:current train perplexity3.6983704566955566
INFO:root:current mean train loss 1658.3748995716353
INFO:root:current train perplexity3.70017671585083
INFO:root:current mean train loss 1658.085488895402
INFO:root:current train perplexity3.699491500854492
INFO:root:current mean train loss 1658.8424586735464
INFO:root:current train perplexity3.7019283771514893
INFO:root:current mean train loss 1659.6108598508445
INFO:root:current train perplexity3.705899477005005
INFO:root:current mean train loss 1661.2198691273036
INFO:root:current train perplexity3.707627773284912

100%|██████████| 1/1 [07:41<00:00, 461.98s/it][A100%|██████████| 1/1 [07:41<00:00, 461.98s/it]
INFO:root:final mean train loss: 1660.6809581269895
INFO:root:final train perplexity: 3.7084197998046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:47<00:00, 47.39s/it][A100%|██████████| 1/1 [00:47<00:00, 47.39s/it]
INFO:root:eval mean loss: 1792.6975071683844
INFO:root:eval perplexity: 4.266002178192139
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.30s/it][A100%|██████████| 1/1 [00:38<00:00, 38.30s/it]
INFO:root:eval mean loss: 2242.6945766151375
INFO:root:eval perplexity: 6.320406436920166
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/72
 36%|███▌      | 72/200 [10:40:12<19:16:37, 542.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1636.5620011039402
INFO:root:current train perplexity3.6353604793548584
INFO:root:current mean train loss 1631.260345210874
INFO:root:current train perplexity3.6606481075286865
INFO:root:current mean train loss 1640.8362467812851
INFO:root:current train perplexity3.680687665939331
INFO:root:current mean train loss 1641.4282188769835
INFO:root:current train perplexity3.674391031265259
INFO:root:current mean train loss 1643.5966793989178
INFO:root:current train perplexity3.6815237998962402
INFO:root:current mean train loss 1642.6150411164556
INFO:root:current train perplexity3.6772778034210205
INFO:root:current mean train loss 1645.6931471725145
INFO:root:current train perplexity3.685516595840454
INFO:root:current mean train loss 1648.160422339644
INFO:root:current train perplexity3.687666893005371
INFO:root:current mean train loss 1649.8450160901427
INFO:root:current train perplexity3.6889846324920654
INFO:root:current mean train loss 1653.4187515605956
INFO:root:current train perplexity3.694199562072754
INFO:root:current mean train loss 1653.521774575391
INFO:root:current train perplexity3.6940665245056152
INFO:root:current mean train loss 1654.7584253516668
INFO:root:current train perplexity3.6943068504333496
INFO:root:current mean train loss 1654.288571623633
INFO:root:current train perplexity3.693028211593628
INFO:root:current mean train loss 1655.6414956390543
INFO:root:current train perplexity3.6943933963775635
INFO:root:current mean train loss 1656.0942766265975
INFO:root:current train perplexity3.6959385871887207
INFO:root:current mean train loss 1655.9935938333572
INFO:root:current train perplexity3.6978137493133545
INFO:root:current mean train loss 1655.9772955987899
INFO:root:current train perplexity3.6969525814056396
INFO:root:current mean train loss 1656.2769035742301
INFO:root:current train perplexity3.6973049640655518
INFO:root:current mean train loss 1658.131645432816
INFO:root:current train perplexity3.7009942531585693
INFO:root:current mean train loss 1658.0541927438817
INFO:root:current train perplexity3.7005975246429443

100%|██████████| 1/1 [07:45<00:00, 465.63s/it][A100%|██████████| 1/1 [07:45<00:00, 465.68s/it]
INFO:root:final mean train loss: 1657.8863056931182
INFO:root:final train perplexity: 3.700249671936035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.84s/it][A100%|██████████| 1/1 [00:41<00:00, 41.84s/it]
INFO:root:eval mean loss: 1797.9326145902594
INFO:root:eval perplexity: 4.28411340713501
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.25s/it][A100%|██████████| 1/1 [00:39<00:00, 39.25s/it]
INFO:root:eval mean loss: 2248.705468143977
INFO:root:eval perplexity: 6.351716995239258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/73
 36%|███▋      | 73/200 [10:49:21<19:12:03, 544.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1644.564144897461
INFO:root:current train perplexity3.6763811111450195
INFO:root:current mean train loss 1641.958047921317
INFO:root:current train perplexity3.666123628616333
INFO:root:current mean train loss 1647.7546290079752
INFO:root:current train perplexity3.6598386764526367
INFO:root:current mean train loss 1652.9226404526655
INFO:root:current train perplexity3.6671335697174072
INFO:root:current mean train loss 1652.5978085604581
INFO:root:current train perplexity3.6723341941833496
INFO:root:current mean train loss 1651.1936482747396
INFO:root:current train perplexity3.6741135120391846
INFO:root:current mean train loss 1652.829448890686
INFO:root:current train perplexity3.681551456451416
INFO:root:current mean train loss 1654.1967519399282
INFO:root:current train perplexity3.686734676361084
INFO:root:current mean train loss 1654.579425920759
INFO:root:current train perplexity3.690437078475952
INFO:root:current mean train loss 1655.9178887549867
INFO:root:current train perplexity3.695608615875244
INFO:root:current mean train loss 1654.7347467275765
INFO:root:current train perplexity3.692147970199585
INFO:root:current mean train loss 1655.266670628598
INFO:root:current train perplexity3.695026159286499
INFO:root:current mean train loss 1655.87592527328
INFO:root:current train perplexity3.6958060264587402
INFO:root:current mean train loss 1656.0493482902868
INFO:root:current train perplexity3.6964783668518066
INFO:root:current mean train loss 1656.6713673061795
INFO:root:current train perplexity3.6983559131622314
INFO:root:current mean train loss 1656.3975554230926
INFO:root:current train perplexity3.6982500553131104
INFO:root:current mean train loss 1657.065873681045
INFO:root:current train perplexity3.699969530105591
INFO:root:current mean train loss 1658.1410613662895
INFO:root:current train perplexity3.699784755706787
INFO:root:current mean train loss 1659.8545425415039
INFO:root:current train perplexity3.703713893890381
INFO:root:current mean train loss 1659.9227438385954
INFO:root:current train perplexity3.7042043209075928

100%|██████████| 1/1 [07:48<00:00, 468.51s/it][A100%|██████████| 1/1 [07:48<00:00, 468.51s/it]
INFO:root:final mean train loss: 1658.9664586423564
INFO:root:final train perplexity: 3.7034049034118652
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.07s/it][A100%|██████████| 1/1 [00:41<00:00, 41.07s/it]
INFO:root:eval mean loss: 1793.2148714539007
INFO:root:eval perplexity: 4.267788887023926
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.64s/it][A100%|██████████| 1/1 [00:38<00:00, 38.64s/it]
INFO:root:eval mean loss: 2244.9088498379324
INFO:root:eval perplexity: 6.3319220542907715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/74
 37%|███▋      | 74/200 [10:58:31<19:06:58, 546.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1655.7034119723137
INFO:root:current train perplexity3.790264129638672
INFO:root:current mean train loss 1661.1447683929637
INFO:root:current train perplexity3.7338128089904785
INFO:root:current mean train loss 1657.1078693078186
INFO:root:current train perplexity3.7090096473693848
INFO:root:current mean train loss 1657.9656743068322
INFO:root:current train perplexity3.7091245651245117
INFO:root:current mean train loss 1658.0398502141172
INFO:root:current train perplexity3.701526403427124
INFO:root:current mean train loss 1657.2662561714542
INFO:root:current train perplexity3.696856737136841
INFO:root:current mean train loss 1655.1497131997955
INFO:root:current train perplexity3.6873385906219482
INFO:root:current mean train loss 1656.383744717274
INFO:root:current train perplexity3.6897165775299072
INFO:root:current mean train loss 1656.2181840894382
INFO:root:current train perplexity3.6898977756500244
INFO:root:current mean train loss 1656.0414226535968
INFO:root:current train perplexity3.690983533859253
INFO:root:current mean train loss 1656.908068582035
INFO:root:current train perplexity3.692671775817871
INFO:root:current mean train loss 1656.2583009922619
INFO:root:current train perplexity3.690540313720703
INFO:root:current mean train loss 1657.5041225193604
INFO:root:current train perplexity3.692938804626465
INFO:root:current mean train loss 1657.8014552904212
INFO:root:current train perplexity3.694506883621216
INFO:root:current mean train loss 1660.2160731664487
INFO:root:current train perplexity3.700550079345703
INFO:root:current mean train loss 1664.7049409233
INFO:root:current train perplexity3.714082717895508
INFO:root:current mean train loss 1704.4413132791283
INFO:root:current train perplexity3.834625720977783
INFO:root:current mean train loss 1770.5915573893044
INFO:root:current train perplexity4.040493488311768
INFO:root:current mean train loss 1816.411605908913
INFO:root:current train perplexity4.189192295074463
INFO:root:current mean train loss 1864.3086538807006
INFO:root:current train perplexity4.3524298667907715

100%|██████████| 1/1 [07:46<00:00, 466.23s/it][A100%|██████████| 1/1 [07:46<00:00, 466.23s/it]
INFO:root:final mean train loss: 1872.8304405193164
INFO:root:final train perplexity: 4.3843159675598145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.79s/it][A100%|██████████| 1/1 [00:41<00:00, 41.79s/it]
INFO:root:eval mean loss: 2372.2256547643783
INFO:root:eval perplexity: 6.818502426147461
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.57s/it][A100%|██████████| 1/1 [00:38<00:00, 38.57s/it]
INFO:root:eval mean loss: 2770.676645698277
INFO:root:eval perplexity: 9.75571060180664
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/75
 38%|███▊      | 75/200 [11:07:41<18:59:48, 547.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2769.921622611381
INFO:root:current train perplexity8.903651237487793
INFO:root:current mean train loss 2993.499805669675
INFO:root:current train perplexity10.522307395935059
INFO:root:current mean train loss 3130.7338889463103
INFO:root:current train perplexity11.7393159866333
INFO:root:current mean train loss 3255.4305364435368
INFO:root:current train perplexity13.004096984863281
INFO:root:current mean train loss 3268.2294064292423
INFO:root:current train perplexity13.125645637512207
INFO:root:current mean train loss 3211.979488146845
INFO:root:current train perplexity12.600634574890137
INFO:root:current mean train loss 3165.3125077878685
INFO:root:current train perplexity12.120405197143555
INFO:root:current mean train loss 3088.8281888740007
INFO:root:current train perplexity11.391313552856445
INFO:root:current mean train loss 3028.102910022168
INFO:root:current train perplexity10.861294746398926
INFO:root:current mean train loss 2974.323371276229
INFO:root:current train perplexity10.438814163208008
INFO:root:current mean train loss 2938.0772200429906
INFO:root:current train perplexity10.156113624572754
INFO:root:current mean train loss 2919.5032371508
INFO:root:current train perplexity10.018810272216797
INFO:root:current mean train loss 3091.462140093793
INFO:root:current train perplexity11.482389450073242
INFO:root:current mean train loss 3197.1289631983773
INFO:root:current train perplexity12.471550941467285
INFO:root:current mean train loss 3250.805766339865
INFO:root:current train perplexity13.00686264038086
INFO:root:current mean train loss 3245.2800000589414
INFO:root:current train perplexity12.946081161499023
INFO:root:current mean train loss 3439.1963223875496
INFO:root:current train perplexity15.094618797302246
INFO:root:current mean train loss 3708.24803620933
INFO:root:current train perplexity18.663192749023438
INFO:root:current mean train loss 3960.257194527185
INFO:root:current train perplexity22.758543014526367
INFO:root:current mean train loss 4173.513574973186
INFO:root:current train perplexity26.910722732543945

100%|██████████| 1/1 [07:53<00:00, 473.53s/it][A100%|██████████| 1/1 [07:53<00:00, 473.53s/it]
INFO:root:final mean train loss: 4188.177276442043
INFO:root:final train perplexity: 27.256723403930664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.12s/it][A100%|██████████| 1/1 [00:42<00:00, 42.12s/it]
INFO:root:eval mean loss: 7721.005649864251
INFO:root:eval perplexity: 516.9525146484375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.15s/it][A100%|██████████| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 7778.158596174091
INFO:root:eval perplexity: 598.6320190429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/76
 38%|███▊      | 76/200 [11:16:58<18:57:04, 550.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 8041.791541466346
INFO:root:current train perplexity582.4005126953125
INFO:root:current mean train loss 8251.037365019634
INFO:root:current train perplexity668.946044921875
INFO:root:current mean train loss 8003.688869201031
INFO:root:current train perplexity553.05810546875
INFO:root:current mean train loss 7937.922816596068
INFO:root:current train perplexity532.9022216796875
INFO:root:current mean train loss 7859.872624228296
INFO:root:current train perplexity500.9823303222656
INFO:root:current mean train loss 7818.765513463673
INFO:root:current train perplexity481.82769775390625
INFO:root:current mean train loss 7782.503253323987
INFO:root:current train perplexity469.1078186035156
INFO:root:current mean train loss 7755.7693491476375
INFO:root:current train perplexity459.6818542480469
INFO:root:current mean train loss 7739.887033310536
INFO:root:current train perplexity452.41168212890625
INFO:root:current mean train loss 7691.702579071014
INFO:root:current train perplexity437.20697021484375
INFO:root:current mean train loss 7681.318043401982
INFO:root:current train perplexity432.53326416015625
INFO:root:current mean train loss 7677.963111601989
INFO:root:current train perplexity429.8683166503906
INFO:root:current mean train loss 7680.7635958529
INFO:root:current train perplexity428.5762634277344
INFO:root:current mean train loss 7677.965375558838
INFO:root:current train perplexity426.64837646484375
INFO:root:current mean train loss 7671.965787891411
INFO:root:current train perplexity425.7013854980469
INFO:root:current mean train loss 7671.740602350625
INFO:root:current train perplexity425.6703186035156
INFO:root:current mean train loss 7672.680230355559
INFO:root:current train perplexity425.66485595703125
INFO:root:current mean train loss 7672.360968797983
INFO:root:current train perplexity425.9090881347656
INFO:root:current mean train loss 7674.563687522723
INFO:root:current train perplexity425.99786376953125

100%|██████████| 1/1 [07:47<00:00, 467.31s/it][A100%|██████████| 1/1 [07:47<00:00, 467.31s/it]
INFO:root:final mean train loss: 7669.697353530399
INFO:root:final train perplexity: 425.34832763671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.53s/it][A100%|██████████| 1/1 [00:41<00:00, 41.53s/it]
INFO:root:eval mean loss: 7171.732927471188
INFO:root:eval perplexity: 331.4482116699219
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.12s/it][A100%|██████████| 1/1 [00:39<00:00, 39.12s/it]
INFO:root:eval mean loss: 7247.610718639185
INFO:root:eval perplexity: 387.0169372558594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/77
 38%|███▊      | 77/200 [11:26:09<18:48:11, 550.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7626.304992675781
INFO:root:current train perplexity425.9939880371094
INFO:root:current mean train loss 7654.126817491319
INFO:root:current train perplexity417.1969299316406
INFO:root:current mean train loss 7640.804595947266
INFO:root:current train perplexity417.83758544921875
INFO:root:current mean train loss 7653.542746803977
INFO:root:current train perplexity417.3172912597656
INFO:root:current mean train loss 7643.852865780101
INFO:root:current train perplexity415.5016784667969
INFO:root:current mean train loss 7640.128529466043
INFO:root:current train perplexity416.9636535644531
INFO:root:current mean train loss 7649.7810114810345
INFO:root:current train perplexity416.78692626953125
INFO:root:current mean train loss 7645.561241365422
INFO:root:current train perplexity417.0296630859375
INFO:root:current mean train loss 7633.458088185527
INFO:root:current train perplexity416.3306884765625
INFO:root:current mean train loss 7632.050052054653
INFO:root:current train perplexity414.64666748046875
INFO:root:current mean train loss 7635.808201865544
INFO:root:current train perplexity413.51666259765625
INFO:root:current mean train loss 7634.279014835289
INFO:root:current train perplexity412.1630554199219
INFO:root:current mean train loss 7632.7926938896935
INFO:root:current train perplexity410.940673828125
INFO:root:current mean train loss 7626.186728007932
INFO:root:current train perplexity409.8033752441406
INFO:root:current mean train loss 7622.771084525369
INFO:root:current train perplexity408.8122863769531
INFO:root:current mean train loss 7620.046795346692
INFO:root:current train perplexity408.34429931640625
INFO:root:current mean train loss 7620.516811997143
INFO:root:current train perplexity408.1690979003906
INFO:root:current mean train loss 7617.49890079543
INFO:root:current train perplexity407.92706298828125
INFO:root:current mean train loss 7618.073656470375
INFO:root:current train perplexity407.9237365722656
INFO:root:current mean train loss 7616.821835435911
INFO:root:current train perplexity407.12237548828125

100%|██████████| 1/1 [07:38<00:00, 458.23s/it][A100%|██████████| 1/1 [07:38<00:00, 458.23s/it]
INFO:root:final mean train loss: 7612.180813526302
INFO:root:final train perplexity: 406.4725341796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.23s/it][A100%|██████████| 1/1 [00:41<00:00, 41.23s/it]
INFO:root:eval mean loss: 7121.059878518396
INFO:root:eval perplexity: 318.1321716308594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.16s/it][A100%|██████████| 1/1 [00:39<00:00, 39.16s/it]
INFO:root:eval mean loss: 7191.049666167996
INFO:root:eval perplexity: 369.43255615234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/78
 39%|███▉      | 78/200 [11:35:10<18:33:27, 547.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7405.8446484375
INFO:root:current train perplexity389.45928955078125
INFO:root:current mean train loss 7565.60683203125
INFO:root:current train perplexity393.36865234375
INFO:root:current mean train loss 7550.142580295139
INFO:root:current train perplexity394.7525329589844
INFO:root:current mean train loss 7548.249232271634
INFO:root:current train perplexity394.4501647949219
INFO:root:current mean train loss 7565.152711397059
INFO:root:current train perplexity396.67529296875
INFO:root:current mean train loss 7562.956251860119
INFO:root:current train perplexity396.9935607910156
INFO:root:current mean train loss 7569.2210234375
INFO:root:current train perplexity396.64599609375
INFO:root:current mean train loss 7568.345871497845
INFO:root:current train perplexity396.0778503417969
INFO:root:current mean train loss 7571.573740530303
INFO:root:current train perplexity395.7421875
INFO:root:current mean train loss 7565.680425464527
INFO:root:current train perplexity394.1976318359375
INFO:root:current mean train loss 7558.830555926067
INFO:root:current train perplexity392.69696044921875
INFO:root:current mean train loss 7556.929582465278
INFO:root:current train perplexity391.2258605957031
INFO:root:current mean train loss 7552.4484598214285
INFO:root:current train perplexity388.67620849609375
INFO:root:current mean train loss 7546.26881485849
INFO:root:current train perplexity386.12127685546875
INFO:root:current mean train loss 7536.060736705044
INFO:root:current train perplexity382.4062805175781
INFO:root:current mean train loss 7524.537222400102
INFO:root:current train perplexity378.15374755859375
INFO:root:current mean train loss 7501.544911658654
INFO:root:current train perplexity372.0995178222656
INFO:root:current mean train loss 7465.865538383152
INFO:root:current train perplexity362.0843505859375
INFO:root:current mean train loss 7429.130272902397
INFO:root:current train perplexity351.2173767089844
INFO:root:current mean train loss 7389.723528053977
INFO:root:current train perplexity340.39453125

100%|██████████| 1/1 [07:50<00:00, 470.87s/it][A100%|██████████| 1/1 [07:50<00:00, 470.87s/it]
INFO:root:final mean train loss: 7365.201095911931
INFO:root:final train perplexity: 334.48785400390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.34s/it][A100%|██████████| 1/1 [00:43<00:00, 43.34s/it]
INFO:root:eval mean loss: 6333.3869663536125
INFO:root:eval perplexity: 168.18592834472656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.80s/it][A100%|██████████| 1/1 [00:38<00:00, 38.80s/it]
INFO:root:eval mean loss: 6472.016591173538
INFO:root:eval perplexity: 204.5536651611328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/79
 40%|███▉      | 79/200 [11:44:26<18:29:07, 549.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6604.748116629465
INFO:root:current train perplexity183.44650268554688
INFO:root:current mean train loss 6617.675055705326
INFO:root:current train perplexity190.1328582763672
INFO:root:current mean train loss 7040.835049715909
INFO:root:current train perplexity263.7692565917969
INFO:root:current mean train loss 7231.165191828856
INFO:root:current train perplexity305.56695556640625
INFO:root:current mean train loss 7276.820836131928
INFO:root:current train perplexity314.5979919433594
INFO:root:current mean train loss 7269.093337393335
INFO:root:current train perplexity312.4571228027344
INFO:root:current mean train loss 7234.3788765880545
INFO:root:current train perplexity304.082763671875
INFO:root:current mean train loss 7210.355266725278
INFO:root:current train perplexity296.8551940917969
INFO:root:current mean train loss 7179.523972173768
INFO:root:current train perplexity289.9969482421875
INFO:root:current mean train loss 7157.547304189889
INFO:root:current train perplexity284.73199462890625
INFO:root:current mean train loss 7144.8132338276755
INFO:root:current train perplexity280.579345703125
INFO:root:current mean train loss 7123.564902925241
INFO:root:current train perplexity276.5865478515625
INFO:root:current mean train loss 7219.89453832654
INFO:root:current train perplexity298.8097839355469
INFO:root:current mean train loss 7300.879116189107
INFO:root:current train perplexity319.73126220703125
INFO:root:current mean train loss 7358.815131707264
INFO:root:current train perplexity334.54290771484375
INFO:root:current mean train loss 7393.541835443519
INFO:root:current train perplexity342.6751708984375
INFO:root:current mean train loss 7405.2973079704625
INFO:root:current train perplexity345.0618896484375
INFO:root:current mean train loss 7396.195678851087
INFO:root:current train perplexity342.557373046875
INFO:root:current mean train loss 7379.295874845192
INFO:root:current train perplexity338.580322265625
INFO:root:current mean train loss 7363.626543039151
INFO:root:current train perplexity334.1643981933594

100%|██████████| 1/1 [07:48<00:00, 468.77s/it][A100%|██████████| 1/1 [07:48<00:00, 468.77s/it]
INFO:root:final mean train loss: 7356.495297430503
INFO:root:final train perplexity: 332.19757080078125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.36s/it][A100%|██████████| 1/1 [00:41<00:00, 41.36s/it]
INFO:root:eval mean loss: 6599.515320257092
INFO:root:eval perplexity: 208.60134887695312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.13s/it][A100%|██████████| 1/1 [00:39<00:00, 39.13s/it]
INFO:root:eval mean loss: 6704.9006832474515
INFO:root:eval perplexity: 247.71798706054688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/80
 40%|████      | 80/200 [11:53:37<18:20:58, 550.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6930.1358332229875
INFO:root:current train perplexity242.34426879882812
INFO:root:current mean train loss 6989.782340187697
INFO:root:current train perplexity244.9268035888672
INFO:root:current mean train loss 7001.729405465733
INFO:root:current train perplexity243.8643341064453
INFO:root:current mean train loss 7000.238447184018
INFO:root:current train perplexity244.59661865234375
INFO:root:current mean train loss 7001.4709063095725
INFO:root:current train perplexity244.9791259765625
INFO:root:current mean train loss 6996.304297923189
INFO:root:current train perplexity244.8943328857422
INFO:root:current mean train loss 6998.775272815108
INFO:root:current train perplexity244.98509216308594
INFO:root:current mean train loss 6997.909296128747
INFO:root:current train perplexity245.22012329101562
INFO:root:current mean train loss 6991.117193184298
INFO:root:current train perplexity245.04408264160156
INFO:root:current mean train loss 6985.88327023185
INFO:root:current train perplexity244.8953857421875
INFO:root:current mean train loss 6975.829260634296
INFO:root:current train perplexity244.50881958007812
INFO:root:current mean train loss 6976.566332523323
INFO:root:current train perplexity244.59661865234375
INFO:root:current mean train loss 6977.613838953286
INFO:root:current train perplexity244.77723693847656
INFO:root:current mean train loss 6977.105305989583
INFO:root:current train perplexity244.7554168701172
INFO:root:current mean train loss 6971.424953949623
INFO:root:current train perplexity244.72320556640625
INFO:root:current mean train loss 6972.60345987512
INFO:root:current train perplexity244.71994018554688
INFO:root:current mean train loss 6968.930986640107
INFO:root:current train perplexity244.5734100341797
INFO:root:current mean train loss 6968.076701517197
INFO:root:current train perplexity244.6752471923828
INFO:root:current mean train loss 6965.7065232693985
INFO:root:current train perplexity244.40565490722656
INFO:root:current mean train loss 6969.673742133662
INFO:root:current train perplexity244.40110778808594

100%|██████████| 1/1 [07:52<00:00, 472.29s/it][A100%|██████████| 1/1 [07:52<00:00, 472.29s/it]
INFO:root:final mean train loss: 6967.383356306929
INFO:root:final train perplexity: 244.36021423339844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.05s/it][A100%|██████████| 1/1 [00:41<00:00, 41.05s/it]
INFO:root:eval mean loss: 6623.903730676529
INFO:root:eval perplexity: 212.7589569091797
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.40s/it][A100%|██████████| 1/1 [00:38<00:00, 38.40s/it]
INFO:root:eval mean loss: 6721.189518921764
INFO:root:eval perplexity: 251.0577392578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/81
 40%|████      | 81/200 [12:02:51<18:14:03, 551.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6909.636442485608
INFO:root:current train perplexity241.50146484375
INFO:root:current mean train loss 6934.26718417081
INFO:root:current train perplexity241.801513671875
INFO:root:current mean train loss 6960.3795661373415
INFO:root:current train perplexity241.49766540527344
INFO:root:current mean train loss 6946.3789984520445
INFO:root:current train perplexity240.61190795898438
INFO:root:current mean train loss 6942.256419462316
INFO:root:current train perplexity240.9519805908203
INFO:root:current mean train loss 6956.8187917073565
INFO:root:current train perplexity241.65455627441406
INFO:root:current mean train loss 6954.615190314118
INFO:root:current train perplexity242.00025939941406
INFO:root:current mean train loss 6948.44966738986
INFO:root:current train perplexity241.87417602539062
INFO:root:current mean train loss 6953.595645155537
INFO:root:current train perplexity241.98480224609375
INFO:root:current mean train loss 6955.976297347272
INFO:root:current train perplexity242.04330444335938
INFO:root:current mean train loss 6955.897081566566
INFO:root:current train perplexity242.078857421875
INFO:root:current mean train loss 6960.472265126754
INFO:root:current train perplexity243.0264434814453
INFO:root:current mean train loss 6965.476222310321
INFO:root:current train perplexity243.90293884277344
INFO:root:current mean train loss 6964.609089341275
INFO:root:current train perplexity244.0747833251953
INFO:root:current mean train loss 6962.359527174373
INFO:root:current train perplexity243.8771209716797
INFO:root:current mean train loss 6965.037193337068
INFO:root:current train perplexity244.08059692382812
INFO:root:current mean train loss 6966.96143219067
INFO:root:current train perplexity244.39505004882812
INFO:root:current mean train loss 6968.1683473329285
INFO:root:current train perplexity244.5939483642578
INFO:root:current mean train loss 6966.689692060068
INFO:root:current train perplexity244.50660705566406
INFO:root:current mean train loss 6971.3137740779985
INFO:root:current train perplexity244.6833038330078

100%|██████████| 1/1 [07:45<00:00, 465.64s/it][A100%|██████████| 1/1 [07:45<00:00, 465.64s/it]
INFO:root:final mean train loss: 6969.239537533882
INFO:root:final train perplexity: 244.71853637695312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.05s/it][A100%|██████████| 1/1 [00:41<00:00, 41.05s/it]
INFO:root:eval mean loss: 6543.1091343223625
INFO:root:eval perplexity: 199.2938232421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.74s/it][A100%|██████████| 1/1 [00:38<00:00, 38.74s/it]
INFO:root:eval mean loss: 6643.961828353557
INFO:root:eval perplexity: 235.61334228515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/82
 41%|████      | 82/200 [12:11:59<18:02:41, 550.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6962.366525957661
INFO:root:current train perplexity246.41818237304688
INFO:root:current mean train loss 6985.17221654388
INFO:root:current train perplexity247.8324737548828
INFO:root:current mean train loss 6988.561271797675
INFO:root:current train perplexity247.29417419433594
INFO:root:current mean train loss 6989.802917014552
INFO:root:current train perplexity247.0848388671875
INFO:root:current mean train loss 6976.919175091912
INFO:root:current train perplexity246.12142944335938
INFO:root:current mean train loss 6972.803841036046
INFO:root:current train perplexity245.32327270507812
INFO:root:current mean train loss 6967.0789303469965
INFO:root:current train perplexity244.36708068847656
INFO:root:current mean train loss 6968.2259344459335
INFO:root:current train perplexity243.62677001953125
INFO:root:current mean train loss 6970.847925816244
INFO:root:current train perplexity243.3524169921875
INFO:root:current mean train loss 6962.560413126259
INFO:root:current train perplexity242.9200897216797
INFO:root:current mean train loss 6952.669434040485
INFO:root:current train perplexity242.56578063964844
INFO:root:current mean train loss 6950.534820633382
INFO:root:current train perplexity242.194091796875
INFO:root:current mean train loss 6956.689445572312
INFO:root:current train perplexity242.264892578125
INFO:root:current mean train loss 6957.592524564789
INFO:root:current train perplexity242.23486328125
INFO:root:current mean train loss 6957.076802094671
INFO:root:current train perplexity242.43869018554688
INFO:root:current mean train loss 6955.486484448564
INFO:root:current train perplexity242.47372436523438
INFO:root:current mean train loss 6953.191550744334
INFO:root:current train perplexity242.37893676757812
INFO:root:current mean train loss 6955.876239357484
INFO:root:current train perplexity242.32867431640625
INFO:root:current mean train loss 6958.978388202423
INFO:root:current train perplexity242.30520629882812

100%|██████████| 1/1 [07:52<00:00, 472.52s/it][A100%|██████████| 1/1 [07:52<00:00, 472.52s/it]
INFO:root:final mean train loss: 6956.065992577534
INFO:root:final train perplexity: 242.18739318847656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.17s/it][A100%|██████████| 1/1 [00:41<00:00, 41.17s/it]
INFO:root:eval mean loss: 6535.3001336713205
INFO:root:eval perplexity: 198.0384521484375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.10s/it][A100%|██████████| 1/1 [00:39<00:00, 39.10s/it]
INFO:root:eval mean loss: 6639.9972962724405
INFO:root:eval perplexity: 234.84657287597656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/83
 42%|████▏     | 83/200 [12:21:15<17:56:26, 552.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6960.163330078125
INFO:root:current train perplexity247.56944274902344
INFO:root:current mean train loss 6898.996764026989
INFO:root:current train perplexity237.72898864746094
INFO:root:current mean train loss 6907.746868024554
INFO:root:current train perplexity238.5008544921875
INFO:root:current mean train loss 6908.6833338583665
INFO:root:current train perplexity237.7462158203125
INFO:root:current mean train loss 6907.588614710366
INFO:root:current train perplexity237.20301818847656
INFO:root:current mean train loss 6921.985646446078
INFO:root:current train perplexity237.837158203125
INFO:root:current mean train loss 6924.521903816599
INFO:root:current train perplexity237.77410888671875
INFO:root:current mean train loss 6929.007273327465
INFO:root:current train perplexity238.68856811523438
INFO:root:current mean train loss 6922.871283637152
INFO:root:current train perplexity238.4167022705078
INFO:root:current mean train loss 6921.2763290908315
INFO:root:current train perplexity236.5872802734375
INFO:root:current mean train loss 6930.28497447401
INFO:root:current train perplexity236.5865936279297
INFO:root:current mean train loss 6934.1012748099665
INFO:root:current train perplexity236.48609924316406
INFO:root:current mean train loss 6931.400312742123
INFO:root:current train perplexity236.1237335205078
INFO:root:current mean train loss 6926.374752132037
INFO:root:current train perplexity235.7533721923828
INFO:root:current mean train loss 6929.607702376995
INFO:root:current train perplexity235.7831573486328
INFO:root:current mean train loss 6924.9866446994
INFO:root:current train perplexity235.50942993164062
INFO:root:current mean train loss 6922.395070179056
INFO:root:current train perplexity234.9982452392578
INFO:root:current mean train loss 6919.4921283922695
INFO:root:current train perplexity234.38441467285156
INFO:root:current mean train loss 6914.015624460462
INFO:root:current train perplexity233.4098663330078
INFO:root:current mean train loss 6906.198534389316
INFO:root:current train perplexity232.21414184570312

100%|██████████| 1/1 [07:48<00:00, 468.70s/it][A100%|██████████| 1/1 [07:48<00:00, 468.70s/it]
INFO:root:final mean train loss: 6898.1872077207045
INFO:root:final train perplexity: 231.37368774414062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.68s/it][A100%|██████████| 1/1 [00:38<00:00, 38.68s/it]
INFO:root:eval mean loss: 6430.925126745346
INFO:root:eval perplexity: 181.9987030029297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.59s/it][A100%|██████████| 1/1 [00:37<00:00, 37.59s/it]
INFO:root:eval mean loss: 6535.201358010583
INFO:root:eval perplexity: 215.4601593017578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/84
 42%|████▏     | 84/200 [12:30:22<17:44:31, 550.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6770.169578269676
INFO:root:current train perplexity204.76036071777344
INFO:root:current mean train loss 6784.529731329971
INFO:root:current train perplexity202.93093872070312
INFO:root:current mean train loss 6754.2280187396755
INFO:root:current train perplexity203.10780334472656
INFO:root:current mean train loss 6750.948055535646
INFO:root:current train perplexity202.666748046875
INFO:root:current mean train loss 6757.866602019906
INFO:root:current train perplexity202.8715362548828
INFO:root:current mean train loss 6759.674903826198
INFO:root:current train perplexity203.75909423828125
INFO:root:current mean train loss 6752.362951056619
INFO:root:current train perplexity203.38072204589844
INFO:root:current mean train loss 6742.939041410548
INFO:root:current train perplexity203.16708374023438
INFO:root:current mean train loss 6738.787026715538
INFO:root:current train perplexity202.7786865234375
INFO:root:current mean train loss 6740.533480186421
INFO:root:current train perplexity202.8040313720703
INFO:root:current mean train loss 6731.576908813596
INFO:root:current train perplexity202.3701934814453
INFO:root:current mean train loss 6733.759696737051
INFO:root:current train perplexity202.35494995117188
INFO:root:current mean train loss 6730.886706015689
INFO:root:current train perplexity202.16253662109375
INFO:root:current mean train loss 6728.793709450947
INFO:root:current train perplexity202.22366333007812
INFO:root:current mean train loss 6724.013122002562
INFO:root:current train perplexity201.69717407226562
INFO:root:current mean train loss 6723.838871024681
INFO:root:current train perplexity201.552490234375
INFO:root:current mean train loss 6722.5434933447295
INFO:root:current train perplexity201.02122497558594
INFO:root:current mean train loss 6721.790094761056
INFO:root:current train perplexity200.67750549316406
INFO:root:current mean train loss 6716.8102005080045
INFO:root:current train perplexity200.0869140625
INFO:root:current mean train loss 6712.60740667164
INFO:root:current train perplexity199.70420837402344

100%|██████████| 1/1 [07:34<00:00, 455.00s/it][A100%|██████████| 1/1 [07:34<00:00, 455.00s/it]
INFO:root:final mean train loss: 6711.41264776479
INFO:root:final train perplexity: 199.66326904296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.83s/it][A100%|██████████| 1/1 [00:39<00:00, 39.83s/it]
INFO:root:eval mean loss: 6441.210099457004
INFO:root:eval perplexity: 183.51974487304688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.29s/it][A100%|██████████| 1/1 [00:37<00:00, 37.29s/it]
INFO:root:eval mean loss: 6555.716693851119
INFO:root:eval perplexity: 219.12518310546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/85
 42%|████▎     | 85/200 [12:39:17<17:26:04, 545.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6768.9395308061075
INFO:root:current train perplexity209.5349578857422
INFO:root:current mean train loss 6802.9732496473525
INFO:root:current train perplexity217.52333068847656
INFO:root:current mean train loss 6850.031476130251
INFO:root:current train perplexity224.6798858642578
INFO:root:current mean train loss 6898.996485510538
INFO:root:current train perplexity232.34085083007812
INFO:root:current mean train loss 6919.605727187148
INFO:root:current train perplexity235.8890838623047
INFO:root:current mean train loss 6962.0942750818585
INFO:root:current train perplexity241.46795654296875
INFO:root:current mean train loss 6964.874708850932
INFO:root:current train perplexity243.63629150390625
INFO:root:current mean train loss 6964.7419715799315
INFO:root:current train perplexity243.91748046875
INFO:root:current mean train loss 6966.978808940869
INFO:root:current train perplexity244.44854736328125
INFO:root:current mean train loss 6966.177512993247
INFO:root:current train perplexity244.79019165039062
INFO:root:current mean train loss 6973.440261782357
INFO:root:current train perplexity245.168212890625
INFO:root:current mean train loss 6975.00214732777
INFO:root:current train perplexity245.1939239501953
INFO:root:current mean train loss 6975.219209235581
INFO:root:current train perplexity245.38739013671875
INFO:root:current mean train loss 6972.449596223377
INFO:root:current train perplexity244.95285034179688
INFO:root:current mean train loss 6975.891556251082
INFO:root:current train perplexity245.4796142578125
INFO:root:current mean train loss 6984.84457476522
INFO:root:current train perplexity247.22886657714844
INFO:root:current mean train loss 6997.630108835633
INFO:root:current train perplexity248.90843200683594
INFO:root:current mean train loss 7003.349510262866
INFO:root:current train perplexity250.16673278808594
INFO:root:current mean train loss 7007.446963759151
INFO:root:current train perplexity251.3017120361328
INFO:root:current mean train loss 7009.740211015866
INFO:root:current train perplexity252.2607421875

100%|██████████| 1/1 [07:33<00:00, 453.80s/it][A100%|██████████| 1/1 [07:33<00:00, 453.80s/it]
INFO:root:final mean train loss: 7009.399920441921
INFO:root:final train perplexity: 252.59884643554688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.89s/it][A100%|██████████| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 6856.230475675975
INFO:root:eval perplexity: 256.76531982421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.71s/it][A100%|██████████| 1/1 [00:38<00:00, 38.71s/it]
INFO:root:eval mean loss: 6949.645547636857
INFO:root:eval perplexity: 302.9303283691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/86
 43%|████▎     | 86/200 [12:48:14<17:11:54, 543.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7123.595783171107
INFO:root:current train perplexity268.9934387207031
INFO:root:current mean train loss 7117.649859884511
INFO:root:current train perplexity268.1266174316406
INFO:root:current mean train loss 7116.902164152299
INFO:root:current train perplexity269.4129333496094
INFO:root:current mean train loss 7109.797163099463
INFO:root:current train perplexity269.85546875
INFO:root:current mean train loss 7088.75595999695
INFO:root:current train perplexity269.9767150878906
INFO:root:current mean train loss 7075.910756809826
INFO:root:current train perplexity269.6027526855469
INFO:root:current mean train loss 7076.739382652941
INFO:root:current train perplexity269.8433837890625
INFO:root:current mean train loss 7074.656640111695
INFO:root:current train perplexity269.9235534667969
INFO:root:current mean train loss 7077.918997486571
INFO:root:current train perplexity270.1537780761719
INFO:root:current mean train loss 7085.956712673159
INFO:root:current train perplexity270.4636535644531
INFO:root:current mean train loss 7091.001763519086
INFO:root:current train perplexity270.7904052734375
INFO:root:current mean train loss 7089.816238863318
INFO:root:current train perplexity270.8324890136719
INFO:root:current mean train loss 7087.160756049886
INFO:root:current train perplexity270.826171875
INFO:root:current mean train loss 7089.67433398581
INFO:root:current train perplexity270.92742919921875
INFO:root:current mean train loss 7092.878580060746
INFO:root:current train perplexity270.98480224609375
INFO:root:current mean train loss 7091.906809599715
INFO:root:current train perplexity271.0057373046875
INFO:root:current mean train loss 7093.675615157943
INFO:root:current train perplexity270.9293518066406
INFO:root:current mean train loss 7096.976155183046
INFO:root:current train perplexity270.88739013671875
INFO:root:current mean train loss 7096.858635625168
INFO:root:current train perplexity270.8647766113281
INFO:root:current mean train loss 7098.5752560675355
INFO:root:current train perplexity270.9148864746094

100%|██████████| 1/1 [07:52<00:00, 472.68s/it][A100%|██████████| 1/1 [07:52<00:00, 472.68s/it]
INFO:root:final mean train loss: 7098.135773336052
INFO:root:final train perplexity: 270.9226379394531
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.92s/it][A100%|██████████| 1/1 [00:40<00:00, 40.92s/it]
INFO:root:eval mean loss: 6857.565940478169
INFO:root:eval perplexity: 257.04290771484375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.58s/it][A100%|██████████| 1/1 [00:38<00:00, 38.58s/it]
INFO:root:eval mean loss: 6953.103397017675
INFO:root:eval perplexity: 303.7927551269531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/87
 44%|████▎     | 87/200 [12:57:28<17:09:24, 546.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7096.0991774338945
INFO:root:current train perplexity270.5111083984375
INFO:root:current mean train loss 7124.283307364817
INFO:root:current train perplexity271.3275451660156
INFO:root:current mean train loss 7101.751528074416
INFO:root:current train perplexity271.85009765625
INFO:root:current mean train loss 7121.1946537078375
INFO:root:current train perplexity272.117919921875
INFO:root:current mean train loss 7116.916757240455
INFO:root:current train perplexity271.6759338378906
INFO:root:current mean train loss 7111.05781993404
INFO:root:current train perplexity271.07061767578125
INFO:root:current mean train loss 7105.861439752719
INFO:root:current train perplexity270.67266845703125
INFO:root:current mean train loss 7107.673351768356
INFO:root:current train perplexity270.1964416503906
INFO:root:current mean train loss 7112.731485353787
INFO:root:current train perplexity270.12481689453125
INFO:root:current mean train loss 7108.156701335634
INFO:root:current train perplexity269.98870849609375
INFO:root:current mean train loss 7102.499098174426
INFO:root:current train perplexity269.800537109375
INFO:root:current mean train loss 7105.86890560338
INFO:root:current train perplexity269.6329650878906
INFO:root:current mean train loss 7106.923263048342
INFO:root:current train perplexity269.4228210449219
INFO:root:current mean train loss 7107.845705251043
INFO:root:current train perplexity269.5071105957031
INFO:root:current mean train loss 7103.1685548196465
INFO:root:current train perplexity269.3028564453125
INFO:root:current mean train loss 7100.96536637803
INFO:root:current train perplexity269.2550964355469
INFO:root:current mean train loss 7099.59391004451
INFO:root:current train perplexity269.2968444824219
INFO:root:current mean train loss 7094.023690703213
INFO:root:current train perplexity269.1961669921875
INFO:root:current mean train loss 7094.926037350656
INFO:root:current train perplexity269.27667236328125
INFO:root:current mean train loss 7091.555910177974
INFO:root:current train perplexity269.1259765625

100%|██████████| 1/1 [07:53<00:00, 473.57s/it][A100%|██████████| 1/1 [07:53<00:00, 473.58s/it]
INFO:root:final mean train loss: 7089.644504410537
INFO:root:final train perplexity: 269.1132507324219
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.89s/it][A100%|██████████| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 6839.967009848737
INFO:root:eval perplexity: 253.40811157226562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.17s/it][A100%|██████████| 1/1 [00:42<00:00, 42.17s/it]
INFO:root:eval mean loss: 6936.667800795102
INFO:root:eval perplexity: 299.71533203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/88
 44%|████▍     | 88/200 [13:06:49<17:08:04, 550.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7095.0711862664475
INFO:root:current train perplexity270.2605895996094
INFO:root:current mean train loss 7077.007296674679
INFO:root:current train perplexity269.6015930175781
INFO:root:current mean train loss 7075.102613546081
INFO:root:current train perplexity268.40625
INFO:root:current mean train loss 7078.92947982595
INFO:root:current train perplexity268.144775390625
INFO:root:current mean train loss 7080.428658656881
INFO:root:current train perplexity267.7974548339844
INFO:root:current mean train loss 7084.3387030593485
INFO:root:current train perplexity267.68701171875
INFO:root:current mean train loss 7096.979752135791
INFO:root:current train perplexity268.3583679199219
INFO:root:current mean train loss 7098.769616008255
INFO:root:current train perplexity268.1282653808594
INFO:root:current mean train loss 7103.609922747905
INFO:root:current train perplexity267.9964904785156
INFO:root:current mean train loss 7098.2652515507225
INFO:root:current train perplexity267.82427978515625
INFO:root:current mean train loss 7096.249389091039
INFO:root:current train perplexity267.7695007324219
INFO:root:current mean train loss 7090.5106637356175
INFO:root:current train perplexity267.5836486816406
INFO:root:current mean train loss 7086.686612044522
INFO:root:current train perplexity267.58428955078125
INFO:root:current mean train loss 7078.939330967182
INFO:root:current train perplexity267.3589172363281
INFO:root:current mean train loss 7081.194980338106
INFO:root:current train perplexity267.2604064941406
INFO:root:current mean train loss 7077.82816632788
INFO:root:current train perplexity267.0854797363281
INFO:root:current mean train loss 7083.5712406664825
INFO:root:current train perplexity267.4118347167969
INFO:root:current mean train loss 7085.184005592792
INFO:root:current train perplexity267.4813537597656
INFO:root:current mean train loss 7081.842054285537
INFO:root:current train perplexity267.2908630371094

100%|██████████| 1/1 [07:49<00:00, 469.45s/it][A100%|██████████| 1/1 [07:49<00:00, 469.45s/it]
INFO:root:final mean train loss: 7081.6325522310735
INFO:root:final train perplexity: 267.41693115234375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.77s/it][A100%|██████████| 1/1 [00:41<00:00, 41.77s/it]
INFO:root:eval mean loss: 6834.472185283688
INFO:root:eval perplexity: 252.28395080566406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.93s/it][A100%|██████████| 1/1 [00:38<00:00, 38.93s/it]
INFO:root:eval mean loss: 6929.766418024158
INFO:root:eval perplexity: 298.0198974609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/89
 44%|████▍     | 89/200 [13:16:02<16:59:58, 551.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7189.484212239583
INFO:root:current train perplexity269.0587463378906
INFO:root:current mean train loss 7115.449659075056
INFO:root:current train perplexity269.7677307128906
INFO:root:current mean train loss 7107.952823279039
INFO:root:current train perplexity269.1630554199219
INFO:root:current mean train loss 7110.861245179788
INFO:root:current train perplexity268.97357177734375
INFO:root:current mean train loss 7087.400146484375
INFO:root:current train perplexity267.9849853515625
INFO:root:current mean train loss 7081.624097824097
INFO:root:current train perplexity267.9526672363281
INFO:root:current mean train loss 7086.024452359069
INFO:root:current train perplexity267.5341491699219
INFO:root:current mean train loss 7086.579897762685
INFO:root:current train perplexity267.412353515625
INFO:root:current mean train loss 7095.745918762507
INFO:root:current train perplexity268.0128479003906
INFO:root:current mean train loss 7095.269296211109
INFO:root:current train perplexity267.9793701171875
INFO:root:current mean train loss 7091.8075607360115
INFO:root:current train perplexity267.6849670410156
INFO:root:current mean train loss 7093.9516087813345
INFO:root:current train perplexity267.927734375
INFO:root:current mean train loss 7086.685265670122
INFO:root:current train perplexity267.7364196777344
INFO:root:current mean train loss 7087.913326356469
INFO:root:current train perplexity267.5597839355469
INFO:root:current mean train loss 7086.506858760845
INFO:root:current train perplexity267.60406494140625
INFO:root:current mean train loss 7086.4400799463665
INFO:root:current train perplexity267.4176940917969
INFO:root:current mean train loss 7086.5463458267095
INFO:root:current train perplexity267.4298095703125
INFO:root:current mean train loss 7083.8443891578745
INFO:root:current train perplexity267.297607421875
INFO:root:current mean train loss 7084.6367898903145
INFO:root:current train perplexity267.323486328125
INFO:root:current mean train loss 7080.871157083551
INFO:root:current train perplexity266.8973083496094

100%|██████████| 1/1 [07:51<00:00, 471.44s/it][A100%|██████████| 1/1 [07:51<00:00, 471.44s/it]
INFO:root:final mean train loss: 7078.916397779563
INFO:root:final train perplexity: 266.8443908691406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.40s/it][A100%|██████████| 1/1 [00:42<00:00, 42.40s/it]
INFO:root:eval mean loss: 6832.0036067015735
INFO:root:eval perplexity: 251.78076171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.75s/it][A100%|██████████| 1/1 [00:38<00:00, 38.75s/it]
INFO:root:eval mean loss: 6925.441959462268
INFO:root:eval perplexity: 296.9619445800781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/90
 45%|████▌     | 90/200 [13:25:17<16:52:56, 552.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7079.045612203664
INFO:root:current train perplexity268.0996398925781
INFO:root:current mean train loss 7079.676413366037
INFO:root:current train perplexity268.3700256347656
INFO:root:current mean train loss 7068.449026849072
INFO:root:current train perplexity265.8538513183594
INFO:root:current mean train loss 7058.660496117496
INFO:root:current train perplexity265.604248046875
INFO:root:current mean train loss 7056.775356479458
INFO:root:current train perplexity265.1697998046875
INFO:root:current mean train loss 7046.966171062736
INFO:root:current train perplexity264.7720642089844
INFO:root:current mean train loss 7049.82981884688
INFO:root:current train perplexity265.11090087890625
INFO:root:current mean train loss 7056.689946094822
INFO:root:current train perplexity265.5574035644531
INFO:root:current mean train loss 7063.2666910905455
INFO:root:current train perplexity265.6537780761719
INFO:root:current mean train loss 7061.105547064216
INFO:root:current train perplexity265.59259033203125
INFO:root:current mean train loss 7061.760362571368
INFO:root:current train perplexity265.6374206542969
INFO:root:current mean train loss 7067.375807026406
INFO:root:current train perplexity265.5194091796875
INFO:root:current mean train loss 7068.577548915531
INFO:root:current train perplexity265.6343994140625
INFO:root:current mean train loss 7068.1535800678375
INFO:root:current train perplexity265.2927551269531
INFO:root:current mean train loss 7067.068032031797
INFO:root:current train perplexity265.0086364746094
INFO:root:current mean train loss 7069.495408751124
INFO:root:current train perplexity264.8968505859375
INFO:root:current mean train loss 7072.4054379963745
INFO:root:current train perplexity264.68231201171875
INFO:root:current mean train loss 7070.575840894303
INFO:root:current train perplexity264.4642028808594
INFO:root:current mean train loss 7069.263530115927
INFO:root:current train perplexity264.34228515625
INFO:root:current mean train loss 7068.682785769829
INFO:root:current train perplexity264.1260681152344

100%|██████████| 1/1 [07:54<00:00, 474.49s/it][A100%|██████████| 1/1 [07:54<00:00, 474.49s/it]
INFO:root:final mean train loss: 7065.1822297389135
INFO:root:final train perplexity: 263.9676818847656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.87s/it][A100%|██████████| 1/1 [00:41<00:00, 41.87s/it]
INFO:root:eval mean loss: 6773.49535440215
INFO:root:eval perplexity: 240.13760375976562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.56s/it][A100%|██████████| 1/1 [00:39<00:00, 39.56s/it]
INFO:root:eval mean loss: 6868.179290122174
INFO:root:eval perplexity: 283.305908203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/91
 46%|████▌     | 91/200 [13:34:35<16:46:54, 554.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7038.202074133832
INFO:root:current train perplexity257.326904296875
INFO:root:current mean train loss 7078.701797276327
INFO:root:current train perplexity261.63897705078125
INFO:root:current mean train loss 7080.42974506161
INFO:root:current train perplexity262.9553527832031
INFO:root:current mean train loss 7076.482227126987
INFO:root:current train perplexity261.95379638671875
INFO:root:current mean train loss 7081.279631884108
INFO:root:current train perplexity261.65679931640625
INFO:root:current mean train loss 7083.690181969723
INFO:root:current train perplexity261.4444274902344
INFO:root:current mean train loss 7072.5241056743425
INFO:root:current train perplexity261.28875732421875
INFO:root:current mean train loss 7065.313594378351
INFO:root:current train perplexity261.02276611328125
INFO:root:current mean train loss 7057.5677268026
INFO:root:current train perplexity260.56463623046875
INFO:root:current mean train loss 7056.11441369087
INFO:root:current train perplexity260.4469909667969
INFO:root:current mean train loss 7058.309709421307
INFO:root:current train perplexity260.35040283203125
INFO:root:current mean train loss 7055.995942067518
INFO:root:current train perplexity260.6015319824219
INFO:root:current mean train loss 7048.137614193544
INFO:root:current train perplexity260.21795654296875
INFO:root:current mean train loss 7052.735904053097
INFO:root:current train perplexity260.1361999511719
INFO:root:current mean train loss 7052.383267351206
INFO:root:current train perplexity260.3157653808594
INFO:root:current mean train loss 7051.728368129952
INFO:root:current train perplexity260.3203430175781
INFO:root:current mean train loss 7049.596580607344
INFO:root:current train perplexity260.1531982421875
INFO:root:current mean train loss 7048.910911883412
INFO:root:current train perplexity260.08795166015625
INFO:root:current mean train loss 7048.249518595951
INFO:root:current train perplexity260.220458984375
INFO:root:current mean train loss 7048.410830208601
INFO:root:current train perplexity260.2530822753906

100%|██████████| 1/1 [07:53<00:00, 473.11s/it][A100%|██████████| 1/1 [07:53<00:00, 473.11s/it]
INFO:root:final mean train loss: 7046.925013862952
INFO:root:final train perplexity: 260.19140625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.90s/it][A100%|██████████| 1/1 [00:44<00:00, 44.90s/it]
INFO:root:eval mean loss: 6781.891776443374
INFO:root:eval perplexity: 241.774658203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.84s/it][A100%|██████████| 1/1 [00:39<00:00, 39.84s/it]
INFO:root:eval mean loss: 6874.207826005651
INFO:root:eval perplexity: 284.7134094238281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/92
 46%|████▌     | 92/200 [13:43:56<16:41:02, 556.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7055.540341331845
INFO:root:current train perplexity256.84417724609375
INFO:root:current mean train loss 7057.330281825153
INFO:root:current train perplexity258.2830810546875
INFO:root:current mean train loss 7041.5643825748575
INFO:root:current train perplexity257.564453125
INFO:root:current mean train loss 7039.416883232179
INFO:root:current train perplexity258.16363525390625
INFO:root:current mean train loss 7028.910643476647
INFO:root:current train perplexity257.75146484375
INFO:root:current mean train loss 7045.284296770926
INFO:root:current train perplexity258.1241149902344
INFO:root:current mean train loss 7044.984127545249
INFO:root:current train perplexity257.9601135253906
INFO:root:current mean train loss 7042.7759704189875
INFO:root:current train perplexity257.5225830078125
INFO:root:current mean train loss 7035.6534374321045
INFO:root:current train perplexity257.4626770019531
INFO:root:current mean train loss 7036.379013235819
INFO:root:current train perplexity257.3519592285156
INFO:root:current mean train loss 7039.4648906029515
INFO:root:current train perplexity257.2192077636719
INFO:root:current mean train loss 7040.92737204764
INFO:root:current train perplexity257.1420593261719
INFO:root:current mean train loss 7037.975196240351
INFO:root:current train perplexity257.1822814941406
INFO:root:current mean train loss 7037.71961192567
INFO:root:current train perplexity257.1751708984375
INFO:root:current mean train loss 7039.9483266271145
INFO:root:current train perplexity257.26434326171875
INFO:root:current mean train loss 7036.85097237634
INFO:root:current train perplexity256.903076171875
INFO:root:current mean train loss 7032.15995894092
INFO:root:current train perplexity256.5483093261719
INFO:root:current mean train loss 7031.2682464305335
INFO:root:current train perplexity256.33984375
INFO:root:current mean train loss 7030.636361253691
INFO:root:current train perplexity256.2454833984375
INFO:root:current mean train loss 7028.032334019199
INFO:root:current train perplexity256.1019592285156

100%|██████████| 1/1 [07:50<00:00, 470.76s/it][A100%|██████████| 1/1 [07:50<00:00, 470.76s/it]
INFO:root:final mean train loss: 7026.638519702628
INFO:root:final train perplexity: 256.05908203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.65s/it][A100%|██████████| 1/1 [00:39<00:00, 39.65s/it]
INFO:root:eval mean loss: 6737.266857823582
INFO:root:eval perplexity: 233.1998291015625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.06s/it][A100%|██████████| 1/1 [00:37<00:00, 37.06s/it]
INFO:root:eval mean loss: 6830.289273742243
INFO:root:eval perplexity: 274.61676025390625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/93
 46%|████▋     | 93/200 [13:53:06<16:28:33, 554.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7082.52758178711
INFO:root:current train perplexity255.94837951660156
INFO:root:current mean train loss 7103.109212239583
INFO:root:current train perplexity255.06932067871094
INFO:root:current mean train loss 7060.9665579659595
INFO:root:current train perplexity253.89276123046875
INFO:root:current mean train loss 7041.209521484375
INFO:root:current train perplexity253.0435791015625
INFO:root:current mean train loss 7034.978480021159
INFO:root:current train perplexity253.2111053466797
INFO:root:current mean train loss 7034.707761988147
INFO:root:current train perplexity253.52764892578125
INFO:root:current mean train loss 7029.3510993508735
INFO:root:current train perplexity253.24371337890625
INFO:root:current mean train loss 7022.691002478967
INFO:root:current train perplexity253.00074768066406
INFO:root:current mean train loss 7024.02695201527
INFO:root:current train perplexity252.82431030273438
INFO:root:current mean train loss 7016.496738480549
INFO:root:current train perplexity252.19073486328125
INFO:root:current mean train loss 7014.429267939815
INFO:root:current train perplexity251.9420623779297
INFO:root:current mean train loss 7015.547478730799
INFO:root:current train perplexity251.92271423339844
INFO:root:current mean train loss 7011.549464797974
INFO:root:current train perplexity251.54124450683594
INFO:root:current mean train loss 7008.278413722826
INFO:root:current train perplexity251.1470489501953
INFO:root:current mean train loss 7004.484787729624
INFO:root:current train perplexity251.07138061523438
INFO:root:current mean train loss 7002.173949268196
INFO:root:current train perplexity250.81280517578125
INFO:root:current mean train loss 7000.209889439174
INFO:root:current train perplexity250.51397705078125
INFO:root:current mean train loss 7001.040644476387
INFO:root:current train perplexity250.43576049804688
INFO:root:current mean train loss 6999.504568286652
INFO:root:current train perplexity250.30287170410156
INFO:root:current mean train loss 6998.398116418087
INFO:root:current train perplexity250.07728576660156

100%|██████████| 1/1 [07:36<00:00, 456.76s/it][A100%|██████████| 1/1 [07:36<00:00, 456.76s/it]
INFO:root:final mean train loss: 6996.744729246447
INFO:root:final train perplexity: 250.08872985839844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.61s/it][A100%|██████████| 1/1 [00:41<00:00, 41.61s/it]
INFO:root:eval mean loss: 6696.585014613807
INFO:root:eval perplexity: 225.64779663085938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.74s/it][A100%|██████████| 1/1 [00:38<00:00, 38.74s/it]
INFO:root:eval mean loss: 6791.530696787732
INFO:root:eval perplexity: 266.0042419433594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/94
 47%|████▋     | 94/200 [14:02:05<16:11:27, 549.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6986.972178036404
INFO:root:current train perplexity247.5294189453125
INFO:root:current mean train loss 6980.7119140625
INFO:root:current train perplexity248.41932678222656
INFO:root:current mean train loss 6982.937955400358
INFO:root:current train perplexity249.71360778808594
INFO:root:current mean train loss 6972.646432718042
INFO:root:current train perplexity248.57122802734375
INFO:root:current mean train loss 6974.122180347711
INFO:root:current train perplexity248.23744201660156
INFO:root:current mean train loss 6972.978155752722
INFO:root:current train perplexity247.9105987548828
INFO:root:current mean train loss 6965.59694029098
INFO:root:current train perplexity247.56436157226562
INFO:root:current mean train loss 6965.327463339084
INFO:root:current train perplexity247.447998046875
INFO:root:current mean train loss 6964.48295860333
INFO:root:current train perplexity247.49436950683594
INFO:root:current mean train loss 6969.802348451605
INFO:root:current train perplexity247.67901611328125
INFO:root:current mean train loss 6972.108089088851
INFO:root:current train perplexity247.52906799316406
INFO:root:current mean train loss 6977.027712918363
INFO:root:current train perplexity247.65232849121094
INFO:root:current mean train loss 6978.010545670297
INFO:root:current train perplexity247.6337890625
INFO:root:current mean train loss 6979.091944722508
INFO:root:current train perplexity247.648193359375
INFO:root:current mean train loss 6977.5000068496365
INFO:root:current train perplexity247.60638427734375
INFO:root:current mean train loss 6978.894404058391
INFO:root:current train perplexity247.65232849121094
INFO:root:current mean train loss 6980.379911873435
INFO:root:current train perplexity247.7192840576172
INFO:root:current mean train loss 6985.70782956403
INFO:root:current train perplexity247.87643432617188
INFO:root:current mean train loss 6987.135102042287
INFO:root:current train perplexity247.87538146972656

100%|██████████| 1/1 [07:39<00:00, 459.73s/it][A100%|██████████| 1/1 [07:39<00:00, 459.73s/it]
INFO:root:final mean train loss: 6986.6470316292
INFO:root:final train perplexity: 248.1037139892578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.15s/it][A100%|██████████| 1/1 [00:40<00:00, 40.15s/it]
INFO:root:eval mean loss: 6716.721497880651
INFO:root:eval perplexity: 229.35472106933594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.44s/it][A100%|██████████| 1/1 [00:38<00:00, 38.44s/it]
INFO:root:eval mean loss: 6812.050686883588
INFO:root:eval perplexity: 270.5298156738281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/95
 48%|████▊     | 95/200 [14:11:06<15:57:29, 547.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7035.395577566965
INFO:root:current train perplexity255.63101196289062
INFO:root:current mean train loss 6977.332391036184
INFO:root:current train perplexity250.25848388671875
INFO:root:current mean train loss 6984.332003869743
INFO:root:current train perplexity250.7140350341797
INFO:root:current mean train loss 6984.614627911027
INFO:root:current train perplexity250.74200439453125
INFO:root:current mean train loss 6983.250389209692
INFO:root:current train perplexity249.87393188476562
INFO:root:current mean train loss 6983.755502188716
INFO:root:current train perplexity248.95175170898438
INFO:root:current mean train loss 6974.980912497455
INFO:root:current train perplexity248.44940185546875
INFO:root:current mean train loss 6964.260361273416
INFO:root:current train perplexity248.2588653564453
INFO:root:current mean train loss 6973.5930703652875
INFO:root:current train perplexity248.2429962158203
INFO:root:current mean train loss 6971.409476716357
INFO:root:current train perplexity248.17601013183594
INFO:root:current mean train loss 6979.34590874245
INFO:root:current train perplexity248.15943908691406
INFO:root:current mean train loss 6983.197915643935
INFO:root:current train perplexity247.89523315429688
INFO:root:current mean train loss 6984.891566570351
INFO:root:current train perplexity248.04078674316406
INFO:root:current mean train loss 6987.734873314426
INFO:root:current train perplexity248.07733154296875
INFO:root:current mean train loss 6986.61999011006
INFO:root:current train perplexity248.07911682128906
INFO:root:current mean train loss 6987.8278692489885
INFO:root:current train perplexity248.29046630859375
INFO:root:current mean train loss 6987.834751890199
INFO:root:current train perplexity248.40048217773438
INFO:root:current mean train loss 6990.256059644381
INFO:root:current train perplexity248.48838806152344
INFO:root:current mean train loss 6992.847363927267
INFO:root:current train perplexity248.61070251464844
INFO:root:current mean train loss 6990.6895689451085
INFO:root:current train perplexity248.6800537109375

100%|██████████| 1/1 [07:26<00:00, 446.94s/it][A100%|██████████| 1/1 [07:26<00:00, 446.94s/it]
INFO:root:final mean train loss: 6990.196764170252
INFO:root:final train perplexity: 248.7997283935547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.85s/it][A100%|██████████| 1/1 [00:38<00:00, 38.85s/it]
INFO:root:eval mean loss: 6712.70504176363
INFO:root:eval perplexity: 228.61050415039062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.55s/it][A100%|██████████| 1/1 [00:35<00:00, 35.55s/it]
INFO:root:eval mean loss: 6807.605678260749
INFO:root:eval perplexity: 269.54296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/96
 48%|████▊     | 96/200 [14:19:50<15:36:17, 540.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6979.251197076613
INFO:root:current train perplexity249.4969940185547
INFO:root:current mean train loss 7041.773840052481
INFO:root:current train perplexity251.8129425048828
INFO:root:current mean train loss 7008.010556175595
INFO:root:current train perplexity249.93601989746094
INFO:root:current mean train loss 7007.995021301454
INFO:root:current train perplexity250.20909118652344
INFO:root:current mean train loss 6997.343872353538
INFO:root:current train perplexity249.18809509277344
INFO:root:current mean train loss 7000.737117099223
INFO:root:current train perplexity249.2986297607422
INFO:root:current mean train loss 7010.803612662193
INFO:root:current train perplexity249.7470703125
INFO:root:current mean train loss 7004.567803629446
INFO:root:current train perplexity249.5676727294922
INFO:root:current mean train loss 7001.784363013312
INFO:root:current train perplexity249.73170471191406
INFO:root:current mean train loss 7005.085445022993
INFO:root:current train perplexity249.95913696289062
INFO:root:current mean train loss 7006.564057195684
INFO:root:current train perplexity250.15444946289062
INFO:root:current mean train loss 7007.537614493534
INFO:root:current train perplexity250.5447998046875
INFO:root:current mean train loss 7007.112445896375
INFO:root:current train perplexity250.390380859375
INFO:root:current mean train loss 7003.497561528573
INFO:root:current train perplexity250.1798553466797
INFO:root:current mean train loss 7002.883118571475
INFO:root:current train perplexity250.12713623046875
INFO:root:current mean train loss 6998.969165246367
INFO:root:current train perplexity249.85069274902344
INFO:root:current mean train loss 6998.861164666041
INFO:root:current train perplexity249.81210327148438
INFO:root:current mean train loss 6997.800049251246
INFO:root:current train perplexity249.61801147460938
INFO:root:current mean train loss 6994.8125594684425
INFO:root:current train perplexity249.437744140625
INFO:root:current mean train loss 6995.466469668404
INFO:root:current train perplexity249.4864044189453

100%|██████████| 1/1 [07:33<00:00, 453.05s/it][A100%|██████████| 1/1 [07:33<00:00, 453.05s/it]
INFO:root:final mean train loss: 6993.1777711869245
INFO:root:final train perplexity: 249.38577270507812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.65s/it][A100%|██████████| 1/1 [00:39<00:00, 39.65s/it]
INFO:root:eval mean loss: 6710.3580988890735
INFO:root:eval perplexity: 228.1768341064453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.88s/it][A100%|██████████| 1/1 [00:36<00:00, 36.88s/it]
INFO:root:eval mean loss: 6806.187290489251
INFO:root:eval perplexity: 269.2287902832031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/97
 48%|████▊     | 97/200 [14:28:42<15:23:10, 537.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7029.604248046875
INFO:root:current train perplexity247.55030822753906
INFO:root:current mean train loss 6981.2756248680325
INFO:root:current train perplexity250.24058532714844
INFO:root:current mean train loss 6963.3271996282765
INFO:root:current train perplexity248.6868133544922
INFO:root:current mean train loss 6972.201676993534
INFO:root:current train perplexity248.31640625
INFO:root:current mean train loss 6985.796742030552
INFO:root:current train perplexity248.64898681640625
INFO:root:current mean train loss 6996.143335495552
INFO:root:current train perplexity249.2777099609375
INFO:root:current mean train loss 7000.49195240162
INFO:root:current train perplexity249.3551025390625
INFO:root:current mean train loss 7007.903776607412
INFO:root:current train perplexity249.0710906982422
INFO:root:current mean train loss 7004.401152412846
INFO:root:current train perplexity249.02952575683594
INFO:root:current mean train loss 7006.229729117221
INFO:root:current train perplexity249.44024658203125
INFO:root:current mean train loss 7009.903895720271
INFO:root:current train perplexity249.70420837402344
INFO:root:current mean train loss 7008.0638836053195
INFO:root:current train perplexity249.32443237304688
INFO:root:current mean train loss 7007.247692010342
INFO:root:current train perplexity248.99270629882812
INFO:root:current mean train loss 7004.308251446008
INFO:root:current train perplexity248.69761657714844
INFO:root:current mean train loss 6999.679701662854
INFO:root:current train perplexity248.6155548095703
INFO:root:current mean train loss 6998.466435080043
INFO:root:current train perplexity248.5676727294922
INFO:root:current mean train loss 6995.659719522717
INFO:root:current train perplexity248.47108459472656
INFO:root:current mean train loss 6990.6181249553065
INFO:root:current train perplexity248.3082275390625
INFO:root:current mean train loss 6990.440521636567
INFO:root:current train perplexity248.12442016601562
INFO:root:current mean train loss 6987.489075333676
INFO:root:current train perplexity247.89901733398438

100%|██████████| 1/1 [07:37<00:00, 457.21s/it][A100%|██████████| 1/1 [07:37<00:00, 457.21s/it]
INFO:root:final mean train loss: 6984.97468878534
INFO:root:final train perplexity: 247.77635192871094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.44s/it][A100%|██████████| 1/1 [00:39<00:00, 39.44s/it]
INFO:root:eval mean loss: 6681.007542386968
INFO:root:eval perplexity: 222.8212127685547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.97s/it][A100%|██████████| 1/1 [00:36<00:00, 36.97s/it]
INFO:root:eval mean loss: 6775.511432187777
INFO:root:eval perplexity: 262.5238952636719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/98
 49%|████▉     | 98/200 [14:37:38<15:13:23, 537.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7009.2250826322115
INFO:root:current train perplexity247.23275756835938
INFO:root:current mean train loss 7028.359886955493
INFO:root:current train perplexity247.78709411621094
INFO:root:current mean train loss 7012.017386497641
INFO:root:current train perplexity247.26658630371094
INFO:root:current mean train loss 6999.013930062072
INFO:root:current train perplexity247.2628173828125
INFO:root:current mean train loss 6986.668000252016
INFO:root:current train perplexity247.05868530273438
INFO:root:current mean train loss 6981.740258573009
INFO:root:current train perplexity247.12725830078125
INFO:root:current mean train loss 6985.363174782659
INFO:root:current train perplexity247.3775634765625
INFO:root:current mean train loss 6993.224303640727
INFO:root:current train perplexity247.76312255859375
INFO:root:current mean train loss 6997.321937093569
INFO:root:current train perplexity248.01063537597656
INFO:root:current mean train loss 6995.76764339783
INFO:root:current train perplexity248.25662231445312
INFO:root:current mean train loss 6990.846411935153
INFO:root:current train perplexity248.2158966064453
INFO:root:current mean train loss 6995.610419041712
INFO:root:current train perplexity248.6620330810547
INFO:root:current mean train loss 6988.675224261981
INFO:root:current train perplexity248.7495574951172
INFO:root:current mean train loss 6989.058751860119
INFO:root:current train perplexity248.80506896972656
INFO:root:current mean train loss 6989.595975429287
INFO:root:current train perplexity248.8498077392578
INFO:root:current mean train loss 6991.837021078774
INFO:root:current train perplexity248.9148406982422
INFO:root:current mean train loss 6992.541907434778
INFO:root:current train perplexity248.95318603515625
INFO:root:current mean train loss 6992.894601518236
INFO:root:current train perplexity249.20033264160156
INFO:root:current mean train loss 6994.190429425687
INFO:root:current train perplexity249.31301879882812
INFO:root:current mean train loss 6994.256071584765
INFO:root:current train perplexity249.31324768066406

100%|██████████| 1/1 [07:40<00:00, 460.91s/it][A100%|██████████| 1/1 [07:40<00:00, 460.91s/it]
INFO:root:final mean train loss: 6993.407077591166
INFO:root:final train perplexity: 249.43084716796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.66s/it][A100%|██████████| 1/1 [00:43<00:00, 43.66s/it]
INFO:root:eval mean loss: 6728.717503324468
INFO:root:eval perplexity: 231.59190368652344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.05s/it][A100%|██████████| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 6824.012800933621
INFO:root:eval perplexity: 273.20355224609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/99
 50%|████▉     | 99/200 [14:46:46<15:09:29, 540.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6923.137808450839
INFO:root:current train perplexity248.336181640625
INFO:root:current mean train loss 6977.109656700721
INFO:root:current train perplexity250.29750061035156
INFO:root:current mean train loss 6995.543067445146
INFO:root:current train perplexity250.5317840576172
INFO:root:current mean train loss 6995.719275349722
INFO:root:current train perplexity250.1138916015625
INFO:root:current mean train loss 6998.8119772756745
INFO:root:current train perplexity250.70948791503906
INFO:root:current mean train loss 6995.826825433581
INFO:root:current train perplexity250.10720825195312
INFO:root:current mean train loss 6993.0441472117855
INFO:root:current train perplexity250.60670471191406
INFO:root:current mean train loss 6993.055742112572
INFO:root:current train perplexity250.48532104492188
INFO:root:current mean train loss 6996.092043783659
INFO:root:current train perplexity250.96221923828125
INFO:root:current mean train loss 6999.62718135422
INFO:root:current train perplexity251.1678924560547
INFO:root:current mean train loss 6997.9651632538125
INFO:root:current train perplexity251.2773895263672
INFO:root:current mean train loss 7000.578938802083
INFO:root:current train perplexity251.20814514160156
INFO:root:current mean train loss 6997.559163538416
INFO:root:current train perplexity251.27354431152344
INFO:root:current mean train loss 6999.633359078215
INFO:root:current train perplexity251.3123779296875
INFO:root:current mean train loss 7002.166897298836
INFO:root:current train perplexity251.45274353027344
INFO:root:current mean train loss 7003.126938927188
INFO:root:current train perplexity251.54771423339844
INFO:root:current mean train loss 7004.368902000037
INFO:root:current train perplexity251.67514038085938
INFO:root:current mean train loss 7005.968826174067
INFO:root:current train perplexity251.61813354492188
INFO:root:current mean train loss 7006.742155068992
INFO:root:current train perplexity251.49314880371094
INFO:root:current mean train loss 7005.848617291956
INFO:root:current train perplexity251.50082397460938

100%|██████████| 1/1 [08:00<00:00, 480.09s/it][A100%|██████████| 1/1 [08:00<00:00, 480.09s/it]
INFO:root:final mean train loss: 7003.935030646235
INFO:root:final train perplexity: 251.51197814941406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.86s/it][A100%|██████████| 1/1 [00:43<00:00, 43.87s/it]
INFO:root:eval mean loss: 6730.6558985067595
INFO:root:eval perplexity: 231.95562744140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.17s/it][A100%|██████████| 1/1 [00:42<00:00, 42.17s/it]
INFO:root:eval mean loss: 6824.489219719637
INFO:root:eval perplexity: 273.3105163574219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/100
 50%|█████     | 100/200 [14:56:14<15:14:44, 548.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7034.371182528409
INFO:root:current train perplexity251.63278198242188
INFO:root:current mean train loss 7026.641704616834
INFO:root:current train perplexity250.3682861328125
INFO:root:current mean train loss 7023.942875992893
INFO:root:current train perplexity250.29393005371094
INFO:root:current mean train loss 7015.597641564849
INFO:root:current train perplexity251.10215759277344
INFO:root:current mean train loss 6999.185478378632
INFO:root:current train perplexity249.9755859375
INFO:root:current mean train loss 6999.976170407711
INFO:root:current train perplexity250.0707244873047
INFO:root:current mean train loss 7000.456171344108
INFO:root:current train perplexity250.34799194335938
INFO:root:current mean train loss 7010.0919496538645
INFO:root:current train perplexity250.5450439453125
INFO:root:current mean train loss 7004.437454919529
INFO:root:current train perplexity250.32603454589844
INFO:root:current mean train loss 7009.725149954642
INFO:root:current train perplexity250.13523864746094
INFO:root:current mean train loss 7005.071998603134
INFO:root:current train perplexity250.02291870117188
INFO:root:current mean train loss 7004.560230856443
INFO:root:current train perplexity249.89930725097656
INFO:root:current mean train loss 7000.819734381014
INFO:root:current train perplexity249.80447387695312
INFO:root:current mean train loss 7000.730490389341
INFO:root:current train perplexity249.7052764892578
INFO:root:current mean train loss 7001.406674110865
INFO:root:current train perplexity249.57028198242188
INFO:root:current mean train loss 6998.215569606492
INFO:root:current train perplexity249.4592742919922
INFO:root:current mean train loss 6997.947291490399
INFO:root:current train perplexity249.40313720703125
INFO:root:current mean train loss 6999.194752835777
INFO:root:current train perplexity249.6231231689453
INFO:root:current mean train loss 6997.237381568013
INFO:root:current train perplexity249.56968688964844

100%|██████████| 1/1 [07:48<00:00, 468.45s/it][A100%|██████████| 1/1 [07:48<00:00, 468.45s/it]
INFO:root:final mean train loss: 6994.649895079859
INFO:root:final train perplexity: 249.67550659179688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.78s/it][A100%|██████████| 1/1 [00:40<00:00, 40.78s/it]
INFO:root:eval mean loss: 6735.605042802526
INFO:root:eval perplexity: 232.8863525390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.14s/it][A100%|██████████| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 6829.828185602283
INFO:root:eval perplexity: 274.5127868652344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/101
 50%|█████     | 101/200 [15:05:24<15:06:13, 549.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7071.092956542969
INFO:root:current train perplexity250.3708038330078
INFO:root:current mean train loss 6999.719242490572
INFO:root:current train perplexity251.18310546875
INFO:root:current mean train loss 7001.866106951678
INFO:root:current train perplexity250.8312225341797
INFO:root:current mean train loss 7009.671363540842
INFO:root:current train perplexity251.3159637451172
INFO:root:current mean train loss 7017.901807344877
INFO:root:current train perplexity250.90394592285156
INFO:root:current mean train loss 7006.449989023135
INFO:root:current train perplexity250.63035583496094
INFO:root:current mean train loss 7004.945261769481
INFO:root:current train perplexity250.69789123535156
INFO:root:current mean train loss 7002.207638191777
INFO:root:current train perplexity250.93577575683594
INFO:root:current mean train loss 6999.332046209597
INFO:root:current train perplexity250.9205780029297
INFO:root:current mean train loss 6999.929571293327
INFO:root:current train perplexity250.92620849609375
INFO:root:current mean train loss 6998.091573880413
INFO:root:current train perplexity250.77967834472656
INFO:root:current mean train loss 7003.886018267669
INFO:root:current train perplexity251.07701110839844
INFO:root:current mean train loss 6998.281145597759
INFO:root:current train perplexity250.881103515625
INFO:root:current mean train loss 7000.069792680827
INFO:root:current train perplexity251.1517333984375
INFO:root:current mean train loss 7008.393589515471
INFO:root:current train perplexity251.64825439453125
INFO:root:current mean train loss 7008.666301636708
INFO:root:current train perplexity251.63458251953125
INFO:root:current mean train loss 7010.338899215849
INFO:root:current train perplexity251.8179931640625
INFO:root:current mean train loss 7008.064348412005
INFO:root:current train perplexity251.8071746826172
INFO:root:current mean train loss 7009.017844851322
INFO:root:current train perplexity251.93052673339844
INFO:root:current mean train loss 7007.045162192964
INFO:root:current train perplexity251.74990844726562

100%|██████████| 1/1 [08:07<00:00, 487.46s/it][A100%|██████████| 1/1 [08:07<00:00, 487.47s/it]
INFO:root:final mean train loss: 7005.055834086808
INFO:root:final train perplexity: 251.7344207763672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.55s/it][A100%|██████████| 1/1 [00:46<00:00, 46.55s/it]
INFO:root:eval mean loss: 6737.766049215979
INFO:root:eval perplexity: 233.29403686523438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.28s/it][A100%|██████████| 1/1 [00:40<00:00, 40.28s/it]
INFO:root:eval mean loss: 6830.6898565630545
INFO:root:eval perplexity: 274.7072448730469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/102
 51%|█████     | 102/200 [15:15:01<15:10:34, 557.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6956.314556699811
INFO:root:current train perplexity253.4064178466797
INFO:root:current mean train loss 6984.45294143562
INFO:root:current train perplexity250.8412628173828
INFO:root:current mean train loss 6982.285692730687
INFO:root:current train perplexity250.8984375
INFO:root:current mean train loss 6984.81789162209
INFO:root:current train perplexity251.20394897460938
INFO:root:current mean train loss 6978.928859789983
INFO:root:current train perplexity250.7545623779297
INFO:root:current mean train loss 6977.817620998476
INFO:root:current train perplexity250.47122192382812
INFO:root:current mean train loss 6994.159234455223
INFO:root:current train perplexity250.93960571289062
INFO:root:current mean train loss 6999.325936727277
INFO:root:current train perplexity250.9555206298828
INFO:root:current mean train loss 7002.9587569402765
INFO:root:current train perplexity250.87799072265625
INFO:root:current mean train loss 7003.5001240328575
INFO:root:current train perplexity251.20130920410156
INFO:root:current mean train loss 7005.597250215544
INFO:root:current train perplexity251.56895446777344
INFO:root:current mean train loss 7005.240027512687
INFO:root:current train perplexity251.4677276611328
INFO:root:current mean train loss 7003.651827747998
INFO:root:current train perplexity251.51617431640625
INFO:root:current mean train loss 7002.917265082872
INFO:root:current train perplexity251.24143981933594
INFO:root:current mean train loss 7003.8018341383895
INFO:root:current train perplexity251.2686309814453
INFO:root:current mean train loss 7003.2401283099925
INFO:root:current train perplexity251.22012329101562
INFO:root:current mean train loss 7001.9657981858545
INFO:root:current train perplexity250.93540954589844
INFO:root:current mean train loss 7001.802144380229
INFO:root:current train perplexity250.9737091064453
INFO:root:current mean train loss 6997.806677119562
INFO:root:current train perplexity250.8392333984375
INFO:root:current mean train loss 7001.595742025835
INFO:root:current train perplexity250.82798767089844

100%|██████████| 1/1 [08:08<00:00, 488.22s/it][A100%|██████████| 1/1 [08:08<00:00, 488.22s/it]
INFO:root:final mean train loss: 7000.404666841
INFO:root:final train perplexity: 250.8120880126953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.39s/it][A100%|██████████| 1/1 [00:43<00:00, 43.39s/it]
INFO:root:eval mean loss: 6732.575449842087
INFO:root:eval perplexity: 232.31614685058594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.25s/it][A100%|██████████| 1/1 [00:41<00:00, 41.25s/it]
INFO:root:eval mean loss: 6826.236385264295
INFO:root:eval perplexity: 273.7033386230469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/103
 52%|█████▏    | 103/200 [15:24:37<15:09:58, 562.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6995.788193359375
INFO:root:current train perplexity253.11767578125
INFO:root:current mean train loss 6970.106009114584
INFO:root:current train perplexity249.3520050048828
INFO:root:current mean train loss 6973.209314453125
INFO:root:current train perplexity247.1073455810547
INFO:root:current mean train loss 6976.991798270089
INFO:root:current train perplexity247.75921630859375
INFO:root:current mean train loss 6983.303229166667
INFO:root:current train perplexity247.9821319580078
INFO:root:current mean train loss 6977.400910866478
INFO:root:current train perplexity247.58738708496094
INFO:root:current mean train loss 6986.157205528846
INFO:root:current train perplexity248.06101989746094
INFO:root:current mean train loss 6999.948512369791
INFO:root:current train perplexity249.05197143554688
INFO:root:current mean train loss 6991.550850758272
INFO:root:current train perplexity249.44381713867188
INFO:root:current mean train loss 6997.684421772204
INFO:root:current train perplexity249.8074493408203
INFO:root:current mean train loss 7003.143749069941
INFO:root:current train perplexity249.84271240234375
INFO:root:current mean train loss 7002.25240234375
INFO:root:current train perplexity250.0695343017578
INFO:root:current mean train loss 6998.879415625
INFO:root:current train perplexity249.7762451171875
INFO:root:current mean train loss 7006.196471715856
INFO:root:current train perplexity250.09075927734375
INFO:root:current mean train loss 7003.54388436153
INFO:root:current train perplexity249.99490356445312
INFO:root:current mean train loss 6998.827834866432
INFO:root:current train perplexity249.54505920410156
INFO:root:current mean train loss 6998.175143821023
INFO:root:current train perplexity249.33155822753906
INFO:root:current mean train loss 6996.602242466518
INFO:root:current train perplexity249.2668914794922
INFO:root:current mean train loss 6996.911904296875
INFO:root:current train perplexity249.1818084716797
INFO:root:current mean train loss 6991.437765925481
INFO:root:current train perplexity248.6497039794922

100%|██████████| 1/1 [08:02<00:00, 482.98s/it][A100%|██████████| 1/1 [08:02<00:00, 482.98s/it]
INFO:root:final mean train loss: 6988.735480712029
INFO:root:final train perplexity: 248.51292419433594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.67s/it][A100%|██████████| 1/1 [00:45<00:00, 45.67s/it]
INFO:root:eval mean loss: 6643.416488322806
INFO:root:eval perplexity: 216.1451873779297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.67s/it][A100%|██████████| 1/1 [00:40<00:00, 40.67s/it]
INFO:root:eval mean loss: 6740.744895556294
INFO:root:eval perplexity: 255.12649536132812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/104
 52%|█████▏    | 104/200 [15:34:09<15:05:00, 565.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7010.729317280784
INFO:root:current train perplexity239.6187744140625
INFO:root:current mean train loss 6961.157957522455
INFO:root:current train perplexity238.72567749023438
INFO:root:current mean train loss 6960.310585279143
INFO:root:current train perplexity239.63201904296875
INFO:root:current mean train loss 6985.233736376022
INFO:root:current train perplexity240.30462646484375
INFO:root:current mean train loss 6976.608775888317
INFO:root:current train perplexity240.37452697753906
INFO:root:current mean train loss 6975.1310953345455
INFO:root:current train perplexity240.9005126953125
INFO:root:current mean train loss 6979.46457581756
INFO:root:current train perplexity241.1852264404297
INFO:root:current mean train loss 6972.880655659224
INFO:root:current train perplexity241.33834838867188
INFO:root:current mean train loss 6970.284868462551
INFO:root:current train perplexity241.5695343017578
INFO:root:current mean train loss 6966.143784942154
INFO:root:current train perplexity241.73292541503906
INFO:root:current mean train loss 6973.4142551583
INFO:root:current train perplexity242.1910858154297
INFO:root:current mean train loss 6973.076995718857
INFO:root:current train perplexity242.86228942871094
INFO:root:current mean train loss 6978.900631875246
INFO:root:current train perplexity243.83840942382812
INFO:root:current mean train loss 6982.2374161313555
INFO:root:current train perplexity244.63873291015625
INFO:root:current mean train loss 6981.821598274008
INFO:root:current train perplexity245.09292602539062
INFO:root:current mean train loss 6977.454795813059
INFO:root:current train perplexity245.0790252685547
INFO:root:current mean train loss 6981.742364417742
INFO:root:current train perplexity245.51718139648438
INFO:root:current mean train loss 6978.763178619747
INFO:root:current train perplexity245.71347045898438
INFO:root:current mean train loss 6980.665885381795
INFO:root:current train perplexity246.06158447265625
INFO:root:current mean train loss 6978.557676267794
INFO:root:current train perplexity246.2472686767578

100%|██████████| 1/1 [08:03<00:00, 483.55s/it][A100%|██████████| 1/1 [08:03<00:00, 483.55s/it]
INFO:root:final mean train loss: 6977.25336749086
INFO:root:final train perplexity: 246.27122497558594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.99s/it][A100%|██████████| 1/1 [00:43<00:00, 43.99s/it]
INFO:root:eval mean loss: 6736.159938081782
INFO:root:eval perplexity: 232.99087524414062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.96s/it][A100%|██████████| 1/1 [00:41<00:00, 41.96s/it]
INFO:root:eval mean loss: 6829.408193601784
INFO:root:eval perplexity: 274.41790771484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/105
 52%|█████▎    | 105/200 [15:43:41<14:58:34, 567.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7027.666451590402
INFO:root:current train perplexity251.4447021484375
INFO:root:current mean train loss 7029.257374639096
INFO:root:current train perplexity251.138427734375
INFO:root:current mean train loss 7017.580573283451
INFO:root:current train perplexity250.7008819580078
INFO:root:current mean train loss 7016.448162078857
INFO:root:current train perplexity250.875
INFO:root:current mean train loss 6996.925219323024
INFO:root:current train perplexity250.02076721191406
INFO:root:current mean train loss 6989.903777658123
INFO:root:current train perplexity249.5689697265625
INFO:root:current mean train loss 6983.8961759868425
INFO:root:current train perplexity249.07501220703125
INFO:root:current mean train loss 6984.946820317483
INFO:root:current train perplexity249.18394470214844
INFO:root:current mean train loss 6992.129598350007
INFO:root:current train perplexity248.97549438476562
INFO:root:current mean train loss 6989.26936005383
INFO:root:current train perplexity248.73721313476562
INFO:root:current mean train loss 6985.147834355541
INFO:root:current train perplexity248.395751953125
INFO:root:current mean train loss 6986.046370222762
INFO:root:current train perplexity247.96096801757812
INFO:root:current mean train loss 6988.748859916521
INFO:root:current train perplexity247.92478942871094
INFO:root:current mean train loss 6984.30755509393
INFO:root:current train perplexity247.62599182128906
INFO:root:current mean train loss 6984.244100154249
INFO:root:current train perplexity247.21766662597656
INFO:root:current mean train loss 6979.979929914378
INFO:root:current train perplexity247.0940399169922
INFO:root:current mean train loss 6980.488444493672
INFO:root:current train perplexity247.2646942138672
INFO:root:current mean train loss 6983.925421881569
INFO:root:current train perplexity247.4845733642578
INFO:root:current mean train loss 6986.128519564424
INFO:root:current train perplexity247.45767211914062

100%|██████████| 1/1 [08:02<00:00, 482.19s/it][A100%|██████████| 1/1 [08:02<00:00, 482.19s/it]
INFO:root:final mean train loss: 6983.379978966725
INFO:root:final train perplexity: 247.46475219726562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.47s/it][A100%|██████████| 1/1 [00:44<00:00, 44.47s/it]
INFO:root:eval mean loss: 6709.833200008311
INFO:root:eval perplexity: 228.07980346679688
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.23s/it][A100%|██████████| 1/1 [00:40<00:00, 40.23s/it]
INFO:root:eval mean loss: 6807.980677395002
INFO:root:eval perplexity: 269.62615966796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/106
 53%|█████▎    | 106/200 [15:53:10<14:50:03, 568.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7013.4013671875
INFO:root:current train perplexity256.8717346191406
INFO:root:current mean train loss 6969.584883586015
INFO:root:current train perplexity242.9540252685547
INFO:root:current mean train loss 6968.902134833645
INFO:root:current train perplexity242.6465301513672
INFO:root:current mean train loss 6960.497396374066
INFO:root:current train perplexity242.01295471191406
INFO:root:current mean train loss 6959.974979543329
INFO:root:current train perplexity242.5707550048828
INFO:root:current mean train loss 6954.756872972805
INFO:root:current train perplexity242.0144500732422
INFO:root:current mean train loss 6963.1107894719735
INFO:root:current train perplexity242.3008270263672
INFO:root:current mean train loss 6964.465708168019
INFO:root:current train perplexity242.67892456054688
INFO:root:current mean train loss 6961.119679502185
INFO:root:current train perplexity243.11279296875
INFO:root:current mean train loss 6960.743978045228
INFO:root:current train perplexity243.0828857421875
INFO:root:current mean train loss 6966.448549497378
INFO:root:current train perplexity243.2199249267578
INFO:root:current mean train loss 6963.993960568518
INFO:root:current train perplexity243.2572784423828
INFO:root:current mean train loss 6970.258251993781
INFO:root:current train perplexity243.55313110351562
INFO:root:current mean train loss 6966.069763418164
INFO:root:current train perplexity243.21424865722656
INFO:root:current mean train loss 6961.364828345267
INFO:root:current train perplexity243.2191162109375
INFO:root:current mean train loss 6963.844848226183
INFO:root:current train perplexity243.1096649169922
INFO:root:current mean train loss 6959.606367236298
INFO:root:current train perplexity242.68841552734375
INFO:root:current mean train loss 6957.784386367394
INFO:root:current train perplexity242.72463989257812
INFO:root:current mean train loss 6958.540145340262
INFO:root:current train perplexity242.6339111328125
INFO:root:current mean train loss 6959.152932718388
INFO:root:current train perplexity242.4856414794922

100%|██████████| 1/1 [08:05<00:00, 485.87s/it][A100%|██████████| 1/1 [08:05<00:00, 485.87s/it]
INFO:root:final mean train loss: 6956.294464126718
INFO:root:final train perplexity: 242.2312774658203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.57s/it][A100%|██████████| 1/1 [00:42<00:00, 42.57s/it]
INFO:root:eval mean loss: 6645.285467918883
INFO:root:eval perplexity: 216.47235107421875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.67s/it][A100%|██████████| 1/1 [00:41<00:00, 41.67s/it]
INFO:root:eval mean loss: 6738.937136386303
INFO:root:eval perplexity: 254.74758911132812
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/107
 54%|█████▎    | 107/200 [16:02:43<14:42:39, 569.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6933.054768880208
INFO:root:current train perplexity240.4136199951172
INFO:root:current mean train loss 6946.964698920816
INFO:root:current train perplexity242.34149169921875
INFO:root:current mean train loss 6932.8228950150515
INFO:root:current train perplexity243.4420166015625
INFO:root:current mean train loss 6927.238890833824
INFO:root:current train perplexity242.794677734375
INFO:root:current mean train loss 6942.337099796277
INFO:root:current train perplexity243.04324340820312
INFO:root:current mean train loss 6961.875497707529
INFO:root:current train perplexity243.29161071777344
INFO:root:current mean train loss 6957.795047500758
INFO:root:current train perplexity243.05413818359375
INFO:root:current mean train loss 6953.047355800618
INFO:root:current train perplexity242.8697052001953
INFO:root:current mean train loss 6948.997207007374
INFO:root:current train perplexity242.28407287597656
INFO:root:current mean train loss 6953.522448171977
INFO:root:current train perplexity242.14859008789062
INFO:root:current mean train loss 6955.60929921568
INFO:root:current train perplexity242.1820831298828
INFO:root:current mean train loss 6956.936599431183
INFO:root:current train perplexity241.5706787109375
INFO:root:current mean train loss 6955.234307650862
INFO:root:current train perplexity241.33029174804688
INFO:root:current mean train loss 6955.911420668745
INFO:root:current train perplexity241.01943969726562
INFO:root:current mean train loss 6956.187534778848
INFO:root:current train perplexity240.8080596923828
INFO:root:current mean train loss 6955.425080350893
INFO:root:current train perplexity240.85147094726562
INFO:root:current mean train loss 6955.114792266205
INFO:root:current train perplexity240.55523681640625
INFO:root:current mean train loss 6952.816318427586
INFO:root:current train perplexity240.42611694335938
INFO:root:current mean train loss 6952.3851061864
INFO:root:current train perplexity240.27711486816406
INFO:root:current mean train loss 6948.684414255979
INFO:root:current train perplexity240.0643310546875

100%|██████████| 1/1 [07:41<00:00, 461.55s/it][A100%|██████████| 1/1 [07:41<00:00, 461.55s/it]
INFO:root:final mean train loss: 6943.759827922104
INFO:root:final train perplexity: 239.84671020507812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.12s/it][A100%|██████████| 1/1 [00:45<00:00, 45.12s/it]
INFO:root:eval mean loss: 6642.3930179244235
INFO:root:eval perplexity: 215.96633911132812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.81s/it][A100%|██████████| 1/1 [00:40<00:00, 40.81s/it]
INFO:root:eval mean loss: 6736.335044049202
INFO:root:eval perplexity: 254.20323181152344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/108
 54%|█████▍    | 108/200 [16:11:53<14:24:10, 563.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6894.028152901786
INFO:root:current train perplexity235.4897918701172
INFO:root:current mean train loss 6939.838151041667
INFO:root:current train perplexity236.90528869628906
INFO:root:current mean train loss 6957.11424950133
INFO:root:current train perplexity237.99041748046875
INFO:root:current mean train loss 6941.010573111007
INFO:root:current train perplexity237.06744384765625
INFO:root:current mean train loss 6943.63560075431
INFO:root:current train perplexity237.40704345703125
INFO:root:current mean train loss 6947.334662492699
INFO:root:current train perplexity238.35804748535156
INFO:root:current mean train loss 6950.497507843258
INFO:root:current train perplexity238.4251251220703
INFO:root:current mean train loss 6962.971312313988
INFO:root:current train perplexity239.14871215820312
INFO:root:current mean train loss 6963.099926319236
INFO:root:current train perplexity239.71099853515625
INFO:root:current mean train loss 6958.720013786765
INFO:root:current train perplexity239.95835876464844
INFO:root:current mean train loss 6959.448058197464
INFO:root:current train perplexity240.1782684326172
INFO:root:current mean train loss 6957.8379727939155
INFO:root:current train perplexity240.27780151367188
INFO:root:current mean train loss 6954.192079168775
INFO:root:current train perplexity240.63783264160156
INFO:root:current mean train loss 6957.856632578418
INFO:root:current train perplexity240.73849487304688
INFO:root:current mean train loss 6954.788340456228
INFO:root:current train perplexity240.9109649658203
INFO:root:current mean train loss 6954.35400390625
INFO:root:current train perplexity240.9136199951172
INFO:root:current mean train loss 6955.584036338876
INFO:root:current train perplexity240.9916229248047
INFO:root:current mean train loss 6953.19812032826
INFO:root:current train perplexity241.0269012451172
INFO:root:current mean train loss 6952.3116064586175
INFO:root:current train perplexity241.17417907714844
INFO:root:current mean train loss 6953.108862493944
INFO:root:current train perplexity241.231689453125

100%|██████████| 1/1 [07:50<00:00, 470.35s/it][A100%|██████████| 1/1 [07:50<00:00, 470.35s/it]
INFO:root:final mean train loss: 6950.798906673522
INFO:root:final train perplexity: 241.18280029296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.78s/it][A100%|██████████| 1/1 [00:42<00:00, 42.78s/it]
INFO:root:eval mean loss: 6652.119601202349
INFO:root:eval perplexity: 217.6728515625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.28s/it][A100%|██████████| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 6747.234766317598
INFO:root:eval perplexity: 256.4913330078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/109
 55%|█████▍    | 109/200 [16:21:11<14:12:13, 561.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6982.38130070613
INFO:root:current train perplexity238.78225708007812
INFO:root:current mean train loss 6937.65717837685
INFO:root:current train perplexity238.0484161376953
INFO:root:current mean train loss 6932.152638268849
INFO:root:current train perplexity237.68637084960938
INFO:root:current mean train loss 6951.170358831232
INFO:root:current train perplexity238.47605895996094
INFO:root:current mean train loss 6954.798901583241
INFO:root:current train perplexity238.20693969726562
INFO:root:current mean train loss 6956.2780425583105
INFO:root:current train perplexity237.99281311035156
INFO:root:current mean train loss 6947.277957097153
INFO:root:current train perplexity238.26214599609375
INFO:root:current mean train loss 6935.262445977393
INFO:root:current train perplexity237.5339813232422
INFO:root:current mean train loss 6936.128992215009
INFO:root:current train perplexity237.26942443847656
INFO:root:current mean train loss 6936.269684094341
INFO:root:current train perplexity236.9958953857422
INFO:root:current mean train loss 6931.048113340661
INFO:root:current train perplexity236.88551330566406
INFO:root:current mean train loss 6928.030348036024
INFO:root:current train perplexity236.62631225585938
INFO:root:current mean train loss 6927.586318140974
INFO:root:current train perplexity236.42645263671875
INFO:root:current mean train loss 6924.845175839035
INFO:root:current train perplexity236.10244750976562
INFO:root:current mean train loss 6927.758505240617
INFO:root:current train perplexity236.0406494140625
INFO:root:current mean train loss 6923.841352325125
INFO:root:current train perplexity235.77157592773438
INFO:root:current mean train loss 6926.422356483145
INFO:root:current train perplexity235.90945434570312
INFO:root:current mean train loss 6922.416313554599
INFO:root:current train perplexity235.6483917236328
INFO:root:current mean train loss 6920.154738753712
INFO:root:current train perplexity235.54537963867188
INFO:root:current mean train loss 6921.959368846456
INFO:root:current train perplexity235.63243103027344

100%|██████████| 1/1 [08:02<00:00, 482.17s/it][A100%|██████████| 1/1 [08:02<00:00, 482.17s/it]
INFO:root:final mean train loss: 6921.859491468498
INFO:root:final train perplexity: 235.73684692382812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.89s/it][A100%|██████████| 1/1 [00:43<00:00, 43.89s/it]
INFO:root:eval mean loss: 6648.4778905557405
INFO:root:eval perplexity: 217.03224182128906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.47s/it][A100%|██████████| 1/1 [00:45<00:00, 45.52s/it]
INFO:root:eval mean loss: 6739.232078173482
INFO:root:eval perplexity: 254.80941772460938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/110
 55%|█████▌    | 110/200 [16:30:45<14:08:26, 565.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6945.615496206975
INFO:root:current train perplexity238.8413543701172
INFO:root:current mean train loss 6894.852331037352
INFO:root:current train perplexity234.40162658691406
INFO:root:current mean train loss 6892.883567611524
INFO:root:current train perplexity233.98765563964844
INFO:root:current mean train loss 6900.171583883807
INFO:root:current train perplexity234.14022827148438
INFO:root:current mean train loss 6899.55622626266
INFO:root:current train perplexity234.1217041015625
INFO:root:current mean train loss 6907.515326367531
INFO:root:current train perplexity234.9371795654297
INFO:root:current mean train loss 6907.113017038023
INFO:root:current train perplexity234.98983764648438
INFO:root:current mean train loss 6904.347912137313
INFO:root:current train perplexity234.95431518554688
INFO:root:current mean train loss 6915.552130906573
INFO:root:current train perplexity235.42904663085938
INFO:root:current mean train loss 6906.268248314951
INFO:root:current train perplexity235.03018188476562
INFO:root:current mean train loss 6909.636820151719
INFO:root:current train perplexity235.32130432128906
INFO:root:current mean train loss 6914.465832843242
INFO:root:current train perplexity235.51931762695312
INFO:root:current mean train loss 6914.035523326685
INFO:root:current train perplexity235.64344787597656
INFO:root:current mean train loss 6913.9826669073
INFO:root:current train perplexity235.5918731689453
INFO:root:current mean train loss 6917.455372955135
INFO:root:current train perplexity235.61468505859375
INFO:root:current mean train loss 6920.281824173936
INFO:root:current train perplexity235.70492553710938
INFO:root:current mean train loss 6921.095986907392
INFO:root:current train perplexity235.6903076171875
INFO:root:current mean train loss 6922.402779587249
INFO:root:current train perplexity235.77517700195312
INFO:root:current mean train loss 6920.312902590373
INFO:root:current train perplexity235.68389892578125
INFO:root:current mean train loss 6924.380432841861
INFO:root:current train perplexity235.858154296875

100%|██████████| 1/1 [08:03<00:00, 483.51s/it][A100%|██████████| 1/1 [08:03<00:00, 483.51s/it]
INFO:root:final mean train loss: 6922.759758361109
INFO:root:final train perplexity: 235.9044952392578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.73s/it][A100%|██████████| 1/1 [00:44<00:00, 44.73s/it]
INFO:root:eval mean loss: 6647.54214282746
INFO:root:eval perplexity: 216.86805725097656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.75s/it][A100%|██████████| 1/1 [00:43<00:00, 43.75s/it]
INFO:root:eval mean loss: 6737.027528154089
INFO:root:eval perplexity: 254.34812927246094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/111
 56%|█████▌    | 111/200 [16:40:19<14:02:58, 568.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7006.2519020258
INFO:root:current train perplexity238.6807098388672
INFO:root:current mean train loss 6947.294507098454
INFO:root:current train perplexity236.2242889404297
INFO:root:current mean train loss 6945.086186762456
INFO:root:current train perplexity236.37326049804688
INFO:root:current mean train loss 6929.189539143458
INFO:root:current train perplexity236.01116943359375
INFO:root:current mean train loss 6930.786765769676
INFO:root:current train perplexity235.67300415039062
INFO:root:current mean train loss 6921.595442319486
INFO:root:current train perplexity235.116943359375
INFO:root:current mean train loss 6925.007137020545
INFO:root:current train perplexity235.05763244628906
INFO:root:current mean train loss 6921.034546209049
INFO:root:current train perplexity234.82540893554688
INFO:root:current mean train loss 6923.606139447835
INFO:root:current train perplexity234.76304626464844
INFO:root:current mean train loss 6923.640637380357
INFO:root:current train perplexity234.86270141601562
INFO:root:current mean train loss 6922.005372892208
INFO:root:current train perplexity234.963623046875
INFO:root:current mean train loss 6920.884222175379
INFO:root:current train perplexity235.1595458984375
INFO:root:current mean train loss 6924.460396441849
INFO:root:current train perplexity235.19430541992188
INFO:root:current mean train loss 6921.5318407991745
INFO:root:current train perplexity235.04071044921875
INFO:root:current mean train loss 6914.229864477309
INFO:root:current train perplexity234.84645080566406
INFO:root:current mean train loss 6913.171679810648
INFO:root:current train perplexity234.65414428710938
INFO:root:current mean train loss 6912.8585119643385
INFO:root:current train perplexity234.6002197265625
INFO:root:current mean train loss 6916.83935546875
INFO:root:current train perplexity234.6158905029297
INFO:root:current mean train loss 6916.116951903003
INFO:root:current train perplexity234.5463104248047

100%|██████████| 1/1 [08:00<00:00, 480.54s/it][A100%|██████████| 1/1 [08:00<00:00, 480.54s/it]
INFO:root:final mean train loss: 6915.803785053817
INFO:root:final train perplexity: 234.6128692626953
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.67s/it][A100%|██████████| 1/1 [00:44<00:00, 44.67s/it]
INFO:root:eval mean loss: 6640.352369376108
INFO:root:eval perplexity: 215.60980224609375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.15s/it][A100%|██████████| 1/1 [00:43<00:00, 43.15s/it]
INFO:root:eval mean loss: 6732.697540932513
INFO:root:eval perplexity: 253.44424438476562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/112
 56%|█████▌    | 112/200 [16:49:50<13:54:36, 569.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6714.771158854167
INFO:root:current train perplexity216.49176025390625
INFO:root:current mean train loss 6866.138894682949
INFO:root:current train perplexity233.62777709960938
INFO:root:current mean train loss 6882.853467518473
INFO:root:current train perplexity232.58261108398438
INFO:root:current mean train loss 6923.285049891708
INFO:root:current train perplexity233.9495086669922
INFO:root:current mean train loss 6915.538627529854
INFO:root:current train perplexity233.79371643066406
INFO:root:current mean train loss 6916.442751692967
INFO:root:current train perplexity233.7751007080078
INFO:root:current mean train loss 6917.156000595978
INFO:root:current train perplexity233.76953125
INFO:root:current mean train loss 6910.641900921275
INFO:root:current train perplexity233.1404571533203
INFO:root:current mean train loss 6916.889395479841
INFO:root:current train perplexity233.4312286376953
INFO:root:current mean train loss 6909.1574055448855
INFO:root:current train perplexity232.95765686035156
INFO:root:current mean train loss 6909.188173273149
INFO:root:current train perplexity233.169921875
INFO:root:current mean train loss 6907.689407971158
INFO:root:current train perplexity233.39996337890625
INFO:root:current mean train loss 6909.116980092088
INFO:root:current train perplexity233.56573486328125
INFO:root:current mean train loss 6907.048987762615
INFO:root:current train perplexity233.58599853515625
INFO:root:current mean train loss 6909.3264182777975
INFO:root:current train perplexity233.76394653320312
INFO:root:current mean train loss 6908.457614393608
INFO:root:current train perplexity233.68014526367188
INFO:root:current mean train loss 6910.234770072228
INFO:root:current train perplexity233.75625610351562
INFO:root:current mean train loss 6908.768355705006
INFO:root:current train perplexity233.60226440429688
INFO:root:current mean train loss 6908.630416049206
INFO:root:current train perplexity233.60816955566406
INFO:root:current mean train loss 6911.188855025371
INFO:root:current train perplexity233.7353057861328

100%|██████████| 1/1 [08:05<00:00, 485.30s/it][A100%|██████████| 1/1 [08:05<00:00, 485.30s/it]
INFO:root:final mean train loss: 6910.713843549073
INFO:root:final train perplexity: 233.67245483398438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.96s/it][A100%|██████████| 1/1 [00:42<00:00, 42.96s/it]
INFO:root:eval mean loss: 6617.247889309065
INFO:root:eval perplexity: 211.61624145507812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.82s/it][A100%|██████████| 1/1 [00:41<00:00, 41.82s/it]
INFO:root:eval mean loss: 6706.330375941932
INFO:root:eval perplexity: 248.0094451904297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/113
 56%|█████▋    | 113/200 [16:59:23<13:46:41, 570.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6963.885278320313
INFO:root:current train perplexity227.12203979492188
INFO:root:current mean train loss 6928.557604980469
INFO:root:current train perplexity230.89260864257812
INFO:root:current mean train loss 6902.577667791194
INFO:root:current train perplexity230.65040588378906
INFO:root:current mean train loss 6915.917126464844
INFO:root:current train perplexity231.29051208496094
INFO:root:current mean train loss 6930.091858491443
INFO:root:current train perplexity231.74722290039062
INFO:root:current mean train loss 6927.849938964844
INFO:root:current train perplexity232.31326293945312
INFO:root:current mean train loss 6927.857421087449
INFO:root:current train perplexity232.7955322265625
INFO:root:current mean train loss 6923.204406738281
INFO:root:current train perplexity232.81240844726562
INFO:root:current mean train loss 6919.78208305545
INFO:root:current train perplexity233.15768432617188
INFO:root:current mean train loss 6916.714739724865
INFO:root:current train perplexity233.23385620117188
INFO:root:current mean train loss 6907.912292241115
INFO:root:current train perplexity232.9290008544922
INFO:root:current mean train loss 6907.195803833008
INFO:root:current train perplexity232.9955291748047
INFO:root:current mean train loss 6900.566138896004
INFO:root:current train perplexity232.82205200195312
INFO:root:current mean train loss 6901.312823671283
INFO:root:current train perplexity232.75946044921875
INFO:root:current mean train loss 6904.834630144146
INFO:root:current train perplexity232.85536193847656
INFO:root:current mean train loss 6904.799257940995
INFO:root:current train perplexity232.89523315429688
INFO:root:current mean train loss 6906.945041232639
INFO:root:current train perplexity233.08876037597656
INFO:root:current mean train loss 6907.633751873637
INFO:root:current train perplexity233.1594696044922
INFO:root:current mean train loss 6907.901147729224
INFO:root:current train perplexity233.17247009277344
INFO:root:current mean train loss 6908.164464060465
INFO:root:current train perplexity233.23675537109375

100%|██████████| 1/1 [07:57<00:00, 477.81s/it][A100%|██████████| 1/1 [07:57<00:00, 477.81s/it]
INFO:root:final mean train loss: 6908.662068869571
INFO:root:final train perplexity: 233.29437255859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.06s/it][A100%|██████████| 1/1 [00:43<00:00, 43.06s/it]
INFO:root:eval mean loss: 6617.892010195035
INFO:root:eval perplexity: 211.7265625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.18s/it][A100%|██████████| 1/1 [00:41<00:00, 41.18s/it]
INFO:root:eval mean loss: 6705.0431496911015
INFO:root:eval perplexity: 247.74705505371094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/114
 57%|█████▋    | 114/200 [17:08:48<13:34:47, 568.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6855.376807960304
INFO:root:current train perplexity232.76068115234375
INFO:root:current mean train loss 6923.133582344891
INFO:root:current train perplexity235.61053466796875
INFO:root:current mean train loss 6905.712740226134
INFO:root:current train perplexity235.0009307861328
INFO:root:current mean train loss 6898.19282617767
INFO:root:current train perplexity234.93650817871094
INFO:root:current mean train loss 6911.613599694293
INFO:root:current train perplexity234.8128662109375
INFO:root:current mean train loss 6914.457338585312
INFO:root:current train perplexity234.1848907470703
INFO:root:current mean train loss 6902.407023431368
INFO:root:current train perplexity233.61953735351562
INFO:root:current mean train loss 6898.254620452425
INFO:root:current train perplexity233.46351623535156
INFO:root:current mean train loss 6899.390141385716
INFO:root:current train perplexity233.27545166015625
INFO:root:current mean train loss 6904.237079046325
INFO:root:current train perplexity233.2815704345703
INFO:root:current mean train loss 6901.725541205852
INFO:root:current train perplexity233.2935791015625
INFO:root:current mean train loss 6897.000113374011
INFO:root:current train perplexity232.9203338623047
INFO:root:current mean train loss 6899.390346715213
INFO:root:current train perplexity232.92654418945312
INFO:root:current mean train loss 6900.484559429343
INFO:root:current train perplexity233.0443115234375
INFO:root:current mean train loss 6899.471120049909
INFO:root:current train perplexity232.89390563964844
INFO:root:current mean train loss 6903.411937825309
INFO:root:current train perplexity232.81085205078125
INFO:root:current mean train loss 6906.002263334224
INFO:root:current train perplexity233.0543212890625
INFO:root:current mean train loss 6907.534170692105
INFO:root:current train perplexity233.0360870361328
INFO:root:current mean train loss 6908.365401831281
INFO:root:current train perplexity233.0153045654297
INFO:root:current mean train loss 6908.641771717298
INFO:root:current train perplexity233.1046600341797

100%|██████████| 1/1 [08:05<00:00, 485.10s/it][A100%|██████████| 1/1 [08:05<00:00, 485.10s/it]
INFO:root:final mean train loss: 6908.202592273583
INFO:root:final train perplexity: 233.2098388671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.69s/it][A100%|██████████| 1/1 [00:43<00:00, 43.69s/it]
INFO:root:eval mean loss: 6632.165821697695
INFO:root:eval perplexity: 214.18617248535156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.94s/it][A100%|██████████| 1/1 [00:41<00:00, 41.95s/it]
INFO:root:eval mean loss: 6719.44019247285
INFO:root:eval perplexity: 250.69692993164062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/115
 57%|█████▊    | 115/200 [17:18:21<13:27:25, 569.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6893.34923864294
INFO:root:current train perplexity232.31448364257812
INFO:root:current mean train loss 6917.93525834517
INFO:root:current train perplexity233.58599853515625
INFO:root:current mean train loss 6921.874263733391
INFO:root:current train perplexity234.007080078125
INFO:root:current mean train loss 6917.3884994593045
INFO:root:current train perplexity234.292236328125
INFO:root:current mean train loss 6905.921341547357
INFO:root:current train perplexity233.92251586914062
INFO:root:current mean train loss 6918.289417693762
INFO:root:current train perplexity234.42152404785156
INFO:root:current mean train loss 6912.833181772028
INFO:root:current train perplexity234.50582885742188
INFO:root:current mean train loss 6915.233452834881
INFO:root:current train perplexity234.55894470214844
INFO:root:current mean train loss 6921.565844783739
INFO:root:current train perplexity235.3083953857422
INFO:root:current mean train loss 6917.797897114943
INFO:root:current train perplexity235.36337280273438
INFO:root:current mean train loss 6917.426373765862
INFO:root:current train perplexity235.4747314453125
INFO:root:current mean train loss 6918.745384599762
INFO:root:current train perplexity235.50135803222656
INFO:root:current mean train loss 6918.541287411533
INFO:root:current train perplexity235.39952087402344
INFO:root:current mean train loss 6918.97457078852
INFO:root:current train perplexity235.3440704345703
INFO:root:current mean train loss 6917.82327039632
INFO:root:current train perplexity235.14675903320312
INFO:root:current mean train loss 6919.583100818452
INFO:root:current train perplexity235.21786499023438
INFO:root:current mean train loss 6920.149799314635
INFO:root:current train perplexity235.3042449951172
INFO:root:current mean train loss 6921.693077374483
INFO:root:current train perplexity235.34767150878906
INFO:root:current mean train loss 6923.189681727009
INFO:root:current train perplexity235.34239196777344
INFO:root:current mean train loss 6922.796190806519
INFO:root:current train perplexity235.32803344726562

100%|██████████| 1/1 [08:03<00:00, 483.33s/it][A100%|██████████| 1/1 [08:03<00:00, 483.33s/it]
INFO:root:final mean train loss: 6919.835411298774
INFO:root:final train perplexity: 235.36068725585938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.01s/it][A100%|██████████| 1/1 [00:44<00:00, 44.01s/it]
INFO:root:eval mean loss: 6652.10270355441
INFO:root:eval perplexity: 217.66983032226562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.44s/it][A100%|██████████| 1/1 [00:43<00:00, 43.44s/it]
INFO:root:eval mean loss: 6737.1452567459
INFO:root:eval perplexity: 254.3725128173828
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/116
 58%|█████▊    | 116/200 [17:27:57<13:20:27, 571.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6929.0136168573945
INFO:root:current train perplexity236.9100341796875
INFO:root:current mean train loss 6930.287963153326
INFO:root:current train perplexity236.2693634033203
INFO:root:current mean train loss 6936.825979084986
INFO:root:current train perplexity235.2888641357422
INFO:root:current mean train loss 6954.515954030492
INFO:root:current train perplexity236.0902862548828
INFO:root:current mean train loss 6939.771092505971
INFO:root:current train perplexity235.63771057128906
INFO:root:current mean train loss 6945.708111283658
INFO:root:current train perplexity236.0588836669922
INFO:root:current mean train loss 6934.023315247765
INFO:root:current train perplexity235.33465576171875
INFO:root:current mean train loss 6929.096007113327
INFO:root:current train perplexity235.3288116455078
INFO:root:current mean train loss 6938.4130730437355
INFO:root:current train perplexity235.44342041015625
INFO:root:current mean train loss 6940.992950345166
INFO:root:current train perplexity235.87603759765625
INFO:root:current mean train loss 6934.026467943949
INFO:root:current train perplexity235.49224853515625
INFO:root:current mean train loss 6938.216047982493
INFO:root:current train perplexity235.49147033691406
INFO:root:current mean train loss 6938.011682253762
INFO:root:current train perplexity235.61749267578125
INFO:root:current mean train loss 6938.18388650506
INFO:root:current train perplexity235.64154052734375
INFO:root:current mean train loss 6929.767466593728
INFO:root:current train perplexity235.41232299804688
INFO:root:current mean train loss 6926.587658761338
INFO:root:current train perplexity235.5370635986328
INFO:root:current mean train loss 6925.868222212092
INFO:root:current train perplexity235.73268127441406
INFO:root:current mean train loss 6926.94995958101
INFO:root:current train perplexity235.7266082763672
INFO:root:current mean train loss 6926.051175319849
INFO:root:current train perplexity235.70616149902344
INFO:root:current mean train loss 6924.202120939165
INFO:root:current train perplexity235.73841857910156

100%|██████████| 1/1 [08:08<00:00, 488.29s/it][A100%|██████████| 1/1 [08:08<00:00, 488.29s/it]
INFO:root:final mean train loss: 6921.662398576375
INFO:root:final train perplexity: 235.7001953125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.17s/it][A100%|██████████| 1/1 [00:46<00:00, 46.17s/it]
INFO:root:eval mean loss: 6631.933675130208
INFO:root:eval perplexity: 214.1460418701172
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.13s/it][A100%|██████████| 1/1 [00:41<00:00, 41.13s/it]
INFO:root:eval mean loss: 6717.724113302028
INFO:root:eval perplexity: 250.3434600830078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/117
 58%|█████▊    | 117/200 [17:37:37<13:14:16, 574.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6936.9241499467325
INFO:root:current train perplexity236.1675262451172
INFO:root:current mean train loss 6938.439645320811
INFO:root:current train perplexity235.3070526123047
INFO:root:current mean train loss 6913.461691962348
INFO:root:current train perplexity235.4431915283203
INFO:root:current mean train loss 6925.121975928238
INFO:root:current train perplexity235.76876831054688
INFO:root:current mean train loss 6922.580421322682
INFO:root:current train perplexity235.69908142089844
INFO:root:current mean train loss 6933.224167596726
INFO:root:current train perplexity236.25808715820312
INFO:root:current mean train loss 6936.3252755098565
INFO:root:current train perplexity236.4921875
INFO:root:current mean train loss 6932.731910047192
INFO:root:current train perplexity236.98471069335938
INFO:root:current mean train loss 6929.958196966498
INFO:root:current train perplexity237.16796875
INFO:root:current mean train loss 6936.371317627942
INFO:root:current train perplexity237.2180633544922
INFO:root:current mean train loss 6938.308945599724
INFO:root:current train perplexity237.31739807128906
INFO:root:current mean train loss 6934.438938128025
INFO:root:current train perplexity236.7877197265625
INFO:root:current mean train loss 6932.520011949243
INFO:root:current train perplexity236.43919372558594
INFO:root:current mean train loss 6925.549464860635
INFO:root:current train perplexity236.01780700683594
INFO:root:current mean train loss 6920.099811184791
INFO:root:current train perplexity235.79676818847656
INFO:root:current mean train loss 6923.8065314689275
INFO:root:current train perplexity235.82711791992188
INFO:root:current mean train loss 6924.233551748556
INFO:root:current train perplexity235.73358154296875
INFO:root:current mean train loss 6926.169541190401
INFO:root:current train perplexity235.4992218017578
INFO:root:current mean train loss 6924.188652426509
INFO:root:current train perplexity235.43824768066406

100%|██████████| 1/1 [08:04<00:00, 484.71s/it][A100%|██████████| 1/1 [08:04<00:00, 484.71s/it]
INFO:root:final mean train loss: 6918.536585389877
INFO:root:final train perplexity: 235.11952209472656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.98s/it][A100%|██████████| 1/1 [00:42<00:00, 42.98s/it]
INFO:root:eval mean loss: 6620.2804102255095
INFO:root:eval perplexity: 212.1361541748047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.72s/it][A100%|██████████| 1/1 [00:42<00:00, 42.72s/it]
INFO:root:eval mean loss: 6712.77783462849
INFO:root:eval perplexity: 249.3275146484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/118
 59%|█████▉    | 118/200 [17:47:10<13:04:15, 573.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6802.98515625
INFO:root:current train perplexity218.595458984375
INFO:root:current mean train loss 6828.195107886905
INFO:root:current train perplexity229.6374969482422
INFO:root:current mean train loss 6879.61016339558
INFO:root:current train perplexity230.4713134765625
INFO:root:current mean train loss 6898.495858414447
INFO:root:current train perplexity231.43128967285156
INFO:root:current mean train loss 6895.109196566358
INFO:root:current train perplexity231.1461944580078
INFO:root:current mean train loss 6883.344256652228
INFO:root:current train perplexity231.20693969726562
INFO:root:current mean train loss 6895.726483406508
INFO:root:current train perplexity232.18235778808594
INFO:root:current mean train loss 6898.712253435284
INFO:root:current train perplexity232.8551483154297
INFO:root:current mean train loss 6892.9978715789985
INFO:root:current train perplexity232.78309631347656
INFO:root:current mean train loss 6895.791072815953
INFO:root:current train perplexity233.06631469726562
INFO:root:current mean train loss 6903.205420164801
INFO:root:current train perplexity233.40061950683594
INFO:root:current mean train loss 6901.602880638433
INFO:root:current train perplexity233.52487182617188
INFO:root:current mean train loss 6901.92395333571
INFO:root:current train perplexity233.62298583984375
INFO:root:current mean train loss 6906.5988835009575
INFO:root:current train perplexity233.9922332763672
INFO:root:current mean train loss 6907.272725756227
INFO:root:current train perplexity233.8775634765625
INFO:root:current mean train loss 6904.067624195391
INFO:root:current train perplexity233.85760498046875
INFO:root:current mean train loss 6907.107821322527
INFO:root:current train perplexity234.01600646972656
INFO:root:current mean train loss 6912.777655333578
INFO:root:current train perplexity234.13174438476562
INFO:root:current mean train loss 6907.120657137292
INFO:root:current train perplexity233.96713256835938
INFO:root:current mean train loss 6913.209550576197
INFO:root:current train perplexity234.0703582763672

100%|██████████| 1/1 [08:07<00:00, 487.70s/it][A100%|██████████| 1/1 [08:07<00:00, 487.70s/it]
INFO:root:final mean train loss: 6911.935689936728
INFO:root:final train perplexity: 233.89785766601562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.86s/it][A100%|██████████| 1/1 [00:45<00:00, 45.86s/it]
INFO:root:eval mean loss: 6622.459470924756
INFO:root:eval perplexity: 212.5105438232422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.32s/it][A100%|██████████| 1/1 [00:43<00:00, 43.32s/it]
INFO:root:eval mean loss: 6713.834624161957
INFO:root:eval perplexity: 249.54421997070312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/119
 60%|█████▉    | 119/200 [17:56:49<12:57:00, 575.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6882.576793323864
INFO:root:current train perplexity232.1146240234375
INFO:root:current mean train loss 6893.10742987961
INFO:root:current train perplexity231.54100036621094
INFO:root:current mean train loss 6912.052747571791
INFO:root:current train perplexity231.587158203125
INFO:root:current mean train loss 6920.751184309492
INFO:root:current train perplexity231.9404754638672
INFO:root:current mean train loss 6932.994559482376
INFO:root:current train perplexity232.11660766601562
INFO:root:current mean train loss 6917.626329210069
INFO:root:current train perplexity231.864501953125
INFO:root:current mean train loss 6923.10217795418
INFO:root:current train perplexity232.36178588867188
INFO:root:current mean train loss 6923.346493031509
INFO:root:current train perplexity232.5640869140625
INFO:root:current mean train loss 6923.727116123023
INFO:root:current train perplexity232.7362518310547
INFO:root:current mean train loss 6921.503001711632
INFO:root:current train perplexity232.72715759277344
INFO:root:current mean train loss 6915.972414020456
INFO:root:current train perplexity232.9915313720703
INFO:root:current mean train loss 6913.076556146251
INFO:root:current train perplexity233.17825317382812
INFO:root:current mean train loss 6916.545672277772
INFO:root:current train perplexity233.83718872070312
INFO:root:current mean train loss 6917.174966462982
INFO:root:current train perplexity234.17462158203125
INFO:root:current mean train loss 6916.816352339904
INFO:root:current train perplexity234.40878295898438
INFO:root:current mean train loss 6916.3686510604875
INFO:root:current train perplexity234.6781005859375
INFO:root:current mean train loss 6914.723323647985
INFO:root:current train perplexity234.90805053710938
INFO:root:current mean train loss 6916.781866731544
INFO:root:current train perplexity235.17849731445312
INFO:root:current mean train loss 6916.512619738783
INFO:root:current train perplexity235.2263946533203
INFO:root:current mean train loss 6918.504905168769
INFO:root:current train perplexity235.30906677246094

100%|██████████| 1/1 [07:54<00:00, 474.06s/it][A100%|██████████| 1/1 [07:54<00:00, 474.06s/it]
INFO:root:final mean train loss: 6921.099707375977
INFO:root:final train perplexity: 235.595703125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.80s/it][A100%|██████████| 1/1 [00:43<00:00, 43.80s/it]
INFO:root:eval mean loss: 6679.902082294437
INFO:root:eval perplexity: 222.6218719482422
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.77s/it][A100%|██████████| 1/1 [00:42<00:00, 42.77s/it]
INFO:root:eval mean loss: 6768.317545572917
INFO:root:eval perplexity: 260.9757080078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/120
 60%|██████    | 120/200 [18:06:12<12:42:25, 571.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6943.093487079327
INFO:root:current train perplexity239.48855590820312
INFO:root:current mean train loss 6884.931352574191
INFO:root:current train perplexity236.19749450683594
INFO:root:current mean train loss 6903.651751274843
INFO:root:current train perplexity237.35418701171875
INFO:root:current mean train loss 6895.295160974373
INFO:root:current train perplexity236.65609741210938
INFO:root:current mean train loss 6908.5490583624005
INFO:root:current train perplexity237.57997131347656
INFO:root:current mean train loss 6930.991837821776
INFO:root:current train perplexity237.59423828125
INFO:root:current mean train loss 6926.428094281837
INFO:root:current train perplexity237.2284698486328
INFO:root:current mean train loss 6927.849058984903
INFO:root:current train perplexity237.57431030273438
INFO:root:current mean train loss 6933.831835122728
INFO:root:current train perplexity237.85824584960938
INFO:root:current mean train loss 6930.672848442492
INFO:root:current train perplexity237.38531494140625
INFO:root:current mean train loss 6930.674246853194
INFO:root:current train perplexity237.18130493164062
INFO:root:current mean train loss 6930.004838657128
INFO:root:current train perplexity236.8938751220703
INFO:root:current mean train loss 6929.902961687853
INFO:root:current train perplexity236.7352294921875
INFO:root:current mean train loss 6929.939305437243
INFO:root:current train perplexity236.5240020751953
INFO:root:current mean train loss 6928.023677738447
INFO:root:current train perplexity236.75192260742188
INFO:root:current mean train loss 6922.933222542032
INFO:root:current train perplexity236.68838500976562
INFO:root:current mean train loss 6922.235706080308
INFO:root:current train perplexity236.6061248779297
INFO:root:current mean train loss 6925.496756116572
INFO:root:current train perplexity236.84689331054688
INFO:root:current mean train loss 6927.893957473067
INFO:root:current train perplexity237.0269775390625
INFO:root:current mean train loss 6930.281480416371
INFO:root:current train perplexity237.10679626464844

100%|██████████| 1/1 [08:08<00:00, 488.67s/it][A100%|██████████| 1/1 [08:08<00:00, 488.67s/it]
INFO:root:final mean train loss: 6929.345836583617
INFO:root:final train perplexity: 237.13381958007812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:46<00:00, 46.15s/it][A100%|██████████| 1/1 [00:46<00:00, 46.15s/it]
INFO:root:eval mean loss: 6686.758636691046
INFO:root:eval perplexity: 223.86050415039062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.80s/it][A100%|██████████| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 6775.751066600177
INFO:root:eval perplexity: 262.5758361816406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/121
 60%|██████    | 121/200 [18:15:52<12:35:52, 574.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6828.513706752232
INFO:root:current train perplexity238.42898559570312
INFO:root:current mean train loss 6874.7759195963545
INFO:root:current train perplexity238.01153564453125
INFO:root:current mean train loss 6890.185363769531
INFO:root:current train perplexity238.13299560546875
INFO:root:current mean train loss 6893.200702795822
INFO:root:current train perplexity238.04637145996094
INFO:root:current mean train loss 6896.111911706757
INFO:root:current train perplexity238.7623291015625
INFO:root:current mean train loss 6915.9867953293615
INFO:root:current train perplexity238.8324737548828
INFO:root:current mean train loss 6923.34740913205
INFO:root:current train perplexity239.0117950439453
INFO:root:current mean train loss 6928.993047159185
INFO:root:current train perplexity239.03265380859375
INFO:root:current mean train loss 6930.928206114011
INFO:root:current train perplexity238.905029296875
INFO:root:current mean train loss 6931.452304217606
INFO:root:current train perplexity238.7183837890625
INFO:root:current mean train loss 6925.168107466264
INFO:root:current train perplexity238.45445251464844
INFO:root:current mean train loss 6920.02029281669
INFO:root:current train perplexity238.06011962890625
INFO:root:current mean train loss 6922.727126200488
INFO:root:current train perplexity238.08860778808594
INFO:root:current mean train loss 6921.432621508573
INFO:root:current train perplexity238.090087890625
INFO:root:current mean train loss 6928.061689104353
INFO:root:current train perplexity238.30088806152344
INFO:root:current mean train loss 6935.486267560552
INFO:root:current train perplexity238.55328369140625
INFO:root:current mean train loss 6934.536090058405
INFO:root:current train perplexity238.34713745117188
INFO:root:current mean train loss 6938.234211776143
INFO:root:current train perplexity238.53257751464844
INFO:root:current mean train loss 6935.442003973599
INFO:root:current train perplexity238.60833740234375
INFO:root:current mean train loss 6937.454622795245
INFO:root:current train perplexity238.767578125

100%|██████████| 1/1 [08:03<00:00, 483.43s/it][A100%|██████████| 1/1 [08:03<00:00, 483.43s/it]
INFO:root:final mean train loss: 6938.493103119681
INFO:root:final train perplexity: 238.85194396972656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:45<00:00, 45.62s/it][A100%|██████████| 1/1 [00:45<00:00, 45.62s/it]
INFO:root:eval mean loss: 6681.807241453346
INFO:root:eval perplexity: 222.96543884277344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.75s/it][A100%|██████████| 1/1 [00:41<00:00, 41.75s/it]
INFO:root:eval mean loss: 6767.874599159187
INFO:root:eval perplexity: 260.88079833984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/122
 61%|██████    | 122/200 [18:25:25<12:25:59, 573.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6920.231017230308
INFO:root:current train perplexity235.14239501953125
INFO:root:current mean train loss 6907.409715950144
INFO:root:current train perplexity235.9508514404297
INFO:root:current mean train loss 6921.950214986836
INFO:root:current train perplexity237.2490692138672
INFO:root:current mean train loss 6921.683860799263
INFO:root:current train perplexity237.3487548828125
INFO:root:current mean train loss 6924.763891756408
INFO:root:current train perplexity237.90736389160156
INFO:root:current mean train loss 6935.132718763634
INFO:root:current train perplexity238.5440673828125
INFO:root:current mean train loss 6929.3118579065285
INFO:root:current train perplexity238.53314208984375
INFO:root:current mean train loss 6929.887946085664
INFO:root:current train perplexity238.89854431152344
INFO:root:current mean train loss 6931.147345718786
INFO:root:current train perplexity239.04428100585938
INFO:root:current mean train loss 6930.4853746467115
INFO:root:current train perplexity239.19581604003906
INFO:root:current mean train loss 6930.246176571237
INFO:root:current train perplexity239.22262573242188
INFO:root:current mean train loss 6930.767706335251
INFO:root:current train perplexity239.0428009033203
INFO:root:current mean train loss 6934.453562266791
INFO:root:current train perplexity239.27601623535156
INFO:root:current mean train loss 6938.897475873999
INFO:root:current train perplexity239.57717895507812
INFO:root:current mean train loss 6943.693241033923
INFO:root:current train perplexity239.82510375976562
INFO:root:current mean train loss 6944.6676064968415
INFO:root:current train perplexity239.75363159179688
INFO:root:current mean train loss 6943.224613169175
INFO:root:current train perplexity239.46754455566406
INFO:root:current mean train loss 6942.318468983538
INFO:root:current train perplexity239.406005859375
INFO:root:current mean train loss 6942.612206666277
INFO:root:current train perplexity239.39344787597656
INFO:root:current mean train loss 6943.560681505005
INFO:root:current train perplexity239.44618225097656

100%|██████████| 1/1 [08:02<00:00, 482.77s/it][A100%|██████████| 1/1 [08:02<00:00, 482.78s/it]
INFO:root:final mean train loss: 6941.440152428454
INFO:root:final train perplexity: 239.40794372558594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:44<00:00, 44.20s/it][A100%|██████████| 1/1 [00:44<00:00, 44.20s/it]
INFO:root:eval mean loss: 6684.326859278036
INFO:root:eval perplexity: 223.42051696777344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.66s/it][A100%|██████████| 1/1 [00:40<00:00, 40.66s/it]
INFO:root:eval mean loss: 6769.187596097906
INFO:root:eval perplexity: 261.1626892089844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/123
 62%|██████▏   | 123/200 [18:34:55<12:15:05, 572.80s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6963.662022569444
INFO:root:current train perplexity237.53726196289062
INFO:root:current mean train loss 6937.459025493421
INFO:root:current train perplexity237.71414184570312
INFO:root:current mean train loss 6942.585669787177
INFO:root:current train perplexity238.42727661132812
INFO:root:current mean train loss 6936.894279597356
INFO:root:current train perplexity238.404541015625
INFO:root:current mean train loss 6957.998153499681
INFO:root:current train perplexity238.88771057128906
INFO:root:current mean train loss 6948.46340207892
INFO:root:current train perplexity238.94252014160156
INFO:root:current mean train loss 6950.317370782383
INFO:root:current train perplexity238.9556121826172
INFO:root:current mean train loss 6953.476352971717
INFO:root:current train perplexity239.3363800048828
INFO:root:current mean train loss 6956.950158005618
INFO:root:current train perplexity240.00218200683594
INFO:root:current mean train loss 6958.132690183081
INFO:root:current train perplexity240.15948486328125
INFO:root:current mean train loss 6957.648334916141
INFO:root:current train perplexity240.48585510253906
INFO:root:current mean train loss 6955.261463941045
INFO:root:current train perplexity240.90269470214844
INFO:root:current mean train loss 6963.403845309472
INFO:root:current train perplexity241.32786560058594
INFO:root:current mean train loss 6961.072985400742
INFO:root:current train perplexity241.52565002441406
INFO:root:current mean train loss 6959.95415301227
INFO:root:current train perplexity241.53302001953125
INFO:root:current mean train loss 6958.3445767000785
INFO:root:current train perplexity241.3509979248047
INFO:root:current mean train loss 6956.298336954512
INFO:root:current train perplexity241.42881774902344
INFO:root:current mean train loss 6959.286011151362
INFO:root:current train perplexity241.54464721679688
INFO:root:current mean train loss 6955.4850771949405
INFO:root:current train perplexity241.56723022460938

100%|██████████| 1/1 [08:04<00:00, 484.58s/it][A100%|██████████| 1/1 [08:04<00:00, 484.58s/it]
INFO:root:final mean train loss: 6952.684596290146
INFO:root:final train perplexity: 241.542236328125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.50s/it][A100%|██████████| 1/1 [00:43<00:00, 43.50s/it]
INFO:root:eval mean loss: 6713.670074246454
INFO:root:eval perplexity: 228.7891387939453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.42s/it][A100%|██████████| 1/1 [00:40<00:00, 40.42s/it]
INFO:root:eval mean loss: 6796.982786354443
INFO:root:eval perplexity: 267.19921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/124
 62%|██████▏   | 124/200 [18:44:27<12:04:58, 572.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6937.170968191965
INFO:root:current train perplexity239.01065063476562
INFO:root:current mean train loss 6979.059930819218
INFO:root:current train perplexity247.11123657226562
INFO:root:current mean train loss 6976.462546233393
INFO:root:current train perplexity244.8748321533203
INFO:root:current mean train loss 6990.5525228394745
INFO:root:current train perplexity245.29965209960938
INFO:root:current mean train loss 6981.451823316569
INFO:root:current train perplexity244.69822692871094
INFO:root:current mean train loss 6980.055831638314
INFO:root:current train perplexity244.3811798095703
INFO:root:current mean train loss 6975.903549571406
INFO:root:current train perplexity243.80282592773438
INFO:root:current mean train loss 6977.874091810688
INFO:root:current train perplexity244.2857666015625
INFO:root:current mean train loss 6975.495926149125
INFO:root:current train perplexity244.01788330078125
INFO:root:current mean train loss 6972.329881628135
INFO:root:current train perplexity243.7667999267578
INFO:root:current mean train loss 6969.617422185328
INFO:root:current train perplexity243.9810028076172
INFO:root:current mean train loss 6974.176591964488
INFO:root:current train perplexity244.22998046875
INFO:root:current mean train loss 6975.959759071432
INFO:root:current train perplexity244.1952667236328
INFO:root:current mean train loss 6978.6834678503965
INFO:root:current train perplexity244.15138244628906
INFO:root:current mean train loss 6976.156925334266
INFO:root:current train perplexity244.0978240966797
INFO:root:current mean train loss 6972.6631769839705
INFO:root:current train perplexity243.9387664794922
INFO:root:current mean train loss 6967.126247897382
INFO:root:current train perplexity243.90550231933594
INFO:root:current mean train loss 6969.916232734237
INFO:root:current train perplexity244.12623596191406
INFO:root:current mean train loss 6967.7351259317065
INFO:root:current train perplexity244.06861877441406
INFO:root:current mean train loss 6967.903696701298
INFO:root:current train perplexity244.14544677734375

100%|██████████| 1/1 [07:57<00:00, 477.14s/it][A100%|██████████| 1/1 [07:57<00:00, 477.14s/it]
INFO:root:final mean train loss: 6967.015481076454
INFO:root:final train perplexity: 244.28936767578125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 6737.20569453679
INFO:root:eval perplexity: 233.18826293945312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.59s/it][A100%|██████████| 1/1 [00:37<00:00, 37.59s/it]
INFO:root:eval mean loss: 6818.559129647329
INFO:root:eval perplexity: 271.9813232421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/125
 62%|██████▎   | 125/200 [18:53:43<11:49:21, 567.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7197.032307942708
INFO:root:current train perplexity257.1138610839844
INFO:root:current mean train loss 7027.312452746975
INFO:root:current train perplexity248.79830932617188
INFO:root:current mean train loss 7006.1237553187775
INFO:root:current train perplexity246.69964599609375
INFO:root:current mean train loss 6990.596981095679
INFO:root:current train perplexity244.95249938964844
INFO:root:current mean train loss 6977.485720076651
INFO:root:current train perplexity244.43374633789062
INFO:root:current mean train loss 6971.964951842796
INFO:root:current train perplexity244.59686279296875
INFO:root:current mean train loss 6978.939638577975
INFO:root:current train perplexity245.00401306152344
INFO:root:current mean train loss 6979.182552443025
INFO:root:current train perplexity245.28958129882812
INFO:root:current mean train loss 6969.966174079376
INFO:root:current train perplexity245.0784454345703
INFO:root:current mean train loss 6972.601365919237
INFO:root:current train perplexity244.78273010253906
INFO:root:current mean train loss 6968.09775352478
INFO:root:current train perplexity244.8944549560547
INFO:root:current mean train loss 6970.246350054214
INFO:root:current train perplexity244.75880432128906
INFO:root:current mean train loss 6975.391197054993
INFO:root:current train perplexity244.8780975341797
INFO:root:current mean train loss 6975.6043597909975
INFO:root:current train perplexity245.11338806152344
INFO:root:current mean train loss 6977.338897362184
INFO:root:current train perplexity245.16001892089844
INFO:root:current mean train loss 6976.712189601788
INFO:root:current train perplexity245.2041015625
INFO:root:current mean train loss 6977.68703246469
INFO:root:current train perplexity245.26174926757812
INFO:root:current mean train loss 6978.593317797455
INFO:root:current train perplexity245.2401123046875
INFO:root:current mean train loss 6975.799819946289
INFO:root:current train perplexity245.23497009277344
INFO:root:current mean train loss 6974.103872699698
INFO:root:current train perplexity245.29824829101562

100%|██████████| 1/1 [07:53<00:00, 473.83s/it][A100%|██████████| 1/1 [07:53<00:00, 473.84s/it]
INFO:root:final mean train loss: 6972.6041222215
INFO:root:final train perplexity: 245.369140625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.19s/it][A100%|██████████| 1/1 [00:43<00:00, 43.20s/it]
INFO:root:eval mean loss: 6770.273318026928
INFO:root:eval perplexity: 239.51229858398438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.95s/it][A100%|██████████| 1/1 [00:40<00:00, 40.95s/it]
INFO:root:eval mean loss: 6849.596419097684
INFO:root:eval perplexity: 279.0107727050781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/126
 63%|██████▎   | 126/200 [19:03:03<11:37:19, 565.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7014.689786585366
INFO:root:current train perplexity251.06515502929688
INFO:root:current mean train loss 7016.726247368129
INFO:root:current train perplexity249.18655395507812
INFO:root:current mean train loss 7013.570502949948
INFO:root:current train perplexity248.7928466796875
INFO:root:current mean train loss 7004.893261145986
INFO:root:current train perplexity248.27023315429688
INFO:root:current mean train loss 7001.506660997732
INFO:root:current train perplexity247.8295135498047
INFO:root:current mean train loss 6998.527212879794
INFO:root:current train perplexity246.81895446777344
INFO:root:current mean train loss 6979.3534478293195
INFO:root:current train perplexity246.18869018554688
INFO:root:current mean train loss 6983.526340170589
INFO:root:current train perplexity246.31480407714844
INFO:root:current mean train loss 6981.332067827549
INFO:root:current train perplexity246.36390686035156
INFO:root:current mean train loss 6975.63815557336
INFO:root:current train perplexity246.5819091796875
INFO:root:current mean train loss 6983.548457106298
INFO:root:current train perplexity247.14329528808594
INFO:root:current mean train loss 6985.545425562144
INFO:root:current train perplexity247.29063415527344
INFO:root:current mean train loss 6982.718595764504
INFO:root:current train perplexity247.14669799804688
INFO:root:current mean train loss 6979.316664773257
INFO:root:current train perplexity246.76434326171875
INFO:root:current mean train loss 6975.158305118516
INFO:root:current train perplexity246.2789764404297
INFO:root:current mean train loss 6977.447681028451
INFO:root:current train perplexity246.239990234375
INFO:root:current mean train loss 6982.159507886293
INFO:root:current train perplexity246.44825744628906
INFO:root:current mean train loss 6981.22685109357
INFO:root:current train perplexity246.49961853027344
INFO:root:current mean train loss 6979.0322183404905
INFO:root:current train perplexity246.4237060546875
INFO:root:current mean train loss 6981.055924931978
INFO:root:current train perplexity246.47999572753906

100%|██████████| 1/1 [07:59<00:00, 479.62s/it][A100%|██████████| 1/1 [07:59<00:00, 479.62s/it]
INFO:root:final mean train loss: 6978.273007083649
INFO:root:final train perplexity: 246.46942138671875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.86s/it][A100%|██████████| 1/1 [00:42<00:00, 42.86s/it]
INFO:root:eval mean loss: 6746.8449222212985
INFO:root:eval perplexity: 235.01426696777344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.25s/it][A100%|██████████| 1/1 [00:41<00:00, 41.25s/it]
INFO:root:eval mean loss: 6828.286532787566
INFO:root:eval perplexity: 274.16510009765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/127
 64%|██████▎   | 127/200 [19:12:30<11:28:14, 565.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7039.407622238686
INFO:root:current train perplexity247.74078369140625
INFO:root:current mean train loss 6960.9885315714
INFO:root:current train perplexity245.97677612304688
INFO:root:current mean train loss 6988.885246335998
INFO:root:current train perplexity247.41885375976562
INFO:root:current mean train loss 6987.143792008555
INFO:root:current train perplexity247.4457550048828
INFO:root:current mean train loss 6983.073772047284
INFO:root:current train perplexity248.30255126953125
INFO:root:current mean train loss 6993.3581490465385
INFO:root:current train perplexity248.98748779296875
INFO:root:current mean train loss 6993.939185238174
INFO:root:current train perplexity249.273193359375
INFO:root:current mean train loss 6991.861065947601
INFO:root:current train perplexity248.52655029296875
INFO:root:current mean train loss 6998.941498442963
INFO:root:current train perplexity248.66737365722656
INFO:root:current mean train loss 6992.603692996477
INFO:root:current train perplexity247.94476318359375
INFO:root:current mean train loss 6989.494913198547
INFO:root:current train perplexity247.64547729492188
INFO:root:current mean train loss 6985.714160662241
INFO:root:current train perplexity247.33392333984375
INFO:root:current mean train loss 6979.701680339576
INFO:root:current train perplexity247.2646942138672
INFO:root:current mean train loss 6977.266792847938
INFO:root:current train perplexity247.08543395996094
INFO:root:current mean train loss 6976.11521126704
INFO:root:current train perplexity247.0430145263672
INFO:root:current mean train loss 6977.945080895479
INFO:root:current train perplexity246.993896484375
INFO:root:current mean train loss 6979.517234737824
INFO:root:current train perplexity246.97352600097656
INFO:root:current mean train loss 6978.838576107438
INFO:root:current train perplexity246.59719848632812
INFO:root:current mean train loss 6975.749930095365
INFO:root:current train perplexity246.55946350097656
INFO:root:current mean train loss 6979.990717918076
INFO:root:current train perplexity246.6505889892578

100%|██████████| 1/1 [08:00<00:00, 480.07s/it][A100%|██████████| 1/1 [08:00<00:00, 480.07s/it]
INFO:root:final mean train loss: 6978.849768934379
INFO:root:final train perplexity: 246.58155822753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.16s/it][A100%|██████████| 1/1 [00:42<00:00, 42.16s/it]
INFO:root:eval mean loss: 6706.028519434286
INFO:root:eval perplexity: 227.37864685058594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.76s/it][A100%|██████████| 1/1 [00:39<00:00, 39.76s/it]
INFO:root:eval mean loss: 6791.480853141622
INFO:root:eval perplexity: 265.99322509765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/128
 64%|██████▍   | 128/200 [19:21:54<11:18:27, 565.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6933.51775390625
INFO:root:current train perplexity244.69158935546875
INFO:root:current mean train loss 6971.526922433036
INFO:root:current train perplexity244.31814575195312
INFO:root:current mean train loss 6982.035520241478
INFO:root:current train perplexity243.86410522460938
INFO:root:current mean train loss 6974.987028645834
INFO:root:current train perplexity243.68521118164062
INFO:root:current mean train loss 6965.984171463816
INFO:root:current train perplexity243.8921356201172
INFO:root:current mean train loss 6965.33740234375
INFO:root:current train perplexity244.16476440429688
INFO:root:current mean train loss 6963.686469184027
INFO:root:current train perplexity244.695556640625
INFO:root:current mean train loss 6966.864090221774
INFO:root:current train perplexity244.15591430664062
INFO:root:current mean train loss 6971.735217633928
INFO:root:current train perplexity244.29029846191406
INFO:root:current mean train loss 6965.895021534455
INFO:root:current train perplexity244.1817626953125
INFO:root:current mean train loss 6963.731326762355
INFO:root:current train perplexity244.03848266601562
INFO:root:current mean train loss 6965.921165641623
INFO:root:current train perplexity244.28366088867188
INFO:root:current mean train loss 6968.257540594363
INFO:root:current train perplexity244.89596557617188
INFO:root:current mean train loss 6968.757535511364
INFO:root:current train perplexity245.15780639648438
INFO:root:current mean train loss 6977.373939022776
INFO:root:current train perplexity245.69508361816406
INFO:root:current mean train loss 6979.30101531498
INFO:root:current train perplexity245.7820281982422
INFO:root:current mean train loss 6980.205655608675
INFO:root:current train perplexity245.80393981933594
INFO:root:current mean train loss 6980.71275253081
INFO:root:current train perplexity245.95648193359375
INFO:root:current mean train loss 6981.74812890625
INFO:root:current train perplexity246.01559448242188
INFO:root:current mean train loss 6977.1532350178
INFO:root:current train perplexity245.80641174316406

100%|██████████| 1/1 [07:53<00:00, 473.55s/it][A100%|██████████| 1/1 [07:53<00:00, 473.55s/it]
INFO:root:final mean train loss: 6975.00674113756
INFO:root:final train perplexity: 245.8348846435547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:43<00:00, 43.17s/it][A100%|██████████| 1/1 [00:43<00:00, 43.17s/it]
INFO:root:eval mean loss: 6732.649415793994
INFO:root:eval perplexity: 232.33010864257812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.44s/it][A100%|██████████| 1/1 [00:40<00:00, 40.44s/it]
INFO:root:eval mean loss: 6815.972193941157
INFO:root:eval perplexity: 271.40338134765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/129
 64%|██████▍   | 129/200 [19:31:14<11:07:01, 563.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6951.5883205247965
INFO:root:current train perplexity243.43609619140625
INFO:root:current mean train loss 6974.656964619954
INFO:root:current train perplexity246.55287170410156
INFO:root:current mean train loss 6965.072294052333
INFO:root:current train perplexity246.03260803222656
INFO:root:current mean train loss 6960.922977369659
INFO:root:current train perplexity245.59458923339844
INFO:root:current mean train loss 6967.909345425242
INFO:root:current train perplexity245.8924560546875
INFO:root:current mean train loss 6970.594894821579
INFO:root:current train perplexity245.50372314453125
INFO:root:current mean train loss 6972.70322025718
INFO:root:current train perplexity245.315673828125
INFO:root:current mean train loss 6975.095618662208
INFO:root:current train perplexity245.5108642578125
INFO:root:current mean train loss 6970.195282940373
INFO:root:current train perplexity245.39674377441406
INFO:root:current mean train loss 6972.161332161196
INFO:root:current train perplexity245.3700714111328
INFO:root:current mean train loss 6978.503142975189
INFO:root:current train perplexity245.66415405273438
INFO:root:current mean train loss 6972.764521861236
INFO:root:current train perplexity245.62796020507812
INFO:root:current mean train loss 6975.129228243518
INFO:root:current train perplexity245.87135314941406
INFO:root:current mean train loss 6975.462196087015
INFO:root:current train perplexity245.98651123046875
INFO:root:current mean train loss 6976.1912874523505
INFO:root:current train perplexity246.1430206298828
INFO:root:current mean train loss 6978.033655827968
INFO:root:current train perplexity246.2595977783203
INFO:root:current mean train loss 6977.289557995808
INFO:root:current train perplexity246.2371826171875
INFO:root:current mean train loss 6979.947737285069
INFO:root:current train perplexity246.28237915039062
INFO:root:current mean train loss 6980.955436077481
INFO:root:current train perplexity246.24856567382812

100%|██████████| 1/1 [08:00<00:00, 480.94s/it][A100%|██████████| 1/1 [08:00<00:00, 480.94s/it]
INFO:root:final mean train loss: 6976.070463441203
INFO:root:final train perplexity: 246.04141235351562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.24s/it][A100%|██████████| 1/1 [00:42<00:00, 42.24s/it]
INFO:root:eval mean loss: 6733.4935969359485
INFO:root:eval perplexity: 232.48880004882812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.31s/it][A100%|██████████| 1/1 [00:40<00:00, 40.31s/it]
INFO:root:eval mean loss: 6816.791896089594
INFO:root:eval perplexity: 271.5864562988281
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/130
 65%|██████▌   | 130/200 [19:40:40<10:58:26, 564.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6962.915798611111
INFO:root:current train perplexity250.19178771972656
INFO:root:current mean train loss 6985.331946136755
INFO:root:current train perplexity248.23187255859375
INFO:root:current mean train loss 7003.989968039773
INFO:root:current train perplexity247.0311279296875
INFO:root:current mean train loss 6989.504203327266
INFO:root:current train perplexity245.771484375
INFO:root:current mean train loss 6973.987293942925
INFO:root:current train perplexity245.5557098388672
INFO:root:current mean train loss 6969.744485011972
INFO:root:current train perplexity245.50582885742188
INFO:root:current mean train loss 6971.421996068093
INFO:root:current train perplexity245.2114715576172
INFO:root:current mean train loss 6973.281571618257
INFO:root:current train perplexity245.321044921875
INFO:root:current mean train loss 6967.384009966007
INFO:root:current train perplexity245.1056671142578
INFO:root:current mean train loss 6976.763947439666
INFO:root:current train perplexity245.5727996826172
INFO:root:current mean train loss 6973.173092073681
INFO:root:current train perplexity245.59938049316406
INFO:root:current mean train loss 6968.240743349865
INFO:root:current train perplexity245.49330139160156
INFO:root:current mean train loss 6971.684365145523
INFO:root:current train perplexity245.5263214111328
INFO:root:current mean train loss 6974.397645954688
INFO:root:current train perplexity245.90219116210938
INFO:root:current mean train loss 6969.563181999645
INFO:root:current train perplexity245.61871337890625
INFO:root:current mean train loss 6971.160330659273
INFO:root:current train perplexity245.7227325439453
INFO:root:current mean train loss 6974.100430258021
INFO:root:current train perplexity245.95132446289062
INFO:root:current mean train loss 6976.051029533444
INFO:root:current train perplexity245.90933227539062
INFO:root:current mean train loss 6976.928700140789
INFO:root:current train perplexity245.9970703125
INFO:root:current mean train loss 6976.379408087513
INFO:root:current train perplexity245.97642517089844

100%|██████████| 1/1 [07:47<00:00, 467.03s/it][A100%|██████████| 1/1 [07:47<00:00, 467.03s/it]
INFO:root:final mean train loss: 6975.565362096374
INFO:root:final train perplexity: 245.94322204589844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.63s/it][A100%|██████████| 1/1 [00:38<00:00, 38.63s/it]
INFO:root:eval mean loss: 6728.5132320755765
INFO:root:eval perplexity: 231.55369567871094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.52s/it][A100%|██████████| 1/1 [00:36<00:00, 36.52s/it]
INFO:root:eval mean loss: 6812.93196701158
INFO:root:eval perplexity: 270.725830078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/131
 66%|██████▌   | 131/200 [19:49:45<10:42:16, 558.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6915.105975811298
INFO:root:current train perplexity247.24560546875
INFO:root:current mean train loss 6980.999790736607
INFO:root:current train perplexity245.58920288085938
INFO:root:current mean train loss 6951.044122476494
INFO:root:current train perplexity246.1572265625
INFO:root:current mean train loss 6961.927274551859
INFO:root:current train perplexity247.22886657714844
INFO:root:current mean train loss 6962.18085662412
INFO:root:current train perplexity247.17616271972656
INFO:root:current mean train loss 6966.093644174786
INFO:root:current train perplexity246.7844696044922
INFO:root:current mean train loss 6974.838710407099
INFO:root:current train perplexity246.92901611328125
INFO:root:current mean train loss 6994.312389026989
INFO:root:current train perplexity247.20834350585938
INFO:root:current mean train loss 6986.6296215288285
INFO:root:current train perplexity246.78799438476562
INFO:root:current mean train loss 6989.797488251721
INFO:root:current train perplexity246.779296875
INFO:root:current mean train loss 6991.197920949836
INFO:root:current train perplexity246.910888671875
INFO:root:current mean train loss 6983.663941947436
INFO:root:current train perplexity246.85037231445312
INFO:root:current mean train loss 6987.052172413463
INFO:root:current train perplexity246.9662322998047
INFO:root:current mean train loss 6990.222237933635
INFO:root:current train perplexity247.2421875
INFO:root:current mean train loss 6988.8989168785065
INFO:root:current train perplexity247.03619384765625
INFO:root:current mean train loss 6985.179073468729
INFO:root:current train perplexity246.82907104492188
INFO:root:current mean train loss 6984.285668855224
INFO:root:current train perplexity246.6796417236328
INFO:root:current mean train loss 6982.323285470832
INFO:root:current train perplexity246.37283325195312
INFO:root:current mean train loss 6977.238092462178
INFO:root:current train perplexity246.1907958984375
INFO:root:current mean train loss 6979.281406929436
INFO:root:current train perplexity246.1463165283203

100%|██████████| 1/1 [07:35<00:00, 455.30s/it][A100%|██████████| 1/1 [07:35<00:00, 455.30s/it]
INFO:root:final mean train loss: 6976.2550672414745
INFO:root:final train perplexity: 246.0771942138672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.78s/it][A100%|██████████| 1/1 [00:39<00:00, 39.78s/it]
INFO:root:eval mean loss: 6783.942074606604
INFO:root:eval perplexity: 242.1761932373047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.54s/it][A100%|██████████| 1/1 [00:37<00:00, 37.54s/it]
INFO:root:eval mean loss: 6861.059583298704
INFO:root:eval perplexity: 281.6523742675781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/132
 66%|██████▌   | 132/200 [19:58:40<10:24:59, 551.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7001.81369231468
INFO:root:current train perplexity248.60488891601562
INFO:root:current mean train loss 7019.248869782561
INFO:root:current train perplexity245.25672912597656
INFO:root:current mean train loss 6988.8965084876545
INFO:root:current train perplexity242.7549591064453
INFO:root:current mean train loss 6976.88350292684
INFO:root:current train perplexity242.30406188964844
INFO:root:current mean train loss 6970.403546266577
INFO:root:current train perplexity243.4506072998047
INFO:root:current mean train loss 6968.729501179788
INFO:root:current train perplexity244.14625549316406
INFO:root:current mean train loss 6965.536444917622
INFO:root:current train perplexity244.70932006835938
INFO:root:current mean train loss 6976.558880278432
INFO:root:current train perplexity245.45127868652344
INFO:root:current mean train loss 6986.888846219788
INFO:root:current train perplexity246.66128540039062
INFO:root:current mean train loss 6989.187346214707
INFO:root:current train perplexity246.80105590820312
INFO:root:current mean train loss 6989.135473000809
INFO:root:current train perplexity246.626708984375
INFO:root:current mean train loss 6982.7188824297355
INFO:root:current train perplexity245.65899658203125
INFO:root:current mean train loss 6980.495247998165
INFO:root:current train perplexity244.79847717285156
INFO:root:current mean train loss 6971.4183723182705
INFO:root:current train perplexity244.06884765625
INFO:root:current mean train loss 6973.991757758359
INFO:root:current train perplexity243.7125244140625
INFO:root:current mean train loss 6966.798404715854
INFO:root:current train perplexity243.01751708984375
INFO:root:current mean train loss 6961.059807469187
INFO:root:current train perplexity242.56797790527344
INFO:root:current mean train loss 6960.699519338515
INFO:root:current train perplexity242.07481384277344
INFO:root:current mean train loss 6958.749916014565
INFO:root:current train perplexity241.6498260498047
INFO:root:current mean train loss 6954.990650281057
INFO:root:current train perplexity241.27713012695312

100%|██████████| 1/1 [07:28<00:00, 448.01s/it][A100%|██████████| 1/1 [07:28<00:00, 448.01s/it]
INFO:root:final mean train loss: 6950.540114779335
INFO:root:final train perplexity: 241.1334686279297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.97s/it][A100%|██████████| 1/1 [00:38<00:00, 38.97s/it]
INFO:root:eval mean loss: 6605.880760679854
INFO:root:eval perplexity: 209.6785888671875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.69s/it][A100%|██████████| 1/1 [00:36<00:00, 36.69s/it]
INFO:root:eval mean loss: 6694.925730170933
INFO:root:eval perplexity: 245.69496154785156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/133
 66%|██████▋   | 133/200 [20:07:26<10:07:17, 543.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6900.527416992188
INFO:root:current train perplexity232.2264404296875
INFO:root:current mean train loss 6947.685626220703
INFO:root:current train perplexity233.8986358642578
INFO:root:current mean train loss 6932.664702899639
INFO:root:current train perplexity234.36386108398438
INFO:root:current mean train loss 6928.1370320638025
INFO:root:current train perplexity234.0838623046875
INFO:root:current mean train loss 6921.969813603941
INFO:root:current train perplexity234.21180725097656
INFO:root:current mean train loss 6933.002496337891
INFO:root:current train perplexity234.38978576660156
INFO:root:current mean train loss 6932.448141571969
INFO:root:current train perplexity234.47250366210938
INFO:root:current mean train loss 6943.016969058388
INFO:root:current train perplexity234.7633819580078
INFO:root:current mean train loss 6933.374251112827
INFO:root:current train perplexity234.42979431152344
INFO:root:current mean train loss 6930.942238871256
INFO:root:current train perplexity234.4783172607422
INFO:root:current mean train loss 6928.286530807783
INFO:root:current train perplexity234.6600799560547
INFO:root:current mean train loss 6923.621892679149
INFO:root:current train perplexity234.46994018554688
INFO:root:current mean train loss 6918.021604895213
INFO:root:current train perplexity234.27078247070312
INFO:root:current mean train loss 6920.649312815946
INFO:root:current train perplexity234.40989685058594
INFO:root:current mean train loss 6926.398533149615
INFO:root:current train perplexity234.7836456298828
INFO:root:current mean train loss 6926.31329283103
INFO:root:current train perplexity235.06100463867188
INFO:root:current mean train loss 6926.940219961879
INFO:root:current train perplexity235.24545288085938
INFO:root:current mean train loss 6924.712412886186
INFO:root:current train perplexity235.375732421875
INFO:root:current mean train loss 6923.466202536963
INFO:root:current train perplexity235.37437438964844
INFO:root:current mean train loss 6920.376157924107
INFO:root:current train perplexity235.16761779785156

100%|██████████| 1/1 [07:30<00:00, 450.09s/it][A100%|██████████| 1/1 [07:30<00:00, 450.09s/it]
INFO:root:final mean train loss: 6918.913722697617
INFO:root:final train perplexity: 235.18948364257812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.05s/it][A100%|██████████| 1/1 [00:39<00:00, 39.05s/it]
INFO:root:eval mean loss: 6650.283365885417
INFO:root:eval perplexity: 217.3495635986328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.10s/it][A100%|██████████| 1/1 [00:37<00:00, 37.10s/it]
INFO:root:eval mean loss: 6734.533372811392
INFO:root:eval perplexity: 253.8270263671875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/134
 67%|██████▋   | 134/200 [20:16:15<9:53:12, 539.27s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6946.609064275568
INFO:root:current train perplexity235.38583374023438
INFO:root:current mean train loss 6916.831766419492
INFO:root:current train perplexity234.5086212158203
INFO:root:current mean train loss 6915.548956805618
INFO:root:current train perplexity234.40643310546875
INFO:root:current mean train loss 6929.883892676558
INFO:root:current train perplexity235.2369384765625
INFO:root:current mean train loss 6908.471274321934
INFO:root:current train perplexity234.22398376464844
INFO:root:current mean train loss 6904.800805790998
INFO:root:current train perplexity234.33413696289062
INFO:root:current mean train loss 6894.566479095504
INFO:root:current train perplexity233.5931396484375
INFO:root:current mean train loss 6899.26715708454
INFO:root:current train perplexity233.4733123779297
INFO:root:current mean train loss 6898.238811845246
INFO:root:current train perplexity233.41632080078125
INFO:root:current mean train loss 6905.204615832107
INFO:root:current train perplexity233.72515869140625
INFO:root:current mean train loss 6906.762456385649
INFO:root:current train perplexity233.5599365234375
INFO:root:current mean train loss 6906.46277902971
INFO:root:current train perplexity233.65484619140625
INFO:root:current mean train loss 6910.063738483139
INFO:root:current train perplexity233.97003173828125
INFO:root:current mean train loss 6910.793012365537
INFO:root:current train perplexity234.19740295410156
INFO:root:current mean train loss 6910.287393351706
INFO:root:current train perplexity234.29949951171875
INFO:root:current mean train loss 6915.667730027842
INFO:root:current train perplexity234.48939514160156
INFO:root:current mean train loss 6920.706163000335
INFO:root:current train perplexity234.5555877685547
INFO:root:current mean train loss 6917.7330821675405
INFO:root:current train perplexity234.5006866455078
INFO:root:current mean train loss 6918.375212533714
INFO:root:current train perplexity234.5272979736328
INFO:root:current mean train loss 6917.2187146817305
INFO:root:current train perplexity234.67529296875

100%|██████████| 1/1 [07:36<00:00, 456.46s/it][A100%|██████████| 1/1 [07:36<00:00, 456.46s/it]
INFO:root:final mean train loss: 6916.463698640593
INFO:root:final train perplexity: 234.7352752685547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.60s/it][A100%|██████████| 1/1 [00:38<00:00, 38.60s/it]
INFO:root:eval mean loss: 6687.460894212655
INFO:root:eval perplexity: 223.98779296875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.21s/it][A100%|██████████| 1/1 [00:37<00:00, 37.21s/it]
INFO:root:eval mean loss: 6772.609337772883
INFO:root:eval perplexity: 261.8982238769531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/135
 68%|██████▊   | 135/200 [20:25:09<9:42:43, 537.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6938.580467711104
INFO:root:current train perplexity238.564208984375
INFO:root:current mean train loss 6918.62492449259
INFO:root:current train perplexity239.37197875976562
INFO:root:current mean train loss 6919.124334010948
INFO:root:current train perplexity238.42022705078125
INFO:root:current mean train loss 6927.900452589625
INFO:root:current train perplexity238.5949249267578
INFO:root:current mean train loss 6939.260961617535
INFO:root:current train perplexity238.09780883789062
INFO:root:current mean train loss 6943.219960016835
INFO:root:current train perplexity238.1063232421875
INFO:root:current mean train loss 6939.340938513148
INFO:root:current train perplexity238.60287475585938
INFO:root:current mean train loss 6942.937763819466
INFO:root:current train perplexity239.12294006347656
INFO:root:current mean train loss 6953.645482142233
INFO:root:current train perplexity239.64939880371094
INFO:root:current mean train loss 6946.607057874591
INFO:root:current train perplexity239.6019744873047
INFO:root:current mean train loss 6946.203847602691
INFO:root:current train perplexity240.03515625
INFO:root:current mean train loss 6946.230038539049
INFO:root:current train perplexity240.03858947753906
INFO:root:current mean train loss 6947.6469402047915
INFO:root:current train perplexity240.20803833007812
INFO:root:current mean train loss 6945.038116411294
INFO:root:current train perplexity240.06915283203125
INFO:root:current mean train loss 6951.699796255334
INFO:root:current train perplexity240.42474365234375
INFO:root:current mean train loss 6953.525220308579
INFO:root:current train perplexity240.74961853027344
INFO:root:current mean train loss 6954.971074956648
INFO:root:current train perplexity240.95509338378906
INFO:root:current mean train loss 6954.637011882055
INFO:root:current train perplexity241.16578674316406
INFO:root:current mean train loss 6956.907498803788
INFO:root:current train perplexity241.36376953125

100%|██████████| 1/1 [07:29<00:00, 449.97s/it][A100%|██████████| 1/1 [07:29<00:00, 449.97s/it]
INFO:root:final mean train loss: 6952.142245586509
INFO:root:final train perplexity: 241.4385986328125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.83s/it][A100%|██████████| 1/1 [00:38<00:00, 38.83s/it]
INFO:root:eval mean loss: 6726.762665877105
INFO:root:eval perplexity: 231.22589111328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.81s/it][A100%|██████████| 1/1 [00:36<00:00, 36.81s/it]
INFO:root:eval mean loss: 6809.95666590481
INFO:root:eval perplexity: 270.06427001953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/136
 68%|██████▊   | 136/200 [20:33:57<9:30:37, 534.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7044.016068892045
INFO:root:current train perplexity254.91162109375
INFO:root:current mean train loss 6985.566023543074
INFO:root:current train perplexity244.4384002685547
INFO:root:current mean train loss 6958.875583160545
INFO:root:current train perplexity243.69044494628906
INFO:root:current mean train loss 6973.62360894795
INFO:root:current train perplexity243.63954162597656
INFO:root:current mean train loss 6952.464544365876
INFO:root:current train perplexity243.05355834960938
INFO:root:current mean train loss 6952.773490054733
INFO:root:current train perplexity243.1380615234375
INFO:root:current mean train loss 6968.034516130064
INFO:root:current train perplexity244.10411071777344
INFO:root:current mean train loss 6961.470269783975
INFO:root:current train perplexity244.0562744140625
INFO:root:current mean train loss 6958.291006593904
INFO:root:current train perplexity243.69346618652344
INFO:root:current mean train loss 6959.639846751509
INFO:root:current train perplexity243.8841094970703
INFO:root:current mean train loss 6959.183766652757
INFO:root:current train perplexity243.94190979003906
INFO:root:current mean train loss 6956.65289356084
INFO:root:current train perplexity243.5863494873047
INFO:root:current mean train loss 6957.800021611788
INFO:root:current train perplexity243.73007202148438
INFO:root:current mean train loss 6962.906913704948
INFO:root:current train perplexity243.63827514648438
INFO:root:current mean train loss 6962.991232046753
INFO:root:current train perplexity243.80667114257812
INFO:root:current mean train loss 6963.630842571145
INFO:root:current train perplexity243.81561279296875
INFO:root:current mean train loss 6964.137569226218
INFO:root:current train perplexity243.98216247558594
INFO:root:current mean train loss 6966.472283832098
INFO:root:current train perplexity244.0448760986328
INFO:root:current mean train loss 6967.715906590799
INFO:root:current train perplexity244.28878784179688
INFO:root:current mean train loss 6970.251475575206
INFO:root:current train perplexity244.403564453125

100%|██████████| 1/1 [07:34<00:00, 454.80s/it][A100%|██████████| 1/1 [07:34<00:00, 454.80s/it]
INFO:root:final mean train loss: 6966.929626310948
INFO:root:final train perplexity: 244.27272033691406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.33s/it][A100%|██████████| 1/1 [00:42<00:00, 42.37s/it]
INFO:root:eval mean loss: 6716.317528257979
INFO:root:eval perplexity: 229.2797088623047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.96s/it][A100%|██████████| 1/1 [00:37<00:00, 37.96s/it]
INFO:root:eval mean loss: 6798.854390029366
INFO:root:eval perplexity: 267.6105651855469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/137
 68%|██████▊   | 137/200 [20:42:55<9:22:30, 535.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6904.3790283203125
INFO:root:current train perplexity239.5804901123047
INFO:root:current mean train loss 6919.5265464782715
INFO:root:current train perplexity241.760009765625
INFO:root:current mean train loss 6945.163649174205
INFO:root:current train perplexity242.96885681152344
INFO:root:current mean train loss 6952.812331781155
INFO:root:current train perplexity242.89784240722656
INFO:root:current mean train loss 6958.04513064946
INFO:root:current train perplexity243.1067657470703
INFO:root:current mean train loss 6969.286101370147
INFO:root:current train perplexity244.07839965820312
INFO:root:current mean train loss 6968.559749919138
INFO:root:current train perplexity244.47023010253906
INFO:root:current mean train loss 6971.151321578812
INFO:root:current train perplexity245.38739013671875
INFO:root:current mean train loss 6976.7159954568615
INFO:root:current train perplexity245.90805053710938
INFO:root:current mean train loss 6984.374385439116
INFO:root:current train perplexity246.904296875
INFO:root:current mean train loss 6982.185499851806
INFO:root:current train perplexity247.09107971191406
INFO:root:current mean train loss 6979.3782292359265
INFO:root:current train perplexity247.4031524658203
INFO:root:current mean train loss 6984.668004536085
INFO:root:current train perplexity248.2238311767578
INFO:root:current mean train loss 6986.213072627424
INFO:root:current train perplexity248.5594940185547
INFO:root:current mean train loss 6992.431067544205
INFO:root:current train perplexity249.08616638183594
INFO:root:current mean train loss 6993.881802064586
INFO:root:current train perplexity249.10304260253906
INFO:root:current mean train loss 6995.292933658534
INFO:root:current train perplexity249.37091064453125
INFO:root:current mean train loss 6995.082596672906
INFO:root:current train perplexity249.51673889160156
INFO:root:current mean train loss 6996.816085181038
INFO:root:current train perplexity249.8948974609375
INFO:root:current mean train loss 6999.346661199673
INFO:root:current train perplexity250.16529846191406

100%|██████████| 1/1 [07:28<00:00, 448.96s/it][A100%|██████████| 1/1 [07:28<00:00, 448.96s/it]
INFO:root:final mean train loss: 6997.919108565384
INFO:root:final train perplexity: 250.32066345214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.08s/it][A100%|██████████| 1/1 [00:39<00:00, 39.08s/it]
INFO:root:eval mean loss: 6792.675873019171
INFO:root:eval perplexity: 243.8938751220703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.55s/it][A100%|██████████| 1/1 [00:38<00:00, 38.55s/it]
INFO:root:eval mean loss: 6870.314796826518
INFO:root:eval perplexity: 283.80389404296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/138
 69%|██████▉   | 138/200 [20:51:44<9:11:36, 533.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7074.594552951389
INFO:root:current train perplexity253.87242126464844
INFO:root:current mean train loss 7017.220416891164
INFO:root:current train perplexity251.92295837402344
INFO:root:current mean train loss 7015.786547353317
INFO:root:current train perplexity250.46023559570312
INFO:root:current mean train loss 7026.649527287138
INFO:root:current train perplexity251.19664001464844
INFO:root:current mean train loss 7030.265088439256
INFO:root:current train perplexity252.0320587158203
INFO:root:current mean train loss 7020.976441549598
INFO:root:current train perplexity252.26242065429688
INFO:root:current mean train loss 7015.796863644622
INFO:root:current train perplexity252.3915252685547
INFO:root:current mean train loss 7021.035865404782
INFO:root:current train perplexity252.6945037841797
INFO:root:current mean train loss 7025.048124306582
INFO:root:current train perplexity252.7392120361328
INFO:root:current mean train loss 7018.457330935847
INFO:root:current train perplexity252.72100830078125
INFO:root:current mean train loss 7012.530682285436
INFO:root:current train perplexity252.534423828125
INFO:root:current mean train loss 7013.943758102484
INFO:root:current train perplexity252.67486572265625
INFO:root:current mean train loss 7014.575533383534
INFO:root:current train perplexity252.49468994140625
INFO:root:current mean train loss 7006.901548341659
INFO:root:current train perplexity252.0989990234375
INFO:root:current mean train loss 7007.464489281466
INFO:root:current train perplexity251.91070556640625
INFO:root:current mean train loss 7009.574317670408
INFO:root:current train perplexity251.72361755371094
INFO:root:current mean train loss 7006.212979673252
INFO:root:current train perplexity251.49591064453125
INFO:root:current mean train loss 7005.22702867568
INFO:root:current train perplexity251.46461486816406
INFO:root:current mean train loss 7002.040398723323
INFO:root:current train perplexity251.0661163330078
INFO:root:current mean train loss 7001.578529934528
INFO:root:current train perplexity250.90431213378906

100%|██████████| 1/1 [07:17<00:00, 437.97s/it][A100%|██████████| 1/1 [07:17<00:00, 437.97s/it]
INFO:root:final mean train loss: 7000.781198783409
INFO:root:final train perplexity: 250.8865966796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.17s/it][A100%|██████████| 1/1 [00:38<00:00, 38.17s/it]
INFO:root:eval mean loss: 6765.208333333333
INFO:root:eval perplexity: 238.53269958496094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.91s/it][A100%|██████████| 1/1 [00:36<00:00, 36.91s/it]
INFO:root:eval mean loss: 6845.919974685561
INFO:root:eval perplexity: 278.1684265136719
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/139
 70%|██████▉   | 139/200 [21:00:20<8:57:07, 528.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6990.323683215725
INFO:root:current train perplexity253.4030303955078
INFO:root:current mean train loss 6974.534622757523
INFO:root:current train perplexity250.69801330566406
INFO:root:current mean train loss 6967.152597208969
INFO:root:current train perplexity249.05386352539062
INFO:root:current mean train loss 6993.374287810773
INFO:root:current train perplexity249.87095642089844
INFO:root:current mean train loss 7007.080742906182
INFO:root:current train perplexity250.61805725097656
INFO:root:current mean train loss 7004.801636176601
INFO:root:current train perplexity250.72024536132812
INFO:root:current mean train loss 7000.943267914464
INFO:root:current train perplexity250.57945251464844
INFO:root:current mean train loss 6997.020262390297
INFO:root:current train perplexity250.30551147460938
INFO:root:current mean train loss 7002.13831387761
INFO:root:current train perplexity250.49880981445312
INFO:root:current mean train loss 7000.03788900078
INFO:root:current train perplexity250.4285888671875
INFO:root:current mean train loss 7004.246726860434
INFO:root:current train perplexity250.56092834472656
INFO:root:current mean train loss 7000.400594425694
INFO:root:current train perplexity250.4373016357422
INFO:root:current mean train loss 7008.51058935779
INFO:root:current train perplexity251.05690002441406
INFO:root:current mean train loss 7011.659897410747
INFO:root:current train perplexity251.68858337402344
INFO:root:current mean train loss 7010.155846550103
INFO:root:current train perplexity251.81089782714844
INFO:root:current mean train loss 7012.302075414133
INFO:root:current train perplexity252.19049072265625
INFO:root:current mean train loss 7012.317096072127
INFO:root:current train perplexity252.4235382080078
INFO:root:current mean train loss 7013.741320676078
INFO:root:current train perplexity252.37371826171875
INFO:root:current mean train loss 7012.231505626511
INFO:root:current train perplexity252.45640563964844
INFO:root:current mean train loss 7008.451114883967
INFO:root:current train perplexity252.40127563476562

100%|██████████| 1/1 [07:32<00:00, 452.94s/it][A100%|██████████| 1/1 [07:32<00:00, 452.94s/it]
INFO:root:final mean train loss: 7008.714688130358
INFO:root:final train perplexity: 252.46229553222656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.94s/it][A100%|██████████| 1/1 [00:38<00:00, 38.94s/it]
INFO:root:eval mean loss: 6809.626523714539
INFO:root:eval perplexity: 247.26246643066406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.90s/it][A100%|██████████| 1/1 [00:38<00:00, 38.90s/it]
INFO:root:eval mean loss: 6885.679752431018
INFO:root:eval perplexity: 287.4113464355469
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/140
 70%|███████   | 140/200 [21:09:13<8:49:48, 529.81s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7001.1906645569625
INFO:root:current train perplexity253.54360961914062
INFO:root:current mean train loss 6994.666266585195
INFO:root:current train perplexity252.50552368164062
INFO:root:current mean train loss 7004.531346256161
INFO:root:current train perplexity252.23138427734375
INFO:root:current mean train loss 7011.749398344739
INFO:root:current train perplexity252.65847778320312
INFO:root:current mean train loss 7007.6893858461635
INFO:root:current train perplexity252.940673828125
INFO:root:current mean train loss 7014.329255046417
INFO:root:current train perplexity252.9137725830078
INFO:root:current mean train loss 7009.484584263393
INFO:root:current train perplexity252.38418579101562
INFO:root:current mean train loss 7014.019639060494
INFO:root:current train perplexity252.22694396972656
INFO:root:current mean train loss 7007.622487490224
INFO:root:current train perplexity252.12484741210938
INFO:root:current mean train loss 7006.212608329609
INFO:root:current train perplexity252.4165496826172
INFO:root:current mean train loss 7007.688669793356
INFO:root:current train perplexity252.52671813964844
INFO:root:current mean train loss 7006.9068952435855
INFO:root:current train perplexity252.68197631835938
INFO:root:current mean train loss 7012.806260384089
INFO:root:current train perplexity252.79766845703125
INFO:root:current mean train loss 7010.654861992386
INFO:root:current train perplexity252.7299346923828
INFO:root:current mean train loss 7012.826072171864
INFO:root:current train perplexity252.93826293945312
INFO:root:current mean train loss 7015.1044034372035
INFO:root:current train perplexity253.0707244873047
INFO:root:current mean train loss 7015.0274054031415
INFO:root:current train perplexity253.16281127929688
INFO:root:current mean train loss 7015.245500621399
INFO:root:current train perplexity253.19119262695312
INFO:root:current mean train loss 7012.670881546451
INFO:root:current train perplexity253.13046264648438
INFO:root:current mean train loss 7014.697454374447
INFO:root:current train perplexity253.24020385742188

100%|██████████| 1/1 [07:34<00:00, 454.86s/it][A100%|██████████| 1/1 [07:34<00:00, 454.86s/it]
INFO:root:final mean train loss: 7012.648906082562
INFO:root:final train perplexity: 253.24745178222656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.89s/it][A100%|██████████| 1/1 [00:38<00:00, 38.89s/it]
INFO:root:eval mean loss: 6774.499710840536
INFO:root:eval perplexity: 240.3329315185547
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.89s/it][A100%|██████████| 1/1 [00:36<00:00, 36.89s/it]
INFO:root:eval mean loss: 6854.589927727449
INFO:root:eval perplexity: 280.1584777832031
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/141
 70%|███████   | 141/200 [21:18:07<8:42:04, 530.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6985.810312906901
INFO:root:current train perplexity250.58291625976562
INFO:root:current mean train loss 6990.769249740912
INFO:root:current train perplexity252.0319366455078
INFO:root:current mean train loss 7012.853482633023
INFO:root:current train perplexity252.9908447265625
INFO:root:current mean train loss 7026.199356849747
INFO:root:current train perplexity253.76095581054688
INFO:root:current mean train loss 7026.847630654612
INFO:root:current train perplexity253.01353454589844
INFO:root:current mean train loss 7016.997669194368
INFO:root:current train perplexity252.9167938232422
INFO:root:current mean train loss 7005.719452956627
INFO:root:current train perplexity252.09564208984375
INFO:root:current mean train loss 7003.812950862712
INFO:root:current train perplexity251.5167694091797
INFO:root:current mean train loss 7005.576902117048
INFO:root:current train perplexity251.1537628173828
INFO:root:current mean train loss 7007.1367643425265
INFO:root:current train perplexity250.9149627685547
INFO:root:current mean train loss 7006.443329971202
INFO:root:current train perplexity250.67051696777344
INFO:root:current mean train loss 7002.570030391017
INFO:root:current train perplexity250.550537109375
INFO:root:current mean train loss 7003.023270971981
INFO:root:current train perplexity250.7194061279297
INFO:root:current mean train loss 6999.711569187634
INFO:root:current train perplexity250.7717742919922
INFO:root:current mean train loss 7002.919221765855
INFO:root:current train perplexity251.1665802001953
INFO:root:current mean train loss 7003.457018094553
INFO:root:current train perplexity251.21568298339844
INFO:root:current mean train loss 7000.516748968161
INFO:root:current train perplexity251.35958862304688
INFO:root:current mean train loss 7003.102038818904
INFO:root:current train perplexity251.5290069580078
INFO:root:current mean train loss 7004.501740403316
INFO:root:current train perplexity251.54531860351562

100%|██████████| 1/1 [07:30<00:00, 450.74s/it][A100%|██████████| 1/1 [07:30<00:00, 450.74s/it]
INFO:root:final mean train loss: 7004.963205065321
INFO:root:final train perplexity: 251.7159423828125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.90s/it][A100%|██████████| 1/1 [00:40<00:00, 40.90s/it]
INFO:root:eval mean loss: 6800.639508186503
INFO:root:eval perplexity: 245.47071838378906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.16s/it][A100%|██████████| 1/1 [00:41<00:00, 41.16s/it]
INFO:root:eval mean loss: 6878.34512826906
INFO:root:eval perplexity: 285.6835632324219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/142
 71%|███████   | 142/200 [21:27:02<8:34:27, 532.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6706.827561598558
INFO:root:current train perplexity242.37257385253906
INFO:root:current mean train loss 6952.152244365321
INFO:root:current train perplexity251.26504516601562
INFO:root:current mean train loss 6990.260210350645
INFO:root:current train perplexity252.02015686035156
INFO:root:current mean train loss 6978.981612232928
INFO:root:current train perplexity251.38607788085938
INFO:root:current mean train loss 6997.151514972382
INFO:root:current train perplexity251.69374084472656
INFO:root:current mean train loss 7001.840448152717
INFO:root:current train perplexity251.66757202148438
INFO:root:current mean train loss 7002.914025858992
INFO:root:current train perplexity251.5168914794922
INFO:root:current mean train loss 7003.317349940831
INFO:root:current train perplexity251.05294799804688
INFO:root:current mean train loss 6998.504631765068
INFO:root:current train perplexity250.9551544189453
INFO:root:current mean train loss 7003.167556411727
INFO:root:current train perplexity250.96485900878906
INFO:root:current mean train loss 7003.012570952616
INFO:root:current train perplexity251.0300750732422
INFO:root:current mean train loss 7000.081630710214
INFO:root:current train perplexity250.79330444335938
INFO:root:current mean train loss 7001.629501606941
INFO:root:current train perplexity250.5432586669922
INFO:root:current mean train loss 7005.907816367574
INFO:root:current train perplexity250.7368621826172
INFO:root:current mean train loss 7008.77530216074
INFO:root:current train perplexity250.8953399658203
INFO:root:current mean train loss 7015.49038444161
INFO:root:current train perplexity251.35659790039062
INFO:root:current mean train loss 7009.678667346366
INFO:root:current train perplexity251.26156616210938
INFO:root:current mean train loss 7010.668587866681
INFO:root:current train perplexity251.29403686523438
INFO:root:current mean train loss 7010.092733847128
INFO:root:current train perplexity251.30650329589844
INFO:root:current mean train loss 7005.476174019047
INFO:root:current train perplexity251.17483520507812

100%|██████████| 1/1 [07:32<00:00, 452.45s/it][A100%|██████████| 1/1 [07:32<00:00, 452.45s/it]
INFO:root:final mean train loss: 7002.214840425846
INFO:root:final train perplexity: 251.170654296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.94s/it][A100%|██████████| 1/1 [00:38<00:00, 38.94s/it]
INFO:root:eval mean loss: 6791.5119663536125
INFO:root:eval perplexity: 243.6641845703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.32s/it][A100%|██████████| 1/1 [00:37<00:00, 37.32s/it]
INFO:root:eval mean loss: 6869.932225869902
INFO:root:eval perplexity: 283.7144470214844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/143
 72%|███████▏  | 143/200 [21:35:53<8:25:19, 531.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7099.7298828125
INFO:root:current train perplexity251.2667236328125
INFO:root:current mean train loss 7028.781779597356
INFO:root:current train perplexity249.7342071533203
INFO:root:current mean train loss 7048.625564707881
INFO:root:current train perplexity252.68052673339844
INFO:root:current mean train loss 7037.847024443656
INFO:root:current train perplexity252.689208984375
INFO:root:current mean train loss 7021.302176825945
INFO:root:current train perplexity252.53814697265625
INFO:root:current mean train loss 7016.245849609375
INFO:root:current train perplexity251.77728271484375
INFO:root:current mean train loss 7024.752446056547
INFO:root:current train perplexity251.90577697753906
INFO:root:current mean train loss 7021.250735097388
INFO:root:current train perplexity251.53392028808594
INFO:root:current mean train loss 7014.355488751882
INFO:root:current train perplexity251.4677276611328
INFO:root:current mean train loss 7011.829020182292
INFO:root:current train perplexity251.3081817626953
INFO:root:current mean train loss 7007.191416205249
INFO:root:current train perplexity251.1603546142578
INFO:root:current mean train loss 7004.258898817754
INFO:root:current train perplexity251.02685546875
INFO:root:current mean train loss 6999.469507828379
INFO:root:current train perplexity250.7396240234375
INFO:root:current mean train loss 6995.194444607613
INFO:root:current train perplexity250.3718719482422
INFO:root:current mean train loss 6996.245259915865
INFO:root:current train perplexity250.24118041992188
INFO:root:current mean train loss 6996.030920649509
INFO:root:current train perplexity250.0178985595703
INFO:root:current mean train loss 6996.336397023581
INFO:root:current train perplexity250.2030029296875
INFO:root:current mean train loss 6997.132362886109
INFO:root:current train perplexity250.19512939453125
INFO:root:current mean train loss 6996.96093776682
INFO:root:current train perplexity250.08444213867188
INFO:root:current mean train loss 6998.560139299304
INFO:root:current train perplexity250.16123962402344

100%|██████████| 1/1 [07:34<00:00, 454.56s/it][A100%|██████████| 1/1 [07:34<00:00, 454.56s/it]
INFO:root:final mean train loss: 6997.203485855356
INFO:root:final train perplexity: 250.17926025390625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.18s/it][A100%|██████████| 1/1 [00:38<00:00, 38.18s/it]
INFO:root:eval mean loss: 6770.518783244681
INFO:root:eval perplexity: 239.5599365234375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.47s/it][A100%|██████████| 1/1 [00:35<00:00, 35.47s/it]
INFO:root:eval mean loss: 6851.749080576795
INFO:root:eval perplexity: 279.5047912597656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/144
 72%|███████▏  | 144/200 [21:44:44<8:16:07, 531.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6988.591838430851
INFO:root:current train perplexity251.5511932373047
INFO:root:current mean train loss 7031.094404363308
INFO:root:current train perplexity251.69314575195312
INFO:root:current mean train loss 6984.2716365922315
INFO:root:current train perplexity248.85549926757812
INFO:root:current mean train loss 6992.698610860951
INFO:root:current train perplexity249.0943603515625
INFO:root:current mean train loss 6995.350487625839
INFO:root:current train perplexity248.61199951171875
INFO:root:current mean train loss 6990.651471627913
INFO:root:current train perplexity249.0492401123047
INFO:root:current mean train loss 6991.883021547768
INFO:root:current train perplexity249.86654663085938
INFO:root:current mean train loss 6980.825109029869
INFO:root:current train perplexity249.5485076904297
INFO:root:current mean train loss 6986.740418849617
INFO:root:current train perplexity249.85951232910156
INFO:root:current mean train loss 6988.487137630345
INFO:root:current train perplexity249.7127685546875
INFO:root:current mean train loss 6989.406626354315
INFO:root:current train perplexity249.79327392578125
INFO:root:current mean train loss 6988.233276260762
INFO:root:current train perplexity249.8553466796875
INFO:root:current mean train loss 6993.975385456345
INFO:root:current train perplexity250.04412841796875
INFO:root:current mean train loss 6992.4167217659615
INFO:root:current train perplexity250.2592010498047
INFO:root:current mean train loss 6991.938076354095
INFO:root:current train perplexity250.12332153320312
INFO:root:current mean train loss 6996.605920418047
INFO:root:current train perplexity250.1326141357422
INFO:root:current mean train loss 6996.634222497343
INFO:root:current train perplexity250.12451171875
INFO:root:current mean train loss 6997.307946714457
INFO:root:current train perplexity249.97105407714844
INFO:root:current mean train loss 6998.940453480306
INFO:root:current train perplexity250.18569946289062
INFO:root:current mean train loss 6999.751036751252
INFO:root:current train perplexity250.14419555664062

100%|██████████| 1/1 [07:10<00:00, 430.86s/it][A100%|██████████| 1/1 [07:10<00:00, 430.86s/it]
INFO:root:final mean train loss: 6996.724634737062
INFO:root:final train perplexity: 250.0846710205078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.02s/it][A100%|██████████| 1/1 [00:38<00:00, 38.02s/it]
INFO:root:eval mean loss: 6774.319928108378
INFO:root:eval perplexity: 240.2979736328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:35<00:00, 35.08s/it][A100%|██████████| 1/1 [00:35<00:00, 35.08s/it]
INFO:root:eval mean loss: 6853.420805802582
INFO:root:eval perplexity: 279.8893127441406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/145
 72%|███████▎  | 145/200 [21:53:10<8:00:20, 524.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7015.596244812012
INFO:root:current train perplexity249.92742919921875
INFO:root:current mean train loss 7023.748445836509
INFO:root:current train perplexity249.59266662597656
INFO:root:current mean train loss 6993.186207164417
INFO:root:current train perplexity249.5144805908203
INFO:root:current mean train loss 6997.209932767428
INFO:root:current train perplexity249.11978149414062
INFO:root:current mean train loss 6986.8030479694235
INFO:root:current train perplexity248.98500061035156
INFO:root:current mean train loss 6975.423094837378
INFO:root:current train perplexity248.80223083496094
INFO:root:current mean train loss 6979.773275720067
INFO:root:current train perplexity248.8242950439453
INFO:root:current mean train loss 6984.167547575466
INFO:root:current train perplexity249.1713409423828
INFO:root:current mean train loss 6981.578306410048
INFO:root:current train perplexity249.19522094726562
INFO:root:current mean train loss 6983.301347534686
INFO:root:current train perplexity249.23599243164062
INFO:root:current mean train loss 6988.147806497444
INFO:root:current train perplexity249.65634155273438
INFO:root:current mean train loss 6982.0384949359695
INFO:root:current train perplexity249.65420532226562
INFO:root:current mean train loss 6989.436902396286
INFO:root:current train perplexity250.03817749023438
INFO:root:current mean train loss 6990.704428395918
INFO:root:current train perplexity250.0344696044922
INFO:root:current mean train loss 6992.349536999978
INFO:root:current train perplexity250.01254272460938
INFO:root:current mean train loss 6997.25568610384
INFO:root:current train perplexity250.16244506835938
INFO:root:current mean train loss 6995.866509364201
INFO:root:current train perplexity250.00454711914062
INFO:root:current mean train loss 6995.522820505155
INFO:root:current train perplexity250.11376953125
INFO:root:current mean train loss 6994.680747625654
INFO:root:current train perplexity249.93089294433594
INFO:root:current mean train loss 6996.295469575404
INFO:root:current train perplexity249.8436737060547

100%|██████████| 1/1 [07:14<00:00, 434.21s/it][A100%|██████████| 1/1 [07:14<00:00, 434.21s/it]
INFO:root:final mean train loss: 6995.816415237527
INFO:root:final train perplexity: 249.90550231933594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.93s/it][A100%|██████████| 1/1 [00:36<00:00, 36.93s/it]
INFO:root:eval mean loss: 6784.108123129987
INFO:root:eval perplexity: 242.20875549316406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:34<00:00, 34.55s/it][A100%|██████████| 1/1 [00:34<00:00, 34.55s/it]
INFO:root:eval mean loss: 6863.579147447085
INFO:root:eval perplexity: 282.23638916015625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/146
 73%|███████▎  | 146/200 [22:01:39<7:47:23, 519.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 7041.943618586034
INFO:root:current train perplexity252.171142578125
INFO:root:current mean train loss 7003.826174572687
INFO:root:current train perplexity250.8356475830078
INFO:root:current mean train loss 7002.7248213689945
INFO:root:current train perplexity251.28253173828125
INFO:root:current mean train loss 7001.305384678478
INFO:root:current train perplexity251.7493133544922
INFO:root:current mean train loss 7007.6005859375
INFO:root:current train perplexity251.1033477783203
INFO:root:current mean train loss 7002.6395669172225
INFO:root:current train perplexity250.50860595703125
INFO:root:current mean train loss 6998.495634149
INFO:root:current train perplexity250.11663818359375
INFO:root:current mean train loss 6984.906422555217
INFO:root:current train perplexity249.09912109375
INFO:root:current mean train loss 6993.7385999352655
INFO:root:current train perplexity249.07037353515625
INFO:root:current mean train loss 6988.1950800562245
INFO:root:current train perplexity248.7581024169922
INFO:root:current mean train loss 6987.148848089876
INFO:root:current train perplexity248.7455291748047
INFO:root:current mean train loss 6985.42892923767
INFO:root:current train perplexity248.71836853027344
INFO:root:current mean train loss 6986.6782268491415
INFO:root:current train perplexity248.5513153076172
INFO:root:current mean train loss 6983.135759866039
INFO:root:current train perplexity248.54669189453125
INFO:root:current mean train loss 6985.2146927487765
INFO:root:current train perplexity248.69985961914062
INFO:root:current mean train loss 6983.270596141682
INFO:root:current train perplexity248.4022674560547
INFO:root:current mean train loss 6986.147476622918
INFO:root:current train perplexity248.46990966796875
INFO:root:current mean train loss 6987.611699613542
INFO:root:current train perplexity248.4053497314453
INFO:root:current mean train loss 6987.69172294491
INFO:root:current train perplexity248.3267059326172
INFO:root:current mean train loss 6990.160658827218
INFO:root:current train perplexity248.4013214111328

100%|██████████| 1/1 [07:27<00:00, 447.70s/it][A100%|██████████| 1/1 [07:27<00:00, 447.70s/it]
INFO:root:final mean train loss: 6988.17842616833
INFO:root:final train perplexity: 248.403564453125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.51s/it][A100%|██████████| 1/1 [00:38<00:00, 38.51s/it]
INFO:root:eval mean loss: 6775.559436987478
INFO:root:eval perplexity: 240.5390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.58s/it][A100%|██████████| 1/1 [00:36<00:00, 36.58s/it]
INFO:root:eval mean loss: 6855.8309178994905
INFO:root:eval perplexity: 280.44439697265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilroberta_roberta_not_concat_1e-5/147
 74%|███████▎  | 147/200 [22:10:24<7:40:17, 521.08s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6980.215521364796
INFO:root:current train perplexity249.15887451171875
INFO:root:current mean train loss 6966.515269886364
INFO:root:current train perplexity247.33285522460938
INFO:root:current mean train loss 6977.168191589765
INFO:root:current train perplexity247.37721252441406
INFO:root:current mean train loss 6999.47988477544
INFO:root:current train perplexity248.5341339111328
INFO:root:current mean train loss 6993.075130600527
INFO:root:current train perplexity248.60916137695312
INFO:root:current mean train loss 6991.98534094769
INFO:root:current train perplexity248.75502014160156
INFO:root:current mean train loss 6993.442200231689
INFO:root:current train perplexity248.83853149414062
INFO:root:current mean train loss 6994.565637727131
INFO:root:current train perplexity249.37197875976562
INFO:root:current mean train loss 6995.589158090026
INFO:root:current train perplexity249.54481506347656
INFO:root:current mean train loss 7002.468710369959
INFO:root:current train perplexity250.10923767089844
INFO:root:current mean train loss 6996.568225075422
INFO:root:current train perplexity250.2218475341797
INFO:root:current mean train loss 6999.040545684866
INFO:root:current train perplexity250.26361083984375
INFO:root:current mean train loss 7001.213430442869
INFO:root:current train perplexity250.2101593017578
INFO:root:current mean train loss 7005.806745406384
INFO:root:current train perplexity250.46847534179688
INFO:root:current mean train loss 7000.807534068863
INFO:root:current train perplexity250.2415313720703
INFO:root:current mean train loss 6997.375333057924
INFO:root:current train perplexity250.04734802246094
INFO:root:current mean train loss 6996.5275775383725
INFO:root:current train perplexity249.9976348876953
INFO:root:current mean train loss 6998.537253578194
INFO:root:current train perplexity249.96795654296875
INFO:root:current mean train loss 7001.010007450277
INFO:root:current train perplexity250.13619995117188

100%|██████████| 1/1 [07:31<00:00, 451.99s/it][A100%|██████████| 1/1 [07:31<00:00, 451.99s/it]
INFO:root:final mean train loss: 6995.538007142744
INFO:root:final train perplexity: 249.8505859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 29928105 ON ga015 CANCELLED AT 2023-02-09T01:48:20 ***
