INFO:root:Output: large_mpnet_gpt2_final
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Using pad_token, but it is not set yet.
Some weights of RetrievalGenerationModelGPT2 were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.10.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.6.crossattention.c_attn_v.bias', 'h.4.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.8.crossattention.masked_bias', 'h.8.crossattention.c_proj.bias', 'h.11.crossattention.bias', 'h.6.crossattention.bias', 'h.7.ln_cross_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.5.crossattention.c_attn_v.weight', 'h.10.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.1.crossattention.c_attn_v.weight', 'h.3.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.bias', 'h.7.crossattention.masked_bias', 'h.11.crossattention.masked_bias', 'h.9.crossattention.c_attn.weight', 'h.10.crossattention.bias', 'h.3.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.8.crossattention.c_attn_v.weight', 'h.6.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight', 'h.5.crossattention.bias', 'h.4.crossattention.c_proj.weight', 'h.9.crossattention.c_attn_v.bias', 'h.8.crossattention.bias', 'h.3.crossattention.c_attn_v.weight', 'h.5.crossattention.c_attn_v.bias', 'h.1.crossattention.c_attn_v.bias', 'h.1.crossattention.bias', 'h.2.crossattention.masked_bias', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.masked_bias', 'h.0.crossattention.c_attn_v.weight', 'h.6.crossattention.c_proj.weight', 'h.4.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.4.ln_cross_attn.weight', 'h.0.crossattention.masked_bias', 'h.0.ln_cross_attn.weight', 'h.2.crossattention.bias', 'h.11.crossattention.c_attn_v.weight', 'h.6.ln_cross_attn.weight', 'h.4.crossattention.c_attn_v.weight', 'h.8.crossattention.c_attn_v.bias', 'h.5.crossattention.q_attn.weight', 'h.9.crossattention.masked_bias', 'h.5.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.5.crossattention.masked_bias', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.c_attn_v.weight', 'h.3.crossattention.masked_bias', 'h.1.crossattention.c_proj.weight', 'h.7.crossattention.bias', 'h.9.ln_cross_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.9.crossattention.c_attn_v.weight', 'h.2.crossattention.c_attn_v.bias', 'h.10.crossattention.c_attn_v.bias', 'h.3.crossattention.q_attn.weight', 'h.1.crossattention.masked_bias', 'h.9.crossattention.bias', 'h.4.crossattention.c_attn_v.bias', 'h.2.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.7.crossattention.c_attn_v.bias', 'h.2.crossattention.c_attn_v.weight', 'h.9.crossattention.c_proj.bias', 'h.0.crossattention.q_attn.weight', 'h.6.crossattention.c_attn_v.weight', 'h.6.crossattention.c_proj.bias', 'h.4.crossattention.bias', 'h.3.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.weight', 'h.11.crossattention.c_attn_v.bias', 'h.11.crossattention.q_attn.weight', 'h.4.crossattention.masked_bias', 'h.8.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.0.crossattention.c_attn_v.bias', 'h.0.crossattention.bias', 'h.8.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.8.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.weight', 'h.3.crossattention.bias', 'h.11.crossattention.c_attn.weight', 'h.3.crossattention.c_attn_v.bias', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_attn_v.weight', 'h.7.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.6.crossattention.masked_bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4691.931687480272
INFO:root:current train perplexity61.32377243041992
INFO:root:current mean train loss 4413.500575386699
INFO:root:current train perplexity48.205448150634766
INFO:root:current mean train loss 4166.263171345892
INFO:root:current train perplexity38.67386245727539
INFO:root:current mean train loss 3963.1909424440005
INFO:root:current train perplexity32.50235366821289
INFO:root:current mean train loss 3805.2369445923096
INFO:root:current train perplexity28.312366485595703
INFO:root:current mean train loss 3681.573997433874
INFO:root:current train perplexity25.336437225341797
INFO:root:current mean train loss 3577.488778263032
INFO:root:current train perplexity23.110061645507812
INFO:root:current mean train loss 3491.2579182229742
INFO:root:current train perplexity21.365863800048828
INFO:root:current mean train loss 3417.229824316515
INFO:root:current train perplexity20.00594711303711
INFO:root:current mean train loss 3351.2703206135825
INFO:root:current train perplexity18.873632431030273
INFO:root:current mean train loss 3292.824100345129
INFO:root:current train perplexity17.959671020507812
INFO:root:current mean train loss 3242.764192328242
INFO:root:current train perplexity17.18793296813965
INFO:root:current mean train loss 3197.9237674187475
INFO:root:current train perplexity16.522869110107422
INFO:root:current mean train loss 3154.6517018992363
INFO:root:current train perplexity15.935498237609863
INFO:root:current mean train loss 3114.817087368141
INFO:root:current train perplexity15.413798332214355
INFO:root:current mean train loss 3082.7007942891555
INFO:root:current train perplexity14.978736877441406
INFO:root:current mean train loss 3051.869319886583
INFO:root:current train perplexity14.579322814941406
INFO:root:current mean train loss 3024.2824551644358
INFO:root:current train perplexity14.220967292785645
INFO:root:current mean train loss 2998.5557183802534
INFO:root:current train perplexity13.900053024291992

100%|██████████| 1/1 [05:39<00:00, 339.90s/it][A100%|██████████| 1/1 [05:39<00:00, 339.90s/it]
INFO:root:final mean train loss: 2977.293395903756
INFO:root:final train perplexity: 13.648584365844727
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.85s/it][A100%|██████████| 1/1 [00:22<00:00, 22.85s/it]
INFO:root:eval mean loss: 2299.7461288127492
INFO:root:eval perplexity: 7.971520900726318
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.55s/it][A100%|██████████| 1/1 [00:22<00:00, 22.55s/it]
INFO:root:eval mean loss: 2504.2187352823025
INFO:root:eval perplexity: 9.979341506958008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/1
  0%|          | 1/200 [06:49<22:38:55, 409.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2483.6702423095703
INFO:root:current train perplexity8.909979820251465
INFO:root:current mean train loss 2451.055318898168
INFO:root:current train perplexity8.601075172424316
INFO:root:current mean train loss 2449.6330102991174
INFO:root:current train perplexity8.600385665893555
INFO:root:current mean train loss 2445.919687778135
INFO:root:current train perplexity8.585335731506348
INFO:root:current mean train loss 2443.432121570294
INFO:root:current train perplexity8.524589538574219
INFO:root:current mean train loss 2439.063390450884
INFO:root:current train perplexity8.489912033081055
INFO:root:current mean train loss 2429.3937027175707
INFO:root:current train perplexity8.437432289123535
INFO:root:current mean train loss 2420.608325809074
INFO:root:current train perplexity8.394431114196777
INFO:root:current mean train loss 2413.6592178344727
INFO:root:current train perplexity8.344240188598633
INFO:root:current mean train loss 2407.9358311282494
INFO:root:current train perplexity8.297386169433594
INFO:root:current mean train loss 2401.771636001707
INFO:root:current train perplexity8.25347900390625
INFO:root:current mean train loss 2396.4976676476044
INFO:root:current train perplexity8.211800575256348
INFO:root:current mean train loss 2391.633865456832
INFO:root:current train perplexity8.179625511169434
INFO:root:current mean train loss 2386.76479907746
INFO:root:current train perplexity8.142776489257812
INFO:root:current mean train loss 2382.9527330991236
INFO:root:current train perplexity8.106985092163086
INFO:root:current mean train loss 2379.1519176312045
INFO:root:current train perplexity8.072331428527832
INFO:root:current mean train loss 2375.4862661078423
INFO:root:current train perplexity8.04301929473877
INFO:root:current mean train loss 2370.852881869514
INFO:root:current train perplexity8.009458541870117
INFO:root:current mean train loss 2365.536787192727
INFO:root:current train perplexity7.973410129547119
INFO:root:current mean train loss 2361.9764041144067
INFO:root:current train perplexity7.948985576629639

100%|██████████| 1/1 [05:49<00:00, 349.95s/it][A100%|██████████| 1/1 [05:49<00:00, 349.95s/it]
INFO:root:final mean train loss: 2358.7620342368136
INFO:root:final train perplexity: 7.930006980895996
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.71s/it][A100%|██████████| 1/1 [00:24<00:00, 24.71s/it]
INFO:root:eval mean loss: 2109.4982836567765
INFO:root:eval perplexity: 6.713681221008301
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.55s/it][A100%|██████████| 1/1 [00:24<00:00, 24.55s/it]
INFO:root:eval mean loss: 2354.71582161112
INFO:root:eval perplexity: 8.698711395263672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/2
  1%|          | 2/200 [14:11<23:34:01, 428.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2262.2457571318655
INFO:root:current train perplexity7.3414812088012695
INFO:root:current mean train loss 2260.034497253877
INFO:root:current train perplexity7.276949405670166
INFO:root:current mean train loss 2250.0809467626746
INFO:root:current train perplexity7.238317966461182
INFO:root:current mean train loss 2260.413215339363
INFO:root:current train perplexity7.22563362121582
INFO:root:current mean train loss 2252.942502345554
INFO:root:current train perplexity7.195837497711182
INFO:root:current mean train loss 2249.4388805625586
INFO:root:current train perplexity7.1612772941589355
INFO:root:current mean train loss 2248.95859309433
INFO:root:current train perplexity7.1507344245910645
INFO:root:current mean train loss 2241.138898862498
INFO:root:current train perplexity7.1286725997924805
INFO:root:current mean train loss 2236.49178641183
INFO:root:current train perplexity7.119174480438232
INFO:root:current mean train loss 2232.3775047310423
INFO:root:current train perplexity7.110466003417969
INFO:root:current mean train loss 2231.92886739548
INFO:root:current train perplexity7.0977253913879395
INFO:root:current mean train loss 2228.264933735106
INFO:root:current train perplexity7.077181339263916
INFO:root:current mean train loss 2226.5960341899836
INFO:root:current train perplexity7.066920280456543
INFO:root:current mean train loss 2224.004711840802
INFO:root:current train perplexity7.049955368041992
INFO:root:current mean train loss 2220.1652357549992
INFO:root:current train perplexity7.0255961418151855
INFO:root:current mean train loss 2217.4759708611077
INFO:root:current train perplexity7.021596908569336
INFO:root:current mean train loss 2216.3118734272143
INFO:root:current train perplexity7.007441997528076
INFO:root:current mean train loss 2215.0731640709528
INFO:root:current train perplexity6.996968746185303
INFO:root:current mean train loss 2213.570767949464
INFO:root:current train perplexity6.983802318572998
INFO:root:current mean train loss 2211.9254401098924
INFO:root:current train perplexity6.969303131103516

100%|██████████| 1/1 [05:43<00:00, 343.75s/it][A100%|██████████| 1/1 [05:43<00:00, 343.76s/it]
INFO:root:final mean train loss: 2210.616930370552
INFO:root:final train perplexity: 6.962954998016357
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.04s/it][A100%|██████████| 1/1 [00:24<00:00, 24.04s/it]
INFO:root:eval mean loss: 2024.2182487325465
INFO:root:eval perplexity: 6.216264724731445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.81s/it][A100%|██████████| 1/1 [00:22<00:00, 22.81s/it]
INFO:root:eval mean loss: 2295.1097308219746
INFO:root:eval perplexity: 8.235198974609375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/3
  2%|▏         | 3/200 [21:47<24:08:51, 441.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2153.6687622070312
INFO:root:current train perplexity6.52447509765625
INFO:root:current mean train loss 2157.484140625
INFO:root:current train perplexity6.56667947769165
INFO:root:current mean train loss 2157.136075683594
INFO:root:current train perplexity6.585022926330566
INFO:root:current mean train loss 2157.0761443219867
INFO:root:current train perplexity6.581401824951172
INFO:root:current mean train loss 2156.434427625868
INFO:root:current train perplexity6.5761613845825195
INFO:root:current mean train loss 2152.3611971768464
INFO:root:current train perplexity6.57014274597168
INFO:root:current mean train loss 2149.6857239708534
INFO:root:current train perplexity6.560448169708252
INFO:root:current mean train loss 2147.8313753255206
INFO:root:current train perplexity6.552507400512695
INFO:root:current mean train loss 2148.9819580078124
INFO:root:current train perplexity6.54856538772583
INFO:root:current mean train loss 2145.7552826891447
INFO:root:current train perplexity6.533108711242676
INFO:root:current mean train loss 2142.5590475027902
INFO:root:current train perplexity6.524776458740234
INFO:root:current mean train loss 2140.818580693784
INFO:root:current train perplexity6.522937774658203
INFO:root:current mean train loss 2138.545876269531
INFO:root:current train perplexity6.513138294219971
INFO:root:current mean train loss 2135.92554208261
INFO:root:current train perplexity6.504763603210449
INFO:root:current mean train loss 2133.0029609206626
INFO:root:current train perplexity6.495536804199219
INFO:root:current mean train loss 2131.981458779612
INFO:root:current train perplexity6.496735095977783
INFO:root:current mean train loss 2130.32173421224
INFO:root:current train perplexity6.484861373901367
INFO:root:current mean train loss 2130.0709281529016
INFO:root:current train perplexity6.477639198303223
INFO:root:current mean train loss 2127.676853687183
INFO:root:current train perplexity6.4708051681518555
INFO:root:current mean train loss 2126.8836638621797
INFO:root:current train perplexity6.463494777679443

100%|██████████| 1/1 [05:42<00:00, 342.79s/it][A100%|██████████| 1/1 [05:42<00:00, 342.79s/it]
INFO:root:final mean train loss: 2125.2384113844632
INFO:root:final train perplexity: 6.460158824920654
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.98s/it][A100%|██████████| 1/1 [00:23<00:00, 23.98s/it]
INFO:root:eval mean loss: 1972.3765505526928
INFO:root:eval perplexity: 5.932075500488281
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.59s/it][A100%|██████████| 1/1 [00:22<00:00, 22.59s/it]
INFO:root:eval mean loss: 2258.3730070506426
INFO:root:eval perplexity: 7.961911678314209
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/4
  2%|▏         | 4/200 [28:19<22:57:05, 421.56s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2070.9960627769356
INFO:root:current train perplexity6.2234272956848145
INFO:root:current mean train loss 2085.3056399408215
INFO:root:current train perplexity6.193658351898193
INFO:root:current mean train loss 2091.82356496518
INFO:root:current train perplexity6.203040599822998
INFO:root:current mean train loss 2086.73770050132
INFO:root:current train perplexity6.212126731872559
INFO:root:current mean train loss 2090.023988515458
INFO:root:current train perplexity6.214737415313721
INFO:root:current mean train loss 2082.485637469687
INFO:root:current train perplexity6.198126316070557
INFO:root:current mean train loss 2082.0837052787083
INFO:root:current train perplexity6.185748100280762
INFO:root:current mean train loss 2080.9519253050644
INFO:root:current train perplexity6.185140132904053
INFO:root:current mean train loss 2075.9879971232517
INFO:root:current train perplexity6.169247627258301
INFO:root:current mean train loss 2075.675360126357
INFO:root:current train perplexity6.1700873374938965
INFO:root:current mean train loss 2075.029411737787
INFO:root:current train perplexity6.166799068450928
INFO:root:current mean train loss 2073.959392217458
INFO:root:current train perplexity6.168365478515625
INFO:root:current mean train loss 2071.812417624217
INFO:root:current train perplexity6.162421226501465
INFO:root:current mean train loss 2070.581151307894
INFO:root:current train perplexity6.156436443328857
INFO:root:current mean train loss 2070.875618755858
INFO:root:current train perplexity6.1525397300720215
INFO:root:current mean train loss 2071.226634713899
INFO:root:current train perplexity6.153344631195068
INFO:root:current mean train loss 2069.827312174284
INFO:root:current train perplexity6.147087097167969
INFO:root:current mean train loss 2069.775358777567
INFO:root:current train perplexity6.146006107330322
INFO:root:current mean train loss 2069.7250318154333
INFO:root:current train perplexity6.144318103790283
INFO:root:current mean train loss 2067.4864742742557
INFO:root:current train perplexity6.135779857635498

100%|██████████| 1/1 [05:38<00:00, 338.07s/it][A100%|██████████| 1/1 [05:38<00:00, 338.07s/it]
INFO:root:final mean train loss: 2066.4046702267124
INFO:root:final train perplexity: 6.134976387023926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.05s/it][A100%|██████████| 1/1 [00:24<00:00, 24.05s/it]
INFO:root:eval mean loss: 1939.7526686648105
INFO:root:eval perplexity: 5.759933948516846
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.76s/it][A100%|██████████| 1/1 [00:21<00:00, 21.76s/it]
INFO:root:eval mean loss: 2243.0906826587434
INFO:root:eval perplexity: 7.850913047790527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/5
  2%|▎         | 5/200 [34:45<22:08:12, 408.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2012.4279305594307
INFO:root:current train perplexity5.96244478225708
INFO:root:current mean train loss 2025.668585072393
INFO:root:current train perplexity5.9946136474609375
INFO:root:current mean train loss 2035.8636358556614
INFO:root:current train perplexity5.99738073348999
INFO:root:current mean train loss 2032.7149283091228
INFO:root:current train perplexity5.985851764678955
INFO:root:current mean train loss 2032.8323813194086
INFO:root:current train perplexity5.976020812988281
INFO:root:current mean train loss 2032.344143593148
INFO:root:current train perplexity5.975620746612549
INFO:root:current mean train loss 2034.043035496048
INFO:root:current train perplexity5.967283248901367
INFO:root:current mean train loss 2031.8140744579082
INFO:root:current train perplexity5.960099697113037
INFO:root:current mean train loss 2028.9199800102958
INFO:root:current train perplexity5.9403510093688965
INFO:root:current mean train loss 2027.5106311580998
INFO:root:current train perplexity5.935032367706299
INFO:root:current mean train loss 2028.1675278779765
INFO:root:current train perplexity5.935239791870117
INFO:root:current mean train loss 2026.2865175814243
INFO:root:current train perplexity5.926069259643555
INFO:root:current mean train loss 2025.4888835205838
INFO:root:current train perplexity5.918322563171387
INFO:root:current mean train loss 2024.9498066102838
INFO:root:current train perplexity5.917697429656982
INFO:root:current mean train loss 2024.6855864409167
INFO:root:current train perplexity5.916210651397705
INFO:root:current mean train loss 2023.0469445122612
INFO:root:current train perplexity5.907895565032959
INFO:root:current mean train loss 2022.0462992253608
INFO:root:current train perplexity5.90223503112793
INFO:root:current mean train loss 2020.6438668426376
INFO:root:current train perplexity5.895424842834473
INFO:root:current mean train loss 2020.4573377216445
INFO:root:current train perplexity5.891857147216797

100%|██████████| 1/1 [05:37<00:00, 337.22s/it][A100%|██████████| 1/1 [05:37<00:00, 337.22s/it]
INFO:root:final mean train loss: 2020.0219835213081
INFO:root:final train perplexity: 5.890193462371826
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.58s/it][A100%|██████████| 1/1 [00:23<00:00, 23.58s/it]
INFO:root:eval mean loss: 1913.5054875367077
INFO:root:eval perplexity: 5.6250715255737305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.41s/it][A100%|██████████| 1/1 [00:22<00:00, 22.41s/it]
INFO:root:eval mean loss: 2225.9508173516456
INFO:root:eval perplexity: 7.7282633781433105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/6
  3%|▎         | 6/200 [41:10<21:35:32, 400.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1859.447021484375
INFO:root:current train perplexity5.037502765655518
INFO:root:current mean train loss 1978.1733736850247
INFO:root:current train perplexity5.7876057624816895
INFO:root:current mean train loss 1978.8107072061566
INFO:root:current train perplexity5.729007244110107
INFO:root:current mean train loss 1990.8563155367524
INFO:root:current train perplexity5.7496161460876465
INFO:root:current mean train loss 1989.7468736605751
INFO:root:current train perplexity5.750679969787598
INFO:root:current mean train loss 1991.430416998035
INFO:root:current train perplexity5.747547149658203
INFO:root:current mean train loss 1990.387706077436
INFO:root:current train perplexity5.7464985847473145
INFO:root:current mean train loss 1989.6585665497391
INFO:root:current train perplexity5.737238883972168
INFO:root:current mean train loss 1990.3539900380872
INFO:root:current train perplexity5.734322547912598
INFO:root:current mean train loss 1991.8056445529273
INFO:root:current train perplexity5.734335422515869
INFO:root:current mean train loss 1991.6969801438795
INFO:root:current train perplexity5.729525089263916
INFO:root:current mean train loss 1989.5011964221092
INFO:root:current train perplexity5.728713035583496
INFO:root:current mean train loss 1989.4506023829426
INFO:root:current train perplexity5.721872806549072
INFO:root:current mean train loss 1988.896266412405
INFO:root:current train perplexity5.7152557373046875
INFO:root:current mean train loss 1986.0763699234085
INFO:root:current train perplexity5.712705612182617
INFO:root:current mean train loss 1985.5752217434472
INFO:root:current train perplexity5.713127136230469
INFO:root:current mean train loss 1986.6551181238044
INFO:root:current train perplexity5.714024066925049
INFO:root:current mean train loss 1985.5025667059358
INFO:root:current train perplexity5.711327075958252
INFO:root:current mean train loss 1985.5569496980845
INFO:root:current train perplexity5.708207130432129
INFO:root:current mean train loss 1984.4893509866313
INFO:root:current train perplexity5.703507423400879

100%|██████████| 1/1 [05:44<00:00, 344.35s/it][A100%|██████████| 1/1 [05:44<00:00, 344.35s/it]
INFO:root:final mean train loss: 1983.2481614659666
INFO:root:final train perplexity: 5.70308256149292
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.04s/it][A100%|██████████| 1/1 [00:23<00:00, 23.04s/it]
INFO:root:eval mean loss: 1892.0918990331338
INFO:root:eval perplexity: 5.517387866973877
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.44s/it][A100%|██████████| 1/1 [00:23<00:00, 23.44s/it]
INFO:root:eval mean loss: 2217.952309466423
INFO:root:eval perplexity: 7.671685695648193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/7
  4%|▎         | 7/200 [47:43<21:20:35, 398.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.4447292751736
INFO:root:current train perplexity5.573029518127441
INFO:root:current mean train loss 1954.2350743180614
INFO:root:current train perplexity5.587871074676514
INFO:root:current mean train loss 1948.8358574263546
INFO:root:current train perplexity5.54595422744751
INFO:root:current mean train loss 1958.8869187457008
INFO:root:current train perplexity5.557178974151611
INFO:root:current mean train loss 1958.916743082293
INFO:root:current train perplexity5.560915470123291
INFO:root:current mean train loss 1960.0109302417652
INFO:root:current train perplexity5.573362827301025
INFO:root:current mean train loss 1962.1381166328504
INFO:root:current train perplexity5.58499002456665
INFO:root:current mean train loss 1958.8293089800227
INFO:root:current train perplexity5.577201843261719
INFO:root:current mean train loss 1959.3135255100092
INFO:root:current train perplexity5.576911926269531
INFO:root:current mean train loss 1956.92591116579
INFO:root:current train perplexity5.565371990203857
INFO:root:current mean train loss 1956.3289158189696
INFO:root:current train perplexity5.564981937408447
INFO:root:current mean train loss 1954.0321338633091
INFO:root:current train perplexity5.562681674957275
INFO:root:current mean train loss 1953.1101034129977
INFO:root:current train perplexity5.5587158203125
INFO:root:current mean train loss 1952.89930431036
INFO:root:current train perplexity5.555714130401611
INFO:root:current mean train loss 1953.1565619765956
INFO:root:current train perplexity5.5568366050720215
INFO:root:current mean train loss 1953.0741014241858
INFO:root:current train perplexity5.555537700653076
INFO:root:current mean train loss 1952.5122193288155
INFO:root:current train perplexity5.551619052886963
INFO:root:current mean train loss 1952.1941978800976
INFO:root:current train perplexity5.552270412445068
INFO:root:current mean train loss 1952.798560349199
INFO:root:current train perplexity5.554361820220947
INFO:root:current mean train loss 1952.949543401024
INFO:root:current train perplexity5.553685188293457

100%|██████████| 1/1 [05:36<00:00, 336.42s/it][A100%|██████████| 1/1 [05:36<00:00, 336.42s/it]
INFO:root:final mean train loss: 1952.5940497278625
INFO:root:final train perplexity: 5.55165958404541
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.19s/it][A100%|██████████| 1/1 [00:24<00:00, 24.19s/it]
INFO:root:eval mean loss: 1877.025069432901
INFO:root:eval perplexity: 5.442859172821045
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.34s/it][A100%|██████████| 1/1 [00:23<00:00, 23.34s/it]
INFO:root:eval mean loss: 2210.5364453471298
INFO:root:eval perplexity: 7.619597911834717
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/8
  4%|▍         | 8/200 [54:08<21:01:36, 394.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1941.7386265345983
INFO:root:current train perplexity5.455619812011719
INFO:root:current mean train loss 1944.9237594039353
INFO:root:current train perplexity5.441135406494141
INFO:root:current mean train loss 1931.8473456200134
INFO:root:current train perplexity5.425340175628662
INFO:root:current mean train loss 1929.2375881821363
INFO:root:current train perplexity5.409510135650635
INFO:root:current mean train loss 1934.1236900592673
INFO:root:current train perplexity5.424758434295654
INFO:root:current mean train loss 1934.9389977000585
INFO:root:current train perplexity5.4327168464660645
INFO:root:current mean train loss 1931.9639431210014
INFO:root:current train perplexity5.427700519561768
INFO:root:current mean train loss 1928.131707223905
INFO:root:current train perplexity5.419657230377197
INFO:root:current mean train loss 1929.3743953499252
INFO:root:current train perplexity5.424932479858398
INFO:root:current mean train loss 1929.079666741519
INFO:root:current train perplexity5.421445846557617
INFO:root:current mean train loss 1927.8194443265022
INFO:root:current train perplexity5.4183549880981445
INFO:root:current mean train loss 1925.0593775812224
INFO:root:current train perplexity5.415673732757568
INFO:root:current mean train loss 1927.6877280293206
INFO:root:current train perplexity5.425552845001221
INFO:root:current mean train loss 1927.0156901956052
INFO:root:current train perplexity5.420560836791992
INFO:root:current mean train loss 1927.2329197687554
INFO:root:current train perplexity5.42387580871582
INFO:root:current mean train loss 1926.3832310381463
INFO:root:current train perplexity5.421413898468018
INFO:root:current mean train loss 1925.8618662049646
INFO:root:current train perplexity5.42276668548584
INFO:root:current mean train loss 1926.3326824792866
INFO:root:current train perplexity5.423005104064941
INFO:root:current mean train loss 1926.903046435946
INFO:root:current train perplexity5.423744201660156
INFO:root:current mean train loss 1926.9463217407542
INFO:root:current train perplexity5.423609733581543

100%|██████████| 1/1 [05:36<00:00, 336.15s/it][A100%|██████████| 1/1 [05:36<00:00, 336.15s/it]
INFO:root:final mean train loss: 1925.6052455392319
INFO:root:final train perplexity: 5.42167329788208
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.73s/it][A100%|██████████| 1/1 [00:23<00:00, 23.73s/it]
INFO:root:eval mean loss: 1862.9092130187555
INFO:root:eval perplexity: 5.373948574066162
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.51s/it][A100%|██████████| 1/1 [00:22<00:00, 22.51s/it]
INFO:root:eval mean loss: 2201.2744058379044
INFO:root:eval perplexity: 7.555041790008545
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/9
  4%|▍         | 9/200 [1:00:33<20:45:13, 391.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1893.3900052584136
INFO:root:current train perplexity5.293708324432373
INFO:root:current mean train loss 1910.2334514417146
INFO:root:current train perplexity5.31480073928833
INFO:root:current mean train loss 1914.949455140129
INFO:root:current train perplexity5.3216552734375
INFO:root:current mean train loss 1914.6666596152565
INFO:root:current train perplexity5.329960346221924
INFO:root:current mean train loss 1916.1580986090466
INFO:root:current train perplexity5.345179557800293
INFO:root:current mean train loss 1910.3295582204626
INFO:root:current train perplexity5.331949234008789
INFO:root:current mean train loss 1910.3264817313914
INFO:root:current train perplexity5.333290100097656
INFO:root:current mean train loss 1910.4455196299452
INFO:root:current train perplexity5.33186674118042
INFO:root:current mean train loss 1907.8312266175176
INFO:root:current train perplexity5.330141544342041
INFO:root:current mean train loss 1906.9248770064667
INFO:root:current train perplexity5.32749605178833
INFO:root:current mean train loss 1905.5730155683743
INFO:root:current train perplexity5.327402591705322
INFO:root:current mean train loss 1904.1618132061428
INFO:root:current train perplexity5.324875831604004
INFO:root:current mean train loss 1904.4562502730007
INFO:root:current train perplexity5.321238040924072
INFO:root:current mean train loss 1903.3164285513071
INFO:root:current train perplexity5.317774295806885
INFO:root:current mean train loss 1904.368870506602
INFO:root:current train perplexity5.319973945617676
INFO:root:current mean train loss 1905.3534027571532
INFO:root:current train perplexity5.322382926940918
INFO:root:current mean train loss 1906.366079408955
INFO:root:current train perplexity5.319713115692139
INFO:root:current mean train loss 1905.346535739289
INFO:root:current train perplexity5.315429210662842
INFO:root:current mean train loss 1903.4105098716127
INFO:root:current train perplexity5.313714981079102
INFO:root:current mean train loss 1903.2516806555575
INFO:root:current train perplexity5.313620567321777

100%|██████████| 1/1 [05:32<00:00, 332.85s/it][A100%|██████████| 1/1 [05:32<00:00, 332.86s/it]
INFO:root:final mean train loss: 1902.3737878227137
INFO:root:final train perplexity: 5.312225341796875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.97s/it][A100%|██████████| 1/1 [00:23<00:00, 23.97s/it]
INFO:root:eval mean loss: 1852.024069495235
INFO:root:eval perplexity: 5.321403980255127
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.19s/it][A100%|██████████| 1/1 [00:23<00:00, 23.19s/it]
INFO:root:eval mean loss: 2198.3557146221187
INFO:root:eval perplexity: 7.534811973571777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/10
  5%|▌         | 10/200 [1:06:55<20:29:42, 388.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1848.7228172412817
INFO:root:current train perplexity5.139746189117432
INFO:root:current mean train loss 1854.9359318659856
INFO:root:current train perplexity5.155165195465088
INFO:root:current mean train loss 1868.8055061425418
INFO:root:current train perplexity5.185000419616699
INFO:root:current mean train loss 1878.2067758617038
INFO:root:current train perplexity5.2001237869262695
INFO:root:current mean train loss 1877.1446731014291
INFO:root:current train perplexity5.20059871673584
INFO:root:current mean train loss 1879.6504961761314
INFO:root:current train perplexity5.200131893157959
INFO:root:current mean train loss 1881.3306264378386
INFO:root:current train perplexity5.202524185180664
INFO:root:current mean train loss 1881.6693739078755
INFO:root:current train perplexity5.204884052276611
INFO:root:current mean train loss 1880.7199721078466
INFO:root:current train perplexity5.208233833312988
INFO:root:current mean train loss 1880.2719105503015
INFO:root:current train perplexity5.2119364738464355
INFO:root:current mean train loss 1879.4745447428234
INFO:root:current train perplexity5.213633060455322
INFO:root:current mean train loss 1881.6145073831133
INFO:root:current train perplexity5.212673187255859
INFO:root:current mean train loss 1882.9068509245408
INFO:root:current train perplexity5.215899467468262
INFO:root:current mean train loss 1883.57873615407
INFO:root:current train perplexity5.21812629699707
INFO:root:current mean train loss 1884.5491609307192
INFO:root:current train perplexity5.220547199249268
INFO:root:current mean train loss 1883.7954530247919
INFO:root:current train perplexity5.219104290008545
INFO:root:current mean train loss 1883.5070830037166
INFO:root:current train perplexity5.217923164367676
INFO:root:current mean train loss 1883.6922182349447
INFO:root:current train perplexity5.219632148742676
INFO:root:current mean train loss 1883.3841348957287
INFO:root:current train perplexity5.220637798309326
INFO:root:current mean train loss 1882.3316020510292
INFO:root:current train perplexity5.218682765960693

100%|██████████| 1/1 [05:42<00:00, 342.27s/it][A100%|██████████| 1/1 [05:42<00:00, 342.27s/it]
INFO:root:final mean train loss: 1881.8151146008158
INFO:root:final train perplexity: 5.217211723327637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.89s/it][A100%|██████████| 1/1 [00:24<00:00, 24.89s/it]
INFO:root:eval mean loss: 1844.5815879875886
INFO:root:eval perplexity: 5.285775661468506
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.39s/it][A100%|██████████| 1/1 [00:23<00:00, 23.39s/it]
INFO:root:eval mean loss: 2198.082803929106
INFO:root:eval perplexity: 7.532922267913818
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/11
  6%|▌         | 11/200 [1:15:56<22:50:11, 434.98s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.8135333393896
INFO:root:current train perplexity5.125404357910156
INFO:root:current mean train loss 1862.750473186534
INFO:root:current train perplexity5.137197971343994
INFO:root:current mean train loss 1867.8877503721865
INFO:root:current train perplexity5.138989448547363
INFO:root:current mean train loss 1867.8968635519552
INFO:root:current train perplexity5.130856990814209
INFO:root:current mean train loss 1872.3969412595648
INFO:root:current train perplexity5.1415696144104
INFO:root:current mean train loss 1873.3650402290423
INFO:root:current train perplexity5.140619277954102
INFO:root:current mean train loss 1871.0453249205652
INFO:root:current train perplexity5.137934684753418
INFO:root:current mean train loss 1868.6427146387464
INFO:root:current train perplexity5.133831977844238
INFO:root:current mean train loss 1866.5490285903552
INFO:root:current train perplexity5.138201713562012
INFO:root:current mean train loss 1865.9789813740017
INFO:root:current train perplexity5.136255264282227
INFO:root:current mean train loss 1865.5707319902451
INFO:root:current train perplexity5.136179447174072
INFO:root:current mean train loss 1866.1639556627451
INFO:root:current train perplexity5.140458106994629
INFO:root:current mean train loss 1865.489252117151
INFO:root:current train perplexity5.140297889709473
INFO:root:current mean train loss 1865.0409475439383
INFO:root:current train perplexity5.136142253875732
INFO:root:current mean train loss 1864.356452952163
INFO:root:current train perplexity5.131932735443115
INFO:root:current mean train loss 1865.2057269602676
INFO:root:current train perplexity5.135105133056641
INFO:root:current mean train loss 1864.6783765111813
INFO:root:current train perplexity5.131864547729492
INFO:root:current mean train loss 1865.2600762686563
INFO:root:current train perplexity5.132908344268799
INFO:root:current mean train loss 1863.934041772642
INFO:root:current train perplexity5.130501747131348

100%|██████████| 1/1 [05:33<00:00, 333.68s/it][A100%|██████████| 1/1 [05:33<00:00, 333.68s/it]
INFO:root:final mean train loss: 1862.3612163041616
INFO:root:final train perplexity: 5.128870964050293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.62s/it][A100%|██████████| 1/1 [00:23<00:00, 23.62s/it]
INFO:root:eval mean loss: 1834.7855428059895
INFO:root:eval perplexity: 5.2392425537109375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.47s/it][A100%|██████████| 1/1 [00:23<00:00, 23.47s/it]
INFO:root:eval mean loss: 2189.891734454649
INFO:root:eval perplexity: 7.476451873779297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/12
  6%|▌         | 12/200 [1:22:46<22:19:11, 427.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1875.3557942708333
INFO:root:current train perplexity4.797155857086182
INFO:root:current mean train loss 1864.9112489570691
INFO:root:current train perplexity5.088120460510254
INFO:root:current mean train loss 1855.5936219163716
INFO:root:current train perplexity5.059680461883545
INFO:root:current mean train loss 1848.2990086117986
INFO:root:current train perplexity5.0573883056640625
INFO:root:current mean train loss 1848.0536003775396
INFO:root:current train perplexity5.053389549255371
INFO:root:current mean train loss 1851.1233616368197
INFO:root:current train perplexity5.062701225280762
INFO:root:current mean train loss 1847.7581068453305
INFO:root:current train perplexity5.055181503295898
INFO:root:current mean train loss 1847.7442710301275
INFO:root:current train perplexity5.053567409515381
INFO:root:current mean train loss 1845.574126475181
INFO:root:current train perplexity5.045901298522949
INFO:root:current mean train loss 1846.6382290152617
INFO:root:current train perplexity5.050593376159668
INFO:root:current mean train loss 1848.2013180429415
INFO:root:current train perplexity5.046865463256836
INFO:root:current mean train loss 1845.6741215143004
INFO:root:current train perplexity5.045705318450928
INFO:root:current mean train loss 1845.5084228515625
INFO:root:current train perplexity5.0480475425720215
INFO:root:current mean train loss 1846.442314235778
INFO:root:current train perplexity5.053946495056152
INFO:root:current mean train loss 1846.2726467836778
INFO:root:current train perplexity5.053265571594238
INFO:root:current mean train loss 1846.3945113516456
INFO:root:current train perplexity5.05622673034668
INFO:root:current mean train loss 1846.6550902939557
INFO:root:current train perplexity5.057748317718506
INFO:root:current mean train loss 1846.1927398484522
INFO:root:current train perplexity5.054008960723877
INFO:root:current mean train loss 1846.58835080909
INFO:root:current train perplexity5.057430267333984
INFO:root:current mean train loss 1846.1622109185128
INFO:root:current train perplexity5.0570149421691895

100%|██████████| 1/1 [05:35<00:00, 335.33s/it][A100%|██████████| 1/1 [05:35<00:00, 335.33s/it]
INFO:root:final mean train loss: 1845.9135491029217
INFO:root:final train perplexity: 5.055347919464111
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.77s/it][A100%|██████████| 1/1 [00:23<00:00, 23.77s/it]
INFO:root:eval mean loss: 1827.2181907275044
INFO:root:eval perplexity: 5.20357608795166
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 22.00s/it][A100%|██████████| 1/1 [00:21<00:00, 22.00s/it]
INFO:root:eval mean loss: 2186.7220026110926
INFO:root:eval perplexity: 7.454713344573975
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/13
  6%|▋         | 13/200 [1:29:09<21:30:18, 414.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1831.3936401367187
INFO:root:current train perplexity4.963240623474121
INFO:root:current mean train loss 1835.0723744710288
INFO:root:current train perplexity4.931284427642822
INFO:root:current mean train loss 1829.9084544788707
INFO:root:current train perplexity4.945122718811035
INFO:root:current mean train loss 1832.3332588195801
INFO:root:current train perplexity4.954257965087891
INFO:root:current mean train loss 1833.8395458403088
INFO:root:current train perplexity4.965429306030273
INFO:root:current mean train loss 1830.6016301081731
INFO:root:current train perplexity4.973474025726318
INFO:root:current mean train loss 1830.568260734312
INFO:root:current train perplexity4.973056793212891
INFO:root:current mean train loss 1829.2647518581814
INFO:root:current train perplexity4.968442916870117
INFO:root:current mean train loss 1831.2370873427972
INFO:root:current train perplexity4.9728474617004395
INFO:root:current mean train loss 1832.0046836521315
INFO:root:current train perplexity4.977532863616943
INFO:root:current mean train loss 1830.305209649778
INFO:root:current train perplexity4.9795331954956055
INFO:root:current mean train loss 1829.7851731436592
INFO:root:current train perplexity4.982290267944336
INFO:root:current mean train loss 1829.347632236168
INFO:root:current train perplexity4.979884147644043
INFO:root:current mean train loss 1829.5561308889678
INFO:root:current train perplexity4.981588363647461
INFO:root:current mean train loss 1829.8123971858495
INFO:root:current train perplexity4.98037052154541
INFO:root:current mean train loss 1829.1650460494193
INFO:root:current train perplexity4.977409839630127
INFO:root:current mean train loss 1829.6977664146893
INFO:root:current train perplexity4.978118419647217
INFO:root:current mean train loss 1831.4132899794467
INFO:root:current train perplexity4.9832444190979
INFO:root:current mean train loss 1829.9169627430674
INFO:root:current train perplexity4.983186721801758
INFO:root:current mean train loss 1829.340201886495
INFO:root:current train perplexity4.981450080871582

100%|██████████| 1/1 [05:31<00:00, 331.35s/it][A100%|██████████| 1/1 [05:31<00:00, 331.36s/it]
INFO:root:final mean train loss: 1829.4677718062023
INFO:root:final train perplexity: 4.982888221740723
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.73s/it][A100%|██████████| 1/1 [00:23<00:00, 23.73s/it]
INFO:root:eval mean loss: 1821.3439950063719
INFO:root:eval perplexity: 5.176058292388916
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.56s/it][A100%|██████████| 1/1 [00:22<00:00, 22.56s/it]
INFO:root:eval mean loss: 2185.452241938165
INFO:root:eval perplexity: 7.446022033691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/14
  7%|▋         | 14/200 [1:35:28<20:51:10, 403.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1844.8281810863598
INFO:root:current train perplexity5.043003559112549
INFO:root:current mean train loss 1829.412208278684
INFO:root:current train perplexity4.971292018890381
INFO:root:current mean train loss 1825.1336273322127
INFO:root:current train perplexity4.93615198135376
INFO:root:current mean train loss 1821.1722973560368
INFO:root:current train perplexity4.907973289489746
INFO:root:current mean train loss 1819.1982276619708
INFO:root:current train perplexity4.9179158210754395
INFO:root:current mean train loss 1820.0335400117842
INFO:root:current train perplexity4.913711071014404
INFO:root:current mean train loss 1821.980084525547
INFO:root:current train perplexity4.918432235717773
INFO:root:current mean train loss 1817.82542421504
INFO:root:current train perplexity4.915655136108398
INFO:root:current mean train loss 1819.8334001292749
INFO:root:current train perplexity4.92167854309082
INFO:root:current mean train loss 1819.2037309221168
INFO:root:current train perplexity4.920934200286865
INFO:root:current mean train loss 1817.4939449828985
INFO:root:current train perplexity4.919698238372803
INFO:root:current mean train loss 1815.6085720414537
INFO:root:current train perplexity4.916562557220459
INFO:root:current mean train loss 1816.071765107114
INFO:root:current train perplexity4.9164958000183105
INFO:root:current mean train loss 1816.2745474542178
INFO:root:current train perplexity4.916374683380127
INFO:root:current mean train loss 1817.0956544497815
INFO:root:current train perplexity4.921358585357666
INFO:root:current mean train loss 1817.4082973184877
INFO:root:current train perplexity4.9234724044799805
INFO:root:current mean train loss 1816.100807930976
INFO:root:current train perplexity4.925770282745361
INFO:root:current mean train loss 1815.7549370659722
INFO:root:current train perplexity4.922811508178711
INFO:root:current mean train loss 1815.8887202512587
INFO:root:current train perplexity4.923193454742432
INFO:root:current mean train loss 1814.8705160429506
INFO:root:current train perplexity4.918330669403076

100%|██████████| 1/1 [05:41<00:00, 341.33s/it][A100%|██████████| 1/1 [05:41<00:00, 341.33s/it]
INFO:root:final mean train loss: 1814.7349182529035
INFO:root:final train perplexity: 4.918857574462891
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.58s/it][A100%|██████████| 1/1 [00:23<00:00, 23.58s/it]
INFO:root:eval mean loss: 1815.561889215564
INFO:root:eval perplexity: 5.149113655090332
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.13s/it][A100%|██████████| 1/1 [00:23<00:00, 23.13s/it]
INFO:root:eval mean loss: 2183.330455590647
INFO:root:eval perplexity: 7.431525230407715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/15
  8%|▊         | 15/200 [1:41:58<20:31:48, 399.50s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1783.8329784252026
INFO:root:current train perplexity4.8145060539245605
INFO:root:current mean train loss 1805.5060274198459
INFO:root:current train perplexity4.87812614440918
INFO:root:current mean train loss 1802.30975918507
INFO:root:current train perplexity4.854780673980713
INFO:root:current mean train loss 1802.8637974625926
INFO:root:current train perplexity4.861664772033691
INFO:root:current mean train loss 1804.113935428569
INFO:root:current train perplexity4.867411136627197
INFO:root:current mean train loss 1802.2480457732825
INFO:root:current train perplexity4.864694595336914
INFO:root:current mean train loss 1800.862996232619
INFO:root:current train perplexity4.8602471351623535
INFO:root:current mean train loss 1802.6468837748155
INFO:root:current train perplexity4.862310409545898
INFO:root:current mean train loss 1801.7066637526073
INFO:root:current train perplexity4.8613362312316895
INFO:root:current mean train loss 1802.058601811247
INFO:root:current train perplexity4.864192962646484
INFO:root:current mean train loss 1801.4406995393294
INFO:root:current train perplexity4.86340856552124
INFO:root:current mean train loss 1801.099978442002
INFO:root:current train perplexity4.86619234085083
INFO:root:current mean train loss 1801.7522915926847
INFO:root:current train perplexity4.868950843811035
INFO:root:current mean train loss 1800.746964650457
INFO:root:current train perplexity4.8662333488464355
INFO:root:current mean train loss 1801.215315827969
INFO:root:current train perplexity4.862462520599365
INFO:root:current mean train loss 1801.5689247160806
INFO:root:current train perplexity4.860886096954346
INFO:root:current mean train loss 1801.1786870252986
INFO:root:current train perplexity4.859940052032471
INFO:root:current mean train loss 1800.876244504748
INFO:root:current train perplexity4.859632968902588
INFO:root:current mean train loss 1800.2419228167983
INFO:root:current train perplexity4.8595499992370605
INFO:root:current mean train loss 1800.9235381299177
INFO:root:current train perplexity4.858692646026611

100%|██████████| 1/1 [05:35<00:00, 335.05s/it][A100%|██████████| 1/1 [05:35<00:00, 335.05s/it]
INFO:root:final mean train loss: 1801.0532385383178
INFO:root:final train perplexity: 4.860133647918701
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.11s/it][A100%|██████████| 1/1 [00:24<00:00, 24.11s/it]
INFO:root:eval mean loss: 1811.3255385811447
INFO:root:eval perplexity: 5.12946081161499
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.63s/it][A100%|██████████| 1/1 [00:22<00:00, 22.63s/it]
INFO:root:eval mean loss: 2181.65219311004
INFO:root:eval perplexity: 7.420073986053467
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/16
  8%|▊         | 16/200 [1:48:22<20:10:33, 394.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1774.5425458021566
INFO:root:current train perplexity4.709718704223633
INFO:root:current mean train loss 1773.7015923394097
INFO:root:current train perplexity4.749068260192871
INFO:root:current mean train loss 1778.5548347951742
INFO:root:current train perplexity4.756248950958252
INFO:root:current mean train loss 1783.463727349541
INFO:root:current train perplexity4.7819342613220215
INFO:root:current mean train loss 1784.6604094616673
INFO:root:current train perplexity4.783073902130127
INFO:root:current mean train loss 1784.8840729668345
INFO:root:current train perplexity4.7845845222473145
INFO:root:current mean train loss 1784.303984549646
INFO:root:current train perplexity4.780440330505371
INFO:root:current mean train loss 1783.7112623875244
INFO:root:current train perplexity4.783539772033691
INFO:root:current mean train loss 1783.8670351573712
INFO:root:current train perplexity4.782421112060547
INFO:root:current mean train loss 1784.6938264102328
INFO:root:current train perplexity4.787286281585693
INFO:root:current mean train loss 1784.7381077756625
INFO:root:current train perplexity4.791263580322266
INFO:root:current mean train loss 1785.993294368128
INFO:root:current train perplexity4.794710636138916
INFO:root:current mean train loss 1787.3885319407393
INFO:root:current train perplexity4.800466537475586
INFO:root:current mean train loss 1786.813387346998
INFO:root:current train perplexity4.800987720489502
INFO:root:current mean train loss 1786.4382231276024
INFO:root:current train perplexity4.799258232116699
INFO:root:current mean train loss 1787.418303724613
INFO:root:current train perplexity4.799952983856201
INFO:root:current mean train loss 1788.354786807231
INFO:root:current train perplexity4.802512168884277
INFO:root:current mean train loss 1789.0485691649976
INFO:root:current train perplexity4.805037975311279
INFO:root:current mean train loss 1789.0017094410784
INFO:root:current train perplexity4.805964469909668
INFO:root:current mean train loss 1788.3114650964374
INFO:root:current train perplexity4.803481578826904

100%|██████████| 1/1 [05:42<00:00, 342.40s/it][A100%|██████████| 1/1 [05:42<00:00, 342.40s/it]
INFO:root:final mean train loss: 1787.9578671515499
INFO:root:final train perplexity: 4.804581642150879
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.56s/it][A100%|██████████| 1/1 [00:23<00:00, 23.56s/it]
INFO:root:eval mean loss: 1807.8792365324412
INFO:root:eval perplexity: 5.113528728485107
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.55s/it][A100%|██████████| 1/1 [00:22<00:00, 22.55s/it]
INFO:root:eval mean loss: 2182.148914093667
INFO:root:eval perplexity: 7.423460483551025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/17
  8%|▊         | 17/200 [1:54:53<20:00:25, 393.58s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1756.3091153231535
INFO:root:current train perplexity4.73378324508667
INFO:root:current mean train loss 1761.672465222947
INFO:root:current train perplexity4.7186198234558105
INFO:root:current mean train loss 1771.1089469061958
INFO:root:current train perplexity4.749170780181885
INFO:root:current mean train loss 1772.650942772934
INFO:root:current train perplexity4.751712322235107
INFO:root:current mean train loss 1776.6603215952389
INFO:root:current train perplexity4.760128021240234
INFO:root:current mean train loss 1775.2452270092608
INFO:root:current train perplexity4.760662078857422
INFO:root:current mean train loss 1775.186201051224
INFO:root:current train perplexity4.7595953941345215
INFO:root:current mean train loss 1773.3530687051375
INFO:root:current train perplexity4.756482124328613
INFO:root:current mean train loss 1772.9795735677083
INFO:root:current train perplexity4.753279209136963
INFO:root:current mean train loss 1771.612074879017
INFO:root:current train perplexity4.748623847961426
INFO:root:current mean train loss 1772.2391718696144
INFO:root:current train perplexity4.7477946281433105
INFO:root:current mean train loss 1773.4132390391546
INFO:root:current train perplexity4.7505998611450195
INFO:root:current mean train loss 1774.5508135683049
INFO:root:current train perplexity4.752799987792969
INFO:root:current mean train loss 1775.4408171815899
INFO:root:current train perplexity4.753373146057129
INFO:root:current mean train loss 1776.3121078655283
INFO:root:current train perplexity4.757045745849609
INFO:root:current mean train loss 1775.5744011636345
INFO:root:current train perplexity4.754293918609619
INFO:root:current mean train loss 1774.784033101882
INFO:root:current train perplexity4.753190040588379
INFO:root:current mean train loss 1774.897060727113
INFO:root:current train perplexity4.751073837280273
INFO:root:current mean train loss 1775.119907055871
INFO:root:current train perplexity4.750469207763672

100%|██████████| 1/1 [05:40<00:00, 340.47s/it][A100%|██████████| 1/1 [05:40<00:00, 340.47s/it]
INFO:root:final mean train loss: 1775.5547572764494
INFO:root:final train perplexity: 4.752552509307861
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.56s/it][A100%|██████████| 1/1 [00:23<00:00, 23.56s/it]
INFO:root:eval mean loss: 1802.6701508650542
INFO:root:eval perplexity: 5.089541912078857
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.62s/it][A100%|██████████| 1/1 [00:22<00:00, 22.62s/it]
INFO:root:eval mean loss: 2178.7031998871066
INFO:root:eval perplexity: 7.40000057220459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/18
  9%|▉         | 18/200 [2:01:22<19:49:17, 392.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1771.6665283203124
INFO:root:current train perplexity4.549520969390869
INFO:root:current mean train loss 1763.6115385509672
INFO:root:current train perplexity4.661201477050781
INFO:root:current mean train loss 1759.3623034965701
INFO:root:current train perplexity4.645353317260742
INFO:root:current mean train loss 1761.7827808817879
INFO:root:current train perplexity4.674072742462158
INFO:root:current mean train loss 1762.3976215880593
INFO:root:current train perplexity4.687982559204102
INFO:root:current mean train loss 1762.8439477297338
INFO:root:current train perplexity4.704158782958984
INFO:root:current mean train loss 1761.7196835856791
INFO:root:current train perplexity4.701447486877441
INFO:root:current mean train loss 1762.2859967170878
INFO:root:current train perplexity4.696926116943359
INFO:root:current mean train loss 1759.8470563616072
INFO:root:current train perplexity4.688777446746826
INFO:root:current mean train loss 1762.6259075017265
INFO:root:current train perplexity4.6902570724487305
INFO:root:current mean train loss 1763.562789446323
INFO:root:current train perplexity4.692040920257568
INFO:root:current mean train loss 1765.766954627404
INFO:root:current train perplexity4.694593906402588
INFO:root:current mean train loss 1765.0697900795838
INFO:root:current train perplexity4.698781490325928
INFO:root:current mean train loss 1764.5191357608956
INFO:root:current train perplexity4.69934606552124
INFO:root:current mean train loss 1764.8211075643628
INFO:root:current train perplexity4.700275421142578
INFO:root:current mean train loss 1763.498790571143
INFO:root:current train perplexity4.698390960693359
INFO:root:current mean train loss 1765.26304699669
INFO:root:current train perplexity4.703495502471924
INFO:root:current mean train loss 1765.0394514783036
INFO:root:current train perplexity4.705706596374512
INFO:root:current mean train loss 1764.4829156341975
INFO:root:current train perplexity4.703403472900391
INFO:root:current mean train loss 1765.2826371801182
INFO:root:current train perplexity4.705776691436768

100%|██████████| 1/1 [05:31<00:00, 331.84s/it][A100%|██████████| 1/1 [05:31<00:00, 331.84s/it]
INFO:root:final mean train loss: 1763.9838629264273
INFO:root:final train perplexity: 4.704522609710693
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.82s/it][A100%|██████████| 1/1 [00:23<00:00, 23.82s/it]
INFO:root:eval mean loss: 1800.7843606286015
INFO:root:eval perplexity: 5.080885410308838
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.88s/it][A100%|██████████| 1/1 [00:21<00:00, 21.88s/it]
INFO:root:eval mean loss: 2181.2649064300754
INFO:root:eval perplexity: 7.417436599731445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/19
 10%|▉         | 19/200 [2:07:41<19:31:22, 388.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1782.8725142045455
INFO:root:current train perplexity4.701476097106934
INFO:root:current mean train loss 1763.293662149398
INFO:root:current train perplexity4.661019325256348
INFO:root:current mean train loss 1769.167426031989
INFO:root:current train perplexity4.645830154418945
INFO:root:current mean train loss 1762.735316685268
INFO:root:current train perplexity4.647355079650879
INFO:root:current mean train loss 1761.8263066730228
INFO:root:current train perplexity4.647701263427734
INFO:root:current mean train loss 1764.2017838635206
INFO:root:current train perplexity4.6602783203125
INFO:root:current mean train loss 1762.9886975058405
INFO:root:current train perplexity4.658599376678467
INFO:root:current mean train loss 1761.6703391458188
INFO:root:current train perplexity4.6606364250183105
INFO:root:current mean train loss 1758.9491222223805
INFO:root:current train perplexity4.66549825668335
INFO:root:current mean train loss 1759.0545173694668
INFO:root:current train perplexity4.665082931518555
INFO:root:current mean train loss 1759.5082680539842
INFO:root:current train perplexity4.667031764984131
INFO:root:current mean train loss 1757.3963514249806
INFO:root:current train perplexity4.661746501922607
INFO:root:current mean train loss 1755.857741335605
INFO:root:current train perplexity4.659511089324951
INFO:root:current mean train loss 1755.619395292112
INFO:root:current train perplexity4.663725852966309
INFO:root:current mean train loss 1756.406062602326
INFO:root:current train perplexity4.663989067077637
INFO:root:current mean train loss 1754.9706186703095
INFO:root:current train perplexity4.663641929626465
INFO:root:current mean train loss 1754.6632423259769
INFO:root:current train perplexity4.662746429443359
INFO:root:current mean train loss 1754.5223701290968
INFO:root:current train perplexity4.664029598236084
INFO:root:current mean train loss 1753.77169226972
INFO:root:current train perplexity4.662721633911133
INFO:root:current mean train loss 1754.2503809457514
INFO:root:current train perplexity4.662051200866699

100%|██████████| 1/1 [05:43<00:00, 343.12s/it][A100%|██████████| 1/1 [05:43<00:00, 343.12s/it]
INFO:root:final mean train loss: 1753.5320043366664
INFO:root:final train perplexity: 4.661554336547852
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.93s/it][A100%|██████████| 1/1 [00:23<00:00, 23.93s/it]
INFO:root:eval mean loss: 1796.4943155058731
INFO:root:eval perplexity: 5.061247825622559
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.44s/it][A100%|██████████| 1/1 [00:23<00:00, 23.44s/it]
INFO:root:eval mean loss: 2181.1061063725897
INFO:root:eval perplexity: 7.41635274887085
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/20
 10%|█         | 20/200 [2:14:14<19:28:37, 389.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1737.710709009415
INFO:root:current train perplexity4.635412693023682
INFO:root:current mean train loss 1727.447369253035
INFO:root:current train perplexity4.567633628845215
INFO:root:current mean train loss 1730.0918918753268
INFO:root:current train perplexity4.557340621948242
INFO:root:current mean train loss 1736.5660609242486
INFO:root:current train perplexity4.58433723449707
INFO:root:current mean train loss 1741.5460463678103
INFO:root:current train perplexity4.59481954574585
INFO:root:current mean train loss 1738.9932933800296
INFO:root:current train perplexity4.590602874755859
INFO:root:current mean train loss 1739.2830289025821
INFO:root:current train perplexity4.5868821144104
INFO:root:current mean train loss 1739.8950687558145
INFO:root:current train perplexity4.593357563018799
INFO:root:current mean train loss 1738.2731860846245
INFO:root:current train perplexity4.588261127471924
INFO:root:current mean train loss 1736.583971894968
INFO:root:current train perplexity4.5882039070129395
INFO:root:current mean train loss 1739.6659488916628
INFO:root:current train perplexity4.598447799682617
INFO:root:current mean train loss 1740.484459666854
INFO:root:current train perplexity4.603111267089844
INFO:root:current mean train loss 1741.8121512276787
INFO:root:current train perplexity4.605844020843506
INFO:root:current mean train loss 1742.9387197003068
INFO:root:current train perplexity4.607740879058838
INFO:root:current mean train loss 1743.0793490963233
INFO:root:current train perplexity4.606976509094238
INFO:root:current mean train loss 1742.3156057733308
INFO:root:current train perplexity4.606677055358887
INFO:root:current mean train loss 1741.3731388559859
INFO:root:current train perplexity4.6078200340271
INFO:root:current mean train loss 1741.8561794112097
INFO:root:current train perplexity4.610493183135986
INFO:root:current mean train loss 1741.7071237818193
INFO:root:current train perplexity4.608639717102051
INFO:root:current mean train loss 1742.4054625551992
INFO:root:current train perplexity4.612779140472412

100%|██████████| 1/1 [05:38<00:00, 338.36s/it][A100%|██████████| 1/1 [05:38<00:00, 338.37s/it]
INFO:root:final mean train loss: 1741.9870657793392
INFO:root:final train perplexity: 4.614549160003662
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.12s/it][A100%|██████████| 1/1 [00:23<00:00, 23.12s/it]
INFO:root:eval mean loss: 1792.1263362803359
INFO:root:eval perplexity: 5.0413312911987305
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.49s/it][A100%|██████████| 1/1 [00:22<00:00, 22.50s/it]
INFO:root:eval mean loss: 2176.2944392211048
INFO:root:eval perplexity: 7.383641719818115
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/21
 10%|█         | 21/200 [2:20:39<19:18:51, 388.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1765.446762084961
INFO:root:current train perplexity4.61224365234375
INFO:root:current mean train loss 1737.6772914788662
INFO:root:current train perplexity4.570766925811768
INFO:root:current mean train loss 1727.4019212722778
INFO:root:current train perplexity4.549020290374756
INFO:root:current mean train loss 1726.434105005157
INFO:root:current train perplexity4.535378932952881
INFO:root:current mean train loss 1729.4310096606873
INFO:root:current train perplexity4.551562309265137
INFO:root:current mean train loss 1727.8383268726816
INFO:root:current train perplexity4.552031993865967
INFO:root:current mean train loss 1729.6985447581221
INFO:root:current train perplexity4.55561637878418
INFO:root:current mean train loss 1732.3100366340113
INFO:root:current train perplexity4.556689262390137
INFO:root:current mean train loss 1732.444009085682
INFO:root:current train perplexity4.563457489013672
INFO:root:current mean train loss 1732.1575823029714
INFO:root:current train perplexity4.562145709991455
INFO:root:current mean train loss 1730.1783165209222
INFO:root:current train perplexity4.5631489753723145
INFO:root:current mean train loss 1731.204204308533
INFO:root:current train perplexity4.56795597076416
INFO:root:current mean train loss 1731.7793860951806
INFO:root:current train perplexity4.570257663726807
INFO:root:current mean train loss 1731.8218210045918
INFO:root:current train perplexity4.5689239501953125
INFO:root:current mean train loss 1732.968375656631
INFO:root:current train perplexity4.571808338165283
INFO:root:current mean train loss 1733.5588597785538
INFO:root:current train perplexity4.571359634399414
INFO:root:current mean train loss 1733.2638844660514
INFO:root:current train perplexity4.570590019226074
INFO:root:current mean train loss 1732.3869084595005
INFO:root:current train perplexity4.569181442260742
INFO:root:current mean train loss 1731.5195077698806
INFO:root:current train perplexity4.568141460418701
INFO:root:current mean train loss 1731.9389031845124
INFO:root:current train perplexity4.571062088012695

100%|██████████| 1/1 [05:33<00:00, 333.15s/it][A100%|██████████| 1/1 [05:33<00:00, 333.15s/it]
INFO:root:final mean train loss: 1731.205554186907
INFO:root:final train perplexity: 4.571080684661865
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.57s/it][A100%|██████████| 1/1 [00:23<00:00, 23.57s/it]
INFO:root:eval mean loss: 1791.0616840335495
INFO:root:eval perplexity: 5.036490440368652
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.42s/it][A100%|██████████| 1/1 [00:22<00:00, 22.42s/it]
INFO:root:eval mean loss: 2178.171459874363
INFO:root:eval perplexity: 7.3963847160339355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/22
 11%|█         | 22/200 [2:27:01<19:05:51, 386.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1689.5846315148758
INFO:root:current train perplexity4.494522571563721
INFO:root:current mean train loss 1695.9282508805998
INFO:root:current train perplexity4.475959777832031
INFO:root:current mean train loss 1710.9544838706215
INFO:root:current train perplexity4.498661518096924
INFO:root:current mean train loss 1714.0985460869429
INFO:root:current train perplexity4.515405654907227
INFO:root:current mean train loss 1715.733587349696
INFO:root:current train perplexity4.525141716003418
INFO:root:current mean train loss 1718.6374707712969
INFO:root:current train perplexity4.52799654006958
INFO:root:current mean train loss 1720.3198886094794
INFO:root:current train perplexity4.531520366668701
INFO:root:current mean train loss 1720.6644745070444
INFO:root:current train perplexity4.529771327972412
INFO:root:current mean train loss 1722.6056252181324
INFO:root:current train perplexity4.528316497802734
INFO:root:current mean train loss 1721.1489207629431
INFO:root:current train perplexity4.524163722991943
INFO:root:current mean train loss 1721.230226202091
INFO:root:current train perplexity4.5220947265625
INFO:root:current mean train loss 1721.4240009798925
INFO:root:current train perplexity4.523500919342041
INFO:root:current mean train loss 1720.804162108608
INFO:root:current train perplexity4.522700309753418
INFO:root:current mean train loss 1721.8187196291196
INFO:root:current train perplexity4.525638580322266
INFO:root:current mean train loss 1720.841580910822
INFO:root:current train perplexity4.525059223175049
INFO:root:current mean train loss 1721.1741730725773
INFO:root:current train perplexity4.527767181396484
INFO:root:current mean train loss 1721.6606357754595
INFO:root:current train perplexity4.530284404754639
INFO:root:current mean train loss 1721.7254599427613
INFO:root:current train perplexity4.53087043762207
INFO:root:current mean train loss 1722.6981606552197
INFO:root:current train perplexity4.53516960144043
INFO:root:current mean train loss 1721.8987451815328
INFO:root:current train perplexity4.532519817352295

100%|██████████| 1/1 [05:45<00:00, 345.58s/it][A100%|██████████| 1/1 [05:45<00:00, 345.58s/it]
INFO:root:final mean train loss: 1721.4710813706051
INFO:root:final train perplexity: 4.532184600830078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.96s/it][A100%|██████████| 1/1 [00:23<00:00, 23.96s/it]
INFO:root:eval mean loss: 1790.173803884087
INFO:root:eval perplexity: 5.032454967498779
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.50s/it][A100%|██████████| 1/1 [00:22<00:00, 22.50s/it]
INFO:root:eval mean loss: 2183.8908059411015
INFO:root:eval perplexity: 7.435349941253662
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/23
 12%|█▏        | 23/200 [2:33:34<19:06:16, 388.57s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1685.5278333875867
INFO:root:current train perplexity4.446300983428955
INFO:root:current mean train loss 1702.4113152754935
INFO:root:current train perplexity4.466744422912598
INFO:root:current mean train loss 1701.0050882273706
INFO:root:current train perplexity4.467371463775635
INFO:root:current mean train loss 1697.2605481270032
INFO:root:current train perplexity4.455224990844727
INFO:root:current mean train loss 1699.1510968889509
INFO:root:current train perplexity4.465985298156738
INFO:root:current mean train loss 1700.2799595719678
INFO:root:current train perplexity4.472671985626221
INFO:root:current mean train loss 1700.1772625467052
INFO:root:current train perplexity4.4720563888549805
INFO:root:current mean train loss 1700.4750876124901
INFO:root:current train perplexity4.474395751953125
INFO:root:current mean train loss 1700.3765747070313
INFO:root:current train perplexity4.4763994216918945
INFO:root:current mean train loss 1703.725950915404
INFO:root:current train perplexity4.480236530303955
INFO:root:current mean train loss 1703.9236794008027
INFO:root:current train perplexity4.47824239730835
INFO:root:current mean train loss 1705.4181045660453
INFO:root:current train perplexity4.4836530685424805
INFO:root:current mean train loss 1707.7429202057594
INFO:root:current train perplexity4.488653659820557
INFO:root:current mean train loss 1707.7447014458746
INFO:root:current train perplexity4.48744535446167
INFO:root:current mean train loss 1708.4136626173186
INFO:root:current train perplexity4.492327690124512
INFO:root:current mean train loss 1710.6631088160868
INFO:root:current train perplexity4.494070529937744
INFO:root:current mean train loss 1711.3858584071052
INFO:root:current train perplexity4.494033336639404
INFO:root:current mean train loss 1712.381537172115
INFO:root:current train perplexity4.49407958984375
INFO:root:current mean train loss 1712.347649339141
INFO:root:current train perplexity4.493710041046143

100%|██████████| 1/1 [05:35<00:00, 335.78s/it][A100%|██████████| 1/1 [05:35<00:00, 335.78s/it]
INFO:root:final mean train loss: 1711.9773502936582
INFO:root:final train perplexity: 4.494569778442383
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.32s/it][A100%|██████████| 1/1 [00:23<00:00, 23.32s/it]
INFO:root:eval mean loss: 1786.6008612450132
INFO:root:eval perplexity: 5.016250133514404
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.35s/it][A100%|██████████| 1/1 [00:22<00:00, 22.35s/it]
INFO:root:eval mean loss: 2177.322055248504
INFO:root:eval perplexity: 7.3906168937683105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/24
 12%|█▏        | 24/200 [2:39:58<18:55:15, 387.02s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1789.6508091517858
INFO:root:current train perplexity4.675016403198242
INFO:root:current mean train loss 1720.2102415851343
INFO:root:current train perplexity4.501420974731445
INFO:root:current mean train loss 1718.9409321218297
INFO:root:current train perplexity4.484379768371582
INFO:root:current mean train loss 1715.5382191412612
INFO:root:current train perplexity4.472434997558594
INFO:root:current mean train loss 1714.9249303569372
INFO:root:current train perplexity4.4766154289245605
INFO:root:current mean train loss 1710.6170984151565
INFO:root:current train perplexity4.460179805755615
INFO:root:current mean train loss 1709.9767961832013
INFO:root:current train perplexity4.463574409484863
INFO:root:current mean train loss 1710.7853720744563
INFO:root:current train perplexity4.463651180267334
INFO:root:current mean train loss 1707.5222217885978
INFO:root:current train perplexity4.460757255554199
INFO:root:current mean train loss 1706.27019543632
INFO:root:current train perplexity4.460066318511963
INFO:root:current mean train loss 1705.9600294277946
INFO:root:current train perplexity4.457180500030518
INFO:root:current mean train loss 1705.0394803178988
INFO:root:current train perplexity4.458917140960693
INFO:root:current mean train loss 1702.9322087020053
INFO:root:current train perplexity4.4541144371032715
INFO:root:current mean train loss 1703.3729167725169
INFO:root:current train perplexity4.45625638961792
INFO:root:current mean train loss 1702.80287371041
INFO:root:current train perplexity4.4551310539245605
INFO:root:current mean train loss 1703.6930672000715
INFO:root:current train perplexity4.456963062286377
INFO:root:current mean train loss 1702.5625344865725
INFO:root:current train perplexity4.455403804779053
INFO:root:current mean train loss 1702.4055836548496
INFO:root:current train perplexity4.453713893890381
INFO:root:current mean train loss 1702.1916166135602
INFO:root:current train perplexity4.452929973602295
INFO:root:current mean train loss 1703.0861159006088
INFO:root:current train perplexity4.456210136413574

100%|██████████| 1/1 [05:34<00:00, 334.33s/it][A100%|██████████| 1/1 [05:34<00:00, 334.33s/it]
INFO:root:final mean train loss: 1702.506593335835
INFO:root:final train perplexity: 4.457357406616211
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.96s/it][A100%|██████████| 1/1 [00:22<00:00, 22.97s/it]
INFO:root:eval mean loss: 1786.419701975288
INFO:root:eval perplexity: 5.015430450439453
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.13s/it][A100%|██████████| 1/1 [00:23<00:00, 23.13s/it]
INFO:root:eval mean loss: 2182.251095169825
INFO:root:eval perplexity: 7.424158096313477
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/25
 12%|█▎        | 25/200 [2:46:20<18:44:46, 385.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1710.3011983235676
INFO:root:current train perplexity4.451423168182373
INFO:root:current mean train loss 1693.1473014585433
INFO:root:current train perplexity4.400121688842773
INFO:root:current mean train loss 1683.2822865077428
INFO:root:current train perplexity4.381422519683838
INFO:root:current mean train loss 1681.6256355944975
INFO:root:current train perplexity4.382302761077881
INFO:root:current mean train loss 1685.0747792945717
INFO:root:current train perplexity4.391695976257324
INFO:root:current mean train loss 1686.455800995572
INFO:root:current train perplexity4.394476890563965
INFO:root:current mean train loss 1686.553439409305
INFO:root:current train perplexity4.3993239402771
INFO:root:current mean train loss 1688.8457553926753
INFO:root:current train perplexity4.407340049743652
INFO:root:current mean train loss 1690.8349846404733
INFO:root:current train perplexity4.410130500793457
INFO:root:current mean train loss 1692.0342305501301
INFO:root:current train perplexity4.412832260131836
INFO:root:current mean train loss 1692.315416932106
INFO:root:current train perplexity4.41375732421875
INFO:root:current mean train loss 1692.5584909025035
INFO:root:current train perplexity4.414742946624756
INFO:root:current mean train loss 1692.151086545458
INFO:root:current train perplexity4.414371967315674
INFO:root:current mean train loss 1692.9352115504332
INFO:root:current train perplexity4.415032386779785
INFO:root:current mean train loss 1692.5464576121126
INFO:root:current train perplexity4.417778968811035
INFO:root:current mean train loss 1691.8885327436792
INFO:root:current train perplexity4.4166951179504395
INFO:root:current mean train loss 1691.9262028586102
INFO:root:current train perplexity4.418817520141602
INFO:root:current mean train loss 1692.6361498202082
INFO:root:current train perplexity4.418309688568115
INFO:root:current mean train loss 1692.3077667637874
INFO:root:current train perplexity4.41740083694458
INFO:root:current mean train loss 1692.8442380909116
INFO:root:current train perplexity4.420241832733154

100%|██████████| 1/1 [05:36<00:00, 336.82s/it][A100%|██████████| 1/1 [05:36<00:00, 336.83s/it]
INFO:root:final mean train loss: 1693.6675803164792
INFO:root:final train perplexity: 4.4229044914245605
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.01s/it][A100%|██████████| 1/1 [00:23<00:00, 23.01s/it]
INFO:root:eval mean loss: 1783.829876405973
INFO:root:eval perplexity: 5.003718852996826
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.49s/it][A100%|██████████| 1/1 [00:22<00:00, 22.49s/it]
INFO:root:eval mean loss: 2180.4427308427526
INFO:root:eval perplexity: 7.411834716796875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/26
 13%|█▎        | 26/200 [2:52:45<18:37:14, 385.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1665.565224252096
INFO:root:current train perplexity4.314680576324463
INFO:root:current mean train loss 1674.0341268769394
INFO:root:current train perplexity4.349618434906006
INFO:root:current mean train loss 1678.445674658811
INFO:root:current train perplexity4.357892036437988
INFO:root:current mean train loss 1679.5082445787893
INFO:root:current train perplexity4.366781711578369
INFO:root:current mean train loss 1677.9138964179422
INFO:root:current train perplexity4.365489959716797
INFO:root:current mean train loss 1679.1746693045286
INFO:root:current train perplexity4.367481231689453
INFO:root:current mean train loss 1681.5710173084658
INFO:root:current train perplexity4.369491100311279
INFO:root:current mean train loss 1680.375256001708
INFO:root:current train perplexity4.364805698394775
INFO:root:current mean train loss 1680.7242135536655
INFO:root:current train perplexity4.367101669311523
INFO:root:current mean train loss 1680.6937574980489
INFO:root:current train perplexity4.3658576011657715
INFO:root:current mean train loss 1682.6555436104106
INFO:root:current train perplexity4.372613906860352
INFO:root:current mean train loss 1682.4776225110922
INFO:root:current train perplexity4.37502908706665
INFO:root:current mean train loss 1684.2758535282157
INFO:root:current train perplexity4.380320072174072
INFO:root:current mean train loss 1684.652314074443
INFO:root:current train perplexity4.381526470184326
INFO:root:current mean train loss 1685.0350518845419
INFO:root:current train perplexity4.38411808013916
INFO:root:current mean train loss 1685.2476801095575
INFO:root:current train perplexity4.385406970977783
INFO:root:current mean train loss 1685.824692525637
INFO:root:current train perplexity4.387572765350342
INFO:root:current mean train loss 1686.3731538705754
INFO:root:current train perplexity4.388145923614502
INFO:root:current mean train loss 1685.2792273194552
INFO:root:current train perplexity4.387646198272705
INFO:root:current mean train loss 1685.713164764357
INFO:root:current train perplexity4.390157699584961

100%|██████████| 1/1 [05:41<00:00, 341.30s/it][A100%|██████████| 1/1 [05:41<00:00, 341.30s/it]
INFO:root:final mean train loss: 1685.285087612381
INFO:root:final train perplexity: 4.390477180480957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.01s/it][A100%|██████████| 1/1 [00:23<00:00, 23.01s/it]
INFO:root:eval mean loss: 1784.9525124840702
INFO:root:eval perplexity: 5.008792877197266
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.63s/it][A100%|██████████| 1/1 [00:23<00:00, 23.63s/it]
INFO:root:eval mean loss: 2186.233465100011
INFO:root:eval perplexity: 7.451369285583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/27
 14%|█▎        | 27/200 [2:59:15<18:34:48, 386.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.1343909954203
INFO:root:current train perplexity4.281033039093018
INFO:root:current mean train loss 1661.4096463360363
INFO:root:current train perplexity4.296938896179199
INFO:root:current mean train loss 1663.5540255760961
INFO:root:current train perplexity4.301420211791992
INFO:root:current mean train loss 1666.855069123167
INFO:root:current train perplexity4.319049835205078
INFO:root:current mean train loss 1669.0232482643628
INFO:root:current train perplexity4.319413185119629
INFO:root:current mean train loss 1665.3929005831374
INFO:root:current train perplexity4.3259758949279785
INFO:root:current mean train loss 1665.0704310454857
INFO:root:current train perplexity4.328398704528809
INFO:root:current mean train loss 1669.028744015656
INFO:root:current train perplexity4.332437992095947
INFO:root:current mean train loss 1669.7787348962886
INFO:root:current train perplexity4.337214946746826
INFO:root:current mean train loss 1670.5372036673082
INFO:root:current train perplexity4.343584060668945
INFO:root:current mean train loss 1671.8099257932492
INFO:root:current train perplexity4.343098163604736
INFO:root:current mean train loss 1673.7160928097003
INFO:root:current train perplexity4.344239234924316
INFO:root:current mean train loss 1674.33295599569
INFO:root:current train perplexity4.345168590545654
INFO:root:current mean train loss 1675.0411475831877
INFO:root:current train perplexity4.346130847930908
INFO:root:current mean train loss 1675.6965010529193
INFO:root:current train perplexity4.347729206085205
INFO:root:current mean train loss 1676.2335776254363
INFO:root:current train perplexity4.3506340980529785
INFO:root:current mean train loss 1676.258929833572
INFO:root:current train perplexity4.351466178894043
INFO:root:current mean train loss 1676.9037915816757
INFO:root:current train perplexity4.355162143707275
INFO:root:current mean train loss 1677.334645512543
INFO:root:current train perplexity4.355000019073486
INFO:root:current mean train loss 1677.0186873563584
INFO:root:current train perplexity4.356485366821289

100%|██████████| 1/1 [05:43<00:00, 343.79s/it][A100%|██████████| 1/1 [05:43<00:00, 343.80s/it]
INFO:root:final mean train loss: 1676.4189817858535
INFO:root:final train perplexity: 4.356438636779785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.27s/it][A100%|██████████| 1/1 [00:24<00:00, 24.27s/it]
INFO:root:eval mean loss: 1782.391500270113
INFO:root:eval perplexity: 4.997226715087891
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.63s/it][A100%|██████████| 1/1 [00:21<00:00, 21.63s/it]
INFO:root:eval mean loss: 2184.332654587766
INFO:root:eval perplexity: 7.438369274139404
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/28
 14%|█▍        | 28/200 [3:05:46<18:32:44, 388.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1681.61779296875
INFO:root:current train perplexity4.313944339752197
INFO:root:current mean train loss 1654.715295061384
INFO:root:current train perplexity4.268819332122803
INFO:root:current mean train loss 1658.5272713955967
INFO:root:current train perplexity4.2895684242248535
INFO:root:current mean train loss 1659.9098076171874
INFO:root:current train perplexity4.2908453941345215
INFO:root:current mean train loss 1660.1012777549342
INFO:root:current train perplexity4.2970356941223145
INFO:root:current mean train loss 1662.245173445992
INFO:root:current train perplexity4.2969584465026855
INFO:root:current mean train loss 1664.4245442708334
INFO:root:current train perplexity4.305831432342529
INFO:root:current mean train loss 1666.3045114037297
INFO:root:current train perplexity4.313380241394043
INFO:root:current mean train loss 1668.4701476004464
INFO:root:current train perplexity4.313692569732666
INFO:root:current mean train loss 1669.0507853816105
INFO:root:current train perplexity4.316661357879639
INFO:root:current mean train loss 1666.0703732512718
INFO:root:current train perplexity4.313148498535156
INFO:root:current mean train loss 1665.714354637633
INFO:root:current train perplexity4.314117908477783
INFO:root:current mean train loss 1665.742690333946
INFO:root:current train perplexity4.317646503448486
INFO:root:current mean train loss 1667.027331587358
INFO:root:current train perplexity4.319066047668457
INFO:root:current mean train loss 1667.7120310017215
INFO:root:current train perplexity4.321195125579834
INFO:root:current mean train loss 1667.5694855995785
INFO:root:current train perplexity4.320204257965088
INFO:root:current mean train loss 1668.3480136427238
INFO:root:current train perplexity4.32263708114624
INFO:root:current mean train loss 1668.3493130364216
INFO:root:current train perplexity4.322428226470947
INFO:root:current mean train loss 1667.6834378255207
INFO:root:current train perplexity4.322859287261963
INFO:root:current mean train loss 1668.5225779395766
INFO:root:current train perplexity4.325065612792969

100%|██████████| 1/1 [05:37<00:00, 337.73s/it][A100%|██████████| 1/1 [05:37<00:00, 337.73s/it]
INFO:root:final mean train loss: 1668.254565294259
INFO:root:final train perplexity: 4.325326919555664
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.51s/it][A100%|██████████| 1/1 [00:23<00:00, 23.51s/it]
INFO:root:eval mean loss: 1781.812571424119
INFO:root:eval perplexity: 4.994616508483887
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.33s/it][A100%|██████████| 1/1 [00:21<00:00, 21.33s/it]
INFO:root:eval mean loss: 2185.5464408279313
INFO:root:eval perplexity: 7.446666717529297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/29
 14%|█▍        | 29/200 [3:12:11<18:23:08, 387.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1656.2224797787874
INFO:root:current train perplexity4.276484966278076
INFO:root:current mean train loss 1651.864559809367
INFO:root:current train perplexity4.268028259277344
INFO:root:current mean train loss 1649.1925002842734
INFO:root:current train perplexity4.265838623046875
INFO:root:current mean train loss 1650.0422895781849
INFO:root:current train perplexity4.272372722625732
INFO:root:current mean train loss 1652.5548703573584
INFO:root:current train perplexity4.280828952789307
INFO:root:current mean train loss 1652.5416975279113
INFO:root:current train perplexity4.282290935516357
INFO:root:current mean train loss 1651.6874499017792
INFO:root:current train perplexity4.286189556121826
INFO:root:current mean train loss 1651.6379231154317
INFO:root:current train perplexity4.288488388061523
INFO:root:current mean train loss 1653.5324961572485
INFO:root:current train perplexity4.291186809539795
INFO:root:current mean train loss 1653.7517081229917
INFO:root:current train perplexity4.289844989776611
INFO:root:current mean train loss 1656.2496916942107
INFO:root:current train perplexity4.294229030609131
INFO:root:current mean train loss 1656.5495336135762
INFO:root:current train perplexity4.294355869293213
INFO:root:current mean train loss 1656.6058788949122
INFO:root:current train perplexity4.292559623718262
INFO:root:current mean train loss 1656.908651242311
INFO:root:current train perplexity4.293612003326416
INFO:root:current mean train loss 1657.4902292205566
INFO:root:current train perplexity4.2936110496521
INFO:root:current mean train loss 1658.0483135434251
INFO:root:current train perplexity4.293995380401611
INFO:root:current mean train loss 1658.764664381672
INFO:root:current train perplexity4.292104721069336
INFO:root:current mean train loss 1659.1082691465106
INFO:root:current train perplexity4.2898478507995605
INFO:root:current mean train loss 1660.0079564423188
INFO:root:current train perplexity4.292183876037598

100%|██████████| 1/1 [05:33<00:00, 333.47s/it][A100%|██████████| 1/1 [05:33<00:00, 333.47s/it]
INFO:root:final mean train loss: 1659.6618252521926
INFO:root:final train perplexity: 4.292823314666748
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.67s/it][A100%|██████████| 1/1 [00:23<00:00, 23.67s/it]
INFO:root:eval mean loss: 1782.1918776491855
INFO:root:eval perplexity: 4.996326446533203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.76s/it][A100%|██████████| 1/1 [00:22<00:00, 22.77s/it]
INFO:root:eval mean loss: 2188.523708911652
INFO:root:eval perplexity: 7.467063903808594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/30
 15%|█▌        | 30/200 [3:18:33<18:12:12, 385.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1659.7654351128472
INFO:root:current train perplexity4.1667704582214355
INFO:root:current mean train loss 1639.0983774727638
INFO:root:current train perplexity4.203323841094971
INFO:root:current mean train loss 1643.273894241552
INFO:root:current train perplexity4.22145938873291
INFO:root:current mean train loss 1643.7202985942556
INFO:root:current train perplexity4.221160411834717
INFO:root:current mean train loss 1642.2721881446746
INFO:root:current train perplexity4.222324371337891
INFO:root:current mean train loss 1641.4210586397962
INFO:root:current train perplexity4.226128101348877
INFO:root:current mean train loss 1645.8901984554598
INFO:root:current train perplexity4.24053430557251
INFO:root:current mean train loss 1646.102470365667
INFO:root:current train perplexity4.239898204803467
INFO:root:current mean train loss 1646.0752689470025
INFO:root:current train perplexity4.244670391082764
INFO:root:current mean train loss 1647.033662399443
INFO:root:current train perplexity4.246042251586914
INFO:root:current mean train loss 1645.1536908787707
INFO:root:current train perplexity4.2435994148254395
INFO:root:current mean train loss 1644.321681690818
INFO:root:current train perplexity4.242499351501465
INFO:root:current mean train loss 1645.943353619824
INFO:root:current train perplexity4.246469020843506
INFO:root:current mean train loss 1647.2651514529819
INFO:root:current train perplexity4.2496137619018555
INFO:root:current mean train loss 1647.7558034080575
INFO:root:current train perplexity4.252713680267334
INFO:root:current mean train loss 1649.3325298048946
INFO:root:current train perplexity4.257387638092041
INFO:root:current mean train loss 1649.1362045221701
INFO:root:current train perplexity4.258622646331787
INFO:root:current mean train loss 1651.0544839304325
INFO:root:current train perplexity4.263495922088623
INFO:root:current mean train loss 1651.9468538654385
INFO:root:current train perplexity4.26373291015625
INFO:root:current mean train loss 1652.6709339907184
INFO:root:current train perplexity4.263782024383545

100%|██████████| 1/1 [05:36<00:00, 336.03s/it][A100%|██████████| 1/1 [05:36<00:00, 336.03s/it]
INFO:root:final mean train loss: 1651.7449341435897
INFO:root:final train perplexity: 4.263091564178467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.73s/it][A100%|██████████| 1/1 [00:23<00:00, 23.73s/it]
INFO:root:eval mean loss: 1782.9780901104
INFO:root:eval perplexity: 4.999873638153076
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.15s/it][A100%|██████████| 1/1 [00:23<00:00, 23.15s/it]
INFO:root:eval mean loss: 2192.334715498255
INFO:root:eval perplexity: 7.493251323699951
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/31
 16%|█▌        | 31/200 [3:25:05<18:12:00, 387.70s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1618.806884765625
INFO:root:current train perplexity4.1937575340271
INFO:root:current mean train loss 1615.7315664140006
INFO:root:current train perplexity4.158965110778809
INFO:root:current mean train loss 1629.5638924657771
INFO:root:current train perplexity4.181379795074463
INFO:root:current mean train loss 1632.8302432569258
INFO:root:current train perplexity4.2064595222473145
INFO:root:current mean train loss 1636.3904275670297
INFO:root:current train perplexity4.220066070556641
INFO:root:current mean train loss 1639.3029304765475
INFO:root:current train perplexity4.227611064910889
INFO:root:current mean train loss 1636.4019221589208
INFO:root:current train perplexity4.228283882141113
INFO:root:current mean train loss 1641.6298698656487
INFO:root:current train perplexity4.23619270324707
INFO:root:current mean train loss 1639.4952271394523
INFO:root:current train perplexity4.234375953674316
INFO:root:current mean train loss 1640.9976637904124
INFO:root:current train perplexity4.238586902618408
INFO:root:current mean train loss 1641.574658012762
INFO:root:current train perplexity4.235274314880371
INFO:root:current mean train loss 1642.1916825885671
INFO:root:current train perplexity4.236306667327881
INFO:root:current mean train loss 1643.0973342472344
INFO:root:current train perplexity4.23676872253418
INFO:root:current mean train loss 1643.5151774088542
INFO:root:current train perplexity4.237406253814697
INFO:root:current mean train loss 1643.0685726984352
INFO:root:current train perplexity4.235954284667969
INFO:root:current mean train loss 1642.9675760931611
INFO:root:current train perplexity4.23623514175415
INFO:root:current mean train loss 1644.0496470321148
INFO:root:current train perplexity4.237002849578857
INFO:root:current mean train loss 1644.4982492882295
INFO:root:current train perplexity4.2363972663879395
INFO:root:current mean train loss 1644.794565624893
INFO:root:current train perplexity4.23744010925293
INFO:root:current mean train loss 1645.040452238184
INFO:root:current train perplexity4.236384868621826

100%|██████████| 1/1 [05:37<00:00, 337.20s/it][A100%|██████████| 1/1 [05:37<00:00, 337.20s/it]
INFO:root:final mean train loss: 1644.684795493137
INFO:root:final train perplexity: 4.236751556396484
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.72s/it][A100%|██████████| 1/1 [00:23<00:00, 23.72s/it]
INFO:root:eval mean loss: 1781.41520225579
INFO:root:eval perplexity: 4.992825508117676
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.54s/it][A100%|██████████| 1/1 [00:22<00:00, 22.54s/it]
INFO:root:eval mean loss: 2192.4401340349345
INFO:root:eval perplexity: 7.493976593017578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/32
 16%|█▌        | 32/200 [3:31:31<18:03:38, 387.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1600.0572225881178
INFO:root:current train perplexity4.140085697174072
INFO:root:current mean train loss 1619.868364667559
INFO:root:current train perplexity4.172286033630371
INFO:root:current mean train loss 1616.0818504050926
INFO:root:current train perplexity4.168283462524414
INFO:root:current mean train loss 1623.905764565871
INFO:root:current train perplexity4.172791004180908
INFO:root:current mean train loss 1627.2430717519928
INFO:root:current train perplexity4.1745781898498535
INFO:root:current mean train loss 1627.6116484752677
INFO:root:current train perplexity4.171849727630615
INFO:root:current mean train loss 1630.51114902526
INFO:root:current train perplexity4.178877830505371
INFO:root:current mean train loss 1630.9277199171433
INFO:root:current train perplexity4.181684970855713
INFO:root:current mean train loss 1630.6960204498907
INFO:root:current train perplexity4.181314468383789
INFO:root:current mean train loss 1632.3692171292998
INFO:root:current train perplexity4.1843671798706055
INFO:root:current mean train loss 1634.5084599525107
INFO:root:current train perplexity4.191368579864502
INFO:root:current mean train loss 1632.448074300771
INFO:root:current train perplexity4.189502716064453
INFO:root:current mean train loss 1631.9079558417764
INFO:root:current train perplexity4.187485694885254
INFO:root:current mean train loss 1633.651575697773
INFO:root:current train perplexity4.188516139984131
INFO:root:current mean train loss 1633.8849455581633
INFO:root:current train perplexity4.1910786628723145
INFO:root:current mean train loss 1634.6569078980679
INFO:root:current train perplexity4.19120454788208
INFO:root:current mean train loss 1635.509556181171
INFO:root:current train perplexity4.1951704025268555
INFO:root:current mean train loss 1635.3362384386878
INFO:root:current train perplexity4.19818115234375
INFO:root:current mean train loss 1637.072657468716
INFO:root:current train perplexity4.2030792236328125
INFO:root:current mean train loss 1637.289407475855
INFO:root:current train perplexity4.204832077026367

100%|██████████| 1/1 [05:31<00:00, 331.04s/it][A100%|██████████| 1/1 [05:31<00:00, 331.04s/it]
INFO:root:final mean train loss: 1636.6119575904465
INFO:root:final train perplexity: 4.206831932067871
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.38s/it][A100%|██████████| 1/1 [00:23<00:00, 23.38s/it]
INFO:root:eval mean loss: 1779.788691094581
INFO:root:eval perplexity: 4.985499858856201
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.40s/it][A100%|██████████| 1/1 [00:23<00:00, 23.40s/it]
INFO:root:eval mean loss: 2190.4125777440713
INFO:root:eval perplexity: 7.480030059814453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/33
 16%|█▋        | 33/200 [3:37:51<17:51:13, 384.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1606.098368326823
INFO:root:current train perplexity4.124039173126221
INFO:root:current mean train loss 1608.2230087280273
INFO:root:current train perplexity4.105710983276367
INFO:root:current mean train loss 1610.770668381911
INFO:root:current train perplexity4.112456321716309
INFO:root:current mean train loss 1620.3065741644966
INFO:root:current train perplexity4.136790752410889
INFO:root:current mean train loss 1628.5130049995755
INFO:root:current train perplexity4.159271717071533
INFO:root:current mean train loss 1626.3619807652065
INFO:root:current train perplexity4.150860786437988
INFO:root:current mean train loss 1628.1794389204545
INFO:root:current train perplexity4.154232501983643
INFO:root:current mean train loss 1632.6549064234684
INFO:root:current train perplexity4.168472766876221
INFO:root:current mean train loss 1633.0689216081487
INFO:root:current train perplexity4.172973155975342
INFO:root:current mean train loss 1633.8624375661213
INFO:root:current train perplexity4.1775593757629395
INFO:root:current mean train loss 1634.060268301334
INFO:root:current train perplexity4.1799845695495605
INFO:root:current mean train loss 1633.328958340349
INFO:root:current train perplexity4.1775102615356445
INFO:root:current mean train loss 1632.817012435671
INFO:root:current train perplexity4.180086612701416
INFO:root:current mean train loss 1632.7416365679574
INFO:root:current train perplexity4.1794233322143555
INFO:root:current mean train loss 1631.4486068934611
INFO:root:current train perplexity4.178590774536133
INFO:root:current mean train loss 1632.0857431265024
INFO:root:current train perplexity4.181951522827148
INFO:root:current mean train loss 1631.1280683034875
INFO:root:current train perplexity4.181704044342041
INFO:root:current mean train loss 1630.7205356251109
INFO:root:current train perplexity4.182841777801514
INFO:root:current mean train loss 1630.6062287361392
INFO:root:current train perplexity4.181805610656738
INFO:root:current mean train loss 1629.8530902473294
INFO:root:current train perplexity4.180561065673828

100%|██████████| 1/1 [05:43<00:00, 343.42s/it][A100%|██████████| 1/1 [05:43<00:00, 343.42s/it]
INFO:root:final mean train loss: 1629.218205331253
INFO:root:final train perplexity: 4.179615497589111
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.22s/it][A100%|██████████| 1/1 [00:24<00:00, 24.22s/it]
INFO:root:eval mean loss: 1781.9682521955342
INFO:root:eval perplexity: 4.995317459106445
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.84s/it][A100%|██████████| 1/1 [00:22<00:00, 22.84s/it]
INFO:root:eval mean loss: 2197.4593272107713
INFO:root:eval perplexity: 7.528611183166504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/34
 17%|█▋        | 34/200 [3:44:23<17:51:05, 387.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1597.5765095500203
INFO:root:current train perplexity4.091561794281006
INFO:root:current mean train loss 1616.6477492165432
INFO:root:current train perplexity4.125548839569092
INFO:root:current mean train loss 1615.9790426867103
INFO:root:current train perplexity4.130029678344727
INFO:root:current mean train loss 1613.6134454135238
INFO:root:current train perplexity4.130796909332275
INFO:root:current mean train loss 1613.8884041904153
INFO:root:current train perplexity4.127773761749268
INFO:root:current mean train loss 1614.2247629678022
INFO:root:current train perplexity4.123612880706787
INFO:root:current mean train loss 1616.5105195038427
INFO:root:current train perplexity4.123420715332031
INFO:root:current mean train loss 1618.461790421141
INFO:root:current train perplexity4.131199359893799
INFO:root:current mean train loss 1618.0404036031482
INFO:root:current train perplexity4.1349639892578125
INFO:root:current mean train loss 1619.5399907091623
INFO:root:current train perplexity4.137800693511963
INFO:root:current mean train loss 1619.5984216546615
INFO:root:current train perplexity4.140593528747559
INFO:root:current mean train loss 1618.7929932262903
INFO:root:current train perplexity4.14276123046875
INFO:root:current mean train loss 1620.76641611504
INFO:root:current train perplexity4.145295143127441
INFO:root:current mean train loss 1621.4054653635906
INFO:root:current train perplexity4.148504257202148
INFO:root:current mean train loss 1620.7676216802165
INFO:root:current train perplexity4.149296283721924
INFO:root:current mean train loss 1621.0767388788295
INFO:root:current train perplexity4.1499810218811035
INFO:root:current mean train loss 1623.1676219014375
INFO:root:current train perplexity4.155882835388184
INFO:root:current mean train loss 1622.6958274347567
INFO:root:current train perplexity4.154254913330078
INFO:root:current mean train loss 1622.8887327475693
INFO:root:current train perplexity4.1548991203308105
INFO:root:current mean train loss 1622.9078096226724
INFO:root:current train perplexity4.154534816741943

100%|██████████| 1/1 [05:34<00:00, 334.73s/it][A100%|██████████| 1/1 [05:34<00:00, 334.73s/it]
INFO:root:final mean train loss: 1622.353544064982
INFO:root:final train perplexity: 4.154504299163818
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.14s/it][A100%|██████████| 1/1 [00:24<00:00, 24.14s/it]
INFO:root:eval mean loss: 1782.2336564300754
INFO:root:eval perplexity: 4.996514320373535
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2201.4879920905364
INFO:root:eval perplexity: 7.556524276733398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/35
 18%|█▊        | 35/200 [3:50:46<17:41:11, 385.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1606.712849069149
INFO:root:current train perplexity4.1271443367004395
INFO:root:current mean train loss 1602.7620138581267
INFO:root:current train perplexity4.103801250457764
INFO:root:current mean train loss 1611.4255985597365
INFO:root:current train perplexity4.102817058563232
INFO:root:current mean train loss 1610.0879197483741
INFO:root:current train perplexity4.1066179275512695
INFO:root:current mean train loss 1613.7480975317087
INFO:root:current train perplexity4.115445137023926
INFO:root:current mean train loss 1614.1519310948022
INFO:root:current train perplexity4.118005275726318
INFO:root:current mean train loss 1615.7077017572497
INFO:root:current train perplexity4.123388290405273
INFO:root:current mean train loss 1616.2490700210071
INFO:root:current train perplexity4.121825695037842
INFO:root:current mean train loss 1615.5817749569612
INFO:root:current train perplexity4.122237205505371
INFO:root:current mean train loss 1615.0791490888691
INFO:root:current train perplexity4.121372222900391
INFO:root:current mean train loss 1615.3474215938143
INFO:root:current train perplexity4.123457431793213
INFO:root:current mean train loss 1615.8542564302634
INFO:root:current train perplexity4.125146865844727
INFO:root:current mean train loss 1615.8040658281611
INFO:root:current train perplexity4.128067493438721
INFO:root:current mean train loss 1614.7082640375606
INFO:root:current train perplexity4.125361442565918
INFO:root:current mean train loss 1615.0168431702068
INFO:root:current train perplexity4.125404357910156
INFO:root:current mean train loss 1614.3056233213417
INFO:root:current train perplexity4.122027397155762
INFO:root:current mean train loss 1614.6756641518548
INFO:root:current train perplexity4.123758792877197
INFO:root:current mean train loss 1614.1077242609915
INFO:root:current train perplexity4.1234846115112305
INFO:root:current mean train loss 1613.9554775926858
INFO:root:current train perplexity4.124074935913086

100%|██████████| 1/1 [05:39<00:00, 339.19s/it][A100%|██████████| 1/1 [05:39<00:00, 339.19s/it]
INFO:root:final mean train loss: 1614.1912794396906
INFO:root:final train perplexity: 4.124842643737793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.68s/it][A100%|██████████| 1/1 [00:23<00:00, 23.68s/it]
INFO:root:eval mean loss: 1781.5192104907746
INFO:root:eval perplexity: 4.993293762207031
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.61s/it][A100%|██████████| 1/1 [00:23<00:00, 23.61s/it]
INFO:root:eval mean loss: 2202.428592330175
INFO:root:eval perplexity: 7.563056468963623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/36
 18%|█▊        | 36/200 [3:57:15<17:36:51, 386.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1564.9720458984375
INFO:root:current train perplexity4.014250755310059
INFO:root:current mean train loss 1601.6733354448197
INFO:root:current train perplexity4.056712627410889
INFO:root:current mean train loss 1597.8677203282361
INFO:root:current train perplexity4.054858207702637
INFO:root:current mean train loss 1598.1272561419814
INFO:root:current train perplexity4.064220428466797
INFO:root:current mean train loss 1601.85250995571
INFO:root:current train perplexity4.082526206970215
INFO:root:current mean train loss 1601.3616307924872
INFO:root:current train perplexity4.075084209442139
INFO:root:current mean train loss 1601.985374338303
INFO:root:current train perplexity4.078793525695801
INFO:root:current mean train loss 1602.3686739764637
INFO:root:current train perplexity4.083925247192383
INFO:root:current mean train loss 1606.083990094694
INFO:root:current train perplexity4.08907413482666
INFO:root:current mean train loss 1607.887866559327
INFO:root:current train perplexity4.090207099914551
INFO:root:current mean train loss 1607.2403248108694
INFO:root:current train perplexity4.090145111083984
INFO:root:current mean train loss 1607.3964149344622
INFO:root:current train perplexity4.090198040008545
INFO:root:current mean train loss 1608.9493294297713
INFO:root:current train perplexity4.095745086669922
INFO:root:current mean train loss 1610.5468144769618
INFO:root:current train perplexity4.101328372955322
INFO:root:current mean train loss 1609.994520245504
INFO:root:current train perplexity4.104001522064209
INFO:root:current mean train loss 1609.9127568889344
INFO:root:current train perplexity4.1041483879089355
INFO:root:current mean train loss 1609.5286628822596
INFO:root:current train perplexity4.10302734375
INFO:root:current mean train loss 1608.1877789567047
INFO:root:current train perplexity4.101483345031738
INFO:root:current mean train loss 1608.8752185941598
INFO:root:current train perplexity4.103155136108398
INFO:root:current mean train loss 1609.3842336513933
INFO:root:current train perplexity4.105435371398926

100%|██████████| 1/1 [05:33<00:00, 333.80s/it][A100%|██████████| 1/1 [05:33<00:00, 333.80s/it]
INFO:root:final mean train loss: 1608.2214147709142
INFO:root:final train perplexity: 4.1032819747924805
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.95s/it][A100%|██████████| 1/1 [00:23<00:00, 23.95s/it]
INFO:root:eval mean loss: 1785.0470182811114
INFO:root:eval perplexity: 5.009219169616699
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.45s/it][A100%|██████████| 1/1 [00:22<00:00, 22.45s/it]
INFO:root:eval mean loss: 2207.6000790426915
INFO:root:eval perplexity: 7.599071979522705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/37
 18%|█▊        | 37/200 [4:03:37<17:26:41, 385.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1555.8164803641182
INFO:root:current train perplexity3.956807851791382
INFO:root:current mean train loss 1575.8336067199707
INFO:root:current train perplexity4.004183769226074
INFO:root:current mean train loss 1591.776192648369
INFO:root:current train perplexity4.03662633895874
INFO:root:current mean train loss 1593.0840040997762
INFO:root:current train perplexity4.039543151855469
INFO:root:current mean train loss 1593.0241756260953
INFO:root:current train perplexity4.051255702972412
INFO:root:current mean train loss 1592.8214090520685
INFO:root:current train perplexity4.0501909255981445
INFO:root:current mean train loss 1592.8460712797323
INFO:root:current train perplexity4.053574085235596
INFO:root:current mean train loss 1595.500519804902
INFO:root:current train perplexity4.05742073059082
INFO:root:current mean train loss 1596.6747202113054
INFO:root:current train perplexity4.059800148010254
INFO:root:current mean train loss 1596.3281290777797
INFO:root:current train perplexity4.060426235198975
INFO:root:current mean train loss 1596.35495078239
INFO:root:current train perplexity4.060544490814209
INFO:root:current mean train loss 1599.2206165503103
INFO:root:current train perplexity4.066539287567139
INFO:root:current mean train loss 1600.083224318315
INFO:root:current train perplexity4.069771766662598
INFO:root:current mean train loss 1600.9302887514414
INFO:root:current train perplexity4.07267427444458
INFO:root:current mean train loss 1600.9271247927882
INFO:root:current train perplexity4.07313346862793
INFO:root:current mean train loss 1599.3678470631544
INFO:root:current train perplexity4.072384357452393
INFO:root:current mean train loss 1600.5197756155703
INFO:root:current train perplexity4.076881408691406
INFO:root:current mean train loss 1600.7260747132477
INFO:root:current train perplexity4.077305793762207
INFO:root:current mean train loss 1600.9755844016045
INFO:root:current train perplexity4.077178478240967
INFO:root:current mean train loss 1601.8453440685985
INFO:root:current train perplexity4.078703880310059

100%|██████████| 1/1 [05:31<00:00, 331.69s/it][A100%|██████████| 1/1 [05:31<00:00, 331.69s/it]
INFO:root:final mean train loss: 1601.2206209137532
INFO:root:final train perplexity: 4.078141689300537
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.54s/it][A100%|██████████| 1/1 [00:22<00:00, 22.54s/it]
INFO:root:eval mean loss: 1783.7030501128934
INFO:root:eval perplexity: 5.003146648406982
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.17s/it][A100%|██████████| 1/1 [00:22<00:00, 22.17s/it]
INFO:root:eval mean loss: 2207.9382021207334
INFO:root:eval perplexity: 7.601432800292969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/38
 19%|█▉        | 38/200 [4:09:55<17:14:38, 383.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1573.8999457465277
INFO:root:current train perplexity4.00835657119751
INFO:root:current mean train loss 1573.9410164668643
INFO:root:current train perplexity3.9954922199249268
INFO:root:current mean train loss 1590.4884546396684
INFO:root:current train perplexity4.026474952697754
INFO:root:current mean train loss 1589.882522361866
INFO:root:current train perplexity4.033196926116943
INFO:root:current mean train loss 1587.8277933527913
INFO:root:current train perplexity4.037303447723389
INFO:root:current mean train loss 1587.5039349197248
INFO:root:current train perplexity4.034943580627441
INFO:root:current mean train loss 1588.3611163472021
INFO:root:current train perplexity4.040390968322754
INFO:root:current mean train loss 1588.491520946938
INFO:root:current train perplexity4.043623447418213
INFO:root:current mean train loss 1588.1105714335245
INFO:root:current train perplexity4.043149948120117
INFO:root:current mean train loss 1589.3944014291915
INFO:root:current train perplexity4.041346073150635
INFO:root:current mean train loss 1591.1400306519138
INFO:root:current train perplexity4.040325164794922
INFO:root:current mean train loss 1592.0834799953943
INFO:root:current train perplexity4.044025897979736
INFO:root:current mean train loss 1591.6437416658823
INFO:root:current train perplexity4.047788143157959
INFO:root:current mean train loss 1591.8699747872618
INFO:root:current train perplexity4.047275543212891
INFO:root:current mean train loss 1592.830553227725
INFO:root:current train perplexity4.049555778503418
INFO:root:current mean train loss 1592.9146195988824
INFO:root:current train perplexity4.048405647277832
INFO:root:current mean train loss 1594.3677829359801
INFO:root:current train perplexity4.050817966461182
INFO:root:current mean train loss 1594.6367203589498
INFO:root:current train perplexity4.052633762359619
INFO:root:current mean train loss 1594.7093635538406
INFO:root:current train perplexity4.054275989532471
INFO:root:current mean train loss 1594.5993855062059
INFO:root:current train perplexity4.054236888885498

100%|██████████| 1/1 [05:38<00:00, 338.65s/it][A100%|██████████| 1/1 [05:38<00:00, 338.65s/it]
INFO:root:final mean train loss: 1594.6822830177111
INFO:root:final train perplexity: 4.054801940917969
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.79s/it][A100%|██████████| 1/1 [00:23<00:00, 23.79s/it]
INFO:root:eval mean loss: 1784.7645826407359
INFO:root:eval perplexity: 5.0079426765441895
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:21<00:00, 21.49s/it][A100%|██████████| 1/1 [00:21<00:00, 21.49s/it]
INFO:root:eval mean loss: 2209.824397526734
INFO:root:eval perplexity: 7.614616394042969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/39
 20%|█▉        | 39/200 [4:16:21<17:10:25, 384.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1579.9445151052166
INFO:root:current train perplexity3.9983303546905518
INFO:root:current mean train loss 1572.8597103165991
INFO:root:current train perplexity3.998694658279419
INFO:root:current mean train loss 1586.7470861536856
INFO:root:current train perplexity4.022906303405762
INFO:root:current mean train loss 1587.3040666949025
INFO:root:current train perplexity4.029361724853516
INFO:root:current mean train loss 1583.8149992707488
INFO:root:current train perplexity4.0215864181518555
INFO:root:current mean train loss 1585.0436253666453
INFO:root:current train perplexity4.017188549041748
INFO:root:current mean train loss 1585.7734074434125
INFO:root:current train perplexity4.020533561706543
INFO:root:current mean train loss 1587.4037100724347
INFO:root:current train perplexity4.025581359863281
INFO:root:current mean train loss 1588.321943314059
INFO:root:current train perplexity4.026557922363281
INFO:root:current mean train loss 1588.6252454095472
INFO:root:current train perplexity4.026749134063721
INFO:root:current mean train loss 1589.4128440957509
INFO:root:current train perplexity4.028254985809326
INFO:root:current mean train loss 1589.9496098582388
INFO:root:current train perplexity4.027215480804443
INFO:root:current mean train loss 1588.469589692855
INFO:root:current train perplexity4.0245256423950195
INFO:root:current mean train loss 1588.3253144251617
INFO:root:current train perplexity4.0251641273498535
INFO:root:current mean train loss 1588.317312425856
INFO:root:current train perplexity4.025668621063232
INFO:root:current mean train loss 1587.9222716112906
INFO:root:current train perplexity4.027329921722412
INFO:root:current mean train loss 1588.402870958606
INFO:root:current train perplexity4.027039527893066
INFO:root:current mean train loss 1588.364398241855
INFO:root:current train perplexity4.028326988220215
INFO:root:current mean train loss 1588.9503397383316
INFO:root:current train perplexity4.030117988586426
INFO:root:current mean train loss 1588.8838586214242
INFO:root:current train perplexity4.031928062438965

100%|██████████| 1/1 [05:35<00:00, 335.11s/it][A100%|██████████| 1/1 [05:35<00:00, 335.11s/it]
INFO:root:final mean train loss: 1588.3664335019048
INFO:root:final train perplexity: 4.032382011413574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.47s/it][A100%|██████████| 1/1 [00:23<00:00, 23.47s/it]
INFO:root:eval mean loss: 1783.7345161167443
INFO:root:eval perplexity: 5.003288269042969
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.32s/it][A100%|██████████| 1/1 [00:23<00:00, 23.32s/it]
INFO:root:eval mean loss: 2210.6788343064327
INFO:root:eval perplexity: 7.620596885681152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/40
 20%|██        | 40/200 [4:22:45<17:03:52, 383.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1572.6343329707279
INFO:root:current train perplexity3.930349588394165
INFO:root:current mean train loss 1557.5307794496334
INFO:root:current train perplexity3.9250574111938477
INFO:root:current mean train loss 1558.1582814425124
INFO:root:current train perplexity3.9387927055358887
INFO:root:current mean train loss 1563.0017360395366
INFO:root:current train perplexity3.9504268169403076
INFO:root:current mean train loss 1568.6462139854352
INFO:root:current train perplexity3.9665446281433105
INFO:root:current mean train loss 1570.6378456339728
INFO:root:current train perplexity3.9709279537200928
INFO:root:current mean train loss 1574.7724312738746
INFO:root:current train perplexity3.9818665981292725
INFO:root:current mean train loss 1577.2292348839658
INFO:root:current train perplexity3.9910707473754883
INFO:root:current mean train loss 1578.3739614996623
INFO:root:current train perplexity3.9954731464385986
INFO:root:current mean train loss 1578.9374342890146
INFO:root:current train perplexity3.99318790435791
INFO:root:current mean train loss 1579.851338383884
INFO:root:current train perplexity3.9937479496002197
INFO:root:current mean train loss 1580.06664583499
INFO:root:current train perplexity3.9969794750213623
INFO:root:current mean train loss 1579.8270413515809
INFO:root:current train perplexity3.9946863651275635
INFO:root:current mean train loss 1579.236514815565
INFO:root:current train perplexity3.996720790863037
INFO:root:current mean train loss 1580.8570840563461
INFO:root:current train perplexity3.999479293823242
INFO:root:current mean train loss 1580.4384958896553
INFO:root:current train perplexity4.001589298248291
INFO:root:current mean train loss 1581.020503304841
INFO:root:current train perplexity4.0032148361206055
INFO:root:current mean train loss 1581.4786431160853
INFO:root:current train perplexity4.005700588226318
INFO:root:current mean train loss 1581.9297140059539
INFO:root:current train perplexity4.007885456085205
INFO:root:current mean train loss 1581.5620367002946
INFO:root:current train perplexity4.007293224334717

100%|██████████| 1/1 [05:31<00:00, 331.61s/it][A100%|██████████| 1/1 [05:31<00:00, 331.61s/it]
INFO:root:final mean train loss: 1581.328701648337
INFO:root:final train perplexity: 4.007546901702881
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.86s/it][A100%|██████████| 1/1 [00:23<00:00, 23.86s/it]
INFO:root:eval mean loss: 1785.1292936717364
INFO:root:eval perplexity: 5.009592056274414
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.03s/it][A100%|██████████| 1/1 [00:23<00:00, 23.03s/it]
INFO:root:eval mean loss: 2211.4543993794327
INFO:root:eval perplexity: 7.626027584075928
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/41
 20%|██        | 41/200 [4:29:05<16:54:37, 382.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1574.351079305013
INFO:root:current train perplexity3.969754695892334
INFO:root:current mean train loss 1576.781801807637
INFO:root:current train perplexity3.9858810901641846
INFO:root:current mean train loss 1569.8439260946739
INFO:root:current train perplexity3.9729015827178955
INFO:root:current mean train loss 1567.6528896755642
INFO:root:current train perplexity3.9660756587982178
INFO:root:current mean train loss 1565.799094907699
INFO:root:current train perplexity3.963324546813965
INFO:root:current mean train loss 1566.6314172936766
INFO:root:current train perplexity3.9602866172790527
INFO:root:current mean train loss 1570.5758537204786
INFO:root:current train perplexity3.9647090435028076
INFO:root:current mean train loss 1570.2268520336056
INFO:root:current train perplexity3.9621548652648926
INFO:root:current mean train loss 1570.257725579398
INFO:root:current train perplexity3.965003490447998
INFO:root:current mean train loss 1570.7392163870325
INFO:root:current train perplexity3.9672889709472656
INFO:root:current mean train loss 1571.4370923564381
INFO:root:current train perplexity3.970104455947876
INFO:root:current mean train loss 1572.2998975670855
INFO:root:current train perplexity3.971238136291504
INFO:root:current mean train loss 1571.7439010996877
INFO:root:current train perplexity3.971729040145874
INFO:root:current mean train loss 1572.8900560963802
INFO:root:current train perplexity3.976262331008911
INFO:root:current mean train loss 1572.3069382121855
INFO:root:current train perplexity3.9755451679229736
INFO:root:current mean train loss 1573.0123453929011
INFO:root:current train perplexity3.9762983322143555
INFO:root:current mean train loss 1574.001851711633
INFO:root:current train perplexity3.9792838096618652
INFO:root:current mean train loss 1574.6946691840158
INFO:root:current train perplexity3.983046293258667
INFO:root:current mean train loss 1574.8272391532544
INFO:root:current train perplexity3.984647274017334

100%|██████████| 1/1 [05:30<00:00, 330.68s/it][A100%|██████████| 1/1 [05:30<00:00, 330.68s/it]
INFO:root:final mean train loss: 1575.2147711726432
INFO:root:final train perplexity: 3.9860949516296387
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.34s/it][A100%|██████████| 1/1 [00:23<00:00, 23.34s/it]
INFO:root:eval mean loss: 1786.3886238260473
INFO:root:eval perplexity: 5.015289306640625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.40s/it][A100%|██████████| 1/1 [00:22<00:00, 22.40s/it]
INFO:root:eval mean loss: 2217.8809775494515
INFO:root:eval perplexity: 7.671182632446289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/42
 21%|██        | 42/200 [4:35:23<16:44:42, 381.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1583.3897329477163
INFO:root:current train perplexity4.084768772125244
INFO:root:current mean train loss 1567.9427122943168
INFO:root:current train perplexity3.9539542198181152
INFO:root:current mean train loss 1567.6452120928698
INFO:root:current train perplexity3.9494993686676025
INFO:root:current mean train loss 1565.5853985186202
INFO:root:current train perplexity3.9461469650268555
INFO:root:current mean train loss 1564.9138207239332
INFO:root:current train perplexity3.947011709213257
INFO:root:current mean train loss 1560.4361774526376
INFO:root:current train perplexity3.9444587230682373
INFO:root:current mean train loss 1562.6163531205393
INFO:root:current train perplexity3.951401710510254
INFO:root:current mean train loss 1564.1517457253133
INFO:root:current train perplexity3.951953887939453
INFO:root:current mean train loss 1565.8435887410633
INFO:root:current train perplexity3.9553096294403076
INFO:root:current mean train loss 1563.8861248438357
INFO:root:current train perplexity3.9553143978118896
INFO:root:current mean train loss 1563.7853048311406
INFO:root:current train perplexity3.9539594650268555
INFO:root:current mean train loss 1563.6009408517239
INFO:root:current train perplexity3.9502921104431152
INFO:root:current mean train loss 1565.3259889204837
INFO:root:current train perplexity3.9508092403411865
INFO:root:current mean train loss 1565.8411660699198
INFO:root:current train perplexity3.9519548416137695
INFO:root:current mean train loss 1566.036785063639
INFO:root:current train perplexity3.9516077041625977
INFO:root:current mean train loss 1566.8246673362112
INFO:root:current train perplexity3.955047607421875
INFO:root:current mean train loss 1568.2495593208744
INFO:root:current train perplexity3.958530902862549
INFO:root:current mean train loss 1569.1733325038538
INFO:root:current train perplexity3.9620556831359863
INFO:root:current mean train loss 1569.7556010949565
INFO:root:current train perplexity3.9634010791778564
INFO:root:current mean train loss 1568.604930823845
INFO:root:current train perplexity3.9626455307006836

100%|██████████| 1/1 [05:34<00:00, 334.22s/it][A100%|██████████| 1/1 [05:34<00:00, 334.22s/it]
INFO:root:final mean train loss: 1568.8498834945672
INFO:root:final train perplexity: 3.96388578414917
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.56s/it][A100%|██████████| 1/1 [00:23<00:00, 23.56s/it]
INFO:root:eval mean loss: 1787.9058270826408
INFO:root:eval perplexity: 5.022162437438965
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.39s/it][A100%|██████████| 1/1 [00:22<00:00, 22.39s/it]
INFO:root:eval mean loss: 2220.153542376579
INFO:root:eval perplexity: 7.687213897705078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/43
 22%|██▏       | 43/200 [4:41:46<16:38:49, 381.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.4955403645833
INFO:root:current train perplexity3.925835609436035
INFO:root:current mean train loss 1564.3523512620193
INFO:root:current train perplexity3.936375379562378
INFO:root:current mean train loss 1558.8409100076426
INFO:root:current train perplexity3.928920030593872
INFO:root:current mean train loss 1562.3633896336412
INFO:root:current train perplexity3.9383294582366943
INFO:root:current mean train loss 1558.0160462845204
INFO:root:current train perplexity3.9316651821136475
INFO:root:current mean train loss 1561.112473282724
INFO:root:current train perplexity3.9308485984802246
INFO:root:current mean train loss 1562.118205140129
INFO:root:current train perplexity3.9293978214263916
INFO:root:current mean train loss 1561.5476878545055
INFO:root:current train perplexity3.9265854358673096
INFO:root:current mean train loss 1562.5086437547063
INFO:root:current train perplexity3.929921865463257
INFO:root:current mean train loss 1563.6400402438255
INFO:root:current train perplexity3.935701608657837
INFO:root:current mean train loss 1564.290838800819
INFO:root:current train perplexity3.939655065536499
INFO:root:current mean train loss 1562.7925193584072
INFO:root:current train perplexity3.9377267360687256
INFO:root:current mean train loss 1562.580012822345
INFO:root:current train perplexity3.9386448860168457
INFO:root:current mean train loss 1562.8455704997357
INFO:root:current train perplexity3.940675735473633
INFO:root:current mean train loss 1563.4489248422476
INFO:root:current train perplexity3.9421815872192383
INFO:root:current mean train loss 1563.6322395673765
INFO:root:current train perplexity3.9436874389648438
INFO:root:current mean train loss 1562.622219942245
INFO:root:current train perplexity3.9425764083862305
INFO:root:current mean train loss 1562.9164413893154
INFO:root:current train perplexity3.9418482780456543
INFO:root:current mean train loss 1562.0637819383965
INFO:root:current train perplexity3.940610408782959
INFO:root:current mean train loss 1562.2429954410218
INFO:root:current train perplexity3.9401721954345703

100%|██████████| 1/1 [05:31<00:00, 331.46s/it][A100%|██████████| 1/1 [05:31<00:00, 331.46s/it]
INFO:root:final mean train loss: 1562.5703508816641
INFO:root:final train perplexity: 3.942094087600708
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.84s/it][A100%|██████████| 1/1 [00:22<00:00, 22.84s/it]
INFO:root:eval mean loss: 1789.0812287026263
INFO:root:eval perplexity: 5.027494430541992
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.00s/it][A100%|██████████| 1/1 [00:23<00:00, 23.01s/it]
INFO:root:eval mean loss: 2224.3834973057956
INFO:root:eval perplexity: 7.71714448928833
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/44
 22%|██▏       | 44/200 [4:48:05<16:30:33, 380.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1537.6725580743018
INFO:root:current train perplexity3.8819146156311035
INFO:root:current mean train loss 1545.4507010323662
INFO:root:current train perplexity3.8964803218841553
INFO:root:current mean train loss 1547.3383423345774
INFO:root:current train perplexity3.9109110832214355
INFO:root:current mean train loss 1552.5523058976496
INFO:root:current train perplexity3.9218850135803223
INFO:root:current mean train loss 1553.4733924951063
INFO:root:current train perplexity3.9243078231811523
INFO:root:current mean train loss 1554.1831554573241
INFO:root:current train perplexity3.9205868244171143
INFO:root:current mean train loss 1555.880853148848
INFO:root:current train perplexity3.920870542526245
INFO:root:current mean train loss 1556.305366158645
INFO:root:current train perplexity3.923501491546631
INFO:root:current mean train loss 1557.5839623245186
INFO:root:current train perplexity3.9207680225372314
INFO:root:current mean train loss 1556.4432000519732
INFO:root:current train perplexity3.9208803176879883
INFO:root:current mean train loss 1557.2955737328004
INFO:root:current train perplexity3.9231371879577637
INFO:root:current mean train loss 1557.0680377564433
INFO:root:current train perplexity3.9215407371520996
INFO:root:current mean train loss 1556.5183201402115
INFO:root:current train perplexity3.918639659881592
INFO:root:current mean train loss 1557.1056609631646
INFO:root:current train perplexity3.9178481101989746
INFO:root:current mean train loss 1557.2737307387051
INFO:root:current train perplexity3.9175667762756348
INFO:root:current mean train loss 1557.2724932896836
INFO:root:current train perplexity3.9178802967071533
INFO:root:current mean train loss 1557.4493860315395
INFO:root:current train perplexity3.9214389324188232
INFO:root:current mean train loss 1557.6208788866852
INFO:root:current train perplexity3.920837879180908
INFO:root:current mean train loss 1557.6731280216864
INFO:root:current train perplexity3.9218223094940186
INFO:root:current mean train loss 1557.7009311199922
INFO:root:current train perplexity3.9227938652038574

100%|██████████| 1/1 [05:32<00:00, 332.12s/it][A100%|██████████| 1/1 [05:32<00:00, 332.12s/it]
INFO:root:final mean train loss: 1557.2119005196514
INFO:root:final train perplexity: 3.9235939979553223
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.74s/it][A100%|██████████| 1/1 [00:23<00:00, 23.74s/it]
INFO:root:eval mean loss: 1790.699276755042
INFO:root:eval perplexity: 5.034842491149902
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.31s/it][A100%|██████████| 1/1 [00:22<00:00, 22.31s/it]
INFO:root:eval mean loss: 2226.9316947341813
INFO:root:eval perplexity: 7.735231399536133
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/45
 22%|██▎       | 45/200 [4:54:25<16:23:31, 380.72s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1543.160364151001
INFO:root:current train perplexity3.8364434242248535
INFO:root:current mean train loss 1540.045393036633
INFO:root:current train perplexity3.840538740158081
INFO:root:current mean train loss 1543.1197944409919
INFO:root:current train perplexity3.8571434020996094
INFO:root:current mean train loss 1545.7071590213984
INFO:root:current train perplexity3.8636910915374756
INFO:root:current mean train loss 1548.306174968851
INFO:root:current train perplexity3.876194953918457
INFO:root:current mean train loss 1549.2461379030917
INFO:root:current train perplexity3.8758180141448975
INFO:root:current mean train loss 1551.17344150773
INFO:root:current train perplexity3.8787195682525635
INFO:root:current mean train loss 1550.5073326869785
INFO:root:current train perplexity3.8839967250823975
INFO:root:current mean train loss 1549.9903281882957
INFO:root:current train perplexity3.889035224914551
INFO:root:current mean train loss 1549.5669587321302
INFO:root:current train perplexity3.890772581100464
INFO:root:current mean train loss 1549.9259636671022
INFO:root:current train perplexity3.890744924545288
INFO:root:current mean train loss 1550.3201871786741
INFO:root:current train perplexity3.8932595252990723
INFO:root:current mean train loss 1550.0405423128152
INFO:root:current train perplexity3.896855354309082
INFO:root:current mean train loss 1550.2315113593406
INFO:root:current train perplexity3.8961730003356934
INFO:root:current mean train loss 1551.1376993148053
INFO:root:current train perplexity3.897507905960083
INFO:root:current mean train loss 1551.9977805681547
INFO:root:current train perplexity3.899686336517334
INFO:root:current mean train loss 1551.4382355763362
INFO:root:current train perplexity3.902064323425293
INFO:root:current mean train loss 1551.071413831646
INFO:root:current train perplexity3.900724411010742
INFO:root:current mean train loss 1550.9169467385746
INFO:root:current train perplexity3.901085376739502
INFO:root:current mean train loss 1551.0621383262992
INFO:root:current train perplexity3.900287389755249

100%|██████████| 1/1 [05:41<00:00, 341.71s/it][A100%|██████████| 1/1 [05:41<00:00, 341.71s/it]
INFO:root:final mean train loss: 1550.4729940825139
INFO:root:final train perplexity: 3.900451898574829
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.59s/it][A100%|██████████| 1/1 [00:23<00:00, 23.59s/it]
INFO:root:eval mean loss: 1793.4121327501662
INFO:root:eval perplexity: 5.047186374664307
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.72s/it][A100%|██████████| 1/1 [00:22<00:00, 22.72s/it]
INFO:root:eval mean loss: 2232.8784551958665
INFO:root:eval perplexity: 7.777604103088379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/46
 23%|██▎       | 46/200 [5:00:55<16:24:14, 383.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1553.3963924455054
INFO:root:current train perplexity3.8568549156188965
INFO:root:current mean train loss 1540.4017010262
INFO:root:current train perplexity3.852388620376587
INFO:root:current mean train loss 1542.163994731428
INFO:root:current train perplexity3.8573861122131348
INFO:root:current mean train loss 1544.2850575684874
INFO:root:current train perplexity3.8552474975585938
INFO:root:current mean train loss 1539.9389907297623
INFO:root:current train perplexity3.8581900596618652
INFO:root:current mean train loss 1541.2290497088802
INFO:root:current train perplexity3.868489980697632
INFO:root:current mean train loss 1542.5988957745387
INFO:root:current train perplexity3.872363328933716
INFO:root:current mean train loss 1542.0481124272267
INFO:root:current train perplexity3.8752806186676025
INFO:root:current mean train loss 1541.5469384599355
INFO:root:current train perplexity3.8723227977752686
INFO:root:current mean train loss 1541.4055150894337
INFO:root:current train perplexity3.8752732276916504
INFO:root:current mean train loss 1541.2651000186097
INFO:root:current train perplexity3.8749027252197266
INFO:root:current mean train loss 1542.3719651935264
INFO:root:current train perplexity3.8743162155151367
INFO:root:current mean train loss 1544.2219872932524
INFO:root:current train perplexity3.8780677318573
INFO:root:current mean train loss 1544.8444056086225
INFO:root:current train perplexity3.880157947540283
INFO:root:current mean train loss 1544.829314546759
INFO:root:current train perplexity3.8800599575042725
INFO:root:current mean train loss 1545.3205015120968
INFO:root:current train perplexity3.8818178176879883
INFO:root:current mean train loss 1545.6634373344316
INFO:root:current train perplexity3.883042097091675
INFO:root:current mean train loss 1545.1698586808236
INFO:root:current train perplexity3.8815393447875977
INFO:root:current mean train loss 1544.9451880934137
INFO:root:current train perplexity3.8812334537506104
INFO:root:current mean train loss 1545.5298886171558
INFO:root:current train perplexity3.8819382190704346

100%|██████████| 1/1 [05:31<00:00, 331.42s/it][A100%|██████████| 1/1 [05:31<00:00, 331.42s/it]
INFO:root:final mean train loss: 1544.948221072968
INFO:root:final train perplexity: 3.8815805912017822
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.50s/it][A100%|██████████| 1/1 [00:23<00:00, 23.51s/it]
INFO:root:eval mean loss: 1794.9077771775267
INFO:root:eval perplexity: 5.0540056228637695
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.34s/it][A100%|██████████| 1/1 [00:23<00:00, 23.34s/it]
INFO:root:eval mean loss: 2235.3090352809177
INFO:root:eval perplexity: 7.7949910163879395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/47
 24%|██▎       | 47/200 [5:07:15<16:15:18, 382.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.266018614477
INFO:root:current train perplexity3.8283979892730713
INFO:root:current mean train loss 1519.0639636107164
INFO:root:current train perplexity3.8161892890930176
INFO:root:current mean train loss 1522.3760933075976
INFO:root:current train perplexity3.8190627098083496
INFO:root:current mean train loss 1527.7681378695233
INFO:root:current train perplexity3.827739953994751
INFO:root:current mean train loss 1527.3510926028332
INFO:root:current train perplexity3.83066725730896
INFO:root:current mean train loss 1530.5539101693143
INFO:root:current train perplexity3.8293967247009277
INFO:root:current mean train loss 1531.6230897220294
INFO:root:current train perplexity3.8334639072418213
INFO:root:current mean train loss 1532.253991301496
INFO:root:current train perplexity3.837865114212036
INFO:root:current mean train loss 1533.0828550207057
INFO:root:current train perplexity3.843334197998047
INFO:root:current mean train loss 1534.1503956399126
INFO:root:current train perplexity3.8451907634735107
INFO:root:current mean train loss 1534.4432950045893
INFO:root:current train perplexity3.8451976776123047
INFO:root:current mean train loss 1534.040268734023
INFO:root:current train perplexity3.8480465412139893
INFO:root:current mean train loss 1534.8388843036764
INFO:root:current train perplexity3.848860502243042
INFO:root:current mean train loss 1535.2901347628308
INFO:root:current train perplexity3.85040283203125
INFO:root:current mean train loss 1535.7574986049108
INFO:root:current train perplexity3.85367751121521
INFO:root:current mean train loss 1537.2577156382001
INFO:root:current train perplexity3.8545572757720947
INFO:root:current mean train loss 1538.3100882845856
INFO:root:current train perplexity3.8591794967651367
INFO:root:current mean train loss 1538.9609978562335
INFO:root:current train perplexity3.859288454055786
INFO:root:current mean train loss 1539.7598769546685
INFO:root:current train perplexity3.8613669872283936

100%|██████████| 1/1 [05:35<00:00, 335.40s/it][A100%|██████████| 1/1 [05:35<00:00, 335.40s/it]
INFO:root:final mean train loss: 1539.0510615561866
INFO:root:final train perplexity: 3.8615384101867676
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.09s/it][A100%|██████████| 1/1 [00:23<00:00, 23.09s/it]
INFO:root:eval mean loss: 1795.0037564757868
INFO:root:eval perplexity: 5.054442882537842
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.61s/it][A100%|██████████| 1/1 [00:22<00:00, 22.61s/it]
INFO:root:eval mean loss: 2237.0200009176915
INFO:root:eval perplexity: 7.807251453399658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/48
 24%|██▍       | 48/200 [5:13:38<16:09:19, 382.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1513.0733154296875
INFO:root:current train perplexity3.80682110786438
INFO:root:current mean train loss 1517.6885190217392
INFO:root:current train perplexity3.81595778465271
INFO:root:current mean train loss 1528.611110101744
INFO:root:current train perplexity3.8242111206054688
INFO:root:current mean train loss 1528.1765303354414
INFO:root:current train perplexity3.8296003341674805
INFO:root:current mean train loss 1527.3389289580196
INFO:root:current train perplexity3.832454204559326
INFO:root:current mean train loss 1526.4088826892446
INFO:root:current train perplexity3.83052396774292
INFO:root:current mean train loss 1529.39840574187
INFO:root:current train perplexity3.833270311355591
INFO:root:current mean train loss 1529.9077100633742
INFO:root:current train perplexity3.8251636028289795
INFO:root:current mean train loss 1528.323864671204
INFO:root:current train perplexity3.826235294342041
INFO:root:current mean train loss 1529.979372785391
INFO:root:current train perplexity3.8272879123687744
INFO:root:current mean train loss 1529.154122007774
INFO:root:current train perplexity3.8252999782562256
INFO:root:current mean train loss 1531.5470585981293
INFO:root:current train perplexity3.8282642364501953
INFO:root:current mean train loss 1532.3448857060184
INFO:root:current train perplexity3.830242156982422
INFO:root:current mean train loss 1531.1398824597493
INFO:root:current train perplexity3.832064628601074
INFO:root:current mean train loss 1532.399762329533
INFO:root:current train perplexity3.8348727226257324
INFO:root:current mean train loss 1533.5154350054147
INFO:root:current train perplexity3.8371970653533936
INFO:root:current mean train loss 1532.890161510739
INFO:root:current train perplexity3.8390161991119385
INFO:root:current mean train loss 1532.3756425952076
INFO:root:current train perplexity3.8373420238494873
INFO:root:current mean train loss 1532.0442630988507
INFO:root:current train perplexity3.8375091552734375
INFO:root:current mean train loss 1533.1235938009954
INFO:root:current train perplexity3.839229106903076

100%|██████████| 1/1 [05:39<00:00, 339.03s/it][A100%|██████████| 1/1 [05:39<00:00, 339.03s/it]
INFO:root:final mean train loss: 1532.9428239092344
INFO:root:final train perplexity: 3.8408870697021484
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.81s/it][A100%|██████████| 1/1 [00:23<00:00, 23.81s/it]
INFO:root:eval mean loss: 1796.5165405273438
INFO:root:eval perplexity: 5.061349391937256
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.46s/it][A100%|██████████| 1/1 [00:22<00:00, 22.46s/it]
INFO:root:eval mean loss: 2242.8214604803857
INFO:root:eval perplexity: 7.848972320556641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/49
 24%|██▍       | 49/200 [5:20:05<16:06:25, 384.01s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1542.3902320861816
INFO:root:current train perplexity3.784590244293213
INFO:root:current mean train loss 1523.3496047511246
INFO:root:current train perplexity3.7911744117736816
INFO:root:current mean train loss 1519.240125458816
INFO:root:current train perplexity3.807967185974121
INFO:root:current mean train loss 1520.9621876176582
INFO:root:current train perplexity3.8009185791015625
INFO:root:current mean train loss 1521.5769051445855
INFO:root:current train perplexity3.8005876541137695
INFO:root:current mean train loss 1523.1389786569696
INFO:root:current train perplexity3.807257652282715
INFO:root:current mean train loss 1529.7555963057505
INFO:root:current train perplexity3.815711259841919
INFO:root:current mean train loss 1528.377442073301
INFO:root:current train perplexity3.814793348312378
INFO:root:current mean train loss 1527.5977191925049
INFO:root:current train perplexity3.8142149448394775
INFO:root:current mean train loss 1527.69683942672
INFO:root:current train perplexity3.815927743911743
INFO:root:current mean train loss 1527.3294567729151
INFO:root:current train perplexity3.81978178024292
INFO:root:current mean train loss 1526.9889842326565
INFO:root:current train perplexity3.8178136348724365
INFO:root:current mean train loss 1525.922201577719
INFO:root:current train perplexity3.8161988258361816
INFO:root:current mean train loss 1526.8776539295643
INFO:root:current train perplexity3.818143129348755
INFO:root:current mean train loss 1527.2485882636556
INFO:root:current train perplexity3.8193955421447754
INFO:root:current mean train loss 1528.5822718050088
INFO:root:current train perplexity3.8215084075927734
INFO:root:current mean train loss 1528.8014437357585
INFO:root:current train perplexity3.823244333267212
INFO:root:current mean train loss 1528.92260100825
INFO:root:current train perplexity3.823993682861328
INFO:root:current mean train loss 1528.8301844700975
INFO:root:current train perplexity3.823392868041992
INFO:root:current mean train loss 1528.5564235268666
INFO:root:current train perplexity3.8239810466766357

100%|██████████| 1/1 [05:30<00:00, 330.35s/it][A100%|██████████| 1/1 [05:30<00:00, 330.35s/it]
INFO:root:final mean train loss: 1527.9202917178832
INFO:root:final train perplexity: 3.8239901065826416
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.61s/it][A100%|██████████| 1/1 [00:24<00:00, 24.61s/it]
INFO:root:eval mean loss: 1799.0762355073969
INFO:root:eval perplexity: 5.073057651519775
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.24s/it][A100%|██████████| 1/1 [00:23<00:00, 23.24s/it]
INFO:root:eval mean loss: 2245.75462049119
INFO:root:eval perplexity: 7.870150566101074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/50
 25%|██▌       | 50/200 [5:26:25<15:57:07, 382.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1508.7528225645726
INFO:root:current train perplexity3.744809627532959
INFO:root:current mean train loss 1520.7700891686766
INFO:root:current train perplexity3.8025355339050293
INFO:root:current mean train loss 1517.0071879314132
INFO:root:current train perplexity3.7827811241149902
INFO:root:current mean train loss 1514.345015124127
INFO:root:current train perplexity3.782299518585205
INFO:root:current mean train loss 1519.28941484549
INFO:root:current train perplexity3.7932796478271484
INFO:root:current mean train loss 1519.6757994827242
INFO:root:current train perplexity3.794375419616699
INFO:root:current mean train loss 1521.579156672826
INFO:root:current train perplexity3.797933340072632
INFO:root:current mean train loss 1520.3562986325517
INFO:root:current train perplexity3.7990622520446777
INFO:root:current mean train loss 1522.3739038114693
INFO:root:current train perplexity3.8023574352264404
INFO:root:current mean train loss 1523.2580796654784
INFO:root:current train perplexity3.7995009422302246
INFO:root:current mean train loss 1521.7986519434432
INFO:root:current train perplexity3.79921293258667
INFO:root:current mean train loss 1520.59523471072
INFO:root:current train perplexity3.7982566356658936
INFO:root:current mean train loss 1520.084460830383
INFO:root:current train perplexity3.7976412773132324
INFO:root:current mean train loss 1521.6152470435277
INFO:root:current train perplexity3.7985851764678955
INFO:root:current mean train loss 1521.8626814290028
INFO:root:current train perplexity3.799349784851074
INFO:root:current mean train loss 1522.6585691783257
INFO:root:current train perplexity3.8007373809814453
INFO:root:current mean train loss 1521.9718578701818
INFO:root:current train perplexity3.8002302646636963
INFO:root:current mean train loss 1521.8391497150158
INFO:root:current train perplexity3.801875114440918
INFO:root:current mean train loss 1521.776231517142
INFO:root:current train perplexity3.804492950439453
INFO:root:current mean train loss 1522.6681178774572
INFO:root:current train perplexity3.8044965267181396

100%|██████████| 1/1 [05:44<00:00, 344.73s/it][A100%|██████████| 1/1 [05:44<00:00, 344.73s/it]
INFO:root:final mean train loss: 1522.3256032600345
INFO:root:final train perplexity: 3.80525541305542
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.91s/it][A100%|██████████| 1/1 [00:23<00:00, 23.91s/it]
INFO:root:eval mean loss: 1798.639876994681
INFO:root:eval perplexity: 5.071059703826904
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.77s/it][A100%|██████████| 1/1 [00:22<00:00, 22.77s/it]
INFO:root:eval mean loss: 2244.9453601160794
INFO:root:eval perplexity: 7.86430025100708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/51
 26%|██▌       | 51/200 [5:32:59<15:58:32, 385.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1527.173214074337
INFO:root:current train perplexity3.7871956825256348
INFO:root:current mean train loss 1509.1185971915004
INFO:root:current train perplexity3.779686689376831
INFO:root:current mean train loss 1502.999048677602
INFO:root:current train perplexity3.774446487426758
INFO:root:current mean train loss 1503.1936208589482
INFO:root:current train perplexity3.7815515995025635
INFO:root:current mean train loss 1506.9198260000335
INFO:root:current train perplexity3.781399726867676
INFO:root:current mean train loss 1508.5138122774265
INFO:root:current train perplexity3.786355972290039
INFO:root:current mean train loss 1509.8328254401863
INFO:root:current train perplexity3.7888100147247314
INFO:root:current mean train loss 1510.9073252067865
INFO:root:current train perplexity3.7834534645080566
INFO:root:current mean train loss 1510.565778842424
INFO:root:current train perplexity3.782196283340454
INFO:root:current mean train loss 1513.0452216170095
INFO:root:current train perplexity3.7843079566955566
INFO:root:current mean train loss 1512.8312790174646
INFO:root:current train perplexity3.7854745388031006
INFO:root:current mean train loss 1513.8253913997173
INFO:root:current train perplexity3.7833805084228516
INFO:root:current mean train loss 1512.8175294704347
INFO:root:current train perplexity3.7806992530822754
INFO:root:current mean train loss 1512.467320365403
INFO:root:current train perplexity3.7808046340942383
INFO:root:current mean train loss 1513.0579882279587
INFO:root:current train perplexity3.783066987991333
INFO:root:current mean train loss 1514.2684168712085
INFO:root:current train perplexity3.7850725650787354
INFO:root:current mean train loss 1515.0758192192893
INFO:root:current train perplexity3.785825252532959
INFO:root:current mean train loss 1515.4500852003778
INFO:root:current train perplexity3.78668475151062
INFO:root:current mean train loss 1516.2654345676958
INFO:root:current train perplexity3.786325454711914
INFO:root:current mean train loss 1517.1215189471006
INFO:root:current train perplexity3.7863290309906006

100%|██████████| 1/1 [05:30<00:00, 330.34s/it][A100%|██████████| 1/1 [05:30<00:00, 330.34s/it]
INFO:root:final mean train loss: 1516.7064637123065
INFO:root:final train perplexity: 3.7865302562713623
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.68s/it][A100%|██████████| 1/1 [00:23<00:00, 23.68s/it]
INFO:root:eval mean loss: 1801.8393740823083
INFO:root:eval perplexity: 5.085726737976074
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2252.5933682056184
INFO:root:eval perplexity: 7.9197492599487305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/52
 26%|██▌       | 52/200 [5:39:17<15:46:14, 383.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1497.6769401826054
INFO:root:current train perplexity3.7142276763916016
INFO:root:current mean train loss 1501.2916613302596
INFO:root:current train perplexity3.725641965866089
INFO:root:current mean train loss 1501.7311929763416
INFO:root:current train perplexity3.7344706058502197
INFO:root:current mean train loss 1503.5872754926158
INFO:root:current train perplexity3.743513584136963
INFO:root:current mean train loss 1500.362969629513
INFO:root:current train perplexity3.743605375289917
INFO:root:current mean train loss 1501.9709550127975
INFO:root:current train perplexity3.7446770668029785
INFO:root:current mean train loss 1504.010600099801
INFO:root:current train perplexity3.7452359199523926
INFO:root:current mean train loss 1506.2105678592452
INFO:root:current train perplexity3.749943494796753
INFO:root:current mean train loss 1506.5381578248866
INFO:root:current train perplexity3.755244493484497
INFO:root:current mean train loss 1507.6343638981832
INFO:root:current train perplexity3.757136344909668
INFO:root:current mean train loss 1508.9232119122591
INFO:root:current train perplexity3.760756015777588
INFO:root:current mean train loss 1509.722314907148
INFO:root:current train perplexity3.7619361877441406
INFO:root:current mean train loss 1510.2029924637995
INFO:root:current train perplexity3.763977289199829
INFO:root:current mean train loss 1509.6115552305055
INFO:root:current train perplexity3.7623867988586426
INFO:root:current mean train loss 1510.5713491510558
INFO:root:current train perplexity3.7629575729370117
INFO:root:current mean train loss 1510.9797166642402
INFO:root:current train perplexity3.765267848968506
INFO:root:current mean train loss 1512.17562356678
INFO:root:current train perplexity3.7673184871673584
INFO:root:current mean train loss 1511.794812812741
INFO:root:current train perplexity3.7670605182647705
INFO:root:current mean train loss 1511.7636250694952
INFO:root:current train perplexity3.766031503677368
INFO:root:current mean train loss 1511.0723703358428
INFO:root:current train perplexity3.7678492069244385

100%|██████████| 1/1 [05:39<00:00, 339.90s/it][A100%|██████████| 1/1 [05:39<00:00, 339.90s/it]
INFO:root:final mean train loss: 1511.0723703358428
INFO:root:final train perplexity: 3.7678492069244385
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.80s/it][A100%|██████████| 1/1 [00:23<00:00, 23.81s/it]
INFO:root:eval mean loss: 1804.4407132196088
INFO:root:eval perplexity: 5.097681999206543
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.70s/it][A100%|██████████| 1/1 [00:22<00:00, 22.70s/it]
INFO:root:eval mean loss: 2255.6623279760915
INFO:root:eval perplexity: 7.942110538482666
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/53
 26%|██▋       | 53/200 [5:45:46<15:43:40, 385.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1500.4596044921875
INFO:root:current train perplexity3.704220771789551
INFO:root:current mean train loss 1504.372243041992
INFO:root:current train perplexity3.7183609008789062
INFO:root:current mean train loss 1503.7558793131511
INFO:root:current train perplexity3.7244105339050293
INFO:root:current mean train loss 1501.8218127441405
INFO:root:current train perplexity3.725446939468384
INFO:root:current mean train loss 1502.1995390625
INFO:root:current train perplexity3.728902816772461
INFO:root:current mean train loss 1499.548379720052
INFO:root:current train perplexity3.727520704269409
INFO:root:current mean train loss 1503.4851604352677
INFO:root:current train perplexity3.7356433868408203
INFO:root:current mean train loss 1501.8783222961426
INFO:root:current train perplexity3.7349462509155273
INFO:root:current mean train loss 1502.7252014160156
INFO:root:current train perplexity3.7330996990203857
INFO:root:current mean train loss 1504.868432861328
INFO:root:current train perplexity3.739689588546753
INFO:root:current mean train loss 1504.1808336292613
INFO:root:current train perplexity3.739999532699585
INFO:root:current mean train loss 1503.5330370076497
INFO:root:current train perplexity3.7425694465637207
INFO:root:current mean train loss 1503.67014056866
INFO:root:current train perplexity3.743169069290161
INFO:root:current mean train loss 1503.702100655692
INFO:root:current train perplexity3.742732524871826
INFO:root:current mean train loss 1505.7929596354168
INFO:root:current train perplexity3.7458620071411133
INFO:root:current mean train loss 1505.8147387695312
INFO:root:current train perplexity3.745675802230835
INFO:root:current mean train loss 1505.7784215590534
INFO:root:current train perplexity3.746617555618286
INFO:root:current mean train loss 1505.8852239312066
INFO:root:current train perplexity3.7480649948120117
INFO:root:current mean train loss 1505.61116718493
INFO:root:current train perplexity3.7490627765655518

100%|██████████| 1/1 [05:33<00:00, 333.28s/it][A100%|██████████| 1/1 [05:33<00:00, 333.28s/it]
INFO:root:final mean train loss: 1505.817162679652
INFO:root:final train perplexity: 3.750507116317749
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.06s/it][A100%|██████████| 1/1 [00:23<00:00, 23.06s/it]
INFO:root:eval mean loss: 1806.3824138235539
INFO:root:eval perplexity: 5.106624126434326
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.86s/it][A100%|██████████| 1/1 [00:22<00:00, 22.86s/it]
INFO:root:eval mean loss: 2258.79735072792
INFO:root:eval perplexity: 7.965015888214111
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/54
 27%|██▋       | 54/200 [5:52:07<15:34:18, 383.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.9749612247242
INFO:root:current train perplexity3.8120408058166504
INFO:root:current mean train loss 1499.6351954794338
INFO:root:current train perplexity3.724480390548706
INFO:root:current mean train loss 1503.07080640661
INFO:root:current train perplexity3.714052677154541
INFO:root:current mean train loss 1496.9736936551162
INFO:root:current train perplexity3.7145445346832275
INFO:root:current mean train loss 1497.7826864484975
INFO:root:current train perplexity3.7201411724090576
INFO:root:current mean train loss 1499.0537005485373
INFO:root:current train perplexity3.7228143215179443
INFO:root:current mean train loss 1499.4162290996633
INFO:root:current train perplexity3.7236883640289307
INFO:root:current mean train loss 1498.7700535815463
INFO:root:current train perplexity3.719558000564575
INFO:root:current mean train loss 1499.0690713273123
INFO:root:current train perplexity3.722390651702881
INFO:root:current mean train loss 1497.8697224890523
INFO:root:current train perplexity3.7232887744903564
INFO:root:current mean train loss 1497.9447520808367
INFO:root:current train perplexity3.7251508235931396
INFO:root:current mean train loss 1499.5282814073691
INFO:root:current train perplexity3.729175329208374
INFO:root:current mean train loss 1499.2214179936254
INFO:root:current train perplexity3.727667808532715
INFO:root:current mean train loss 1500.4113224524842
INFO:root:current train perplexity3.7280354499816895
INFO:root:current mean train loss 1500.6585342741046
INFO:root:current train perplexity3.7282371520996094
INFO:root:current mean train loss 1501.2241873995756
INFO:root:current train perplexity3.728504180908203
INFO:root:current mean train loss 1502.097439814882
INFO:root:current train perplexity3.731088876724243
INFO:root:current mean train loss 1502.0407889026783
INFO:root:current train perplexity3.7330477237701416
INFO:root:current mean train loss 1501.8982866084634
INFO:root:current train perplexity3.7329649925231934
INFO:root:current mean train loss 1501.0884411194452
INFO:root:current train perplexity3.7316737174987793

100%|██████████| 1/1 [05:38<00:00, 338.16s/it][A100%|██████████| 1/1 [05:38<00:00, 338.16s/it]
INFO:root:final mean train loss: 1500.6470233188154
INFO:root:final train perplexity: 3.73352313041687
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.82s/it][A100%|██████████| 1/1 [00:23<00:00, 23.82s/it]
INFO:root:eval mean loss: 1805.6976240442154
INFO:root:eval perplexity: 5.103468894958496
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.48s/it][A100%|██████████| 1/1 [00:23<00:00, 23.48s/it]
INFO:root:eval mean loss: 2259.3225114971187
INFO:root:eval perplexity: 7.968860626220703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/55
 28%|██▊       | 55/200 [5:58:34<15:30:23, 384.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1502.8917846679688
INFO:root:current train perplexity3.754934549331665
INFO:root:current mean train loss 1486.9948347860309
INFO:root:current train perplexity3.706742525100708
INFO:root:current mean train loss 1485.0741399781316
INFO:root:current train perplexity3.700562000274658
INFO:root:current mean train loss 1486.8411803102779
INFO:root:current train perplexity3.7098512649536133
INFO:root:current mean train loss 1483.661372734105
INFO:root:current train perplexity3.6940908432006836
INFO:root:current mean train loss 1485.0559728958187
INFO:root:current train perplexity3.6951372623443604
INFO:root:current mean train loss 1487.3432353407802
INFO:root:current train perplexity3.7017810344696045
INFO:root:current mean train loss 1488.0396455769967
INFO:root:current train perplexity3.706299304962158
INFO:root:current mean train loss 1489.0811493871308
INFO:root:current train perplexity3.706406831741333
INFO:root:current mean train loss 1490.2298054664498
INFO:root:current train perplexity3.7082622051239014
INFO:root:current mean train loss 1491.0916275821294
INFO:root:current train perplexity3.706763744354248
INFO:root:current mean train loss 1491.3310790154458
INFO:root:current train perplexity3.7073400020599365
INFO:root:current mean train loss 1493.1766968762663
INFO:root:current train perplexity3.707425832748413
INFO:root:current mean train loss 1494.8356757900347
INFO:root:current train perplexity3.7109272480010986
INFO:root:current mean train loss 1493.8604200887214
INFO:root:current train perplexity3.7103633880615234
INFO:root:current mean train loss 1493.7202964892133
INFO:root:current train perplexity3.7119171619415283
INFO:root:current mean train loss 1494.299957686276
INFO:root:current train perplexity3.711472749710083
INFO:root:current mean train loss 1495.3321172764831
INFO:root:current train perplexity3.7148072719573975
INFO:root:current mean train loss 1495.188615672071
INFO:root:current train perplexity3.714421033859253
INFO:root:current mean train loss 1495.2211396494474
INFO:root:current train perplexity3.7155234813690186

100%|██████████| 1/1 [05:35<00:00, 335.63s/it][A100%|██████████| 1/1 [05:35<00:00, 335.63s/it]
INFO:root:final mean train loss: 1494.9862823871067
INFO:root:final train perplexity: 3.7150166034698486
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.42s/it][A100%|██████████| 1/1 [00:23<00:00, 23.42s/it]
INFO:root:eval mean loss: 1811.2554355918937
INFO:root:eval perplexity: 5.129135608673096
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2267.591986040697
INFO:root:eval perplexity: 8.02962589263916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/56
 28%|██▊       | 56/200 [6:04:57<15:22:36, 384.42s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.2017965877758
INFO:root:current train perplexity3.711843967437744
INFO:root:current mean train loss 1488.350800975269
INFO:root:current train perplexity3.690018653869629
INFO:root:current mean train loss 1490.7517955521664
INFO:root:current train perplexity3.6894516944885254
INFO:root:current mean train loss 1489.068597255609
INFO:root:current train perplexity3.6913843154907227
INFO:root:current mean train loss 1486.570350934555
INFO:root:current train perplexity3.685504674911499
INFO:root:current mean train loss 1486.0546305633932
INFO:root:current train perplexity3.688673257827759
INFO:root:current mean train loss 1484.9051506171395
INFO:root:current train perplexity3.688981056213379
INFO:root:current mean train loss 1486.2942861666215
INFO:root:current train perplexity3.69338321685791
INFO:root:current mean train loss 1487.0077935654745
INFO:root:current train perplexity3.6931509971618652
INFO:root:current mean train loss 1487.4788222769207
INFO:root:current train perplexity3.6968400478363037
INFO:root:current mean train loss 1488.5363509362364
INFO:root:current train perplexity3.6947011947631836
INFO:root:current mean train loss 1487.5439016174794
INFO:root:current train perplexity3.6944918632507324
INFO:root:current mean train loss 1487.6853259579837
INFO:root:current train perplexity3.6952438354492188
INFO:root:current mean train loss 1487.5925746553478
INFO:root:current train perplexity3.694472551345825
INFO:root:current mean train loss 1489.2703642894448
INFO:root:current train perplexity3.6976194381713867
INFO:root:current mean train loss 1488.8384761060154
INFO:root:current train perplexity3.6992061138153076
INFO:root:current mean train loss 1489.2609829270139
INFO:root:current train perplexity3.6997900009155273
INFO:root:current mean train loss 1489.4549933520132
INFO:root:current train perplexity3.6996519565582275
INFO:root:current mean train loss 1489.9718048573827
INFO:root:current train perplexity3.697842836380005
INFO:root:current mean train loss 1489.9863884406234
INFO:root:current train perplexity3.699197769165039

100%|██████████| 1/1 [05:34<00:00, 334.90s/it][A100%|██████████| 1/1 [05:34<00:00, 334.90s/it]
INFO:root:final mean train loss: 1489.9763424902687
INFO:root:final train perplexity: 3.698713541030884
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.60s/it][A100%|██████████| 1/1 [00:23<00:00, 23.60s/it]
INFO:root:eval mean loss: 1811.6765210307237
INFO:root:eval perplexity: 5.1310858726501465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.35s/it][A100%|██████████| 1/1 [00:22<00:00, 22.35s/it]
INFO:root:eval mean loss: 2269.1270258477393
INFO:root:eval perplexity: 8.040960311889648
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/57
 28%|██▊       | 57/200 [6:11:20<15:15:03, 383.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1491.2323877671186
INFO:root:current train perplexity3.687382936477661
INFO:root:current mean train loss 1490.009302048456
INFO:root:current train perplexity3.6755542755126953
INFO:root:current mean train loss 1484.870706131209
INFO:root:current train perplexity3.6682345867156982
INFO:root:current mean train loss 1485.1997587784476
INFO:root:current train perplexity3.6710762977600098
INFO:root:current mean train loss 1485.4220002003206
INFO:root:current train perplexity3.67502498626709
INFO:root:current mean train loss 1485.826977582045
INFO:root:current train perplexity3.677283525466919
INFO:root:current mean train loss 1486.1132410472026
INFO:root:current train perplexity3.6785998344421387
INFO:root:current mean train loss 1486.431272983551
INFO:root:current train perplexity3.6757307052612305
INFO:root:current mean train loss 1484.5399477910337
INFO:root:current train perplexity3.6716854572296143
INFO:root:current mean train loss 1484.2727159232147
INFO:root:current train perplexity3.6706366539001465
INFO:root:current mean train loss 1485.0892666591687
INFO:root:current train perplexity3.674313545227051
INFO:root:current mean train loss 1485.2586160947199
INFO:root:current train perplexity3.6759743690490723
INFO:root:current mean train loss 1485.60544275711
INFO:root:current train perplexity3.6765196323394775
INFO:root:current mean train loss 1483.9076349804973
INFO:root:current train perplexity3.6749355792999268
INFO:root:current mean train loss 1484.1239469356692
INFO:root:current train perplexity3.6766891479492188
INFO:root:current mean train loss 1484.2169208137357
INFO:root:current train perplexity3.678140640258789
INFO:root:current mean train loss 1484.7455837332088
INFO:root:current train perplexity3.679957389831543
INFO:root:current mean train loss 1484.9012904102447
INFO:root:current train perplexity3.679964780807495
INFO:root:current mean train loss 1484.4345062059815
INFO:root:current train perplexity3.680220127105713
INFO:root:current mean train loss 1485.0898772449027
INFO:root:current train perplexity3.681309938430786

100%|██████████| 1/1 [05:31<00:00, 331.29s/it][A100%|██████████| 1/1 [05:31<00:00, 331.29s/it]
INFO:root:final mean train loss: 1484.7799576736734
INFO:root:final train perplexity: 3.681879758834839
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.23s/it][A100%|██████████| 1/1 [00:23<00:00, 23.23s/it]
INFO:root:eval mean loss: 1813.7126633664395
INFO:root:eval perplexity: 5.1405253410339355
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.07s/it][A100%|██████████| 1/1 [00:23<00:00, 23.07s/it]
INFO:root:eval mean loss: 2270.8528780024103
INFO:root:eval perplexity: 8.053716659545898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/58
 29%|██▉       | 58/200 [6:17:40<15:05:40, 382.68s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1470.7876349954045
INFO:root:current train perplexity3.632030487060547
INFO:root:current mean train loss 1465.9041173986486
INFO:root:current train perplexity3.642483949661255
INFO:root:current mean train loss 1468.198057582922
INFO:root:current train perplexity3.6402804851531982
INFO:root:current mean train loss 1468.1507175197848
INFO:root:current train perplexity3.646801710128784
INFO:root:current mean train loss 1471.7889708843427
INFO:root:current train perplexity3.652510643005371
INFO:root:current mean train loss 1473.2934722639557
INFO:root:current train perplexity3.654081106185913
INFO:root:current mean train loss 1473.755586899806
INFO:root:current train perplexity3.6549925804138184
INFO:root:current mean train loss 1472.2252010661325
INFO:root:current train perplexity3.6515891551971436
INFO:root:current mean train loss 1472.7995921334305
INFO:root:current train perplexity3.6515824794769287
INFO:root:current mean train loss 1472.9808616057264
INFO:root:current train perplexity3.6541616916656494
INFO:root:current mean train loss 1472.4871875675044
INFO:root:current train perplexity3.6557276248931885
INFO:root:current mean train loss 1473.4190910757845
INFO:root:current train perplexity3.655374765396118
INFO:root:current mean train loss 1475.1459722496656
INFO:root:current train perplexity3.6591193675994873
INFO:root:current mean train loss 1475.5954451468017
INFO:root:current train perplexity3.6584889888763428
INFO:root:current mean train loss 1476.3750665016046
INFO:root:current train perplexity3.660093069076538
INFO:root:current mean train loss 1477.5279460148856
INFO:root:current train perplexity3.660349130630493
INFO:root:current mean train loss 1478.1109106227977
INFO:root:current train perplexity3.664083242416382
INFO:root:current mean train loss 1478.8342767282695
INFO:root:current train perplexity3.6640233993530273
INFO:root:current mean train loss 1479.846332709922
INFO:root:current train perplexity3.665076971054077

100%|██████████| 1/1 [05:45<00:00, 345.86s/it][A100%|██████████| 1/1 [05:45<00:00, 345.86s/it]
INFO:root:final mean train loss: 1479.972596015103
INFO:root:final train perplexity: 3.6663739681243896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.38s/it][A100%|██████████| 1/1 [00:23<00:00, 23.38s/it]
INFO:root:eval mean loss: 1811.0104248912621
INFO:root:eval perplexity: 5.128002166748047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.80s/it][A100%|██████████| 1/1 [00:22<00:00, 22.80s/it]
INFO:root:eval mean loss: 2269.9378324468084
INFO:root:eval perplexity: 8.046951293945312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/59
 30%|██▉       | 59/200 [6:24:14<15:07:15, 386.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1435.1746215820312
INFO:root:current train perplexity3.7514116764068604
INFO:root:current mean train loss 1481.950231215533
INFO:root:current train perplexity3.645378351211548
INFO:root:current mean train loss 1475.5941035204594
INFO:root:current train perplexity3.657149076461792
INFO:root:current mean train loss 1469.8021030047082
INFO:root:current train perplexity3.646350383758545
INFO:root:current mean train loss 1471.2056726863727
INFO:root:current train perplexity3.6508986949920654
INFO:root:current mean train loss 1474.7523490024278
INFO:root:current train perplexity3.6536290645599365
INFO:root:current mean train loss 1474.2471193839544
INFO:root:current train perplexity3.648822546005249
INFO:root:current mean train loss 1476.1163994335382
INFO:root:current train perplexity3.6486668586730957
INFO:root:current mean train loss 1475.5481573471106
INFO:root:current train perplexity3.6449263095855713
INFO:root:current mean train loss 1475.0680712511694
INFO:root:current train perplexity3.646148920059204
INFO:root:current mean train loss 1474.4246971145599
INFO:root:current train perplexity3.6441330909729004
INFO:root:current mean train loss 1473.7904750595508
INFO:root:current train perplexity3.6443746089935303
INFO:root:current mean train loss 1473.659887634379
INFO:root:current train perplexity3.6470868587493896
INFO:root:current mean train loss 1473.6637435420866
INFO:root:current train perplexity3.646355390548706
INFO:root:current mean train loss 1473.565726591755
INFO:root:current train perplexity3.648016929626465
INFO:root:current mean train loss 1474.1557467647303
INFO:root:current train perplexity3.646672248840332
INFO:root:current mean train loss 1474.7927976839253
INFO:root:current train perplexity3.6481027603149414
INFO:root:current mean train loss 1474.9771948701207
INFO:root:current train perplexity3.649453639984131
INFO:root:current mean train loss 1475.6891270848146
INFO:root:current train perplexity3.6498196125030518
INFO:root:current mean train loss 1475.3493886600659
INFO:root:current train perplexity3.649904489517212

100%|██████████| 1/1 [05:31<00:00, 331.58s/it][A100%|██████████| 1/1 [05:31<00:00, 331.58s/it]
INFO:root:final mean train loss: 1475.1331568269254
INFO:root:final train perplexity: 3.6508305072784424
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:24<00:00, 24.26s/it][A100%|██████████| 1/1 [00:24<00:00, 24.26s/it]
INFO:root:eval mean loss: 1816.0243430712544
INFO:root:eval perplexity: 5.15126371383667
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.43s/it][A100%|██████████| 1/1 [00:23<00:00, 23.43s/it]
INFO:root:eval mean loss: 2276.1868385693706
INFO:root:eval perplexity: 8.093278884887695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/60
 30%|███       | 60/200 [6:30:35<14:57:23, 384.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1433.2124151932567
INFO:root:current train perplexity3.59258770942688
INFO:root:current mean train loss 1461.8642178062632
INFO:root:current train perplexity3.5997486114501953
INFO:root:current mean train loss 1465.536239833048
INFO:root:current train perplexity3.602492094039917
INFO:root:current mean train loss 1462.1630312163254
INFO:root:current train perplexity3.6159422397613525
INFO:root:current mean train loss 1463.1562001813284
INFO:root:current train perplexity3.621492862701416
INFO:root:current mean train loss 1462.05989395171
INFO:root:current train perplexity3.623060703277588
INFO:root:current mean train loss 1464.4772838783572
INFO:root:current train perplexity3.624727487564087
INFO:root:current mean train loss 1465.6526500293376
INFO:root:current train perplexity3.623150110244751
INFO:root:current mean train loss 1464.5111294142057
INFO:root:current train perplexity3.621077537536621
INFO:root:current mean train loss 1466.7042794212034
INFO:root:current train perplexity3.6224617958068848
INFO:root:current mean train loss 1467.0221232375875
INFO:root:current train perplexity3.625790596008301
INFO:root:current mean train loss 1467.9284973417252
INFO:root:current train perplexity3.629289388656616
INFO:root:current mean train loss 1467.3300077267804
INFO:root:current train perplexity3.6287097930908203
INFO:root:current mean train loss 1468.389348305564
INFO:root:current train perplexity3.628882646560669
INFO:root:current mean train loss 1469.3165695265702
INFO:root:current train perplexity3.6289680004119873
INFO:root:current mean train loss 1469.1208888261706
INFO:root:current train perplexity3.630059242248535
INFO:root:current mean train loss 1470.009399866454
INFO:root:current train perplexity3.6304712295532227
INFO:root:current mean train loss 1470.0189811879636
INFO:root:current train perplexity3.6317670345306396
INFO:root:current mean train loss 1470.0306779673756
INFO:root:current train perplexity3.6315577030181885
INFO:root:current mean train loss 1469.5367311415043
INFO:root:current train perplexity3.6316683292388916

100%|██████████| 1/1 [05:41<00:00, 341.81s/it][A100%|██████████| 1/1 [05:41<00:00, 341.81s/it]
INFO:root:final mean train loss: 1469.7027008010473
INFO:root:final train perplexity: 3.6334681510925293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.66s/it][A100%|██████████| 1/1 [00:23<00:00, 23.66s/it]
INFO:root:eval mean loss: 1816.533388394836
INFO:root:eval perplexity: 5.15363073348999
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.66s/it][A100%|██████████| 1/1 [00:22<00:00, 22.66s/it]
INFO:root:eval mean loss: 2278.038392844775
INFO:root:eval perplexity: 8.107056617736816
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/61
 30%|███       | 61/200 [6:37:05<14:54:48, 386.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1455.4961513943142
INFO:root:current train perplexity3.584638833999634
INFO:root:current mean train loss 1468.2041105382582
INFO:root:current train perplexity3.616453170776367
INFO:root:current mean train loss 1464.100329382945
INFO:root:current train perplexity3.607055425643921
INFO:root:current mean train loss 1462.1504988897414
INFO:root:current train perplexity3.6070351600646973
INFO:root:current mean train loss 1462.5656371510356
INFO:root:current train perplexity3.60569167137146
INFO:root:current mean train loss 1462.242090936917
INFO:root:current train perplexity3.6086134910583496
INFO:root:current mean train loss 1462.5691294160279
INFO:root:current train perplexity3.6102046966552734
INFO:root:current mean train loss 1462.9974652165952
INFO:root:current train perplexity3.613849639892578
INFO:root:current mean train loss 1462.3963682913895
INFO:root:current train perplexity3.609002113342285
INFO:root:current mean train loss 1462.3952734531501
INFO:root:current train perplexity3.6080663204193115
INFO:root:current mean train loss 1462.6814179291596
INFO:root:current train perplexity3.609727144241333
INFO:root:current mean train loss 1463.2636865965078
INFO:root:current train perplexity3.611159086227417
INFO:root:current mean train loss 1464.3274272437234
INFO:root:current train perplexity3.6146790981292725
INFO:root:current mean train loss 1464.9498835580791
INFO:root:current train perplexity3.616128921508789
INFO:root:current mean train loss 1464.961197536968
INFO:root:current train perplexity3.614859104156494
INFO:root:current mean train loss 1465.8596134980519
INFO:root:current train perplexity3.6173882484436035
INFO:root:current mean train loss 1465.3052656924522
INFO:root:current train perplexity3.617873430252075
INFO:root:current mean train loss 1465.955828196442
INFO:root:current train perplexity3.619831085205078
INFO:root:current mean train loss 1466.5808391363273
INFO:root:current train perplexity3.620276927947998
INFO:root:current mean train loss 1466.1703416808577
INFO:root:current train perplexity3.6203622817993164

100%|██████████| 1/1 [05:36<00:00, 336.66s/it][A100%|██████████| 1/1 [05:36<00:00, 336.66s/it]
INFO:root:final mean train loss: 1465.5080247533724
INFO:root:final train perplexity: 3.6201138496398926
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.54s/it][A100%|██████████| 1/1 [00:23<00:00, 23.54s/it]
INFO:root:eval mean loss: 1823.7040569765347
INFO:root:eval perplexity: 5.187097072601318
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.42s/it][A100%|██████████| 1/1 [00:22<00:00, 22.42s/it]
INFO:root:eval mean loss: 2289.207342053136
INFO:root:eval perplexity: 8.190665245056152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/62
 31%|███       | 62/200 [6:43:30<14:47:12, 385.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1445.550905623526
INFO:root:current train perplexity3.5826988220214844
INFO:root:current mean train loss 1448.149805006638
INFO:root:current train perplexity3.5774145126342773
INFO:root:current mean train loss 1450.9170462265315
INFO:root:current train perplexity3.580453395843506
INFO:root:current mean train loss 1455.9516221173424
INFO:root:current train perplexity3.5874648094177246
INFO:root:current mean train loss 1455.8889130514451
INFO:root:current train perplexity3.592644214630127
INFO:root:current mean train loss 1457.7065710029808
INFO:root:current train perplexity3.594796895980835
INFO:root:current mean train loss 1458.5923573141988
INFO:root:current train perplexity3.595087766647339
INFO:root:current mean train loss 1457.0550601954162
INFO:root:current train perplexity3.5906331539154053
INFO:root:current mean train loss 1457.4415287496336
INFO:root:current train perplexity3.5948593616485596
INFO:root:current mean train loss 1458.5769805107636
INFO:root:current train perplexity3.5943143367767334
INFO:root:current mean train loss 1458.4547424026591
INFO:root:current train perplexity3.595454216003418
INFO:root:current mean train loss 1459.6826004597383
INFO:root:current train perplexity3.5957319736480713
INFO:root:current mean train loss 1458.6453384923059
INFO:root:current train perplexity3.599241018295288
INFO:root:current mean train loss 1459.1913729580965
INFO:root:current train perplexity3.5999772548675537
INFO:root:current mean train loss 1459.8436113792047
INFO:root:current train perplexity3.6027519702911377
INFO:root:current mean train loss 1460.160363840274
INFO:root:current train perplexity3.6036479473114014
INFO:root:current mean train loss 1459.73445763562
INFO:root:current train perplexity3.6024179458618164
INFO:root:current mean train loss 1459.6480712890625
INFO:root:current train perplexity3.602285623550415
INFO:root:current mean train loss 1460.604431514668
INFO:root:current train perplexity3.603213310241699
INFO:root:current mean train loss 1460.6505358217926
INFO:root:current train perplexity3.603994607925415

100%|██████████| 1/1 [05:40<00:00, 340.27s/it][A100%|██████████| 1/1 [05:40<00:00, 340.27s/it]
INFO:root:final mean train loss: 1460.4055088368318
INFO:root:final train perplexity: 3.6039340496063232
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.17s/it][A100%|██████████| 1/1 [00:23<00:00, 23.17s/it]
INFO:root:eval mean loss: 1822.6554400937778
INFO:root:eval perplexity: 5.182188987731934
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.86s/it][A100%|██████████| 1/1 [00:22<00:00, 22.86s/it]
INFO:root:eval mean loss: 2289.348960497701
INFO:root:eval perplexity: 8.191731452941895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/63
 32%|███▏      | 63/200 [6:49:58<14:42:31, 386.51s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1453.5816179547992
INFO:root:current train perplexity3.5663766860961914
INFO:root:current mean train loss 1455.9591825597427
INFO:root:current train perplexity3.588179111480713
INFO:root:current mean train loss 1457.168910047743
INFO:root:current train perplexity3.588047742843628
INFO:root:current mean train loss 1458.5617233688768
INFO:root:current train perplexity3.585085868835449
INFO:root:current mean train loss 1458.1523463472406
INFO:root:current train perplexity3.57861590385437
INFO:root:current mean train loss 1456.7187801963405
INFO:root:current train perplexity3.5807132720947266
INFO:root:current mean train loss 1457.4443938753498
INFO:root:current train perplexity3.580315589904785
INFO:root:current mean train loss 1454.8846337256493
INFO:root:current train perplexity3.5773990154266357
INFO:root:current mean train loss 1455.0709361810793
INFO:root:current train perplexity3.577254056930542
INFO:root:current mean train loss 1455.2421949248953
INFO:root:current train perplexity3.5811948776245117
INFO:root:current mean train loss 1454.3554470739632
INFO:root:current train perplexity3.578716278076172
INFO:root:current mean train loss 1452.3299382136418
INFO:root:current train perplexity3.5758602619171143
INFO:root:current mean train loss 1453.507641217089
INFO:root:current train perplexity3.5786378383636475
INFO:root:current mean train loss 1453.0320261711622
INFO:root:current train perplexity3.5769667625427246
INFO:root:current mean train loss 1454.2531754889455
INFO:root:current train perplexity3.5803334712982178
INFO:root:current mean train loss 1454.6147592338027
INFO:root:current train perplexity3.582468271255493
INFO:root:current mean train loss 1455.038476123924
INFO:root:current train perplexity3.5841805934906006
INFO:root:current mean train loss 1454.135777153403
INFO:root:current train perplexity3.583562135696411
INFO:root:current mean train loss 1453.9817230061415
INFO:root:current train perplexity3.5835726261138916
INFO:root:current mean train loss 1455.4923471828401
INFO:root:current train perplexity3.5867373943328857

100%|██████████| 1/1 [05:31<00:00, 331.76s/it][A100%|██████████| 1/1 [05:31<00:00, 331.76s/it]
INFO:root:final mean train loss: 1455.064709207955
INFO:root:final train perplexity: 3.5870769023895264
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.91s/it][A100%|██████████| 1/1 [00:23<00:00, 23.91s/it]
INFO:root:eval mean loss: 1825.0659300892066
INFO:root:eval perplexity: 5.193477153778076
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.99s/it][A100%|██████████| 1/1 [00:22<00:00, 22.99s/it]
INFO:root:eval mean loss: 2291.4694503892397
INFO:root:eval perplexity: 8.2077054977417
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/64
 32%|███▏      | 64/200 [6:56:19<14:32:06, 384.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1451.9663240279274
INFO:root:current train perplexity3.563875198364258
INFO:root:current mean train loss 1439.7829681233288
INFO:root:current train perplexity3.5517003536224365
INFO:root:current mean train loss 1437.9558956132948
INFO:root:current train perplexity3.5386593341827393
INFO:root:current mean train loss 1439.316033099665
INFO:root:current train perplexity3.540482997894287
INFO:root:current mean train loss 1444.0467007928805
INFO:root:current train perplexity3.5450968742370605
INFO:root:current mean train loss 1444.238735634383
INFO:root:current train perplexity3.545445203781128
INFO:root:current mean train loss 1446.3131373598412
INFO:root:current train perplexity3.5493154525756836
INFO:root:current mean train loss 1445.8628204229371
INFO:root:current train perplexity3.55057692527771
INFO:root:current mean train loss 1448.074220263837
INFO:root:current train perplexity3.556814193725586
INFO:root:current mean train loss 1448.6132294288643
INFO:root:current train perplexity3.558135986328125
INFO:root:current mean train loss 1448.3206792724384
INFO:root:current train perplexity3.5599212646484375
INFO:root:current mean train loss 1449.0964028439605
INFO:root:current train perplexity3.5613555908203125
INFO:root:current mean train loss 1449.4057326950394
INFO:root:current train perplexity3.5646274089813232
INFO:root:current mean train loss 1449.9313380455233
INFO:root:current train perplexity3.5677716732025146
INFO:root:current mean train loss 1449.3547332086416
INFO:root:current train perplexity3.567652463912964
INFO:root:current mean train loss 1449.34342847895
INFO:root:current train perplexity3.566300630569458
INFO:root:current mean train loss 1449.0016562343703
INFO:root:current train perplexity3.5674521923065186
INFO:root:current mean train loss 1450.0522387162493
INFO:root:current train perplexity3.568718910217285
INFO:root:current mean train loss 1450.3472156453904
INFO:root:current train perplexity3.5706026554107666

100%|██████████| 1/1 [05:32<00:00, 332.45s/it][A100%|██████████| 1/1 [05:32<00:00, 332.45s/it]
INFO:root:final mean train loss: 1450.585202307997
INFO:root:final train perplexity: 3.572998523712158
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:23<00:00, 23.75s/it][A100%|██████████| 1/1 [00:23<00:00, 23.75s/it]
INFO:root:eval mean loss: 1825.8062895646333
INFO:root:eval perplexity: 5.196948528289795
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:22<00:00, 22.14s/it][A100%|██████████| 1/1 [00:22<00:00, 22.14s/it]
INFO:root:eval mean loss: 2294.875883061835
INFO:root:eval perplexity: 8.233429908752441
INFO:root:evalaution complete
INFO:root:checkpoint. save model: large_mpnet_gpt2_final/65
 32%|███▎      | 65/200 [7:02:39<14:22:42, 383.43s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1433.02587890625
INFO:root:current train perplexity3.412452220916748
INFO:root:current mean train loss 1422.935045682467
INFO:root:current train perplexity3.5221574306488037
INFO:root:current mean train loss 1427.707556631051
INFO:root:current train perplexity3.518495559692383
INFO:root:current mean train loss 1433.9923742193926
INFO:root:current train perplexity3.5242936611175537
INFO:root:current mean train loss 1432.4237133063893
INFO:root:current train perplexity3.525315999984741
INFO:root:current mean train loss 1435.322971404545
INFO:root:current train perplexity3.5303218364715576
INFO:root:current mean train loss 1436.78604792917
INFO:root:current train perplexity3.5287017822265625
INFO:root:current mean train loss 1439.501525185325
INFO:root:current train perplexity3.528766632080078
INFO:root:current mean train loss 1440.2766606724676
INFO:root:current train perplexity3.532042980194092
INFO:root:current mean train loss 1442.512819273282
INFO:root:current train perplexity3.5389297008514404
INFO:root:current mean train loss 1441.7857606439477
INFO:root:current train perplexity3.542492151260376
INFO:root:current mean train loss 1441.9069897195568
INFO:root:current train perplexity3.5444276332855225
slurmstepd: error: *** JOB 26260774 ON ga018 CANCELLED AT 2022-10-25T09:40:20 ***
