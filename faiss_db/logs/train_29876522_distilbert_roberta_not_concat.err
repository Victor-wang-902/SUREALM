INFO:root:Output: distilbert_roberta_not_concat
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
/scratch/zw2374/public/faiss_db/models_roberta.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModelRoberta were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models_roberta.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 10722.48199771149
INFO:root:current train perplexity4725.1982421875
INFO:root:current mean train loss 9264.984956520886
INFO:root:current train perplexity1507.8779296875
INFO:root:current mean train loss 8625.075807705372
INFO:root:current train perplexity913.2388916015625
INFO:root:current mean train loss 8251.246032561874
INFO:root:current train perplexity674.13671875
INFO:root:current mean train loss 7915.023702678795
INFO:root:current train perplexity522.8300170898438
INFO:root:current mean train loss 7606.269964100324
INFO:root:current train perplexity406.402587890625
INFO:root:current mean train loss 7289.921712938126
INFO:root:current train perplexity316.16619873046875
INFO:root:current mean train loss 6982.4491341105095
INFO:root:current train perplexity248.34339904785156
INFO:root:current mean train loss 6702.302086954255
INFO:root:current train perplexity199.5443878173828
INFO:root:current mean train loss 6461.675124587478
INFO:root:current train perplexity164.0081329345703
INFO:root:current mean train loss 6237.683760360982
INFO:root:current train perplexity137.69895935058594
INFO:root:current mean train loss 6038.028138276038
INFO:root:current train perplexity117.43694305419922
INFO:root:current mean train loss 5859.628371170432
INFO:root:current train perplexity101.89786529541016
INFO:root:current mean train loss 5694.4414837328
INFO:root:current train perplexity89.45893096923828
INFO:root:current mean train loss 5547.259631909555
INFO:root:current train perplexity79.53345489501953
INFO:root:current mean train loss 5411.133491940764
INFO:root:current train perplexity71.40734100341797
INFO:root:current mean train loss 5286.190541196108
INFO:root:current train perplexity64.64369201660156
INFO:root:current mean train loss 5170.3928727494
INFO:root:current train perplexity59.01396179199219
INFO:root:current mean train loss 5064.099532237362
INFO:root:current train perplexity54.291419982910156

100%|██████████| 1/1 [08:07<00:00, 487.97s/it][A100%|██████████| 1/1 [08:07<00:00, 487.97s/it]
INFO:root:final mean train loss: 4978.404979680802
INFO:root:final train perplexity: 50.853363037109375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.43s/it][A100%|██████████| 1/1 [00:40<00:00, 40.43s/it]
INFO:root:eval mean loss: 2773.440260001108
INFO:root:eval perplexity: 9.433891296386719
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.58s/it][A100%|██████████| 1/1 [00:39<00:00, 39.58s/it]
INFO:root:eval mean loss: 3086.1650654677806
INFO:root:eval perplexity: 12.644527435302734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/1
  0%|          | 1/200 [09:29<31:29:49, 569.79s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3028.7389068603516
INFO:root:current train perplexity11.175212860107422
INFO:root:current mean train loss 3006.3411233836205
INFO:root:current train perplexity10.813607215881348
INFO:root:current mean train loss 2984.209645589193
INFO:root:current train perplexity10.598556518554688
INFO:root:current mean train loss 2980.495684273635
INFO:root:current train perplexity10.490832328796387
INFO:root:current mean train loss 2962.209367605356
INFO:root:current train perplexity10.328804969787598
INFO:root:current mean train loss 2939.156470483588
INFO:root:current train perplexity10.182612419128418
INFO:root:current mean train loss 2917.865132913961
INFO:root:current train perplexity10.028276443481445
INFO:root:current mean train loss 2896.8686785990967
INFO:root:current train perplexity9.86949348449707
INFO:root:current mean train loss 2878.888499540441
INFO:root:current train perplexity9.733784675598145
INFO:root:current mean train loss 2862.8400505765558
INFO:root:current train perplexity9.601282119750977
INFO:root:current mean train loss 2846.698952742449
INFO:root:current train perplexity9.481207847595215
INFO:root:current mean train loss 2834.766879611545
INFO:root:current train perplexity9.367227554321289
INFO:root:current mean train loss 2821.464260302092
INFO:root:current train perplexity9.263958930969238
INFO:root:current mean train loss 2807.3400275975373
INFO:root:current train perplexity9.163350105285645
INFO:root:current mean train loss 2793.9852879411083
INFO:root:current train perplexity9.071488380432129
INFO:root:current mean train loss 2782.3128707201313
INFO:root:current train perplexity8.984660148620605
INFO:root:current mean train loss 2771.3754091168394
INFO:root:current train perplexity8.900833129882812
INFO:root:current mean train loss 2760.2411887428975
INFO:root:current train perplexity8.816987991333008
INFO:root:current mean train loss 2748.3254734661086
INFO:root:current train perplexity8.741745948791504
INFO:root:current mean train loss 2737.9068969216873
INFO:root:current train perplexity8.665684700012207

100%|██████████| 1/1 [08:14<00:00, 494.26s/it][A100%|██████████| 1/1 [08:14<00:00, 494.26s/it]
INFO:root:final mean train loss: 2729.1904118971215
INFO:root:final train perplexity: 8.618157386779785
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.41s/it][A100%|██████████| 1/1 [00:41<00:00, 41.41s/it]
INFO:root:eval mean loss: 2318.670865539118
INFO:root:eval perplexity: 6.529318809509277
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.14s/it][A100%|██████████| 1/1 [00:39<00:00, 39.14s/it]
INFO:root:eval mean loss: 2658.1893544298537
INFO:root:eval perplexity: 8.893972396850586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/2
  1%|          | 2/200 [19:07<31:36:15, 574.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2480.563839074337
INFO:root:current train perplexity7.23018217086792
INFO:root:current mean train loss 2481.0563928130873
INFO:root:current train perplexity7.108712196350098
INFO:root:current mean train loss 2472.650609618093
INFO:root:current train perplexity7.048740863800049
INFO:root:current mean train loss 2468.4567995730104
INFO:root:current train perplexity7.003950119018555
INFO:root:current mean train loss 2466.7844181897735
INFO:root:current train perplexity6.987117290496826
INFO:root:current mean train loss 2459.568085690153
INFO:root:current train perplexity6.963796138763428
INFO:root:current mean train loss 2455.305169995927
INFO:root:current train perplexity6.944830417633057
INFO:root:current mean train loss 2451.0012227015477
INFO:root:current train perplexity6.924536228179932
INFO:root:current mean train loss 2447.9073871736196
INFO:root:current train perplexity6.891877174377441
INFO:root:current mean train loss 2442.6395992430334
INFO:root:current train perplexity6.858897686004639
INFO:root:current mean train loss 2436.129392404178
INFO:root:current train perplexity6.827131748199463
INFO:root:current mean train loss 2430.874223943106
INFO:root:current train perplexity6.793128967285156
INFO:root:current mean train loss 2426.724468296172
INFO:root:current train perplexity6.76915979385376
INFO:root:current mean train loss 2421.2034668518204
INFO:root:current train perplexity6.751607418060303
INFO:root:current mean train loss 2415.1339277439156
INFO:root:current train perplexity6.724185466766357
INFO:root:current mean train loss 2410.269097912335
INFO:root:current train perplexity6.697526454925537
INFO:root:current mean train loss 2406.666911978409
INFO:root:current train perplexity6.674955368041992
INFO:root:current mean train loss 2400.630011997124
INFO:root:current train perplexity6.647073745727539
INFO:root:current mean train loss 2395.8387985937075
INFO:root:current train perplexity6.621192455291748
INFO:root:current mean train loss 2391.5803452524815
INFO:root:current train perplexity6.5993804931640625

100%|██████████| 1/1 [07:51<00:00, 471.00s/it][A100%|██████████| 1/1 [07:51<00:00, 471.00s/it]
INFO:root:final mean train loss: 2388.699971055239
INFO:root:final train perplexity: 6.5873870849609375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.80s/it][A100%|██████████| 1/1 [00:41<00:00, 41.80s/it]
INFO:root:eval mean loss: 2143.3908492284463
INFO:root:eval perplexity: 5.665886402130127
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 2495.789507493905
INFO:root:eval perplexity: 7.782363414764404
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/3
  2%|▏         | 3/200 [28:21<30:55:29, 565.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2261.5551440429685
INFO:root:current train perplexity6.0100507736206055
INFO:root:current mean train loss 2269.766493326823
INFO:root:current train perplexity5.9833664894104
INFO:root:current mean train loss 2257.915352050781
INFO:root:current train perplexity5.9288554191589355
INFO:root:current mean train loss 2257.4285107421874
INFO:root:current train perplexity5.946034908294678
INFO:root:current mean train loss 2266.9682370334203
INFO:root:current train perplexity5.963745594024658
INFO:root:current mean train loss 2264.7245003995026
INFO:root:current train perplexity5.947020530700684
INFO:root:current mean train loss 2259.0409183443508
INFO:root:current train perplexity5.930121898651123
INFO:root:current mean train loss 2254.319348470052
INFO:root:current train perplexity5.912262439727783
INFO:root:current mean train loss 2247.8366175034466
INFO:root:current train perplexity5.8912553787231445
INFO:root:current mean train loss 2247.648695132607
INFO:root:current train perplexity5.891067028045654
INFO:root:current mean train loss 2247.421697126116
INFO:root:current train perplexity5.890042304992676
INFO:root:current mean train loss 2243.946970851732
INFO:root:current train perplexity5.887328624725342
INFO:root:current mean train loss 2242.413505859375
INFO:root:current train perplexity5.886445999145508
INFO:root:current mean train loss 2242.178516619647
INFO:root:current train perplexity5.881239891052246
INFO:root:current mean train loss 2242.7238094356144
INFO:root:current train perplexity5.886574745178223
INFO:root:current mean train loss 2242.6920664535032
INFO:root:current train perplexity5.879875659942627
INFO:root:current mean train loss 2241.8978955817947
INFO:root:current train perplexity5.873161315917969
INFO:root:current mean train loss 2243.2667077985493
INFO:root:current train perplexity5.873661994934082
INFO:root:current mean train loss 2242.8736924619934
INFO:root:current train perplexity5.868986129760742
INFO:root:current mean train loss 2241.404169921875
INFO:root:current train perplexity5.861847877502441

100%|██████████| 1/1 [07:49<00:00, 469.91s/it][A100%|██████████| 1/1 [07:49<00:00, 469.91s/it]
INFO:root:final mean train loss: 2240.6541310674424
INFO:root:final train perplexity: 5.860994338989258
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.28s/it][A100%|██████████| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 2064.710947888963
INFO:root:eval perplexity: 5.316390037536621
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.42s/it][A100%|██████████| 1/1 [00:39<00:00, 39.42s/it]
INFO:root:eval mean loss: 2430.4411404657026
INFO:root:eval perplexity: 7.375290870666504
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/4
  2%|▏         | 4/200 [37:35<30:31:49, 560.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2153.6937201201026
INFO:root:current train perplexity5.449929237365723
INFO:root:current mean train loss 2184.108641847165
INFO:root:current train perplexity5.523375988006592
INFO:root:current mean train loss 2187.056672628453
INFO:root:current train perplexity5.5513691902160645
INFO:root:current mean train loss 2180.959155007344
INFO:root:current train perplexity5.5432047843933105
INFO:root:current mean train loss 2181.346114295453
INFO:root:current train perplexity5.558915138244629
INFO:root:current mean train loss 2182.160643024209
INFO:root:current train perplexity5.575647354125977
INFO:root:current mean train loss 2184.3146857357456
INFO:root:current train perplexity5.59244441986084
INFO:root:current mean train loss 2187.0065510537197
INFO:root:current train perplexity5.610774517059326
INFO:root:current mean train loss 2188.2708758537883
INFO:root:current train perplexity5.621832847595215
INFO:root:current mean train loss 2188.7887312564635
INFO:root:current train perplexity5.622548580169678
INFO:root:current mean train loss 2188.6458448501203
INFO:root:current train perplexity5.619972229003906
INFO:root:current mean train loss 2189.482093425316
INFO:root:current train perplexity5.620673656463623
INFO:root:current mean train loss 2187.616535816027
INFO:root:current train perplexity5.617269515991211
INFO:root:current mean train loss 2188.123055447604
INFO:root:current train perplexity5.617016315460205
INFO:root:current mean train loss 2187.677424414595
INFO:root:current train perplexity5.6176981925964355
INFO:root:current mean train loss 2187.509223670205
INFO:root:current train perplexity5.618223190307617
INFO:root:current mean train loss 2187.0788782917243
INFO:root:current train perplexity5.6142354011535645
INFO:root:current mean train loss 2187.5748740748354
INFO:root:current train perplexity5.612869739532471
INFO:root:current mean train loss 2186.003875650693
INFO:root:current train perplexity5.609152793884277
INFO:root:current mean train loss 2184.0246927080025
INFO:root:current train perplexity5.603212356567383

100%|██████████| 1/1 [07:36<00:00, 456.96s/it][A100%|██████████| 1/1 [07:36<00:00, 456.96s/it]
INFO:root:final mean train loss: 2183.260218325493
INFO:root:final train perplexity: 5.601442813873291
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.34s/it][A100%|██████████| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 2035.6384610656305
INFO:root:eval perplexity: 5.192775726318359
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.75s/it][A100%|██████████| 1/1 [00:38<00:00, 38.75s/it]
INFO:root:eval mean loss: 2402.536272630624
INFO:root:eval perplexity: 7.208017826080322
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/5
  2%|▎         | 5/200 [46:34<29:56:26, 552.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2092.9728030250185
INFO:root:current train perplexity5.282959938049316
INFO:root:current mean train loss 2121.0268494979196
INFO:root:current train perplexity5.337888717651367
INFO:root:current mean train loss 2117.5697988590723
INFO:root:current train perplexity5.324730396270752
INFO:root:current mean train loss 2116.2486079533896
INFO:root:current train perplexity5.331523418426514
INFO:root:current mean train loss 2123.919331448137
INFO:root:current train perplexity5.356204986572266
INFO:root:current mean train loss 2126.1810499217413
INFO:root:current train perplexity5.373581409454346
INFO:root:current mean train loss 2129.4508859734788
INFO:root:current train perplexity5.386718273162842
INFO:root:current mean train loss 2131.6085178608796
INFO:root:current train perplexity5.384711265563965
INFO:root:current mean train loss 2134.4176391325386
INFO:root:current train perplexity5.397409915924072
INFO:root:current mean train loss 2133.4328146833714
INFO:root:current train perplexity5.391897678375244
INFO:root:current mean train loss 2133.5194329406063
INFO:root:current train perplexity5.385761737823486
INFO:root:current mean train loss 2136.6785252545333
INFO:root:current train perplexity5.397499084472656
INFO:root:current mean train loss 2133.970156945915
INFO:root:current train perplexity5.393106460571289
INFO:root:current mean train loss 2132.899983135951
INFO:root:current train perplexity5.391918182373047
INFO:root:current mean train loss 2134.890424702688
INFO:root:current train perplexity5.3935723304748535
INFO:root:current mean train loss 2134.823061393969
INFO:root:current train perplexity5.390501976013184
INFO:root:current mean train loss 2134.451992660124
INFO:root:current train perplexity5.389178276062012
INFO:root:current mean train loss 2134.289633438726
INFO:root:current train perplexity5.388814926147461
INFO:root:current mean train loss 2133.435431931935
INFO:root:current train perplexity5.383615016937256

100%|██████████| 1/1 [07:44<00:00, 464.79s/it][A100%|██████████| 1/1 [07:44<00:00, 464.79s/it]
INFO:root:final mean train loss: 2131.3288118070986
INFO:root:final train perplexity: 5.376513481140137
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.33s/it][A100%|██████████| 1/1 [00:41<00:00, 41.33s/it]
INFO:root:eval mean loss: 2009.0055425116357
INFO:root:eval perplexity: 5.082059860229492
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.13s/it][A100%|██████████| 1/1 [00:39<00:00, 39.14s/it]
INFO:root:eval mean loss: 2388.596571469138
INFO:root:eval perplexity: 7.125882148742676
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/6
  3%|▎         | 6/200 [55:41<29:41:41, 551.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2001.525390625
INFO:root:current train perplexity5.207303524017334
INFO:root:current mean train loss 2077.7931162012687
INFO:root:current train perplexity5.1558074951171875
INFO:root:current mean train loss 2082.0814202911224
INFO:root:current train perplexity5.177170753479004
INFO:root:current mean train loss 2088.0604811760277
INFO:root:current train perplexity5.175053119659424
INFO:root:current mean train loss 2087.2850542710607
INFO:root:current train perplexity5.18535852432251
INFO:root:current mean train loss 2089.0583162288704
INFO:root:current train perplexity5.181185245513916
INFO:root:current mean train loss 2088.7749407319184
INFO:root:current train perplexity5.178297519683838
INFO:root:current mean train loss 2088.024477448511
INFO:root:current train perplexity5.170014381408691
INFO:root:current mean train loss 2088.592269154524
INFO:root:current train perplexity5.17095422744751
INFO:root:current mean train loss 2086.8440813917696
INFO:root:current train perplexity5.168776035308838
INFO:root:current mean train loss 2086.0200525792566
INFO:root:current train perplexity5.164761066436768
INFO:root:current mean train loss 2083.7256107728767
INFO:root:current train perplexity5.160140037536621
INFO:root:current mean train loss 2078.9392559423136
INFO:root:current train perplexity5.15204381942749
INFO:root:current mean train loss 2077.95379268051
INFO:root:current train perplexity5.14998722076416
INFO:root:current mean train loss 2077.7927602458903
INFO:root:current train perplexity5.145887851715088
INFO:root:current mean train loss 2075.6473275628746
INFO:root:current train perplexity5.140279769897461
INFO:root:current mean train loss 2075.8456546323587
INFO:root:current train perplexity5.140364646911621
INFO:root:current mean train loss 2076.375819830247
INFO:root:current train perplexity5.142152786254883
INFO:root:current mean train loss 2076.361205580233
INFO:root:current train perplexity5.139841556549072
INFO:root:current mean train loss 2075.147451177012
INFO:root:current train perplexity5.137611389160156

100%|██████████| 1/1 [07:48<00:00, 468.69s/it][A100%|██████████| 1/1 [07:48<00:00, 468.69s/it]
INFO:root:final mean train loss: 2074.53032856305
INFO:root:final train perplexity: 5.14083194732666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.90s/it][A100%|██████████| 1/1 [00:41<00:00, 41.90s/it]
INFO:root:eval mean loss: 1978.0215410814217
INFO:root:eval perplexity: 4.956223964691162
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.56s/it][A100%|██████████| 1/1 [00:39<00:00, 39.56s/it]
INFO:root:eval mean loss: 2356.4933424063606
INFO:root:eval perplexity: 6.94027042388916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/7
  4%|▎         | 7/200 [1:04:54<29:34:16, 551.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1992.450690375434
INFO:root:current train perplexity4.835044860839844
INFO:root:current mean train loss 2014.7253469693458
INFO:root:current train perplexity4.940427780151367
INFO:root:current mean train loss 2016.7139825383458
INFO:root:current train perplexity4.9432573318481445
INFO:root:current mean train loss 2020.6175130208333
INFO:root:current train perplexity4.9465837478637695
INFO:root:current mean train loss 2030.2676102487665
INFO:root:current train perplexity4.953307628631592
INFO:root:current mean train loss 2027.7342713109315
INFO:root:current train perplexity4.944201469421387
INFO:root:current mean train loss 2029.5324758387692
INFO:root:current train perplexity4.945845127105713
INFO:root:current mean train loss 2028.7389097930993
INFO:root:current train perplexity4.944341659545898
INFO:root:current mean train loss 2029.350184060542
INFO:root:current train perplexity4.945204257965088
INFO:root:current mean train loss 2031.0565407613783
INFO:root:current train perplexity4.948546409606934
INFO:root:current mean train loss 2032.6655194295647
INFO:root:current train perplexity4.953856945037842
INFO:root:current mean train loss 2033.7887372339349
INFO:root:current train perplexity4.960803031921387
INFO:root:current mean train loss 2033.317614024496
INFO:root:current train perplexity4.96236515045166
INFO:root:current mean train loss 2033.6547117103032
INFO:root:current train perplexity4.965114593505859
INFO:root:current mean train loss 2033.0070611391484
INFO:root:current train perplexity4.964837074279785
INFO:root:current mean train loss 2033.3925356657608
INFO:root:current train perplexity4.9690046310424805
INFO:root:current mean train loss 2033.1082589393493
INFO:root:current train perplexity4.966315746307373
INFO:root:current mean train loss 2032.3766731022401
INFO:root:current train perplexity4.966588020324707
INFO:root:current mean train loss 2031.5628688296076
INFO:root:current train perplexity4.964107990264893
INFO:root:current mean train loss 2032.0713292858773
INFO:root:current train perplexity4.966816425323486

100%|██████████| 1/1 [08:01<00:00, 481.99s/it][A100%|██████████| 1/1 [08:01<00:00, 481.99s/it]
INFO:root:final mean train loss: 2031.077305903894
INFO:root:final train perplexity: 4.967525005340576
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.21s/it][A100%|██████████| 1/1 [00:42<00:00, 42.21s/it]
INFO:root:eval mean loss: 1970.6696409401318
INFO:root:eval perplexity: 4.926825523376465
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.33s/it][A100%|██████████| 1/1 [00:38<00:00, 38.33s/it]
INFO:root:eval mean loss: 2357.080497146498
INFO:root:eval perplexity: 6.9436211585998535
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/8
  4%|▍         | 8/200 [1:14:19<29:38:40, 555.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1996.7178850446428
INFO:root:current train perplexity4.79579496383667
INFO:root:current mean train loss 1999.6334780092593
INFO:root:current train perplexity4.809810161590576
INFO:root:current mean train loss 1991.1527899559508
INFO:root:current train perplexity4.797623157501221
INFO:root:current mean train loss 1989.6849274137128
INFO:root:current train perplexity4.803744792938232
INFO:root:current mean train loss 1992.3622308840697
INFO:root:current train perplexity4.820870399475098
INFO:root:current mean train loss 1991.3894729756864
INFO:root:current train perplexity4.827956676483154
INFO:root:current mean train loss 1995.7020369402067
INFO:root:current train perplexity4.839320659637451
INFO:root:current mean train loss 1993.5738002232142
INFO:root:current train perplexity4.833601474761963
INFO:root:current mean train loss 1996.6004761473148
INFO:root:current train perplexity4.850602149963379
INFO:root:current mean train loss 1999.8231554979945
INFO:root:current train perplexity4.857086181640625
INFO:root:current mean train loss 1999.75971856601
INFO:root:current train perplexity4.857886791229248
INFO:root:current mean train loss 2004.49784812087
INFO:root:current train perplexity4.8665900230407715
INFO:root:current mean train loss 2003.088859477796
INFO:root:current train perplexity4.863533973693848
INFO:root:current mean train loss 2003.8918753291784
INFO:root:current train perplexity4.863539695739746
INFO:root:current mean train loss 2004.4630474024118
INFO:root:current train perplexity4.865359783172607
INFO:root:current mean train loss 2004.2911201203685
INFO:root:current train perplexity4.865210056304932
INFO:root:current mean train loss 2003.216064154482
INFO:root:current train perplexity4.863927364349365
INFO:root:current mean train loss 2003.6568691462537
INFO:root:current train perplexity4.864706039428711
INFO:root:current mean train loss 2004.3980005082383
INFO:root:current train perplexity4.863784313201904
INFO:root:current mean train loss 2004.7741567370194
INFO:root:current train perplexity4.864317893981934

100%|██████████| 1/1 [07:53<00:00, 473.43s/it][A100%|██████████| 1/1 [07:53<00:00, 473.43s/it]
INFO:root:final mean train loss: 2004.1996832081481
INFO:root:final train perplexity: 4.863264560699463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.43s/it][A100%|██████████| 1/1 [00:40<00:00, 40.43s/it]
INFO:root:eval mean loss: 1943.2372094553414
INFO:root:eval perplexity: 4.818660736083984
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.42s/it][A100%|██████████| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 2341.7100107179467
INFO:root:eval perplexity: 6.856430530548096
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/9
  4%|▍         | 9/200 [1:23:34<29:28:18, 555.49s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1978.4945608285757
INFO:root:current train perplexity4.746902942657471
INFO:root:current mean train loss 1985.1877465499074
INFO:root:current train perplexity4.748772144317627
INFO:root:current mean train loss 1983.6979244171628
INFO:root:current train perplexity4.75830078125
INFO:root:current mean train loss 1982.564940365878
INFO:root:current train perplexity4.760365009307861
INFO:root:current mean train loss 1981.1104855157632
INFO:root:current train perplexity4.765856742858887
INFO:root:current mean train loss 1980.965956757034
INFO:root:current train perplexity4.766122341156006
INFO:root:current mean train loss 1979.417409885149
INFO:root:current train perplexity4.765543460845947
INFO:root:current mean train loss 1980.2381239546107
INFO:root:current train perplexity4.7656636238098145
INFO:root:current mean train loss 1979.6631873762103
INFO:root:current train perplexity4.767516136169434
INFO:root:current mean train loss 1979.1346003428228
INFO:root:current train perplexity4.766022682189941
INFO:root:current mean train loss 2082.012160500646
INFO:root:current train perplexity5.1709136962890625
INFO:root:current mean train loss 2200.0049680074058
INFO:root:current train perplexity5.672576427459717
INFO:root:current mean train loss 2194.669927432514
INFO:root:current train perplexity5.6541829109191895
INFO:root:current mean train loss 2179.7336009550377
INFO:root:current train perplexity5.5906853675842285
INFO:root:current mean train loss 2167.559378295562
INFO:root:current train perplexity5.537539958953857
INFO:root:current mean train loss 2184.6706984215175
INFO:root:current train perplexity5.613306522369385
INFO:root:current mean train loss 2178.4227662906233
INFO:root:current train perplexity5.581684112548828
INFO:root:current mean train loss 2170.7982440408505
INFO:root:current train perplexity5.549384117126465
INFO:root:current mean train loss 2177.6909842110094
INFO:root:current train perplexity5.578097343444824
INFO:root:current mean train loss 2170.6306338075733
INFO:root:current train perplexity5.544816017150879

100%|██████████| 1/1 [07:51<00:00, 471.90s/it][A100%|██████████| 1/1 [07:51<00:00, 471.91s/it]
INFO:root:final mean train loss: 2167.239207088376
INFO:root:final train perplexity: 5.531065940856934
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.71s/it][A100%|██████████| 1/1 [00:40<00:00, 40.71s/it]
INFO:root:eval mean loss: 1943.514282659436
INFO:root:eval perplexity: 4.819741249084473
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.45s/it][A100%|██████████| 1/1 [00:38<00:00, 38.45s/it]
INFO:root:eval mean loss: 2345.5450543342754
INFO:root:eval perplexity: 6.878082752227783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/10
  5%|▌         | 10/200 [1:32:47<29:17:03, 554.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2374.3549150107565
INFO:root:current train perplexity6.4418182373046875
INFO:root:current mean train loss 2168.0612959099944
INFO:root:current train perplexity5.5184550285339355
INFO:root:current mean train loss 2156.4213676594445
INFO:root:current train perplexity5.4711384773254395
INFO:root:current mean train loss 2221.9011974799923
INFO:root:current train perplexity5.771740436553955
INFO:root:current mean train loss 2220.3479326650786
INFO:root:current train perplexity5.73914909362793
INFO:root:current mean train loss 2186.8686465513097
INFO:root:current train perplexity5.597568988800049
INFO:root:current mean train loss 2163.722035497828
INFO:root:current train perplexity5.500836372375488
INFO:root:current mean train loss 2143.5404646418338
INFO:root:current train perplexity5.4204792976379395
INFO:root:current mean train loss 2129.8622306586685
INFO:root:current train perplexity5.357620716094971
INFO:root:current mean train loss 2118.7936199176424
INFO:root:current train perplexity5.3072357177734375
INFO:root:current mean train loss 2107.207493038909
INFO:root:current train perplexity5.261014938354492
INFO:root:current mean train loss 2098.182359054213
INFO:root:current train perplexity5.232194423675537
INFO:root:current mean train loss 2089.5745347476177
INFO:root:current train perplexity5.202791690826416
INFO:root:current mean train loss 2081.1316572458227
INFO:root:current train perplexity5.1725850105285645
INFO:root:current mean train loss 2075.662226874947
INFO:root:current train perplexity5.147921085357666
INFO:root:current mean train loss 2072.226366674011
INFO:root:current train perplexity5.127521991729736
INFO:root:current mean train loss 2068.8030037488534
INFO:root:current train perplexity5.112250328063965
INFO:root:current mean train loss 2062.9475978163423
INFO:root:current train perplexity5.0915446281433105
INFO:root:current mean train loss 2057.8097397348597
INFO:root:current train perplexity5.073479652404785
INFO:root:current mean train loss 2054.3798448088933
INFO:root:current train perplexity5.058977127075195

100%|██████████| 1/1 [07:39<00:00, 459.60s/it][A100%|██████████| 1/1 [07:39<00:00, 459.60s/it]
INFO:root:final mean train loss: 2053.689752729746
INFO:root:final train perplexity: 5.056969165802002
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.38s/it][A100%|██████████| 1/1 [00:42<00:00, 42.38s/it]
INFO:root:eval mean loss: 1944.2515713306184
INFO:root:eval perplexity: 4.822618007659912
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.80s/it][A100%|██████████| 1/1 [00:38<00:00, 38.80s/it]
INFO:root:eval mean loss: 2352.423650646886
INFO:root:eval perplexity: 6.917088985443115
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/11
  6%|▌         | 11/200 [1:41:51<28:56:41, 551.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1942.4775504178779
INFO:root:current train perplexity4.655539512634277
INFO:root:current mean train loss 1956.01169577978
INFO:root:current train perplexity4.690249919891357
INFO:root:current mean train loss 1952.6342410641116
INFO:root:current train perplexity4.676791191101074
INFO:root:current mean train loss 1944.4729845116176
INFO:root:current train perplexity4.6538286209106445
INFO:root:current mean train loss 1971.4419025688014
INFO:root:current train perplexity4.744283199310303
INFO:root:current mean train loss 3066.5674557213897
INFO:root:current train perplexity11.254755020141602
INFO:root:current mean train loss 3653.5916000677616
INFO:root:current train perplexity17.844694137573242
INFO:root:current mean train loss 4056.320040714953
INFO:root:current train perplexity24.475242614746094
INFO:root:current mean train loss 4372.255343262821
INFO:root:current train perplexity31.432355880737305
INFO:root:current mean train loss 4595.695636617726
INFO:root:current train perplexity37.567665100097656
INFO:root:current mean train loss 4778.305290657732
INFO:root:current train perplexity43.35598373413086
INFO:root:current mean train loss 4967.508560360785
INFO:root:current train perplexity50.25548553466797
INFO:root:current mean train loss 5109.321031822572
INFO:root:current train perplexity56.285770416259766
INFO:root:current mean train loss 5227.817140257203
INFO:root:current train perplexity62.008628845214844
INFO:root:current mean train loss 5335.34123646876
INFO:root:current train perplexity67.55644989013672
INFO:root:current mean train loss 5433.509702973528
INFO:root:current train perplexity72.91349792480469
INFO:root:current mean train loss 5523.150326766153
INFO:root:current train perplexity78.05560302734375
INFO:root:current mean train loss 5587.176432063839
INFO:root:current train perplexity82.19142150878906
INFO:root:current mean train loss 5647.598013140617
INFO:root:current train perplexity86.20621490478516

100%|██████████| 1/1 [07:51<00:00, 471.70s/it][A100%|██████████| 1/1 [07:51<00:00, 471.70s/it]
INFO:root:final mean train loss: 5698.635634214062
INFO:root:final train perplexity: 89.77900695800781
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.87s/it][A100%|██████████| 1/1 [00:40<00:00, 40.87s/it]
INFO:root:eval mean loss: 6452.064451393506
INFO:root:eval perplexity: 185.13876342773438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.04s/it][A100%|██████████| 1/1 [00:39<00:00, 39.04s/it]
INFO:root:eval mean loss: 6586.882235912566
INFO:root:eval perplexity: 224.81202697753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/12
  6%|▌         | 12/200 [1:51:05<28:50:03, 552.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6844.050944010417
INFO:root:current train perplexity192.080322265625
INFO:root:current mean train loss 6756.664181014867
INFO:root:current train perplexity204.6927032470703
INFO:root:current mean train loss 6740.6943142895625
INFO:root:current train perplexity206.0918426513672
INFO:root:current mean train loss 6781.208078718028
INFO:root:current train perplexity209.5125732421875
INFO:root:current mean train loss 6773.572160214408
INFO:root:current train perplexity209.91107177734375
INFO:root:current mean train loss 6784.870507424204
INFO:root:current train perplexity211.2732391357422
INFO:root:current mean train loss 6792.02587161847
INFO:root:current train perplexity212.13645935058594
INFO:root:current mean train loss 6796.046851384691
INFO:root:current train perplexity212.06008911132812
INFO:root:current mean train loss 6792.3336864200655
INFO:root:current train perplexity211.18057250976562
INFO:root:current mean train loss 6787.3254537825305
INFO:root:current train perplexity210.67617797851562
INFO:root:current mean train loss 6789.132228315055
INFO:root:current train perplexity211.24765014648438
INFO:root:current mean train loss 6780.191106552442
INFO:root:current train perplexity210.13429260253906
INFO:root:current mean train loss 6770.171081086347
INFO:root:current train perplexity209.07237243652344
INFO:root:current mean train loss 6771.96215089577
INFO:root:current train perplexity209.4662322998047
INFO:root:current mean train loss 6767.125754869588
INFO:root:current train perplexity208.87107849121094
INFO:root:current mean train loss 6766.747179469186
INFO:root:current train perplexity208.6152801513672
INFO:root:current mean train loss 6769.66442162888
INFO:root:current train perplexity208.25379943847656
INFO:root:current mean train loss 6764.409885301215
INFO:root:current train perplexity207.7923583984375
INFO:root:current mean train loss 6763.516687140356
INFO:root:current train perplexity207.38462829589844
INFO:root:current mean train loss 6761.057861969587
INFO:root:current train perplexity207.084228515625

100%|██████████| 1/1 [07:48<00:00, 468.13s/it][A100%|██████████| 1/1 [07:48<00:00, 468.13s/it]
INFO:root:final mean train loss: 6755.5440290319275
INFO:root:final train perplexity: 206.73968505859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.30s/it][A100%|██████████| 1/1 [00:41<00:00, 41.30s/it]
INFO:root:eval mean loss: 6529.484139516844
INFO:root:eval perplexity: 197.10850524902344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.96s/it][A100%|██████████| 1/1 [00:39<00:00, 39.96s/it]
INFO:root:eval mean loss: 6672.137539478059
INFO:root:eval perplexity: 241.13450622558594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/13
  6%|▋         | 13/200 [2:00:16<28:40:31, 552.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6720.99794921875
INFO:root:current train perplexity199.51251220703125
INFO:root:current mean train loss 6751.430485026041
INFO:root:current train perplexity203.50750732421875
INFO:root:current mean train loss 6762.862098277699
INFO:root:current train perplexity204.52099609375
INFO:root:current mean train loss 6773.157432556152
INFO:root:current train perplexity206.37742614746094
INFO:root:current mean train loss 6769.394711449033
INFO:root:current train perplexity205.5560760498047
INFO:root:current mean train loss 6768.972888183594
INFO:root:current train perplexity205.5635223388672
INFO:root:current mean train loss 6756.1558073966735
INFO:root:current train perplexity204.5974578857422
INFO:root:current mean train loss 6748.411694335938
INFO:root:current train perplexity203.5562286376953
INFO:root:current mean train loss 6739.143802400915
INFO:root:current train perplexity202.72068786621094
INFO:root:current mean train loss 6741.956840183424
INFO:root:current train perplexity202.82278442382812
INFO:root:current mean train loss 6736.650142176011
INFO:root:current train perplexity202.331787109375
INFO:root:current mean train loss 6728.72268284389
INFO:root:current train perplexity202.00083923339844
INFO:root:current mean train loss 6726.929977266906
INFO:root:current train perplexity201.76663208007812
INFO:root:current mean train loss 6723.881085390033
INFO:root:current train perplexity201.5349884033203
INFO:root:current mean train loss 6730.385147309639
INFO:root:current train perplexity201.67678833007812
INFO:root:current mean train loss 6724.11822734632
INFO:root:current train perplexity201.36199951171875
INFO:root:current mean train loss 6722.072991114487
INFO:root:current train perplexity201.2364501953125
INFO:root:current mean train loss 6723.185201103743
INFO:root:current train perplexity201.14385986328125
INFO:root:current mean train loss 6720.291745364011
INFO:root:current train perplexity200.9886474609375
INFO:root:current mean train loss 6720.076787821452
INFO:root:current train perplexity200.84800720214844

100%|██████████| 1/1 [08:00<00:00, 480.77s/it][A100%|██████████| 1/1 [08:00<00:00, 480.77s/it]
INFO:root:final mean train loss: 6718.641501591654
INFO:root:final train perplexity: 200.80557250976562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.28s/it][A100%|██████████| 1/1 [00:42<00:00, 42.28s/it]
INFO:root:eval mean loss: 6487.9142581587985
INFO:root:eval perplexity: 190.5882568359375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.64s/it][A100%|██████████| 1/1 [00:38<00:00, 38.64s/it]
INFO:root:eval mean loss: 6617.8169057859595
INFO:root:eval perplexity: 230.6029052734375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/14
  7%|▋         | 14/200 [2:09:40<28:42:33, 555.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6638.823532516892
INFO:root:current train perplexity198.0299530029297
INFO:root:current mean train loss 6735.951966668567
INFO:root:current train perplexity201.73382568359375
INFO:root:current mean train loss 6752.398870154272
INFO:root:current train perplexity203.42465209960938
INFO:root:current mean train loss 6728.122468761591
INFO:root:current train perplexity201.335693359375
INFO:root:current mean train loss 6721.38771654212
INFO:root:current train perplexity200.5102081298828
INFO:root:current mean train loss 6718.244564347649
INFO:root:current train perplexity199.8099365234375
INFO:root:current mean train loss 6720.961138331534
INFO:root:current train perplexity200.18759155273438
INFO:root:current mean train loss 6708.6667463905615
INFO:root:current train perplexity199.4204559326172
INFO:root:current mean train loss 6708.5337923293755
INFO:root:current train perplexity199.49444580078125
INFO:root:current mean train loss 6704.485922179329
INFO:root:current train perplexity199.1925506591797
INFO:root:current mean train loss 6706.670365895462
INFO:root:current train perplexity199.10926818847656
INFO:root:current mean train loss 6705.495293260774
INFO:root:current train perplexity199.10243225097656
INFO:root:current mean train loss 6704.447846273116
INFO:root:current train perplexity198.86228942871094
INFO:root:current mean train loss 6704.994098261032
INFO:root:current train perplexity198.9652862548828
INFO:root:current mean train loss 6704.943775620324
INFO:root:current train perplexity198.8772735595703
INFO:root:current mean train loss 6706.524472834153
INFO:root:current train perplexity198.97193908691406
INFO:root:current mean train loss 6706.957609611236
INFO:root:current train perplexity199.09152221679688
INFO:root:current mean train loss 6711.07068693329
INFO:root:current train perplexity198.95570373535156
INFO:root:current mean train loss 6709.272994936972
INFO:root:current train perplexity198.72862243652344
INFO:root:current mean train loss 6706.885740422932
INFO:root:current train perplexity198.6588897705078

100%|██████████| 1/1 [07:52<00:00, 472.86s/it][A100%|██████████| 1/1 [07:52<00:00, 472.87s/it]
INFO:root:final mean train loss: 6705.477081067984
INFO:root:final train perplexity: 198.7301483154297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.16s/it][A100%|██████████| 1/1 [00:40<00:00, 40.17s/it]
INFO:root:eval mean loss: 6478.601694093529
INFO:root:eval perplexity: 189.15757751464844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.31s/it][A100%|██████████| 1/1 [00:37<00:00, 37.31s/it]
INFO:root:eval mean loss: 6612.897318955009
INFO:root:eval perplexity: 229.6719970703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/15
  8%|▊         | 15/200 [2:18:53<28:30:47, 554.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6654.2137134693285
INFO:root:current train perplexity192.36619567871094
INFO:root:current mean train loss 6743.121512276785
INFO:root:current train perplexity197.3770294189453
INFO:root:current mean train loss 6734.23810054749
INFO:root:current train perplexity199.5401153564453
INFO:root:current mean train loss 6755.291348042461
INFO:root:current train perplexity200.7857666015625
INFO:root:current mean train loss 6740.106594808301
INFO:root:current train perplexity199.57398986816406
INFO:root:current mean train loss 6733.188434256544
INFO:root:current train perplexity198.51051330566406
INFO:root:current mean train loss 6711.09372685517
INFO:root:current train perplexity197.27879333496094
INFO:root:current mean train loss 6709.062526551103
INFO:root:current train perplexity197.08096313476562
INFO:root:current mean train loss 6701.268932047717
INFO:root:current train perplexity196.4660186767578
INFO:root:current mean train loss 6695.709969126703
INFO:root:current train perplexity195.52647399902344
INFO:root:current mean train loss 6689.397687010793
INFO:root:current train perplexity194.74905395507812
INFO:root:current mean train loss 6684.384221491822
INFO:root:current train perplexity194.29501342773438
INFO:root:current mean train loss 6685.826786704421
INFO:root:current train perplexity194.07186889648438
INFO:root:current mean train loss 6679.172949651495
INFO:root:current train perplexity193.70066833496094
INFO:root:current mean train loss 6673.738519681697
INFO:root:current train perplexity193.2144317626953
INFO:root:current mean train loss 6668.8948938475305
INFO:root:current train perplexity192.9159698486328
INFO:root:current mean train loss 6664.784053631821
INFO:root:current train perplexity192.69642639160156
INFO:root:current mean train loss 6664.859274225877
INFO:root:current train perplexity192.58251953125
INFO:root:current mean train loss 6665.883805917948
INFO:root:current train perplexity192.54396057128906
INFO:root:current mean train loss 6665.453244696376
INFO:root:current train perplexity192.32080078125

100%|██████████| 1/1 [07:53<00:00, 473.86s/it][A100%|██████████| 1/1 [07:53<00:00, 473.86s/it]
INFO:root:final mean train loss: 6664.235913221366
INFO:root:final train perplexity: 192.36610412597656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.26s/it][A100%|██████████| 1/1 [00:41<00:00, 41.26s/it]
INFO:root:eval mean loss: 6470.958849318484
INFO:root:eval perplexity: 187.99114990234375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.27s/it][A100%|██████████| 1/1 [00:38<00:00, 38.27s/it]
INFO:root:eval mean loss: 6621.129364230109
INFO:root:eval perplexity: 231.2317352294922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/16
  8%|▊         | 16/200 [2:28:09<28:22:31, 555.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6681.340613996479
INFO:root:current train perplexity191.08966064453125
INFO:root:current mean train loss 6672.204130116959
INFO:root:current train perplexity191.9739227294922
INFO:root:current mean train loss 6702.798220926546
INFO:root:current train perplexity192.94044494628906
INFO:root:current mean train loss 6692.629060236271
INFO:root:current train perplexity192.74273681640625
INFO:root:current mean train loss 6679.910533605361
INFO:root:current train perplexity191.76065063476562
INFO:root:current mean train loss 6676.309708844133
INFO:root:current train perplexity191.7522430419922
INFO:root:current mean train loss 6667.009113613078
INFO:root:current train perplexity191.81552124023438
INFO:root:current mean train loss 6671.527952993271
INFO:root:current train perplexity192.21435546875
INFO:root:current mean train loss 6667.060247515427
INFO:root:current train perplexity191.9599151611328
INFO:root:current mean train loss 6670.798624967817
INFO:root:current train perplexity192.19529724121094
INFO:root:current mean train loss 6673.302148072771
INFO:root:current train perplexity192.4893341064453
INFO:root:current mean train loss 6669.387200776579
INFO:root:current train perplexity192.5209197998047
INFO:root:current mean train loss 6665.604756112928
INFO:root:current train perplexity192.45840454101562
INFO:root:current mean train loss 6667.896836607061
INFO:root:current train perplexity192.8509521484375
INFO:root:current mean train loss 6669.876746327434
INFO:root:current train perplexity192.8880157470703
INFO:root:current mean train loss 6668.145910932029
INFO:root:current train perplexity192.68650817871094
INFO:root:current mean train loss 6670.094784419884
INFO:root:current train perplexity192.7730712890625
INFO:root:current mean train loss 6668.484400916678
INFO:root:current train perplexity192.76976013183594
INFO:root:current mean train loss 6671.685442485636
INFO:root:current train perplexity192.90713500976562
INFO:root:current mean train loss 6671.17075252291
INFO:root:current train perplexity193.0616455078125

100%|██████████| 1/1 [07:48<00:00, 468.34s/it][A100%|██████████| 1/1 [07:48<00:00, 468.34s/it]
INFO:root:final mean train loss: 6668.524891017004
INFO:root:final train perplexity: 193.0183868408203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.05s/it][A100%|██████████| 1/1 [00:40<00:00, 40.05s/it]
INFO:root:eval mean loss: 6489.139579177749
INFO:root:eval perplexity: 190.7772979736328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.40s/it][A100%|██████████| 1/1 [00:39<00:00, 39.40s/it]
INFO:root:eval mean loss: 6645.81082304826
INFO:root:eval perplexity: 235.97166442871094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/17
  8%|▊         | 17/200 [2:37:20<28:08:44, 553.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6638.711264870383
INFO:root:current train perplexity192.4076690673828
INFO:root:current mean train loss 6696.196691634807
INFO:root:current train perplexity193.55479431152344
INFO:root:current mean train loss 6696.796293470595
INFO:root:current train perplexity193.75665283203125
INFO:root:current mean train loss 6704.289880496939
INFO:root:current train perplexity193.92404174804688
INFO:root:current mean train loss 6698.587008116675
INFO:root:current train perplexity193.96510314941406
INFO:root:current mean train loss 6692.526789035927
INFO:root:current train perplexity194.0149688720703
INFO:root:current mean train loss 6699.630211408748
INFO:root:current train perplexity194.48707580566406
INFO:root:current mean train loss 6697.997740150103
INFO:root:current train perplexity194.4407196044922
INFO:root:current mean train loss 6693.67898532077
INFO:root:current train perplexity194.46250915527344
INFO:root:current mean train loss 6688.056393024893
INFO:root:current train perplexity194.3447723388672
INFO:root:current mean train loss 6681.481196235208
INFO:root:current train perplexity194.20184326171875
INFO:root:current mean train loss 6678.021807840778
INFO:root:current train perplexity193.90176391601562
INFO:root:current mean train loss 6675.38068498884
INFO:root:current train perplexity193.93597412109375
INFO:root:current mean train loss 6672.7058327094965
INFO:root:current train perplexity193.9528045654297
INFO:root:current mean train loss 6674.624312862273
INFO:root:current train perplexity193.92782592773438
INFO:root:current mean train loss 6672.0618635124765
INFO:root:current train perplexity193.87522888183594
INFO:root:current mean train loss 6673.630011246668
INFO:root:current train perplexity193.9187774658203
INFO:root:current mean train loss 6675.144007194229
INFO:root:current train perplexity193.90194702148438
INFO:root:current mean train loss 6679.186468350686
INFO:root:current train perplexity194.16702270507812

100%|██████████| 1/1 [07:42<00:00, 462.20s/it][A100%|██████████| 1/1 [07:42<00:00, 462.20s/it]
INFO:root:final mean train loss: 6675.900198439667
INFO:root:final train perplexity: 194.14498901367188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.76s/it][A100%|██████████| 1/1 [00:41<00:00, 41.76s/it]
INFO:root:eval mean loss: 6517.756058496786
INFO:root:eval perplexity: 195.2467803955078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.36s/it][A100%|██████████| 1/1 [00:39<00:00, 39.36s/it]
INFO:root:eval mean loss: 6673.1693487505545
INFO:root:eval perplexity: 241.33937072753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/18
  9%|▉         | 18/200 [2:46:25<27:52:16, 551.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6588.557421875
INFO:root:current train perplexity192.46336364746094
INFO:root:current mean train loss 6736.516685267857
INFO:root:current train perplexity197.70594787597656
INFO:root:current mean train loss 6706.280171017531
INFO:root:current train perplexity197.10606384277344
INFO:root:current mean train loss 6718.765442494877
INFO:root:current train perplexity196.9735870361328
INFO:root:current mean train loss 6712.967361111111
INFO:root:current train perplexity196.12661743164062
INFO:root:current mean train loss 6672.396884668936
INFO:root:current train perplexity193.01480102539062
INFO:root:current mean train loss 6649.7859132877065
INFO:root:current train perplexity189.74838256835938
INFO:root:current mean train loss 6644.24081200133
INFO:root:current train perplexity188.8785400390625
INFO:root:current mean train loss 6638.809652198175
INFO:root:current train perplexity187.73646545410156
INFO:root:current mean train loss 6623.984978202693
INFO:root:current train perplexity186.3566131591797
INFO:root:current mean train loss 6613.022731071206
INFO:root:current train perplexity185.1563262939453
INFO:root:current mean train loss 6602.478583675056
INFO:root:current train perplexity184.25083923339844
INFO:root:current mean train loss 6602.840044735477
INFO:root:current train perplexity183.79629516601562
INFO:root:current mean train loss 6602.241194100216
INFO:root:current train perplexity183.2888641357422
INFO:root:current mean train loss 6597.688660058941
INFO:root:current train perplexity182.85203552246094
INFO:root:current mean train loss 6599.756894336587
INFO:root:current train perplexity182.6373291015625
INFO:root:current mean train loss 6596.353950058411
INFO:root:current train perplexity182.29104614257812
INFO:root:current mean train loss 6599.459226081378
INFO:root:current train perplexity182.2272491455078
INFO:root:current mean train loss 6597.945638471693
INFO:root:current train perplexity182.0633544921875
INFO:root:current mean train loss 6595.8912463090555
INFO:root:current train perplexity182.0203857421875

100%|██████████| 1/1 [07:46<00:00, 466.63s/it][A100%|██████████| 1/1 [07:46<00:00, 466.63s/it]
INFO:root:final mean train loss: 6593.607267117176
INFO:root:final train perplexity: 181.93699645996094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.96s/it][A100%|██████████| 1/1 [00:40<00:00, 40.96s/it]
INFO:root:eval mean loss: 6380.987805089207
INFO:root:eval perplexity: 174.7906494140625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.63s/it][A100%|██████████| 1/1 [00:38<00:00, 38.63s/it]
INFO:root:eval mean loss: 6527.835295981549
INFO:root:eval perplexity: 214.15931701660156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/19
 10%|▉         | 19/200 [2:55:34<27:40:34, 550.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6532.824085582386
INFO:root:current train perplexity176.08680725097656
INFO:root:current mean train loss 6514.671058529713
INFO:root:current train perplexity176.2887725830078
INFO:root:current mean train loss 6545.8784839527025
INFO:root:current train perplexity178.6630401611328
INFO:root:current mean train loss 6559.629620475058
INFO:root:current train perplexity178.89569091796875
INFO:root:current mean train loss 6571.504079809686
INFO:root:current train perplexity179.39430236816406
INFO:root:current mean train loss 6552.706217447917
INFO:root:current train perplexity178.8936309814453
INFO:root:current mean train loss 6557.750425479803
INFO:root:current train perplexity179.3682098388672
INFO:root:current mean train loss 6553.4995151001995
INFO:root:current train perplexity179.08335876464844
INFO:root:current mean train loss 6559.412900010455
INFO:root:current train perplexity179.1522979736328
INFO:root:current mean train loss 6571.151838521895
INFO:root:current train perplexity179.5081024169922
INFO:root:current mean train loss 6571.654278241958
INFO:root:current train perplexity179.35597229003906
INFO:root:current mean train loss 6576.003404913102
INFO:root:current train perplexity179.80194091796875
INFO:root:current mean train loss 6579.608126726166
INFO:root:current train perplexity179.9976043701172
INFO:root:current mean train loss 6582.445695516381
INFO:root:current train perplexity180.1423797607422
INFO:root:current mean train loss 6580.656112306061
INFO:root:current train perplexity180.09762573242188
INFO:root:current mean train loss 6582.7991669062085
INFO:root:current train perplexity180.15225219726562
INFO:root:current mean train loss 6585.724019343404
INFO:root:current train perplexity180.47132873535156
INFO:root:current mean train loss 6585.269007524408
INFO:root:current train perplexity180.46142578125
INFO:root:current mean train loss 6586.154356905186
INFO:root:current train perplexity180.72320556640625
INFO:root:current mean train loss 6588.0132364358415
INFO:root:current train perplexity180.94784545898438

100%|██████████| 1/1 [07:43<00:00, 463.60s/it][A100%|██████████| 1/1 [07:43<00:00, 463.61s/it]
INFO:root:final mean train loss: 6586.988874303652
INFO:root:final train perplexity: 180.9890899658203
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.45s/it][A100%|██████████| 1/1 [00:41<00:00, 41.45s/it]
INFO:root:eval mean loss: 6402.307682984264
INFO:root:eval perplexity: 177.8324737548828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.10s/it][A100%|██████████| 1/1 [00:38<00:00, 38.10s/it]
INFO:root:eval mean loss: 6541.797229956228
INFO:root:eval perplexity: 216.6317901611328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/20
 10%|█         | 20/200 [3:04:39<27:26:55, 548.97s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6569.5973557692305
INFO:root:current train perplexity184.19287109375
INFO:root:current mean train loss 6550.847965377698
INFO:root:current train perplexity182.82736206054688
INFO:root:current mean train loss 6597.353632077014
INFO:root:current train perplexity183.56936645507812
INFO:root:current mean train loss 6618.634915422198
INFO:root:current train perplexity185.54847717285156
INFO:root:current mean train loss 6605.137912202805
INFO:root:current train perplexity185.5045928955078
INFO:root:current mean train loss 6612.354007529859
INFO:root:current train perplexity185.85052490234375
INFO:root:current mean train loss 6606.853251998973
INFO:root:current train perplexity186.0306854248047
INFO:root:current mean train loss 6611.642469764885
INFO:root:current train perplexity185.94775390625
INFO:root:current mean train loss 6609.381615367067
INFO:root:current train perplexity184.9864501953125
INFO:root:current mean train loss 6608.47249244958
INFO:root:current train perplexity184.3843536376953
INFO:root:current mean train loss 6605.867903708494
INFO:root:current train perplexity183.96499633789062
INFO:root:current mean train loss 6604.908608239821
INFO:root:current train perplexity183.58993530273438
INFO:root:current mean train loss 6608.532570605706
INFO:root:current train perplexity183.10899353027344
INFO:root:current mean train loss 6602.95439584403
INFO:root:current train perplexity182.7887420654297
INFO:root:current mean train loss 6594.891224578158
INFO:root:current train perplexity182.39068603515625
INFO:root:current mean train loss 6597.085852788438
INFO:root:current train perplexity182.109375
INFO:root:current mean train loss 6594.327358466958
INFO:root:current train perplexity181.82772827148438
INFO:root:current mean train loss 6593.57895499389
INFO:root:current train perplexity181.802490234375
INFO:root:current mean train loss 6591.528556620446
INFO:root:current train perplexity181.5663299560547
INFO:root:current mean train loss 6590.932614417467
INFO:root:current train perplexity181.3802032470703

100%|██████████| 1/1 [07:41<00:00, 461.96s/it][A100%|██████████| 1/1 [07:41<00:00, 461.96s/it]
INFO:root:final mean train loss: 6588.985195450391
INFO:root:final train perplexity: 181.2745361328125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.40s/it][A100%|██████████| 1/1 [00:39<00:00, 39.40s/it]
INFO:root:eval mean loss: 6362.517181612921
INFO:root:eval perplexity: 172.1975860595703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.72s/it][A100%|██████████| 1/1 [00:38<00:00, 38.72s/it]
INFO:root:eval mean loss: 6497.200847219913
INFO:root:eval perplexity: 208.83314514160156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/21
 10%|█         | 21/200 [3:13:42<27:12:00, 547.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6587.477730887277
INFO:root:current train perplexity177.9412078857422
INFO:root:current mean train loss 6553.391391851963
INFO:root:current train perplexity176.36891174316406
INFO:root:current mean train loss 6568.498136520386
INFO:root:current train perplexity176.87945556640625
INFO:root:current mean train loss 6539.295194818733
INFO:root:current train perplexity176.26893615722656
INFO:root:current mean train loss 6537.396652489378
INFO:root:current train perplexity176.07891845703125
INFO:root:current mean train loss 6543.916510053676
INFO:root:current train perplexity176.3521728515625
INFO:root:current mean train loss 6539.5374793075935
INFO:root:current train perplexity176.0606231689453
INFO:root:current mean train loss 6545.362683815931
INFO:root:current train perplexity176.37530517578125
INFO:root:current mean train loss 6555.6651080835645
INFO:root:current train perplexity176.34494018554688
INFO:root:current mean train loss 6553.954218525268
INFO:root:current train perplexity176.393798828125
INFO:root:current mean train loss 6551.929658369584
INFO:root:current train perplexity176.00706481933594
INFO:root:current mean train loss 6555.762043989241
INFO:root:current train perplexity176.07630920410156
INFO:root:current mean train loss 6553.828517646547
INFO:root:current train perplexity176.02073669433594
INFO:root:current mean train loss 6551.222969527793
INFO:root:current train perplexity176.079345703125
INFO:root:current mean train loss 6550.071656950227
INFO:root:current train perplexity176.00076293945312
INFO:root:current mean train loss 6551.225240437721
INFO:root:current train perplexity176.05859375
INFO:root:current mean train loss 6547.250355301272
INFO:root:current train perplexity175.83912658691406
INFO:root:current mean train loss 6545.848832462806
INFO:root:current train perplexity175.68667602539062
INFO:root:current mean train loss 6548.74907158161
INFO:root:current train perplexity175.79644775390625
INFO:root:current mean train loss 6551.639149172418
INFO:root:current train perplexity175.81748962402344

100%|██████████| 1/1 [07:42<00:00, 462.05s/it][A100%|██████████| 1/1 [07:42<00:00, 462.06s/it]
INFO:root:final mean train loss: 6550.001437142469
INFO:root:final train perplexity: 175.78236389160156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.04s/it][A100%|██████████| 1/1 [00:40<00:00, 40.04s/it]
INFO:root:eval mean loss: 6330.709363572141
INFO:root:eval perplexity: 167.82191467285156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.76s/it][A100%|██████████| 1/1 [00:38<00:00, 38.76s/it]
INFO:root:eval mean loss: 6471.650182845745
INFO:root:eval perplexity: 204.49212646484375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/22
 11%|█         | 22/200 [3:22:45<26:59:33, 545.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6531.009692048373
INFO:root:current train perplexity173.61187744140625
INFO:root:current mean train loss 6477.904593230672
INFO:root:current train perplexity172.93148803710938
INFO:root:current mean train loss 6496.506691062844
INFO:root:current train perplexity172.49581909179688
INFO:root:current mean train loss 6503.045658878603
INFO:root:current train perplexity172.65850830078125
INFO:root:current mean train loss 6516.276031687698
INFO:root:current train perplexity173.09970092773438
INFO:root:current mean train loss 6520.419649187391
INFO:root:current train perplexity173.71206665039062
INFO:root:current mean train loss 6528.610198475808
INFO:root:current train perplexity173.7354278564453
INFO:root:current mean train loss 6530.5863392424
INFO:root:current train perplexity173.570068359375
INFO:root:current mean train loss 6528.042262336233
INFO:root:current train perplexity173.75399780273438
INFO:root:current mean train loss 6529.293207119572
INFO:root:current train perplexity173.65823364257812
INFO:root:current mean train loss 6532.8072162781045
INFO:root:current train perplexity173.7697296142578
INFO:root:current mean train loss 6532.756520823343
INFO:root:current train perplexity173.67381286621094
INFO:root:current mean train loss 6531.890556341443
INFO:root:current train perplexity173.5835723876953
INFO:root:current mean train loss 6527.844897976603
INFO:root:current train perplexity173.4959259033203
INFO:root:current mean train loss 6528.2465004852975
INFO:root:current train perplexity173.5702362060547
INFO:root:current mean train loss 6529.767859980928
INFO:root:current train perplexity173.4962615966797
INFO:root:current mean train loss 6532.743819579348
INFO:root:current train perplexity173.55104064941406
INFO:root:current mean train loss 6535.116337896133
INFO:root:current train perplexity173.68846130371094
INFO:root:current mean train loss 6535.41447048727
INFO:root:current train perplexity173.79368591308594
INFO:root:current mean train loss 6538.252236738945
INFO:root:current train perplexity173.95310974121094

100%|██████████| 1/1 [07:41<00:00, 461.22s/it][A100%|██████████| 1/1 [07:41<00:00, 461.22s/it]
INFO:root:final mean train loss: 6536.792563572593
INFO:root:final train perplexity: 173.95950317382812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.94s/it][A100%|██████████| 1/1 [00:40<00:00, 40.94s/it]
INFO:root:eval mean loss: 6331.175261801862
INFO:root:eval perplexity: 167.88522338867188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.03s/it][A100%|██████████| 1/1 [00:39<00:00, 39.03s/it]
INFO:root:eval mean loss: 6460.943866702682
INFO:root:eval perplexity: 202.6999053955078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/23
 12%|█▏        | 23/200 [3:31:49<26:48:21, 545.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6503.370806206597
INFO:root:current train perplexity172.8944549560547
INFO:root:current mean train loss 6457.524725020559
INFO:root:current train perplexity172.3374786376953
INFO:root:current mean train loss 6473.398068763469
INFO:root:current train perplexity172.9281005859375
INFO:root:current mean train loss 6488.927690554888
INFO:root:current train perplexity173.08294677734375
INFO:root:current mean train loss 6503.412317641901
INFO:root:current train perplexity173.2281036376953
INFO:root:current mean train loss 6521.574831998146
INFO:root:current train perplexity174.10655212402344
INFO:root:current mean train loss 6529.41080799932
INFO:root:current train perplexity174.2209930419922
INFO:root:current mean train loss 6532.639765254154
INFO:root:current train perplexity174.55345153808594
INFO:root:current mean train loss 6527.806039874474
INFO:root:current train perplexity174.44801330566406
INFO:root:current mean train loss 6535.035788549558
INFO:root:current train perplexity174.5047607421875
INFO:root:current mean train loss 6534.073537844037
INFO:root:current train perplexity174.30242919921875
INFO:root:current mean train loss 6538.073956144958
INFO:root:current train perplexity174.44053649902344
INFO:root:current mean train loss 6540.442640579578
INFO:root:current train perplexity174.52806091308594
INFO:root:current mean train loss 6535.3993603164345
INFO:root:current train perplexity174.32012939453125
INFO:root:current mean train loss 6536.862170983641
INFO:root:current train perplexity174.19308471679688
INFO:root:current mean train loss 6535.613640551297
INFO:root:current train perplexity174.02264404296875
INFO:root:current mean train loss 6537.146253813795
INFO:root:current train perplexity174.0068817138672
INFO:root:current mean train loss 6540.879327153893
INFO:root:current train perplexity174.10189819335938
INFO:root:current mean train loss 6540.987371083416
INFO:root:current train perplexity174.2106170654297

100%|██████████| 1/1 [07:44<00:00, 464.56s/it][A100%|██████████| 1/1 [07:44<00:00, 464.56s/it]
INFO:root:final mean train loss: 6538.885206260046
INFO:root:final train perplexity: 174.24708557128906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.65s/it][A100%|██████████| 1/1 [00:39<00:00, 39.65s/it]
INFO:root:eval mean loss: 6324.107807998116
INFO:root:eval perplexity: 166.92771911621094
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.46s/it][A100%|██████████| 1/1 [00:39<00:00, 39.46s/it]
INFO:root:eval mean loss: 6456.145761476341
INFO:root:eval perplexity: 201.90203857421875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/24
 12%|█▏        | 24/200 [3:40:55<26:40:02, 545.47s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6170.173409598215
INFO:root:current train perplexity157.38632202148438
INFO:root:current mean train loss 6477.6984566661795
INFO:root:current train perplexity171.13311767578125
INFO:root:current mean train loss 6484.870055857488
INFO:root:current train perplexity170.80343627929688
INFO:root:current mean train loss 6521.814259084894
INFO:root:current train perplexity172.66946411132812
INFO:root:current mean train loss 6535.9844769752
INFO:root:current train perplexity173.2701416015625
INFO:root:current mean train loss 6541.280416936329
INFO:root:current train perplexity173.7216796875
INFO:root:current mean train loss 6538.238319862026
INFO:root:current train perplexity173.6304168701172
INFO:root:current mean train loss 6539.529731977104
INFO:root:current train perplexity173.44241333007812
INFO:root:current mean train loss 6535.687542959069
INFO:root:current train perplexity173.43348693847656
INFO:root:current mean train loss 6540.257007132029
INFO:root:current train perplexity173.71812438964844
INFO:root:current mean train loss 6539.004003712295
INFO:root:current train perplexity173.93270874023438
INFO:root:current mean train loss 6539.382269965277
INFO:root:current train perplexity173.98240661621094
INFO:root:current mean train loss 6545.682181092067
INFO:root:current train perplexity174.2381134033203
INFO:root:current mean train loss 6542.579579756838
INFO:root:current train perplexity173.89523315429688
INFO:root:current mean train loss 6543.393787549418
INFO:root:current train perplexity173.7009735107422
INFO:root:current mean train loss 6542.4605726661
INFO:root:current train perplexity173.71397399902344
INFO:root:current mean train loss 6536.039783831479
INFO:root:current train perplexity173.49990844726562
INFO:root:current mean train loss 6536.082621363777
INFO:root:current train perplexity173.58074951171875
INFO:root:current mean train loss 6533.013744022811
INFO:root:current train perplexity173.38833618164062
INFO:root:current mean train loss 6533.591637357843
INFO:root:current train perplexity173.36129760742188

100%|██████████| 1/1 [07:45<00:00, 465.98s/it][A100%|██████████| 1/1 [07:45<00:00, 465.99s/it]
INFO:root:final mean train loss: 6531.95271305117
INFO:root:final train perplexity: 173.2963409423828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.20s/it][A100%|██████████| 1/1 [00:40<00:00, 40.20s/it]
INFO:root:eval mean loss: 6319.933218015846
INFO:root:eval perplexity: 166.36488342285156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.42s/it][A100%|██████████| 1/1 [00:38<00:00, 38.42s/it]
INFO:root:eval mean loss: 6454.2581890999
INFO:root:eval perplexity: 201.5889129638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/25
 12%|█▎        | 25/200 [3:50:02<26:32:08, 545.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6796.989400227864
INFO:root:current train perplexity178.528564453125
INFO:root:current mean train loss 6593.486284809728
INFO:root:current train perplexity174.08572387695312
INFO:root:current mean train loss 6584.410228184292
INFO:root:current train perplexity174.99998474121094
INFO:root:current mean train loss 6566.935980902777
INFO:root:current train perplexity173.6131134033203
INFO:root:current mean train loss 6562.3575750386935
INFO:root:current train perplexity173.8921661376953
INFO:root:current mean train loss 6550.733491620945
INFO:root:current train perplexity173.6703338623047
INFO:root:current mean train loss 6546.49819163787
INFO:root:current train perplexity173.83628845214844
INFO:root:current mean train loss 6543.814602846599
INFO:root:current train perplexity173.35476684570312
INFO:root:current mean train loss 6548.0826261946295
INFO:root:current train perplexity173.550537109375
INFO:root:current mean train loss 6545.315513709923
INFO:root:current train perplexity173.6014404296875
INFO:root:current mean train loss 6541.93504858017
INFO:root:current train perplexity173.53573608398438
INFO:root:current mean train loss 6536.9649089120885
INFO:root:current train perplexity173.3583221435547
INFO:root:current mean train loss 6538.215631223193
INFO:root:current train perplexity173.26353454589844
INFO:root:current mean train loss 6530.781774791706
INFO:root:current train perplexity172.95811462402344
INFO:root:current mean train loss 6532.345255305258
INFO:root:current train perplexity172.97610473632812
INFO:root:current mean train loss 6530.513636311208
INFO:root:current train perplexity172.9248809814453
INFO:root:current mean train loss 6531.958635001347
INFO:root:current train perplexity172.9713897705078
INFO:root:current mean train loss 6529.602429170896
INFO:root:current train perplexity172.91458129882812
INFO:root:current mean train loss 6531.233120031524
INFO:root:current train perplexity173.02674865722656
INFO:root:current mean train loss 6531.933529796323
INFO:root:current train perplexity173.00364685058594

100%|██████████| 1/1 [07:40<00:00, 460.24s/it][A100%|██████████| 1/1 [07:40<00:00, 460.24s/it]
INFO:root:final mean train loss: 6529.783337691668
INFO:root:final train perplexity: 172.99993896484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.03s/it][A100%|██████████| 1/1 [00:39<00:00, 39.03s/it]
INFO:root:eval mean loss: 6312.962755568484
INFO:root:eval perplexity: 165.42906188964844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.43s/it][A100%|██████████| 1/1 [00:38<00:00, 38.43s/it]
INFO:root:eval mean loss: 6443.815279913287
INFO:root:eval perplexity: 199.86558532714844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/26
 13%|█▎        | 26/200 [3:59:02<26:18:04, 544.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6508.443371284298
INFO:root:current train perplexity174.60621643066406
INFO:root:current mean train loss 6499.909224706339
INFO:root:current train perplexity170.91725158691406
INFO:root:current mean train loss 6503.573991830913
INFO:root:current train perplexity171.38259887695312
INFO:root:current mean train loss 6516.906975978281
INFO:root:current train perplexity172.0285186767578
INFO:root:current mean train loss 6520.825608303217
INFO:root:current train perplexity172.12213134765625
INFO:root:current mean train loss 6524.60433694836
INFO:root:current train perplexity172.2479248046875
INFO:root:current mean train loss 6525.56771417341
INFO:root:current train perplexity172.4396514892578
INFO:root:current mean train loss 6529.623966108932
INFO:root:current train perplexity172.38250732421875
INFO:root:current mean train loss 6530.28598417992
INFO:root:current train perplexity172.3649139404297
INFO:root:current mean train loss 6527.0418806248335
INFO:root:current train perplexity172.34519958496094
INFO:root:current mean train loss 6520.63574171845
INFO:root:current train perplexity172.0546112060547
INFO:root:current mean train loss 6527.381777309514
INFO:root:current train perplexity172.29129028320312
INFO:root:current mean train loss 6527.843833806532
INFO:root:current train perplexity172.49203491210938
INFO:root:current mean train loss 6526.487880356894
INFO:root:current train perplexity172.3794708251953
INFO:root:current mean train loss 6526.815088466668
INFO:root:current train perplexity172.38760375976562
INFO:root:current mean train loss 6529.73241775582
INFO:root:current train perplexity172.52066040039062
INFO:root:current mean train loss 6529.770141229624
INFO:root:current train perplexity172.51473999023438
INFO:root:current mean train loss 6529.861335697426
INFO:root:current train perplexity172.4787139892578
INFO:root:current mean train loss 6529.305715251137
INFO:root:current train perplexity172.49557495117188
INFO:root:current mean train loss 6527.965830626529
INFO:root:current train perplexity172.53480529785156

100%|██████████| 1/1 [07:47<00:00, 467.54s/it][A100%|██████████| 1/1 [07:47<00:00, 467.54s/it]
INFO:root:final mean train loss: 6526.170916412553
INFO:root:final train perplexity: 172.50733947753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.65s/it][A100%|██████████| 1/1 [00:40<00:00, 40.65s/it]
INFO:root:eval mean loss: 6311.010433981604
INFO:root:eval perplexity: 165.1679229736328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.34s/it][A100%|██████████| 1/1 [00:38<00:00, 38.34s/it]
INFO:root:eval mean loss: 6443.099721922096
INFO:root:eval perplexity: 199.7482147216797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/27
 14%|█▎        | 27/200 [4:08:11<26:13:07, 545.59s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6460.413439520474
INFO:root:current train perplexity170.4346466064453
INFO:root:current mean train loss 6485.3645081339
INFO:root:current train perplexity171.11949157714844
INFO:root:current mean train loss 6499.260306898013
INFO:root:current train perplexity171.65118408203125
INFO:root:current mean train loss 6501.358053367231
INFO:root:current train perplexity171.31069946289062
INFO:root:current mean train loss 6508.722775655022
INFO:root:current train perplexity171.8727264404297
INFO:root:current mean train loss 6518.032515330981
INFO:root:current train perplexity172.16351318359375
INFO:root:current mean train loss 6510.640125587718
INFO:root:current train perplexity172.13174438476562
INFO:root:current mean train loss 6520.639214266574
INFO:root:current train perplexity172.34347534179688
INFO:root:current mean train loss 6510.592360845535
INFO:root:current train perplexity172.0132598876953
INFO:root:current mean train loss 6513.748623332301
INFO:root:current train perplexity172.02227783203125
INFO:root:current mean train loss 6517.189311440365
INFO:root:current train perplexity172.20423889160156
INFO:root:current mean train loss 6523.991560492903
INFO:root:current train perplexity172.37937927246094
INFO:root:current mean train loss 6524.321315844222
INFO:root:current train perplexity172.38473510742188
INFO:root:current mean train loss 6525.77932492061
INFO:root:current train perplexity172.4956512451172
INFO:root:current mean train loss 6524.450425052512
INFO:root:current train perplexity172.48643493652344
INFO:root:current mean train loss 6522.714518438102
INFO:root:current train perplexity172.44210815429688
INFO:root:current mean train loss 6520.17105481708
INFO:root:current train perplexity172.4446563720703
INFO:root:current mean train loss 6524.128356586409
INFO:root:current train perplexity172.54295349121094
INFO:root:current mean train loss 6525.558179052577
INFO:root:current train perplexity172.53497314453125
INFO:root:current mean train loss 6526.531887159649
INFO:root:current train perplexity172.5056915283203

100%|██████████| 1/1 [07:43<00:00, 463.72s/it][A100%|██████████| 1/1 [07:43<00:00, 463.72s/it]
INFO:root:final mean train loss: 6525.91700376048
INFO:root:final train perplexity: 172.47279357910156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.22s/it][A100%|██████████| 1/1 [00:40<00:00, 40.22s/it]
INFO:root:eval mean loss: 6307.063883463542
INFO:root:eval perplexity: 164.64125061035156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.32s/it][A100%|██████████| 1/1 [00:39<00:00, 39.32s/it]
INFO:root:eval mean loss: 6439.294654359209
INFO:root:eval perplexity: 199.12417602539062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/28
 14%|█▍        | 28/200 [4:17:16<26:04:05, 545.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6556.45345703125
INFO:root:current train perplexity174.6964111328125
INFO:root:current mean train loss 6526.932661830358
INFO:root:current train perplexity172.5675506591797
INFO:root:current mean train loss 6535.178284801136
INFO:root:current train perplexity172.77503967285156
INFO:root:current mean train loss 6535.60636328125
INFO:root:current train perplexity172.87582397460938
INFO:root:current mean train loss 6533.820964226974
INFO:root:current train perplexity172.23707580566406
INFO:root:current mean train loss 6540.9434375
INFO:root:current train perplexity172.26434326171875
INFO:root:current mean train loss 6539.379416956019
INFO:root:current train perplexity172.1656494140625
INFO:root:current mean train loss 6536.348333543347
INFO:root:current train perplexity172.02883911132812
INFO:root:current mean train loss 6528.87584765625
INFO:root:current train perplexity172.1756591796875
INFO:root:current mean train loss 6534.555009014423
INFO:root:current train perplexity172.19528198242188
INFO:root:current mean train loss 6534.236061046512
INFO:root:current train perplexity172.49302673339844
INFO:root:current mean train loss 6529.41220703125
INFO:root:current train perplexity172.31610107421875
INFO:root:current mean train loss 6524.301107536765
INFO:root:current train perplexity172.1040802001953
INFO:root:current mean train loss 6529.196533735795
INFO:root:current train perplexity172.35406494140625
INFO:root:current mean train loss 6526.164080045021
INFO:root:current train perplexity172.33221435546875
INFO:root:current mean train loss 6524.896125372024
INFO:root:current train perplexity172.34864807128906
INFO:root:current mean train loss 6527.39178229944
INFO:root:current train perplexity172.42123413085938
INFO:root:current mean train loss 6528.8824177486795
INFO:root:current train perplexity172.5751190185547
INFO:root:current mean train loss 6529.687616927084
INFO:root:current train perplexity172.74851989746094
INFO:root:current mean train loss 6528.621807258703
INFO:root:current train perplexity172.61026000976562

100%|██████████| 1/1 [07:41<00:00, 461.04s/it][A100%|██████████| 1/1 [07:41<00:00, 461.04s/it]
INFO:root:final mean train loss: 6526.903817211926
INFO:root:final train perplexity: 172.60714721679688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.67s/it][A100%|██████████| 1/1 [00:39<00:00, 39.67s/it]
INFO:root:eval mean loss: 6310.774483322251
INFO:root:eval perplexity: 165.1363525390625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.52s/it][A100%|██████████| 1/1 [00:39<00:00, 39.52s/it]
INFO:root:eval mean loss: 6441.733842565658
INFO:root:eval perplexity: 199.52383422851562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/29
 14%|█▍        | 29/200 [4:26:19<25:52:31, 544.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6516.1297182829485
INFO:root:current train perplexity174.12623596191406
INFO:root:current mean train loss 6492.3994878133135
INFO:root:current train perplexity171.9215850830078
INFO:root:current mean train loss 6495.946039905287
INFO:root:current train perplexity172.47689819335938
INFO:root:current mean train loss 6506.1065785933515
INFO:root:current train perplexity172.72421264648438
INFO:root:current mean train loss 6513.855703958651
INFO:root:current train perplexity172.59783935546875
INFO:root:current mean train loss 6511.69380559148
INFO:root:current train perplexity171.98110961914062
INFO:root:current mean train loss 6510.946910703802
INFO:root:current train perplexity171.87338256835938
INFO:root:current mean train loss 6510.276820943813
INFO:root:current train perplexity171.77777099609375
INFO:root:current mean train loss 6509.579474889644
INFO:root:current train perplexity171.69137573242188
INFO:root:current mean train loss 6516.658233642578
INFO:root:current train perplexity171.9440460205078
INFO:root:current mean train loss 6517.742714235635
INFO:root:current train perplexity172.0077667236328
INFO:root:current mean train loss 6517.756472593986
INFO:root:current train perplexity171.96372985839844
INFO:root:current mean train loss 6518.464423495549
INFO:root:current train perplexity172.06158447265625
INFO:root:current mean train loss 6518.960769828709
INFO:root:current train perplexity172.27987670898438
INFO:root:current mean train loss 6523.64422820145
INFO:root:current train perplexity172.4481964111328
INFO:root:current mean train loss 6526.4331744783485
INFO:root:current train perplexity172.5554656982422
INFO:root:current mean train loss 6525.973128082059
INFO:root:current train perplexity172.48463439941406
INFO:root:current mean train loss 6527.9752524239675
INFO:root:current train perplexity172.42543029785156
INFO:root:current mean train loss 6528.956003846376
INFO:root:current train perplexity172.5556182861328

100%|██████████| 1/1 [07:47<00:00, 467.15s/it][A100%|██████████| 1/1 [07:47<00:00, 467.15s/it]
INFO:root:final mean train loss: 6527.249124639514
INFO:root:final train perplexity: 172.65414428710938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.28s/it][A100%|██████████| 1/1 [00:40<00:00, 40.28s/it]
INFO:root:eval mean loss: 6301.965714691379
INFO:root:eval perplexity: 163.96339416503906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.09s/it][A100%|██████████| 1/1 [00:38<00:00, 38.09s/it]
INFO:root:eval mean loss: 6432.900695367908
INFO:root:eval perplexity: 198.0802001953125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/30
 15%|█▌        | 30/200 [4:35:27<25:46:04, 545.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6421.756618923611
INFO:root:current train perplexity168.34487915039062
INFO:root:current mean train loss 6538.445975487385
INFO:root:current train perplexity171.62892150878906
INFO:root:current mean train loss 6516.122880999552
INFO:root:current train perplexity172.1856689453125
INFO:root:current mean train loss 6522.712718383394
INFO:root:current train perplexity171.85592651367188
INFO:root:current mean train loss 6511.712974193918
INFO:root:current train perplexity171.4051513671875
INFO:root:current mean train loss 6525.410499677677
INFO:root:current train perplexity171.87322998046875
INFO:root:current mean train loss 6514.349491514009
INFO:root:current train perplexity171.77105712890625
INFO:root:current mean train loss 6511.3134379958565
INFO:root:current train perplexity171.6334991455078
INFO:root:current mean train loss 6514.264398563041
INFO:root:current train perplexity171.83995056152344
INFO:root:current mean train loss 6524.579014542079
INFO:root:current train perplexity172.22335815429688
INFO:root:current mean train loss 6520.41327563646
INFO:root:current train perplexity172.25474548339844
INFO:root:current mean train loss 6518.793045800693
INFO:root:current train perplexity172.05165100097656
INFO:root:current mean train loss 6517.401382130764
INFO:root:current train perplexity172.0203094482422
INFO:root:current mean train loss 6520.766999946285
INFO:root:current train perplexity172.14430236816406
INFO:root:current mean train loss 6520.5555399995565
INFO:root:current train perplexity171.95372009277344
INFO:root:current mean train loss 6522.913334770025
INFO:root:current train perplexity172.04188537597656
INFO:root:current mean train loss 6522.522962267907
INFO:root:current train perplexity171.99496459960938
INFO:root:current mean train loss 6522.36905691148
INFO:root:current train perplexity171.97996520996094
INFO:root:current mean train loss 6524.2572086939435
INFO:root:current train perplexity172.11090087890625
INFO:root:current mean train loss 6524.672704489834
INFO:root:current train perplexity172.1580047607422

100%|██████████| 1/1 [07:40<00:00, 460.96s/it][A100%|██████████| 1/1 [07:40<00:00, 460.96s/it]
INFO:root:final mean train loss: 6524.047108183229
INFO:root:final train perplexity: 172.21844482421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.04s/it][A100%|██████████| 1/1 [00:39<00:00, 39.06s/it]
INFO:root:eval mean loss: 6300.732901498781
INFO:root:eval perplexity: 163.8000030517578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.21s/it][A100%|██████████| 1/1 [00:38<00:00, 38.21s/it]
INFO:root:eval mean loss: 6433.655914090204
INFO:root:eval perplexity: 198.20330810546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/31
 16%|█▌        | 31/200 [4:44:27<25:32:39, 544.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6349.066143329327
INFO:root:current train perplexity168.62710571289062
INFO:root:current mean train loss 6526.641617063492
INFO:root:current train perplexity171.72134399414062
INFO:root:current mean train loss 6541.214884800193
INFO:root:current train perplexity172.05262756347656
INFO:root:current mean train loss 6532.143076890817
INFO:root:current train perplexity172.8424530029297
INFO:root:current mean train loss 6543.908810611062
INFO:root:current train perplexity173.3452606201172
INFO:root:current mean train loss 6544.234692475642
INFO:root:current train perplexity172.72076416015625
INFO:root:current mean train loss 6541.986996586711
INFO:root:current train perplexity172.84393310546875
INFO:root:current mean train loss 6536.962715758436
INFO:root:current train perplexity172.8787078857422
INFO:root:current mean train loss 6533.388532957211
INFO:root:current train perplexity172.6209716796875
INFO:root:current mean train loss 6537.185158253746
INFO:root:current train perplexity172.5697784423828
INFO:root:current mean train loss 6537.266668665478
INFO:root:current train perplexity172.4832305908203
INFO:root:current mean train loss 6534.552001953125
INFO:root:current train perplexity172.55430603027344
INFO:root:current mean train loss 6538.662858126019
INFO:root:current train perplexity172.5591583251953
INFO:root:current mean train loss 6533.1322678786055
INFO:root:current train perplexity172.43035888671875
INFO:root:current mean train loss 6531.765888315766
INFO:root:current train perplexity172.32235717773438
INFO:root:current mean train loss 6532.039855397075
INFO:root:current train perplexity172.41046142578125
INFO:root:current mean train loss 6526.934429473689
INFO:root:current train perplexity172.25958251953125
INFO:root:current mean train loss 6526.495499099254
INFO:root:current train perplexity172.23452758789062
INFO:root:current mean train loss 6525.067711453056
INFO:root:current train perplexity172.2398681640625
INFO:root:current mean train loss 6525.85481948298
INFO:root:current train perplexity172.388916015625

100%|██████████| 1/1 [07:41<00:00, 461.44s/it][A100%|██████████| 1/1 [07:41<00:00, 461.44s/it]
INFO:root:final mean train loss: 6525.946234275521
INFO:root:final train perplexity: 172.4767303466797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:42<00:00, 42.89s/it][A100%|██████████| 1/1 [00:42<00:00, 42.89s/it]
INFO:root:eval mean loss: 6300.618129432624
INFO:root:eval perplexity: 163.78468322753906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.41s/it][A100%|██████████| 1/1 [00:39<00:00, 39.41s/it]
INFO:root:eval mean loss: 6433.374410426363
INFO:root:eval perplexity: 198.157470703125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/32
 16%|█▌        | 32/200 [4:53:34<25:25:17, 544.75s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6547.225744912791
INFO:root:current train perplexity172.27174377441406
INFO:root:current mean train loss 6547.456239073426
INFO:root:current train perplexity171.54637145996094
INFO:root:current mean train loss 6545.858334137089
INFO:root:current train perplexity171.8258514404297
INFO:root:current mean train loss 6551.454091597577
INFO:root:current train perplexity172.46324157714844
INFO:root:current mean train loss 6531.770136366041
INFO:root:current train perplexity171.6941680908203
INFO:root:current mean train loss 6537.647558953441
INFO:root:current train perplexity171.92019653320312
INFO:root:current mean train loss 6535.021426662131
INFO:root:current train perplexity171.80184936523438
INFO:root:current mean train loss 6535.745626498359
INFO:root:current train perplexity171.982177734375
INFO:root:current mean train loss 6523.7658705886715
INFO:root:current train perplexity171.96978759765625
INFO:root:current mean train loss 6527.847913594413
INFO:root:current train perplexity172.21572875976562
INFO:root:current mean train loss 6527.151802567713
INFO:root:current train perplexity172.22525024414062
INFO:root:current mean train loss 6525.536165279144
INFO:root:current train perplexity172.30213928222656
INFO:root:current mean train loss 6523.174651878646
INFO:root:current train perplexity172.3042755126953
INFO:root:current mean train loss 6522.001560827555
INFO:root:current train perplexity172.4235382080078
INFO:root:current mean train loss 6522.193159731246
INFO:root:current train perplexity172.48619079589844
INFO:root:current mean train loss 6523.925027467798
INFO:root:current train perplexity172.4528045654297
INFO:root:current mean train loss 6524.198021376198
INFO:root:current train perplexity172.50067138671875
INFO:root:current mean train loss 6524.8904557964
INFO:root:current train perplexity172.3893280029297
INFO:root:current mean train loss 6528.487841982332
INFO:root:current train perplexity172.5556182861328
INFO:root:current mean train loss 6529.908090792669
INFO:root:current train perplexity172.67027282714844

100%|██████████| 1/1 [07:39<00:00, 459.80s/it][A100%|██████████| 1/1 [07:39<00:00, 459.80s/it]
INFO:root:final mean train loss: 6527.505962546883
INFO:root:final train perplexity: 172.68922424316406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.32s/it][A100%|██████████| 1/1 [00:39<00:00, 39.32s/it]
INFO:root:eval mean loss: 6298.234710909796
INFO:root:eval perplexity: 163.4691619873047
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.23s/it][A100%|██████████| 1/1 [00:37<00:00, 37.23s/it]
INFO:root:eval mean loss: 6433.217869535406
INFO:root:eval perplexity: 198.13177490234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/33
 16%|█▋        | 33/200 [5:02:32<25:11:13, 542.95s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6450.953784179687
INFO:root:current train perplexity173.11570739746094
INFO:root:current mean train loss 6512.984991455078
INFO:root:current train perplexity173.00851440429688
INFO:root:current mean train loss 6508.061345027043
INFO:root:current train perplexity172.4080810546875
INFO:root:current mean train loss 6514.055045572916
INFO:root:current train perplexity172.4283905029297
INFO:root:current mean train loss 6514.0835120159645
INFO:root:current train perplexity172.50091552734375
INFO:root:current mean train loss 6510.338756452288
INFO:root:current train perplexity172.4331512451172
INFO:root:current mean train loss 6501.778047318892
INFO:root:current train perplexity171.90780639648438
INFO:root:current mean train loss 6507.442590332032
INFO:root:current train perplexity172.06739807128906
INFO:root:current mean train loss 6517.924637195676
INFO:root:current train perplexity172.70355224609375
INFO:root:current mean train loss 6523.289520772299
INFO:root:current train perplexity172.80947875976562
INFO:root:current mean train loss 6528.361451577241
INFO:root:current train perplexity172.80380249023438
INFO:root:current mean train loss 6525.249988213901
INFO:root:current train perplexity172.67950439453125
INFO:root:current mean train loss 6527.243174138145
INFO:root:current train perplexity172.71351623535156
INFO:root:current mean train loss 6533.435489789177
INFO:root:current train perplexity172.8203582763672
INFO:root:current mean train loss 6535.509626832727
INFO:root:current train perplexity172.97535705566406
INFO:root:current mean train loss 6534.948648775541
INFO:root:current train perplexity172.97164916992188
INFO:root:current mean train loss 6534.684412356457
INFO:root:current train perplexity172.93321228027344
INFO:root:current mean train loss 6532.606510231712
INFO:root:current train perplexity172.88868713378906
INFO:root:current mean train loss 6532.580843098958
INFO:root:current train perplexity172.893310546875
INFO:root:current mean train loss 6529.691854920679
INFO:root:current train perplexity172.7670440673828

100%|██████████| 1/1 [07:37<00:00, 457.73s/it][A100%|██████████| 1/1 [07:37<00:00, 457.73s/it]
INFO:root:final mean train loss: 6527.608244541493
INFO:root:final train perplexity: 172.70321655273438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.04s/it][A100%|██████████| 1/1 [00:40<00:00, 40.04s/it]
INFO:root:eval mean loss: 6298.219582848515
INFO:root:eval perplexity: 163.46713256835938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.93s/it][A100%|██████████| 1/1 [00:38<00:00, 38.93s/it]
INFO:root:eval mean loss: 6434.035878282913
INFO:root:eval perplexity: 198.26512145996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/34
 17%|█▋        | 34/200 [5:11:31<24:58:52, 541.76s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6528.887105570211
INFO:root:current train perplexity170.3153076171875
INFO:root:current mean train loss 6522.045823953919
INFO:root:current train perplexity172.01515197753906
INFO:root:current mean train loss 6529.0185705522335
INFO:root:current train perplexity173.09689331054688
INFO:root:current mean train loss 6531.051831637516
INFO:root:current train perplexity173.25784301757812
INFO:root:current mean train loss 6533.578863051952
INFO:root:current train perplexity173.12306213378906
INFO:root:current mean train loss 6530.216187581239
INFO:root:current train perplexity172.78121948242188
INFO:root:current mean train loss 6524.545413041221
INFO:root:current train perplexity172.3751983642578
INFO:root:current mean train loss 6520.956310453869
INFO:root:current train perplexity172.1281280517578
INFO:root:current mean train loss 6521.4450124046825
INFO:root:current train perplexity171.90658569335938
INFO:root:current mean train loss 6525.55532571408
INFO:root:current train perplexity171.7496795654297
INFO:root:current mean train loss 6525.032802797847
INFO:root:current train perplexity171.64422607421875
INFO:root:current mean train loss 6526.636426693925
INFO:root:current train perplexity171.80947875976562
INFO:root:current mean train loss 6522.7148475736585
INFO:root:current train perplexity171.81971740722656
INFO:root:current mean train loss 6521.097409449891
INFO:root:current train perplexity171.60069274902344
INFO:root:current mean train loss 6523.784007450174
INFO:root:current train perplexity171.94552612304688
INFO:root:current mean train loss 6525.54110108196
INFO:root:current train perplexity172.02703857421875
INFO:root:current mean train loss 6527.700844607186
INFO:root:current train perplexity172.26705932617188
INFO:root:current mean train loss 6523.7683241484065
INFO:root:current train perplexity172.12640380859375
INFO:root:current mean train loss 6523.756000630578
INFO:root:current train perplexity172.1522674560547
INFO:root:current mean train loss 6526.338852368645
INFO:root:current train perplexity172.25038146972656

100%|██████████| 1/1 [07:43<00:00, 463.52s/it][A100%|██████████| 1/1 [07:43<00:00, 463.52s/it]
INFO:root:final mean train loss: 6524.095713713045
INFO:root:final train perplexity: 172.22508239746094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.66s/it][A100%|██████████| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 6301.929081477172
INFO:root:eval perplexity: 163.95855712890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.10s/it][A100%|██████████| 1/1 [00:39<00:00, 39.10s/it]
INFO:root:eval mean loss: 6434.535783050754
INFO:root:eval perplexity: 198.3467254638672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/35
 18%|█▊        | 35/200 [5:20:36<24:52:16, 542.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6507.10509474734
INFO:root:current train perplexity171.2443084716797
INFO:root:current mean train loss 6528.120303439111
INFO:root:current train perplexity173.1981964111328
INFO:root:current mean train loss 6533.139490659545
INFO:root:current train perplexity174.09210205078125
INFO:root:current mean train loss 6532.246164389673
INFO:root:current train perplexity173.7831573486328
INFO:root:current mean train loss 6531.7020841899675
INFO:root:current train perplexity173.5349884033203
INFO:root:current mean train loss 6531.731888382523
INFO:root:current train perplexity173.0077667236328
INFO:root:current mean train loss 6532.6192630471005
INFO:root:current train perplexity173.1192626953125
INFO:root:current mean train loss 6528.209734015861
INFO:root:current train perplexity172.86297607421875
INFO:root:current mean train loss 6533.041225356544
INFO:root:current train perplexity173.01718139648438
INFO:root:current mean train loss 6534.142891528861
INFO:root:current train perplexity173.11497497558594
INFO:root:current mean train loss 6536.28283401294
INFO:root:current train perplexity173.150634765625
INFO:root:current mean train loss 6541.2281798805225
INFO:root:current train perplexity173.43373107910156
INFO:root:current mean train loss 6544.283791402024
INFO:root:current train perplexity173.45077514648438
INFO:root:current mean train loss 6541.506276550731
INFO:root:current train perplexity173.32452392578125
INFO:root:current mean train loss 6538.478448952058
INFO:root:current train perplexity173.35328674316406
INFO:root:current mean train loss 6539.663826323812
INFO:root:current train perplexity173.40802001953125
INFO:root:current mean train loss 6542.843977422613
INFO:root:current train perplexity173.52149963378906
INFO:root:current mean train loss 6539.099611280222
INFO:root:current train perplexity173.47640991210938
INFO:root:current mean train loss 6536.7327974957925
INFO:root:current train perplexity173.44944763183594

100%|██████████| 1/1 [07:42<00:00, 462.07s/it][A100%|██████████| 1/1 [07:42<00:00, 462.07s/it]
INFO:root:final mean train loss: 6532.4342871438475
INFO:root:final train perplexity: 173.36212158203125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.12s/it][A100%|██████████| 1/1 [00:41<00:00, 41.13s/it]
INFO:root:eval mean loss: 6298.41682769559
INFO:root:eval perplexity: 163.49325561523438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.89s/it][A100%|██████████| 1/1 [00:41<00:00, 41.89s/it]
INFO:root:eval mean loss: 6432.658661970855
INFO:root:eval perplexity: 198.0408172607422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/36
 18%|█▊        | 36/200 [5:29:44<24:47:16, 544.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6557.08447265625
INFO:root:current train perplexity167.79150390625
INFO:root:current mean train loss 6614.228792757602
INFO:root:current train perplexity173.520751953125
INFO:root:current mean train loss 6580.470249555688
INFO:root:current train perplexity173.794677734375
INFO:root:current mean train loss 6567.3363771101285
INFO:root:current train perplexity174.38955688476562
INFO:root:current mean train loss 6561.168748099149
INFO:root:current train perplexity174.2465057373047
INFO:root:current mean train loss 6560.58653566842
INFO:root:current train perplexity175.01458740234375
INFO:root:current mean train loss 6548.96035971384
INFO:root:current train perplexity174.49868774414062
INFO:root:current mean train loss 6545.767242989627
INFO:root:current train perplexity174.04388427734375
INFO:root:current mean train loss 6540.9968216563275
INFO:root:current train perplexity173.52447509765625
INFO:root:current mean train loss 6546.881185253156
INFO:root:current train perplexity173.86495971679688
INFO:root:current mean train loss 6552.528365711548
INFO:root:current train perplexity173.82774353027344
INFO:root:current mean train loss 6546.177601646884
INFO:root:current train perplexity173.6090545654297
INFO:root:current mean train loss 6546.008363278024
INFO:root:current train perplexity173.47036743164062
INFO:root:current mean train loss 6553.992786398741
INFO:root:current train perplexity173.781494140625
INFO:root:current mean train loss 6547.847278705816
INFO:root:current train perplexity173.64515686035156
INFO:root:current mean train loss 6540.276482229277
INFO:root:current train perplexity173.51785278320312
INFO:root:current mean train loss 6538.3563437766725
INFO:root:current train perplexity173.42727661132812
INFO:root:current mean train loss 6539.995905971471
INFO:root:current train perplexity173.52191162109375
INFO:root:current mean train loss 6536.58162843819
INFO:root:current train perplexity173.39569091796875
INFO:root:current mean train loss 6535.326412566228
INFO:root:current train perplexity173.4508514404297

100%|██████████| 1/1 [07:44<00:00, 464.12s/it][A100%|██████████| 1/1 [07:44<00:00, 464.12s/it]
INFO:root:final mean train loss: 6532.388062323696
INFO:root:final train perplexity: 173.35585021972656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.39s/it][A100%|██████████| 1/1 [00:39<00:00, 39.39s/it]
INFO:root:eval mean loss: 6295.46375290891
INFO:root:eval perplexity: 163.1029815673828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.89s/it][A100%|██████████| 1/1 [00:37<00:00, 37.89s/it]
INFO:root:eval mean loss: 6429.857895438553
INFO:root:eval perplexity: 197.58522033691406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/37
 18%|█▊        | 37/200 [5:38:48<24:37:56, 544.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6530.6492919921875
INFO:root:current train perplexity170.21170043945312
INFO:root:current mean train loss 6543.211784362793
INFO:root:current train perplexity172.86041259765625
INFO:root:current mean train loss 6516.546260365269
INFO:root:current train perplexity172.75057983398438
INFO:root:current mean train loss 6519.258337997809
INFO:root:current train perplexity173.33328247070312
INFO:root:current mean train loss 6512.086404105213
INFO:root:current train perplexity173.02633666992188
INFO:root:current mean train loss 6518.674591989228
INFO:root:current train perplexity173.11819458007812
INFO:root:current mean train loss 6525.061803343949
INFO:root:current train perplexity173.07989501953125
INFO:root:current mean train loss 6528.429527198875
INFO:root:current train perplexity173.31973266601562
INFO:root:current mean train loss 6532.297319642587
INFO:root:current train perplexity173.23239135742188
INFO:root:current mean train loss 6536.872492296942
INFO:root:current train perplexity173.49478149414062
INFO:root:current mean train loss 6533.791659225286
INFO:root:current train perplexity173.34857177734375
INFO:root:current mean train loss 6524.825078436669
INFO:root:current train perplexity172.99053955078125
INFO:root:current mean train loss 6528.380468511426
INFO:root:current train perplexity173.18341064453125
INFO:root:current mean train loss 6526.435221109045
INFO:root:current train perplexity173.21124267578125
INFO:root:current mean train loss 6528.6099962934395
INFO:root:current train perplexity173.26089477539062
INFO:root:current mean train loss 6524.228889824833
INFO:root:current train perplexity173.16004943847656
INFO:root:current mean train loss 6528.159706359414
INFO:root:current train perplexity173.23751831054688
INFO:root:current mean train loss 6528.383811950684
INFO:root:current train perplexity173.2313995361328
INFO:root:current mean train loss 6531.498273119102
INFO:root:current train perplexity173.3020477294922
INFO:root:current mean train loss 6532.703459047183
INFO:root:current train perplexity173.32435607910156

100%|██████████| 1/1 [07:42<00:00, 462.80s/it][A100%|██████████| 1/1 [07:42<00:00, 462.80s/it]
INFO:root:final mean train loss: 6532.210196336832
INFO:root:final train perplexity: 173.3314666748047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.57s/it][A100%|██████████| 1/1 [00:39<00:00, 39.57s/it]
INFO:root:eval mean loss: 6296.98645106106
INFO:root:eval perplexity: 163.3040771484375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.86s/it][A100%|██████████| 1/1 [00:38<00:00, 38.86s/it]
INFO:root:eval mean loss: 6432.045228349401
INFO:root:eval perplexity: 197.94093322753906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/38
 19%|█▉        | 38/200 [5:47:51<24:28:35, 543.92s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6466.502734375
INFO:root:current train perplexity169.87164306640625
INFO:root:current mean train loss 6502.532735048491
INFO:root:current train perplexity172.28611755371094
INFO:root:current mean train loss 6491.381612723214
INFO:root:current train perplexity172.39212036132812
INFO:root:current mean train loss 6506.342286571558
INFO:root:current train perplexity173.33889770507812
INFO:root:current mean train loss 6510.72689058111
INFO:root:current train perplexity173.42893981933594
INFO:root:current mean train loss 6512.479245807052
INFO:root:current train perplexity173.0368194580078
INFO:root:current mean train loss 6507.801903918363
INFO:root:current train perplexity172.87220764160156
INFO:root:current mean train loss 6512.565022021812
INFO:root:current train perplexity173.08253479003906
INFO:root:current mean train loss 6518.453167182877
INFO:root:current train perplexity173.2066192626953
INFO:root:current mean train loss 6511.523380146329
INFO:root:current train perplexity172.9163055419922
INFO:root:current mean train loss 6511.31149026241
INFO:root:current train perplexity172.93618774414062
INFO:root:current mean train loss 6517.448213189137
INFO:root:current train perplexity173.1979522705078
INFO:root:current mean train loss 6525.615348895582
INFO:root:current train perplexity173.43109130859375
INFO:root:current mean train loss 6529.016463246398
INFO:root:current train perplexity173.47483825683594
INFO:root:current mean train loss 6528.786505190312
INFO:root:current train perplexity173.470947265625
INFO:root:current mean train loss 6529.988455387844
INFO:root:current train perplexity173.56056213378906
INFO:root:current mean train loss 6532.984285358093
INFO:root:current train perplexity173.6203155517578
INFO:root:current mean train loss 6535.558185216691
INFO:root:current train perplexity173.79161071777344
INFO:root:current mean train loss 6535.134442221375
INFO:root:current train perplexity173.72930908203125
INFO:root:current mean train loss 6535.656312761086
INFO:root:current train perplexity173.68167114257812

100%|██████████| 1/1 [07:44<00:00, 464.77s/it][A100%|██████████| 1/1 [07:44<00:00, 464.77s/it]
INFO:root:final mean train loss: 6535.109381402074
INFO:root:final train perplexity: 173.7285614013672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.57s/it][A100%|██████████| 1/1 [00:41<00:00, 41.57s/it]
INFO:root:eval mean loss: 6295.5345606161345
INFO:root:eval perplexity: 163.11231994628906
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.99s/it][A100%|██████████| 1/1 [00:38<00:00, 38.99s/it]
INFO:root:eval mean loss: 6430.7817270265405
INFO:root:eval perplexity: 197.73536682128906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/39
 20%|█▉        | 39/200 [5:56:59<24:22:36, 545.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6518.775272492439
INFO:root:current train perplexity173.6534423828125
INFO:root:current mean train loss 6494.589711130401
INFO:root:current train perplexity173.75921630859375
INFO:root:current mean train loss 6473.100513254413
INFO:root:current train perplexity172.08447265625
INFO:root:current mean train loss 6469.497031196046
INFO:root:current train perplexity171.6026611328125
INFO:root:current mean train loss 6486.368830957454
INFO:root:current train perplexity172.07937622070312
INFO:root:current mean train loss 6493.252747233652
INFO:root:current train perplexity172.13616943359375
INFO:root:current mean train loss 6503.03309912552
INFO:root:current train perplexity172.35382080078125
INFO:root:current mean train loss 6501.961951228264
INFO:root:current train perplexity172.40553283691406
INFO:root:current mean train loss 6504.634480133411
INFO:root:current train perplexity172.30262756347656
INFO:root:current mean train loss 6511.293798625097
INFO:root:current train perplexity172.4996795654297
INFO:root:current mean train loss 6521.36574840366
INFO:root:current train perplexity172.74505615234375
INFO:root:current mean train loss 6516.379560513258
INFO:root:current train perplexity172.58969116210938
INFO:root:current mean train loss 6522.885752247177
INFO:root:current train perplexity172.95968627929688
INFO:root:current mean train loss 6521.501297781296
INFO:root:current train perplexity172.97872924804688
INFO:root:current mean train loss 6528.357850707507
INFO:root:current train perplexity173.21595764160156
INFO:root:current mean train loss 6531.104034228453
INFO:root:current train perplexity173.46234130859375
INFO:root:current mean train loss 6535.977638363982
INFO:root:current train perplexity173.66552734375
INFO:root:current mean train loss 6534.408846315001
INFO:root:current train perplexity173.5547637939453
INFO:root:current mean train loss 6533.113295672915
INFO:root:current train perplexity173.51976013183594
INFO:root:current mean train loss 6535.720160341409
INFO:root:current train perplexity173.68814086914062

100%|██████████| 1/1 [07:38<00:00, 458.17s/it][A100%|██████████| 1/1 [07:38<00:00, 458.17s/it]
INFO:root:final mean train loss: 6534.930622572137
INFO:root:final train perplexity: 173.70404052734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.98s/it][A100%|██████████| 1/1 [00:39<00:00, 40.00s/it]
INFO:root:eval mean loss: 6300.627746149158
INFO:root:eval perplexity: 163.78602600097656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.85s/it][A100%|██████████| 1/1 [00:37<00:00, 37.85s/it]
INFO:root:eval mean loss: 6434.212680248504
INFO:root:eval perplexity: 198.29396057128906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/40
 20%|██        | 40/200 [6:05:57<24:08:11, 543.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6551.904865506329
INFO:root:current train perplexity174.28256225585938
INFO:root:current mean train loss 6538.409858916725
INFO:root:current train perplexity172.82818603515625
INFO:root:current mean train loss 6537.370514462926
INFO:root:current train perplexity172.96792602539062
INFO:root:current mean train loss 6544.574661939314
INFO:root:current train perplexity173.16905212402344
INFO:root:current mean train loss 6544.058532587422
INFO:root:current train perplexity173.37469482421875
INFO:root:current mean train loss 6536.411050167314
INFO:root:current train perplexity172.92579650878906
INFO:root:current mean train loss 6525.310760453102
INFO:root:current train perplexity172.90550231933594
INFO:root:current mean train loss 6523.069238155889
INFO:root:current train perplexity172.87451171875
INFO:root:current mean train loss 6524.618422923777
INFO:root:current train perplexity173.0320281982422
INFO:root:current mean train loss 6531.088313070576
INFO:root:current train perplexity173.26824951171875
INFO:root:current mean train loss 6531.493667729814
INFO:root:current train perplexity173.46896362304688
INFO:root:current mean train loss 6528.858375245176
INFO:root:current train perplexity173.31146240234375
INFO:root:current mean train loss 6534.641139241473
INFO:root:current train perplexity173.7285614013672
INFO:root:current mean train loss 6533.177967716076
INFO:root:current train perplexity173.6955108642578
INFO:root:current mean train loss 6533.541718168949
INFO:root:current train perplexity173.5381317138672
INFO:root:current mean train loss 6533.456001499168
INFO:root:current train perplexity173.48898315429688
INFO:root:current mean train loss 6534.829853032869
INFO:root:current train perplexity173.60682678222656
INFO:root:current mean train loss 6538.1297749459845
INFO:root:current train perplexity173.7191925048828
INFO:root:current mean train loss 6538.480617910956
INFO:root:current train perplexity173.85501098632812
INFO:root:current mean train loss 6539.064832104283
INFO:root:current train perplexity173.98463439941406

100%|██████████| 1/1 [07:37<00:00, 457.74s/it][A100%|██████████| 1/1 [07:37<00:00, 457.74s/it]
INFO:root:final mean train loss: 6536.992368358587
INFO:root:final train perplexity: 173.98696899414062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.69s/it][A100%|██████████| 1/1 [00:41<00:00, 41.69s/it]
INFO:root:eval mean loss: 6306.252602435173
INFO:root:eval perplexity: 164.5332489013672
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 6440.004560754654
INFO:root:eval perplexity: 199.2404327392578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/41
 20%|██        | 41/200 [6:14:58<23:57:10, 542.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6542.731694539388
INFO:root:current train perplexity175.2488250732422
INFO:root:current mean train loss 6530.18718610491
INFO:root:current train perplexity175.93356323242188
INFO:root:current mean train loss 6534.0949674039275
INFO:root:current train perplexity176.10720825195312
INFO:root:current mean train loss 6530.749070292772
INFO:root:current train perplexity176.0104217529297
INFO:root:current mean train loss 6532.679429577243
INFO:root:current train perplexity175.904541015625
INFO:root:current mean train loss 6542.444294974308
INFO:root:current train perplexity175.7425537109375
INFO:root:current mean train loss 6531.374687808683
INFO:root:current train perplexity175.44422912597656
INFO:root:current mean train loss 6534.45831820234
INFO:root:current train perplexity175.25759887695312
INFO:root:current mean train loss 6535.769736153738
INFO:root:current train perplexity175.21524047851562
INFO:root:current mean train loss 6542.557328925075
INFO:root:current train perplexity175.3238067626953
INFO:root:current mean train loss 6541.670542473341
INFO:root:current train perplexity175.32957458496094
INFO:root:current mean train loss 6539.575904055184
INFO:root:current train perplexity175.24288940429688
INFO:root:current mean train loss 6538.215801851249
INFO:root:current train perplexity175.0852813720703
INFO:root:current mean train loss 6542.065975331304
INFO:root:current train perplexity175.2612762451172
INFO:root:current mean train loss 6540.3203621114635
INFO:root:current train perplexity174.96885681152344
INFO:root:current mean train loss 6542.367899423853
INFO:root:current train perplexity175.09454345703125
INFO:root:current mean train loss 6544.505118603976
INFO:root:current train perplexity175.24581909179688
INFO:root:current mean train loss 6547.183106828107
INFO:root:current train perplexity175.34829711914062
INFO:root:current mean train loss 6544.383366194455
INFO:root:current train perplexity175.17213439941406

100%|██████████| 1/1 [07:38<00:00, 458.15s/it][A100%|██████████| 1/1 [07:38<00:00, 458.15s/it]
INFO:root:final mean train loss: 6546.01183435668
INFO:root:final train perplexity: 175.2298583984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.74s/it][A100%|██████████| 1/1 [00:40<00:00, 40.75s/it]
INFO:root:eval mean loss: 6302.22885326629
INFO:root:eval perplexity: 163.99835205078125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.50s/it][A100%|██████████| 1/1 [00:38<00:00, 38.50s/it]
INFO:root:eval mean loss: 6439.78791019088
INFO:root:eval perplexity: 199.20498657226562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/42
 21%|██        | 42/200 [6:23:58<23:46:04, 541.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6401.949782151442
INFO:root:current train perplexity172.11656188964844
INFO:root:current mean train loss 6543.038673603429
INFO:root:current train perplexity174.33059692382812
INFO:root:current mean train loss 6570.753532588762
INFO:root:current train perplexity176.23086547851562
INFO:root:current mean train loss 6559.160906611921
INFO:root:current train perplexity175.57403564453125
INFO:root:current mean train loss 6567.866868284655
INFO:root:current train perplexity175.729736328125
INFO:root:current mean train loss 6552.684211478131
INFO:root:current train perplexity175.38400268554688
INFO:root:current mean train loss 6544.335461963448
INFO:root:current train perplexity175.0739288330078
INFO:root:current mean train loss 6542.797512573414
INFO:root:current train perplexity175.0318603515625
INFO:root:current mean train loss 6537.727032162899
INFO:root:current train perplexity174.96009826660156
INFO:root:current mean train loss 6546.150767131024
INFO:root:current train perplexity175.1824951171875
INFO:root:current mean train loss 6546.663405995496
INFO:root:current train perplexity175.26620483398438
INFO:root:current mean train loss 6541.752826152572
INFO:root:current train perplexity175.11375427246094
INFO:root:current mean train loss 6545.217128568116
INFO:root:current train perplexity175.29881286621094
INFO:root:current mean train loss 6547.536860585848
INFO:root:current train perplexity175.22560119628906
INFO:root:current mean train loss 6550.317641294011
INFO:root:current train perplexity175.2310333251953
INFO:root:current mean train loss 6547.303881981164
INFO:root:current train perplexity175.12411499023438
INFO:root:current mean train loss 6546.151765259318
INFO:root:current train perplexity175.07717895507812
INFO:root:current mean train loss 6545.446564985588
INFO:root:current train perplexity175.0609893798828
INFO:root:current mean train loss 6548.834972787679
INFO:root:current train perplexity175.16612243652344
INFO:root:current mean train loss 6548.197628071092
INFO:root:current train perplexity175.09095764160156

100%|██████████| 1/1 [07:38<00:00, 458.83s/it][A100%|██████████| 1/1 [07:38<00:00, 458.83s/it]
INFO:root:final mean train loss: 6544.339993460036
INFO:root:final train perplexity: 174.9987335205078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.14s/it][A100%|██████████| 1/1 [00:39<00:00, 39.14s/it]
INFO:root:eval mean loss: 6301.345297955452
INFO:root:eval perplexity: 163.88116455078125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.96s/it][A100%|██████████| 1/1 [00:37<00:00, 37.96s/it]
INFO:root:eval mean loss: 6438.936609146443
INFO:root:eval perplexity: 199.0655975341797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/43
 22%|██▏       | 43/200 [6:32:56<23:34:33, 540.60s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6758.086686197917
INFO:root:current train perplexity182.31503295898438
INFO:root:current mean train loss 6596.627001953125
INFO:root:current train perplexity176.87599182128906
INFO:root:current mean train loss 6573.422911005435
INFO:root:current train perplexity176.14727783203125
INFO:root:current mean train loss 6558.828197502367
INFO:root:current train perplexity176.34056091308594
INFO:root:current mean train loss 6556.205626589753
INFO:root:current train perplexity175.7793426513672
INFO:root:current mean train loss 6543.685861954599
INFO:root:current train perplexity175.31527709960938
INFO:root:current mean train loss 6547.8729802207345
INFO:root:current train perplexity175.4180450439453
INFO:root:current mean train loss 6551.338934744221
INFO:root:current train perplexity175.49375915527344
INFO:root:current mean train loss 6551.2414956701805
INFO:root:current train perplexity175.25567626953125
INFO:root:current mean train loss 6556.446463898689
INFO:root:current train perplexity175.3742218017578
INFO:root:current mean train loss 6551.222673316141
INFO:root:current train perplexity175.21849060058594
INFO:root:current mean train loss 6548.172138153346
INFO:root:current train perplexity175.23312377929688
INFO:root:current mean train loss 6548.232291666666
INFO:root:current train perplexity175.28684997558594
INFO:root:current mean train loss 6549.890022908835
INFO:root:current train perplexity175.28025817871094
INFO:root:current mean train loss 6548.124927952907
INFO:root:current train perplexity175.26937866210938
INFO:root:current mean train loss 6551.289730775123
INFO:root:current train perplexity175.28785705566406
INFO:root:current mean train loss 6549.313042800997
INFO:root:current train perplexity175.17848205566406
INFO:root:current mean train loss 6548.610241769779
INFO:root:current train perplexity175.1985321044922
INFO:root:current mean train loss 6550.546413934426
INFO:root:current train perplexity175.2744903564453
INFO:root:current mean train loss 6549.885927633177
INFO:root:current train perplexity175.39730834960938

100%|██████████| 1/1 [07:43<00:00, 463.21s/it][A100%|██████████| 1/1 [07:43<00:00, 463.21s/it]
INFO:root:final mean train loss: 6547.497294877553
INFO:root:final train perplexity: 175.43536376953125
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.36s/it][A100%|██████████| 1/1 [00:40<00:00, 40.36s/it]
INFO:root:eval mean loss: 6307.767969442598
INFO:root:eval perplexity: 164.73509216308594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.84s/it][A100%|██████████| 1/1 [00:38<00:00, 38.84s/it]
INFO:root:eval mean loss: 6445.240029192985
INFO:root:eval perplexity: 200.09979248046875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/44
 22%|██▏       | 44/200 [6:42:01<23:28:47, 541.85s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6492.957166306516
INFO:root:current train perplexity174.42730712890625
INFO:root:current mean train loss 6534.160980017007
INFO:root:current train perplexity175.16102600097656
INFO:root:current mean train loss 6517.591188006073
INFO:root:current train perplexity174.30416870117188
INFO:root:current mean train loss 6520.772027535122
INFO:root:current train perplexity173.9540252685547
INFO:root:current mean train loss 6525.03909308585
INFO:root:current train perplexity174.65269470214844
INFO:root:current mean train loss 6539.761701789591
INFO:root:current train perplexity175.10581970214844
INFO:root:current mean train loss 6534.844314504444
INFO:root:current train perplexity174.81483459472656
INFO:root:current mean train loss 6539.564626997573
INFO:root:current train perplexity175.04379272460938
INFO:root:current mean train loss 6541.80637371329
INFO:root:current train perplexity175.02000427246094
INFO:root:current mean train loss 6549.2460009404695
INFO:root:current train perplexity174.9131317138672
INFO:root:current mean train loss 6550.226981293278
INFO:root:current train perplexity174.91171264648438
INFO:root:current mean train loss 6554.658043060702
INFO:root:current train perplexity175.0019073486328
INFO:root:current mean train loss 6555.629675674744
INFO:root:current train perplexity175.0691680908203
INFO:root:current mean train loss 6554.451803341917
INFO:root:current train perplexity175.32313537597656
INFO:root:current mean train loss 6550.812871525678
INFO:root:current train perplexity175.20370483398438
INFO:root:current mean train loss 6549.916633314984
INFO:root:current train perplexity175.13772583007812
INFO:root:current mean train loss 6550.673410699378
INFO:root:current train perplexity175.1892547607422
INFO:root:current mean train loss 6550.613815368757
INFO:root:current train perplexity175.13037109375
INFO:root:current mean train loss 6548.68157030404
INFO:root:current train perplexity175.12026977539062
INFO:root:current mean train loss 6546.908201871068
INFO:root:current train perplexity175.0724334716797

100%|██████████| 1/1 [07:43<00:00, 463.69s/it][A100%|██████████| 1/1 [07:43<00:00, 463.69s/it]
INFO:root:final mean train loss: 6544.995994764088
INFO:root:final train perplexity: 175.08937072753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.15s/it][A100%|██████████| 1/1 [00:41<00:00, 41.15s/it]
INFO:root:eval mean loss: 6317.716975218861
INFO:root:eval perplexity: 166.06663513183594
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 6453.2633645348515
INFO:root:eval perplexity: 201.42413330078125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/45
 22%|██▎       | 45/200 [6:51:07<23:23:00, 543.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6523.410232543945
INFO:root:current train perplexity173.92599487304688
INFO:root:current mean train loss 6569.8187672684835
INFO:root:current train perplexity176.50680541992188
INFO:root:current mean train loss 6575.5549427379265
INFO:root:current train perplexity176.4822235107422
INFO:root:current mean train loss 6572.263903942737
INFO:root:current train perplexity176.8420867919922
INFO:root:current mean train loss 6562.660356192753
INFO:root:current train perplexity176.46295166015625
INFO:root:current mean train loss 6557.260306716811
INFO:root:current train perplexity176.1456756591797
INFO:root:current mean train loss 6546.764564606081
INFO:root:current train perplexity175.6591033935547
INFO:root:current mean train loss 6540.564745838105
INFO:root:current train perplexity175.2729034423828
INFO:root:current mean train loss 6544.072670830621
INFO:root:current train perplexity175.3515625
INFO:root:current mean train loss 6545.143249258461
INFO:root:current train perplexity175.37547302246094
INFO:root:current mean train loss 6542.851878689644
INFO:root:current train perplexity175.25242614746094
INFO:root:current mean train loss 6545.37647910626
INFO:root:current train perplexity175.24716186523438
INFO:root:current mean train loss 6543.762282745748
INFO:root:current train perplexity175.2042999267578
INFO:root:current mean train loss 6542.561562457043
INFO:root:current train perplexity175.1743927001953
INFO:root:current mean train loss 6544.053000194779
INFO:root:current train perplexity175.23329162597656
INFO:root:current mean train loss 6542.105013874181
INFO:root:current train perplexity175.12168884277344
INFO:root:current mean train loss 6544.654864678016
INFO:root:current train perplexity175.21949768066406
INFO:root:current mean train loss 6542.477981394381
INFO:root:current train perplexity175.12026977539062
INFO:root:current mean train loss 6545.728338020554
INFO:root:current train perplexity175.23394775390625
INFO:root:current mean train loss 6547.743655575754
INFO:root:current train perplexity175.24591064453125

100%|██████████| 1/1 [07:32<00:00, 452.47s/it][A100%|██████████| 1/1 [07:32<00:00, 452.47s/it]
INFO:root:final mean train loss: 6546.017721925428
INFO:root:final train perplexity: 175.23060607910156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.77s/it][A100%|██████████| 1/1 [00:38<00:00, 38.77s/it]
INFO:root:eval mean loss: 6305.88367478391
INFO:root:eval perplexity: 164.48414611816406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.90s/it][A100%|██████████| 1/1 [00:38<00:00, 38.90s/it]
INFO:root:eval mean loss: 6442.479055851064
INFO:root:eval perplexity: 199.6463165283203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/46
 23%|██▎       | 46/200 [6:59:59<23:05:45, 539.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6557.407039689429
INFO:root:current train perplexity177.41017150878906
INFO:root:current mean train loss 6487.40518171616
INFO:root:current train perplexity174.5365447998047
INFO:root:current mean train loss 6530.808635453736
INFO:root:current train perplexity175.65048217773438
INFO:root:current mean train loss 6543.112778871391
INFO:root:current train perplexity175.8388671875
INFO:root:current mean train loss 6538.457350003248
INFO:root:current train perplexity176.047607421875
INFO:root:current mean train loss 6537.694739336812
INFO:root:current train perplexity175.9089813232422
INFO:root:current mean train loss 6545.365521177497
INFO:root:current train perplexity175.95301818847656
INFO:root:current mean train loss 6544.978477487796
INFO:root:current train perplexity175.82058715820312
INFO:root:current mean train loss 6553.547009124929
INFO:root:current train perplexity175.861083984375
INFO:root:current mean train loss 6562.281185791762
INFO:root:current train perplexity175.9954833984375
INFO:root:current mean train loss 6566.021496119045
INFO:root:current train perplexity176.23028564453125
INFO:root:current mean train loss 6561.400702777704
INFO:root:current train perplexity176.02610778808594
INFO:root:current mean train loss 6555.267371148639
INFO:root:current train perplexity175.7355194091797
INFO:root:current mean train loss 6553.830677073905
INFO:root:current train perplexity175.68540954589844
INFO:root:current mean train loss 6553.467432860504
INFO:root:current train perplexity175.68516540527344
INFO:root:current mean train loss 6551.336278154155
INFO:root:current train perplexity175.65969848632812
INFO:root:current mean train loss 6550.516299182499
INFO:root:current train perplexity175.6166534423828
INFO:root:current mean train loss 6550.584063059289
INFO:root:current train perplexity175.59596252441406
INFO:root:current mean train loss 6549.799753289473
INFO:root:current train perplexity175.45970153808594
INFO:root:current mean train loss 6549.877620105849
INFO:root:current train perplexity175.4834747314453

100%|██████████| 1/1 [07:39<00:00, 459.52s/it][A100%|██████████| 1/1 [07:39<00:00, 459.52s/it]
INFO:root:final mean train loss: 6547.750988627947
INFO:root:final train perplexity: 175.4704132080078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.17s/it][A100%|██████████| 1/1 [00:40<00:00, 40.17s/it]
INFO:root:eval mean loss: 6305.302994099069
INFO:root:eval perplexity: 164.4069061279297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.36s/it][A100%|██████████| 1/1 [00:38<00:00, 38.36s/it]
INFO:root:eval mean loss: 6442.9111440672095
INFO:root:eval perplexity: 199.7171630859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/47
 24%|██▎       | 47/200 [7:08:59<22:56:45, 539.91s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6577.980722855548
INFO:root:current train perplexity176.8151092529297
INFO:root:current mean train loss 6544.38385318024
INFO:root:current train perplexity175.58859252929688
INFO:root:current mean train loss 6550.940568962354
INFO:root:current train perplexity175.13621520996094
INFO:root:current mean train loss 6565.203058750785
INFO:root:current train perplexity175.23170471191406
INFO:root:current mean train loss 6570.662881996737
INFO:root:current train perplexity175.1637725830078
INFO:root:current mean train loss 6569.588759406354
INFO:root:current train perplexity175.1448211669922
INFO:root:current mean train loss 6560.450910245791
INFO:root:current train perplexity175.21273803710938
INFO:root:current mean train loss 6562.6118824894265
INFO:root:current train perplexity175.22125244140625
INFO:root:current mean train loss 6559.06702339835
INFO:root:current train perplexity175.28744506835938
INFO:root:current mean train loss 6554.841412806081
INFO:root:current train perplexity175.26980590820312
INFO:root:current mean train loss 6552.586434230561
INFO:root:current train perplexity175.12887573242188
INFO:root:current mean train loss 6546.808492262495
INFO:root:current train perplexity175.03953552246094
INFO:root:current mean train loss 6541.991030371244
INFO:root:current train perplexity174.92764282226562
INFO:root:current mean train loss 6544.669339639776
INFO:root:current train perplexity175.08294677734375
INFO:root:current mean train loss 6541.62246569645
INFO:root:current train perplexity174.98011779785156
INFO:root:current mean train loss 6543.211001361566
INFO:root:current train perplexity174.97186279296875
INFO:root:current mean train loss 6542.747227034102
INFO:root:current train perplexity174.95343017578125
INFO:root:current mean train loss 6546.441101006327
INFO:root:current train perplexity175.07501220703125
INFO:root:current mean train loss 6546.553070615038
INFO:root:current train perplexity175.16787719726562

100%|██████████| 1/1 [07:41<00:00, 461.54s/it][A100%|██████████| 1/1 [07:41<00:00, 461.54s/it]
INFO:root:final mean train loss: 6545.665309796355
INFO:root:final train perplexity: 175.18182373046875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.78s/it][A100%|██████████| 1/1 [00:38<00:00, 38.78s/it]
INFO:root:eval mean loss: 6308.715241993573
INFO:root:eval perplexity: 164.8613739013672
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.48s/it][A100%|██████████| 1/1 [00:37<00:00, 37.48s/it]
INFO:root:eval mean loss: 6445.870219345634
INFO:root:eval perplexity: 200.2035369873047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/48
 24%|██▍       | 48/200 [7:17:59<22:47:54, 539.96s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6638.140787760417
INFO:root:current train perplexity180.46168518066406
INFO:root:current mean train loss 6529.783228600543
INFO:root:current train perplexity176.35789489746094
INFO:root:current mean train loss 6525.530520984738
INFO:root:current train perplexity175.9456329345703
INFO:root:current mean train loss 6545.495651971726
INFO:root:current train perplexity176.22027587890625
INFO:root:current mean train loss 6543.850796545557
INFO:root:current train perplexity176.0847930908203
INFO:root:current mean train loss 6545.686846746055
INFO:root:current train perplexity176.0466766357422
INFO:root:current mean train loss 6550.970434768801
INFO:root:current train perplexity175.83099365234375
INFO:root:current mean train loss 6539.982933375219
INFO:root:current train perplexity175.6022491455078
INFO:root:current mean train loss 6547.466688434624
INFO:root:current train perplexity175.73367309570312
INFO:root:current mean train loss 6549.623654158128
INFO:root:current train perplexity175.7394561767578
INFO:root:current mean train loss 6544.667819138701
INFO:root:current train perplexity175.4193878173828
INFO:root:current mean train loss 6539.749877820207
INFO:root:current train perplexity175.253173828125
INFO:root:current mean train loss 6544.093294270833
INFO:root:current train perplexity175.24440002441406
INFO:root:current mean train loss 6544.829695668964
INFO:root:current train perplexity175.37814331054688
INFO:root:current mean train loss 6547.417665083923
INFO:root:current train perplexity175.4732666015625
INFO:root:current mean train loss 6547.816148734014
INFO:root:current train perplexity175.47225952148438
INFO:root:current mean train loss 6550.316159539474
INFO:root:current train perplexity175.54380798339844
INFO:root:current mean train loss 6551.0963337623
INFO:root:current train perplexity175.5718536376953
INFO:root:current mean train loss 6550.674344653926
INFO:root:current train perplexity175.4899139404297
INFO:root:current mean train loss 6550.418679116351
INFO:root:current train perplexity175.50538635253906

100%|██████████| 1/1 [07:40<00:00, 460.48s/it][A100%|██████████| 1/1 [07:40<00:00, 460.48s/it]
INFO:root:final mean train loss: 6547.876546470192
INFO:root:final train perplexity: 175.48789978027344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.62s/it][A100%|██████████| 1/1 [00:40<00:00, 40.62s/it]
INFO:root:eval mean loss: 6306.633484319592
INFO:root:eval perplexity: 164.5839385986328
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.05s/it][A100%|██████████| 1/1 [00:39<00:00, 39.05s/it]
INFO:root:eval mean loss: 6444.733717898105
INFO:root:eval perplexity: 200.0166015625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/49
 24%|██▍       | 49/200 [7:27:02<22:40:57, 540.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6594.425231933594
INFO:root:current train perplexity174.222900390625
INFO:root:current mean train loss 6540.5648267341385
INFO:root:current train perplexity173.9164581298828
INFO:root:current mean train loss 6581.440246582031
INFO:root:current train perplexity175.65139770507812
INFO:root:current mean train loss 6584.520984327937
INFO:root:current train perplexity175.60292053222656
INFO:root:current mean train loss 6561.919956913701
INFO:root:current train perplexity175.00323486328125
INFO:root:current mean train loss 6558.174340269619
INFO:root:current train perplexity175.146240234375
INFO:root:current mean train loss 6555.303574187846
INFO:root:current train perplexity174.98829650878906
INFO:root:current mean train loss 6545.480758250085
INFO:root:current train perplexity174.8418426513672
INFO:root:current mean train loss 6546.902002187876
INFO:root:current train perplexity174.74774169921875
INFO:root:current mean train loss 6547.544264895721
INFO:root:current train perplexity174.9660186767578
INFO:root:current mean train loss 6548.4085698090785
INFO:root:current train perplexity174.7868194580078
INFO:root:current mean train loss 6549.280190619479
INFO:root:current train perplexity175.0577392578125
INFO:root:current mean train loss 6549.361396294135
INFO:root:current train perplexity174.98521423339844
INFO:root:current mean train loss 6544.635145765883
INFO:root:current train perplexity175.03085327148438
INFO:root:current mean train loss 6546.608572336549
INFO:root:current train perplexity175.12770080566406
INFO:root:current mean train loss 6545.717096791877
INFO:root:current train perplexity175.1384735107422
INFO:root:current mean train loss 6547.579541973039
INFO:root:current train perplexity175.19869995117188
INFO:root:current mean train loss 6549.4249123800155
INFO:root:current train perplexity175.26930236816406
INFO:root:current mean train loss 6550.285575766751
INFO:root:current train perplexity175.3409423828125
INFO:root:current mean train loss 6552.162819809055
INFO:root:current train perplexity175.5529327392578

100%|██████████| 1/1 [07:42<00:00, 462.28s/it][A100%|██████████| 1/1 [07:42<00:00, 462.28s/it]
INFO:root:final mean train loss: 6547.66151164291
INFO:root:final train perplexity: 175.45803833007812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.70s/it][A100%|██████████| 1/1 [00:39<00:00, 39.70s/it]
INFO:root:eval mean loss: 6308.528458832004
INFO:root:eval perplexity: 164.83653259277344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.39s/it][A100%|██████████| 1/1 [00:38<00:00, 38.39s/it]
INFO:root:eval mean loss: 6446.325919076906
INFO:root:eval perplexity: 200.27857971191406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/50
 25%|██▌       | 50/200 [7:36:05<22:33:26, 541.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6499.88307158801
INFO:root:current train perplexity175.6097869873047
INFO:root:current mean train loss 6547.512803455327
INFO:root:current train perplexity177.42303466796875
INFO:root:current mean train loss 6569.031042137299
INFO:root:current train perplexity177.79660034179688
INFO:root:current mean train loss 6569.528260151773
INFO:root:current train perplexity176.7005615234375
INFO:root:current mean train loss 6568.962353406876
INFO:root:current train perplexity176.27230834960938
INFO:root:current mean train loss 6567.595504788536
INFO:root:current train perplexity176.4258575439453
INFO:root:current mean train loss 6562.401550010834
INFO:root:current train perplexity176.44789123535156
INFO:root:current mean train loss 6552.112208856601
INFO:root:current train perplexity175.93255615234375
INFO:root:current mean train loss 6554.482229783201
INFO:root:current train perplexity175.7899169921875
INFO:root:current mean train loss 6556.799185203175
INFO:root:current train perplexity175.86151123046875
INFO:root:current mean train loss 6555.953482017844
INFO:root:current train perplexity175.6742706298828
INFO:root:current mean train loss 6548.38873136967
INFO:root:current train perplexity175.4017333984375
INFO:root:current mean train loss 6553.752706462044
INFO:root:current train perplexity175.64830017089844
INFO:root:current mean train loss 6551.70423802064
INFO:root:current train perplexity175.5327606201172
INFO:root:current mean train loss 6556.863804913949
INFO:root:current train perplexity175.7563018798828
INFO:root:current mean train loss 6554.783009262528
INFO:root:current train perplexity175.5953826904297
INFO:root:current mean train loss 6551.803126421316
INFO:root:current train perplexity175.64913940429688
INFO:root:current mean train loss 6550.9983227022585
INFO:root:current train perplexity175.58616638183594
INFO:root:current mean train loss 6551.701626882352
INFO:root:current train perplexity175.67837524414062
INFO:root:current mean train loss 6552.519974436009
INFO:root:current train perplexity175.74331665039062

100%|██████████| 1/1 [07:39<00:00, 459.07s/it][A100%|██████████| 1/1 [07:39<00:00, 459.07s/it]
INFO:root:final mean train loss: 6549.8323717910835
INFO:root:final train perplexity: 175.75889587402344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.68s/it][A100%|██████████| 1/1 [00:39<00:00, 39.68s/it]
INFO:root:eval mean loss: 6319.695724595523
INFO:root:eval perplexity: 166.33282470703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.67s/it][A100%|██████████| 1/1 [00:38<00:00, 38.67s/it]
INFO:root:eval mean loss: 6456.965505180629
INFO:root:eval perplexity: 202.0381317138672
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/51
 26%|██▌       | 51/200 [7:45:04<22:23:10, 540.88s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6667.511334043561
INFO:root:current train perplexity179.63525390625
INFO:root:current mean train loss 6602.74266695689
INFO:root:current train perplexity178.10411071777344
INFO:root:current mean train loss 6579.426355806509
INFO:root:current train perplexity176.8475799560547
INFO:root:current mean train loss 6565.685337421021
INFO:root:current train perplexity176.2298583984375
INFO:root:current mean train loss 6550.759434515826
INFO:root:current train perplexity175.6598663330078
INFO:root:current mean train loss 6552.359006632343
INFO:root:current train perplexity175.4523468017578
INFO:root:current mean train loss 6553.465641422673
INFO:root:current train perplexity175.60409545898438
INFO:root:current mean train loss 6557.448332066947
INFO:root:current train perplexity175.8126220703125
INFO:root:current mean train loss 6555.845927531394
INFO:root:current train perplexity175.73182678222656
INFO:root:current mean train loss 6555.44078048169
INFO:root:current train perplexity175.808349609375
INFO:root:current mean train loss 6556.791709112629
INFO:root:current train perplexity175.9403533935547
INFO:root:current mean train loss 6556.092453081448
INFO:root:current train perplexity176.01763916015625
INFO:root:current mean train loss 6555.407046060426
INFO:root:current train perplexity175.90101623535156
INFO:root:current mean train loss 6556.849436367588
INFO:root:current train perplexity175.94085693359375
INFO:root:current mean train loss 6553.752253554528
INFO:root:current train perplexity175.88079833984375
INFO:root:current mean train loss 6553.256146544241
INFO:root:current train perplexity175.95982360839844
INFO:root:current mean train loss 6551.531131886349
INFO:root:current train perplexity175.96250915527344
INFO:root:current mean train loss 6553.007558129247
INFO:root:current train perplexity176.05516052246094
INFO:root:current mean train loss 6554.76218949918
INFO:root:current train perplexity176.1258544921875
INFO:root:current mean train loss 6554.470445821147
INFO:root:current train perplexity176.25010681152344

100%|██████████| 1/1 [07:40<00:00, 460.04s/it][A100%|██████████| 1/1 [07:40<00:00, 460.04s/it]
INFO:root:final mean train loss: 6553.315372930652
INFO:root:final train perplexity: 176.2427978515625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.94s/it][A100%|██████████| 1/1 [00:39<00:00, 39.94s/it]
INFO:root:eval mean loss: 6311.924269655918
INFO:root:eval perplexity: 165.29013061523438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.38s/it][A100%|██████████| 1/1 [00:38<00:00, 38.38s/it]
INFO:root:eval mean loss: 6449.217652232935
INFO:root:eval perplexity: 200.75531005859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/52
 26%|██▌       | 52/200 [7:54:05<22:14:04, 540.84s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6553.636242234563
INFO:root:current train perplexity175.0886993408203
INFO:root:current mean train loss 6571.115885416667
INFO:root:current train perplexity176.63882446289062
INFO:root:current mean train loss 6570.227942800353
INFO:root:current train perplexity176.8639373779297
INFO:root:current mean train loss 6574.069949157555
INFO:root:current train perplexity177.61094665527344
INFO:root:current mean train loss 6563.903151486477
INFO:root:current train perplexity177.42234802246094
INFO:root:current mean train loss 6560.075068007612
INFO:root:current train perplexity177.04002380371094
INFO:root:current mean train loss 6559.1592440291
INFO:root:current train perplexity176.9777374267578
INFO:root:current mean train loss 6557.456587868175
INFO:root:current train perplexity177.14541625976562
INFO:root:current mean train loss 6555.5868471519325
INFO:root:current train perplexity176.96669006347656
INFO:root:current mean train loss 6548.501354073945
INFO:root:current train perplexity176.75086975097656
INFO:root:current mean train loss 6544.591123741199
INFO:root:current train perplexity176.5141143798828
INFO:root:current mean train loss 6542.011086006842
INFO:root:current train perplexity176.36495971679688
INFO:root:current mean train loss 6543.705253951919
INFO:root:current train perplexity176.36654663085938
INFO:root:current mean train loss 6550.845672055766
INFO:root:current train perplexity176.6090850830078
INFO:root:current mean train loss 6549.667555867541
INFO:root:current train perplexity176.46438598632812
INFO:root:current mean train loss 6558.575616042522
INFO:root:current train perplexity176.76023864746094
INFO:root:current mean train loss 6559.0533674288845
INFO:root:current train perplexity176.78164672851562
INFO:root:current mean train loss 6561.3156227544
INFO:root:current train perplexity176.8180694580078
INFO:root:current mean train loss 6560.761193646691
INFO:root:current train perplexity176.73789978027344
INFO:root:current mean train loss 6557.079558448964
INFO:root:current train perplexity176.76705932617188

100%|██████████| 1/1 [07:41<00:00, 461.78s/it][A100%|██████████| 1/1 [07:41<00:00, 461.78s/it]
INFO:root:final mean train loss: 6557.079558448964
INFO:root:final train perplexity: 176.76705932617188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.26s/it][A100%|██████████| 1/1 [00:40<00:00, 40.26s/it]
INFO:root:eval mean loss: 6318.686061128657
INFO:root:eval perplexity: 166.1969451904297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 6455.180008692098
INFO:root:eval perplexity: 201.74180603027344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/53
 26%|██▋       | 53/200 [8:03:08<22:06:31, 541.44s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6616.060756835937
INFO:root:current train perplexity177.9727783203125
INFO:root:current mean train loss 6571.874790039063
INFO:root:current train perplexity176.90298461914062
INFO:root:current mean train loss 6565.525807291667
INFO:root:current train perplexity177.94163513183594
INFO:root:current mean train loss 6585.85086303711
INFO:root:current train perplexity178.2120819091797
INFO:root:current mean train loss 6580.990471679687
INFO:root:current train perplexity177.74913024902344
INFO:root:current mean train loss 6591.053837890625
INFO:root:current train perplexity177.9670867919922
INFO:root:current mean train loss 6587.047546037947
INFO:root:current train perplexity177.95359802246094
INFO:root:current mean train loss 6585.332885131836
INFO:root:current train perplexity177.7198028564453
INFO:root:current mean train loss 6580.576920572917
INFO:root:current train perplexity177.4563751220703
INFO:root:current mean train loss 6575.107916015625
INFO:root:current train perplexity177.26641845703125
INFO:root:current mean train loss 6575.293329190341
INFO:root:current train perplexity177.29371643066406
INFO:root:current mean train loss 6572.203216959635
INFO:root:current train perplexity177.16830444335938
INFO:root:current mean train loss 6570.8569151893025
INFO:root:current train perplexity177.16856384277344
INFO:root:current mean train loss 6567.7963957868305
INFO:root:current train perplexity177.1659393310547
INFO:root:current mean train loss 6564.709135416667
INFO:root:current train perplexity177.0470428466797
INFO:root:current mean train loss 6560.338040161133
INFO:root:current train perplexity176.8182373046875
INFO:root:current mean train loss 6558.199461741728
INFO:root:current train perplexity176.77304077148438
INFO:root:current mean train loss 6557.636552463107
INFO:root:current train perplexity176.80068969726562
INFO:root:current mean train loss 6559.749501696135
INFO:root:current train perplexity176.87271118164062

100%|██████████| 1/1 [07:44<00:00, 464.21s/it][A100%|██████████| 1/1 [07:44<00:00, 464.21s/it]
INFO:root:final mean train loss: 6557.734815635046
INFO:root:final train perplexity: 176.85853576660156
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.10s/it][A100%|██████████| 1/1 [00:40<00:00, 40.10s/it]
INFO:root:eval mean loss: 6313.778886510971
INFO:root:eval perplexity: 165.53826904296875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.59s/it][A100%|██████████| 1/1 [00:39<00:00, 39.59s/it]
INFO:root:eval mean loss: 6450.490980648825
INFO:root:eval perplexity: 200.96554565429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/54
 27%|██▋       | 54/200 [8:12:14<22:00:58, 542.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6357.844985064338
INFO:root:current train perplexity174.0952606201172
INFO:root:current mean train loss 6554.511864817041
INFO:root:current train perplexity175.6287078857422
INFO:root:current mean train loss 6572.092858942972
INFO:root:current train perplexity176.9009552001953
INFO:root:current mean train loss 6563.29361106319
INFO:root:current train perplexity176.5042724609375
INFO:root:current mean train loss 6548.693154460806
INFO:root:current train perplexity175.94613647460938
INFO:root:current mean train loss 6547.539178667493
INFO:root:current train perplexity176.3307342529297
INFO:root:current mean train loss 6546.129548058904
INFO:root:current train perplexity176.17189025878906
INFO:root:current mean train loss 6546.312344049643
INFO:root:current train perplexity176.46649169921875
INFO:root:current mean train loss 6546.604510714696
INFO:root:current train perplexity176.3470458984375
INFO:root:current mean train loss 6548.535532178639
INFO:root:current train perplexity176.34788513183594
INFO:root:current mean train loss 6552.14339384756
INFO:root:current train perplexity176.27952575683594
INFO:root:current mean train loss 6554.922233888905
INFO:root:current train perplexity176.24356079101562
INFO:root:current mean train loss 6550.624475207991
INFO:root:current train perplexity176.13812255859375
INFO:root:current mean train loss 6548.413889358509
INFO:root:current train perplexity176.0821075439453
INFO:root:current mean train loss 6547.716449530258
INFO:root:current train perplexity176.06455993652344
INFO:root:current mean train loss 6551.606204551541
INFO:root:current train perplexity176.2682647705078
INFO:root:current mean train loss 6551.923506529743
INFO:root:current train perplexity176.21279907226562
INFO:root:current mean train loss 6553.890719129932
INFO:root:current train perplexity176.28709411621094
INFO:root:current mean train loss 6553.982075214123
INFO:root:current train perplexity176.29306030273438
INFO:root:current mean train loss 6555.622962056192
INFO:root:current train perplexity176.42071533203125

100%|██████████| 1/1 [07:36<00:00, 456.07s/it][A100%|██████████| 1/1 [07:36<00:00, 456.07s/it]
INFO:root:final mean train loss: 6554.752137923325
INFO:root:final train perplexity: 176.4426727294922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.50s/it][A100%|██████████| 1/1 [00:40<00:00, 40.50s/it]
INFO:root:eval mean loss: 6310.967943123892
INFO:root:eval perplexity: 165.16226196289062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.49s/it][A100%|██████████| 1/1 [00:38<00:00, 38.49s/it]
INFO:root:eval mean loss: 6448.615847323803
INFO:root:eval perplexity: 200.65597534179688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/55
 28%|██▊       | 55/200 [8:21:12<21:47:58, 541.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6610.485193589155
INFO:root:current train perplexity179.35426330566406
INFO:root:current mean train loss 6558.730261048274
INFO:root:current train perplexity177.07127380371094
INFO:root:current mean train loss 6546.019731570513
INFO:root:current train perplexity176.25355529785156
INFO:root:current mean train loss 6538.777389069517
INFO:root:current train perplexity175.80885314941406
INFO:root:current mean train loss 6545.509289719542
INFO:root:current train perplexity175.82260131835938
INFO:root:current mean train loss 6541.251446556063
INFO:root:current train perplexity175.76165771484375
INFO:root:current mean train loss 6538.169351186662
INFO:root:current train perplexity175.70787048339844
INFO:root:current mean train loss 6558.740488494124
INFO:root:current train perplexity176.05523681640625
INFO:root:current mean train loss 6559.743471433791
INFO:root:current train perplexity176.0730438232422
INFO:root:current mean train loss 6564.695096066984
INFO:root:current train perplexity176.07086181640625
INFO:root:current mean train loss 6564.853916544517
INFO:root:current train perplexity175.96737670898438
INFO:root:current mean train loss 6559.212199969687
INFO:root:current train perplexity175.91871643066406
INFO:root:current mean train loss 6556.257491991238
INFO:root:current train perplexity175.9022674560547
INFO:root:current mean train loss 6560.787037267499
INFO:root:current train perplexity176.15945434570312
INFO:root:current mean train loss 6559.086527932139
INFO:root:current train perplexity176.1660919189453
INFO:root:current mean train loss 6559.764275382986
INFO:root:current train perplexity176.37892150878906
INFO:root:current mean train loss 6560.008293908258
INFO:root:current train perplexity176.40440368652344
INFO:root:current mean train loss 6557.680123405061
INFO:root:current train perplexity176.37428283691406
INFO:root:current mean train loss 6558.021560519186
INFO:root:current train perplexity176.39666748046875
INFO:root:current mean train loss 6556.925517164071
INFO:root:current train perplexity176.42298889160156

100%|██████████| 1/1 [07:43<00:00, 463.68s/it][A100%|██████████| 1/1 [07:43<00:00, 463.68s/it]
INFO:root:final mean train loss: 6554.444743207892
INFO:root:final train perplexity: 176.3998565673828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.79s/it][A100%|██████████| 1/1 [00:40<00:00, 40.79s/it]
INFO:root:eval mean loss: 6313.823323567708
INFO:root:eval perplexity: 165.54434204101562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.98s/it][A100%|██████████| 1/1 [00:38<00:00, 38.98s/it]
INFO:root:eval mean loss: 6450.999836373837
INFO:root:eval perplexity: 201.0495147705078
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/56
 28%|██▊       | 56/200 [8:30:18<21:42:17, 542.62s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6482.5079656862745
INFO:root:current train perplexity172.57972717285156
INFO:root:current mean train loss 6520.965752405836
INFO:root:current train perplexity174.6585235595703
INFO:root:current mean train loss 6527.611808624875
INFO:root:current train perplexity175.14273071289062
INFO:root:current mean train loss 6535.711720697561
INFO:root:current train perplexity175.43101501464844
INFO:root:current mean train loss 6541.767546727758
INFO:root:current train perplexity175.86520385742188
INFO:root:current mean train loss 6547.315154973911
INFO:root:current train perplexity176.21926879882812
INFO:root:current mean train loss 6543.662834671419
INFO:root:current train perplexity176.3383026123047
INFO:root:current mean train loss 6550.692377611102
INFO:root:current train perplexity176.33316040039062
INFO:root:current mean train loss 6560.639358681881
INFO:root:current train perplexity176.80331420898438
INFO:root:current mean train loss 6558.3167820879335
INFO:root:current train perplexity176.9185028076172
INFO:root:current mean train loss 6558.147233754311
INFO:root:current train perplexity176.8467254638672
INFO:root:current mean train loss 6555.858941443582
INFO:root:current train perplexity176.72467041015625
INFO:root:current mean train loss 6557.272868814323
INFO:root:current train perplexity176.58062744140625
INFO:root:current mean train loss 6556.82620801721
INFO:root:current train perplexity176.55612182617188
INFO:root:current mean train loss 6553.304029279377
INFO:root:current train perplexity176.4113006591797
INFO:root:current mean train loss 6551.683894400286
INFO:root:current train perplexity176.0649871826172
INFO:root:current mean train loss 6553.456457793099
INFO:root:current train perplexity176.0901641845703
INFO:root:current mean train loss 6551.850725924472
INFO:root:current train perplexity176.0673370361328
INFO:root:current mean train loss 6552.579622290316
INFO:root:current train perplexity175.9713897705078
INFO:root:current mean train loss 6552.476171574674
INFO:root:current train perplexity176.0359344482422

100%|██████████| 1/1 [07:43<00:00, 463.07s/it][A100%|██████████| 1/1 [07:43<00:00, 463.07s/it]
INFO:root:final mean train loss: 6552.077577499566
INFO:root:final train perplexity: 176.07069396972656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.95s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6311.934293273493
INFO:root:eval perplexity: 165.2914581298828
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.67s/it][A100%|██████████| 1/1 [00:39<00:00, 39.67s/it]
INFO:root:eval mean loss: 6449.935117464539
INFO:root:eval perplexity: 200.87376403808594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/57
 28%|██▊       | 57/200 [8:39:23<21:35:00, 543.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6496.842033835019
INFO:root:current train perplexity174.9573516845703
INFO:root:current mean train loss 6485.910720098586
INFO:root:current train perplexity173.73187255859375
INFO:root:current mean train loss 6496.51097357451
INFO:root:current train perplexity173.38040161132812
INFO:root:current mean train loss 6524.89741715141
INFO:root:current train perplexity174.5658416748047
INFO:root:current mean train loss 6529.448241144164
INFO:root:current train perplexity174.7085723876953
INFO:root:current mean train loss 6531.7733248858385
INFO:root:current train perplexity174.92340087890625
INFO:root:current mean train loss 6542.462556576301
INFO:root:current train perplexity175.3563232421875
INFO:root:current mean train loss 6536.752728144328
INFO:root:current train perplexity175.1809844970703
INFO:root:current mean train loss 6540.802942513321
INFO:root:current train perplexity175.2379608154297
INFO:root:current mean train loss 6543.548093181011
INFO:root:current train perplexity175.2189178466797
INFO:root:current mean train loss 6546.757276213571
INFO:root:current train perplexity175.23855590820312
INFO:root:current mean train loss 6543.541464191593
INFO:root:current train perplexity175.2201690673828
INFO:root:current mean train loss 6550.796261182719
INFO:root:current train perplexity175.61329650878906
INFO:root:current mean train loss 6549.353074101677
INFO:root:current train perplexity175.67552185058594
INFO:root:current mean train loss 6549.49817027578
INFO:root:current train perplexity175.87904357910156
INFO:root:current mean train loss 6552.937669092295
INFO:root:current train perplexity176.09327697753906
INFO:root:current mean train loss 6553.770636030238
INFO:root:current train perplexity176.23683166503906
INFO:root:current mean train loss 6555.900886915389
INFO:root:current train perplexity176.30474853515625
INFO:root:current mean train loss 6557.814376275595
INFO:root:current train perplexity176.31643676757812
INFO:root:current mean train loss 6554.826232413935
INFO:root:current train perplexity176.2348175048828

100%|██████████| 1/1 [07:48<00:00, 468.87s/it][A100%|██████████| 1/1 [07:48<00:00, 468.87s/it]
INFO:root:final mean train loss: 6553.71454469928
INFO:root:final train perplexity: 176.29827880859375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.44s/it][A100%|██████████| 1/1 [00:41<00:00, 41.45s/it]
INFO:root:eval mean loss: 6313.960506358045
INFO:root:eval perplexity: 165.56265258789062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.28s/it][A100%|██████████| 1/1 [00:38<00:00, 38.28s/it]
INFO:root:eval mean loss: 6450.769775390625
INFO:root:eval perplexity: 201.0115509033203
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/58
 29%|██▉       | 58/200 [8:48:34<21:31:22, 545.65s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6531.801849724265
INFO:root:current train perplexity177.70921325683594
INFO:root:current mean train loss 6482.872247149493
INFO:root:current train perplexity175.2734832763672
INFO:root:current mean train loss 6505.262143640351
INFO:root:current train perplexity176.44158935546875
INFO:root:current mean train loss 6520.279495992289
INFO:root:current train perplexity176.31761169433594
INFO:root:current mean train loss 6518.54235160277
INFO:root:current train perplexity176.29071044921875
INFO:root:current mean train loss 6531.247361611912
INFO:root:current train perplexity176.43275451660156
INFO:root:current mean train loss 6537.739562899179
INFO:root:current train perplexity176.2553253173828
INFO:root:current mean train loss 6532.3158041401275
INFO:root:current train perplexity176.01260375976562
INFO:root:current mean train loss 6538.7118666137
INFO:root:current train perplexity176.31600952148438
INFO:root:current mean train loss 6541.244985326776
INFO:root:current train perplexity176.5664825439453
INFO:root:current mean train loss 6538.390007560484
INFO:root:current train perplexity176.3688201904297
INFO:root:current mean train loss 6542.66087157173
INFO:root:current train perplexity176.3457794189453
INFO:root:current mean train loss 6544.797061952821
INFO:root:current train perplexity176.45933532714844
INFO:root:current mean train loss 6543.239956213335
INFO:root:current train perplexity176.3179473876953
INFO:root:current mean train loss 6544.349212831439
INFO:root:current train perplexity176.19676208496094
INFO:root:current mean train loss 6547.519620588525
INFO:root:current train perplexity176.29693603515625
INFO:root:current mean train loss 6554.6625014489055
INFO:root:current train perplexity176.46926879882812
INFO:root:current mean train loss 6555.375937718837
INFO:root:current train perplexity176.3973388671875
INFO:root:current mean train loss 6554.753941737815
INFO:root:current train perplexity176.39523315429688

100%|██████████| 1/1 [07:38<00:00, 458.77s/it][A100%|██████████| 1/1 [07:38<00:00, 458.78s/it]
INFO:root:final mean train loss: 6554.516519936058
INFO:root:final train perplexity: 176.4098663330078
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.68s/it][A100%|██████████| 1/1 [00:39<00:00, 39.68s/it]
INFO:root:eval mean loss: 6313.501094304078
INFO:root:eval perplexity: 165.5010986328125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.75s/it][A100%|██████████| 1/1 [00:38<00:00, 38.75s/it]
INFO:root:eval mean loss: 6450.533030841368
INFO:root:eval perplexity: 200.97254943847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/59
 30%|██▉       | 59/200 [8:57:33<21:17:59, 543.83s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6603.89404296875
INFO:root:current train perplexity184.6398468017578
INFO:root:current mean train loss 6559.881223192402
INFO:root:current train perplexity175.2438201904297
INFO:root:current mean train loss 6555.947335724783
INFO:root:current train perplexity176.29214477539062
INFO:root:current mean train loss 6571.788205582575
INFO:root:current train perplexity176.94955444335938
INFO:root:current mean train loss 6573.475967331312
INFO:root:current train perplexity176.79159545898438
INFO:root:current mean train loss 6572.795336233192
INFO:root:current train perplexity176.91419982910156
INFO:root:current mean train loss 6576.606039763289
INFO:root:current train perplexity176.61463928222656
INFO:root:current mean train loss 6579.302947215545
INFO:root:current train perplexity176.6356201171875
INFO:root:current mean train loss 6584.155314229076
INFO:root:current train perplexity176.97520446777344
INFO:root:current mean train loss 6577.151919345898
INFO:root:current train perplexity176.92230224609375
INFO:root:current mean train loss 6569.540002514502
INFO:root:current train perplexity176.75146484375
INFO:root:current mean train loss 6564.9182771381575
INFO:root:current train perplexity176.5741424560547
INFO:root:current mean train loss 6561.594835024309
INFO:root:current train perplexity176.556884765625
INFO:root:current mean train loss 6563.207291516657
INFO:root:current train perplexity176.76502990722656
INFO:root:current mean train loss 6562.862477431794
INFO:root:current train perplexity176.7208709716797
INFO:root:current mean train loss 6558.675141478029
INFO:root:current train perplexity176.5876922607422
INFO:root:current mean train loss 6556.5641068781215
INFO:root:current train perplexity176.54273986816406
INFO:root:current mean train loss 6557.413195528239
INFO:root:current train perplexity176.5632781982422
INFO:root:current mean train loss 6560.026566618688
INFO:root:current train perplexity176.68666076660156
INFO:root:current mean train loss 6556.765512299964
INFO:root:current train perplexity176.58828735351562

100%|██████████| 1/1 [07:39<00:00, 459.53s/it][A100%|██████████| 1/1 [07:39<00:00, 459.53s/it]
INFO:root:final mean train loss: 6555.895813880878
INFO:root:final train perplexity: 176.60202026367188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.86s/it][A100%|██████████| 1/1 [00:40<00:00, 40.86s/it]
INFO:root:eval mean loss: 6311.25203969969
INFO:root:eval perplexity: 165.20022583007812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.05s/it][A100%|██████████| 1/1 [00:38<00:00, 38.05s/it]
INFO:root:eval mean loss: 6448.821047519115
INFO:root:eval perplexity: 200.68994140625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/60
 30%|███       | 60/200 [9:06:34<21:06:49, 542.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6561.842028166118
INFO:root:current train perplexity174.7543182373047
INFO:root:current mean train loss 6572.748543362658
INFO:root:current train perplexity177.02972412109375
INFO:root:current mean train loss 6563.220038705765
INFO:root:current train perplexity176.38833618164062
INFO:root:current mean train loss 6562.084272139498
INFO:root:current train perplexity175.8443145751953
INFO:root:current mean train loss 6572.039097460472
INFO:root:current train perplexity176.25608825683594
INFO:root:current mean train loss 6566.660389571291
INFO:root:current train perplexity175.7456512451172
INFO:root:current mean train loss 6576.6568526605415
INFO:root:current train perplexity176.50283813476562
INFO:root:current mean train loss 6585.657241502956
INFO:root:current train perplexity176.7853546142578
INFO:root:current mean train loss 6583.0504974626065
INFO:root:current train perplexity176.85398864746094
INFO:root:current mean train loss 6574.610892975551
INFO:root:current train perplexity176.64251708984375
INFO:root:current mean train loss 6568.55923297197
INFO:root:current train perplexity176.4315643310547
INFO:root:current mean train loss 6561.9839434449
INFO:root:current train perplexity176.312744140625
INFO:root:current mean train loss 6560.246763083855
INFO:root:current train perplexity176.28407287597656
INFO:root:current mean train loss 6561.644941421057
INFO:root:current train perplexity176.22506713867188
INFO:root:current mean train loss 6559.99434915103
INFO:root:current train perplexity176.0225067138672
INFO:root:current mean train loss 6560.205825172811
INFO:root:current train perplexity176.12879943847656
INFO:root:current mean train loss 6555.783253792851
INFO:root:current train perplexity176.0304718017578
INFO:root:current mean train loss 6555.304387259581
INFO:root:current train perplexity175.91770935058594
INFO:root:current mean train loss 6552.896444915218
INFO:root:current train perplexity175.87022399902344
INFO:root:current mean train loss 6552.8504269089535
INFO:root:current train perplexity175.9254150390625

100%|██████████| 1/1 [07:41<00:00, 461.84s/it][A100%|██████████| 1/1 [07:41<00:00, 461.84s/it]
INFO:root:final mean train loss: 6550.743021985707
INFO:root:final train perplexity: 175.88523864746094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.02s/it][A100%|██████████| 1/1 [00:40<00:00, 40.02s/it]
INFO:root:eval mean loss: 6309.25764800809
INFO:root:eval perplexity: 164.9337921142578
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.99s/it][A100%|██████████| 1/1 [00:38<00:00, 38.99s/it]
INFO:root:eval mean loss: 6447.21477102726
INFO:root:eval perplexity: 200.42503356933594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/61
 30%|███       | 61/200 [9:15:37<20:58:04, 543.05s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6506.544542100694
INFO:root:current train perplexity173.32583618164062
INFO:root:current mean train loss 6524.9592536477485
INFO:root:current train perplexity175.5235595703125
INFO:root:current mean train loss 6543.8132034560385
INFO:root:current train perplexity175.9446258544922
INFO:root:current mean train loss 6548.93301100958
INFO:root:current train perplexity175.93045043945312
INFO:root:current mean train loss 6546.839594009819
INFO:root:current train perplexity175.6097869873047
INFO:root:current mean train loss 6557.100031155259
INFO:root:current train perplexity175.83274841308594
INFO:root:current mean train loss 6558.556347349155
INFO:root:current train perplexity175.79385375976562
INFO:root:current mean train loss 6556.1598855723505
INFO:root:current train perplexity175.76535034179688
INFO:root:current mean train loss 6553.279168964003
INFO:root:current train perplexity175.71766662597656
INFO:root:current mean train loss 6553.896798419138
INFO:root:current train perplexity175.83132934570312
INFO:root:current mean train loss 6552.467087204392
INFO:root:current train perplexity175.99673461914062
INFO:root:current mean train loss 6551.865496568277
INFO:root:current train perplexity175.83912658691406
INFO:root:current mean train loss 6554.488327865847
INFO:root:current train perplexity175.8816375732422
INFO:root:current mean train loss 6551.69076337072
INFO:root:current train perplexity175.8687286376953
INFO:root:current mean train loss 6550.228343230436
INFO:root:current train perplexity175.7086181640625
INFO:root:current mean train loss 6555.652393976848
INFO:root:current train perplexity175.9061279296875
INFO:root:current mean train loss 6557.491712649469
INFO:root:current train perplexity175.9714813232422
INFO:root:current mean train loss 6553.883951916672
INFO:root:current train perplexity175.8314971923828
INFO:root:current mean train loss 6555.162422396259
INFO:root:current train perplexity175.8912811279297
INFO:root:current mean train loss 6554.380238178348
INFO:root:current train perplexity175.8023223876953

100%|██████████| 1/1 [07:32<00:00, 452.12s/it][A100%|██████████| 1/1 [07:32<00:00, 452.12s/it]
INFO:root:final mean train loss: 6549.84907332443
INFO:root:final train perplexity: 175.76124572753906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.51s/it][A100%|██████████| 1/1 [00:38<00:00, 38.51s/it]
INFO:root:eval mean loss: 6312.622437389185
INFO:root:eval perplexity: 165.383544921875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.99s/it][A100%|██████████| 1/1 [00:37<00:00, 37.99s/it]
INFO:root:eval mean loss: 6450.197116716534
INFO:root:eval perplexity: 200.9170684814453
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/62
 31%|███       | 62/200 [9:24:28<20:40:45, 539.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6485.015173570165
INFO:root:current train perplexity174.38938903808594
INFO:root:current mean train loss 6550.197527318219
INFO:root:current train perplexity176.0570831298828
INFO:root:current mean train loss 6564.252877578434
INFO:root:current train perplexity176.17105102539062
INFO:root:current mean train loss 6556.714250343041
INFO:root:current train perplexity176.24188232421875
INFO:root:current mean train loss 6555.552876655629
INFO:root:current train perplexity175.67343139648438
INFO:root:current mean train loss 6561.965154554701
INFO:root:current train perplexity175.63926696777344
INFO:root:current mean train loss 6554.640779784409
INFO:root:current train perplexity175.3348388671875
INFO:root:current mean train loss 6557.878661136703
INFO:root:current train perplexity175.54087829589844
INFO:root:current mean train loss 6558.996140689112
INFO:root:current train perplexity175.7845458984375
INFO:root:current mean train loss 6562.382048567845
INFO:root:current train perplexity175.96334838867188
INFO:root:current mean train loss 6567.2597090530035
INFO:root:current train perplexity176.29542541503906
INFO:root:current mean train loss 6572.489527997615
INFO:root:current train perplexity176.69509887695312
INFO:root:current mean train loss 6569.336514240822
INFO:root:current train perplexity176.75778198242188
INFO:root:current mean train loss 6565.22000913768
INFO:root:current train perplexity176.63351440429688
INFO:root:current mean train loss 6564.9778798176185
INFO:root:current train perplexity176.6334228515625
INFO:root:current mean train loss 6562.663263265656
INFO:root:current train perplexity176.53440856933594
INFO:root:current mean train loss 6563.6418635742775
INFO:root:current train perplexity176.6258544921875
INFO:root:current mean train loss 6560.370746410172
INFO:root:current train perplexity176.50503540039062
INFO:root:current mean train loss 6555.4066945388395
INFO:root:current train perplexity176.310302734375
INFO:root:current mean train loss 6556.114512328789
INFO:root:current train perplexity176.4061737060547

100%|██████████| 1/1 [07:41<00:00, 461.20s/it][A100%|██████████| 1/1 [07:41<00:00, 461.20s/it]
INFO:root:final mean train loss: 6554.5361474634
INFO:root:final train perplexity: 176.4125518798828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.79s/it][A100%|██████████| 1/1 [00:39<00:00, 39.79s/it]
INFO:root:eval mean loss: 6311.1341405557405
INFO:root:eval perplexity: 165.18438720703125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.14s/it][A100%|██████████| 1/1 [00:39<00:00, 39.14s/it]
INFO:root:eval mean loss: 6448.717295545212
INFO:root:eval perplexity: 200.67271423339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/63
 32%|███▏      | 63/200 [9:33:31<20:33:53, 540.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6569.330043247768
INFO:root:current train perplexity179.16065979003906
INFO:root:current mean train loss 6564.298876953125
INFO:root:current train perplexity176.4610137939453
INFO:root:current mean train loss 6567.816549117477
INFO:root:current train perplexity176.569091796875
INFO:root:current mean train loss 6565.9479452597125
INFO:root:current train perplexity177.19163513183594
INFO:root:current mean train loss 6565.121324384973
INFO:root:current train perplexity177.3094482421875
INFO:root:current mean train loss 6568.639462547972
INFO:root:current train perplexity177.20236206054688
INFO:root:current mean train loss 6567.054433156483
INFO:root:current train perplexity177.2617645263672
INFO:root:current mean train loss 6570.855945616883
INFO:root:current train perplexity177.10826110839844
INFO:root:current mean train loss 6564.421252020474
INFO:root:current train perplexity177.06704711914062
INFO:root:current mean train loss 6561.342746758215
INFO:root:current train perplexity176.8810577392578
INFO:root:current mean train loss 6562.792995217582
INFO:root:current train perplexity176.95555114746094
INFO:root:current mean train loss 6561.328913344685
INFO:root:current train perplexity177.06578063964844
INFO:root:current mean train loss 6561.395652374508
INFO:root:current train perplexity176.958251953125
INFO:root:current mean train loss 6566.624504234147
INFO:root:current train perplexity176.9766387939453
INFO:root:current mean train loss 6560.496653114371
INFO:root:current train perplexity176.84100341796875
INFO:root:current mean train loss 6558.094972569168
INFO:root:current train perplexity176.7431182861328
INFO:root:current mean train loss 6559.113968352358
INFO:root:current train perplexity176.60791015625
INFO:root:current mean train loss 6556.582292770127
INFO:root:current train perplexity176.55503845214844
INFO:root:current mean train loss 6556.848509306066
INFO:root:current train perplexity176.57119750976562
INFO:root:current mean train loss 6559.010494576856
INFO:root:current train perplexity176.6907958984375

100%|██████████| 1/1 [07:43<00:00, 463.66s/it][A100%|██████████| 1/1 [07:43<00:00, 463.66s/it]
INFO:root:final mean train loss: 6556.20307144419
INFO:root:final train perplexity: 176.64488220214844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.44s/it][A100%|██████████| 1/1 [00:39<00:00, 39.44s/it]
INFO:root:eval mean loss: 6313.603413466866
INFO:root:eval perplexity: 165.51483154296875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.31s/it][A100%|██████████| 1/1 [00:38<00:00, 38.31s/it]
INFO:root:eval mean loss: 6450.819970529976
INFO:root:eval perplexity: 201.01998901367188
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/64
 32%|███▏      | 64/200 [9:42:35<20:27:10, 541.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6506.580981725934
INFO:root:current train perplexity177.8548583984375
INFO:root:current mean train loss 6518.300979695856
INFO:root:current train perplexity176.73275756835938
INFO:root:current mean train loss 6534.594721458515
INFO:root:current train perplexity176.9068603515625
INFO:root:current mean train loss 6546.360426003311
INFO:root:current train perplexity177.14077758789062
INFO:root:current mean train loss 6544.639549177041
INFO:root:current train perplexity177.17396545410156
INFO:root:current mean train loss 6543.632097130537
INFO:root:current train perplexity177.05429077148438
INFO:root:current mean train loss 6555.913784599026
INFO:root:current train perplexity177.203369140625
INFO:root:current mean train loss 6554.5178126489045
INFO:root:current train perplexity177.10926818847656
INFO:root:current mean train loss 6553.765017263247
INFO:root:current train perplexity177.0188446044922
INFO:root:current mean train loss 6554.2331382187185
INFO:root:current train perplexity176.9993438720703
INFO:root:current mean train loss 6559.598793177179
INFO:root:current train perplexity177.1521759033203
INFO:root:current mean train loss 6563.401134359204
INFO:root:current train perplexity177.07725524902344
INFO:root:current mean train loss 6561.918343212777
INFO:root:current train perplexity176.8928680419922
INFO:root:current mean train loss 6561.221788116213
INFO:root:current train perplexity176.8560028076172
INFO:root:current mean train loss 6556.26443696936
INFO:root:current train perplexity176.71337890625
INFO:root:current mean train loss 6554.869868585578
INFO:root:current train perplexity176.53878784179688
INFO:root:current mean train loss 6555.7212825790975
INFO:root:current train perplexity176.5040283203125
INFO:root:current mean train loss 6556.976469051658
INFO:root:current train perplexity176.47901916503906
INFO:root:current mean train loss 6554.5284261455845
INFO:root:current train perplexity176.35183715820312

100%|██████████| 1/1 [07:28<00:00, 448.50s/it][A100%|██████████| 1/1 [07:28<00:00, 448.50s/it]
INFO:root:final mean train loss: 6554.379323123503
INFO:root:final train perplexity: 176.39085388183594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.44s/it][A100%|██████████| 1/1 [00:38<00:00, 38.44s/it]
INFO:root:eval mean loss: 6309.5765233682405
INFO:root:eval perplexity: 164.97633361816406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.60s/it][A100%|██████████| 1/1 [00:37<00:00, 37.60s/it]
INFO:root:eval mean loss: 6447.079209780863
INFO:root:eval perplexity: 200.402587890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/65
 32%|███▎      | 65/200 [9:51:22<20:08:20, 537.04s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6619.95849609375
INFO:root:current train perplexity173.5967254638672
INFO:root:current mean train loss 6529.622694749099
INFO:root:current train perplexity176.13829040527344
INFO:root:current mean train loss 6545.246369006587
INFO:root:current train perplexity174.85592651367188
INFO:root:current mean train loss 6539.592126143606
INFO:root:current train perplexity175.19802856445312
INFO:root:current mean train loss 6550.868877146504
INFO:root:current train perplexity175.29881286621094
INFO:root:current mean train loss 6537.582233731709
INFO:root:current train perplexity174.8467559814453
INFO:root:current mean train loss 6539.6215019983965
INFO:root:current train perplexity174.9991455078125
INFO:root:current mean train loss 6539.3475598421965
INFO:root:current train perplexity174.94857788085938
INFO:root:current mean train loss 6546.330697586287
INFO:root:current train perplexity175.3352508544922
INFO:root:current mean train loss 6549.25907965466
INFO:root:current train perplexity175.354736328125
INFO:root:current mean train loss 6550.221801271477
INFO:root:current train perplexity175.53024291992188
INFO:root:current mean train loss 6551.182308473449
INFO:root:current train perplexity175.68499755859375
INFO:root:current mean train loss 6552.274396218335
INFO:root:current train perplexity175.8081817626953
INFO:root:current mean train loss 6553.086140825704
INFO:root:current train perplexity175.80316162109375
INFO:root:current mean train loss 6554.3564713959
INFO:root:current train perplexity175.90771484375
INFO:root:current mean train loss 6556.914733886719
INFO:root:current train perplexity176.05322265625
INFO:root:current mean train loss 6556.816531668875
INFO:root:current train perplexity176.04542541503906
INFO:root:current mean train loss 6551.412070977296
INFO:root:current train perplexity175.8689727783203
INFO:root:current mean train loss 6553.118125086613
INFO:root:current train perplexity175.9386749267578
INFO:root:current mean train loss 6553.513015618845
INFO:root:current train perplexity175.8896942138672

100%|██████████| 1/1 [07:43<00:00, 463.14s/it][A100%|██████████| 1/1 [07:43<00:00, 463.14s/it]
INFO:root:final mean train loss: 6551.25043743401
INFO:root:final train perplexity: 175.95579528808594
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.70s/it][A100%|██████████| 1/1 [00:41<00:00, 41.70s/it]
INFO:root:eval mean loss: 6313.647989043107
INFO:root:eval perplexity: 165.5208282470703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.53s/it][A100%|██████████| 1/1 [00:38<00:00, 38.53s/it]
INFO:root:eval mean loss: 6450.73666576629
INFO:root:eval perplexity: 201.00599670410156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/66
 33%|███▎      | 66/200 [10:00:27<20:05:10, 539.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6670.689825148809
INFO:root:current train perplexity181.3535614013672
INFO:root:current mean train loss 6547.506408186983
INFO:root:current train perplexity178.0062255859375
INFO:root:current mean train loss 6572.818343909078
INFO:root:current train perplexity178.26614379882812
INFO:root:current mean train loss 6558.720471913941
INFO:root:current train perplexity177.6580352783203
INFO:root:current mean train loss 6565.495405980923
INFO:root:current train perplexity177.14744567871094
INFO:root:current mean train loss 6550.957631058061
INFO:root:current train perplexity176.85255432128906
INFO:root:current mean train loss 6547.181069784118
INFO:root:current train perplexity176.40220642089844
INFO:root:current mean train loss 6554.671568215803
INFO:root:current train perplexity176.65330505371094
INFO:root:current mean train loss 6554.060115688756
INFO:root:current train perplexity176.87692260742188
INFO:root:current mean train loss 6558.10097878919
INFO:root:current train perplexity176.83973693847656
INFO:root:current mean train loss 6556.29027100805
INFO:root:current train perplexity176.7147216796875
INFO:root:current mean train loss 6553.849039640945
INFO:root:current train perplexity176.66653442382812
INFO:root:current mean train loss 6554.97081109874
INFO:root:current train perplexity176.6234130859375
INFO:root:current mean train loss 6553.209075303984
INFO:root:current train perplexity176.3839569091797
INFO:root:current mean train loss 6557.016112594014
INFO:root:current train perplexity176.61734008789062
INFO:root:current mean train loss 6559.7234401966225
INFO:root:current train perplexity176.63308715820312
INFO:root:current mean train loss 6555.15454297357
INFO:root:current train perplexity176.5153045654297
INFO:root:current mean train loss 6554.945700628268
INFO:root:current train perplexity176.51327514648438
INFO:root:current mean train loss 6558.5850062530035
INFO:root:current train perplexity176.5836639404297
INFO:root:current mean train loss 6558.924546948204
INFO:root:current train perplexity176.56631469726562

100%|██████████| 1/1 [07:32<00:00, 452.24s/it][A100%|██████████| 1/1 [07:32<00:00, 452.24s/it]
INFO:root:final mean train loss: 6554.833634230806
INFO:root:final train perplexity: 176.45404052734375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.98s/it][A100%|██████████| 1/1 [00:37<00:00, 37.98s/it]
INFO:root:eval mean loss: 6307.588089746786
INFO:root:eval perplexity: 164.71121215820312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:36<00:00, 36.99s/it][A100%|██████████| 1/1 [00:36<00:00, 36.99s/it]
INFO:root:eval mean loss: 6445.170892377272
INFO:root:eval perplexity: 200.08843994140625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/67
 34%|███▎      | 67/200 [10:09:17<19:49:28, 536.61s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6563.382170024671
INFO:root:current train perplexity175.69805908203125
INFO:root:current mean train loss 6570.548527372056
INFO:root:current train perplexity175.4856414794922
INFO:root:current mean train loss 6562.727805770745
INFO:root:current train perplexity175.953857421875
INFO:root:current mean train loss 6550.901645998983
INFO:root:current train perplexity175.41134643554688
INFO:root:current mean train loss 6564.411744836259
INFO:root:current train perplexity176.1455078125
INFO:root:current mean train loss 6558.682672550244
INFO:root:current train perplexity175.782958984375
INFO:root:current mean train loss 6551.935446616624
INFO:root:current train perplexity175.49560546875
INFO:root:current mean train loss 6545.836493928904
INFO:root:current train perplexity175.46791076660156
INFO:root:current mean train loss 6545.495718507607
INFO:root:current train perplexity175.4225616455078
INFO:root:current mean train loss 6549.978678038379
INFO:root:current train perplexity175.55821228027344
INFO:root:current mean train loss 6551.818667961224
INFO:root:current train perplexity175.74197387695312
INFO:root:current mean train loss 6556.193316038966
INFO:root:current train perplexity175.90377807617188
INFO:root:current mean train loss 6556.434218103165
INFO:root:current train perplexity175.796875
INFO:root:current mean train loss 6557.462927118367
INFO:root:current train perplexity175.71859741210938
INFO:root:current mean train loss 6557.889598862353
INFO:root:current train perplexity175.81011962890625
INFO:root:current mean train loss 6556.292326491893
INFO:root:current train perplexity175.7651824951172
INFO:root:current mean train loss 6553.905828194158
INFO:root:current train perplexity175.82327270507812
INFO:root:current mean train loss 6551.296752789215
INFO:root:current train perplexity175.78562927246094
INFO:root:current mean train loss 6551.225856644025
INFO:root:current train perplexity175.75411987304688
INFO:root:current mean train loss 6551.51069230118
INFO:root:current train perplexity175.74691772460938

100%|██████████| 1/1 [07:38<00:00, 458.90s/it][A100%|██████████| 1/1 [07:38<00:00, 458.90s/it]
INFO:root:final mean train loss: 6549.829611142957
INFO:root:final train perplexity: 175.75865173339844
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.15s/it][A100%|██████████| 1/1 [00:41<00:00, 41.15s/it]
INFO:root:eval mean loss: 6308.976697556516
INFO:root:eval perplexity: 164.89627075195312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.88s/it][A100%|██████████| 1/1 [00:38<00:00, 38.88s/it]
INFO:root:eval mean loss: 6446.549145854111
INFO:root:eval perplexity: 200.31544494628906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/68
 34%|███▍      | 68/200 [10:18:18<19:43:35, 538.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6603.872975852273
INFO:root:current train perplexity178.82113647460938
INFO:root:current mean train loss 6559.354133064516
INFO:root:current train perplexity175.23562622070312
INFO:root:current mean train loss 6542.104900045956
INFO:root:current train perplexity174.46141052246094
INFO:root:current mean train loss 6550.435051716549
INFO:root:current train perplexity175.7311553955078
INFO:root:current mean train loss 6559.71251717033
INFO:root:current train perplexity176.1332550048828
INFO:root:current mean train loss 6551.515978673987
INFO:root:current train perplexity175.6975555419922
INFO:root:current mean train loss 6552.000685830153
INFO:root:current train perplexity175.93138122558594
INFO:root:current mean train loss 6553.1570823416805
INFO:root:current train perplexity175.97189331054688
INFO:root:current mean train loss 6563.3634622852705
INFO:root:current train perplexity175.90185546875
INFO:root:current mean train loss 6572.751576816099
INFO:root:current train perplexity175.9603271484375
INFO:root:current mean train loss 6570.225068498223
INFO:root:current train perplexity175.68701171875
INFO:root:current mean train loss 6567.205039654356
INFO:root:current train perplexity175.67117309570312
INFO:root:current mean train loss 6565.485330552789
INFO:root:current train perplexity175.6585235595703
INFO:root:current mean train loss 6559.582357731781
INFO:root:current train perplexity175.37371826171875
INFO:root:current mean train loss 6551.221156840636
INFO:root:current train perplexity175.25550842285156
INFO:root:current mean train loss 6548.643419036374
INFO:root:current train perplexity175.18165588378906
INFO:root:current mean train loss 6549.760255086386
INFO:root:current train perplexity175.28367614746094
INFO:root:current mean train loss 6548.2461674790775
INFO:root:current train perplexity175.2749786376953
INFO:root:current mean train loss 6547.886272847877
INFO:root:current train perplexity175.27865600585938
INFO:root:current mean train loss 6546.6462588415125
INFO:root:current train perplexity175.30331420898438

100%|██████████| 1/1 [07:43<00:00, 463.38s/it][A100%|██████████| 1/1 [07:43<00:00, 463.38s/it]
INFO:root:final mean train loss: 6547.00742480518
INFO:root:final train perplexity: 175.36752319335938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.72s/it][A100%|██████████| 1/1 [00:39<00:00, 39.72s/it]
INFO:root:eval mean loss: 6307.992312167553
INFO:root:eval perplexity: 164.7649383544922
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.97s/it][A100%|██████████| 1/1 [00:37<00:00, 37.97s/it]
INFO:root:eval mean loss: 6446.037521470523
INFO:root:eval perplexity: 200.2311248779297
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/69
 34%|███▍      | 69/200 [10:27:22<19:38:11, 539.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6515.164272732205
INFO:root:current train perplexity175.32907104492188
INFO:root:current mean train loss 6533.642683162246
INFO:root:current train perplexity175.3881072998047
INFO:root:current mean train loss 6538.4856710994945
INFO:root:current train perplexity174.88711547851562
INFO:root:current mean train loss 6545.513090400285
INFO:root:current train perplexity174.96084594726562
INFO:root:current mean train loss 6552.1831758143535
INFO:root:current train perplexity175.31979370117188
INFO:root:current mean train loss 6552.354748279065
INFO:root:current train perplexity175.25836181640625
INFO:root:current mean train loss 6550.799255371094
INFO:root:current train perplexity175.4597930908203
INFO:root:current mean train loss 6556.002369302542
INFO:root:current train perplexity175.4568634033203
INFO:root:current mean train loss 6561.117881844897
INFO:root:current train perplexity175.43695068359375
INFO:root:current mean train loss 6555.045086142458
INFO:root:current train perplexity175.49794006347656
INFO:root:current mean train loss 6555.006052956653
INFO:root:current train perplexity175.39144897460938
INFO:root:current mean train loss 6556.351099216084
INFO:root:current train perplexity175.37823486328125
INFO:root:current mean train loss 6556.659748581221
INFO:root:current train perplexity175.38909912109375
INFO:root:current mean train loss 6552.855337426544
INFO:root:current train perplexity175.34469604492188
INFO:root:current mean train loss 6553.368838434634
INFO:root:current train perplexity175.42457580566406
INFO:root:current mean train loss 6552.6018069512365
INFO:root:current train perplexity175.46438598632812
INFO:root:current mean train loss 6553.563435093638
INFO:root:current train perplexity175.5460662841797
INFO:root:current mean train loss 6554.2422817393835
INFO:root:current train perplexity175.4901580810547
INFO:root:current mean train loss 6552.502309945913
INFO:root:current train perplexity175.35675048828125
INFO:root:current mean train loss 6548.285227560852
INFO:root:current train perplexity175.32931518554688

100%|██████████| 1/1 [07:39<00:00, 459.72s/it][A100%|██████████| 1/1 [07:39<00:00, 459.72s/it]
INFO:root:final mean train loss: 6546.6709283548835
INFO:root:final train perplexity: 175.321044921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.66s/it][A100%|██████████| 1/1 [00:39<00:00, 39.66s/it]
INFO:root:eval mean loss: 6306.707912580341
INFO:root:eval perplexity: 164.59375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.40s/it][A100%|██████████| 1/1 [00:38<00:00, 38.40s/it]
INFO:root:eval mean loss: 6444.699324371121
INFO:root:eval perplexity: 200.010986328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/70
 35%|███▌      | 70/200 [10:36:22<19:29:30, 539.77s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6543.15350684691
INFO:root:current train perplexity177.486328125
INFO:root:current mean train loss 6548.434663318452
INFO:root:current train perplexity176.1792755126953
INFO:root:current mean train loss 6570.5855742457825
INFO:root:current train perplexity177.06375122070312
INFO:root:current mean train loss 6564.593879287838
INFO:root:current train perplexity176.88282775878906
INFO:root:current mean train loss 6564.407262509586
INFO:root:current train perplexity177.06527709960938
INFO:root:current mean train loss 6559.313934170735
INFO:root:current train perplexity176.66510009765625
INFO:root:current mean train loss 6547.5390001360665
INFO:root:current train perplexity176.08689880371094
INFO:root:current mean train loss 6547.964879643932
INFO:root:current train perplexity175.90377807617188
INFO:root:current mean train loss 6545.744787089602
INFO:root:current train perplexity175.86570739746094
INFO:root:current mean train loss 6547.103425769401
INFO:root:current train perplexity175.90118408203125
INFO:root:current mean train loss 6542.213071768824
INFO:root:current train perplexity175.9159393310547
INFO:root:current mean train loss 6549.330512198408
INFO:root:current train perplexity176.04550170898438
INFO:root:current mean train loss 6548.876625836404
INFO:root:current train perplexity175.9770965576172
INFO:root:current mean train loss 6549.603872080858
INFO:root:current train perplexity175.93179321289062
INFO:root:current mean train loss 6549.3404376233
INFO:root:current train perplexity176.0160369873047
INFO:root:current mean train loss 6548.961902385541
INFO:root:current train perplexity175.98582458496094
INFO:root:current mean train loss 6549.756719432264
INFO:root:current train perplexity175.85069274902344
INFO:root:current mean train loss 6550.883937539303
INFO:root:current train perplexity175.8963165283203
INFO:root:current mean train loss 6554.5914492104785
INFO:root:current train perplexity175.91989135742188

100%|██████████| 1/1 [07:47<00:00, 467.31s/it][A100%|██████████| 1/1 [07:47<00:00, 467.31s/it]
INFO:root:final mean train loss: 6550.745822769912
INFO:root:final train perplexity: 175.8856658935547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.32s/it][A100%|██████████| 1/1 [00:40<00:00, 40.32s/it]
INFO:root:eval mean loss: 6309.76724048371
INFO:root:eval perplexity: 165.0018310546875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.22s/it][A100%|██████████| 1/1 [00:38<00:00, 38.22s/it]
INFO:root:eval mean loss: 6447.508886026152
INFO:root:eval perplexity: 200.47349548339844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/71
 36%|███▌      | 71/200 [10:45:30<19:26:00, 542.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6589.07373046875
INFO:root:current train perplexity169.63966369628906
INFO:root:current mean train loss 6610.341170400943
INFO:root:current train perplexity178.16357421875
INFO:root:current mean train loss 6567.358609393962
INFO:root:current train perplexity176.2423858642578
INFO:root:current mean train loss 6549.290846481822
INFO:root:current train perplexity175.9005889892578
INFO:root:current mean train loss 6536.273577008928
INFO:root:current train perplexity175.5281524658203
INFO:root:current mean train loss 6541.463952105978
INFO:root:current train perplexity175.47994995117188
INFO:root:current mean train loss 6538.944832276196
INFO:root:current train perplexity175.4227294921875
INFO:root:current mean train loss 6545.310909973663
INFO:root:current train perplexity175.3954620361328
INFO:root:current mean train loss 6546.607814438586
INFO:root:current train perplexity175.5458984375
INFO:root:current mean train loss 6548.208866885692
INFO:root:current train perplexity175.63742065429688
INFO:root:current mean train loss 6547.026698694552
INFO:root:current train perplexity175.68197631835938
INFO:root:current mean train loss 6547.870211665066
INFO:root:current train perplexity175.75001525878906
INFO:root:current mean train loss 6548.477519223544
INFO:root:current train perplexity175.66665649414062
INFO:root:current mean train loss 6541.8279847967315
INFO:root:current train perplexity175.6373291015625
INFO:root:current mean train loss 6547.691026321346
INFO:root:current train perplexity175.70208740234375
INFO:root:current mean train loss 6552.214649864085
INFO:root:current train perplexity175.80801391601562
INFO:root:current mean train loss 6551.272707206374
INFO:root:current train perplexity175.707275390625
INFO:root:current mean train loss 6551.560161058397
INFO:root:current train perplexity175.78965759277344
INFO:root:current mean train loss 6550.296854722539
INFO:root:current train perplexity175.93775939941406
INFO:root:current mean train loss 6555.452127686828
INFO:root:current train perplexity176.09571838378906

100%|██████████| 1/1 [07:38<00:00, 458.85s/it][A100%|██████████| 1/1 [07:38<00:00, 458.85s/it]
INFO:root:final mean train loss: 6551.495252123519
INFO:root:final train perplexity: 175.98977661132812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.87s/it][A100%|██████████| 1/1 [00:39<00:00, 39.87s/it]
INFO:root:eval mean loss: 6311.235503933954
INFO:root:eval perplexity: 165.19793701171875
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.77s/it][A100%|██████████| 1/1 [00:39<00:00, 39.77s/it]
INFO:root:eval mean loss: 6448.320241508754
INFO:root:eval perplexity: 200.60728454589844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/72
 36%|███▌      | 72/200 [10:54:31<19:16:02, 541.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6586.558169157609
INFO:root:current train perplexity180.30050659179688
INFO:root:current mean train loss 6495.438551988059
INFO:root:current train perplexity175.39002990722656
INFO:root:current mean train loss 6510.557015046945
INFO:root:current train perplexity175.9985809326172
INFO:root:current mean train loss 6521.017219850522
INFO:root:current train perplexity175.9347381591797
INFO:root:current mean train loss 6515.580229342125
INFO:root:current train perplexity175.33090209960938
INFO:root:current mean train loss 6517.18309799982
INFO:root:current train perplexity175.29254150390625
INFO:root:current mean train loss 6518.085315979885
INFO:root:current train perplexity175.27749633789062
INFO:root:current mean train loss 6526.589771487076
INFO:root:current train perplexity175.50648498535156
INFO:root:current mean train loss 6531.924199527263
INFO:root:current train perplexity175.5693359375
INFO:root:current mean train loss 6539.351113894908
INFO:root:current train perplexity175.6185760498047
INFO:root:current mean train loss 6540.078766018298
INFO:root:current train perplexity175.63809204101562
INFO:root:current mean train loss 6545.532923113313
INFO:root:current train perplexity175.76141357421875
INFO:root:current mean train loss 6545.823139181316
INFO:root:current train perplexity175.81900024414062
INFO:root:current mean train loss 6549.839866263345
INFO:root:current train perplexity175.89044189453125
INFO:root:current mean train loss 6549.67091387858
INFO:root:current train perplexity175.90931701660156
INFO:root:current mean train loss 6546.370695558725
INFO:root:current train perplexity175.8592529296875
INFO:root:current mean train loss 6548.259755095213
INFO:root:current train perplexity175.9685516357422
INFO:root:current mean train loss 6548.459279950939
INFO:root:current train perplexity175.8979949951172
INFO:root:current mean train loss 6552.08791740949
INFO:root:current train perplexity176.07748413085938
INFO:root:current mean train loss 6552.084599106621
INFO:root:current train perplexity176.044921875

100%|██████████| 1/1 [07:43<00:00, 463.79s/it][A100%|██████████| 1/1 [07:43<00:00, 463.79s/it]
INFO:root:final mean train loss: 6552.199909435278
INFO:root:final train perplexity: 176.087646484375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.41s/it][A100%|██████████| 1/1 [00:40<00:00, 40.41s/it]
INFO:root:eval mean loss: 6308.097967918883
INFO:root:eval perplexity: 164.7790069580078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.86s/it][A100%|██████████| 1/1 [00:38<00:00, 38.86s/it]
INFO:root:eval mean loss: 6445.705559480275
INFO:root:eval perplexity: 200.17642211914062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/73
 36%|███▋      | 73/200 [11:03:36<19:09:11, 542.93s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6546.766809082032
INFO:root:current train perplexity178.17860412597656
INFO:root:current mean train loss 6529.472384207589
INFO:root:current train perplexity175.2456512451172
INFO:root:current mean train loss 6565.979457600912
INFO:root:current train perplexity175.90797424316406
INFO:root:current mean train loss 6576.892682961857
INFO:root:current train perplexity175.96519470214844
INFO:root:current mean train loss 6566.264928089488
INFO:root:current train perplexity175.6644744873047
INFO:root:current mean train loss 6560.2492142288775
INFO:root:current train perplexity175.94236755371094
INFO:root:current mean train loss 6557.688883972168
INFO:root:current train perplexity176.0993194580078
INFO:root:current mean train loss 6556.038478542018
INFO:root:current train perplexity176.09957885742188
INFO:root:current mean train loss 6552.301618884859
INFO:root:current train perplexity176.07009887695312
INFO:root:current mean train loss 6550.779482317986
INFO:root:current train perplexity176.0979766845703
INFO:root:current mean train loss 6551.3852360652045
INFO:root:current train perplexity176.18020629882812
INFO:root:current mean train loss 6550.3335779022755
INFO:root:current train perplexity176.28465270996094
INFO:root:current mean train loss 6551.336560058594
INFO:root:current train perplexity176.2359161376953
INFO:root:current mean train loss 6551.397499927122
INFO:root:current train perplexity176.2759246826172
INFO:root:current mean train loss 6551.23192952474
INFO:root:current train perplexity176.2644805908203
INFO:root:current mean train loss 6550.579856178977
INFO:root:current train perplexity176.30441284179688
INFO:root:current mean train loss 6551.297499940453
INFO:root:current train perplexity176.36058044433594
INFO:root:current mean train loss 6556.257181663074
INFO:root:current train perplexity176.42433166503906
INFO:root:current mean train loss 6557.814943794582
INFO:root:current train perplexity176.438720703125
INFO:root:current mean train loss 6556.544040451837
INFO:root:current train perplexity176.3166046142578

100%|██████████| 1/1 [07:43<00:00, 463.61s/it][A100%|██████████| 1/1 [07:43<00:00, 463.61s/it]
INFO:root:final mean train loss: 6553.344584485707
INFO:root:final train perplexity: 176.24684143066406
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.85s/it][A100%|██████████| 1/1 [00:40<00:00, 40.85s/it]
INFO:root:eval mean loss: 6308.348754017065
INFO:root:eval perplexity: 164.81256103515625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.96s/it][A100%|██████████| 1/1 [00:38<00:00, 38.96s/it]
INFO:root:eval mean loss: 6446.320479589152
INFO:root:eval perplexity: 200.27772521972656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/74
 37%|███▋      | 74/200 [11:12:42<19:01:51, 543.74s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6411.056032415022
INFO:root:current train perplexity174.04779052734375
INFO:root:current mean train loss 6516.283573223527
INFO:root:current train perplexity175.55812072753906
INFO:root:current mean train loss 6538.17233098249
INFO:root:current train perplexity176.2080078125
INFO:root:current mean train loss 6544.627125459559
INFO:root:current train perplexity176.65777587890625
INFO:root:current mean train loss 6558.640878222443
INFO:root:current train perplexity177.14187622070312
INFO:root:current mean train loss 6556.020801482551
INFO:root:current train perplexity176.3207244873047
INFO:root:current mean train loss 6559.748948374477
INFO:root:current train perplexity176.20465087890625
INFO:root:current mean train loss 6561.959780976511
INFO:root:current train perplexity176.28273010253906
INFO:root:current mean train loss 6560.909199628974
INFO:root:current train perplexity176.2621307373047
INFO:root:current mean train loss 6556.638406560214
INFO:root:current train perplexity175.9715576171875
INFO:root:current mean train loss 6557.028863103861
INFO:root:current train perplexity175.86822509765625
INFO:root:current mean train loss 6556.4057558104205
INFO:root:current train perplexity175.73660278320312
INFO:root:current mean train loss 6559.7041874098795
INFO:root:current train perplexity175.96250915527344
INFO:root:current mean train loss 6558.158558271348
INFO:root:current train perplexity175.88063049316406
INFO:root:current mean train loss 6560.130619423473
INFO:root:current train perplexity175.967041015625
INFO:root:current mean train loss 6559.201362859766
INFO:root:current train perplexity175.91603088378906
INFO:root:current mean train loss 6555.01917645783
INFO:root:current train perplexity175.75546264648438
INFO:root:current mean train loss 6554.187915747723
INFO:root:current train perplexity175.72537231445312
INFO:root:current mean train loss 6553.819928080406
INFO:root:current train perplexity175.67787170410156
INFO:root:current mean train loss 6550.703222057438
INFO:root:current train perplexity175.52548217773438

100%|██████████| 1/1 [07:40<00:00, 460.56s/it][A100%|██████████| 1/1 [07:40<00:00, 460.56s/it]
INFO:root:final mean train loss: 6548.170031941424
INFO:root:final train perplexity: 175.5284881591797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.98s/it][A100%|██████████| 1/1 [00:38<00:00, 38.98s/it]
INFO:root:eval mean loss: 6303.720202723293
INFO:root:eval perplexity: 164.19639587402344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.27s/it][A100%|██████████| 1/1 [00:39<00:00, 39.27s/it]
INFO:root:eval mean loss: 6444.219108419215
INFO:root:eval perplexity: 199.93202209472656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/75
 38%|███▊      | 75/200 [11:21:43<18:51:07, 542.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6547.680043813345
INFO:root:current train perplexity175.6454620361328
INFO:root:current mean train loss 6573.595590876437
INFO:root:current train perplexity175.59461975097656
INFO:root:current mean train loss 6572.817618042884
INFO:root:current train perplexity176.06011962890625
INFO:root:current mean train loss 6554.415751900902
INFO:root:current train perplexity175.01116943359375
INFO:root:current mean train loss 6558.3914233501455
INFO:root:current train perplexity175.28501892089844
INFO:root:current mean train loss 6544.03255066556
INFO:root:current train perplexity174.55044555664062
INFO:root:current mean train loss 6552.59507212653
INFO:root:current train perplexity174.9913787841797
INFO:root:current mean train loss 6558.558215237403
INFO:root:current train perplexity175.162109375
INFO:root:current mean train loss 6557.723351240704
INFO:root:current train perplexity175.11634826660156
INFO:root:current mean train loss 6548.4098068331305
INFO:root:current train perplexity174.8678436279297
INFO:root:current mean train loss 6545.654712868803
INFO:root:current train perplexity174.92764282226562
INFO:root:current mean train loss 6541.03389270789
INFO:root:current train perplexity174.69883728027344
INFO:root:current mean train loss 6539.521607786744
INFO:root:current train perplexity174.71824645996094
INFO:root:current mean train loss 6542.132658268513
INFO:root:current train perplexity174.79541015625
INFO:root:current mean train loss 6544.547285103248
INFO:root:current train perplexity175.0090789794922
INFO:root:current mean train loss 6547.407624260443
INFO:root:current train perplexity175.2904510498047
INFO:root:current mean train loss 6546.416257140457
INFO:root:current train perplexity175.32974243164062
INFO:root:current mean train loss 6546.5038729455855
INFO:root:current train perplexity175.3013916015625
INFO:root:current mean train loss 6547.680774538087
INFO:root:current train perplexity175.31996154785156
INFO:root:current mean train loss 6549.569101196413
INFO:root:current train perplexity175.3909454345703

100%|██████████| 1/1 [07:43<00:00, 463.41s/it][A100%|██████████| 1/1 [07:43<00:00, 463.41s/it]
INFO:root:final mean train loss: 6547.055013390183
INFO:root:final train perplexity: 175.3742218017578
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 40.00s/it][A100%|██████████| 1/1 [00:39<00:00, 40.00s/it]
INFO:root:eval mean loss: 6309.597351507092
INFO:root:eval perplexity: 164.9791717529297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.95s/it][A100%|██████████| 1/1 [00:37<00:00, 37.95s/it]
INFO:root:eval mean loss: 6447.574014433732
INFO:root:eval perplexity: 200.4842987060547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/76
 38%|███▊      | 76/200 [11:30:47<18:42:37, 543.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6542.4254646720465
INFO:root:current train perplexity177.686767578125
INFO:root:current mean train loss 6560.451851889725
INFO:root:current train perplexity176.3949737548828
INFO:root:current mean train loss 6557.140893470791
INFO:root:current train perplexity176.62643432617188
INFO:root:current mean train loss 6536.796077016064
INFO:root:current train perplexity175.94052124023438
INFO:root:current mean train loss 6535.272238177826
INFO:root:current train perplexity175.72337341308594
INFO:root:current mean train loss 6541.871227593591
INFO:root:current train perplexity175.68910217285156
INFO:root:current mean train loss 6537.501773641009
INFO:root:current train perplexity175.36276245117188
INFO:root:current mean train loss 6537.825208275719
INFO:root:current train perplexity175.53041076660156
INFO:root:current mean train loss 6541.998431033425
INFO:root:current train perplexity175.6068572998047
INFO:root:current mean train loss 6537.661337289512
INFO:root:current train perplexity175.58433532714844
INFO:root:current mean train loss 6541.235617409487
INFO:root:current train perplexity175.70098876953125
INFO:root:current mean train loss 6544.636564189101
INFO:root:current train perplexity175.6446990966797
INFO:root:current mean train loss 6550.801700323151
INFO:root:current train perplexity175.71749877929688
INFO:root:current mean train loss 6553.695407628842
INFO:root:current train perplexity175.7737274169922
INFO:root:current mean train loss 6549.901011537978
INFO:root:current train perplexity175.62435913085938
INFO:root:current mean train loss 6549.726829811734
INFO:root:current train perplexity175.61598205566406
INFO:root:current mean train loss 6549.8744871747485
INFO:root:current train perplexity175.5233917236328
INFO:root:current mean train loss 6549.495548488972
INFO:root:current train perplexity175.59461975097656
INFO:root:current mean train loss 6551.925538529548
INFO:root:current train perplexity175.70199584960938

100%|██████████| 1/1 [07:43<00:00, 463.86s/it][A100%|██████████| 1/1 [07:43<00:00, 463.86s/it]
INFO:root:final mean train loss: 6548.460186979954
INFO:root:final train perplexity: 175.5686798095703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.30s/it][A100%|██████████| 1/1 [00:40<00:00, 40.30s/it]
INFO:root:eval mean loss: 6310.13863551363
INFO:root:eval perplexity: 165.05133056640625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 39.00s/it][A100%|██████████| 1/1 [00:38<00:00, 39.00s/it]
INFO:root:eval mean loss: 6447.890560068982
INFO:root:eval perplexity: 200.53640747070312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/77
 38%|███▊      | 77/200 [11:39:52<18:34:59, 543.90s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6521.649169921875
INFO:root:current train perplexity177.2307586669922
INFO:root:current mean train loss 6562.960883246527
INFO:root:current train perplexity176.51724243164062
INFO:root:current mean train loss 6549.005075307993
INFO:root:current train perplexity176.39649963378906
INFO:root:current mean train loss 6559.222984413048
INFO:root:current train perplexity176.111083984375
INFO:root:current mean train loss 6551.366213331035
INFO:root:current train perplexity175.51626586914062
INFO:root:current mean train loss 6548.273834468811
INFO:root:current train perplexity176.05918884277344
INFO:root:current mean train loss 6557.9075734991775
INFO:root:current train perplexity176.1841583251953
INFO:root:current mean train loss 6552.98038943878
INFO:root:current train perplexity176.08999633789062
INFO:root:current mean train loss 6541.564812688544
INFO:root:current train perplexity175.69253540039062
INFO:root:current mean train loss 6544.655442830224
INFO:root:current train perplexity175.67837524414062
INFO:root:current mean train loss 6552.10295952691
INFO:root:current train perplexity175.85287475585938
INFO:root:current mean train loss 6554.497569610926
INFO:root:current train perplexity175.8722381591797
INFO:root:current mean train loss 6555.732003117239
INFO:root:current train perplexity175.7718963623047
INFO:root:current mean train loss 6552.435187383529
INFO:root:current train perplexity175.68331909179688
INFO:root:current mean train loss 6551.400064294989
INFO:root:current train perplexity175.5809783935547
INFO:root:current mean train loss 6550.951348342694
INFO:root:current train perplexity175.6703338623047
INFO:root:current mean train loss 6551.570631340369
INFO:root:current train perplexity175.63540649414062
INFO:root:current mean train loss 6549.277999556316
INFO:root:current train perplexity175.5876007080078
INFO:root:current mean train loss 6550.629076122183
INFO:root:current train perplexity175.7052764892578
INFO:root:current mean train loss 6552.195018200505
INFO:root:current train perplexity175.77432250976562

100%|██████████| 1/1 [07:47<00:00, 467.01s/it][A100%|██████████| 1/1 [07:47<00:00, 467.01s/it]
INFO:root:final mean train loss: 6549.787365581072
INFO:root:final train perplexity: 175.75277709960938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.07s/it][A100%|██████████| 1/1 [00:39<00:00, 39.07s/it]
INFO:root:eval mean loss: 6310.934159948471
INFO:root:eval perplexity: 165.15768432617188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.56s/it][A100%|██████████| 1/1 [00:38<00:00, 38.56s/it]
INFO:root:eval mean loss: 6448.706495352671
INFO:root:eval perplexity: 200.67098999023438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/78
 39%|███▉      | 78/200 [11:48:59<18:27:47, 544.82s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6421.2225
INFO:root:current train perplexity176.2197723388672
INFO:root:current mean train loss 6547.58698046875
INFO:root:current train perplexity176.0541534423828
INFO:root:current mean train loss 6529.296955295139
INFO:root:current train perplexity175.90528869628906
INFO:root:current mean train loss 6524.404466646634
INFO:root:current train perplexity175.3362579345703
INFO:root:current mean train loss 6531.748008961397
INFO:root:current train perplexity175.1810760498047
INFO:root:current mean train loss 6530.235237165179
INFO:root:current train perplexity175.35565185546875
INFO:root:current mean train loss 6536.91819140625
INFO:root:current train perplexity175.39964294433594
INFO:root:current mean train loss 6539.47981950431
INFO:root:current train perplexity175.64244079589844
INFO:root:current mean train loss 6544.291209753788
INFO:root:current train perplexity175.7946014404297
INFO:root:current mean train loss 6543.060590688345
INFO:root:current train perplexity175.7367706298828
INFO:root:current mean train loss 6540.217187976372
INFO:root:current train perplexity175.58499145507812
INFO:root:current mean train loss 6543.777047309028
INFO:root:current train perplexity175.7367706298828
INFO:root:current mean train loss 6549.039770009566
INFO:root:current train perplexity176.0103302001953
INFO:root:current mean train loss 6550.970742924528
INFO:root:current train perplexity176.01663208007812
INFO:root:current mean train loss 6552.481088610198
INFO:root:current train perplexity175.98004150390625
INFO:root:current mean train loss 6555.79586225666
INFO:root:current train perplexity176.11981201171875
INFO:root:current mean train loss 6552.470016526442
INFO:root:current train perplexity175.96510314941406
INFO:root:current mean train loss 6550.5773445991845
INFO:root:current train perplexity175.83644104003906
INFO:root:current mean train loss 6553.105437178939
INFO:root:current train perplexity175.9574737548828
INFO:root:current mean train loss 6552.962540584415
INFO:root:current train perplexity175.90504455566406

100%|██████████| 1/1 [07:47<00:00, 467.14s/it][A100%|██████████| 1/1 [07:47<00:00, 467.14s/it]
INFO:root:final mean train loss: 6551.274402981988
INFO:root:final train perplexity: 175.9590606689453
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.04s/it][A100%|██████████| 1/1 [00:41<00:00, 41.04s/it]
INFO:root:eval mean loss: 6309.38634474734
INFO:root:eval perplexity: 164.9510040283203
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.97s/it][A100%|██████████| 1/1 [00:39<00:00, 39.97s/it]
INFO:root:eval mean loss: 6447.443742035129
INFO:root:eval perplexity: 200.4626922607422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/79
 40%|███▉      | 79/200 [11:58:10<18:22:10, 546.54s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6538.527925037202
INFO:root:current train perplexity174.10671997070312
INFO:root:current mean train loss 6511.049949108715
INFO:root:current train perplexity174.7176513671875
INFO:root:current mean train loss 6519.676790095557
INFO:root:current train perplexity174.58499145507812
INFO:root:current mean train loss 6531.512933742233
INFO:root:current train perplexity175.65467834472656
INFO:root:current mean train loss 6543.580518903776
INFO:root:current train perplexity176.2271728515625
INFO:root:current mean train loss 6542.537750807195
INFO:root:current train perplexity175.96754455566406
INFO:root:current mean train loss 6539.093564422703
INFO:root:current train perplexity175.53150939941406
INFO:root:current mean train loss 6546.593160377359
INFO:root:current train perplexity175.76400756835938
INFO:root:current mean train loss 6544.479664419722
INFO:root:current train perplexity175.62628173828125
INFO:root:current mean train loss 6545.480162407942
INFO:root:current train perplexity175.6096954345703
INFO:root:current mean train loss 6552.663793054972
INFO:root:current train perplexity175.8592529296875
INFO:root:current mean train loss 6547.807848073555
INFO:root:current train perplexity175.57872009277344
INFO:root:current mean train loss 6547.370618442406
INFO:root:current train perplexity175.71766662597656
INFO:root:current mean train loss 6542.055045160558
INFO:root:current train perplexity175.569091796875
INFO:root:current mean train loss 6543.567753256111
INFO:root:current train perplexity175.70333862304688
INFO:root:current mean train loss 6548.2671835734845
INFO:root:current train perplexity175.82444763183594
INFO:root:current mean train loss 6551.839252876161
INFO:root:current train perplexity175.95654296875
INFO:root:current mean train loss 6550.906701281752
INFO:root:current train perplexity175.8111114501953
INFO:root:current mean train loss 6548.746095870657
INFO:root:current train perplexity175.77005004882812
INFO:root:current mean train loss 6550.287084986081
INFO:root:current train perplexity175.8651123046875

100%|██████████| 1/1 [07:44<00:00, 464.92s/it][A100%|██████████| 1/1 [07:44<00:00, 464.92s/it]
INFO:root:final mean train loss: 6550.99049759873
INFO:root:final train perplexity: 175.9197235107422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.24s/it][A100%|██████████| 1/1 [00:39<00:00, 39.24s/it]
INFO:root:eval mean loss: 6311.140069190492
INFO:root:eval perplexity: 165.1852569580078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.13s/it][A100%|██████████| 1/1 [00:39<00:00, 39.13s/it]
INFO:root:eval mean loss: 6449.044964296598
INFO:root:eval perplexity: 200.72679138183594
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/80
 40%|████      | 80/200 [12:07:15<18:12:34, 546.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6519.630900754767
INFO:root:current train perplexity175.06173706054688
INFO:root:current mean train loss 6577.307724670794
INFO:root:current train perplexity177.03404235839844
INFO:root:current mean train loss 6588.83453109918
INFO:root:current train perplexity176.35073852539062
INFO:root:current mean train loss 6586.847924192636
INFO:root:current train perplexity176.7679901123047
INFO:root:current mean train loss 6586.458709916258
INFO:root:current train perplexity176.8126678466797
INFO:root:current mean train loss 6581.041953753913
INFO:root:current train perplexity176.6779022216797
INFO:root:current mean train loss 6584.251501149943
INFO:root:current train perplexity176.8625030517578
INFO:root:current mean train loss 6582.353646862648
INFO:root:current train perplexity176.87127685546875
INFO:root:current mean train loss 6576.177016448087
INFO:root:current train perplexity176.78131103515625
INFO:root:current mean train loss 6569.193800813836
INFO:root:current train perplexity176.39413452148438
INFO:root:current mean train loss 6559.504230387601
INFO:root:current train perplexity176.09974670410156
INFO:root:current mean train loss 6559.990050268955
INFO:root:current train perplexity176.13031005859375
INFO:root:current mean train loss 6559.454658102289
INFO:root:current train perplexity176.04164123535156
INFO:root:current mean train loss 6559.398474148041
INFO:root:current train perplexity176.08529663085938
INFO:root:current mean train loss 6553.805573702022
INFO:root:current train perplexity176.02838134765625
INFO:root:current mean train loss 6555.434710940006
INFO:root:current train perplexity176.09864807128906
INFO:root:current mean train loss 6552.554079135115
INFO:root:current train perplexity176.07908630371094
INFO:root:current mean train loss 6551.922429347729
INFO:root:current train perplexity176.17189025878906
INFO:root:current mean train loss 6549.532034034175
INFO:root:current train perplexity175.96678161621094
INFO:root:current mean train loss 6553.453166126292
INFO:root:current train perplexity175.9901885986328

100%|██████████| 1/1 [07:38<00:00, 458.61s/it][A100%|██████████| 1/1 [07:38<00:00, 458.61s/it]
INFO:root:final mean train loss: 6551.303649132864
INFO:root:final train perplexity: 175.9631805419922
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.94s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6312.053333471853
INFO:root:eval perplexity: 165.30738830566406
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.11s/it][A100%|██████████| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 6449.758099062223
INFO:root:eval perplexity: 200.84445190429688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/81
 40%|████      | 81/200 [12:16:14<17:59:06, 544.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6504.173494037829
INFO:root:current train perplexity175.02017211914062
INFO:root:current mean train loss 6529.820972789417
INFO:root:current train perplexity175.56666564941406
INFO:root:current mean train loss 6556.756952700408
INFO:root:current train perplexity175.68357849121094
INFO:root:current mean train loss 6547.046214002244
INFO:root:current train perplexity175.55787658691406
INFO:root:current mean train loss 6542.837231034992
INFO:root:current train perplexity175.7467498779297
INFO:root:current mean train loss 6556.184491475423
INFO:root:current train perplexity176.17735290527344
INFO:root:current mean train loss 6553.911408734745
INFO:root:current train perplexity176.3874053955078
INFO:root:current mean train loss 6548.708117298244
INFO:root:current train perplexity176.3852996826172
INFO:root:current mean train loss 6552.739402736159
INFO:root:current train perplexity176.34738159179688
INFO:root:current mean train loss 6554.8750955550395
INFO:root:current train perplexity176.3726043701172
INFO:root:current mean train loss 6554.25714315535
INFO:root:current train perplexity176.3214874267578
INFO:root:current mean train loss 6554.720449434657
INFO:root:current train perplexity176.43502807617188
INFO:root:current mean train loss 6556.537824576925
INFO:root:current train perplexity176.63031005859375
INFO:root:current mean train loss 6553.889734667401
INFO:root:current train perplexity176.4919891357422
INFO:root:current mean train loss 6550.666280276084
INFO:root:current train perplexity176.2035675048828
INFO:root:current mean train loss 6552.030566220356
INFO:root:current train perplexity176.18104553222656
INFO:root:current mean train loss 6553.783258479079
INFO:root:current train perplexity176.38665771484375
INFO:root:current mean train loss 6554.2902361895585
INFO:root:current train perplexity176.43409729003906
INFO:root:current mean train loss 6552.31027904909
INFO:root:current train perplexity176.2928924560547
INFO:root:current mean train loss 6556.870975880488
INFO:root:current train perplexity176.44216918945312

100%|██████████| 1/1 [07:40<00:00, 460.65s/it][A100%|██████████| 1/1 [07:40<00:00, 460.66s/it]
INFO:root:final mean train loss: 6554.78223505756
INFO:root:final train perplexity: 176.44688415527344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.91s/it][A100%|██████████| 1/1 [00:40<00:00, 40.92s/it]
INFO:root:eval mean loss: 6312.289220065935
INFO:root:eval perplexity: 165.33900451660156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.82s/it][A100%|██████████| 1/1 [00:38<00:00, 38.82s/it]
INFO:root:eval mean loss: 6449.449418737533
INFO:root:eval perplexity: 200.79360961914062
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/82
 41%|████      | 82/200 [12:25:17<17:49:19, 543.73s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6544.458921370968
INFO:root:current train perplexity177.05792236328125
INFO:root:current mean train loss 6556.519746296146
INFO:root:current train perplexity176.70065307617188
INFO:root:current mean train loss 6556.557690513012
INFO:root:current train perplexity175.90386962890625
INFO:root:current mean train loss 6557.930122356075
INFO:root:current train perplexity175.7929229736328
INFO:root:current mean train loss 6549.565833782328
INFO:root:current train perplexity175.66522216796875
INFO:root:current mean train loss 6550.140226470278
INFO:root:current train perplexity175.74464416503906
INFO:root:current mean train loss 6548.322263511228
INFO:root:current train perplexity175.5937957763672
INFO:root:current mean train loss 6553.414494133236
INFO:root:current train perplexity175.64906311035156
INFO:root:current mean train loss 6557.461251902821
INFO:root:current train perplexity175.6818084716797
INFO:root:current mean train loss 6551.373104406628
INFO:root:current train perplexity175.62452697753906
INFO:root:current mean train loss 6543.427766986648
INFO:root:current train perplexity175.5721893310547
INFO:root:current mean train loss 6542.621771122564
INFO:root:current train perplexity175.4856414794922
INFO:root:current mean train loss 6549.093009081352
INFO:root:current train perplexity175.62786865234375
INFO:root:current mean train loss 6550.55397242911
INFO:root:current train perplexity175.69195556640625
INFO:root:current mean train loss 6550.218016106413
INFO:root:current train perplexity175.85203552246094
INFO:root:current mean train loss 6549.242654325075
INFO:root:current train perplexity175.9484100341797
INFO:root:current mean train loss 6546.8414931773295
INFO:root:current train perplexity175.85028076171875
INFO:root:current mean train loss 6550.214606009045
INFO:root:current train perplexity175.9333038330078
INFO:root:current mean train loss 6553.332463816116
INFO:root:current train perplexity175.9446258544922

100%|██████████| 1/1 [07:43<00:00, 464.00s/it][A100%|██████████| 1/1 [07:43<00:00, 464.00s/it]
INFO:root:final mean train loss: 6551.589527093577
INFO:root:final train perplexity: 176.00286865234375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.94s/it][A100%|██████████| 1/1 [00:39<00:00, 39.95s/it]
INFO:root:eval mean loss: 6312.683763436392
INFO:root:eval perplexity: 165.39175415039062
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.25s/it][A100%|██████████| 1/1 [00:38<00:00, 38.25s/it]
INFO:root:eval mean loss: 6450.40816676363
INFO:root:eval perplexity: 200.95193481445312
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/83
 42%|████▏     | 83/200 [12:34:22<17:40:47, 543.99s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6570.405029296875
INFO:root:current train perplexity181.824951171875
INFO:root:current mean train loss 6501.763494318182
INFO:root:current train perplexity173.4886474609375
INFO:root:current mean train loss 6518.543524460565
INFO:root:current train perplexity175.20045471191406
INFO:root:current mean train loss 6524.976463268649
INFO:root:current train perplexity175.4464874267578
INFO:root:current mean train loss 6524.848700695503
INFO:root:current train perplexity175.193603515625
INFO:root:current mean train loss 6535.455444814645
INFO:root:current train perplexity175.22100830078125
INFO:root:current mean train loss 6538.461396164191
INFO:root:current train perplexity175.26170349121094
INFO:root:current mean train loss 6542.884443772007
INFO:root:current train perplexity175.9242401123047
INFO:root:current mean train loss 6538.163055796682
INFO:root:current train perplexity175.88406372070312
INFO:root:current mean train loss 6545.0896559495195
INFO:root:current train perplexity175.7760772705078
INFO:root:current mean train loss 6554.094998259591
INFO:root:current train perplexity175.8429718017578
INFO:root:current mean train loss 6559.0186804969035
INFO:root:current train perplexity175.95462036132812
INFO:root:current mean train loss 6557.269666435304
INFO:root:current train perplexity175.811279296875
INFO:root:current mean train loss 6552.770996839217
INFO:root:current train perplexity175.58566284179688
INFO:root:current mean train loss 6556.542529643174
INFO:root:current train perplexity175.7054443359375
INFO:root:current mean train loss 6552.803532763349
INFO:root:current train perplexity175.60015869140625
INFO:root:current mean train loss 6553.015500655086
INFO:root:current train perplexity175.60794067382812
INFO:root:current mean train loss 6555.219700863487
INFO:root:current train perplexity175.85916137695312
INFO:root:current mean train loss 6555.384372032545
INFO:root:current train perplexity175.9075469970703
INFO:root:current mean train loss 6554.293813911158
INFO:root:current train perplexity175.9280242919922

100%|██████████| 1/1 [07:42<00:00, 462.26s/it][A100%|██████████| 1/1 [07:42<00:00, 462.27s/it]
INFO:root:final mean train loss: 6551.350451124598
INFO:root:final train perplexity: 175.96963500976562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.33s/it][A100%|██████████| 1/1 [00:40<00:00, 40.34s/it]
INFO:root:eval mean loss: 6312.71749986148
INFO:root:eval perplexity: 165.396240234375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.14s/it][A100%|██████████| 1/1 [00:38<00:00, 38.14s/it]
INFO:root:eval mean loss: 6450.453347496952
INFO:root:eval perplexity: 200.95932006835938
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/84
 42%|████▏     | 84/200 [12:43:25<17:31:10, 543.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6588.91943359375
INFO:root:current train perplexity177.57005310058594
INFO:root:current mean train loss 6615.547563207431
INFO:root:current train perplexity177.77845764160156
INFO:root:current mean train loss 6579.89454845815
INFO:root:current train perplexity177.07684326171875
INFO:root:current mean train loss 6575.2580155772175
INFO:root:current train perplexity176.5019989013672
INFO:root:current mean train loss 6580.944673274664
INFO:root:current train perplexity176.52969360351562
INFO:root:current mean train loss 6580.461487858753
INFO:root:current train perplexity176.96905517578125
INFO:root:current mean train loss 6571.565261475777
INFO:root:current train perplexity176.40196228027344
INFO:root:current mean train loss 6562.233528063746
INFO:root:current train perplexity176.19935607910156
INFO:root:current mean train loss 6559.102361935082
INFO:root:current train perplexity175.99774169921875
INFO:root:current mean train loss 6561.196569284318
INFO:root:current train perplexity176.07395935058594
INFO:root:current mean train loss 6554.066592624148
INFO:root:current train perplexity175.92759704589844
INFO:root:current mean train loss 6558.056265423968
INFO:root:current train perplexity176.17347717285156
INFO:root:current mean train loss 6555.756621045996
INFO:root:current train perplexity176.07958984375
INFO:root:current mean train loss 6555.252200761233
INFO:root:current train perplexity176.3446807861328
INFO:root:current mean train loss 6552.183968087553
INFO:root:current train perplexity176.1182098388672
INFO:root:current mean train loss 6555.035345231152
INFO:root:current train perplexity176.41525268554688
INFO:root:current mean train loss 6556.580302008106
INFO:root:current train perplexity176.3520050048828
INFO:root:current mean train loss 6558.324779128366
INFO:root:current train perplexity176.4024658203125
INFO:root:current mean train loss 6556.902028384989
INFO:root:current train perplexity176.37344360351562
INFO:root:current mean train loss 6555.093371182943
INFO:root:current train perplexity176.36302185058594

100%|██████████| 1/1 [07:38<00:00, 458.66s/it][A100%|██████████| 1/1 [07:38<00:00, 458.66s/it]
INFO:root:final mean train loss: 6553.853800147937
INFO:root:final train perplexity: 176.31753540039062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.43s/it][A100%|██████████| 1/1 [00:39<00:00, 39.43s/it]
INFO:root:eval mean loss: 6313.177987173094
INFO:root:eval perplexity: 165.45785522460938
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.00s/it][A100%|██████████| 1/1 [00:39<00:00, 39.00s/it]
INFO:root:eval mean loss: 6450.555311703512
INFO:root:eval perplexity: 200.97618103027344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/85
 42%|████▎     | 85/200 [12:52:24<17:19:41, 542.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6569.1785000887785
INFO:root:current train perplexity178.958740234375
INFO:root:current mean train loss 6530.329420301649
INFO:root:current train perplexity175.31753540039062
INFO:root:current mean train loss 6541.364988233222
INFO:root:current train perplexity176.03585815429688
INFO:root:current mean train loss 6549.062020235284
INFO:root:current train perplexity176.24212646484375
INFO:root:current mean train loss 6551.877387519356
INFO:root:current train perplexity176.4473876953125
INFO:root:current mean train loss 6567.453167186064
INFO:root:current train perplexity176.9249267578125
INFO:root:current mean train loss 6559.13900775791
INFO:root:current train perplexity176.88999938964844
INFO:root:current mean train loss 6557.180153467322
INFO:root:current train perplexity176.82615661621094
INFO:root:current mean train loss 6555.349323580051
INFO:root:current train perplexity176.63890075683594
INFO:root:current mean train loss 6553.288118524067
INFO:root:current train perplexity176.68885803222656
INFO:root:current mean train loss 6559.663748671725
INFO:root:current train perplexity176.88156127929688
INFO:root:current mean train loss 6561.678878677475
INFO:root:current train perplexity176.97537231445312
INFO:root:current mean train loss 6560.506303695237
INFO:root:current train perplexity176.91436767578125
INFO:root:current mean train loss 6558.716071719215
INFO:root:current train perplexity176.7333526611328
INFO:root:current mean train loss 6557.983689918412
INFO:root:current train perplexity176.53701782226562
INFO:root:current mean train loss 6557.32781919173
INFO:root:current train perplexity176.45184326171875
INFO:root:current mean train loss 6561.421869059839
INFO:root:current train perplexity176.4732208251953
INFO:root:current mean train loss 6560.46458757033
INFO:root:current train perplexity176.42811584472656
INFO:root:current mean train loss 6558.527519308823
INFO:root:current train perplexity176.37269592285156
INFO:root:current mean train loss 6555.889089576502
INFO:root:current train perplexity176.335693359375

100%|██████████| 1/1 [07:37<00:00, 457.58s/it][A100%|██████████| 1/1 [07:37<00:00, 457.58s/it]
INFO:root:final mean train loss: 6553.798460621336
INFO:root:final train perplexity: 176.30996704101562
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.73s/it][A100%|██████████| 1/1 [00:39<00:00, 39.73s/it]
INFO:root:eval mean loss: 6313.416912538785
INFO:root:eval perplexity: 165.48980712890625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.12s/it][A100%|██████████| 1/1 [00:38<00:00, 38.12s/it]
INFO:root:eval mean loss: 6450.716952709441
INFO:root:eval perplexity: 201.00302124023438
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/86
 43%|████▎     | 86/200 [13:01:22<17:08:03, 541.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6574.010542072234
INFO:root:current train perplexity174.69766235351562
INFO:root:current mean train loss 6576.360600252329
INFO:root:current train perplexity175.25375366210938
INFO:root:current mean train loss 6572.6281148976295
INFO:root:current train perplexity175.6100311279297
INFO:root:current mean train loss 6564.519516371624
INFO:root:current train perplexity175.66229248046875
INFO:root:current mean train loss 6542.090781122899
INFO:root:current train perplexity175.31912231445312
INFO:root:current mean train loss 6528.7261429784985
INFO:root:current train perplexity174.8859405517578
INFO:root:current mean train loss 6528.170558635117
INFO:root:current train perplexity174.84725952148438
INFO:root:current mean train loss 6527.66876052275
INFO:root:current train perplexity175.09170532226562
INFO:root:current mean train loss 6531.251558983921
INFO:root:current train perplexity175.3090057373047
INFO:root:current mean train loss 6539.400472428622
INFO:root:current train perplexity175.59596252441406
INFO:root:current mean train loss 6544.874658065062
INFO:root:current train perplexity175.90545654296875
INFO:root:current mean train loss 6543.804921336671
INFO:root:current train perplexity175.93406677246094
INFO:root:current mean train loss 6541.17990937562
INFO:root:current train perplexity175.90603637695312
INFO:root:current mean train loss 6543.780363129133
INFO:root:current train perplexity176.00579833984375
INFO:root:current mean train loss 6547.173775319772
INFO:root:current train perplexity176.100830078125
INFO:root:current mean train loss 6546.334188946388
INFO:root:current train perplexity176.12124633789062
INFO:root:current mean train loss 6547.81522743735
INFO:root:current train perplexity176.0544891357422
INFO:root:current mean train loss 6551.531392796618
INFO:root:current train perplexity176.12232971191406
INFO:root:current mean train loss 6551.685902918877
INFO:root:current train perplexity176.1454315185547
INFO:root:current mean train loss 6553.560842931301
INFO:root:current train perplexity176.2158203125

100%|██████████| 1/1 [07:40<00:00, 460.14s/it][A100%|██████████| 1/1 [07:40<00:00, 460.14s/it]
INFO:root:final mean train loss: 6553.138337243523
INFO:root:final train perplexity: 176.2181854248047
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.43s/it][A100%|██████████| 1/1 [00:40<00:00, 40.43s/it]
INFO:root:eval mean loss: 6312.5387629515735
INFO:root:eval perplexity: 165.37234497070312
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.49s/it][A100%|██████████| 1/1 [00:38<00:00, 38.49s/it]
INFO:root:eval mean loss: 6449.853349401596
INFO:root:eval perplexity: 200.86026000976562
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/87
 44%|████▎     | 87/200 [13:10:24<16:59:11, 541.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6547.724465394632
INFO:root:current train perplexity175.4808807373047
INFO:root:current mean train loss 6576.310911714361
INFO:root:current train perplexity176.3270263671875
INFO:root:current mean train loss 6557.19270423505
INFO:root:current train perplexity176.87481689453125
INFO:root:current mean train loss 6576.919064153439
INFO:root:current train perplexity177.2833251953125
INFO:root:current mean train loss 6571.606999991828
INFO:root:current train perplexity176.82766723632812
INFO:root:current mean train loss 6566.94002757353
INFO:root:current train perplexity176.56715393066406
INFO:root:current mean train loss 6562.295009016639
INFO:root:current train perplexity176.34922790527344
INFO:root:current mean train loss 6563.600494933925
INFO:root:current train perplexity176.01150512695312
INFO:root:current mean train loss 6569.207279839657
INFO:root:current train perplexity176.09814453125
INFO:root:current mean train loss 6565.747409812756
INFO:root:current train perplexity176.12232971191406
INFO:root:current mean train loss 6560.526513037744
INFO:root:current train perplexity176.00975036621094
INFO:root:current mean train loss 6563.925928397575
INFO:root:current train perplexity175.948486328125
INFO:root:current mean train loss 6564.894220247702
INFO:root:current train perplexity175.8208465576172
INFO:root:current mean train loss 6566.292349717094
INFO:root:current train perplexity175.94740295410156
INFO:root:current mean train loss 6562.188546600135
INFO:root:current train perplexity175.8542938232422
INFO:root:current mean train loss 6560.400329048341
INFO:root:current train perplexity175.8596649169922
INFO:root:current mean train loss 6559.674066154835
INFO:root:current train perplexity175.95982360839844
INFO:root:current mean train loss 6554.03412229111
INFO:root:current train perplexity175.83065795898438
INFO:root:current mean train loss 6555.956123587676
INFO:root:current train perplexity176.0302276611328
INFO:root:current mean train loss 6552.786506552547
INFO:root:current train perplexity175.93154907226562

100%|██████████| 1/1 [07:43<00:00, 463.61s/it][A100%|██████████| 1/1 [07:43<00:00, 463.61s/it]
INFO:root:final mean train loss: 6551.118929972146
INFO:root:final train perplexity: 175.9375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.57s/it][A100%|██████████| 1/1 [00:39<00:00, 39.57s/it]
INFO:root:eval mean loss: 6311.086659532913
INFO:root:eval perplexity: 165.17808532714844
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.39s/it][A100%|██████████| 1/1 [00:38<00:00, 38.39s/it]
INFO:root:eval mean loss: 6448.576325977948
INFO:root:eval perplexity: 200.64947509765625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/88
 44%|████▍     | 88/200 [13:19:28<16:51:47, 542.03s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6564.119428453947
INFO:root:current train perplexity177.7473602294922
INFO:root:current mean train loss 6545.39997245593
INFO:root:current train perplexity177.0650177001953
INFO:root:current mean train loss 6545.250413797669
INFO:root:current train perplexity176.56353759765625
INFO:root:current mean train loss 6548.884828668908
INFO:root:current train perplexity176.4175262451172
INFO:root:current mean train loss 6551.135142440025
INFO:root:current train perplexity176.32627868652344
INFO:root:current mean train loss 6556.0106995141805
INFO:root:current train perplexity176.43409729003906
INFO:root:current mean train loss 6567.953469958408
INFO:root:current train perplexity176.8775177001953
INFO:root:current mean train loss 6569.099075643671
INFO:root:current train perplexity176.66610717773438
INFO:root:current mean train loss 6574.814975776885
INFO:root:current train perplexity176.7576141357422
INFO:root:current mean train loss 6568.7428661864005
INFO:root:current train perplexity176.49620056152344
INFO:root:current mean train loss 6566.721368881992
INFO:root:current train perplexity176.4409942626953
INFO:root:current mean train loss 6560.90131325183
INFO:root:current train perplexity176.25692749023438
INFO:root:current mean train loss 6557.0166381364625
INFO:root:current train perplexity176.2091827392578
INFO:root:current mean train loss 6549.12541827677
INFO:root:current train perplexity175.9713897705078
INFO:root:current mean train loss 6550.671943261392
INFO:root:current train perplexity175.8363494873047
INFO:root:current mean train loss 6547.304983223942
INFO:root:current train perplexity175.69505310058594
INFO:root:current mean train loss 6552.568677117902
INFO:root:current train perplexity175.88658142089844
INFO:root:current mean train loss 6554.216274318855
INFO:root:current train perplexity175.95059204101562
INFO:root:current mean train loss 6551.148288825445
INFO:root:current train perplexity175.83795166015625

100%|██████████| 1/1 [07:45<00:00, 465.82s/it][A100%|██████████| 1/1 [07:45<00:00, 465.82s/it]
INFO:root:final mean train loss: 6550.895271551351
INFO:root:final train perplexity: 175.90646362304688
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.42s/it][A100%|██████████| 1/1 [00:40<00:00, 40.42s/it]
INFO:root:eval mean loss: 6311.103732061724
INFO:root:eval perplexity: 165.1803741455078
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.87s/it][A100%|██████████| 1/1 [00:38<00:00, 38.87s/it]
INFO:root:eval mean loss: 6448.540737720246
INFO:root:eval perplexity: 200.64353942871094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/89
 44%|████▍     | 89/200 [13:28:35<16:45:45, 543.66s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6678.865071614583
INFO:root:current train perplexity180.8304443359375
INFO:root:current mean train loss 6593.259198869978
INFO:root:current train perplexity178.8891143798828
INFO:root:current mean train loss 6578.94914504717
INFO:root:current train perplexity177.48573303222656
INFO:root:current mean train loss 6580.93209760617
INFO:root:current train perplexity177.271240234375
INFO:root:current mean train loss 6558.2400648987405
INFO:root:current train perplexity176.53163146972656
INFO:root:current mean train loss 6553.429982185364
INFO:root:current train perplexity176.58651733398438
INFO:root:current mean train loss 6558.888644748264
INFO:root:current train perplexity176.52430725097656
INFO:root:current mean train loss 6558.552569785815
INFO:root:current train perplexity176.33148193359375
INFO:root:current mean train loss 6566.608215031365
INFO:root:current train perplexity176.63848876953125
INFO:root:current mean train loss 6566.040970651727
INFO:root:current train perplexity176.60049438476562
INFO:root:current mean train loss 6562.5531575199175
INFO:root:current train perplexity176.3815155029297
INFO:root:current mean train loss 6563.961452566463
INFO:root:current train perplexity176.44932556152344
INFO:root:current mean train loss 6556.8478597005205
INFO:root:current train perplexity176.2786102294922
INFO:root:current mean train loss 6557.8136630174595
INFO:root:current train perplexity176.14718627929688
INFO:root:current mean train loss 6556.380990436327
INFO:root:current train perplexity176.1559295654297
INFO:root:current mean train loss 6556.7346601536665
INFO:root:current train perplexity176.10025024414062
INFO:root:current mean train loss 6557.064782381649
INFO:root:current train perplexity176.13980102539062
INFO:root:current mean train loss 6554.934574305455
INFO:root:current train perplexity176.11065673828125
INFO:root:current mean train loss 6555.599888007899
INFO:root:current train perplexity176.1168670654297
INFO:root:current mean train loss 6551.973315889367
INFO:root:current train perplexity175.8373565673828

100%|██████████| 1/1 [07:36<00:00, 456.26s/it][A100%|██████████| 1/1 [07:36<00:00, 456.26s/it]
INFO:root:final mean train loss: 6550.280160170039
INFO:root:final train perplexity: 175.8210906982422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.45s/it][A100%|██████████| 1/1 [00:39<00:00, 39.45s/it]
INFO:root:eval mean loss: 6311.536811558068
INFO:root:eval perplexity: 165.23818969726562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.80s/it][A100%|██████████| 1/1 [00:37<00:00, 37.80s/it]
INFO:root:eval mean loss: 6448.412336200687
INFO:root:eval perplexity: 200.62249755859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/90
 45%|████▌     | 90/200 [13:37:31<16:32:22, 541.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6569.865318561422
INFO:root:current train perplexity179.32220458984375
INFO:root:current mean train loss 6547.467852925145
INFO:root:current train perplexity176.2608642578125
INFO:root:current mean train loss 6536.131123771834
INFO:root:current train perplexity174.6003875732422
INFO:root:current mean train loss 6530.739116819434
INFO:root:current train perplexity174.9545135498047
INFO:root:current mean train loss 6528.507817052739
INFO:root:current train perplexity174.62245178222656
INFO:root:current mean train loss 6520.098207297082
INFO:root:current train perplexity174.47198486328125
INFO:root:current mean train loss 6521.99838843899
INFO:root:current train perplexity174.57492065429688
INFO:root:current mean train loss 6529.614624190886
INFO:root:current train perplexity175.02259826660156
INFO:root:current mean train loss 6536.453950778423
INFO:root:current train perplexity175.1858367919922
INFO:root:current mean train loss 6535.212427572491
INFO:root:current train perplexity175.2533416748047
INFO:root:current mean train loss 6535.770119655005
INFO:root:current train perplexity175.27398681640625
INFO:root:current mean train loss 6541.580638199596
INFO:root:current train perplexity175.2869415283203
INFO:root:current mean train loss 6544.295666017214
INFO:root:current train perplexity175.5792999267578
INFO:root:current mean train loss 6545.416162954407
INFO:root:current train perplexity175.57980346679688
INFO:root:current mean train loss 6545.001758359211
INFO:root:current train perplexity175.48748779296875
INFO:root:current mean train loss 6548.482783056225
INFO:root:current train perplexity175.5897674560547
INFO:root:current mean train loss 6552.091630217925
INFO:root:current train perplexity175.58450317382812
INFO:root:current mean train loss 6551.294791685494
INFO:root:current train perplexity175.57470703125
INFO:root:current mean train loss 6550.872013448691
INFO:root:current train perplexity175.6096954345703
INFO:root:current mean train loss 6551.409382188796
INFO:root:current train perplexity175.6254425048828

100%|██████████| 1/1 [07:36<00:00, 456.02s/it][A100%|██████████| 1/1 [07:36<00:00, 456.02s/it]
INFO:root:final mean train loss: 6548.8977681139295
INFO:root:final train perplexity: 175.62937927246094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.39s/it][A100%|██████████| 1/1 [00:39<00:00, 39.39s/it]
INFO:root:eval mean loss: 6310.154501191268
INFO:root:eval perplexity: 165.05352783203125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.73s/it][A100%|██████████| 1/1 [00:38<00:00, 38.73s/it]
INFO:root:eval mean loss: 6447.750325520833
INFO:root:eval perplexity: 200.51316833496094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/91
 46%|████▌     | 91/200 [13:46:28<16:20:46, 539.87s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6569.335735818614
INFO:root:current train perplexity177.7890625
INFO:root:current mean train loss 6592.273628130351
INFO:root:current train perplexity178.46975708007812
INFO:root:current mean train loss 6590.485349577617
INFO:root:current train perplexity178.8271942138672
INFO:root:current mean train loss 6589.008426379606
INFO:root:current train perplexity178.50149536132812
INFO:root:current mean train loss 6594.71078641746
INFO:root:current train perplexity178.48626708984375
INFO:root:current mean train loss 6593.870753920559
INFO:root:current train perplexity177.91949462890625
INFO:root:current mean train loss 6582.302390461736
INFO:root:current train perplexity177.65676879882812
INFO:root:current mean train loss 6572.907038057138
INFO:root:current train perplexity177.11349487304688
INFO:root:current mean train loss 6564.758793102652
INFO:root:current train perplexity176.6929931640625
INFO:root:current mean train loss 6562.32040437533
INFO:root:current train perplexity176.46749877929688
INFO:root:current mean train loss 6563.8257349426385
INFO:root:current train perplexity176.3320770263672
INFO:root:current mean train loss 6561.064678518352
INFO:root:current train perplexity176.40557861328125
INFO:root:current mean train loss 6552.648833689682
INFO:root:current train perplexity176.01016235351562
INFO:root:current mean train loss 6556.654473178631
INFO:root:current train perplexity175.921142578125
INFO:root:current mean train loss 6555.993069175203
INFO:root:current train perplexity175.9878387451172
INFO:root:current mean train loss 6555.214942922259
INFO:root:current train perplexity175.9672088623047
INFO:root:current mean train loss 6552.6869028492365
INFO:root:current train perplexity175.78639221191406
INFO:root:current mean train loss 6551.566974233516
INFO:root:current train perplexity175.678466796875
INFO:root:current mean train loss 6550.671906211911
INFO:root:current train perplexity175.7227783203125
INFO:root:current mean train loss 6550.989686626815
INFO:root:current train perplexity175.7665252685547

100%|██████████| 1/1 [07:45<00:00, 465.21s/it][A100%|██████████| 1/1 [07:45<00:00, 465.21s/it]
INFO:root:final mean train loss: 6549.891137289027
INFO:root:final train perplexity: 175.76719665527344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.67s/it][A100%|██████████| 1/1 [00:40<00:00, 40.68s/it]
INFO:root:eval mean loss: 6310.588295794548
INFO:root:eval perplexity: 165.11146545410156
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.72s/it][A100%|██████████| 1/1 [00:38<00:00, 38.72s/it]
INFO:root:eval mean loss: 6447.659482698914
INFO:root:eval perplexity: 200.49826049804688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/92
 46%|████▌     | 92/200 [13:55:34<16:15:35, 542.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6561.597276475694
INFO:root:current train perplexity174.17031860351562
INFO:root:current mean train loss 6564.5627606163725
INFO:root:current train perplexity175.25668334960938
INFO:root:current mean train loss 6550.214645095652
INFO:root:current train perplexity174.846923828125
INFO:root:current mean train loss 6550.800493392734
INFO:root:current train perplexity175.58273315429688
INFO:root:current mean train loss 6539.33440199784
INFO:root:current train perplexity175.08811950683594
INFO:root:current mean train loss 6557.008210583648
INFO:root:current train perplexity175.6612091064453
INFO:root:current mean train loss 6557.52021911529
INFO:root:current train perplexity175.66673278808594
INFO:root:current mean train loss 6555.555489996314
INFO:root:current train perplexity175.40199279785156
INFO:root:current mean train loss 6550.642744468786
INFO:root:current train perplexity175.60174560546875
INFO:root:current mean train loss 6552.040906103972
INFO:root:current train perplexity175.63172912597656
INFO:root:current mean train loss 6555.452602727393
INFO:root:current train perplexity175.62167358398438
INFO:root:current mean train loss 6557.10363486135
INFO:root:current train perplexity175.6126251220703
INFO:root:current mean train loss 6554.362956502376
INFO:root:current train perplexity175.63934326171875
INFO:root:current mean train loss 6554.753801643892
INFO:root:current train perplexity175.72203063964844
INFO:root:current mean train loss 6557.820598526679
INFO:root:current train perplexity175.91619873046875
INFO:root:current mean train loss 6555.776042603867
INFO:root:current train perplexity175.8025665283203
INFO:root:current mean train loss 6551.51436744823
INFO:root:current train perplexity175.5913543701172
INFO:root:current mean train loss 6550.761612674152
INFO:root:current train perplexity175.46932983398438
INFO:root:current mean train loss 6551.64728769332
INFO:root:current train perplexity175.61329650878906
INFO:root:current mean train loss 6549.38474348693
INFO:root:current train perplexity175.5442352294922

100%|██████████| 1/1 [07:42<00:00, 462.27s/it][A100%|██████████| 1/1 [07:42<00:00, 462.27s/it]
INFO:root:final mean train loss: 6548.229529491892
INFO:root:final train perplexity: 175.53677368164062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.24s/it][A100%|██████████| 1/1 [00:40<00:00, 40.24s/it]
INFO:root:eval mean loss: 6307.57165440769
INFO:root:eval perplexity: 164.70892333984375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.90s/it][A100%|██████████| 1/1 [00:38<00:00, 38.90s/it]
INFO:root:eval mean loss: 6445.946685574579
INFO:root:eval perplexity: 200.2161407470703
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/93
 46%|████▋     | 93/200 [14:04:38<16:07:33, 542.55s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6609.941082763672
INFO:root:current train perplexity176.7935333251953
INFO:root:current mean train loss 6633.504044596354
INFO:root:current train perplexity176.8265838623047
INFO:root:current mean train loss 6590.534992327009
INFO:root:current train perplexity175.56674194335938
INFO:root:current mean train loss 6572.005837530839
INFO:root:current train perplexity175.00640869140625
INFO:root:current mean train loss 6566.394591267904
INFO:root:current train perplexity175.142822265625
INFO:root:current mean train loss 6567.700511011584
INFO:root:current train perplexity175.56231689453125
INFO:root:current mean train loss 6564.941843548943
INFO:root:current train perplexity175.6886749267578
INFO:root:current mean train loss 6558.758200620993
INFO:root:current train perplexity175.53619384765625
INFO:root:current mean train loss 6560.878641024503
INFO:root:current train perplexity175.54263305664062
INFO:root:current mean train loss 6555.7728341238835
INFO:root:current train perplexity175.39788818359375
INFO:root:current mean train loss 6556.270422363281
INFO:root:current train perplexity175.572265625
INFO:root:current mean train loss 6559.219923943989
INFO:root:current train perplexity175.8234405517578
INFO:root:current mean train loss 6556.080596923828
INFO:root:current train perplexity175.6572723388672
INFO:root:current mean train loss 6554.517198822464
INFO:root:current train perplexity175.6067657470703
INFO:root:current mean train loss 6551.427671690245
INFO:root:current train perplexity175.6209259033203
INFO:root:current mean train loss 6549.7423098793515
INFO:root:current train perplexity175.51744079589844
INFO:root:current mean train loss 6548.986605398995
INFO:root:current train perplexity175.47158813476562
INFO:root:current mean train loss 6551.380161516854
INFO:root:current train perplexity175.6441192626953
INFO:root:current mean train loss 6551.027347386137
INFO:root:current train perplexity175.707275390625
INFO:root:current mean train loss 6550.356161468198
INFO:root:current train perplexity175.609619140625

100%|██████████| 1/1 [07:42<00:00, 462.67s/it][A100%|██████████| 1/1 [07:42<00:00, 462.67s/it]
INFO:root:final mean train loss: 6548.855769155005
INFO:root:final train perplexity: 175.62351989746094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.82s/it][A100%|██████████| 1/1 [00:40<00:00, 40.82s/it]
INFO:root:eval mean loss: 6309.299304285793
INFO:root:eval perplexity: 164.93936157226562
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.29s/it][A100%|██████████| 1/1 [00:39<00:00, 39.29s/it]
INFO:root:eval mean loss: 6447.972559286348
INFO:root:eval perplexity: 200.54998779296875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/94
 47%|████▋     | 94/200 [14:13:43<15:59:54, 543.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6547.673198896585
INFO:root:current train perplexity175.0370330810547
INFO:root:current mean train loss 6544.627225769353
INFO:root:current train perplexity176.0184783935547
INFO:root:current mean train loss 6548.155977088594
INFO:root:current train perplexity177.0797882080078
INFO:root:current mean train loss 6537.192770239688
INFO:root:current train perplexity176.13719177246094
INFO:root:current mean train loss 6537.780736174862
INFO:root:current train perplexity175.80465698242188
INFO:root:current mean train loss 6536.416090053131
INFO:root:current train perplexity175.5470733642578
INFO:root:current mean train loss 6530.033520472785
INFO:root:current train perplexity175.39169311523438
INFO:root:current mean train loss 6528.161432397859
INFO:root:current train perplexity175.0900421142578
INFO:root:current mean train loss 6527.094473440113
INFO:root:current train perplexity175.08253479003906
INFO:root:current mean train loss 6532.783207043004
INFO:root:current train perplexity175.302490234375
INFO:root:current mean train loss 6534.930518067742
INFO:root:current train perplexity175.20103454589844
INFO:root:current mean train loss 6540.330134418076
INFO:root:current train perplexity175.39212036132812
INFO:root:current mean train loss 6542.019338497495
INFO:root:current train perplexity175.48614501953125
INFO:root:current mean train loss 6543.141326838761
INFO:root:current train perplexity175.51083374023438
INFO:root:current mean train loss 6541.976366469919
INFO:root:current train perplexity175.52841186523438
INFO:root:current mean train loss 6543.38496405614
INFO:root:current train perplexity175.57293701171875
INFO:root:current mean train loss 6544.042797549444
INFO:root:current train perplexity175.51551818847656
INFO:root:current mean train loss 6549.04476672275
INFO:root:current train perplexity175.6209259033203
INFO:root:current mean train loss 6550.68362952812
INFO:root:current train perplexity175.66188049316406

100%|██████████| 1/1 [07:43<00:00, 463.92s/it][A100%|██████████| 1/1 [07:43<00:00, 463.92s/it]
INFO:root:final mean train loss: 6549.863940663609
INFO:root:final train perplexity: 175.76333618164062
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:40<00:00, 40.56s/it][A100%|██████████| 1/1 [00:40<00:00, 40.56s/it]
INFO:root:eval mean loss: 6310.801271262744
INFO:root:eval perplexity: 165.1399688720703
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.23s/it][A100%|██████████| 1/1 [00:39<00:00, 39.23s/it]
INFO:root:eval mean loss: 6448.444079676418
INFO:root:eval perplexity: 200.62765502929688
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/95
 48%|████▊     | 95/200 [14:22:50<15:52:19, 544.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6584.442173549107
INFO:root:current train perplexity179.18048095703125
INFO:root:current mean train loss 6532.770606325384
INFO:root:current train perplexity176.0252685546875
INFO:root:current mean train loss 6539.578195732331
INFO:root:current train perplexity176.36074829101562
INFO:root:current mean train loss 6538.394542135251
INFO:root:current train perplexity176.17726135253906
INFO:root:current mean train loss 6537.978680744263
INFO:root:current train perplexity175.7258758544922
INFO:root:current mean train loss 6539.436084554353
INFO:root:current train perplexity175.25485229492188
INFO:root:current mean train loss 6531.136356117671
INFO:root:current train perplexity174.91204833984375
INFO:root:current mean train loss 6521.792593306854
INFO:root:current train perplexity174.8826141357422
INFO:root:current mean train loss 6532.124912421299
INFO:root:current train perplexity175.09263610839844
INFO:root:current mean train loss 6530.481863076107
INFO:root:current train perplexity175.10415649414062
INFO:root:current mean train loss 6538.422955575074
INFO:root:current train perplexity175.16311645507812
INFO:root:current mean train loss 6542.636901526734
INFO:root:current train perplexity175.07217407226562
INFO:root:current mean train loss 6544.520748333248
INFO:root:current train perplexity175.20947265625
INFO:root:current mean train loss 6547.119193020477
INFO:root:current train perplexity175.22451782226562
INFO:root:current mean train loss 6546.479040164759
INFO:root:current train perplexity175.28175354003906
INFO:root:current mean train loss 6547.761077921174
INFO:root:current train perplexity175.4423828125
INFO:root:current mean train loss 6548.086688073594
INFO:root:current train perplexity175.55938720703125
INFO:root:current mean train loss 6550.383076297221
INFO:root:current train perplexity175.6215057373047
INFO:root:current mean train loss 6552.525716325283
INFO:root:current train perplexity175.6628875732422
INFO:root:current mean train loss 6550.132851276777
INFO:root:current train perplexity175.6573486328125

100%|██████████| 1/1 [07:29<00:00, 449.71s/it][A100%|██████████| 1/1 [07:29<00:00, 449.71s/it]
INFO:root:final mean train loss: 6549.711194444772
INFO:root:final train perplexity: 175.7421417236328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.93s/it][A100%|██████████| 1/1 [00:38<00:00, 38.93s/it]
INFO:root:eval mean loss: 6309.641011123116
INFO:root:eval perplexity: 164.9849853515625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.32s/it][A100%|██████████| 1/1 [00:37<00:00, 37.32s/it]
INFO:root:eval mean loss: 6446.807870851341
INFO:root:eval perplexity: 200.3579559326172
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/96
 48%|████▊     | 96/200 [14:31:38<15:35:02, 539.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6547.061743951613
INFO:root:current train perplexity177.2666778564453
INFO:root:current mean train loss 6602.865398377862
INFO:root:current train perplexity178.41061401367188
INFO:root:current mean train loss 6564.9357476833065
INFO:root:current train perplexity176.29029846191406
INFO:root:current mean train loss 6564.074594918335
INFO:root:current train perplexity176.3531036376953
INFO:root:current mean train loss 6555.452375018126
INFO:root:current train perplexity175.8666229248047
INFO:root:current mean train loss 6558.43271465984
INFO:root:current train perplexity175.91217041015625
INFO:root:current mean train loss 6566.806876640501
INFO:root:current train perplexity176.06204223632812
INFO:root:current mean train loss 6559.406325479865
INFO:root:current train perplexity175.72755432128906
INFO:root:current mean train loss 6557.311103315847
INFO:root:current train perplexity175.90663146972656
INFO:root:current mean train loss 6558.825273982949
INFO:root:current train perplexity175.83786010742188
INFO:root:current mean train loss 6560.296863160009
INFO:root:current train perplexity175.97853088378906
INFO:root:current mean train loss 6561.343950752238
INFO:root:current train perplexity176.2545623779297
INFO:root:current mean train loss 6560.816733886322
INFO:root:current train perplexity176.1349334716797
INFO:root:current mean train loss 6557.399494770145
INFO:root:current train perplexity175.9916229248047
INFO:root:current mean train loss 6557.273731970104
INFO:root:current train perplexity176.01931762695312
INFO:root:current mean train loss 6553.462902744325
INFO:root:current train perplexity175.8168182373047
INFO:root:current mean train loss 6553.861540980897
INFO:root:current train perplexity175.8609161376953
INFO:root:current mean train loss 6553.264211494891
INFO:root:current train perplexity175.78773498535156
INFO:root:current mean train loss 6550.641163682755
INFO:root:current train perplexity175.69320678710938
INFO:root:current mean train loss 6551.593230869287
INFO:root:current train perplexity175.77230834960938

100%|██████████| 1/1 [07:40<00:00, 460.80s/it][A100%|██████████| 1/1 [07:40<00:00, 460.80s/it]
INFO:root:final mean train loss: 6549.6695308560265
INFO:root:final train perplexity: 175.73635864257812
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.68s/it][A100%|██████████| 1/1 [00:41<00:00, 41.68s/it]
INFO:root:eval mean loss: 6310.517588513962
INFO:root:eval perplexity: 165.10202026367188
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.86s/it][A100%|██████████| 1/1 [00:38<00:00, 38.86s/it]
INFO:root:eval mean loss: 6448.403492596132
INFO:root:eval perplexity: 200.62095642089844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/97
 48%|████▊     | 97/200 [14:40:42<15:28:11, 540.69s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6573.567708333333
INFO:root:current train perplexity173.13221740722656
INFO:root:current mean train loss 6537.443161423142
INFO:root:current train perplexity176.1502227783203
INFO:root:current mean train loss 6524.287174347908
INFO:root:current train perplexity175.6328887939453
INFO:root:current mean train loss 6536.210370644756
INFO:root:current train perplexity175.88902282714844
INFO:root:current mean train loss 6552.066830226353
INFO:root:current train perplexity176.5431671142578
INFO:root:current mean train loss 6562.002678418681
INFO:root:current train perplexity176.99411010742188
INFO:root:current mean train loss 6566.614911868249
INFO:root:current train perplexity177.1200714111328
INFO:root:current mean train loss 6572.802975904496
INFO:root:current train perplexity176.824462890625
INFO:root:current mean train loss 6567.888973020158
INFO:root:current train perplexity176.57009887695312
INFO:root:current mean train loss 6568.046851822092
INFO:root:current train perplexity176.62635803222656
INFO:root:current mean train loss 6571.6073231005485
INFO:root:current train perplexity176.81771850585938
INFO:root:current mean train loss 6569.368616615854
INFO:root:current train perplexity176.4942626953125
INFO:root:current mean train loss 6568.316578791691
INFO:root:current train perplexity176.23431396484375
INFO:root:current mean train loss 6566.521293119436
INFO:root:current train perplexity176.1715545654297
INFO:root:current mean train loss 6561.800020165207
INFO:root:current train perplexity176.06431579589844
INFO:root:current mean train loss 6561.293070632974
INFO:root:current train perplexity176.11990356445312
INFO:root:current mean train loss 6558.732106625455
INFO:root:current train perplexity176.06549072265625
INFO:root:current mean train loss 6554.10461733052
INFO:root:current train perplexity175.97097778320312
INFO:root:current mean train loss 6554.325574206067
INFO:root:current train perplexity175.90269470214844
INFO:root:current mean train loss 6551.932030397764
INFO:root:current train perplexity175.80465698242188

100%|██████████| 1/1 [07:44<00:00, 464.28s/it][A100%|██████████| 1/1 [07:44<00:00, 464.29s/it]
INFO:root:final mean train loss: 6549.655275899836
INFO:root:final train perplexity: 175.7342529296875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.30s/it][A100%|██████████| 1/1 [00:41<00:00, 41.30s/it]
INFO:root:eval mean loss: 6310.709157524379
INFO:root:eval perplexity: 165.1276092529297
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.70s/it][A100%|██████████| 1/1 [00:39<00:00, 39.70s/it]
INFO:root:eval mean loss: 6448.0538182901155
INFO:root:eval perplexity: 200.5632781982422
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/98
 49%|████▉     | 98/200 [14:49:49<15:22:43, 542.78s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6594.070582932693
INFO:root:current train perplexity178.38697814941406
INFO:root:current mean train loss 6607.972091027462
INFO:root:current train perplexity178.18922424316406
INFO:root:current mean train loss 6586.99452940743
INFO:root:current train perplexity177.0546417236328
INFO:root:current mean train loss 6572.979624625428
INFO:root:current train perplexity176.80154418945312
INFO:root:current mean train loss 6557.766544858871
INFO:root:current train perplexity176.1610565185547
INFO:root:current mean train loss 6553.998649232577
INFO:root:current train perplexity176.32601928710938
INFO:root:current mean train loss 6556.1368215460525
INFO:root:current train perplexity176.3179473876953
INFO:root:current mean train loss 6562.234348830678
INFO:root:current train perplexity176.3977508544922
INFO:root:current mean train loss 6564.4922885431715
INFO:root:current train perplexity176.34242248535156
INFO:root:current mean train loss 6561.209475692195
INFO:root:current train perplexity176.2528076171875
INFO:root:current mean train loss 6555.599080289026
INFO:root:current train perplexity176.08740234375
INFO:root:current mean train loss 6559.84934364941
INFO:root:current train perplexity176.35411071777344
INFO:root:current mean train loss 6551.728666934288
INFO:root:current train perplexity176.18734741210938
INFO:root:current mean train loss 6550.966482801053
INFO:root:current train perplexity176.06800842285156
INFO:root:current mean train loss 6551.211456444646
INFO:root:current train perplexity176.06170654296875
INFO:root:current mean train loss 6552.228182408147
INFO:root:current train perplexity175.95445251464844
INFO:root:current mean train loss 6552.004233237143
INFO:root:current train perplexity175.85714721679688
INFO:root:current mean train loss 6551.81805340386
INFO:root:current train perplexity175.94891357421875
INFO:root:current mean train loss 6552.935644793063
INFO:root:current train perplexity176.00999450683594
INFO:root:current mean train loss 6552.278197310353
INFO:root:current train perplexity175.9103240966797

100%|██████████| 1/1 [07:45<00:00, 465.01s/it][A100%|██████████| 1/1 [07:45<00:00, 465.01s/it]
INFO:root:final mean train loss: 6551.600229637465
INFO:root:final train perplexity: 176.00437927246094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.25s/it][A100%|██████████| 1/1 [00:39<00:00, 39.25s/it]
INFO:root:eval mean loss: 6310.699677595855
INFO:root:eval perplexity: 165.1263427734375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.20s/it][A100%|██████████| 1/1 [00:38<00:00, 38.20s/it]
INFO:root:eval mean loss: 6448.346460653535
INFO:root:eval perplexity: 200.6114959716797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/99
 50%|████▉     | 99/200 [14:58:54<15:14:40, 543.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6475.054997141769
INFO:root:current train perplexity173.79086303710938
INFO:root:current mean train loss 6523.0583737551515
INFO:root:current train perplexity174.7315673828125
INFO:root:current mean train loss 6542.8229426390735
INFO:root:current train perplexity175.2344512939453
INFO:root:current mean train loss 6544.240006851276
INFO:root:current train perplexity175.13404846191406
INFO:root:current mean train loss 6546.052529742609
INFO:root:current train perplexity175.3745574951172
INFO:root:current mean train loss 6545.26744053372
INFO:root:current train perplexity175.2578582763672
INFO:root:current mean train loss 6541.858705582157
INFO:root:current train perplexity175.47360229492188
INFO:root:current mean train loss 6540.817179257912
INFO:root:current train perplexity175.24832153320312
INFO:root:current mean train loss 6543.5682995854595
INFO:root:current train perplexity175.54808044433594
INFO:root:current mean train loss 6546.989448749364
INFO:root:current train perplexity175.6986541748047
INFO:root:current mean train loss 6544.477328767618
INFO:root:current train perplexity175.6373291015625
INFO:root:current mean train loss 6547.678548590181
INFO:root:current train perplexity175.69688415527344
INFO:root:current mean train loss 6544.447324660565
INFO:root:current train perplexity175.68324279785156
INFO:root:current mean train loss 6545.965195298368
INFO:root:current train perplexity175.650146484375
INFO:root:current mean train loss 6548.343146402665
INFO:root:current train perplexity175.74305725097656
INFO:root:current mean train loss 6550.25827824994
INFO:root:current train perplexity175.94647216796875
INFO:root:current mean train loss 6551.542848856922
INFO:root:current train perplexity176.04684448242188
INFO:root:current mean train loss 6553.188312706054
INFO:root:current train perplexity176.0303192138672
INFO:root:current mean train loss 6554.602869339881
INFO:root:current train perplexity176.04432678222656
INFO:root:current mean train loss 6554.00425607814
INFO:root:current train perplexity176.08236694335938

100%|██████████| 1/1 [07:42<00:00, 462.28s/it][A100%|██████████| 1/1 [07:42<00:00, 462.28s/it]
INFO:root:final mean train loss: 6552.228556376663
INFO:root:final train perplexity: 176.0915985107422
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.98s/it][A100%|██████████| 1/1 [00:39<00:00, 39.98s/it]
INFO:root:eval mean loss: 6311.025234790559
INFO:root:eval perplexity: 165.16981506347656
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 38.00s/it][A100%|██████████| 1/1 [00:37<00:00, 38.00s/it]
INFO:root:eval mean loss: 6448.130715661015
INFO:root:eval perplexity: 200.5760955810547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/100
 50%|█████     | 100/200 [15:07:57<15:05:14, 543.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6586.898274739583
INFO:root:current train perplexity177.03074645996094
INFO:root:current mean train loss 6579.208815071451
INFO:root:current train perplexity176.135009765625
INFO:root:current mean train loss 6575.4563486360785
INFO:root:current train perplexity175.91644287109375
INFO:root:current mean train loss 6566.5611501899275
INFO:root:current train perplexity176.2976837158203
INFO:root:current mean train loss 6552.592500430548
INFO:root:current train perplexity175.75025939941406
INFO:root:current mean train loss 6555.0479371543715
INFO:root:current train perplexity176.05096435546875
INFO:root:current mean train loss 6556.0725003353
INFO:root:current train perplexity176.31382751464844
INFO:root:current mean train loss 6564.774159227354
INFO:root:current train perplexity176.39892578125
INFO:root:current mean train loss 6560.053232432738
INFO:root:current train perplexity176.3343505859375
INFO:root:current mean train loss 6566.171185345502
INFO:root:current train perplexity176.37042236328125
INFO:root:current mean train loss 6561.946344155198
INFO:root:current train perplexity176.31483459472656
INFO:root:current mean train loss 6561.746952620022
INFO:root:current train perplexity176.2720489501953
INFO:root:current mean train loss 6557.5873226550475
INFO:root:current train perplexity176.11837768554688
INFO:root:current mean train loss 6557.916524498526
INFO:root:current train perplexity176.11007690429688
INFO:root:current mean train loss 6558.924695891011
INFO:root:current train perplexity176.0730438232422
INFO:root:current mean train loss 6556.023540408556
INFO:root:current train perplexity176.0118408203125
INFO:root:current mean train loss 6555.927021926961
INFO:root:current train perplexity175.99623107910156
INFO:root:current mean train loss 6556.324334645549
INFO:root:current train perplexity176.034423828125
INFO:root:current mean train loss 6553.848678837958
INFO:root:current train perplexity175.91006469726562

100%|██████████| 1/1 [07:40<00:00, 460.78s/it][A100%|██████████| 1/1 [07:40<00:00, 460.78s/it]
INFO:root:final mean train loss: 6551.1026577471
INFO:root:final train perplexity: 175.93524169921875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.76s/it][A100%|██████████| 1/1 [00:39<00:00, 39.76s/it]
INFO:root:eval mean loss: 6311.131913854721
INFO:root:eval perplexity: 165.18414306640625
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.55s/it][A100%|██████████| 1/1 [00:39<00:00, 39.55s/it]
INFO:root:eval mean loss: 6448.722137667609
INFO:root:eval perplexity: 200.673583984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/101
 50%|█████     | 101/200 [15:16:59<14:55:51, 542.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6609.113220214844
INFO:root:current train perplexity174.5322265625
INFO:root:current mean train loss 6547.571642645474
INFO:root:current train perplexity175.7771759033203
INFO:root:current mean train loss 6554.099218297888
INFO:root:current train perplexity176.174072265625
INFO:root:current mean train loss 6561.073509506033
INFO:root:current train perplexity176.44654846191406
INFO:root:current mean train loss 6568.427086463342
INFO:root:current train perplexity176.12728881835938
INFO:root:current mean train loss 6557.28700812288
INFO:root:current train perplexity175.88893127441406
INFO:root:current mean train loss 6554.317987615412
INFO:root:current train perplexity175.7170867919922
INFO:root:current mean train loss 6551.59401391738
INFO:root:current train perplexity175.85035705566406
INFO:root:current mean train loss 6547.3259008071
INFO:root:current train perplexity175.62159729003906
INFO:root:current mean train loss 6547.517423538141
INFO:root:current train perplexity175.57437133789062
INFO:root:current mean train loss 6545.044878621739
INFO:root:current train perplexity175.37405395507812
INFO:root:current mean train loss 6550.518632567484
INFO:root:current train perplexity175.5762939453125
INFO:root:current mean train loss 6544.299316004703
INFO:root:current train perplexity175.31275939941406
INFO:root:current mean train loss 6546.20577455654
INFO:root:current train perplexity175.52188110351562
INFO:root:current mean train loss 6554.611206399519
INFO:root:current train perplexity175.93263244628906
INFO:root:current mean train loss 6554.723688211165
INFO:root:current train perplexity175.90394592285156
INFO:root:current mean train loss 6556.82316876402
INFO:root:current train perplexity176.09823608398438
INFO:root:current mean train loss 6554.7893103397255
INFO:root:current train perplexity176.10394287109375
INFO:root:current mean train loss 6555.327971471039
INFO:root:current train perplexity176.13568115234375
INFO:root:current mean train loss 6553.289002611642
INFO:root:current train perplexity175.99069213867188

100%|██████████| 1/1 [07:45<00:00, 465.60s/it][A100%|██████████| 1/1 [07:45<00:00, 465.60s/it]
INFO:root:final mean train loss: 6551.59294801713
INFO:root:final train perplexity: 176.0033721923828
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.47s/it][A100%|██████████| 1/1 [00:39<00:00, 39.47s/it]
INFO:root:eval mean loss: 6310.650260762965
INFO:root:eval perplexity: 165.11973571777344
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:37<00:00, 37.92s/it][A100%|██████████| 1/1 [00:37<00:00, 37.92s/it]
INFO:root:eval mean loss: 6448.001082183621
INFO:root:eval perplexity: 200.55458068847656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/102
 51%|█████     | 102/200 [15:26:05<14:48:00, 543.67s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6499.303074692235
INFO:root:current train perplexity176.1544189453125
INFO:root:current mean train loss 6524.580643503289
INFO:root:current train perplexity174.3480682373047
INFO:root:current mean train loss 6520.777427575107
INFO:root:current train perplexity174.1399383544922
INFO:root:current mean train loss 6523.191762563345
INFO:root:current train perplexity174.34474182128906
INFO:root:current mean train loss 6520.033015931726
INFO:root:current train perplexity174.37625122070312
INFO:root:current mean train loss 6518.91777270462
INFO:root:current train perplexity174.18170166015625
INFO:root:current mean train loss 6535.378957160841
INFO:root:current train perplexity174.65060424804688
INFO:root:current mean train loss 6541.153616079042
INFO:root:current train perplexity174.79148864746094
INFO:root:current mean train loss 6545.280595832083
INFO:root:current train perplexity174.8420867919922
INFO:root:current mean train loss 6545.694587666633
INFO:root:current train perplexity175.0399627685547
INFO:root:current mean train loss 6547.47199071273
INFO:root:current train perplexity175.25408935546875
INFO:root:current mean train loss 6546.372334492911
INFO:root:current train perplexity175.08236694335938
INFO:root:current mean train loss 6546.654249749721
INFO:root:current train perplexity175.3580780029297
INFO:root:current mean train loss 6547.27077594594
INFO:root:current train perplexity175.3594207763672
INFO:root:current mean train loss 6549.141184836772
INFO:root:current train perplexity175.52154541015625
INFO:root:current mean train loss 6549.284271737912
INFO:root:current train perplexity175.5823974609375
INFO:root:current mean train loss 6548.639696278896
INFO:root:current train perplexity175.4720916748047
INFO:root:current mean train loss 6548.600396879959
INFO:root:current train perplexity175.51275634765625
INFO:root:current mean train loss 6544.773179374233
INFO:root:current train perplexity175.412353515625
INFO:root:current mean train loss 6549.265489604889
INFO:root:current train perplexity175.53627014160156

100%|██████████| 1/1 [07:32<00:00, 452.45s/it][A100%|██████████| 1/1 [07:32<00:00, 452.45s/it]
INFO:root:final mean train loss: 6548.362501305038
INFO:root:final train perplexity: 175.55519104003906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.77s/it][A100%|██████████| 1/1 [00:41<00:00, 41.77s/it]
INFO:root:eval mean loss: 6309.054396609043
INFO:root:eval perplexity: 164.90673828125
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.79s/it][A100%|██████████| 1/1 [00:38<00:00, 38.79s/it]
INFO:root:eval mean loss: 6446.562205646055
INFO:root:eval perplexity: 200.3175506591797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/103
 52%|█████▏    | 103/200 [15:35:00<14:34:53, 541.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6557.047734375
INFO:root:current train perplexity178.89508056640625
INFO:root:current mean train loss 6527.448291015625
INFO:root:current train perplexity175.62887573242188
INFO:root:current mean train loss 6532.29273046875
INFO:root:current train perplexity174.4146728515625
INFO:root:current mean train loss 6537.336383928571
INFO:root:current train perplexity175.05322265625
INFO:root:current mean train loss 6543.159624565972
INFO:root:current train perplexity175.18824768066406
INFO:root:current mean train loss 6536.365217507102
INFO:root:current train perplexity174.7522430419922
INFO:root:current mean train loss 6543.77643780048
INFO:root:current train perplexity174.9561767578125
INFO:root:current mean train loss 6555.394859375
INFO:root:current train perplexity175.43092346191406
INFO:root:current mean train loss 6546.628435776654
INFO:root:current train perplexity175.56423950195312
INFO:root:current mean train loss 6551.401891447368
INFO:root:current train perplexity175.66949462890625
INFO:root:current mean train loss 6557.361460658482
INFO:root:current train perplexity175.81028747558594
INFO:root:current mean train loss 6556.7465306555705
INFO:root:current train perplexity175.9902801513672
INFO:root:current mean train loss 6553.307823046875
INFO:root:current train perplexity175.7578887939453
INFO:root:current mean train loss 6560.56347800926
INFO:root:current train perplexity176.021240234375
INFO:root:current mean train loss 6558.344000875539
INFO:root:current train perplexity175.99488830566406
INFO:root:current mean train loss 6553.938876953125
INFO:root:current train perplexity175.69973754882812
INFO:root:current mean train loss 6554.255276692708
INFO:root:current train perplexity175.6874237060547
INFO:root:current mean train loss 6553.384797154018
INFO:root:current train perplexity175.7283172607422
INFO:root:current mean train loss 6554.610784681166
INFO:root:current train perplexity175.80181884765625
INFO:root:current mean train loss 6550.593595502804
INFO:root:current train perplexity175.60409545898438

100%|██████████| 1/1 [07:49<00:00, 469.32s/it][A100%|██████████| 1/1 [07:49<00:00, 469.32s/it]
INFO:root:final mean train loss: 6548.4868369667565
INFO:root:final train perplexity: 175.5724334716797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.63s/it][A100%|██████████| 1/1 [00:39<00:00, 39.63s/it]
INFO:root:eval mean loss: 6306.835405931405
INFO:root:eval perplexity: 164.61087036132812
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.08s/it][A100%|██████████| 1/1 [00:39<00:00, 39.08s/it]
INFO:root:eval mean loss: 6445.196555712544
INFO:root:eval perplexity: 200.0927276611328
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/104
 52%|█████▏    | 104/200 [15:44:10<14:30:18, 543.94s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6606.636762476679
INFO:root:current train perplexity174.72982788085938
INFO:root:current mean train loss 6572.286796524139
INFO:root:current train perplexity175.8177490234375
INFO:root:current mean train loss 6567.295287628745
INFO:root:current train perplexity175.86679077148438
INFO:root:current mean train loss 6590.576249042064
INFO:root:current train perplexity176.29971313476562
INFO:root:current mean train loss 6580.8500882461185
INFO:root:current train perplexity176.1282958984375
INFO:root:current mean train loss 6576.920706397432
INFO:root:current train perplexity176.1403045654297
INFO:root:current mean train loss 6578.731375035139
INFO:root:current train perplexity176.0214080810547
INFO:root:current mean train loss 6571.270789831527
INFO:root:current train perplexity175.95277404785156
INFO:root:current mean train loss 6566.713198123919
INFO:root:current train perplexity175.8194122314453
INFO:root:current mean train loss 6560.117478347984
INFO:root:current train perplexity175.55838012695312
INFO:root:current mean train loss 6564.31745877753
INFO:root:current train perplexity175.50555419921875
INFO:root:current mean train loss 6560.150742923896
INFO:root:current train perplexity175.43075561523438
INFO:root:current mean train loss 6561.814798043483
INFO:root:current train perplexity175.56565856933594
INFO:root:current mean train loss 6561.592428032987
INFO:root:current train perplexity175.64202880859375
INFO:root:current mean train loss 6559.114623940227
INFO:root:current train perplexity175.65943908691406
INFO:root:current mean train loss 6554.025866753749
INFO:root:current train perplexity175.513427734375
INFO:root:current mean train loss 6557.5931401610305
INFO:root:current train perplexity175.74440002441406
INFO:root:current mean train loss 6553.265269358818
INFO:root:current train perplexity175.6644744873047
INFO:root:current mean train loss 6553.209536470189
INFO:root:current train perplexity175.64260864257812
INFO:root:current mean train loss 6549.536744963777
INFO:root:current train perplexity175.5323486328125

100%|██████████| 1/1 [07:46<00:00, 466.37s/it][A100%|██████████| 1/1 [07:46<00:00, 466.38s/it]
INFO:root:final mean train loss: 6548.245442954567
INFO:root:final train perplexity: 175.53895568847656
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:41<00:00, 41.15s/it][A100%|██████████| 1/1 [00:41<00:00, 41.15s/it]
INFO:root:eval mean loss: 6307.696391220634
INFO:root:eval perplexity: 164.7255859375
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.23s/it][A100%|██████████| 1/1 [00:38<00:00, 38.23s/it]
INFO:root:eval mean loss: 6446.118142418827
INFO:root:eval perplexity: 200.24440002441406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/105
 52%|█████▎    | 105/200 [15:53:18<14:23:10, 545.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6581.799984886533
INFO:root:current train perplexity177.07101440429688
INFO:root:current mean train loss 6578.5665495499325
INFO:root:current train perplexity176.21356201171875
INFO:root:current mean train loss 6572.292391065141
INFO:root:current train perplexity176.57186889648438
INFO:root:current mean train loss 6573.680386861165
INFO:root:current train perplexity177.0277862548828
INFO:root:current mean train loss 6554.996098794228
INFO:root:current train perplexity176.40818786621094
INFO:root:current mean train loss 6547.785223137843
INFO:root:current train perplexity176.02157592773438
INFO:root:current mean train loss 6542.626511244746
INFO:root:current train perplexity175.7603302001953
INFO:root:current mean train loss 6543.777562978316
INFO:root:current train perplexity175.85546875
INFO:root:current mean train loss 6551.199547953196
INFO:root:current train perplexity175.81362915039062
INFO:root:current mean train loss 6550.02568240282
INFO:root:current train perplexity175.8649444580078
INFO:root:current mean train loss 6547.5454052013665
INFO:root:current train perplexity175.8304901123047
INFO:root:current mean train loss 6550.251214929529
INFO:root:current train perplexity175.80038452148438
INFO:root:current mean train loss 6554.504196024387
INFO:root:current train perplexity176.0146942138672
INFO:root:current mean train loss 6551.52200335023
INFO:root:current train perplexity175.98028564453125
INFO:root:current mean train loss 6551.5773573718625
INFO:root:current train perplexity175.72378540039062
INFO:root:current mean train loss 6547.1723225911455
INFO:root:current train perplexity175.5852508544922
INFO:root:current mean train loss 6546.747576280897
INFO:root:current train perplexity175.57394409179688
INFO:root:current mean train loss 6550.522736006254
INFO:root:current train perplexity175.796875
INFO:root:current mean train loss 6552.234462859525
INFO:root:current train perplexity175.72998046875

100%|██████████| 1/1 [07:38<00:00, 458.72s/it][A100%|██████████| 1/1 [07:38<00:00, 458.72s/it]
INFO:root:final mean train loss: 6549.972226202999
INFO:root:final train perplexity: 175.77833557128906
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:39<00:00, 39.77s/it][A100%|██████████| 1/1 [00:39<00:00, 39.77s/it]
INFO:root:eval mean loss: 6310.76374459774
INFO:root:eval perplexity: 165.13485717773438
INFO:root:evalaution complete
INFO:root:start evaluating on test

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|██████████| 1/1 [00:38<00:00, 38.42s/it][A100%|██████████| 1/1 [00:38<00:00, 38.42s/it]
INFO:root:eval mean loss: 6448.0908835120235
INFO:root:eval perplexity: 200.56939697265625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: distilbert_roberta_not_concat/106
 53%|█████▎    | 106/200 [16:02:18<14:11:18, 543.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6718.9853515625
INFO:root:current train perplexity203.49732971191406
INFO:root:current mean train loss 6556.68355023979
INFO:root:current train perplexity175.46791076660156
INFO:root:current mean train loss 6559.49510018268
INFO:root:current train perplexity175.7366943359375
INFO:root:current mean train loss 6552.037464636109
INFO:root:current train perplexity175.36744689941406
INFO:root:current mean train loss 6548.73399387274
INFO:root:current train perplexity175.35841369628906
INFO:root:current mean train loss 6542.420604104292
INFO:root:current train perplexity174.78614807128906
INFO:root:current mean train loss 6548.24891781926
INFO:root:current train perplexity174.7010040283203
INFO:root:current mean train loss 6548.886007572887
INFO:root:current train perplexity174.86935424804688
INFO:root:current mean train loss 6544.429351616144
INFO:root:current train perplexity174.98228454589844
INFO:root:current mean train loss 6544.315408010197
INFO:root:current train perplexity174.99497985839844
INFO:root:current mean train loss 6549.990938748751
INFO:root:current train perplexity175.13104248046875
INFO:root:current mean train loss 6547.75440251405
INFO:root:current train perplexity175.16612243652344
INFO:root:current mean train loss 6554.308121731239
INFO:root:current train perplexity175.4582061767578
INFO:root:current mean train loss 6550.8250902250675
INFO:root:current train perplexity175.29153442382812
INFO:root:current mean train loss 6547.2811862202225
INFO:root:current train perplexity175.4167938232422
INFO:root:current mean train loss 6551.079673121565
INFO:root:current train perplexity175.54539489746094
INFO:root:current mean train loss 6547.530864803736
INFO:root:current train perplexity175.31996154785156
INFO:root:current mean train loss 6547.283567972424
INFO:root:current train perplexity175.5476531982422
INFO:root:current mean train loss 6549.254366063992
INFO:root:current train perplexity175.66053771972656
INFO:root:current mean train loss 6550.860982141389
INFO:root:current train perplexity175.70208740234375

100%|██████████| 1/1 [07:32<00:00, 452.68s/it][A100%|██████████| 1/1 [07:32<00:00, 452.69s/it]
INFO:root:final mean train loss: 6548.969360536238
INFO:root:final train perplexity: 175.63926696777344
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][Aslurmstepd: error: *** JOB 29876522 ON ga007 CANCELLED AT 2023-02-07T04:40:18 ***
