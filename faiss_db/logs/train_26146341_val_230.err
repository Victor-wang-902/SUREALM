INFO:root:Output: small_val_230
INFO:root:Steps per epochs:992
INFO:root:Total steps:198400
/scratch/zw2374/public/faiss_db/models.py:432: UserWarning: Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.
  warnings.warn("Retrieval mode is activated but not all embedding layers are loaded. Either pass external embeddings or define embedding layers.")
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1 and are newly initialized: ['encoder.layer.3.crossattention.self.key.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'cls.predictions.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'cls.predictions.decoder.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/scratch/zw2374/public/faiss_db/models.py:446: UserWarning: Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.
  warnings.warn("Retrieval mode is activated but not both key embedding layers are initialized. Either pass external embeddings or redefine embedding layers.")
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training

  0%|          | 0/200 [00:00<?, ?it/s]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 24417.82222616793
INFO:root:current train perplexity15275.486328125
INFO:root:current mean train loss 20491.760001177765
INFO:root:current train perplexity3216.477783203125
INFO:root:current mean train loss 17711.5170914768
INFO:root:current train perplexity1078.0146484375
INFO:root:current mean train loss 15825.14469523418
INFO:root:current train perplexity508.1410827636719
INFO:root:current mean train loss 14455.862314472695
INFO:root:current train perplexity296.19134521484375
INFO:root:current mean train loss 13415.728192006209
INFO:root:current train perplexity197.1922607421875
INFO:root:current mean train loss 12607.35792133293
INFO:root:current train perplexity143.36912536621094
INFO:root:current mean train loss 11958.091128925806
INFO:root:current train perplexity111.2596206665039
INFO:root:current mean train loss 11426.045954380736
INFO:root:current train perplexity90.25326538085938


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.91s/it]
INFO:root:final mean train loss: 10997.405402152768
INFO:root:final train perplexity: 76.61517333984375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it]
INFO:root:eval mean loss: 6400.935640375665
INFO:root:eval perplexity: 13.3078031539917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/1

  0%|          | 1/200 [04:14<14:03:54, 254.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 6816.582589285715
INFO:root:current train perplexity14.470640182495117
INFO:root:current mean train loss 6716.105815566589
INFO:root:current train perplexity14.313220024108887
INFO:root:current mean train loss 6689.540558008756
INFO:root:current train perplexity14.087944984436035
INFO:root:current mean train loss 6617.939359285932
INFO:root:current train perplexity13.670827865600586
INFO:root:current mean train loss 6563.617840141278
INFO:root:current train perplexity13.368318557739258
INFO:root:current mean train loss 6514.263210559973
INFO:root:current train perplexity13.103009223937988
INFO:root:current mean train loss 6471.153421669069
INFO:root:current train perplexity12.8318510055542
INFO:root:current mean train loss 6423.857765122215
INFO:root:current train perplexity12.593379020690918
INFO:root:current mean train loss 6379.634025034851
INFO:root:current train perplexity12.368467330932617
INFO:root:current mean train loss 6334.752013958276
INFO:root:current train perplexity12.163582801818848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.40s/it]
INFO:root:final mean train loss: 6297.600890498007
INFO:root:final train perplexity: 11.996150016784668
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.62s/it]
INFO:root:eval mean loss: 5520.828440131871
INFO:root:eval perplexity: 9.322778701782227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/2

  1%|          | 2/200 [07:50<12:44:21, 231.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5982.827571614584
INFO:root:current train perplexity10.619580268859863
INFO:root:current mean train loss 5867.631547214674
INFO:root:current train perplexity10.183588027954102
INFO:root:current mean train loss 5842.850815316134
INFO:root:current train perplexity10.033884048461914
INFO:root:current mean train loss 5824.279699900793
INFO:root:current train perplexity9.928223609924316
INFO:root:current mean train loss 5799.4383718467625
INFO:root:current train perplexity9.837095260620117
INFO:root:current mean train loss 5772.590396503338
INFO:root:current train perplexity9.74563217163086
INFO:root:current mean train loss 5746.512723100864
INFO:root:current train perplexity9.661052703857422
INFO:root:current mean train loss 5727.44783517264
INFO:root:current train perplexity9.582817077636719
INFO:root:current mean train loss 5713.791565016296
INFO:root:current train perplexity9.510709762573242
INFO:root:current mean train loss 5694.213925887978
INFO:root:current train perplexity9.424988746643066


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.93s/it]
INFO:root:final mean train loss: 5670.362113337363
INFO:root:final train perplexity: 9.366334915161133
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.54s/it]
INFO:root:eval mean loss: 5143.3056605995125
INFO:root:eval perplexity: 8.002882957458496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/3

  2%|â–         | 3/200 [12:07<13:19:42, 243.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5536.185504415761
INFO:root:current train perplexity8.722943305969238
INFO:root:current mean train loss 5455.496248570884
INFO:root:current train perplexity8.615131378173828
INFO:root:current mean train loss 5456.213000105101
INFO:root:current train perplexity8.561712265014648
INFO:root:current mean train loss 5428.724069695723
INFO:root:current train perplexity8.479928016662598
INFO:root:current mean train loss 5415.562847453088
INFO:root:current train perplexity8.453200340270996
INFO:root:current mean train loss 5398.678794029338
INFO:root:current train perplexity8.398296356201172
INFO:root:current mean train loss 5388.589857857644
INFO:root:current train perplexity8.365163803100586
INFO:root:current mean train loss 5378.622966507824
INFO:root:current train perplexity8.330246925354004
INFO:root:current mean train loss 5368.00922691373
INFO:root:current train perplexity8.29503059387207
INFO:root:current mean train loss 5357.020794538867
INFO:root:current train perplexity8.259502410888672


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.70s/it]
INFO:root:final mean train loss: 5346.028452473302
INFO:root:final train perplexity: 8.241340637207031
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it]
INFO:root:eval mean loss: 4923.203277371454
INFO:root:eval perplexity: 7.321380615234375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/4

  2%|â–         | 4/200 [16:49<14:05:05, 258.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5284.921465473791
INFO:root:current train perplexity7.859070777893066
INFO:root:current mean train loss 5209.048205659589
INFO:root:current train perplexity7.787271022796631
INFO:root:current mean train loss 5210.99480857684
INFO:root:current train perplexity7.807588577270508
INFO:root:current mean train loss 5200.179385090162
INFO:root:current train perplexity7.7615861892700195
INFO:root:current mean train loss 5193.1991371809745
INFO:root:current train perplexity7.728639602661133
INFO:root:current mean train loss 5182.8017679275545
INFO:root:current train perplexity7.699374675750732
INFO:root:current mean train loss 5174.817153761391
INFO:root:current train perplexity7.674186706542969
INFO:root:current mean train loss 5167.66262905053
INFO:root:current train perplexity7.660662651062012
INFO:root:current mean train loss 5154.346323024782
INFO:root:current train perplexity7.636112689971924
INFO:root:current mean train loss 5146.454797009264
INFO:root:current train perplexity7.607435703277588


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 5136.959613800049
INFO:root:final train perplexity: 7.588843822479248
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it]
INFO:root:eval mean loss: 4789.945083942819
INFO:root:eval perplexity: 6.93730354309082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/5

  2%|â–Ž         | 5/200 [21:20<14:15:23, 263.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 5043.338053385417
INFO:root:current train perplexity7.233151912689209
INFO:root:current mean train loss 5039.673842176259
INFO:root:current train perplexity7.305349826812744
INFO:root:current mean train loss 5047.654572682401
INFO:root:current train perplexity7.316515922546387
INFO:root:current mean train loss 5029.915309849742
INFO:root:current train perplexity7.253373146057129
INFO:root:current mean train loss 5027.408584629485
INFO:root:current train perplexity7.242844104766846
INFO:root:current mean train loss 5015.332037591315
INFO:root:current train perplexity7.219882488250732
INFO:root:current mean train loss 5006.850669228042
INFO:root:current train perplexity7.1980719566345215
INFO:root:current mean train loss 5006.827396872885
INFO:root:current train perplexity7.193361759185791
INFO:root:current mean train loss 5002.862727786986
INFO:root:current train perplexity7.177005290985107
INFO:root:current mean train loss 4994.565592967918
INFO:root:current train perplexity7.160841941833496


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.64s/it]
INFO:root:final mean train loss: 4986.4134329518965
INFO:root:final train perplexity: 7.1512298583984375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4673.080635666001
INFO:root:eval perplexity: 6.617094993591309
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/6

  3%|â–Ž         | 6/200 [26:22<14:53:15, 276.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4858.565564744016
INFO:root:current train perplexity6.8424601554870605
INFO:root:current mean train loss 4912.587648145196
INFO:root:current train perplexity6.919214248657227
INFO:root:current mean train loss 4893.310562689778
INFO:root:current train perplexity6.8991923332214355
INFO:root:current mean train loss 4894.798346879503
INFO:root:current train perplexity6.895049095153809
INFO:root:current mean train loss 4891.399012077041
INFO:root:current train perplexity6.883172512054443
INFO:root:current mean train loss 4882.495327853633
INFO:root:current train perplexity6.861628532409668
INFO:root:current mean train loss 4882.068958594958
INFO:root:current train perplexity6.858482360839844
INFO:root:current mean train loss 4878.965913131693
INFO:root:current train perplexity6.846532344818115
INFO:root:current mean train loss 4875.417581929789
INFO:root:current train perplexity6.838046073913574
INFO:root:current mean train loss 4873.728727024485
INFO:root:current train perplexity6.828758716583252


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.61s/it]
INFO:root:final mean train loss: 4868.855907686295
INFO:root:final train perplexity: 6.8271307945251465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it]
INFO:root:eval mean loss: 4591.848989500221
INFO:root:eval perplexity: 6.403270721435547
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/7

  4%|â–Ž         | 7/200 [29:46<13:32:52, 252.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4764.474076704545
INFO:root:current train perplexity6.63834810256958
INFO:root:current mean train loss 4815.753525075605
INFO:root:current train perplexity6.69736385345459
INFO:root:current mean train loss 4808.674898514093
INFO:root:current train perplexity6.65394401550293
INFO:root:current mean train loss 4807.606300891285
INFO:root:current train perplexity6.64595365524292
INFO:root:current mean train loss 4800.894790951236
INFO:root:current train perplexity6.626824855804443
INFO:root:current mean train loss 4795.055326224662
INFO:root:current train perplexity6.609687805175781
INFO:root:current mean train loss 4793.915432669371
INFO:root:current train perplexity6.608366012573242
INFO:root:current mean train loss 4795.326962825952
INFO:root:current train perplexity6.602812767028809
INFO:root:current mean train loss 4786.456749131945
INFO:root:current train perplexity6.5867228507995605
INFO:root:current mean train loss 4779.21624570517
INFO:root:current train perplexity6.578695774078369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.48s/it]
INFO:root:final mean train loss: 4774.908455571821
INFO:root:final train perplexity: 6.578716278076172
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it]
INFO:root:eval mean loss: 4523.53281527039
INFO:root:eval perplexity: 6.2288007736206055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/8

  4%|â–         | 8/200 [33:12<12:40:56, 237.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4720.139299665178
INFO:root:current train perplexity6.381626605987549
INFO:root:current mean train loss 4708.371378331097
INFO:root:current train perplexity6.401413917541504
INFO:root:current mean train loss 4702.125569970889
INFO:root:current train perplexity6.399418354034424
INFO:root:current mean train loss 4712.846647404443
INFO:root:current train perplexity6.3996453285217285
INFO:root:current mean train loss 4717.44454791273
INFO:root:current train perplexity6.409804344177246
INFO:root:current mean train loss 4709.571906569161
INFO:root:current train perplexity6.397111892700195
INFO:root:current mean train loss 4706.429694864724
INFO:root:current train perplexity6.393527507781982
INFO:root:current mean train loss 4704.590632167431
INFO:root:current train perplexity6.38651704788208
INFO:root:current mean train loss 4700.07549178918
INFO:root:current train perplexity6.379067897796631
INFO:root:current mean train loss 4696.739939783716
INFO:root:current train perplexity6.370369911193848


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.34s/it]
INFO:root:final mean train loss: 4694.706768528108
INFO:root:final train perplexity: 6.373811721801758
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.00s/it]
INFO:root:eval mean loss: 4470.180797387522
INFO:root:eval perplexity: 6.095860481262207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/9

  4%|â–         | 9/200 [36:36<12:03:32, 227.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4617.655452244719
INFO:root:current train perplexity6.154716968536377
INFO:root:current mean train loss 4632.248872098867
INFO:root:current train perplexity6.213072299957275
INFO:root:current mean train loss 4654.628842286958
INFO:root:current train perplexity6.249086856842041
INFO:root:current mean train loss 4649.268770531503
INFO:root:current train perplexity6.238279819488525
INFO:root:current mean train loss 4642.419759114583
INFO:root:current train perplexity6.233603000640869
INFO:root:current mean train loss 4643.6509490272
INFO:root:current train perplexity6.231200218200684
INFO:root:current mean train loss 4645.271070318321
INFO:root:current train perplexity6.236614227294922
INFO:root:current mean train loss 4635.843879511694
INFO:root:current train perplexity6.2235846519470215
INFO:root:current mean train loss 4638.232981632553
INFO:root:current train perplexity6.220851898193359
INFO:root:current mean train loss 4634.14263168005
INFO:root:current train perplexity6.214267253875732


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.42s/it]
INFO:root:final mean train loss: 4630.031643529092
INFO:root:final train perplexity: 6.213232517242432
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4432.690176889406
INFO:root:eval perplexity: 6.004143714904785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/10

  5%|â–Œ         | 10/200 [40:02<11:38:41, 220.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4614.197797171677
INFO:root:current train perplexity6.087754726409912
INFO:root:current mean train loss 4576.461959071666
INFO:root:current train perplexity6.0545501708984375
INFO:root:current mean train loss 4587.491976611504
INFO:root:current train perplexity6.070677757263184
INFO:root:current mean train loss 4582.736275947188
INFO:root:current train perplexity6.075705051422119
INFO:root:current mean train loss 4586.422470825451
INFO:root:current train perplexity6.074352264404297
INFO:root:current mean train loss 4579.643939662268
INFO:root:current train perplexity6.073220729827881
INFO:root:current mean train loss 4578.912600892282
INFO:root:current train perplexity6.070011138916016
INFO:root:current mean train loss 4577.075531593489
INFO:root:current train perplexity6.0670084953308105
INFO:root:current mean train loss 4572.015732766283
INFO:root:current train perplexity6.063971996307373
INFO:root:current mean train loss 4572.257480827854
INFO:root:current train perplexity6.066354751586914


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.47s/it]
INFO:root:final mean train loss: 4569.489556774016
INFO:root:final train perplexity: 6.066586017608643
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4384.305622506649
INFO:root:eval perplexity: 5.887811660766602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/11

  6%|â–Œ         | 11/200 [43:25<11:17:47, 215.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4520.81628277658
INFO:root:current train perplexity5.93345308303833
INFO:root:current mean train loss 4542.020873370655
INFO:root:current train perplexity5.966311931610107
INFO:root:current mean train loss 4529.538484899009
INFO:root:current train perplexity5.94126033782959
INFO:root:current mean train loss 4529.242111166626
INFO:root:current train perplexity5.952970027923584
INFO:root:current mean train loss 4526.131405808843
INFO:root:current train perplexity5.950707912445068
INFO:root:current mean train loss 4521.942293807229
INFO:root:current train perplexity5.942795753479004
INFO:root:current mean train loss 4521.1147023829835
INFO:root:current train perplexity5.943458557128906
INFO:root:current mean train loss 4518.2347376434445
INFO:root:current train perplexity5.937231540679932
INFO:root:current mean train loss 4519.950528356645
INFO:root:current train perplexity5.940483093261719
INFO:root:current mean train loss 4519.540012348024
INFO:root:current train perplexity5.940264701843262


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.42s/it]
INFO:root:final mean train loss: 4516.097806253741
INFO:root:final train perplexity: 5.940131187438965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it]
INFO:root:eval mean loss: 4351.121350011082
INFO:root:eval perplexity: 5.809333324432373
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/12

  6%|â–Œ         | 12/200 [46:48<11:03:11, 211.66s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4483.216542454769
INFO:root:current train perplexity5.8495001792907715
INFO:root:current mean train loss 4468.918082682291
INFO:root:current train perplexity5.845694065093994
INFO:root:current mean train loss 4473.18829366393
INFO:root:current train perplexity5.8547234535217285
INFO:root:current mean train loss 4467.508098669897
INFO:root:current train perplexity5.841805458068848
INFO:root:current mean train loss 4473.312179904514
INFO:root:current train perplexity5.842666149139404
INFO:root:current mean train loss 4467.616440716912
INFO:root:current train perplexity5.83830451965332
INFO:root:current mean train loss 4465.796878512815
INFO:root:current train perplexity5.835911273956299
INFO:root:current mean train loss 4469.601327572229
INFO:root:current train perplexity5.830016136169434
INFO:root:current mean train loss 4471.621011096805
INFO:root:current train perplexity5.833206653594971


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.89s/it]
INFO:root:final mean train loss: 4470.630147257159
INFO:root:final train perplexity: 5.834525108337402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it]
INFO:root:eval mean loss: 4319.847071005098
INFO:root:eval perplexity: 5.736328601837158
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/13

  6%|â–‹         | 13/200 [50:14<10:53:33, 209.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4697.323567708333
INFO:root:current train perplexity6.017658710479736
INFO:root:current mean train loss 4446.988902267901
INFO:root:current train perplexity5.736199855804443
INFO:root:current mean train loss 4445.27333888162
INFO:root:current train perplexity5.736728668212891
INFO:root:current mean train loss 4444.457376108705
INFO:root:current train perplexity5.744884967803955
INFO:root:current mean train loss 4445.813107625427
INFO:root:current train perplexity5.748562812805176
INFO:root:current mean train loss 4442.11215373773
INFO:root:current train perplexity5.751047611236572
INFO:root:current mean train loss 4437.779651546953
INFO:root:current train perplexity5.750897407531738
INFO:root:current mean train loss 4433.967890124911
INFO:root:current train perplexity5.7435503005981445
INFO:root:current mean train loss 4432.509229002082
INFO:root:current train perplexity5.740963459014893
INFO:root:current mean train loss 4432.804415241297
INFO:root:current train perplexity5.740432262420654


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it]
INFO:root:final mean train loss: 4429.24123788649
INFO:root:final train perplexity: 5.740025997161865
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it]
INFO:root:eval mean loss: 4296.512797470634
INFO:root:eval perplexity: 5.682456970214844
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/14

  7%|â–‹         | 14/200 [53:57<11:02:27, 213.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4365.308549360795
INFO:root:current train perplexity5.539397239685059
INFO:root:current mean train loss 4386.817932678772
INFO:root:current train perplexity5.6651530265808105
INFO:root:current mean train loss 4375.12994876518
INFO:root:current train perplexity5.654848575592041
INFO:root:current mean train loss 4379.661245070087
INFO:root:current train perplexity5.652908802032471
INFO:root:current mean train loss 4376.956785921343
INFO:root:current train perplexity5.647319793701172
INFO:root:current mean train loss 4380.2315370443985
INFO:root:current train perplexity5.651266098022461
INFO:root:current mean train loss 4387.9978934380115
INFO:root:current train perplexity5.6570725440979
INFO:root:current mean train loss 4389.344635224375
INFO:root:current train perplexity5.654221057891846
INFO:root:current mean train loss 4391.577238447422
INFO:root:current train perplexity5.658091068267822
INFO:root:current mean train loss 4393.828588625995
INFO:root:current train perplexity5.654411792755127


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.76s/it]
INFO:root:final mean train loss: 4390.356664165373
INFO:root:final train perplexity: 5.652640342712402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it]
INFO:root:eval mean loss: 4272.489436156361
INFO:root:eval perplexity: 5.627523422241211
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/15

  8%|â–Š         | 15/200 [58:04<11:30:07, 223.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4416.692639802632
INFO:root:current train perplexity5.653364181518555
INFO:root:current mean train loss 4353.23791196166
INFO:root:current train perplexity5.5508904457092285
INFO:root:current mean train loss 4352.795504914026
INFO:root:current train perplexity5.569015026092529
INFO:root:current mean train loss 4351.145894304712
INFO:root:current train perplexity5.569417953491211
INFO:root:current mean train loss 4356.595425771927
INFO:root:current train perplexity5.576169490814209
INFO:root:current mean train loss 4353.805387934279
INFO:root:current train perplexity5.570830345153809
INFO:root:current mean train loss 4353.22653922973
INFO:root:current train perplexity5.571681499481201
INFO:root:current mean train loss 4356.019302389386
INFO:root:current train perplexity5.5744428634643555
INFO:root:current mean train loss 4355.911322401556
INFO:root:current train perplexity5.575591087341309
INFO:root:current mean train loss 4356.234855045821
INFO:root:current train perplexity5.573649883270264


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.96s/it]
INFO:root:final mean train loss: 4356.51116906443
INFO:root:final train perplexity: 5.577661991119385
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/it]
INFO:root:eval mean loss: 4251.229346742021
INFO:root:eval perplexity: 5.579349517822266
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/16

  8%|â–Š         | 16/200 [1:01:52<11:30:33, 225.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4349.227783203125
INFO:root:current train perplexity5.5682268142700195
INFO:root:current mean train loss 4298.320631612943
INFO:root:current train perplexity5.497847557067871
INFO:root:current mean train loss 4313.290293958218
INFO:root:current train perplexity5.499379634857178
INFO:root:current mean train loss 4311.523864559442
INFO:root:current train perplexity5.492092132568359
INFO:root:current mean train loss 4317.48276035568
INFO:root:current train perplexity5.494081497192383
INFO:root:current mean train loss 4317.912871909096
INFO:root:current train perplexity5.489995002746582
INFO:root:current mean train loss 4318.422335245963
INFO:root:current train perplexity5.488442897796631
INFO:root:current mean train loss 4317.986661593556
INFO:root:current train perplexity5.494720935821533
INFO:root:current mean train loss 4318.18840659717
INFO:root:current train perplexity5.491607666015625
INFO:root:current mean train loss 4320.550697499494
INFO:root:current train perplexity5.497036457061768


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.11s/it]
INFO:root:final mean train loss: 4321.356915504702
INFO:root:final train perplexity: 5.5008368492126465
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it]
INFO:root:eval mean loss: 4229.841578706782
INFO:root:eval perplexity: 5.531304836273193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/17

  8%|â–Š         | 17/200 [1:05:32<11:21:28, 223.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4257.210358537946
INFO:root:current train perplexity5.411192893981934
INFO:root:current mean train loss 4290.84595630787
INFO:root:current train perplexity5.429224014282227
INFO:root:current mean train loss 4285.31710646609
INFO:root:current train perplexity5.431950569152832
INFO:root:current mean train loss 4287.678369869403
INFO:root:current train perplexity5.42957067489624
INFO:root:current mean train loss 4287.832098037895
INFO:root:current train perplexity5.433959484100342
INFO:root:current mean train loss 4299.426685254819
INFO:root:current train perplexity5.437966823577881
INFO:root:current mean train loss 4294.695161786417
INFO:root:current train perplexity5.431172847747803
INFO:root:current mean train loss 4293.325396603954
INFO:root:current train perplexity5.427702903747559
INFO:root:current mean train loss 4294.666834007766
INFO:root:current train perplexity5.435934543609619
INFO:root:current mean train loss 4293.521164772727
INFO:root:current train perplexity5.437970161437988


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.18s/it]
INFO:root:final mean train loss: 4292.553030383202
INFO:root:final train perplexity: 5.438679218292236
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it]
INFO:root:eval mean loss: 4217.50888948914
INFO:root:eval perplexity: 5.50378942489624
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/18

  9%|â–‰         | 18/200 [1:09:07<11:10:06, 220.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4260.790385401526
INFO:root:current train perplexity5.3888044357299805
INFO:root:current mean train loss 4277.199744591346
INFO:root:current train perplexity5.395602226257324
INFO:root:current mean train loss 4269.489209587191
INFO:root:current train perplexity5.383885860443115
INFO:root:current mean train loss 4276.11086760318
INFO:root:current train perplexity5.398196220397949
INFO:root:current mean train loss 4277.275875599605
INFO:root:current train perplexity5.395047187805176
INFO:root:current mean train loss 4277.070904642179
INFO:root:current train perplexity5.391622543334961
INFO:root:current mean train loss 4273.115685826327
INFO:root:current train perplexity5.385857105255127
INFO:root:current mean train loss 4269.71794134579
INFO:root:current train perplexity5.381242275238037
INFO:root:current mean train loss 4266.04222213727
INFO:root:current train perplexity5.37849760055542
INFO:root:current mean train loss 4265.167788298234
INFO:root:current train perplexity5.375858306884766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.11s/it]
INFO:root:final mean train loss: 4264.18101569145
INFO:root:final train perplexity: 5.378142356872559
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it]
INFO:root:eval mean loss: 4198.698856867797
INFO:root:eval perplexity: 5.462085723876953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/19

 10%|â–‰         | 19/200 [1:12:31<10:51:42, 216.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4207.980004404106
INFO:root:current train perplexity5.240431308746338
INFO:root:current mean train loss 4205.603750064673
INFO:root:current train perplexity5.250675201416016
INFO:root:current mean train loss 4235.189455070344
INFO:root:current train perplexity5.279560089111328
INFO:root:current mean train loss 4227.421173182648
INFO:root:current train perplexity5.290993690490723
INFO:root:current mean train loss 4233.892767591117
INFO:root:current train perplexity5.296388626098633
INFO:root:current mean train loss 4232.998961848486
INFO:root:current train perplexity5.303147315979004
INFO:root:current mean train loss 4240.368282195061
INFO:root:current train perplexity5.315573215484619
INFO:root:current mean train loss 4241.63267206225
INFO:root:current train perplexity5.316943168640137
INFO:root:current mean train loss 4241.625451846633
INFO:root:current train perplexity5.320185661315918
INFO:root:current mean train loss 4244.343015781085
INFO:root:current train perplexity5.32418966293335


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.91s/it]
INFO:root:final mean train loss: 4238.135072585075
INFO:root:final train perplexity: 5.323159217834473
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.60s/it]
INFO:root:eval mean loss: 4185.19160710328
INFO:root:eval perplexity: 5.432332515716553
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/20

 10%|â–ˆ         | 20/200 [1:16:14<10:54:32, 218.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4244.95649331303
INFO:root:current train perplexity5.274750232696533
INFO:root:current mean train loss 4231.286892872937
INFO:root:current train perplexity5.260170936584473
INFO:root:current mean train loss 4223.912808804898
INFO:root:current train perplexity5.261818885803223
INFO:root:current mean train loss 4222.385732666696
INFO:root:current train perplexity5.272056579589844
INFO:root:current mean train loss 4217.913015727124
INFO:root:current train perplexity5.266293525695801
INFO:root:current mean train loss 4214.499524821109
INFO:root:current train perplexity5.262746334075928
INFO:root:current mean train loss 4216.341852816175
INFO:root:current train perplexity5.2635498046875
INFO:root:current mean train loss 4220.331313302866
INFO:root:current train perplexity5.273429870605469
INFO:root:current mean train loss 4218.680464259404
INFO:root:current train perplexity5.273011684417725
INFO:root:current mean train loss 4216.296577397924
INFO:root:current train perplexity5.2698259353637695


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.11s/it]
INFO:root:final mean train loss: 4211.717010744156
INFO:root:final train perplexity: 5.267965316772461
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.01s/it]
INFO:root:eval mean loss: 4175.5244175254875
INFO:root:eval perplexity: 5.411138534545898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/21

 10%|â–ˆ         | 21/200 [1:19:42<10:40:54, 214.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4168.918799556903
INFO:root:current train perplexity5.21538782119751
INFO:root:current mean train loss 4182.397655372848
INFO:root:current train perplexity5.235307216644287
INFO:root:current mean train loss 4186.482591036107
INFO:root:current train perplexity5.222503662109375
INFO:root:current mean train loss 4182.655918048578
INFO:root:current train perplexity5.2106122970581055
INFO:root:current mean train loss 4187.423286519674
INFO:root:current train perplexity5.216289043426514
INFO:root:current mean train loss 4187.258066113453
INFO:root:current train perplexity5.2118401527404785
INFO:root:current mean train loss 4189.40566289121
INFO:root:current train perplexity5.213663578033447
INFO:root:current mean train loss 4190.273601427538
INFO:root:current train perplexity5.2136383056640625
INFO:root:current mean train loss 4191.759548517247
INFO:root:current train perplexity5.2145795822143555
INFO:root:current mean train loss 4190.697068444206
INFO:root:current train perplexity5.217769622802734


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.07s/it]
INFO:root:final mean train loss: 4189.065771595125
INFO:root:final train perplexity: 5.221097946166992
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it]
INFO:root:eval mean loss: 4154.548618614251
INFO:root:eval perplexity: 5.365435600280762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/22

 11%|â–ˆ         | 22/200 [1:23:20<10:40:47, 215.99s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4178.720013020833
INFO:root:current train perplexity5.168345928192139
INFO:root:current mean train loss 4170.419610770089
INFO:root:current train perplexity5.177759170532227
INFO:root:current mean train loss 4162.069042080966
INFO:root:current train perplexity5.156416416168213
INFO:root:current mean train loss 4161.362841796875
INFO:root:current train perplexity5.161647796630859
INFO:root:current mean train loss 4161.617251233552
INFO:root:current train perplexity5.16337251663208
INFO:root:current mean train loss 4166.694430197011
INFO:root:current train perplexity5.168854236602783
INFO:root:current mean train loss 4164.7426023582175
INFO:root:current train perplexity5.171200752258301
INFO:root:current mean train loss 4169.000344947077
INFO:root:current train perplexity5.1737213134765625
INFO:root:current mean train loss 4168.966150111607
INFO:root:current train perplexity5.173033714294434
INFO:root:current mean train loss 4170.436282051282
INFO:root:current train perplexity5.173430442810059


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it]
INFO:root:final mean train loss: 4165.213110585367
INFO:root:final train perplexity: 5.172194957733154
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4147.571356590758
INFO:root:eval perplexity: 5.350318431854248
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/23

 12%|â–ˆâ–        | 23/200 [1:27:09<10:48:05, 219.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4152.3710496282
INFO:root:current train perplexity5.094832897186279
INFO:root:current mean train loss 4148.733964096653
INFO:root:current train perplexity5.114834308624268
INFO:root:current mean train loss 4147.620725382343
INFO:root:current train perplexity5.114614486694336
INFO:root:current mean train loss 4152.404449861292
INFO:root:current train perplexity5.12227725982666
INFO:root:current mean train loss 4154.865251055416
INFO:root:current train perplexity5.129030704498291
INFO:root:current mean train loss 4148.604768573113
INFO:root:current train perplexity5.123311519622803
INFO:root:current mean train loss 4143.148713453971
INFO:root:current train perplexity5.116683006286621
INFO:root:current mean train loss 4148.027261746189
INFO:root:current train perplexity5.1268815994262695
INFO:root:current mean train loss 4146.906298109251
INFO:root:current train perplexity5.129079341888428
INFO:root:current mean train loss 4147.583401964252
INFO:root:current train perplexity5.130068778991699


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.69s/it]
INFO:root:final mean train loss: 4144.464651476952
INFO:root:final train perplexity: 5.13002872467041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it]
INFO:root:eval mean loss: 4138.471998282358
INFO:root:eval perplexity: 5.330667972564697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/24

 12%|â–ˆâ–        | 24/200 [1:30:35<10:32:39, 215.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4090.377623841003
INFO:root:current train perplexity5.041793346405029
INFO:root:current mean train loss 4106.4153113240345
INFO:root:current train perplexity5.066511631011963
INFO:root:current mean train loss 4105.787640443782
INFO:root:current train perplexity5.05959415435791
INFO:root:current mean train loss 4115.515979659527
INFO:root:current train perplexity5.076956748962402
INFO:root:current mean train loss 4116.2203451183805
INFO:root:current train perplexity5.080907821655273
INFO:root:current mean train loss 4121.848206495875
INFO:root:current train perplexity5.084192276000977
INFO:root:current mean train loss 4124.2052625554
INFO:root:current train perplexity5.087118148803711
INFO:root:current mean train loss 4128.761136331088
INFO:root:current train perplexity5.089193820953369
INFO:root:current mean train loss 4127.562259695479
INFO:root:current train perplexity5.091793537139893
INFO:root:current mean train loss 4126.342456128595
INFO:root:current train perplexity5.086909294128418


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.49s/it]
INFO:root:final mean train loss: 4123.020983234529
INFO:root:final train perplexity: 5.086811065673828
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it]
INFO:root:eval mean loss: 4125.706511801862
INFO:root:eval perplexity: 5.303222179412842
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/25

 12%|â–ˆâ–Ž        | 25/200 [1:33:58<10:18:03, 211.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4077.4575688525883
INFO:root:current train perplexity5.056606292724609
INFO:root:current mean train loss 4087.037593975738
INFO:root:current train perplexity5.026518821716309
INFO:root:current mean train loss 4100.7484894309155
INFO:root:current train perplexity5.0374674797058105
INFO:root:current mean train loss 4106.830855214207
INFO:root:current train perplexity5.046804428100586
INFO:root:current mean train loss 4106.299143208292
INFO:root:current train perplexity5.0437541007995605
INFO:root:current mean train loss 4104.298956512808
INFO:root:current train perplexity5.03769588470459
INFO:root:current mean train loss 4104.761494168567
INFO:root:current train perplexity5.039379596710205
INFO:root:current mean train loss 4108.247953068777
INFO:root:current train perplexity5.043050765991211
INFO:root:current mean train loss 4108.531177491049
INFO:root:current train perplexity5.0482096672058105


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.69s/it]
INFO:root:final mean train loss: 4102.866444987635
INFO:root:final train perplexity: 5.046523094177246
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it]
INFO:root:eval mean loss: 4121.366011815714
INFO:root:eval perplexity: 5.293922424316406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/26

 13%|â–ˆâ–Ž        | 26/200 [1:37:33<10:17:19, 212.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4151.184674944197
INFO:root:current train perplexity5.1231689453125
INFO:root:current mean train loss 4076.7175703672606
INFO:root:current train perplexity5.0139312744140625
INFO:root:current mean train loss 4085.3732815802387
INFO:root:current train perplexity5.020628452301025
INFO:root:current mean train loss 4087.1753295500816
INFO:root:current train perplexity5.016545295715332
INFO:root:current mean train loss 4094.7304981428515
INFO:root:current train perplexity5.02872896194458
INFO:root:current mean train loss 4086.167602779832
INFO:root:current train perplexity5.017387866973877
INFO:root:current mean train loss 4088.8176096581547
INFO:root:current train perplexity5.010848522186279
INFO:root:current mean train loss 4090.7649637138657
INFO:root:current train perplexity5.015822887420654
INFO:root:current mean train loss 4088.2097682267463
INFO:root:current train perplexity5.011927604675293
INFO:root:current mean train loss 4086.158617921806
INFO:root:current train perplexity5.009029865264893


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.80s/it]
INFO:root:final mean train loss: 4084.3776344791536
INFO:root:final train perplexity: 5.009846210479736
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.57s/it]
INFO:root:eval mean loss: 4108.409091381316
INFO:root:eval perplexity: 5.266258239746094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/27

 14%|â–ˆâ–Ž        | 27/200 [1:41:12<10:18:53, 214.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4103.405322265625
INFO:root:current train perplexity4.955387115478516
INFO:root:current mean train loss 4045.6656016474185
INFO:root:current train perplexity4.951022624969482
INFO:root:current mean train loss 4044.7791174600293
INFO:root:current train perplexity4.950441360473633
INFO:root:current mean train loss 4057.489308965774
INFO:root:current train perplexity4.9631667137146
INFO:root:current mean train loss 4067.8230509930345
INFO:root:current train perplexity4.976505756378174
INFO:root:current mean train loss 4062.0220475576457
INFO:root:current train perplexity4.965220928192139
INFO:root:current mean train loss 4065.5737912061736
INFO:root:current train perplexity4.967777252197266
INFO:root:current mean train loss 4066.990345006556
INFO:root:current train perplexity4.967628479003906
INFO:root:current mean train loss 4070.2454251342024
INFO:root:current train perplexity4.97102165222168
INFO:root:current mean train loss 4067.355305722763
INFO:root:current train perplexity4.970077991485596


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.66s/it]
INFO:root:final mean train loss: 4065.762365771878
INFO:root:final train perplexity: 4.973186492919922
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4102.37528742797
INFO:root:eval perplexity: 5.253423690795898
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/28

 14%|â–ˆâ–        | 28/200 [1:45:06<10:31:53, 220.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4083.876125169837
INFO:root:current train perplexity4.957724094390869
INFO:root:current mean train loss 4053.0608267435214
INFO:root:current train perplexity4.927066802978516
INFO:root:current mean train loss 4060.5785672996076
INFO:root:current train perplexity4.94939661026001
INFO:root:current mean train loss 4050.882211596604
INFO:root:current train perplexity4.939939022064209
INFO:root:current mean train loss 4062.1668704057697
INFO:root:current train perplexity4.9482293128967285
INFO:root:current mean train loss 4063.2058735659657
INFO:root:current train perplexity4.94797420501709
INFO:root:current mean train loss 4055.100926088483
INFO:root:current train perplexity4.941802501678467
INFO:root:current mean train loss 4051.7540271384423
INFO:root:current train perplexity4.939910888671875
INFO:root:current mean train loss 4054.9430449289944
INFO:root:current train perplexity4.9419331550598145
INFO:root:current mean train loss 4053.095589915696
INFO:root:current train perplexity4.94153356552124


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.55s/it]
INFO:root:final mean train loss: 4048.8114496046496
INFO:root:final train perplexity: 4.940039157867432
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.03s/it]
INFO:root:eval mean loss: 4095.584886483267
INFO:root:eval perplexity: 5.23901891708374
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/29

 14%|â–ˆâ–        | 29/200 [1:48:38<10:21:18, 218.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4045.726499495968
INFO:root:current train perplexity4.8791728019714355
INFO:root:current mean train loss 4032.7217803256203
INFO:root:current train perplexity4.885840892791748
INFO:root:current mean train loss 4049.5310100869183
INFO:root:current train perplexity4.902835369110107
INFO:root:current mean train loss 4052.789752141947
INFO:root:current train perplexity4.9050612449646
INFO:root:current mean train loss 4051.4155267772985
INFO:root:current train perplexity4.903754711151123
INFO:root:current mean train loss 4046.837631771569
INFO:root:current train perplexity4.904199600219727
INFO:root:current mean train loss 4041.479843502377
INFO:root:current train perplexity4.900634765625
INFO:root:current mean train loss 4041.583487076244
INFO:root:current train perplexity4.904255390167236
INFO:root:current mean train loss 4039.506397306991
INFO:root:current train perplexity4.9030280113220215
INFO:root:current mean train loss 4034.1471861154
INFO:root:current train perplexity4.904576301574707


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.14s/it]
INFO:root:final mean train loss: 4031.242380326794
INFO:root:final train perplexity: 4.905916213989258
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it]
INFO:root:eval mean loss: 4091.478448096742
INFO:root:eval perplexity: 5.230326175689697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/30

 15%|â–ˆâ–Œ        | 30/200 [1:52:03<10:06:07, 213.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 4018.829990484776
INFO:root:current train perplexity4.901304244995117
INFO:root:current mean train loss 4009.7233869154675
INFO:root:current train perplexity4.878419876098633
INFO:root:current mean train loss 4014.2095880867546
INFO:root:current train perplexity4.8639020919799805
INFO:root:current mean train loss 4010.4164462919434
INFO:root:current train perplexity4.860916614532471
INFO:root:current mean train loss 4017.7731783438926
INFO:root:current train perplexity4.865817546844482
INFO:root:current mean train loss 4015.7008575269597
INFO:root:current train perplexity4.867694854736328
INFO:root:current mean train loss 4015.116196801032
INFO:root:current train perplexity4.869311332702637
INFO:root:current mean train loss 4019.973833344786
INFO:root:current train perplexity4.873611927032471
INFO:root:current mean train loss 4020.5696353662283
INFO:root:current train perplexity4.878304958343506
INFO:root:current mean train loss 4019.163814459365
INFO:root:current train perplexity4.877000331878662


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.81s/it]
INFO:root:final mean train loss: 4015.528165078932
INFO:root:final train perplexity: 4.875594139099121
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it]
INFO:root:eval mean loss: 4080.4787978584886
INFO:root:eval perplexity: 5.207114219665527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/31

 16%|â–ˆâ–Œ        | 31/200 [1:55:26<9:53:19, 210.65s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3990.583942819149
INFO:root:current train perplexity4.819087505340576
INFO:root:current mean train loss 3986.2854651626276
INFO:root:current train perplexity4.809869766235352
INFO:root:current mean train loss 3979.84031819332
INFO:root:current train perplexity4.799294471740723
INFO:root:current mean train loss 3992.682594673091
INFO:root:current train perplexity4.816497802734375
INFO:root:current mean train loss 3996.965480591093
INFO:root:current train perplexity4.8245720863342285
INFO:root:current mean train loss 3999.3458187235774
INFO:root:current train perplexity4.8355183601379395
INFO:root:current mean train loss 4003.057400215538
INFO:root:current train perplexity4.840672969818115
INFO:root:current mean train loss 3998.5642102916877
INFO:root:current train perplexity4.840903282165527
INFO:root:current mean train loss 4000.7683955781436
INFO:root:current train perplexity4.844234466552734
INFO:root:current mean train loss 4001.023424351983
INFO:root:current train perplexity4.8437724113464355


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.30s/it]
INFO:root:final mean train loss: 3999.5613346099854
INFO:root:final train perplexity: 4.844977378845215
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it]
INFO:root:eval mean loss: 4072.561693123892
INFO:root:eval perplexity: 5.1904706954956055
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/32

 16%|â–ˆâ–Œ        | 32/200 [1:58:49<9:43:57, 208.56s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3973.5608664772726
INFO:root:current train perplexity4.734768867492676
INFO:root:current mean train loss 3996.3963268649195
INFO:root:current train perplexity4.798776626586914
INFO:root:current mean train loss 3976.6909380744487
INFO:root:current train perplexity4.789515018463135
INFO:root:current mean train loss 3971.9851314920775
INFO:root:current train perplexity4.799308776855469
INFO:root:current mean train loss 3980.8906834864356
INFO:root:current train perplexity4.80946683883667
INFO:root:current mean train loss 3981.840574412303
INFO:root:current train perplexity4.809691905975342
INFO:root:current mean train loss 3985.632635824189
INFO:root:current train perplexity4.810899257659912
INFO:root:current mean train loss 3987.2671225036215
INFO:root:current train perplexity4.811237335205078
INFO:root:current mean train loss 3988.169975557383
INFO:root:current train perplexity4.812265396118164
INFO:root:current mean train loss 3987.603060833197
INFO:root:current train perplexity4.816062927246094


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.55s/it]
INFO:root:final mean train loss: 3983.5153220392044
INFO:root:final train perplexity: 4.8144025802612305
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it]
INFO:root:eval mean loss: 4068.1533757203015
INFO:root:eval perplexity: 5.18122673034668
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/33

 16%|â–ˆâ–‹        | 33/200 [2:02:14<9:37:24, 207.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3917.312279110863
INFO:root:current train perplexity4.7330851554870605
INFO:root:current mean train loss 3947.3323577693636
INFO:root:current train perplexity4.775857448577881
INFO:root:current mean train loss 3947.3933142600404
INFO:root:current train perplexity4.776994228363037
INFO:root:current mean train loss 3956.9880404721935
INFO:root:current train perplexity4.777819633483887
INFO:root:current mean train loss 3966.236805332917
INFO:root:current train perplexity4.779952049255371
INFO:root:current mean train loss 3963.0359918787467
INFO:root:current train perplexity4.774224281311035
INFO:root:current mean train loss 3965.7734647494817
INFO:root:current train perplexity4.780642986297607
INFO:root:current mean train loss 3964.058122427404
INFO:root:current train perplexity4.777045726776123
INFO:root:current mean train loss 3965.879577565994
INFO:root:current train perplexity4.778290748596191
INFO:root:current mean train loss 3973.4761416553088
INFO:root:current train perplexity4.789481163024902


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.06s/it]
INFO:root:final mean train loss: 3970.429211524225
INFO:root:final train perplexity: 4.789610385894775
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.88s/it]
INFO:root:eval mean loss: 4062.32623594027
INFO:root:eval perplexity: 5.169032573699951
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/34

 17%|â–ˆâ–‹        | 34/200 [2:06:18<10:04:13, 218.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3989.695274675396
INFO:root:current train perplexity4.769530296325684
INFO:root:current mean train loss 3952.8447708219114
INFO:root:current train perplexity4.7444539070129395
INFO:root:current mean train loss 3949.027726627364
INFO:root:current train perplexity4.747715473175049
INFO:root:current mean train loss 3950.249730853058
INFO:root:current train perplexity4.741237163543701
INFO:root:current mean train loss 3949.2267682830748
INFO:root:current train perplexity4.740242004394531
INFO:root:current mean train loss 3953.0386640077713
INFO:root:current train perplexity4.74945068359375
INFO:root:current mean train loss 3955.982468811126
INFO:root:current train perplexity4.754408836364746
INFO:root:current mean train loss 3955.2117627016455
INFO:root:current train perplexity4.7545552253723145
INFO:root:current mean train loss 3958.5051151805574
INFO:root:current train perplexity4.7579026222229
INFO:root:current mean train loss 3957.433547486483
INFO:root:current train perplexity4.759775638580322


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.01s/it]
INFO:root:final mean train loss: 3954.7334740546444
INFO:root:final train perplexity: 4.760043621063232
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it]
INFO:root:eval mean loss: 4057.104938912899
INFO:root:eval perplexity: 5.158130645751953
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/35

 18%|â–ˆâ–Š        | 35/200 [2:09:51<9:55:44, 216.64s/it] 

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3968.6941875988923
INFO:root:current train perplexity4.75759220123291
INFO:root:current mean train loss 3944.0831278369415
INFO:root:current train perplexity4.718713760375977
INFO:root:current mean train loss 3947.8147725204412
INFO:root:current train perplexity4.73081636428833
INFO:root:current mean train loss 3945.3027936386875
INFO:root:current train perplexity4.727018356323242
INFO:root:current mean train loss 3943.4252511743216
INFO:root:current train perplexity4.729506492614746
INFO:root:current mean train loss 3945.7601383716537
INFO:root:current train perplexity4.726963996887207
INFO:root:current mean train loss 3946.702079042595
INFO:root:current train perplexity4.731355667114258
INFO:root:current mean train loss 3944.839574537167
INFO:root:current train perplexity4.731948375701904
INFO:root:current mean train loss 3943.140751375409
INFO:root:current train perplexity4.730767726898193
INFO:root:current mean train loss 3943.5101691178816
INFO:root:current train perplexity4.733788013458252


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.34s/it]
INFO:root:final mean train loss: 3940.451464683779
INFO:root:final train perplexity: 4.733297824859619
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it]
INFO:root:eval mean loss: 4056.121306723737
INFO:root:eval perplexity: 5.156078338623047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/36

 18%|â–ˆâ–Š        | 36/200 [2:13:15<9:42:18, 213.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3895.70513986171
INFO:root:current train perplexity4.683019161224365
INFO:root:current mean train loss 3901.9537307820856
INFO:root:current train perplexity4.673215389251709
INFO:root:current mean train loss 3915.846131009092
INFO:root:current train perplexity4.686473846435547
INFO:root:current mean train loss 3921.6277789133155
INFO:root:current train perplexity4.696443557739258
INFO:root:current mean train loss 3918.466647984311
INFO:root:current train perplexity4.700572490692139
INFO:root:current mean train loss 3919.7014251656997
INFO:root:current train perplexity4.703286170959473
INFO:root:current mean train loss 3924.353211781864
INFO:root:current train perplexity4.705049991607666
INFO:root:current mean train loss 3927.212247235348
INFO:root:current train perplexity4.707237720489502
INFO:root:current mean train loss 3929.1056366482876
INFO:root:current train perplexity4.706737995147705
INFO:root:current mean train loss 3931.031421912598
INFO:root:current train perplexity4.71070671081543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.57s/it]
INFO:root:final mean train loss: 3928.0849444481632
INFO:root:final train perplexity: 4.71026086807251
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it]
INFO:root:eval mean loss: 4048.172858488475
INFO:root:eval perplexity: 5.139533519744873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/37

 18%|â–ˆâ–Š        | 37/200 [2:16:39<9:31:29, 210.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3888.897193667763
INFO:root:current train perplexity4.637213706970215
INFO:root:current mean train loss 3889.323785556891
INFO:root:current train perplexity4.6439738273620605
INFO:root:current mean train loss 3898.896696239407
INFO:root:current train perplexity4.660858154296875
INFO:root:current mean train loss 3891.6204596024527
INFO:root:current train perplexity4.661098957061768
INFO:root:current mean train loss 3898.8990584556504
INFO:root:current train perplexity4.6676740646362305
INFO:root:current mean train loss 3903.629827419249
INFO:root:current train perplexity4.6718645095825195
INFO:root:current mean train loss 3905.0616203911873
INFO:root:current train perplexity4.673242092132568
INFO:root:current mean train loss 3911.5609940055033
INFO:root:current train perplexity4.67989444732666
INFO:root:current mean train loss 3917.448324295129
INFO:root:current train perplexity4.684619903564453


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.98s/it]
INFO:root:final mean train loss: 3914.1698408434468
INFO:root:final train perplexity: 4.68447208404541
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.63s/it]
INFO:root:eval mean loss: 4046.466239333998
INFO:root:eval perplexity: 5.135987758636475
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/38

 19%|â–ˆâ–‰        | 38/200 [2:20:09<9:27:43, 210.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3827.9949544270835
INFO:root:current train perplexity4.485195159912109
INFO:root:current mean train loss 3900.7558830779735
INFO:root:current train perplexity4.639218330383301
INFO:root:current mean train loss 3894.21106618496
INFO:root:current train perplexity4.637578964233398
INFO:root:current mean train loss 3897.5812529006807
INFO:root:current train perplexity4.63923978805542
INFO:root:current mean train loss 3893.5787205092665
INFO:root:current train perplexity4.636709690093994
INFO:root:current mean train loss 3896.4245518102325
INFO:root:current train perplexity4.637444972991943
INFO:root:current mean train loss 3901.5309410791097
INFO:root:current train perplexity4.646089553833008
INFO:root:current mean train loss 3899.3602289712617
INFO:root:current train perplexity4.6503214836120605
INFO:root:current mean train loss 3901.596182893252
INFO:root:current train perplexity4.651853561401367
INFO:root:current mean train loss 3904.218179797809
INFO:root:current train perplexity4.656044006347656


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.67s/it]
INFO:root:final mean train loss: 3900.956069269488
INFO:root:final train perplexity: 4.660114765167236
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it]
INFO:root:eval mean loss: 4040.800647924978
INFO:root:eval perplexity: 5.124234199523926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/39

 20%|â–ˆâ–‰        | 39/200 [2:23:34<9:19:24, 208.48s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3837.012295809659
INFO:root:current train perplexity4.560708522796631
INFO:root:current mean train loss 3871.144797385276
INFO:root:current train perplexity4.603906631469727
INFO:root:current mean train loss 3881.3176257960604
INFO:root:current train perplexity4.611859321594238
INFO:root:current mean train loss 3881.2416670330085
INFO:root:current train perplexity4.625725269317627
INFO:root:current mean train loss 3884.791887640663
INFO:root:current train perplexity4.634095668792725
INFO:root:current mean train loss 3888.946066899309
INFO:root:current train perplexity4.635525226593018
INFO:root:current mean train loss 3886.8033824864465
INFO:root:current train perplexity4.630555152893066
INFO:root:current mean train loss 3888.449706001121
INFO:root:current train perplexity4.634432315826416
INFO:root:current mean train loss 3889.9762018582383
INFO:root:current train perplexity4.6352105140686035
INFO:root:current mean train loss 3891.5097546373318
INFO:root:current train perplexity4.6376118659973145


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it]
INFO:root:final mean train loss: 3889.0668141764977
INFO:root:final train perplexity: 4.638306617736816
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4037.2255461131426
INFO:root:eval perplexity: 5.116832256317139
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/40

 20%|â–ˆâ–ˆ        | 40/200 [2:26:58<9:12:26, 207.17s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3883.977115028783
INFO:root:current train perplexity4.623661994934082
INFO:root:current mean train loss 3873.45031430541
INFO:root:current train perplexity4.613079071044922
INFO:root:current mean train loss 3870.4781098387557
INFO:root:current train perplexity4.608464241027832
INFO:root:current mean train loss 3879.754352438039
INFO:root:current train perplexity4.61097526550293
INFO:root:current mean train loss 3879.852068261486
INFO:root:current train perplexity4.612588882446289
INFO:root:current mean train loss 3882.799928404233
INFO:root:current train perplexity4.615076541900635
INFO:root:current mean train loss 3881.8882814077647
INFO:root:current train perplexity4.614421367645264
INFO:root:current mean train loss 3886.4244685272515
INFO:root:current train perplexity4.621385097503662
INFO:root:current mean train loss 3883.7915888636103
INFO:root:current train perplexity4.61909818649292
INFO:root:current mean train loss 3880.473945227489
INFO:root:current train perplexity4.617981910705566


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.19s/it]
INFO:root:final mean train loss: 3877.283947790823
INFO:root:final train perplexity: 4.616794586181641
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4036.3476770279253
INFO:root:eval perplexity: 5.115015506744385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/41

 20%|â–ˆâ–ˆ        | 41/200 [2:30:23<9:07:36, 206.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3823.6545500578704
INFO:root:current train perplexity4.5737786293029785
INFO:root:current mean train loss 3846.4700668214814
INFO:root:current train perplexity4.564535140991211
INFO:root:current mean train loss 3852.084925445691
INFO:root:current train perplexity4.576960563659668
INFO:root:current mean train loss 3853.090040854358
INFO:root:current train perplexity4.5709123611450195
INFO:root:current mean train loss 3854.1268439192404
INFO:root:current train perplexity4.576874256134033
INFO:root:current mean train loss 3855.6176697588057
INFO:root:current train perplexity4.5745463371276855
INFO:root:current mean train loss 3858.026946972812
INFO:root:current train perplexity4.574156761169434
INFO:root:current mean train loss 3863.112807744799
INFO:root:current train perplexity4.582555294036865
INFO:root:current mean train loss 3868.7266926886523
INFO:root:current train perplexity4.59126091003418
INFO:root:current mean train loss 3868.957456850054
INFO:root:current train perplexity4.593072891235352


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.87s/it]
INFO:root:final mean train loss: 3864.7618115948094
INFO:root:final train perplexity: 4.594043254852295
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it]
INFO:root:eval mean loss: 4031.047574523493
INFO:root:eval perplexity: 5.104065895080566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/42

 21%|â–ˆâ–ˆ        | 42/200 [2:33:47<9:02:15, 205.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3795.390234375
INFO:root:current train perplexity4.538547515869141
INFO:root:current mean train loss 3817.7732982494213
INFO:root:current train perplexity4.5477519035339355
INFO:root:current mean train loss 3834.690797456782
INFO:root:current train perplexity4.549630641937256
INFO:root:current mean train loss 3846.301123046875
INFO:root:current train perplexity4.562098503112793
INFO:root:current mean train loss 3850.3878283270474
INFO:root:current train perplexity4.564194679260254
INFO:root:current mean train loss 3853.2911005037968
INFO:root:current train perplexity4.567342758178711
INFO:root:current mean train loss 3852.803519469734
INFO:root:current train perplexity4.569418430328369
INFO:root:current mean train loss 3857.186521444515
INFO:root:current train perplexity4.57145357131958
INFO:root:current mean train loss 3854.348650940307
INFO:root:current train perplexity4.569089412689209
INFO:root:current mean train loss 3854.2615310097763
INFO:root:current train perplexity4.5694661140441895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.99s/it]
INFO:root:final mean train loss: 3853.206251636628
INFO:root:final train perplexity: 4.573146820068359
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it]
INFO:root:eval mean loss: 4025.8240525265956
INFO:root:eval perplexity: 5.093295574188232
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/43

 22%|â–ˆâ–ˆâ–       | 43/200 [2:37:15<9:00:06, 206.41s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3880.578005768532
INFO:root:current train perplexity4.554108142852783
INFO:root:current mean train loss 3859.504911836211
INFO:root:current train perplexity4.560339450836182
INFO:root:current mean train loss 3851.630374107832
INFO:root:current train perplexity4.544254302978516
INFO:root:current mean train loss 3849.199371782753
INFO:root:current train perplexity4.545989990234375
INFO:root:current mean train loss 3848.5843315727284
INFO:root:current train perplexity4.543046951293945
INFO:root:current mean train loss 3847.945216282516
INFO:root:current train perplexity4.545759201049805
INFO:root:current mean train loss 3846.62375119982
INFO:root:current train perplexity4.54582405090332
INFO:root:current mean train loss 3844.0330506603295
INFO:root:current train perplexity4.547321796417236
INFO:root:current mean train loss 3845.4355740982724
INFO:root:current train perplexity4.552249431610107
INFO:root:current mean train loss 3844.9159710945783
INFO:root:current train perplexity4.552748203277588


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.06s/it]
INFO:root:final mean train loss: 3841.8921337127686
INFO:root:final train perplexity: 4.5527777671813965
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.02s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.02s/it]
INFO:root:eval mean loss: 4027.8630388408687
INFO:root:eval perplexity: 5.09749698638916
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/44

 22%|â–ˆâ–ˆâ–       | 44/200 [2:40:42<8:57:12, 206.62s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3779.0502786075367
INFO:root:current train perplexity4.477962493896484
INFO:root:current mean train loss 3816.867690332678
INFO:root:current train perplexity4.498000621795654
INFO:root:current mean train loss 3822.391838894422
INFO:root:current train perplexity4.513187408447266
INFO:root:current mean train loss 3820.640093594195
INFO:root:current train perplexity4.511093616485596
INFO:root:current mean train loss 3827.5754286264896
INFO:root:current train perplexity4.512373924255371
INFO:root:current mean train loss 3830.9949390667534
INFO:root:current train perplexity4.52161169052124
INFO:root:current mean train loss 3827.5183863017232
INFO:root:current train perplexity4.5215020179748535
INFO:root:current mean train loss 3831.0067107788573
INFO:root:current train perplexity4.526762962341309
INFO:root:current mean train loss 3829.8000901398173
INFO:root:current train perplexity4.528162956237793
INFO:root:current mean train loss 3832.755946659766
INFO:root:current train perplexity4.530506134033203


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.98s/it]
INFO:root:final mean train loss: 3831.1786560550813
INFO:root:final train perplexity: 4.533575057983398
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it]
INFO:root:eval mean loss: 4021.319611245013
INFO:root:eval perplexity: 5.08402681350708
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/45

 22%|â–ˆâ–ˆâ–Ž       | 45/200 [2:44:17<9:00:01, 209.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3815.501750364142
INFO:root:current train perplexity4.48049783706665
INFO:root:current mean train loss 3825.307621793927
INFO:root:current train perplexity4.513460159301758
INFO:root:current mean train loss 3819.8567554370775
INFO:root:current train perplexity4.511423587799072
INFO:root:current mean train loss 3816.913491251741
INFO:root:current train perplexity4.50747537612915
INFO:root:current mean train loss 3817.4707052525873
INFO:root:current train perplexity4.509944915771484
INFO:root:current mean train loss 3818.805760146467
INFO:root:current train perplexity4.510402679443359
INFO:root:current mean train loss 3823.5379936901318
INFO:root:current train perplexity4.512548446655273
INFO:root:current mean train loss 3826.0998786051755
INFO:root:current train perplexity4.510465621948242
INFO:root:current mean train loss 3823.8125105159525
INFO:root:current train perplexity4.512495517730713
INFO:root:current mean train loss 3823.3992895736933
INFO:root:current train perplexity4.513303279876709


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.08s/it]
INFO:root:final mean train loss: 3819.7822101962183
INFO:root:final train perplexity: 4.513236999511719
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it]
INFO:root:eval mean loss: 4019.803047775377
INFO:root:eval perplexity: 5.0809102058410645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/46

 23%|â–ˆâ–ˆâ–Ž       | 46/200 [2:47:41<8:53:12, 207.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3831.9305438141323
INFO:root:current train perplexity4.503833293914795
INFO:root:current mean train loss 3814.717739813342
INFO:root:current train perplexity4.472313404083252
INFO:root:current mean train loss 3807.972985428371
INFO:root:current train perplexity4.477055072784424
INFO:root:current mean train loss 3814.341635888539
INFO:root:current train perplexity4.486992359161377
INFO:root:current mean train loss 3818.6004541956636
INFO:root:current train perplexity4.489682197570801
INFO:root:current mean train loss 3816.057708040537
INFO:root:current train perplexity4.488958358764648
INFO:root:current mean train loss 3817.205816403322
INFO:root:current train perplexity4.495347499847412
INFO:root:current mean train loss 3813.6160151793715
INFO:root:current train perplexity4.492832660675049
INFO:root:current mean train loss 3812.5223620985616
INFO:root:current train perplexity4.492257595062256
INFO:root:current mean train loss 3811.7587375581697
INFO:root:current train perplexity4.492514133453369


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.47s/it]
INFO:root:final mean train loss: 3808.1177543517083
INFO:root:final train perplexity: 4.492514610290527
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it]
INFO:root:eval mean loss: 4020.756581407912
INFO:root:eval perplexity: 5.082869529724121
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/47

 24%|â–ˆâ–ˆâ–Ž       | 47/200 [2:52:06<9:33:25, 224.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3796.7945279947917
INFO:root:current train perplexity4.419699192047119
INFO:root:current mean train loss 3789.4565694754465
INFO:root:current train perplexity4.456361293792725
INFO:root:current mean train loss 3789.935695134943
INFO:root:current train perplexity4.455509185791016
INFO:root:current mean train loss 3797.0710084635416
INFO:root:current train perplexity4.465573310852051
INFO:root:current mean train loss 3797.1996155427632
INFO:root:current train perplexity4.468143463134766
INFO:root:current mean train loss 3798.816979025136
INFO:root:current train perplexity4.469173908233643
INFO:root:current mean train loss 3798.1683814380785
INFO:root:current train perplexity4.470843315124512
INFO:root:current mean train loss 3797.907123235887
INFO:root:current train perplexity4.473927021026611
INFO:root:current mean train loss 3799.2550005580356
INFO:root:current train perplexity4.475768089294434
INFO:root:current mean train loss 3800.200879657452
INFO:root:current train perplexity4.474358558654785


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.91s/it]
INFO:root:final mean train loss: 3797.483810978551
INFO:root:final train perplexity: 4.473706245422363
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it]
INFO:root:eval mean loss: 4017.351493240248
INFO:root:eval perplexity: 5.075875282287598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/48

 24%|â–ˆâ–ˆâ–       | 48/200 [2:56:15<9:47:37, 231.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3790.549357586596
INFO:root:current train perplexity4.429951190948486
INFO:root:current mean train loss 3796.0802288785007
INFO:root:current train perplexity4.434529781341553
INFO:root:current mean train loss 3784.120646015073
INFO:root:current train perplexity4.431162357330322
INFO:root:current mean train loss 3781.819441753019
INFO:root:current train perplexity4.44146203994751
INFO:root:current mean train loss 3782.8553711948434
INFO:root:current train perplexity4.4466705322265625
INFO:root:current mean train loss 3786.695396671982
INFO:root:current train perplexity4.441083908081055
INFO:root:current mean train loss 3788.2390165315014
INFO:root:current train perplexity4.445353031158447
INFO:root:current mean train loss 3787.250113183968
INFO:root:current train perplexity4.448716640472412
INFO:root:current mean train loss 3788.1361314853484
INFO:root:current train perplexity4.451025485992432
INFO:root:current mean train loss 3791.2037287699486
INFO:root:current train perplexity4.458519458770752


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.97s/it]
INFO:root:final mean train loss: 3788.7745622204197
INFO:root:final train perplexity: 4.45836067199707
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it]
INFO:root:eval mean loss: 4013.8243001302085
INFO:root:eval perplexity: 5.068641185760498
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/49

 24%|â–ˆâ–ˆâ–       | 49/200 [2:59:41<9:23:57, 224.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3775.3169991629466
INFO:root:current train perplexity4.382053852081299
INFO:root:current mean train loss 3776.4362358372873
INFO:root:current train perplexity4.409028053283691
INFO:root:current mean train loss 3768.578814634343
INFO:root:current train perplexity4.41818380355835
INFO:root:current mean train loss 3773.199805686541
INFO:root:current train perplexity4.425104141235352
INFO:root:current mean train loss 3773.363414010788
INFO:root:current train perplexity4.42719030380249
INFO:root:current mean train loss 3776.4121965385734
INFO:root:current train perplexity4.431007385253906
INFO:root:current mean train loss 3775.229292211243
INFO:root:current train perplexity4.430533409118652
INFO:root:current mean train loss 3778.027604557621
INFO:root:current train perplexity4.433621406555176
INFO:root:current mean train loss 3778.0659755103115
INFO:root:current train perplexity4.436144828796387
INFO:root:current mean train loss 3782.806563514994
INFO:root:current train perplexity4.442730903625488


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.54s/it]
INFO:root:final mean train loss: 3779.930849690591
INFO:root:final train perplexity: 4.4428324699401855
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.09s/it]
INFO:root:eval mean loss: 4012.315010666002
INFO:root:eval perplexity: 5.065547943115234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/50

 25%|â–ˆâ–ˆâ–Œ       | 50/200 [3:03:07<9:07:03, 218.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3758.3645241477275
INFO:root:current train perplexity4.378906726837158
INFO:root:current mean train loss 3762.635694340845
INFO:root:current train perplexity4.39354944229126
INFO:root:current mean train loss 3765.665375470318
INFO:root:current train perplexity4.402526378631592
INFO:root:current mean train loss 3767.589696286615
INFO:root:current train perplexity4.404952526092529
INFO:root:current mean train loss 3771.7815817181236
INFO:root:current train perplexity4.41680383682251
INFO:root:current mean train loss 3770.762582005165
INFO:root:current train perplexity4.421806335449219
INFO:root:current mean train loss 3766.0915558778165
INFO:root:current train perplexity4.421144962310791
INFO:root:current mean train loss 3765.870003520025
INFO:root:current train perplexity4.420758247375488
INFO:root:current mean train loss 3766.88202712215
INFO:root:current train perplexity4.420872688293457


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:14<00:00, 194.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:14<00:00, 194.36s/it]
INFO:root:final mean train loss: 3768.700002977925
INFO:root:final train perplexity: 4.423189640045166
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.81s/it]
INFO:root:eval mean loss: 4014.747866799645
INFO:root:eval perplexity: 5.070533752441406
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/51

 26%|â–ˆâ–ˆâ–Œ       | 51/200 [3:06:35<8:55:22, 215.59s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3712.1132463727677
INFO:root:current train perplexity4.381977558135986
INFO:root:current mean train loss 3756.3685451044103
INFO:root:current train perplexity4.383734703063965
INFO:root:current mean train loss 3755.966163524683
INFO:root:current train perplexity4.393016815185547
INFO:root:current mean train loss 3765.7852143029827
INFO:root:current train perplexity4.394503116607666
INFO:root:current mean train loss 3767.088780208653
INFO:root:current train perplexity4.401137828826904
INFO:root:current mean train loss 3763.924717047276
INFO:root:current train perplexity4.399610996246338
INFO:root:current mean train loss 3760.006813011609
INFO:root:current train perplexity4.398941993713379
INFO:root:current mean train loss 3756.975635663455
INFO:root:current train perplexity4.400851249694824
INFO:root:current mean train loss 3760.298890748432
INFO:root:current train perplexity4.404726028442383
INFO:root:current mean train loss 3761.201582095852
INFO:root:current train perplexity4.404610633850098


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:19<00:00, 199.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:19<00:00, 199.08s/it]
INFO:root:final mean train loss: 3758.787286943005
INFO:root:final train perplexity: 4.405925273895264
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.46s/it]
INFO:root:eval mean loss: 4009.7722375748003
INFO:root:eval perplexity: 5.060342788696289
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/52

 26%|â–ˆâ–ˆâ–Œ       | 52/200 [3:10:09<8:50:15, 214.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3762.915380859375
INFO:root:current train perplexity4.4542236328125
INFO:root:current mean train loss 3730.814905315897
INFO:root:current train perplexity4.363229751586914
INFO:root:current mean train loss 3734.164323673692
INFO:root:current train perplexity4.367886543273926
INFO:root:current mean train loss 3733.9731181795637
INFO:root:current train perplexity4.372732162475586
INFO:root:current mean train loss 3739.641167403991
INFO:root:current train perplexity4.372299671173096
INFO:root:current mean train loss 3742.0279808859223
INFO:root:current train perplexity4.374966144561768
INFO:root:current mean train loss 3740.5837311039127
INFO:root:current train perplexity4.380830764770508
INFO:root:current mean train loss 3744.76647932146
INFO:root:current train perplexity4.384698390960693
INFO:root:current mean train loss 3750.6507228359856
INFO:root:current train perplexity4.387159824371338
INFO:root:current mean train loss 3751.1702711428447
INFO:root:current train perplexity4.387388706207275


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:22<00:00, 202.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:22<00:00, 202.15s/it]
INFO:root:final mean train loss: 3749.1946326840307
INFO:root:final train perplexity: 4.389282703399658
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.32s/it]
INFO:root:eval mean loss: 4008.913298911237
INFO:root:eval perplexity: 5.058583736419678
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/53

 26%|â–ˆâ–ˆâ–‹       | 53/200 [3:13:45<8:47:47, 215.43s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3740.52979577106
INFO:root:current train perplexity4.423487186431885
INFO:root:current mean train loss 3743.9731088033536
INFO:root:current train perplexity4.3829121589660645
INFO:root:current mean train loss 3732.25207245831
INFO:root:current train perplexity4.367884159088135
INFO:root:current mean train loss 3741.173042037539
INFO:root:current train perplexity4.372038841247559
INFO:root:current mean train loss 3735.6388352125814
INFO:root:current train perplexity4.369207859039307
INFO:root:current mean train loss 3742.2329437664316
INFO:root:current train perplexity4.37627649307251
INFO:root:current mean train loss 3745.1375326827097
INFO:root:current train perplexity4.376904487609863
INFO:root:current mean train loss 3740.5929844857583
INFO:root:current train perplexity4.3707733154296875
INFO:root:current mean train loss 3742.8210054678007
INFO:root:current train perplexity4.373533248901367
INFO:root:current mean train loss 3744.4088540784974
INFO:root:current train perplexity4.37404727935791


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:16<00:00, 196.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:16<00:00, 196.56s/it]
INFO:root:final mean train loss: 3738.9233482114732
INFO:root:final train perplexity: 4.371531963348389
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.85s/it]
INFO:root:eval mean loss: 4010.129155585106
INFO:root:eval perplexity: 5.061073303222656
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/54

 27%|â–ˆâ–ˆâ–‹       | 54/200 [3:17:15<8:40:25, 213.88s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3715.4647571194555
INFO:root:current train perplexity4.3677144050598145
INFO:root:current mean train loss 3718.3578262911496
INFO:root:current train perplexity4.335967540740967
INFO:root:current mean train loss 3732.3353287337663
INFO:root:current train perplexity4.354801654815674
INFO:root:current mean train loss 3723.5303641604514
INFO:root:current train perplexity4.3462982177734375
INFO:root:current mean train loss 3727.1321956342445
INFO:root:current train perplexity4.353416442871094
INFO:root:current mean train loss 3729.309676060793
INFO:root:current train perplexity4.358384609222412
INFO:root:current mean train loss 3728.1321280550465
INFO:root:current train perplexity4.360612869262695
INFO:root:current mean train loss 3729.775152162064
INFO:root:current train perplexity4.360653877258301
INFO:root:current mean train loss 3731.193791542099
INFO:root:current train perplexity4.36067533493042
INFO:root:current mean train loss 3733.298426381243
INFO:root:current train perplexity4.359869003295898


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it]
INFO:root:final mean train loss: 3732.0878182688066
INFO:root:final train perplexity: 4.359758377075195
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4009.0156440464316
INFO:root:eval perplexity: 5.058793067932129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/55

 28%|â–ˆâ–ˆâ–Š       | 55/200 [3:20:38<8:28:38, 210.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3682.055657802484
INFO:root:current train perplexity4.294699192047119
INFO:root:current mean train loss 3720.479272636578
INFO:root:current train perplexity4.322953224182129
INFO:root:current mean train loss 3721.071864172006
INFO:root:current train perplexity4.324782371520996
INFO:root:current mean train loss 3716.7824058870297
INFO:root:current train perplexity4.319591522216797
INFO:root:current mean train loss 3713.5083180212487
INFO:root:current train perplexity4.320135593414307
INFO:root:current mean train loss 3711.0896978997566
INFO:root:current train perplexity4.325325012207031
INFO:root:current mean train loss 3712.8755956419946
INFO:root:current train perplexity4.32852840423584
INFO:root:current mean train loss 3716.2684863016957
INFO:root:current train perplexity4.334866046905518
INFO:root:current mean train loss 3719.6991570601162
INFO:root:current train perplexity4.3382248878479
INFO:root:current mean train loss 3723.7251418563633
INFO:root:current train perplexity4.341268539428711


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.36s/it]
INFO:root:final mean train loss: 3721.4026822736187
INFO:root:final train perplexity: 4.341418266296387
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4005.514956643395
INFO:root:eval perplexity: 5.051638603210449
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/56

 28%|â–ˆâ–ˆâ–Š       | 56/200 [3:24:02<8:20:12, 208.42s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3717.8583205202794
INFO:root:current train perplexity4.327331066131592
INFO:root:current mean train loss 3698.6199261931333
INFO:root:current train perplexity4.301849842071533
INFO:root:current mean train loss 3705.47900390625
INFO:root:current train perplexity4.305967807769775
INFO:root:current mean train loss 3707.564341256529
INFO:root:current train perplexity4.308332920074463
INFO:root:current mean train loss 3706.715544493673
INFO:root:current train perplexity4.3150858879089355
INFO:root:current mean train loss 3710.4839710744686
INFO:root:current train perplexity4.318719863891602
INFO:root:current mean train loss 3715.812806779487
INFO:root:current train perplexity4.323352813720703
INFO:root:current mean train loss 3716.0671961936287
INFO:root:current train perplexity4.325268745422363
INFO:root:current mean train loss 3717.15198978933
INFO:root:current train perplexity4.326465606689453
INFO:root:current mean train loss 3717.3524944623646
INFO:root:current train perplexity4.327136516571045


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.59s/it]
INFO:root:final mean train loss: 3713.069835724369
INFO:root:final train perplexity: 4.327169418334961
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it]
INFO:root:eval mean loss: 4006.328968237478
INFO:root:eval perplexity: 5.053301811218262
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/57

 28%|â–ˆâ–ˆâ–Š       | 57/200 [3:27:26<8:13:32, 207.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3666.115265447443
INFO:root:current train perplexity4.242366790771484
INFO:root:current mean train loss 3681.9425009450606
INFO:root:current train perplexity4.278951644897461
INFO:root:current mean train loss 3690.3720722273283
INFO:root:current train perplexity4.281991004943848
INFO:root:current mean train loss 3689.2039970290493
INFO:root:current train perplexity4.283257961273193
INFO:root:current mean train loss 3700.2384883671016
INFO:root:current train perplexity4.295688629150391
INFO:root:current mean train loss 3701.0776921452702
INFO:root:current train perplexity4.3011155128479
INFO:root:current mean train loss 3701.8118521886927
INFO:root:current train perplexity4.305480480194092
INFO:root:current mean train loss 3703.25881266815
INFO:root:current train perplexity4.311678886413574
INFO:root:current mean train loss 3704.760145113761
INFO:root:current train perplexity4.311137676239014
INFO:root:current mean train loss 3707.4338856961713
INFO:root:current train perplexity4.31310510635376


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.28s/it]
INFO:root:final mean train loss: 3705.452647424513
INFO:root:final train perplexity: 4.314184188842773
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it]
INFO:root:eval mean loss: 4005.5110832917776
INFO:root:eval perplexity: 5.05163049697876
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/58

 29%|â–ˆâ–ˆâ–‰       | 58/200 [3:30:49<8:07:34, 206.02s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3689.764718191964
INFO:root:current train perplexity4.292069911956787
INFO:root:current mean train loss 3669.7106244607935
INFO:root:current train perplexity4.249011516571045
INFO:root:current mean train loss 3680.0903366727066
INFO:root:current train perplexity4.259499549865723
INFO:root:current mean train loss 3674.7417732007575
INFO:root:current train perplexity4.264503002166748
INFO:root:current mean train loss 3681.7052062592807
INFO:root:current train perplexity4.274113655090332
INFO:root:current mean train loss 3684.778537133659
INFO:root:current train perplexity4.276900768280029
INFO:root:current mean train loss 3687.297324616445
INFO:root:current train perplexity4.280514240264893
INFO:root:current mean train loss 3689.4068422729974
INFO:root:current train perplexity4.287262916564941
INFO:root:current mean train loss 3691.675127756554
INFO:root:current train perplexity4.291928768157959
INFO:root:current mean train loss 3696.468204676548
INFO:root:current train perplexity4.295859336853027


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it]
INFO:root:final mean train loss: 3695.1840189656905
INFO:root:final train perplexity: 4.296741962432861
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it]
INFO:root:eval mean loss: 4004.4711948692375
INFO:root:eval perplexity: 5.049506664276123
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/59

 30%|â–ˆâ–ˆâ–‰       | 59/200 [3:34:13<8:02:36, 205.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3684.9545279489435
INFO:root:current train perplexity4.264543533325195
INFO:root:current mean train loss 3670.792170652869
INFO:root:current train perplexity4.246885776519775
INFO:root:current mean train loss 3685.8018686217138
INFO:root:current train perplexity4.26398229598999
INFO:root:current mean train loss 3688.430125768615
INFO:root:current train perplexity4.272876262664795
INFO:root:current mean train loss 3686.289977379412
INFO:root:current train perplexity4.279156684875488
INFO:root:current mean train loss 3687.752136123577
INFO:root:current train perplexity4.282705307006836
INFO:root:current mean train loss 3688.2236477301835
INFO:root:current train perplexity4.2837300300598145
INFO:root:current mean train loss 3690.2620471207238
INFO:root:current train perplexity4.28300666809082
INFO:root:current mean train loss 3690.461850434576
INFO:root:current train perplexity4.284058570861816
INFO:root:current mean train loss 3690.4887474052202
INFO:root:current train perplexity4.285058975219727


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.71s/it]
INFO:root:final mean train loss: 3688.8959153083065
INFO:root:final train perplexity: 4.286095142364502
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.69s/it]
INFO:root:eval mean loss: 4004.094631330341
INFO:root:eval perplexity: 5.0487380027771
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/60

 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:37:36<7:57:41, 204.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3675.7571975128562
INFO:root:current train perplexity4.269093990325928
INFO:root:current mean train loss 3683.106060688722
INFO:root:current train perplexity4.268043518066406
INFO:root:current mean train loss 3680.1052867383514
INFO:root:current train perplexity4.270813465118408
INFO:root:current mean train loss 3670.005176554255
INFO:root:current train perplexity4.263336181640625
INFO:root:current mean train loss 3671.2182928097272
INFO:root:current train perplexity4.260603427886963
INFO:root:current mean train loss 3674.932369673602
INFO:root:current train perplexity4.265675067901611
INFO:root:current mean train loss 3673.020141781342
INFO:root:current train perplexity4.261636734008789
INFO:root:current mean train loss 3676.4927115091464
INFO:root:current train perplexity4.265597820281982
INFO:root:current mean train loss 3678.1259429549737
INFO:root:current train perplexity4.265365123748779
INFO:root:current mean train loss 3682.539798413161
INFO:root:current train perplexity4.269708156585693


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.91s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.91s/it]
INFO:root:final mean train loss: 3679.3348098877937
INFO:root:final train perplexity: 4.269958019256592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.72s/it]
INFO:root:eval mean loss: 4005.688753601507
INFO:root:eval perplexity: 5.051993370056152
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/61

 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [3:41:04<7:56:12, 205.55s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3658.964593996947
INFO:root:current train perplexity4.215217113494873
INFO:root:current mean train loss 3667.9066769197025
INFO:root:current train perplexity4.243625164031982
INFO:root:current mean train loss 3672.968522872659
INFO:root:current train perplexity4.250274181365967
INFO:root:current mean train loss 3674.314827852471
INFO:root:current train perplexity4.251854419708252
INFO:root:current mean train loss 3675.339280271432
INFO:root:current train perplexity4.255160331726074
INFO:root:current mean train loss 3675.131247837255
INFO:root:current train perplexity4.252957820892334
INFO:root:current mean train loss 3674.266377322712
INFO:root:current train perplexity4.253072261810303
INFO:root:current mean train loss 3672.6307191570045
INFO:root:current train perplexity4.254708766937256
INFO:root:current mean train loss 3671.4372577860768
INFO:root:current train perplexity4.253282070159912
INFO:root:current mean train loss 3673.029338183495
INFO:root:current train perplexity4.254524230957031


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.20s/it]
INFO:root:final mean train loss: 3670.124201989943
INFO:root:final train perplexity: 4.254469871520996
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it]
INFO:root:eval mean loss: 4002.8245338818706
INFO:root:eval perplexity: 5.046145439147949
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/62

 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [3:44:26<7:50:34, 204.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3635.042259457237
INFO:root:current train perplexity4.202186107635498
INFO:root:current mean train loss 3651.28821489383
INFO:root:current train perplexity4.224563121795654
INFO:root:current mean train loss 3655.8298646054027
INFO:root:current train perplexity4.227308750152588
INFO:root:current mean train loss 3658.2586141465586
INFO:root:current train perplexity4.2298383712768555
INFO:root:current mean train loss 3662.926657690183
INFO:root:current train perplexity4.231201648712158
INFO:root:current mean train loss 3663.9732631138395
INFO:root:current train perplexity4.236909866333008
INFO:root:current mean train loss 3664.024808902878
INFO:root:current train perplexity4.236999034881592
INFO:root:current mean train loss 3665.8141171629322
INFO:root:current train perplexity4.24296760559082
INFO:root:current mean train loss 3665.633616390974
INFO:root:current train perplexity4.242202281951904


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.15s/it]
INFO:root:final mean train loss: 3662.5805528702276
INFO:root:final train perplexity: 4.241827011108398
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.30s/it]
INFO:root:eval mean loss: 4002.9282763325577
INFO:root:eval perplexity: 5.046357154846191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/63

 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [3:47:48<7:45:35, 203.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3684.231689453125
INFO:root:current train perplexity4.302093029022217
INFO:root:current mean train loss 3659.7902500189625
INFO:root:current train perplexity4.220873832702637
INFO:root:current mean train loss 3646.2530667911024
INFO:root:current train perplexity4.209844589233398
INFO:root:current mean train loss 3646.2094533183786
INFO:root:current train perplexity4.218504905700684
INFO:root:current mean train loss 3654.7867574005504
INFO:root:current train perplexity4.2244954109191895
INFO:root:current mean train loss 3658.662646678523
INFO:root:current train perplexity4.2214436531066895
INFO:root:current mean train loss 3658.6966319930298
INFO:root:current train perplexity4.220752716064453
INFO:root:current mean train loss 3655.30810720517
INFO:root:current train perplexity4.2183098793029785
INFO:root:current mean train loss 3656.701596308764
INFO:root:current train perplexity4.2214860916137695
INFO:root:current mean train loss 3656.970169151872
INFO:root:current train perplexity4.222942352294922


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.89s/it]
INFO:root:final mean train loss: 3653.6357206529187
INFO:root:final train perplexity: 4.226883411407471
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.82s/it]
INFO:root:eval mean loss: 4002.9339036873894
INFO:root:eval perplexity: 5.0463690757751465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/64

 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [3:51:12<7:41:56, 203.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3619.8000266335225
INFO:root:current train perplexity4.179046154022217
INFO:root:current mean train loss 3659.637792088964
INFO:root:current train perplexity4.209251403808594
INFO:root:current mean train loss 3636.0986455402103
INFO:root:current train perplexity4.199215412139893
INFO:root:current mean train loss 3625.942815357466
INFO:root:current train perplexity4.194097995758057
INFO:root:current mean train loss 3632.7775944248024
INFO:root:current train perplexity4.195623874664307
INFO:root:current mean train loss 3640.011682917227
INFO:root:current train perplexity4.202554702758789
INFO:root:current mean train loss 3643.798995147555
INFO:root:current train perplexity4.207403659820557
INFO:root:current mean train loss 3646.5496683670667
INFO:root:current train perplexity4.212081432342529
INFO:root:current mean train loss 3650.63114385452
INFO:root:current train perplexity4.215516567230225
INFO:root:current mean train loss 3649.0339090156763
INFO:root:current train perplexity4.214841365814209


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.78s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.78s/it]
INFO:root:final mean train loss: 3647.1630121046496
INFO:root:final train perplexity: 4.216103553771973
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it]
INFO:root:eval mean loss: 3999.7537833139404
INFO:root:eval perplexity: 5.039884090423584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/65
################best############
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [3:54:35<7:38:03, 203.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3585.8559313322367
INFO:root:current train perplexity4.184760093688965
INFO:root:current mean train loss 3634.7444114364494
INFO:root:current train perplexity4.186991214752197
INFO:root:current mean train loss 3635.5215925103453
INFO:root:current train perplexity4.186914920806885
INFO:root:current mean train loss 3638.4049986836303
INFO:root:current train perplexity4.189105033874512
INFO:root:current mean train loss 3641.4631697260966
INFO:root:current train perplexity4.196329593658447
INFO:root:current mean train loss 3637.513126204239
INFO:root:current train perplexity4.190340042114258
INFO:root:current mean train loss 3642.498474022491
INFO:root:current train perplexity4.197268962860107
INFO:root:current mean train loss 3641.157391586622
INFO:root:current train perplexity4.200317859649658
INFO:root:current mean train loss 3639.580115088904
INFO:root:current train perplexity4.196765899658203
INFO:root:current mean train loss 3640.500235373878
INFO:root:current train perplexity4.202115535736084


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.42s/it]
INFO:root:final mean train loss: 3640.183651462678
INFO:root:final train perplexity: 4.204510688781738
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.84s/it]
INFO:root:eval mean loss: 4001.1678804438166
INFO:root:eval perplexity: 5.042766571044922
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/66

 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [3:59:33<8:38:12, 232.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3589.5503743489585
INFO:root:current train perplexity4.084072113037109
INFO:root:current mean train loss 3612.953903558686
INFO:root:current train perplexity4.151477336883545
INFO:root:current mean train loss 3628.772750249518
INFO:root:current train perplexity4.1819281578063965
INFO:root:current mean train loss 3630.817847202313
INFO:root:current train perplexity4.180218696594238
INFO:root:current mean train loss 3634.1837921499928
INFO:root:current train perplexity4.180254936218262
INFO:root:current mean train loss 3628.040941965874
INFO:root:current train perplexity4.175235748291016
INFO:root:current mean train loss 3628.414099491004
INFO:root:current train perplexity4.179277420043945
INFO:root:current mean train loss 3628.823094762831
INFO:root:current train perplexity4.183665752410889
INFO:root:current mean train loss 3629.7736503481146
INFO:root:current train perplexity4.185990333557129
INFO:root:current mean train loss 3633.3166351153754
INFO:root:current train perplexity4.189144611358643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:20<00:00, 200.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:20<00:00, 200.40s/it]
INFO:root:final mean train loss: 3631.712333371562
INFO:root:final train perplexity: 4.190481185913086
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.92s/it]
INFO:root:eval mean loss: 4002.68411839262
INFO:root:eval perplexity: 5.045859336853027
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/67

 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [4:04:29<9:16:33, 251.08s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3602.96875
INFO:root:current train perplexity4.1437859535217285
INFO:root:current mean train loss 3599.0196090133104
INFO:root:current train perplexity4.148080348968506
INFO:root:current mean train loss 3608.9041472739364
INFO:root:current train perplexity4.157784461975098
INFO:root:current mean train loss 3617.5559628614737
INFO:root:current train perplexity4.163118839263916
INFO:root:current mean train loss 3616.7539943651223
INFO:root:current train perplexity4.162531852722168
INFO:root:current mean train loss 3624.1164788076812
INFO:root:current train perplexity4.16837215423584
INFO:root:current mean train loss 3624.924271807333
INFO:root:current train perplexity4.166794776916504
INFO:root:current mean train loss 3625.1431627338434
INFO:root:current train perplexity4.1702880859375
INFO:root:current mean train loss 3626.9063213416916
INFO:root:current train perplexity4.174483299255371
INFO:root:current mean train loss 3628.6885379240475
INFO:root:current train perplexity4.176748275756836


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:23<00:00, 203.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:23<00:00, 203.79s/it]
INFO:root:final mean train loss: 3624.1256395155383
INFO:root:final train perplexity: 4.177957534790039
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.59s/it]
INFO:root:eval mean loss: 4001.6059691517066
INFO:root:eval perplexity: 5.0436601638793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/68

 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [4:08:27<9:04:09, 247.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3606.4751714662066
INFO:root:current train perplexity4.139174461364746
INFO:root:current mean train loss 3619.8512193373035
INFO:root:current train perplexity4.157698631286621
INFO:root:current mean train loss 3622.6785069846323
INFO:root:current train perplexity4.164237976074219
INFO:root:current mean train loss 3619.858643289905
INFO:root:current train perplexity4.158023357391357
INFO:root:current mean train loss 3613.5819932235822
INFO:root:current train perplexity4.153002738952637
INFO:root:current mean train loss 3616.372113025869
INFO:root:current train perplexity4.156522274017334
INFO:root:current mean train loss 3620.1218983129615
INFO:root:current train perplexity4.159816265106201
INFO:root:current mean train loss 3621.211608804572
INFO:root:current train perplexity4.163296699523926
INFO:root:current mean train loss 3623.1092997015867
INFO:root:current train perplexity4.166930198669434
INFO:root:current mean train loss 3620.5337556129043
INFO:root:current train perplexity4.165384769439697


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:16<00:00, 196.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:16<00:00, 196.42s/it]
INFO:root:final mean train loss: 3617.4600362470073
INFO:root:final train perplexity: 4.1669840812683105
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.86s/it]
INFO:root:eval mean loss: 4003.315270390071
INFO:root:eval perplexity: 5.047147274017334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/69

 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [4:13:22<9:30:39, 261.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3601.570173674939
INFO:root:current train perplexity4.143634796142578
INFO:root:current mean train loss 3593.265775364756
INFO:root:current train perplexity4.140934467315674
INFO:root:current mean train loss 3597.7758711248757
INFO:root:current train perplexity4.13815450668335
INFO:root:current mean train loss 3606.6255717481304
INFO:root:current train perplexity4.149035930633545
INFO:root:current mean train loss 3598.3776920428563
INFO:root:current train perplexity4.144636631011963
INFO:root:current mean train loss 3599.728530689939
INFO:root:current train perplexity4.148221492767334
INFO:root:current mean train loss 3603.4628084947435
INFO:root:current train perplexity4.149186611175537
INFO:root:current mean train loss 3605.2127843214257
INFO:root:current train perplexity4.150937557220459
INFO:root:current mean train loss 3608.5987908870998
INFO:root:current train perplexity4.151246547698975
INFO:root:current mean train loss 3611.8823306367476
INFO:root:current train perplexity4.153539180755615


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.30s/it]
INFO:root:final mean train loss: 3609.7314827826717
INFO:root:final train perplexity: 4.154297828674316
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it]
INFO:root:eval mean loss: 4005.2213160738033
INFO:root:eval perplexity: 5.051037788391113
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/70

 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [4:16:45<8:48:40, 244.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3563.714040982521
INFO:root:current train perplexity4.1251726150512695
INFO:root:current mean train loss 3573.027853527909
INFO:root:current train perplexity4.119524002075195
INFO:root:current mean train loss 3590.847881538067
INFO:root:current train perplexity4.131321907043457
INFO:root:current mean train loss 3591.101158545874
INFO:root:current train perplexity4.133681774139404
INFO:root:current mean train loss 3593.2588050194036
INFO:root:current train perplexity4.129867076873779
INFO:root:current mean train loss 3594.0783311437835
INFO:root:current train perplexity4.131283760070801
INFO:root:current mean train loss 3596.085811539738
INFO:root:current train perplexity4.133188724517822
INFO:root:current mean train loss 3597.275242660985
INFO:root:current train perplexity4.136878967285156
INFO:root:current mean train loss 3599.697655852099
INFO:root:current train perplexity4.138751983642578
INFO:root:current mean train loss 3605.116203554728
INFO:root:current train perplexity4.141575336456299


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.92s/it]
INFO:root:final mean train loss: 3603.2110558171426
INFO:root:final train perplexity: 4.143625259399414
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it]
INFO:root:eval mean loss: 4004.62291008699
INFO:root:eval perplexity: 5.0498175621032715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/71

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [4:21:56<9:27:59, 264.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3570.3840113397855
INFO:root:current train perplexity4.075537204742432
INFO:root:current mean train loss 3574.073517028443
INFO:root:current train perplexity4.089871406555176
INFO:root:current mean train loss 3573.9486106843983
INFO:root:current train perplexity4.099603652954102
INFO:root:current mean train loss 3586.9352568332765
INFO:root:current train perplexity4.112026691436768
INFO:root:current mean train loss 3587.8915722865363
INFO:root:current train perplexity4.113007545471191
INFO:root:current mean train loss 3584.0620625275574
INFO:root:current train perplexity4.115289211273193
INFO:root:current mean train loss 3590.1678281952773
INFO:root:current train perplexity4.118995666503906
INFO:root:current mean train loss 3592.5369142534837
INFO:root:current train perplexity4.123384952545166
INFO:root:current mean train loss 3591.9011475454154
INFO:root:current train perplexity4.124154567718506
INFO:root:current mean train loss 3597.7143011872254
INFO:root:current train perplexity4.13066291809082


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.01s/it]
INFO:root:final mean train loss: 3595.158269205401
INFO:root:final train perplexity: 4.130481243133545
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.92s/it]
INFO:root:eval mean loss: 4005.9667241522607
INFO:root:eval perplexity: 5.052561283111572
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/72

 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [4:25:21<8:45:44, 246.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3603.3746321614585
INFO:root:current train perplexity4.101534843444824
INFO:root:current mean train loss 3584.6070535714284
INFO:root:current train perplexity4.097449779510498
INFO:root:current mean train loss 3587.616435546875
INFO:root:current train perplexity4.10220193862915
INFO:root:current mean train loss 3585.046551432292
INFO:root:current train perplexity4.109790802001953
INFO:root:current mean train loss 3587.4047553453947
INFO:root:current train perplexity4.1138739585876465
INFO:root:current mean train loss 3587.9142297894023
INFO:root:current train perplexity4.112592697143555
INFO:root:current mean train loss 3587.6152567997683
INFO:root:current train perplexity4.112904071807861
INFO:root:current mean train loss 3589.616869329637
INFO:root:current train perplexity4.117537021636963
INFO:root:current mean train loss 3591.456489676339
INFO:root:current train perplexity4.1186604499816895
INFO:root:current mean train loss 3591.821305088141
INFO:root:current train perplexity4.120244026184082


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.52s/it]
INFO:root:final mean train loss: 3588.8046946371755
INFO:root:final train perplexity: 4.120140552520752
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.52s/it]
INFO:root:eval mean loss: 4007.4043765237147
INFO:root:eval perplexity: 5.055500507354736
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/73

 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [4:30:16<9:11:56, 260.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3551.7777055487577
INFO:root:current train perplexity4.077310085296631
INFO:root:current mean train loss 3569.863996328552
INFO:root:current train perplexity4.093883991241455
INFO:root:current mean train loss 3580.0431343860423
INFO:root:current train perplexity4.105213165283203
INFO:root:current mean train loss 3578.360257858396
INFO:root:current train perplexity4.099662780761719
INFO:root:current mean train loss 3579.0259997128946
INFO:root:current train perplexity4.103934288024902
INFO:root:current mean train loss 3580.191719068262
INFO:root:current train perplexity4.108377933502197
INFO:root:current mean train loss 3582.7893491775712
INFO:root:current train perplexity4.10984992980957
INFO:root:current mean train loss 3582.9828590207935
INFO:root:current train perplexity4.108821868896484
INFO:root:current mean train loss 3584.8164831142058
INFO:root:current train perplexity4.109529972076416
INFO:root:current mean train loss 3585.1011887139975
INFO:root:current train perplexity4.110476016998291


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.60s/it]
INFO:root:final mean train loss: 3583.075669011762
INFO:root:final train perplexity: 4.110838890075684
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.06s/it]
INFO:root:eval mean loss: 4007.453621938719
INFO:root:eval perplexity: 5.055601119995117
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/74

 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [4:35:40<9:47:33, 279.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3585.6438873626375
INFO:root:current train perplexity4.0983757972717285
INFO:root:current mean train loss 3566.700210651178
INFO:root:current train perplexity4.076580047607422
INFO:root:current mean train loss 3574.170371563574
INFO:root:current train perplexity4.085627555847168
INFO:root:current mean train loss 3569.229944253517
INFO:root:current train perplexity4.084228038787842
INFO:root:current mean train loss 3568.669973089836
INFO:root:current train perplexity4.0835466384887695
INFO:root:current mean train loss 3570.400327421082
INFO:root:current train perplexity4.081949710845947
INFO:root:current mean train loss 3571.3316443701383
INFO:root:current train perplexity4.087213039398193
INFO:root:current mean train loss 3573.0350047037964
INFO:root:current train perplexity4.091801166534424
INFO:root:current mean train loss 3574.6320814481624
INFO:root:current train perplexity4.094532489776611
INFO:root:current mean train loss 3577.9975704189264
INFO:root:current train perplexity4.098117351531982


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.97s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.97s/it]
INFO:root:final mean train loss: 3575.2181893010293
INFO:root:final train perplexity: 4.098114490509033
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4006.2319890015515
INFO:root:eval perplexity: 5.053102970123291
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/75

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [4:39:20<9:05:31, 261.85s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3553.489050662879
INFO:root:current train perplexity4.065248966217041
INFO:root:current mean train loss 3566.027122919284
INFO:root:current train perplexity4.0695672035217285
INFO:root:current mean train loss 3564.473130650345
INFO:root:current train perplexity4.074617385864258
INFO:root:current mean train loss 3563.4508414591164
INFO:root:current train perplexity4.075788974761963
INFO:root:current mean train loss 3567.2980056793276
INFO:root:current train perplexity4.077972888946533
INFO:root:current mean train loss 3567.6467835389712
INFO:root:current train perplexity4.0757670402526855
INFO:root:current mean train loss 3571.1214817903924
INFO:root:current train perplexity4.083651065826416
INFO:root:current mean train loss 3569.4808901141073
INFO:root:current train perplexity4.084098815917969
INFO:root:current mean train loss 3569.8801065854423
INFO:root:current train perplexity4.086146831512451


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.66s/it]
INFO:root:final mean train loss: 3568.7685525340416
INFO:root:final train perplexity: 4.087699890136719
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it]
INFO:root:eval mean loss: 4006.89365511414
INFO:root:eval perplexity: 5.054455280303955
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/76

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [4:43:26<8:51:27, 257.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3550.8194754464284
INFO:root:current train perplexity4.0613179206848145
INFO:root:current mean train loss 3560.4367584513725
INFO:root:current train perplexity4.076365947723389
INFO:root:current mean train loss 3554.4689221958033
INFO:root:current train perplexity4.065728664398193
INFO:root:current mean train loss 3553.9657455593447
INFO:root:current train perplexity4.060072898864746
INFO:root:current mean train loss 3557.538787166961
INFO:root:current train perplexity4.06412410736084
INFO:root:current mean train loss 3557.4467913084013
INFO:root:current train perplexity4.064555644989014
INFO:root:current mean train loss 3562.7711702500774
INFO:root:current train perplexity4.070700168609619
INFO:root:current mean train loss 3563.439243170969
INFO:root:current train perplexity4.073084354400635
INFO:root:current mean train loss 3564.057282590807
INFO:root:current train perplexity4.074118614196777
INFO:root:current mean train loss 3563.2051257687604
INFO:root:current train perplexity4.074030876159668


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:22<00:00, 202.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:22<00:00, 202.21s/it]
INFO:root:final mean train loss: 3562.111130376016
INFO:root:final train perplexity: 4.076977252960205
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it]
INFO:root:eval mean loss: 4006.642604097407
INFO:root:eval perplexity: 5.0539422035217285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/77

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [4:47:02<8:22:07, 244.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3504.220914713542
INFO:root:current train perplexity3.9807240962982178
INFO:root:current mean train loss 3546.6267917798914
INFO:root:current train perplexity4.036135196685791
INFO:root:current mean train loss 3551.750748319404
INFO:root:current train perplexity4.0429463386535645
INFO:root:current mean train loss 3547.697632998512
INFO:root:current train perplexity4.04355525970459
INFO:root:current mean train loss 3552.139430181664
INFO:root:current train perplexity4.052918434143066
INFO:root:current mean train loss 3553.569268621056
INFO:root:current train perplexity4.056402683258057
INFO:root:current mean train loss 3550.217890942581
INFO:root:current train perplexity4.053493976593018
INFO:root:current mean train loss 3554.4200249945366
INFO:root:current train perplexity4.059800624847412
INFO:root:current mean train loss 3555.4536531226036
INFO:root:current train perplexity4.062302589416504
INFO:root:current mean train loss 3557.1173289147882
INFO:root:current train perplexity4.064230918884277


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:17<00:00, 197.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:17<00:00, 197.48s/it]
INFO:root:final mean train loss: 3556.0535268475933
INFO:root:final train perplexity: 4.0672454833984375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4006.7060165946364
INFO:root:eval perplexity: 5.054073333740234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/78

 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [4:50:33<7:57:14, 234.71s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3555.0279063349185
INFO:root:current train perplexity4.065832138061523
INFO:root:current mean train loss 3544.7419969512193
INFO:root:current train perplexity4.056237697601318
INFO:root:current mean train loss 3540.9586526502944
INFO:root:current train perplexity4.054385662078857
INFO:root:current mean train loss 3545.666250695385
INFO:root:current train perplexity4.052335262298584
INFO:root:current mean train loss 3543.6143674737737
INFO:root:current train perplexity4.049180030822754
INFO:root:current mean train loss 3542.191889396361
INFO:root:current train perplexity4.050066947937012
INFO:root:current mean train loss 3547.3908848157857
INFO:root:current train perplexity4.052881717681885
INFO:root:current mean train loss 3546.6448648750866
INFO:root:current train perplexity4.0501484870910645
INFO:root:current mean train loss 3548.389234911338
INFO:root:current train perplexity4.050169467926025
INFO:root:current mean train loss 3549.6963536023836
INFO:root:current train perplexity4.0550079345703125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.03s/it]
INFO:root:final mean train loss: 3549.244753314603
INFO:root:final train perplexity: 4.056334972381592
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.93s/it]
INFO:root:eval mean loss: 4008.177775930851
INFO:root:eval perplexity: 5.0570807456970215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/79

 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [4:53:59<7:35:50, 226.03s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3527.507095829133
INFO:root:current train perplexity4.006855010986328
INFO:root:current mean train loss 3535.752143219227
INFO:root:current train perplexity4.027383804321289
INFO:root:current mean train loss 3541.7845802472266
INFO:root:current train perplexity4.030364990234375
INFO:root:current mean train loss 3547.2898981837707
INFO:root:current train perplexity4.0353007316589355
INFO:root:current mean train loss 3545.372056151211
INFO:root:current train perplexity4.040431022644043
INFO:root:current mean train loss 3544.82001916343
INFO:root:current train perplexity4.043437480926514
INFO:root:current mean train loss 3544.3408748669026
INFO:root:current train perplexity4.042606830596924
INFO:root:current mean train loss 3544.5542316149754
INFO:root:current train perplexity4.043527603149414
INFO:root:current mean train loss 3547.9914321623983
INFO:root:current train perplexity4.048252582550049
INFO:root:current mean train loss 3544.6824307385373
INFO:root:current train perplexity4.046630382537842


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.26s/it]
INFO:root:final mean train loss: 3542.672996767106
INFO:root:final train perplexity: 4.045830726623535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it]
INFO:root:eval mean loss: 4009.5237699468084
INFO:root:eval perplexity: 5.059834003448486
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/80

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:57:24<7:19:16, 219.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3500.154484675481
INFO:root:current train perplexity4.038647651672363
INFO:root:current mean train loss 3522.2810936797437
INFO:root:current train perplexity4.018341064453125
INFO:root:current mean train loss 3521.9622654615587
INFO:root:current train perplexity4.01380729675293
INFO:root:current mean train loss 3524.401480255577
INFO:root:current train perplexity4.009393692016602
INFO:root:current mean train loss 3526.712047533457
INFO:root:current train perplexity4.0206170082092285
INFO:root:current mean train loss 3526.930644132653
INFO:root:current train perplexity4.022581100463867
INFO:root:current mean train loss 3533.652162268315
INFO:root:current train perplexity4.028739929199219
INFO:root:current mean train loss 3535.6589897269323
INFO:root:current train perplexity4.033061981201172
INFO:root:current mean train loss 3537.011978313096
INFO:root:current train perplexity4.034329414367676
INFO:root:current mean train loss 3536.740721096246
INFO:root:current train perplexity4.033527851104736


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.41s/it]
INFO:root:final mean train loss: 3536.253266488352
INFO:root:final train perplexity: 4.03559684753418
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it]
INFO:root:eval mean loss: 4009.039052111037
INFO:root:eval perplexity: 5.058842658996582
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/81

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [5:00:49<7:07:22, 215.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3534.9153818982713
INFO:root:current train perplexity4.057122230529785
INFO:root:current mean train loss 3543.62129470929
INFO:root:current train perplexity4.034052848815918
INFO:root:current mean train loss 3527.457870421622
INFO:root:current train perplexity4.017823219299316
INFO:root:current mean train loss 3525.6169722059617
INFO:root:current train perplexity4.0148820877075195
INFO:root:current mean train loss 3531.868195194526
INFO:root:current train perplexity4.016088008880615
INFO:root:current mean train loss 3526.374783085295
INFO:root:current train perplexity4.016303539276123
INFO:root:current mean train loss 3529.114873258187
INFO:root:current train perplexity4.0195136070251465
INFO:root:current mean train loss 3531.895489836952
INFO:root:current train perplexity4.0192341804504395
INFO:root:current mean train loss 3534.800444007342
INFO:root:current train perplexity4.023095607757568
INFO:root:current mean train loss 3532.8550144989113
INFO:root:current train perplexity4.0263261795043945


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.45s/it]
INFO:root:final mean train loss: 3530.1302152449084
INFO:root:final train perplexity: 4.025859832763672
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.39s/it]
INFO:root:eval mean loss: 4011.630791846742
INFO:root:eval perplexity: 5.064146995544434
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/82

 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [5:04:15<6:58:03, 212.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3529.194890802557
INFO:root:current train perplexity4.013147830963135
INFO:root:current mean train loss 3518.3017562373993
INFO:root:current train perplexity4.003843307495117
INFO:root:current mean train loss 3515.2230267693017
INFO:root:current train perplexity4.002891540527344
INFO:root:current mean train loss 3518.8756952849913
INFO:root:current train perplexity4.009930610656738
INFO:root:current mean train loss 3516.829028052026
INFO:root:current train perplexity4.014516830444336
INFO:root:current mean train loss 3520.2028175147802
INFO:root:current train perplexity4.015749931335449
INFO:root:current mean train loss 3521.4243909530055
INFO:root:current train perplexity4.015331745147705
INFO:root:current mean train loss 3524.273399019557
INFO:root:current train perplexity4.016026973724365
INFO:root:current mean train loss 3525.1952619586073
INFO:root:current train perplexity4.015049457550049
INFO:root:current mean train loss 3528.053290402078
INFO:root:current train perplexity4.017074108123779


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.25s/it]
INFO:root:final mean train loss: 3523.458707932503
INFO:root:final train perplexity: 4.01527738571167
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.77s/it]
INFO:root:eval mean loss: 4012.902577501662
INFO:root:eval perplexity: 5.066751956939697
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/83

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [5:07:39<6:49:24, 209.95s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3513.9188523065477
INFO:root:current train perplexity3.9679081439971924
INFO:root:current mean train loss 3514.956745171108
INFO:root:current train perplexity3.9881865978240967
INFO:root:current mean train loss 3507.5289664032794
INFO:root:current train perplexity3.9969751834869385
INFO:root:current mean train loss 3507.3555427320075
INFO:root:current train perplexity3.9980993270874023
INFO:root:current mean train loss 3511.744398475466
INFO:root:current train perplexity3.998095989227295
INFO:root:current mean train loss 3513.306559100244
INFO:root:current train perplexity4.00044059753418
INFO:root:current mean train loss 3517.365973056891
INFO:root:current train perplexity4.0038323402404785
INFO:root:current mean train loss 3519.9740215816473
INFO:root:current train perplexity4.0065016746521
INFO:root:current mean train loss 3519.136467536935
INFO:root:current train perplexity4.004615783691406
INFO:root:current mean train loss 3519.183371412172
INFO:root:current train perplexity4.004958152770996


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.24s/it]
INFO:root:final mean train loss: 3516.850501952633
INFO:root:final train perplexity: 4.004822254180908
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.20s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.20s/it]
INFO:root:eval mean loss: 4012.493603861924
INFO:root:eval perplexity: 5.065913677215576
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/84

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [5:11:02<6:42:04, 207.97s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3487.2850943551935
INFO:root:current train perplexity3.976776599884033
INFO:root:current mean train loss 3499.6894331368785
INFO:root:current train perplexity3.978728771209717
INFO:root:current mean train loss 3494.8436193712523
INFO:root:current train perplexity3.975935220718384
INFO:root:current mean train loss 3499.330565748189
INFO:root:current train perplexity3.9846572875976562
INFO:root:current mean train loss 3501.9170269166334
INFO:root:current train perplexity3.9843077659606934
INFO:root:current mean train loss 3506.4221071687552
INFO:root:current train perplexity3.9865834712982178
INFO:root:current mean train loss 3511.6034301212044
INFO:root:current train perplexity3.99241304397583
INFO:root:current mean train loss 3515.1291738230584
INFO:root:current train perplexity3.9937615394592285
INFO:root:current mean train loss 3515.459973270666
INFO:root:current train perplexity3.9957683086395264
INFO:root:current mean train loss 3514.7205746431673
INFO:root:current train perplexity3.9972100257873535


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.70s/it]
INFO:root:final mean train loss: 3511.8432908211985
INFO:root:final train perplexity: 3.9969189167022705
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4013.3426678163787
INFO:root:eval perplexity: 5.067653656005859
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/85

 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [5:14:26<6:36:18, 206.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3461.475397423853
INFO:root:current train perplexity3.979806661605835
INFO:root:current mean train loss 3490.6113472197976
INFO:root:current train perplexity3.980832815170288
INFO:root:current mean train loss 3503.396296237959
INFO:root:current train perplexity3.9826669692993164
INFO:root:current mean train loss 3507.1894582783643
INFO:root:current train perplexity3.981111526489258
INFO:root:current mean train loss 3506.7126903175563
INFO:root:current train perplexity3.9820878505706787
INFO:root:current mean train loss 3505.402375796092
INFO:root:current train perplexity3.9839186668395996
INFO:root:current mean train loss 3504.407306744178
INFO:root:current train perplexity3.9853758811950684
INFO:root:current mean train loss 3507.3490195513077
INFO:root:current train perplexity3.9870994091033936
INFO:root:current mean train loss 3508.7057533307557
INFO:root:current train perplexity3.9869213104248047
INFO:root:current mean train loss 3509.2412101893674
INFO:root:current train perplexity3.987111806869507


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.80s/it]
INFO:root:final mean train loss: 3506.0510822419196
INFO:root:final train perplexity: 3.987795352935791
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4014.3383806377437
INFO:root:eval perplexity: 5.069694519042969
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/86

 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [5:17:50<6:30:46, 205.67s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3472.235068134878
INFO:root:current train perplexity3.9532411098480225
INFO:root:current mean train loss 3477.036799956133
INFO:root:current train perplexity3.952800989151001
INFO:root:current mean train loss 3480.984027929007
INFO:root:current train perplexity3.967891216278076
INFO:root:current mean train loss 3486.419014706476
INFO:root:current train perplexity3.968888282775879
INFO:root:current mean train loss 3490.8472095779325
INFO:root:current train perplexity3.970273017883301
INFO:root:current mean train loss 3496.9288398703684
INFO:root:current train perplexity3.9740195274353027
INFO:root:current mean train loss 3497.0677687465886
INFO:root:current train perplexity3.9731335639953613
INFO:root:current mean train loss 3500.2882751697507
INFO:root:current train perplexity3.9775583744049072
INFO:root:current mean train loss 3501.7488359969525
INFO:root:current train perplexity3.9783337116241455
INFO:root:current mean train loss 3503.650950886921
INFO:root:current train perplexity3.9795968532562256


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.87s/it]
INFO:root:final mean train loss: 3501.0732906095445
INFO:root:final train perplexity: 3.9799721240997314
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4014.947861258865
INFO:root:eval perplexity: 5.070944786071777
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/87

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [5:21:15<6:27:00, 205.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3490.2747301603617
INFO:root:current train perplexity3.94624400138855
INFO:root:current mean train loss 3503.6610627003206
INFO:root:current train perplexity3.9604580402374268
INFO:root:current mean train loss 3500.518080475371
INFO:root:current train perplexity3.964395761489868
INFO:root:current mean train loss 3491.7932728441456
INFO:root:current train perplexity3.9599246978759766
INFO:root:current mean train loss 3493.190673828125
INFO:root:current train perplexity3.9623565673828125
INFO:root:current mean train loss 3494.526537880777
INFO:root:current train perplexity3.9599387645721436
INFO:root:current mean train loss 3494.1055586780576
INFO:root:current train perplexity3.9625656604766846
INFO:root:current mean train loss 3495.54042907331
INFO:root:current train perplexity3.966569662094116
INFO:root:current mean train loss 3497.916555734986
INFO:root:current train perplexity3.970825433731079


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.54s/it]
INFO:root:final mean train loss: 3495.8935415821693
INFO:root:final train perplexity: 3.9718470573425293
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.54s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.54s/it]
INFO:root:eval mean loss: 4016.7293536679963
INFO:root:eval perplexity: 5.074599266052246
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/88

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [5:24:38<6:22:08, 204.72s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3697.13818359375
INFO:root:current train perplexity4.081518173217773
INFO:root:current mean train loss 3502.9204125265474
INFO:root:current train perplexity3.9623796939849854
INFO:root:current mean train loss 3487.0497890528786
INFO:root:current train perplexity3.9468724727630615
INFO:root:current mean train loss 3482.4407366762066
INFO:root:current train perplexity3.9417824745178223
INFO:root:current mean train loss 3487.6771031230614
INFO:root:current train perplexity3.952070713043213
INFO:root:current mean train loss 3487.9547034201046
INFO:root:current train perplexity3.955193281173706
INFO:root:current mean train loss 3488.814587948927
INFO:root:current train perplexity3.954244613647461
INFO:root:current mean train loss 3486.2955765052675
INFO:root:current train perplexity3.954688787460327
INFO:root:current mean train loss 3489.0539252826316
INFO:root:current train perplexity3.955852508544922
INFO:root:current mean train loss 3490.0611554691827
INFO:root:current train perplexity3.960636615753174


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.51s/it]
INFO:root:final mean train loss: 3489.4081045581447
INFO:root:final train perplexity: 3.9616966247558594
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it]
INFO:root:eval mean loss: 4018.435133047983
INFO:root:eval perplexity: 5.078100681304932
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/89

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [5:28:04<6:19:27, 205.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3487.5199973366475
INFO:root:current train perplexity3.9076712131500244
INFO:root:current mean train loss 3478.9007205447633
INFO:root:current train perplexity3.945061445236206
INFO:root:current mean train loss 3476.147824255776
INFO:root:current train perplexity3.943756580352783
INFO:root:current mean train loss 3474.188230066821
INFO:root:current train perplexity3.94417405128479
INFO:root:current mean train loss 3471.724058128041
INFO:root:current train perplexity3.9364757537841797
INFO:root:current mean train loss 3473.2426466372617
INFO:root:current train perplexity3.9369962215423584
INFO:root:current mean train loss 3475.0118162464196
INFO:root:current train perplexity3.9427907466888428
INFO:root:current mean train loss 3481.3204240973323
INFO:root:current train perplexity3.9448390007019043
INFO:root:current mean train loss 3481.4960645494566
INFO:root:current train perplexity3.946784019470215
INFO:root:current mean train loss 3483.9354664774287
INFO:root:current train perplexity3.9500765800476074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.72s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.72s/it]
INFO:root:final mean train loss: 3484.466683233938
INFO:root:final train perplexity: 3.953981637954712
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.83s/it]
INFO:root:eval mean loss: 4017.4924610760195
INFO:root:eval perplexity: 5.076165199279785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/90

 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [5:31:29<6:16:12, 205.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3437.3738692434213
INFO:root:current train perplexity3.973761796951294
INFO:root:current mean train loss 3468.968772567621
INFO:root:current train perplexity3.9136197566986084
INFO:root:current mean train loss 3483.3391871343465
INFO:root:current train perplexity3.9225029945373535
INFO:root:current mean train loss 3481.902700394299
INFO:root:current train perplexity3.9318201541900635
INFO:root:current mean train loss 3483.2270070806608
INFO:root:current train perplexity3.932004451751709
INFO:root:current mean train loss 3480.563539126475
INFO:root:current train perplexity3.931692361831665
INFO:root:current mean train loss 3477.629014713121
INFO:root:current train perplexity3.933119297027588
INFO:root:current mean train loss 3475.207465202321
INFO:root:current train perplexity3.9375383853912354
INFO:root:current mean train loss 3476.644526182368
INFO:root:current train perplexity3.9391393661499023
INFO:root:current mean train loss 3480.302120171382
INFO:root:current train perplexity3.9422149658203125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it]
INFO:root:final mean train loss: 3476.7523345331992
INFO:root:final train perplexity: 3.941965341567993
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it]
INFO:root:eval mean loss: 4021.027788743905
INFO:root:eval perplexity: 5.0834269523620605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/91

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [5:35:23<6:28:35, 213.91s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3466.288908781829
INFO:root:current train perplexity3.960223436355591
INFO:root:current mean train loss 3466.928468719242
INFO:root:current train perplexity3.926481008529663
INFO:root:current mean train loss 3464.48472884258
INFO:root:current train perplexity3.9269795417785645
INFO:root:current mean train loss 3467.0438407874617
INFO:root:current train perplexity3.9275407791137695
INFO:root:current mean train loss 3461.4884979462454
INFO:root:current train perplexity3.925117254257202
INFO:root:current mean train loss 3467.4810904515534
INFO:root:current train perplexity3.930009603500366
INFO:root:current mean train loss 3470.722498940889
INFO:root:current train perplexity3.930752992630005
INFO:root:current mean train loss 3473.7224087511822
INFO:root:current train perplexity3.933832883834839
INFO:root:current mean train loss 3473.533855249112
INFO:root:current train perplexity3.9343929290771484
INFO:root:current mean train loss 3473.581080234038
INFO:root:current train perplexity3.9341399669647217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.51s/it]
INFO:root:final mean train loss: 3471.9606824690295
INFO:root:final train perplexity: 3.9345204830169678
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it]
INFO:root:eval mean loss: 4021.7300047096633
INFO:root:eval perplexity: 5.0848708152771
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/92

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [5:38:59<6:26:03, 214.47s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3457.568212890625
INFO:root:current train perplexity3.916844367980957
INFO:root:current mean train loss 3458.888868995949
INFO:root:current train perplexity3.910355806350708
INFO:root:current mean train loss 3466.0424555352392
INFO:root:current train perplexity3.92107892036438
INFO:root:current mean train loss 3468.7627186333957
INFO:root:current train perplexity3.92234206199646
INFO:root:current mean train loss 3473.417795887213
INFO:root:current train perplexity3.928455352783203
INFO:root:current mean train loss 3474.5609489084404
INFO:root:current train perplexity3.930708646774292
INFO:root:current mean train loss 3470.0151178795522
INFO:root:current train perplexity3.9248688220977783
INFO:root:current mean train loss 3466.6949584130525
INFO:root:current train perplexity3.9230875968933105
INFO:root:current mean train loss 3468.336670506643
INFO:root:current train perplexity3.9249136447906494
INFO:root:current mean train loss 3467.446580725685
INFO:root:current train perplexity3.9235520362854004


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.32s/it]
INFO:root:final mean train loss: 3467.151377893263
INFO:root:final train perplexity: 3.9270620346069336
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4021.924508602061
INFO:root:eval perplexity: 5.085270881652832
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/93

 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [5:43:08<6:41:06, 224.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3424.6943927143893
INFO:root:current train perplexity3.9004948139190674
INFO:root:current mean train loss 3461.0963535975743
INFO:root:current train perplexity3.8932855129241943
INFO:root:current mean train loss 3453.1384106545784
INFO:root:current train perplexity3.9056217670440674
INFO:root:current mean train loss 3455.928142225082
INFO:root:current train perplexity3.9087140560150146
INFO:root:current mean train loss 3455.992420618475
INFO:root:current train perplexity3.9100584983825684
INFO:root:current mean train loss 3455.549641477469
INFO:root:current train perplexity3.90915846824646
INFO:root:current mean train loss 3457.4313661091805
INFO:root:current train perplexity3.913468599319458
INFO:root:current mean train loss 3462.703829820512
INFO:root:current train perplexity3.915584087371826
INFO:root:current mean train loss 3463.0020235000557
INFO:root:current train perplexity3.916470527648926
INFO:root:current mean train loss 3463.4660957797587
INFO:root:current train perplexity3.917127847671509


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.86s/it]
INFO:root:final mean train loss: 3461.2039933973742
INFO:root:final train perplexity: 3.917858362197876
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it]
INFO:root:eval mean loss: 4023.709808566046
INFO:root:eval perplexity: 5.088943958282471
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/94

 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [5:46:33<6:26:29, 218.77s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3442.7324362362133
INFO:root:current train perplexity3.9101054668426514
INFO:root:current mean train loss 3456.840653779491
INFO:root:current train perplexity3.903555154800415
INFO:root:current mean train loss 3458.442113382408
INFO:root:current train perplexity3.907273054122925
INFO:root:current mean train loss 3457.422551081731
INFO:root:current train perplexity3.8969180583953857
INFO:root:current mean train loss 3455.0153786940477
INFO:root:current train perplexity3.898183584213257
INFO:root:current mean train loss 3458.180915292508
INFO:root:current train perplexity3.9052577018737793
INFO:root:current mean train loss 3458.2242647279427
INFO:root:current train perplexity3.908954381942749
INFO:root:current mean train loss 3458.9122156785743
INFO:root:current train perplexity3.9087648391723633
INFO:root:current mean train loss 3458.508088771941
INFO:root:current train perplexity3.9097564220428467
INFO:root:current mean train loss 3459.732295568809
INFO:root:current train perplexity3.9126322269439697


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 3456.2231575750534
INFO:root:final train perplexity: 3.9101669788360596
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4025.6727199689717
INFO:root:eval perplexity: 5.092983245849609
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/95

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [5:49:55<6:14:12, 213.84s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3420.664265260858
INFO:root:current train perplexity3.8536572456359863
INFO:root:current mean train loss 3429.8966394580384
INFO:root:current train perplexity3.85772967338562
INFO:root:current mean train loss 3433.4195372828185
INFO:root:current train perplexity3.8665108680725098
INFO:root:current mean train loss 3441.7083294796744
INFO:root:current train perplexity3.8765664100646973
INFO:root:current mean train loss 3443.6732504850897
INFO:root:current train perplexity3.8814804553985596
INFO:root:current mean train loss 3447.9610030117956
INFO:root:current train perplexity3.8854706287384033
INFO:root:current mean train loss 3448.333290111675
INFO:root:current train perplexity3.889583110809326
INFO:root:current mean train loss 3451.785717548275
INFO:root:current train perplexity3.89443302154541
INFO:root:current mean train loss 3454.5478416149776
INFO:root:current train perplexity3.898984432220459
INFO:root:current mean train loss 3454.609716644128
INFO:root:current train perplexity3.9003310203552246


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.36s/it]
INFO:root:final mean train loss: 3450.0264616935483
INFO:root:final train perplexity: 3.9006195068359375
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.32s/it]
INFO:root:eval mean loss: 4026.48178295379
INFO:root:eval perplexity: 5.0946502685546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/96

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [5:53:18<6:04:45, 210.44s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3446.7612195370803
INFO:root:current train perplexity3.9135031700134277
INFO:root:current mean train loss 3449.70328873503
INFO:root:current train perplexity3.8979856967926025
INFO:root:current mean train loss 3455.047222466058
INFO:root:current train perplexity3.899258613586426
INFO:root:current mean train loss 3452.8657658964153
INFO:root:current train perplexity3.893954277038574
INFO:root:current mean train loss 3449.916468879651
INFO:root:current train perplexity3.8926074504852295
INFO:root:current mean train loss 3446.527888437638
INFO:root:current train perplexity3.89097261428833
INFO:root:current mean train loss 3447.453541539777
INFO:root:current train perplexity3.890831232070923
INFO:root:current mean train loss 3445.8470384182488
INFO:root:current train perplexity3.8934948444366455
INFO:root:current mean train loss 3449.75505880776
INFO:root:current train perplexity3.8959977626800537
INFO:root:current mean train loss 3449.2738866480577
INFO:root:current train perplexity3.8961009979248047


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it]
INFO:root:final mean train loss: 3446.782481224306
INFO:root:final train perplexity: 3.895630121231079
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it]
INFO:root:eval mean loss: 4027.7057326296544
INFO:root:eval perplexity: 5.097172260284424
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/97

 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [5:56:41<5:57:30, 208.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3427.6237239583334
INFO:root:current train perplexity3.8869800567626953
INFO:root:current mean train loss 3431.7063755580357
INFO:root:current train perplexity3.87911319732666
INFO:root:current mean train loss 3443.499806463068
INFO:root:current train perplexity3.8779659271240234
INFO:root:current mean train loss 3446.4459641927083
INFO:root:current train perplexity3.878014087677002
INFO:root:current mean train loss 3443.5053844572367
INFO:root:current train perplexity3.880507707595825
INFO:root:current mean train loss 3439.5639198369563
INFO:root:current train perplexity3.8822648525238037
INFO:root:current mean train loss 3444.3165610532405
INFO:root:current train perplexity3.88450288772583
INFO:root:current mean train loss 3442.6541982736894
INFO:root:current train perplexity3.8840174674987793
INFO:root:current mean train loss 3444.4384464285713
INFO:root:current train perplexity3.8857598304748535
INFO:root:current mean train loss 3443.301926332131
INFO:root:current train perplexity3.887423038482666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.85s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.85s/it]
INFO:root:final mean train loss: 3441.5607779102943
INFO:root:final train perplexity: 3.88761305809021
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.57s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.57s/it]
INFO:root:eval mean loss: 4026.700752853502
INFO:root:eval perplexity: 5.095102787017822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/98

 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [6:00:05<5:52:00, 207.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3422.0093685288025
INFO:root:current train perplexity3.8593127727508545
INFO:root:current mean train loss 3422.2090444095797
INFO:root:current train perplexity3.866459608078003
INFO:root:current mean train loss 3435.1580590561507
INFO:root:current train perplexity3.8662941455841064
INFO:root:current mean train loss 3437.216112898784
INFO:root:current train perplexity3.8659558296203613
INFO:root:current mean train loss 3437.594908530668
INFO:root:current train perplexity3.8696086406707764
INFO:root:current mean train loss 3436.7360027437553
INFO:root:current train perplexity3.8707518577575684
INFO:root:current mean train loss 3438.8198799814695
INFO:root:current train perplexity3.8752529621124268
INFO:root:current mean train loss 3436.761354253971
INFO:root:current train perplexity3.8751437664031982
INFO:root:current mean train loss 3438.209239022243
INFO:root:current train perplexity3.876215696334839
INFO:root:current mean train loss 3439.0866170206
INFO:root:current train perplexity3.8792619705200195


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.61s/it]
INFO:root:final mean train loss: 3436.411103771579
INFO:root:final train perplexity: 3.8797221183776855
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it]
INFO:root:eval mean loss: 4028.4427741300974
INFO:root:eval perplexity: 5.098691463470459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/99

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [6:04:06<6:05:56, 217.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3400.445140796703
INFO:root:current train perplexity3.822920799255371
INFO:root:current mean train loss 3416.262397486502
INFO:root:current train perplexity3.836162567138672
INFO:root:current mean train loss 3422.2232468857387
INFO:root:current train perplexity3.850580930709839
INFO:root:current mean train loss 3427.880160670756
INFO:root:current train perplexity3.8588039875030518
INFO:root:current mean train loss 3428.2898993404724
INFO:root:current train perplexity3.8603973388671875
INFO:root:current mean train loss 3429.694148804331
INFO:root:current train perplexity3.8618738651275635
INFO:root:current mean train loss 3430.804296733674
INFO:root:current train perplexity3.863929033279419
INFO:root:current mean train loss 3431.626459288085
INFO:root:current train perplexity3.866180419921875
INFO:root:current mean train loss 3430.455815479009
INFO:root:current train perplexity3.8676319122314453
INFO:root:current mean train loss 3434.3423213708534
INFO:root:current train perplexity3.872476577758789


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.24s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.24s/it]
INFO:root:final mean train loss: 3431.606078363234
INFO:root:final train perplexity: 3.8723740577697754
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.77s/it]
INFO:root:eval mean loss: 4030.0570475260415
INFO:root:eval perplexity: 5.102021217346191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/100

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [6:07:46<6:03:10, 217.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3403.3407981178975
INFO:root:current train perplexity3.8261196613311768
INFO:root:current mean train loss 3410.0549181454144
INFO:root:current train perplexity3.847834825515747
INFO:root:current mean train loss 3415.8384621916807
INFO:root:current train perplexity3.853348970413208
INFO:root:current mean train loss 3414.156978138706
INFO:root:current train perplexity3.850675106048584
INFO:root:current mean train loss 3418.653264536886
INFO:root:current train perplexity3.8516690731048584
INFO:root:current mean train loss 3420.681797543432
INFO:root:current train perplexity3.854940891265869
INFO:root:current mean train loss 3425.7689800999196
INFO:root:current train perplexity3.859205722808838
INFO:root:current mean train loss 3429.4881333600592
INFO:root:current train perplexity3.8621833324432373
INFO:root:current mean train loss 3429.3761283696294
INFO:root:current train perplexity3.863969087600708


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.51s/it]
INFO:root:final mean train loss: 3426.5443005715647
INFO:root:final train perplexity: 3.8646485805511475
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it]
INFO:root:eval mean loss: 4031.8648084275264
INFO:root:eval perplexity: 5.105751991271973
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/101

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [6:11:09<5:52:34, 213.68s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3377.476143973214
INFO:root:current train perplexity3.830552816390991
INFO:root:current mean train loss 3440.4251857294103
INFO:root:current train perplexity3.85135555267334
INFO:root:current mean train loss 3427.9968120187955
INFO:root:current train perplexity3.845052719116211
INFO:root:current mean train loss 3419.8436116271378
INFO:root:current train perplexity3.8508358001708984
INFO:root:current mean train loss 3425.6707334776183
INFO:root:current train perplexity3.8540825843811035
INFO:root:current mean train loss 3424.7813679772253
INFO:root:current train perplexity3.8559186458587646
INFO:root:current mean train loss 3422.354426627497
INFO:root:current train perplexity3.8514909744262695
INFO:root:current mean train loss 3421.0010363027095
INFO:root:current train perplexity3.8515937328338623
INFO:root:current mean train loss 3422.208905112492
INFO:root:current train perplexity3.853692054748535
INFO:root:current mean train loss 3422.178120639385
INFO:root:current train perplexity3.8557066917419434


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it]
INFO:root:final mean train loss: 3421.0503383144255
INFO:root:final train perplexity: 3.856281280517578
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.95s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.95s/it]
INFO:root:eval mean loss: 4035.453708513409
INFO:root:eval perplexity: 5.113166332244873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/102

 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [6:15:44<6:18:46, 231.90s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3392.076106770833
INFO:root:current train perplexity3.7886202335357666
INFO:root:current mean train loss 3396.805027173913
INFO:root:current train perplexity3.801950454711914
INFO:root:current mean train loss 3397.646115325218
INFO:root:current train perplexity3.815281391143799
INFO:root:current mean train loss 3413.3186368427578
INFO:root:current train perplexity3.831084728240967
INFO:root:current mean train loss 3419.7307205384036
INFO:root:current train perplexity3.8376362323760986
INFO:root:current mean train loss 3415.375099552488
INFO:root:current train perplexity3.835211992263794
INFO:root:current mean train loss 3417.6858148342226
INFO:root:current train perplexity3.8394579887390137
INFO:root:current mean train loss 3416.0801231971154
INFO:root:current train perplexity3.8421313762664795
INFO:root:current mean train loss 3416.0275183929257
INFO:root:current train perplexity3.8444788455963135
INFO:root:current mean train loss 3417.298807046192
INFO:root:current train perplexity3.8472073078155518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.22s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.23s/it]
INFO:root:final mean train loss: 3416.6774936183806
INFO:root:final train perplexity: 3.8496341705322266
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4031.370840951906
INFO:root:eval perplexity: 5.104732513427734
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/103

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [6:20:33<6:42:41, 249.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3349.918074898098
INFO:root:current train perplexity3.7886948585510254
INFO:root:current mean train loss 3397.954891545986
INFO:root:current train perplexity3.805385112762451
INFO:root:current mean train loss 3403.0936908807457
INFO:root:current train perplexity3.818390369415283
INFO:root:current mean train loss 3401.130640177535
INFO:root:current train perplexity3.8207850456237793
INFO:root:current mean train loss 3400.86677078716
INFO:root:current train perplexity3.824082136154175
INFO:root:current mean train loss 3406.1308859830606
INFO:root:current train perplexity3.833573579788208
INFO:root:current mean train loss 3409.264703300562
INFO:root:current train perplexity3.8374650478363037
INFO:root:current mean train loss 3411.17736968361
INFO:root:current train perplexity3.8410003185272217
INFO:root:current mean train loss 3411.989915775934
INFO:root:current train perplexity3.8410987854003906
INFO:root:current mean train loss 3413.091822532249
INFO:root:current train perplexity3.8404173851013184


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.67s/it]
INFO:root:final mean train loss: 3412.3756609885922
INFO:root:final train perplexity: 3.8431060314178467
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it]
INFO:root:eval mean loss: 4031.756877493351
INFO:root:eval perplexity: 5.10552978515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/104

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [6:23:56<6:16:33, 235.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3356.958267704133
INFO:root:current train perplexity3.7862513065338135
INFO:root:current mean train loss 3381.305090052481
INFO:root:current train perplexity3.8044893741607666
INFO:root:current mean train loss 3377.102823364786
INFO:root:current train perplexity3.810087203979492
INFO:root:current mean train loss 3385.2822339383497
INFO:root:current train perplexity3.817002773284912
INFO:root:current mean train loss 3387.8294034811847
INFO:root:current train perplexity3.8184778690338135
INFO:root:current mean train loss 3393.338986729049
INFO:root:current train perplexity3.8219761848449707
INFO:root:current mean train loss 3399.741990949386
INFO:root:current train perplexity3.826568365097046
INFO:root:current mean train loss 3402.8109513268423
INFO:root:current train perplexity3.8297810554504395
INFO:root:current mean train loss 3409.09424474466
INFO:root:current train perplexity3.835984706878662
INFO:root:current mean train loss 3409.352912484895
INFO:root:current train perplexity3.8348677158355713


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.05s/it]
INFO:root:final mean train loss: 3408.127768547304
INFO:root:final train perplexity: 3.8366706371307373
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4034.572906277704
INFO:root:eval perplexity: 5.11134672164917
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/105

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [6:27:55<6:14:16, 236.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3389.5708446013623
INFO:root:current train perplexity3.8259944915771484
INFO:root:current mean train loss 3405.726365782374
INFO:root:current train perplexity3.82356333732605
INFO:root:current mean train loss 3398.6093933871603
INFO:root:current train perplexity3.821830987930298
INFO:root:current mean train loss 3409.2757082238663
INFO:root:current train perplexity3.8256192207336426
INFO:root:current mean train loss 3405.7128694920984
INFO:root:current train perplexity3.8227827548980713
INFO:root:current mean train loss 3404.790912805108
INFO:root:current train perplexity3.82293438911438
INFO:root:current mean train loss 3407.556892406959
INFO:root:current train perplexity3.8244574069976807
INFO:root:current mean train loss 3408.7655870078866
INFO:root:current train perplexity3.8265912532806396
INFO:root:current mean train loss 3406.401452156585
INFO:root:current train perplexity3.8260703086853027
INFO:root:current mean train loss 3406.1123657876565
INFO:root:current train perplexity3.827998161315918


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.04s/it]
INFO:root:final mean train loss: 3402.52126238423
INFO:root:final train perplexity: 3.8281941413879395
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it]
INFO:root:eval mean loss: 4037.8053160322474
INFO:root:eval perplexity: 5.118031978607178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/106

 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [6:32:43<6:34:30, 251.82s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3336.100637882314
INFO:root:current train perplexity3.782719850540161
INFO:root:current mean train loss 3377.7977901121385
INFO:root:current train perplexity3.8054654598236084
INFO:root:current mean train loss 3378.2562082885247
INFO:root:current train perplexity3.7999448776245117
INFO:root:current mean train loss 3382.7219315674533
INFO:root:current train perplexity3.8067517280578613
INFO:root:current mean train loss 3388.359561792156
INFO:root:current train perplexity3.8053460121154785
INFO:root:current mean train loss 3394.9941821333696
INFO:root:current train perplexity3.8107354640960693
INFO:root:current mean train loss 3397.218208136109
INFO:root:current train perplexity3.813032865524292
INFO:root:current mean train loss 3396.2972652328062
INFO:root:current train perplexity3.8140459060668945
INFO:root:current mean train loss 3399.2683338944435
INFO:root:current train perplexity3.8191606998443604
INFO:root:current mean train loss 3399.263769840615
INFO:root:current train perplexity3.820202112197876


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:14<00:00, 194.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:14<00:00, 194.39s/it]
INFO:root:final mean train loss: 3398.010798423521
INFO:root:final train perplexity: 3.821387529373169
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it]
INFO:root:eval mean loss: 4039.503433552194
INFO:root:eval perplexity: 5.121546745300293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/107

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [6:36:23<6:15:38, 242.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3384.1347212357955
INFO:root:current train perplexity3.8034305572509766
INFO:root:current mean train loss 3387.990810861895
INFO:root:current train perplexity3.799251079559326
INFO:root:current mean train loss 3378.52474341299
INFO:root:current train perplexity3.796903371810913
INFO:root:current mean train loss 3388.4952120928697
INFO:root:current train perplexity3.803420066833496
INFO:root:current mean train loss 3390.7752817007213
INFO:root:current train perplexity3.8004677295684814
INFO:root:current mean train loss 3389.880488985079
INFO:root:current train perplexity3.8048675060272217
INFO:root:current mean train loss 3390.5981217944895
INFO:root:current train perplexity3.8082454204559326
INFO:root:current mean train loss 3394.4641973432326
INFO:root:current train perplexity3.8091745376586914
INFO:root:current mean train loss 3393.9916655244883
INFO:root:current train perplexity3.8094937801361084
INFO:root:current mean train loss 3397.3487713718914
INFO:root:current train perplexity3.8151886463165283


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.64s/it]
INFO:root:final mean train loss: 3393.586973559472
INFO:root:final train perplexity: 3.8147237300872803
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it]
INFO:root:eval mean loss: 4040.5391438802085
INFO:root:eval perplexity: 5.123692512512207
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/108

 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [6:40:45<6:20:24, 248.10s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3340.019333612351
INFO:root:current train perplexity3.7912628650665283
INFO:root:current mean train loss 3360.033180658071
INFO:root:current train perplexity3.797194480895996
INFO:root:current mean train loss 3368.3239504738
INFO:root:current train perplexity3.793461322784424
INFO:root:current mean train loss 3376.7687981555614
INFO:root:current train perplexity3.800507068634033
INFO:root:current mean train loss 3380.972985286177
INFO:root:current train perplexity3.795219659805298
INFO:root:current mean train loss 3385.676500228963
INFO:root:current train perplexity3.8002774715423584
INFO:root:current mean train loss 3388.0045657611
INFO:root:current train perplexity3.8007047176361084
INFO:root:current mean train loss 3388.435611829845
INFO:root:current train perplexity3.8009462356567383
INFO:root:current mean train loss 3388.174197306362
INFO:root:current train perplexity3.8028604984283447
INFO:root:current mean train loss 3390.558913439853
INFO:root:current train perplexity3.8067290782928467


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.43s/it]
INFO:root:final mean train loss: 3388.5749238537205
INFO:root:final train perplexity: 3.8071882724761963
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.71s/it]
INFO:root:eval mean loss: 4041.1486002604165
INFO:root:eval perplexity: 5.124955177307129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/109

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [6:45:29<6:32:35, 258.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3363.84531456316
INFO:root:current train perplexity3.7664875984191895
INFO:root:current mean train loss 3362.941891675804
INFO:root:current train perplexity3.7805235385894775
INFO:root:current mean train loss 3367.3547291210216
INFO:root:current train perplexity3.785580635070801
INFO:root:current mean train loss 3372.1935824576735
INFO:root:current train perplexity3.781773805618286
INFO:root:current mean train loss 3375.1820926220807
INFO:root:current train perplexity3.788780689239502
INFO:root:current mean train loss 3379.5898253646287
INFO:root:current train perplexity3.7958829402923584
INFO:root:current mean train loss 3381.28162003132
INFO:root:current train perplexity3.797593832015991
INFO:root:current mean train loss 3383.1568940752677
INFO:root:current train perplexity3.798426389694214
INFO:root:current mean train loss 3384.280971102271
INFO:root:current train perplexity3.799107789993286
INFO:root:current mean train loss 3387.158603656427
INFO:root:current train perplexity3.8027634620666504


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.12s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.12s/it]
INFO:root:final mean train loss: 3385.3382765246974
INFO:root:final train perplexity: 3.8023293018341064
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it]
INFO:root:eval mean loss: 4043.614777260638
INFO:root:eval perplexity: 5.130068778991699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/110

 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:49:15<6:13:42, 249.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3372.575581610957
INFO:root:current train perplexity3.7709341049194336
INFO:root:current mean train loss 3376.4925543928944
INFO:root:current train perplexity3.773252487182617
INFO:root:current mean train loss 3384.3597626498095
INFO:root:current train perplexity3.7878665924072266
INFO:root:current mean train loss 3386.0614384069922
INFO:root:current train perplexity3.7922708988189697
INFO:root:current mean train loss 3387.2208361536077
INFO:root:current train perplexity3.789135694503784
INFO:root:current mean train loss 3383.1059511280223
INFO:root:current train perplexity3.785121202468872
INFO:root:current mean train loss 3384.147392261713
INFO:root:current train perplexity3.787104606628418
INFO:root:current mean train loss 3380.6586465896785
INFO:root:current train perplexity3.7879745960235596
INFO:root:current mean train loss 3380.570280836711
INFO:root:current train perplexity3.7917280197143555
INFO:root:current mean train loss 3381.9699118500225
INFO:root:current train perplexity3.7938807010650635


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.43s/it]
INFO:root:final mean train loss: 3379.5145970621415
INFO:root:final train perplexity: 3.79360294342041
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it]
INFO:root:eval mean loss: 4043.482601950355
INFO:root:eval perplexity: 5.129794120788574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/111

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [6:54:00<6:25:17, 259.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3383.583642016882
INFO:root:current train perplexity3.774223804473877
INFO:root:current mean train loss 3378.878978056066
INFO:root:current train perplexity3.7764713764190674
INFO:root:current mean train loss 3377.4935817386217
INFO:root:current train perplexity3.781909942626953
INFO:root:current mean train loss 3374.16112839652
INFO:root:current train perplexity3.7771854400634766
INFO:root:current mean train loss 3373.7243542054352
INFO:root:current train perplexity3.7811901569366455
INFO:root:current mean train loss 3374.4169289688034
INFO:root:current train perplexity3.784942626953125
INFO:root:current mean train loss 3378.6706518092706
INFO:root:current train perplexity3.78902268409729
INFO:root:current mean train loss 3376.5924142064405
INFO:root:current train perplexity3.7891845703125
INFO:root:current mean train loss 3378.2940543087657
INFO:root:current train perplexity3.7895734310150146
INFO:root:current mean train loss 3378.934376385195
INFO:root:current train perplexity3.788516044616699


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.11s/it]
INFO:root:final mean train loss: 3375.9868143143194
INFO:root:final train perplexity: 3.788327217102051
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.89s/it]
INFO:root:eval mean loss: 4045.9583835466533
INFO:root:eval perplexity: 5.134932994842529
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/112

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [6:57:42<6:04:19, 248.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3356.885824424342
INFO:root:current train perplexity3.7603297233581543
INFO:root:current mean train loss 3346.9539075020034
INFO:root:current train perplexity3.7675137519836426
INFO:root:current mean train loss 3356.4970612089514
INFO:root:current train perplexity3.7704837322235107
INFO:root:current mean train loss 3365.506665966179
INFO:root:current train perplexity3.7777984142303467
INFO:root:current mean train loss 3366.8951768663196
INFO:root:current train perplexity3.778895854949951
INFO:root:current mean train loss 3365.2640571658353
INFO:root:current train perplexity3.7768900394439697
INFO:root:current mean train loss 3368.879921804744
INFO:root:current train perplexity3.7799344062805176
INFO:root:current mean train loss 3371.8249226120283
INFO:root:current train perplexity3.7819650173187256
INFO:root:current mean train loss 3373.602629353614
INFO:root:current train perplexity3.7844350337982178


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.34s/it]
INFO:root:final mean train loss: 3373.500968994633
INFO:root:final train perplexity: 3.7846133708953857
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it]
INFO:root:eval mean loss: 4045.5460715868794
INFO:root:eval perplexity: 5.134077548980713
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/113

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [7:01:14<5:44:25, 237.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3294.8888346354165
INFO:root:current train perplexity3.7946910858154297
INFO:root:current mean train loss 3349.5256679497875
INFO:root:current train perplexity3.753751754760742
INFO:root:current mean train loss 3366.382854593211
INFO:root:current train perplexity3.772536516189575
INFO:root:current mean train loss 3362.199902021452
INFO:root:current train perplexity3.768561363220215
INFO:root:current mean train loss 3362.427735586616
INFO:root:current train perplexity3.7706058025360107
INFO:root:current mean train loss 3367.1844261578963
INFO:root:current train perplexity3.774714708328247
INFO:root:current mean train loss 3368.7136999734403
INFO:root:current train perplexity3.7757060527801514
INFO:root:current mean train loss 3369.2653944034496
INFO:root:current train perplexity3.7762629985809326
INFO:root:current mean train loss 3372.6140711346125
INFO:root:current train perplexity3.779392957687378
INFO:root:current mean train loss 3371.595326504966
INFO:root:current train perplexity3.777076482772827


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.87s/it]
INFO:root:final mean train loss: 3369.3545177828883
INFO:root:final train perplexity: 3.7784276008605957
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.68s/it]
INFO:root:eval mean loss: 4045.737152316046
INFO:root:eval perplexity: 5.13447380065918
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/114

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [7:04:37<5:25:47, 227.29s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3339.128595525568
INFO:root:current train perplexity3.7676022052764893
INFO:root:current mean train loss 3354.5085625175957
INFO:root:current train perplexity3.774407386779785
INFO:root:current mean train loss 3350.517578125
INFO:root:current train perplexity3.767643451690674
INFO:root:current mean train loss 3359.107973742715
INFO:root:current train perplexity3.7666537761688232
INFO:root:current mean train loss 3360.51990013401
INFO:root:current train perplexity3.766237497329712
INFO:root:current mean train loss 3359.510148796783
INFO:root:current train perplexity3.766037702560425
INFO:root:current mean train loss 3365.067043572908
INFO:root:current train perplexity3.765340566635132
INFO:root:current mean train loss 3364.2465689829464
INFO:root:current train perplexity3.7663207054138184
INFO:root:current mean train loss 3362.761349980252
INFO:root:current train perplexity3.7657034397125244
INFO:root:current mean train loss 3364.183273499674
INFO:root:current train perplexity3.7668561935424805


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.66s/it]
INFO:root:final mean train loss: 3363.6625811669132
INFO:root:final train perplexity: 3.769951820373535
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it]
INFO:root:eval mean loss: 4048.024523146609
INFO:root:eval perplexity: 5.139224529266357
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/115

 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [7:08:53<5:34:10, 235.89s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3310.374383223684
INFO:root:current train perplexity3.716632604598999
INFO:root:current mean train loss 3328.421314912684
INFO:root:current train perplexity3.7448253631591797
INFO:root:current mean train loss 3346.1417843892696
INFO:root:current train perplexity3.751044988632202
INFO:root:current mean train loss 3351.7318164980898
INFO:root:current train perplexity3.7532434463500977
INFO:root:current mean train loss 3351.9571786666543
INFO:root:current train perplexity3.750420331954956
INFO:root:current mean train loss 3354.852118519689
INFO:root:current train perplexity3.7509541511535645
INFO:root:current mean train loss 3356.9600303538973
INFO:root:current train perplexity3.755270004272461
INFO:root:current mean train loss 3360.496882198583
INFO:root:current train perplexity3.757404088973999
INFO:root:current mean train loss 3361.245291275565
INFO:root:current train perplexity3.7588233947753906
INFO:root:current mean train loss 3359.287002048762
INFO:root:current train perplexity3.7605295181274414


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.77s/it]
INFO:root:final mean train loss: 3359.615173155262
INFO:root:final train perplexity: 3.763936758041382
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4048.889303870235
INFO:root:eval perplexity: 5.141022205352783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/116

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [7:13:34<5:49:13, 249.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3359.6942003038193
INFO:root:current train perplexity3.7565670013427734
INFO:root:current mean train loss 3355.747681625246
INFO:root:current train perplexity3.742650032043457
INFO:root:current mean train loss 3355.744280441217
INFO:root:current train perplexity3.741797924041748
INFO:root:current mean train loss 3351.3570231866397
INFO:root:current train perplexity3.743281841278076
INFO:root:current mean train loss 3355.7431137478043
INFO:root:current train perplexity3.7495384216308594
INFO:root:current mean train loss 3356.148157687974
INFO:root:current train perplexity3.7497622966766357
INFO:root:current mean train loss 3356.9457789760268
INFO:root:current train perplexity3.750650644302368
INFO:root:current mean train loss 3357.845380402661
INFO:root:current train perplexity3.753751277923584
INFO:root:current mean train loss 3357.478076939427
INFO:root:current train perplexity3.754211902618408
INFO:root:current mean train loss 3357.933164462817
INFO:root:current train perplexity3.757404088973999


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.71s/it]
INFO:root:final mean train loss: 3355.1653155665244
INFO:root:final train perplexity: 3.7573347091674805
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.23s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.23s/it]
INFO:root:eval mean loss: 4048.1931031139184
INFO:root:eval perplexity: 5.139575481414795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/117

 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [7:17:00<5:27:00, 236.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3348.1942103794645
INFO:root:current train perplexity3.726644992828369
INFO:root:current mean train loss 3342.233405671296
INFO:root:current train perplexity3.7214291095733643
INFO:root:current mean train loss 3342.449462890625
INFO:root:current train perplexity3.733203411102295
INFO:root:current mean train loss 3350.6551546466885
INFO:root:current train perplexity3.741549491882324
INFO:root:current mean train loss 3352.495897876257
INFO:root:current train perplexity3.7421061992645264
INFO:root:current mean train loss 3352.569926438376
INFO:root:current train perplexity3.7436130046844482
INFO:root:current mean train loss 3352.721300596703
INFO:root:current train perplexity3.7478184700012207
INFO:root:current mean train loss 3354.8748232886905
INFO:root:current train perplexity3.7490270137786865
INFO:root:current mean train loss 3354.6636593024887
INFO:root:current train perplexity3.750518798828125
INFO:root:current mean train loss 3354.509384138954
INFO:root:current train perplexity3.7523694038391113


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.59s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:21<00:00, 201.59s/it]
INFO:root:final mean train loss: 3351.380386660176
INFO:root:final train perplexity: 3.7517282962799072
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.29s/it]
INFO:root:eval mean loss: 4052.6330272052305
INFO:root:eval perplexity: 5.148810863494873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/118

 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [7:20:36<5:14:40, 230.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3340.4608693677324
INFO:root:current train perplexity3.7259976863861084
INFO:root:current mean train loss 3332.80578186462
INFO:root:current train perplexity3.718477487564087
INFO:root:current mean train loss 3335.561656057099
INFO:root:current train perplexity3.728398084640503
INFO:root:current mean train loss 3340.3327394713465
INFO:root:current train perplexity3.731748580932617
INFO:root:current mean train loss 3346.892345557633
INFO:root:current train perplexity3.7342352867126465
INFO:root:current mean train loss 3347.5077104375287
INFO:root:current train perplexity3.735626459121704
INFO:root:current mean train loss 3348.7019673254035
INFO:root:current train perplexity3.7388806343078613
INFO:root:current mean train loss 3349.3020880430895
INFO:root:current train perplexity3.7408971786499023
INFO:root:current mean train loss 3348.3875064872477
INFO:root:current train perplexity3.7452945709228516
INFO:root:current mean train loss 3349.455892099682
INFO:root:current train perplexity3.745403528213501


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:15<00:00, 195.18s/it]
INFO:root:final mean train loss: 3347.499088041244
INFO:root:final train perplexity: 3.7459869384765625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.70s/it]
INFO:root:eval mean loss: 4052.544677734375
INFO:root:eval perplexity: 5.148627281188965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/119

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [7:24:30<5:12:20, 231.36s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3352.94242589614
INFO:root:current train perplexity3.743669271469116
INFO:root:current mean train loss 3334.3948562318915
INFO:root:current train perplexity3.724020481109619
INFO:root:current mean train loss 3333.6031479550547
INFO:root:current train perplexity3.7303104400634766
INFO:root:current mean train loss 3328.6135365195423
INFO:root:current train perplexity3.7291901111602783
INFO:root:current mean train loss 3330.511818896376
INFO:root:current train perplexity3.729215383529663
INFO:root:current mean train loss 3335.1282150351635
INFO:root:current train perplexity3.730717420578003
INFO:root:current mean train loss 3339.6690521943406
INFO:root:current train perplexity3.734144449234009
INFO:root:current mean train loss 3343.465902234521
INFO:root:current train perplexity3.7372941970825195
INFO:root:current mean train loss 3343.8733848275924
INFO:root:current train perplexity3.7391552925109863
INFO:root:current mean train loss 3345.7698457318775
INFO:root:current train perplexity3.7401859760284424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:13<00:00, 193.66s/it]
INFO:root:final mean train loss: 3343.817300858036
INFO:root:final train perplexity: 3.7405502796173096
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it]
INFO:root:eval mean loss: 4055.714142495013
INFO:root:eval perplexity: 5.155229568481445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/120

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [7:27:57<4:58:43, 224.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3323.453356726695
INFO:root:current train perplexity3.703524112701416
INFO:root:current mean train loss 3323.4975109940056
INFO:root:current train perplexity3.71028733253479
INFO:root:current mean train loss 3323.331896454211
INFO:root:current train perplexity3.70572829246521
INFO:root:current mean train loss 3330.725637621866
INFO:root:current train perplexity3.7179572582244873
INFO:root:current mean train loss 3332.5150431049155
INFO:root:current train perplexity3.7219345569610596
INFO:root:current mean train loss 3330.9095517944993
INFO:root:current train perplexity3.725170373916626
INFO:root:current mean train loss 3336.882142317313
INFO:root:current train perplexity3.7297699451446533
INFO:root:current mean train loss 3339.693932896389
INFO:root:current train perplexity3.7328755855560303
INFO:root:current mean train loss 3341.631135347697
INFO:root:current train perplexity3.7336819171905518
INFO:root:current mean train loss 3342.2237244607013
INFO:root:current train perplexity3.7347190380096436


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.38s/it]
INFO:root:final mean train loss: 3340.415246778919
INFO:root:final train perplexity: 3.7355329990386963
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.49s/it]
INFO:root:eval mean loss: 4055.9950548537236
INFO:root:eval perplexity: 5.155816078186035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/121

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [7:31:22<4:47:20, 218.24s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3327.0712890625
INFO:root:current train perplexity3.727156400680542
INFO:root:current mean train loss 3334.364507800805
INFO:root:current train perplexity3.725324869155884
INFO:root:current mean train loss 3326.3177065045647
INFO:root:current train perplexity3.7127370834350586
INFO:root:current mean train loss 3326.6086412476584
INFO:root:current train perplexity3.716610908508301
INFO:root:current mean train loss 3330.317326874498
INFO:root:current train perplexity3.715498208999634
INFO:root:current mean train loss 3331.006652509094
INFO:root:current train perplexity3.7179229259490967
INFO:root:current mean train loss 3334.4538640103774
INFO:root:current train perplexity3.720644950866699
INFO:root:current mean train loss 3334.532447466794
INFO:root:current train perplexity3.7220404148101807
INFO:root:current mean train loss 3336.490112727076
INFO:root:current train perplexity3.72385835647583
INFO:root:current mean train loss 3339.0540931804226
INFO:root:current train perplexity3.72865629196167


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.96s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.96s/it]
INFO:root:final mean train loss: 3337.0218976543797
INFO:root:final train perplexity: 3.7305350303649902
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.42s/it]
INFO:root:eval mean loss: 4055.5280900238254
INFO:root:eval perplexity: 5.154841899871826
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/122

 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [7:34:46<4:38:15, 214.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3330.061767578125
INFO:root:current train perplexity3.7124361991882324
INFO:root:current mean train loss 3318.2385909598215
INFO:root:current train perplexity3.7197647094726562
INFO:root:current mean train loss 3327.0423703835227
INFO:root:current train perplexity3.7221434116363525
INFO:root:current mean train loss 3328.069962890625
INFO:root:current train perplexity3.7189838886260986
INFO:root:current mean train loss 3332.753544407895
INFO:root:current train perplexity3.7230162620544434
INFO:root:current mean train loss 3333.9379704483695
INFO:root:current train perplexity3.721102237701416
INFO:root:current mean train loss 3335.676642071759
INFO:root:current train perplexity3.723034143447876
INFO:root:current mean train loss 3336.277541267641
INFO:root:current train perplexity3.7238872051239014
INFO:root:current mean train loss 3334.910580915179
INFO:root:current train perplexity3.7231738567352295
INFO:root:current mean train loss 3334.9365567407854
INFO:root:current train perplexity3.7238547801971436


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.01s/it]
INFO:root:final mean train loss: 3332.8153563468686
INFO:root:final train perplexity: 3.7243494987487793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4058.184485469304
INFO:root:eval perplexity: 5.1603827476501465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/123

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [7:38:09<4:30:30, 210.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3311.153435029179
INFO:root:current train perplexity3.7071146965026855
INFO:root:current mean train loss 3304.4303612213967
INFO:root:current train perplexity3.7033815383911133
INFO:root:current mean train loss 3311.749796405698
INFO:root:current train perplexity3.703868865966797
INFO:root:current mean train loss 3321.5827872572618
INFO:root:current train perplexity3.7131154537200928
INFO:root:current mean train loss 3324.2828123989066
INFO:root:current train perplexity3.718188524246216
INFO:root:current mean train loss 3333.042199895476
INFO:root:current train perplexity3.7197999954223633
INFO:root:current mean train loss 3334.767435501121
INFO:root:current train perplexity3.7204768657684326
INFO:root:current mean train loss 3332.756475494891
INFO:root:current train perplexity3.7172060012817383
INFO:root:current mean train loss 3332.8482139302273
INFO:root:current train perplexity3.7192511558532715
INFO:root:current mean train loss 3332.655471134283
INFO:root:current train perplexity3.7201321125030518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.52s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.52s/it]
INFO:root:final mean train loss: 3329.9765917870304
INFO:root:final train perplexity: 3.720180034637451
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.79s/it]
INFO:root:eval mean loss: 4058.725577280031
INFO:root:eval perplexity: 5.1615118980407715
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/124

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [7:42:24<4:43:55, 224.15s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3302.5765743046018
INFO:root:current train perplexity3.6683552265167236
INFO:root:current mean train loss 3323.8323827613713
INFO:root:current train perplexity3.701850652694702
INFO:root:current mean train loss 3324.353497167633
INFO:root:current train perplexity3.7018356323242188
INFO:root:current mean train loss 3324.7358117457243
INFO:root:current train perplexity3.699967622756958
INFO:root:current mean train loss 3324.243197377005
INFO:root:current train perplexity3.7019708156585693
INFO:root:current mean train loss 3325.626900661617
INFO:root:current train perplexity3.70609974861145
INFO:root:current mean train loss 3327.493920509791
INFO:root:current train perplexity3.708176374435425
INFO:root:current mean train loss 3330.110148780717
INFO:root:current train perplexity3.70927095413208
INFO:root:current mean train loss 3328.8097097822847
INFO:root:current train perplexity3.7126357555389404
INFO:root:current mean train loss 3328.4363108306793
INFO:root:current train perplexity3.7140350341796875


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.37s/it]
INFO:root:final mean train loss: 3325.7510940797865
INFO:root:final train perplexity: 3.7139832973480225
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4057.2366345994014
INFO:root:eval perplexity: 5.15840482711792
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/125

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [7:45:53<4:34:12, 219.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3327.279427576547
INFO:root:current train perplexity3.7051098346710205
INFO:root:current mean train loss 3314.2020797346104
INFO:root:current train perplexity3.700995922088623
INFO:root:current mean train loss 3318.200563564747
INFO:root:current train perplexity3.7032101154327393
INFO:root:current mean train loss 3318.1010815613254
INFO:root:current train perplexity3.702362537384033
INFO:root:current mean train loss 3318.1710393443136
INFO:root:current train perplexity3.7005667686462402
INFO:root:current mean train loss 3320.243820266851
INFO:root:current train perplexity3.7005650997161865
INFO:root:current mean train loss 3320.1300005169214
INFO:root:current train perplexity3.702667236328125
INFO:root:current mean train loss 3323.2973452533442
INFO:root:current train perplexity3.705160140991211
INFO:root:current mean train loss 3324.67165068392
INFO:root:current train perplexity3.7085349559783936


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.80s/it]
INFO:root:final mean train loss: 3321.99722689967
INFO:root:final train perplexity: 3.7084875106811523
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.70s/it]
INFO:root:eval mean loss: 4061.2508293855276
INFO:root:eval perplexity: 5.16678524017334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/126

 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [7:50:52<5:00:01, 243.26s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3325.205636160714
INFO:root:current train perplexity3.754014015197754
INFO:root:current mean train loss 3307.814754307827
INFO:root:current train perplexity3.6876797676086426
INFO:root:current mean train loss 3313.656379736564
INFO:root:current train perplexity3.6904335021972656
INFO:root:current mean train loss 3313.8265122404314
INFO:root:current train perplexity3.692509174346924
INFO:root:current mean train loss 3308.65525304246
INFO:root:current train perplexity3.694944381713867
INFO:root:current mean train loss 3314.04010166266
INFO:root:current train perplexity3.6993961334228516
INFO:root:current mean train loss 3315.5798118629014
INFO:root:current train perplexity3.698011636734009
INFO:root:current mean train loss 3319.671743088092
INFO:root:current train perplexity3.702353000640869
INFO:root:current mean train loss 3320.59784079248
INFO:root:current train perplexity3.702972173690796
INFO:root:current mean train loss 3320.860527332983
INFO:root:current train perplexity3.7018542289733887


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.87s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.87s/it]
INFO:root:final mean train loss: 3318.766907845774
INFO:root:final train perplexity: 3.703763723373413
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.64s/it]
INFO:root:eval mean loss: 4062.3738260472073
INFO:root:eval perplexity: 5.169132232666016
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/127

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [7:55:35<5:10:38, 255.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3277.727962239583
INFO:root:current train perplexity3.706333875656128
INFO:root:current mean train loss 3299.6943210767663
INFO:root:current train perplexity3.7090234756469727
INFO:root:current mean train loss 3304.865353606468
INFO:root:current train perplexity3.7052392959594727
INFO:root:current mean train loss 3311.298763795883
INFO:root:current train perplexity3.699350357055664
INFO:root:current mean train loss 3312.456640625
INFO:root:current train perplexity3.6958699226379395
INFO:root:current mean train loss 3311.605445521086
INFO:root:current train perplexity3.696486234664917
INFO:root:current mean train loss 3313.234248761433
INFO:root:current train perplexity3.6947927474975586
INFO:root:current mean train loss 3314.1296461838942
INFO:root:current train perplexity3.6955857276916504
INFO:root:current mean train loss 3313.547070012941
INFO:root:current train perplexity3.6940581798553467
INFO:root:current mean train loss 3315.663684148736
INFO:root:current train perplexity3.696896553039551


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.09s/it]
INFO:root:final mean train loss: 3314.99024163523
INFO:root:final train perplexity: 3.6982498168945312
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it]
INFO:root:eval mean loss: 4062.3095564605496
INFO:root:eval perplexity: 5.168997764587402
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/128

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [7:59:00<4:48:22, 240.31s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3277.552585767663
INFO:root:current train perplexity3.6889638900756836
INFO:root:current mean train loss 3318.3186412284044
INFO:root:current train perplexity3.692514181137085
INFO:root:current mean train loss 3315.0686702984867
INFO:root:current train perplexity3.6904184818267822
INFO:root:current mean train loss 3314.900549354199
INFO:root:current train perplexity3.6967365741729736
INFO:root:current mean train loss 3312.0357720846264
INFO:root:current train perplexity3.6895885467529297
INFO:root:current mean train loss 3310.061641539944
INFO:root:current train perplexity3.69057297706604
INFO:root:current mean train loss 3311.8643573497693
INFO:root:current train perplexity3.6924612522125244
INFO:root:current mean train loss 3312.755859037323
INFO:root:current train perplexity3.6929450035095215
INFO:root:current mean train loss 3311.0709289328297
INFO:root:current train perplexity3.6909728050231934
INFO:root:current mean train loss 3312.021224099404
INFO:root:current train perplexity3.6909103393554688


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.03s/it]
INFO:root:final mean train loss: 3311.023131155199
INFO:root:final train perplexity: 3.6924657821655273
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it]
INFO:root:eval mean loss: 4064.640952252327
INFO:root:eval perplexity: 5.173872470855713
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/129

 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [8:03:18<4:50:31, 245.51s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3307.4815555695563
INFO:root:current train perplexity3.6542980670928955
INFO:root:current mean train loss 3299.562848506083
INFO:root:current train perplexity3.6749229431152344
INFO:root:current mean train loss 3311.7359275652734
INFO:root:current train perplexity3.6865429878234863
INFO:root:current mean train loss 3312.950341354324
INFO:root:current train perplexity3.6889102458953857
INFO:root:current mean train loss 3310.0436433938153
INFO:root:current train perplexity3.6900243759155273
INFO:root:current mean train loss 3311.73640168903
INFO:root:current train perplexity3.6927623748779297
INFO:root:current mean train loss 3311.1609122734253
INFO:root:current train perplexity3.689387559890747
INFO:root:current mean train loss 3311.9727277220845
INFO:root:current train perplexity3.688701868057251
INFO:root:current mean train loss 3308.4868399095594
INFO:root:current train perplexity3.685847759246826
INFO:root:current mean train loss 3308.6961500780412
INFO:root:current train perplexity3.685816764831543


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.82s/it]
INFO:root:final mean train loss: 3307.566056897563
INFO:root:final train perplexity: 3.6874330043792725
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4065.8753653451904
INFO:root:eval perplexity: 5.176455974578857
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/130

 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [8:07:00<4:38:11, 238.45s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3311.26565630008
INFO:root:current train perplexity3.687074899673462
INFO:root:current mean train loss 3311.8454203434126
INFO:root:current train perplexity3.69048810005188
INFO:root:current mean train loss 3306.6561427415663
INFO:root:current train perplexity3.6794071197509766
INFO:root:current mean train loss 3307.9550766846423
INFO:root:current train perplexity3.6812455654144287
INFO:root:current mean train loss 3308.491972834211
INFO:root:current train perplexity3.6846208572387695
INFO:root:current mean train loss 3302.525677795976
INFO:root:current train perplexity3.682194471359253
INFO:root:current mean train loss 3302.4986096592575
INFO:root:current train perplexity3.6825928688049316
INFO:root:current mean train loss 3302.6366933118024
INFO:root:current train perplexity3.6802937984466553
INFO:root:current mean train loss 3307.3372760540824
INFO:root:current train perplexity3.6840076446533203
INFO:root:current mean train loss 3306.6970381244178
INFO:root:current train perplexity3.6836884021759033


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.84s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.84s/it]
INFO:root:final mean train loss: 3305.127560277139
INFO:root:final train perplexity: 3.683887004852295
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.75s/it]
INFO:root:eval mean loss: 4067.447175587323
INFO:root:eval perplexity: 5.179747581481934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/131

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [8:11:38<4:47:54, 250.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3267.268310546875
INFO:root:current train perplexity3.6719770431518555
INFO:root:current mean train loss 3299.453952088648
INFO:root:current train perplexity3.668659210205078
INFO:root:current mean train loss 3301.115698934084
INFO:root:current train perplexity3.680396556854248
INFO:root:current mean train loss 3297.8655678696864
INFO:root:current train perplexity3.670496940612793
INFO:root:current mean train loss 3300.5753569805647
INFO:root:current train perplexity3.672196388244629
INFO:root:current mean train loss 3300.8474679001943
INFO:root:current train perplexity3.6724765300750732
INFO:root:current mean train loss 3301.4625913923637
INFO:root:current train perplexity3.6755292415618896
INFO:root:current mean train loss 3302.3709787064927
INFO:root:current train perplexity3.677060604095459
INFO:root:current mean train loss 3301.987562087238
INFO:root:current train perplexity3.676901340484619
INFO:root:current mean train loss 3303.6338292799633
INFO:root:current train perplexity3.6781086921691895


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.82s/it]
INFO:root:final mean train loss: 3302.0710679946405
INFO:root:final train perplexity: 3.679447650909424
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it]
INFO:root:eval mean loss: 4067.218588971077
INFO:root:eval perplexity: 5.179268836975098
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/132

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [8:16:01<4:47:56, 254.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3270.7075949928976
INFO:root:current train perplexity3.6539382934570312
INFO:root:current mean train loss 3274.327253969254
INFO:root:current train perplexity3.660229206085205
INFO:root:current mean train loss 3277.4082634420956
INFO:root:current train perplexity3.664661169052124
INFO:root:current mean train loss 3280.4514889139523
INFO:root:current train perplexity3.6610381603240967
INFO:root:current mean train loss 3286.517776656937
INFO:root:current train perplexity3.6630616188049316
INFO:root:current mean train loss 3289.9574113175677
INFO:root:current train perplexity3.6650774478912354
INFO:root:current mean train loss 3292.5199039837785
INFO:root:current train perplexity3.669257640838623
INFO:root:current mean train loss 3294.4438702918046
INFO:root:current train perplexity3.670203685760498
INFO:root:current mean train loss 3297.582685432657
INFO:root:current train perplexity3.6715738773345947
INFO:root:current mean train loss 3299.799175034768
INFO:root:current train perplexity3.6743276119232178


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.30s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.30s/it]
INFO:root:final mean train loss: 3299.7378068739367
INFO:root:final train perplexity: 3.6760611534118652
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it]
INFO:root:eval mean loss: 4068.6284577931074
INFO:root:eval perplexity: 5.182222366333008
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/133

 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [8:19:31<4:28:52, 240.78s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3285.9771399119545
INFO:root:current train perplexity3.6695218086242676
INFO:root:current mean train loss 3285.2320639019363
INFO:root:current train perplexity3.664447546005249
INFO:root:current mean train loss 3290.4171110087927
INFO:root:current train perplexity3.6694014072418213
INFO:root:current mean train loss 3293.860771242252
INFO:root:current train perplexity3.6718833446502686
INFO:root:current mean train loss 3295.206066815436
INFO:root:current train perplexity3.6696271896362305
INFO:root:current mean train loss 3301.8172566225853
INFO:root:current train perplexity3.6717538833618164
INFO:root:current mean train loss 3298.598806620004
INFO:root:current train perplexity3.6681129932403564
INFO:root:current mean train loss 3296.93974206207
INFO:root:current train perplexity3.6672420501708984
INFO:root:current mean train loss 3297.003253039452
INFO:root:current train perplexity3.6682791709899902
INFO:root:current mean train loss 3298.9280048817986
INFO:root:current train perplexity3.6706817150115967


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.82s/it]
INFO:root:final mean train loss: 3296.3697457467356
INFO:root:final train perplexity: 3.671180486679077
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.65s/it]
INFO:root:eval mean loss: 4070.504325271498
INFO:root:eval perplexity: 5.186153411865234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/134

 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [8:24:15<4:39:18, 253.92s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3293.441736355634
INFO:root:current train perplexity3.6646790504455566
INFO:root:current mean train loss 3276.7333042077853
INFO:root:current train perplexity3.6431243419647217
INFO:root:current mean train loss 3278.3843575227743
INFO:root:current train perplexity3.6466517448425293
INFO:root:current mean train loss 3282.538251768868
INFO:root:current train perplexity3.6515586376190186
INFO:root:current mean train loss 3288.196699073613
INFO:root:current train perplexity3.658001184463501
INFO:root:current mean train loss 3294.530069915718
INFO:root:current train perplexity3.661285161972046
INFO:root:current mean train loss 3298.4544745045873
INFO:root:current train perplexity3.6677091121673584
INFO:root:current mean train loss 3296.7901467250326
INFO:root:current train perplexity3.666961431503296
INFO:root:current mean train loss 3296.5943153635367
INFO:root:current train perplexity3.6670680046081543
INFO:root:current mean train loss 3294.834233041404
INFO:root:current train perplexity3.6652374267578125


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.00s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.00s/it]
INFO:root:final mean train loss: 3292.888926967498
INFO:root:final train perplexity: 3.666141986846924
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4071.5574977144283
INFO:root:eval perplexity: 5.188363552093506
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/135

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [8:27:54<4:23:35, 243.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3292.924878856804
INFO:root:current train perplexity3.652435779571533
INFO:root:current mean train loss 3287.9008325331706
INFO:root:current train perplexity3.6680920124053955
INFO:root:current mean train loss 3287.5453025243614
INFO:root:current train perplexity3.662597417831421
INFO:root:current mean train loss 3291.3653226263605
INFO:root:current train perplexity3.66316294670105
INFO:root:current mean train loss 3293.594200564327
INFO:root:current train perplexity3.663799285888672
INFO:root:current mean train loss 3294.851890550788
INFO:root:current train perplexity3.6634392738342285
INFO:root:current mean train loss 3298.10703678721
INFO:root:current train perplexity3.664515495300293
INFO:root:current mean train loss 3297.891736325618
INFO:root:current train perplexity3.6631593704223633
INFO:root:current mean train loss 3296.4778568063853
INFO:root:current train perplexity3.6631829738616943
INFO:root:current mean train loss 3293.497688768833
INFO:root:current train perplexity3.6627612113952637


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.11s/it]
INFO:root:final mean train loss: 3290.820694892637
INFO:root:final train perplexity: 3.663151502609253
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it]
INFO:root:eval mean loss: 4072.687844567265
INFO:root:eval perplexity: 5.190735816955566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/136

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [8:32:21<4:27:01, 250.34s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3263.7478223778735
INFO:root:current train perplexity3.6392786502838135
INFO:root:current mean train loss 3283.336360503008
INFO:root:current train perplexity3.6529200077056885
INFO:root:current mean train loss 3281.558448286422
INFO:root:current train perplexity3.6493735313415527
INFO:root:current mean train loss 3288.6985778020025
INFO:root:current train perplexity3.6540989875793457
INFO:root:current mean train loss 3293.207793750802
INFO:root:current train perplexity3.6577255725860596
INFO:root:current mean train loss 3292.00434586949
INFO:root:current train perplexity3.6552822589874268
INFO:root:current mean train loss 3291.4964505435773
INFO:root:current train perplexity3.6555800437927246
INFO:root:current mean train loss 3288.9100520171537
INFO:root:current train perplexity3.654385566711426
INFO:root:current mean train loss 3288.9897703151423
INFO:root:current train perplexity3.655397891998291
INFO:root:current mean train loss 3289.6782889477267
INFO:root:current train perplexity3.6580193042755127


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.26s/it]
INFO:root:final mean train loss: 3287.370431346278
INFO:root:final train perplexity: 3.6581690311431885
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it]
INFO:root:eval mean loss: 4072.9102601396276
INFO:root:eval perplexity: 5.191202640533447
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/137

 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [8:35:44<4:08:08, 236.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3266.74603464227
INFO:root:current train perplexity3.6348397731781006
INFO:root:current mean train loss 3276.4550280448716
INFO:root:current train perplexity3.6362738609313965
INFO:root:current mean train loss 3279.4013473252116
INFO:root:current train perplexity3.642735719680786
INFO:root:current mean train loss 3279.4829200454906
INFO:root:current train perplexity3.6460728645324707
INFO:root:current mean train loss 3279.0757265033144
INFO:root:current train perplexity3.6478757858276367
INFO:root:current mean train loss 3281.8768854221375
INFO:root:current train perplexity3.6506175994873047
INFO:root:current mean train loss 3280.4315745840827
INFO:root:current train perplexity3.649451494216919
INFO:root:current mean train loss 3284.16296309945
INFO:root:current train perplexity3.652740716934204
INFO:root:current mean train loss 3284.4110174253665
INFO:root:current train perplexity3.651754140853882


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.56s/it]
INFO:root:final mean train loss: 3283.1974781405543
INFO:root:final train perplexity: 3.652151584625244
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.28s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.28s/it]
INFO:root:eval mean loss: 4074.4560754654253
INFO:root:eval perplexity: 5.194447994232178
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/138

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [8:40:14<4:14:42, 246.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3335.804931640625
INFO:root:current train perplexity3.50785756111145
INFO:root:current mean train loss 3254.260455381523
INFO:root:current train perplexity3.6102468967437744
INFO:root:current mean train loss 3260.856199969212
INFO:root:current train perplexity3.6212399005889893
INFO:root:current mean train loss 3266.208326887376
INFO:root:current train perplexity3.6297974586486816
INFO:root:current mean train loss 3272.5123754458746
INFO:root:current train perplexity3.630189895629883
INFO:root:current mean train loss 3274.51687822285
INFO:root:current train perplexity3.6319892406463623
INFO:root:current mean train loss 3276.5213325462532
INFO:root:current train perplexity3.634584426879883
INFO:root:current mean train loss 3277.8741998577525
INFO:root:current train perplexity3.6386313438415527
INFO:root:current mean train loss 3281.0277748725484
INFO:root:current train perplexity3.642611503601074
INFO:root:current mean train loss 3281.571860346155
INFO:root:current train perplexity3.645195722579956


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.90s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.90s/it]
INFO:root:final mean train loss: 3280.588514020366
INFO:root:final train perplexity: 3.6483945846557617
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it]
INFO:root:eval mean loss: 4074.759220204455
INFO:root:eval perplexity: 5.195085525512695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/139

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [8:43:38<3:57:28, 233.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3319.033025568182
INFO:root:current train perplexity3.571199417114258
INFO:root:current mean train loss 3274.6962824641046
INFO:root:current train perplexity3.6393072605133057
INFO:root:current mean train loss 3272.1699288173872
INFO:root:current train perplexity3.631319999694824
INFO:root:current mean train loss 3262.594426685591
INFO:root:current train perplexity3.621377944946289
INFO:root:current mean train loss 3269.193886861314
INFO:root:current train perplexity3.6291353702545166
INFO:root:current mean train loss 3271.3189475102436
INFO:root:current train perplexity3.6371548175811768
INFO:root:current mean train loss 3273.1415368312705
INFO:root:current train perplexity3.636812210083008
INFO:root:current mean train loss 3276.0780309148645
INFO:root:current train perplexity3.6378207206726074
INFO:root:current mean train loss 3278.539421636579
INFO:root:current train perplexity3.6428699493408203
INFO:root:current mean train loss 3280.358835264304
INFO:root:current train perplexity3.64339280128479


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.13s/it]
INFO:root:final mean train loss: 3278.1495123832456
INFO:root:final train perplexity: 3.644885540008545
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it]
INFO:root:eval mean loss: 4076.886017495013
INFO:root:eval perplexity: 5.199554920196533
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/140

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [8:48:49<4:16:47, 256.79s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3251.2229132401317
INFO:root:current train perplexity3.61163330078125
INFO:root:current mean train loss 3251.716530166754
INFO:root:current train perplexity3.6176435947418213
INFO:root:current mean train loss 3253.7163843999715
INFO:root:current train perplexity3.6303205490112305
INFO:root:current mean train loss 3262.774773768123
INFO:root:current train perplexity3.63285756111145
INFO:root:current mean train loss 3267.139052944138
INFO:root:current train perplexity3.6349565982818604
INFO:root:current mean train loss 3269.1180003612717
INFO:root:current train perplexity3.6349754333496094
INFO:root:current mean train loss 3267.597689380553
INFO:root:current train perplexity3.633603811264038
INFO:root:current mean train loss 3270.842964607419
INFO:root:current train perplexity3.6352856159210205
INFO:root:current mean train loss 3272.5174004597834
INFO:root:current train perplexity3.6360795497894287
INFO:root:current mean train loss 3274.4032847141934
INFO:root:current train perplexity3.636528253555298


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.07s/it]
INFO:root:final mean train loss: 3274.384176992601
INFO:root:final train perplexity: 3.639474391937256
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.39s/it]
INFO:root:eval mean loss: 4076.878310616135
INFO:root:eval perplexity: 5.199539661407471
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/141

 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [8:54:00<4:28:34, 273.12s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3227.0580783420137
INFO:root:current train perplexity3.640653610229492
INFO:root:current mean train loss 3267.136034387303
INFO:root:current train perplexity3.6363649368286133
INFO:root:current mean train loss 3266.9257941561123
INFO:root:current train perplexity3.6346352100372314
INFO:root:current mean train loss 3269.9662966480314
INFO:root:current train perplexity3.640686273574829
INFO:root:current mean train loss 3275.094632222446
INFO:root:current train perplexity3.6413447856903076
INFO:root:current mean train loss 3273.754077194764
INFO:root:current train perplexity3.6371500492095947
INFO:root:current mean train loss 3274.510748806943
INFO:root:current train perplexity3.638784170150757
INFO:root:current mean train loss 3274.0567544677397
INFO:root:current train perplexity3.6381986141204834
INFO:root:current mean train loss 3274.7228758879987
INFO:root:current train perplexity3.6393320560455322
INFO:root:current mean train loss 3274.486608346818
INFO:root:current train perplexity3.6371750831604004


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.08s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.08s/it]
INFO:root:final mean train loss: 3272.9731075532973
INFO:root:final train perplexity: 3.637448787689209
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4079.583014738475
INFO:root:eval perplexity: 5.205228328704834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/142

 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [8:59:09<4:34:24, 283.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3329.6820033482145
INFO:root:current train perplexity3.6341469287872314
INFO:root:current mean train loss 3297.517699291088
INFO:root:current train perplexity3.6241226196289062
INFO:root:current mean train loss 3284.561503698471
INFO:root:current train perplexity3.6233675479888916
INFO:root:current mean train loss 3281.041365438433
INFO:root:current train perplexity3.623300075531006
INFO:root:current mean train loss 3274.809094378592
INFO:root:current train perplexity3.6261355876922607
INFO:root:current mean train loss 3278.449759510076
INFO:root:current train perplexity3.629519462585449
INFO:root:current mean train loss 3275.091013702633
INFO:root:current train perplexity3.6267364025115967
INFO:root:current mean train loss 3272.310250252445
INFO:root:current train perplexity3.6275250911712646
INFO:root:current mean train loss 3270.694691476422
INFO:root:current train perplexity3.6297364234924316
INFO:root:current mean train loss 3270.8335344773564
INFO:root:current train perplexity3.6315104961395264


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.01s/it]
INFO:root:final mean train loss: 3269.7315998692666
INFO:root:final train perplexity: 3.6327998638153076
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4079.0323079427085
INFO:root:eval perplexity: 5.204069137573242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/143

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [9:04:14<4:35:51, 290.38s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3253.7466615188955
INFO:root:current train perplexity3.5946850776672363
INFO:root:current mean train loss 3253.52350237653
INFO:root:current train perplexity3.6033377647399902
INFO:root:current mean train loss 3253.4764961902006
INFO:root:current train perplexity3.6084144115448
INFO:root:current mean train loss 3259.419106174836
INFO:root:current train perplexity3.611100435256958
INFO:root:current mean train loss 3259.909825034389
INFO:root:current train perplexity3.6135454177856445
INFO:root:current mean train loss 3262.333756870108
INFO:root:current train perplexity3.6173226833343506
INFO:root:current mean train loss 3265.5865153880736
INFO:root:current train perplexity3.6209630966186523
INFO:root:current mean train loss 3263.227989884758
INFO:root:current train perplexity3.6220242977142334
INFO:root:current mean train loss 3265.5631982479795
INFO:root:current train perplexity3.624781847000122
INFO:root:current mean train loss 3267.342109364644
INFO:root:current train perplexity3.6285557746887207


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.05s/it]
INFO:root:final mean train loss: 3267.2561953759964
INFO:root:final train perplexity: 3.6292543411254883
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.92s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.92s/it]
INFO:root:eval mean loss: 4081.980022024601
INFO:root:eval perplexity: 5.210276126861572
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/144

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [9:09:30<4:38:08, 298.01s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3264.0730219822303
INFO:root:current train perplexity3.5978052616119385
INFO:root:current mean train loss 3256.3269754371895
INFO:root:current train perplexity3.608081102371216
INFO:root:current mean train loss 3257.7027981822707
INFO:root:current train perplexity3.6081390380859375
INFO:root:current mean train loss 3254.137697399172
INFO:root:current train perplexity3.6096208095550537
INFO:root:current mean train loss 3257.556514494699
INFO:root:current train perplexity3.611051321029663
INFO:root:current mean train loss 3259.409507571461
INFO:root:current train perplexity3.6159000396728516
INFO:root:current mean train loss 3263.4773871777793
INFO:root:current train perplexity3.6187167167663574
INFO:root:current mean train loss 3266.118041179469
INFO:root:current train perplexity3.622359037399292
INFO:root:current mean train loss 3265.9996187275083
INFO:root:current train perplexity3.6230995655059814
INFO:root:current mean train loss 3264.7677804202813
INFO:root:current train perplexity3.6234710216522217


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.04s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.04s/it]
INFO:root:final mean train loss: 3263.863778514247
INFO:root:final train perplexity: 3.6243999004364014
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.41s/it]
INFO:root:eval mean loss: 4081.062404767841
INFO:root:eval perplexity: 5.208343505859375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/145

 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [9:14:50<4:39:07, 304.50s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3265.905844478284
INFO:root:current train perplexity3.606332302093506
INFO:root:current mean train loss 3256.788354645735
INFO:root:current train perplexity3.604606866836548
INFO:root:current mean train loss 3252.9358956473216
INFO:root:current train perplexity3.608532667160034
INFO:root:current mean train loss 3253.984081215181
INFO:root:current train perplexity3.611375093460083
INFO:root:current mean train loss 3256.7107215499045
INFO:root:current train perplexity3.612553596496582
INFO:root:current mean train loss 3258.5174955801376
INFO:root:current train perplexity3.6149187088012695
INFO:root:current mean train loss 3263.4832791457225
INFO:root:current train perplexity3.6175689697265625
INFO:root:current mean train loss 3262.635438861269
INFO:root:current train perplexity3.618454933166504
INFO:root:current mean train loss 3263.555533607847
INFO:root:current train perplexity3.61969256401062
INFO:root:current mean train loss 3262.818329334756
INFO:root:current train perplexity3.619798421859741


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it]
INFO:root:final mean train loss: 3260.9716400638704
INFO:root:final train perplexity: 3.6202666759490967
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it]
INFO:root:eval mean loss: 4083.7924839317375
INFO:root:eval perplexity: 5.214095592498779
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/146

 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [9:20:07<4:37:32, 308.37s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3253.8503308652053
INFO:root:current train perplexity3.6067237854003906
INFO:root:current mean train loss 3271.536331633608
INFO:root:current train perplexity3.608633279800415
INFO:root:current mean train loss 3265.043669168422
INFO:root:current train perplexity3.6103386878967285
INFO:root:current mean train loss 3266.1541585064715
INFO:root:current train perplexity3.608288049697876
INFO:root:current mean train loss 3261.7748997298245
INFO:root:current train perplexity3.608153820037842
INFO:root:current mean train loss 3259.5065203200784
INFO:root:current train perplexity3.6105740070343018
INFO:root:current mean train loss 3261.3981721297555
INFO:root:current train perplexity3.6128318309783936
INFO:root:current mean train loss 3260.521227820445
INFO:root:current train perplexity3.6117336750030518
INFO:root:current mean train loss 3261.8124673352795
INFO:root:current train perplexity3.615610361099243
INFO:root:current mean train loss 3262.550853961996
INFO:root:current train perplexity3.6175310611724854


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.65s/it]
INFO:root:final mean train loss: 3259.600092611005
INFO:root:final train perplexity: 3.6183085441589355
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.50s/it]
INFO:root:eval mean loss: 4084.4364940021055
INFO:root:eval perplexity: 5.215454578399658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/147

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [9:25:19<4:33:10, 309.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3234.4064420572918
INFO:root:current train perplexity3.5875353813171387
INFO:root:current mean train loss 3252.3694754464286
INFO:root:current train perplexity3.607823371887207
INFO:root:current mean train loss 3259.3147025923295
INFO:root:current train perplexity3.61543869972229
INFO:root:current mean train loss 3252.8302884114582
INFO:root:current train perplexity3.6085383892059326
INFO:root:current mean train loss 3252.3467105263157
INFO:root:current train perplexity3.6057801246643066
INFO:root:current mean train loss 3254.131810886549
INFO:root:current train perplexity3.606452465057373
INFO:root:current mean train loss 3257.0003276909724
INFO:root:current train perplexity3.6083872318267822
INFO:root:current mean train loss 3258.0301751512097
INFO:root:current train perplexity3.608532667160034
INFO:root:current mean train loss 3261.555558314732
INFO:root:current train perplexity3.6131410598754883
INFO:root:current mean train loss 3259.4429732572116
INFO:root:current train perplexity3.6134140491485596


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it]
INFO:root:final mean train loss: 3256.6505753301803
INFO:root:final train perplexity: 3.6140995025634766
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.74s/it]
INFO:root:eval mean loss: 4084.9470197528813
INFO:root:eval perplexity: 5.216531276702881
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/148

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [9:30:28<4:28:06, 309.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3256.6801875470633
INFO:root:current train perplexity3.609987497329712
INFO:root:current mean train loss 3247.229927104679
INFO:root:current train perplexity3.6041007041931152
INFO:root:current mean train loss 3248.7653178831715
INFO:root:current train perplexity3.6014273166656494
INFO:root:current mean train loss 3251.8780667377205
INFO:root:current train perplexity3.602038621902466
INFO:root:current mean train loss 3252.6584922521997
INFO:root:current train perplexity3.6019623279571533
INFO:root:current mean train loss 3250.9123388588123
INFO:root:current train perplexity3.6036393642425537
INFO:root:current mean train loss 3251.9023905763866
INFO:root:current train perplexity3.6043410301208496
INFO:root:current mean train loss 3251.6596190159044
INFO:root:current train perplexity3.604193925857544
INFO:root:current mean train loss 3252.4132927519818
INFO:root:current train perplexity3.605440616607666
INFO:root:current mean train loss 3254.1881949190933
INFO:root:current train perplexity3.606881618499756


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.62s/it]
INFO:root:final mean train loss: 3252.167369473365
INFO:root:final train perplexity: 3.607712984085083
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.11s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.11s/it]
INFO:root:eval mean loss: 4085.5708146332004
INFO:root:eval perplexity: 5.2178473472595215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/149

 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [9:33:54<3:56:36, 278.35s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3243.3406566577955
INFO:root:current train perplexity3.585825204849243
INFO:root:current mean train loss 3239.285719946417
INFO:root:current train perplexity3.585296392440796
INFO:root:current mean train loss 3243.74424381846
INFO:root:current train perplexity3.593154191970825
INFO:root:current mean train loss 3245.2786849624363
INFO:root:current train perplexity3.5936532020568848
INFO:root:current mean train loss 3245.4320548187693
INFO:root:current train perplexity3.594978094100952
INFO:root:current mean train loss 3244.5026153202994
INFO:root:current train perplexity3.5953762531280518
INFO:root:current mean train loss 3248.3916524398514
INFO:root:current train perplexity3.598367929458618
INFO:root:current mean train loss 3249.0993744938173
INFO:root:current train perplexity3.6024460792541504
INFO:root:current mean train loss 3250.901258954563
INFO:root:current train perplexity3.6033756732940674
INFO:root:current mean train loss 3252.836157497556
INFO:root:current train perplexity3.6050937175750732


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.13s/it]
INFO:root:final mean train loss: 3250.3020933212774
INFO:root:final train perplexity: 3.6050589084625244
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.66s/it]
INFO:root:eval mean loss: 4088.225173841977
INFO:root:eval perplexity: 5.223451137542725
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/150

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [9:37:20<3:33:55, 256.70s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3247.795434816919
INFO:root:current train perplexity3.6153783798217773
INFO:root:current mean train loss 3247.016629779758
INFO:root:current train perplexity3.597256660461426
INFO:root:current mean train loss 3244.9098288239443
INFO:root:current train perplexity3.5939252376556396
INFO:root:current mean train loss 3242.205229871554
INFO:root:current train perplexity3.591217041015625
INFO:root:current mean train loss 3244.456612932897
INFO:root:current train perplexity3.592772960662842
INFO:root:current mean train loss 3245.9748331366077
INFO:root:current train perplexity3.597212076187134
INFO:root:current mean train loss 3245.6214032043545
INFO:root:current train perplexity3.5995399951934814
INFO:root:current mean train loss 3243.4285465474422
INFO:root:current train perplexity3.598482370376587
INFO:root:current mean train loss 3248.05788712719
INFO:root:current train perplexity3.6005563735961914


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it]
INFO:root:final mean train loss: 3248.3783263544883
INFO:root:final train perplexity: 3.6023247241973877
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it]
INFO:root:eval mean loss: 4088.2272291251106
INFO:root:eval perplexity: 5.223455429077148
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/151

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [9:40:44<3:16:31, 240.64s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3218.1802106584823
INFO:root:current train perplexity3.567753791809082
INFO:root:current mean train loss 3243.342586339077
INFO:root:current train perplexity3.5915238857269287
INFO:root:current mean train loss 3241.034232761549
INFO:root:current train perplexity3.59293794631958
INFO:root:current mean train loss 3240.621327552423
INFO:root:current train perplexity3.597989559173584
INFO:root:current mean train loss 3241.179967032018
INFO:root:current train perplexity3.597179889678955
INFO:root:current mean train loss 3243.0830087755794
INFO:root:current train perplexity3.594465970993042
INFO:root:current mean train loss 3242.9927847797826
INFO:root:current train perplexity3.595170259475708
INFO:root:current mean train loss 3247.589421770023
INFO:root:current train perplexity3.5955281257629395
INFO:root:current mean train loss 3248.7690980289653
INFO:root:current train perplexity3.597182512283325
INFO:root:current mean train loss 3251.477105423529
INFO:root:current train perplexity3.5994343757629395


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.55s/it]
INFO:root:final mean train loss: 3246.179707680979
INFO:root:final train perplexity: 3.599200963973999
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.37s/it]
INFO:root:eval mean loss: 4089.5916825964096
INFO:root:eval perplexity: 5.226337909698486
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/152

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [9:44:06<3:03:25, 229.28s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3259.553287760417
INFO:root:current train perplexity3.590013265609741
INFO:root:current mean train loss 3241.2340395720107
INFO:root:current train perplexity3.5863795280456543
INFO:root:current mean train loss 3239.892626953125
INFO:root:current train perplexity3.5881526470184326
INFO:root:current mean train loss 3233.548372395833
INFO:root:current train perplexity3.583918333053589
INFO:root:current mean train loss 3238.529415121423
INFO:root:current train perplexity3.5852725505828857
INFO:root:current mean train loss 3241.7727083965415
INFO:root:current train perplexity3.5896859169006348
INFO:root:current mean train loss 3242.777348910696
INFO:root:current train perplexity3.587066650390625
INFO:root:current mean train loss 3245.6047824246066
INFO:root:current train perplexity3.5913641452789307
INFO:root:current mean train loss 3244.9431868289876
INFO:root:current train perplexity3.593639612197876
INFO:root:current mean train loss 3245.530239551315
INFO:root:current train perplexity3.5950708389282227


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.54s/it]
INFO:root:final mean train loss: 3243.7612700923796
INFO:root:final train perplexity: 3.595768451690674
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.47s/it]
INFO:root:eval mean loss: 4089.8119389960107
INFO:root:eval perplexity: 5.226803302764893
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/153

 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [9:47:30<2:53:37, 221.65s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3198.7157990828805
INFO:root:current train perplexity3.550082206726074
INFO:root:current mean train loss 3231.19391117251
INFO:root:current train perplexity3.5785720348358154
INFO:root:current mean train loss 3230.8114610338425
INFO:root:current train perplexity3.573352098464966
INFO:root:current mean train loss 3232.735978164909
INFO:root:current train perplexity3.5752832889556885
INFO:root:current mean train loss 3234.4897120410387
INFO:root:current train perplexity3.5797371864318848
INFO:root:current mean train loss 3237.616073229117
INFO:root:current train perplexity3.5818839073181152
INFO:root:current mean train loss 3237.8199927267256
INFO:root:current train perplexity3.585271120071411
INFO:root:current mean train loss 3241.1062683696405
INFO:root:current train perplexity3.5886523723602295
INFO:root:current mean train loss 3244.3646556163617
INFO:root:current train perplexity3.5903117656707764
INFO:root:current mean train loss 3244.5432824561553
INFO:root:current train perplexity3.591779947280884


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.45s/it]
INFO:root:final mean train loss: 3241.739136603571
INFO:root:final train perplexity: 3.5929007530212402
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.75s/it]
INFO:root:eval mean loss: 4090.7608997534353
INFO:root:eval perplexity: 5.228808879852295
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/154

 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [9:50:53<2:45:39, 216.07s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3260.3957598286293
INFO:root:current train perplexity3.611862897872925
INFO:root:current mean train loss 3235.711999791269
INFO:root:current train perplexity3.588777542114258
INFO:root:current mean train loss 3232.728843259605
INFO:root:current train perplexity3.577793598175049
INFO:root:current mean train loss 3235.416795989898
INFO:root:current train perplexity3.5827624797821045
INFO:root:current mean train loss 3232.9327015887834
INFO:root:current train perplexity3.5862700939178467
INFO:root:current mean train loss 3232.4665665276307
INFO:root:current train perplexity3.5857701301574707
INFO:root:current mean train loss 3237.2083691870544
INFO:root:current train perplexity3.5899877548217773
INFO:root:current mean train loss 3240.1445365937075
INFO:root:current train perplexity3.5926625728607178
INFO:root:current mean train loss 3242.5424349310883
INFO:root:current train perplexity3.5899012088775635
INFO:root:current mean train loss 3240.787640662762
INFO:root:current train perplexity3.5890331268310547


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.63s/it]
INFO:root:final mean train loss: 3239.0645606133244
INFO:root:final train perplexity: 3.589111328125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.62s/it]
INFO:root:eval mean loss: 4091.004931294326
INFO:root:eval perplexity: 5.229325771331787
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/155

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [9:54:16<2:39:08, 212.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3285.943171574519
INFO:root:current train perplexity3.601456880569458
INFO:root:current mean train loss 3239.3420023746626
INFO:root:current train perplexity3.576413631439209
INFO:root:current mean train loss 3244.297184517194
INFO:root:current train perplexity3.5864107608795166
INFO:root:current mean train loss 3240.9782066682797
INFO:root:current train perplexity3.5888512134552
INFO:root:current mean train loss 3236.658362734019
INFO:root:current train perplexity3.590646266937256
INFO:root:current mean train loss 3237.697076744405
INFO:root:current train perplexity3.5876996517181396
INFO:root:current mean train loss 3237.669560821963
INFO:root:current train perplexity3.5859999656677246
INFO:root:current mean train loss 3239.3692189217904
INFO:root:current train perplexity3.585883378982544
INFO:root:current mean train loss 3239.888005798849
INFO:root:current train perplexity3.584665298461914
INFO:root:current mean train loss 3241.48036318973
INFO:root:current train perplexity3.5888781547546387


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.40s/it]
INFO:root:final mean train loss: 3238.9304397952174
INFO:root:final train perplexity: 3.5889217853546143
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.67s/it]
INFO:root:eval mean loss: 4092.1692777593084
INFO:root:eval perplexity: 5.231788158416748
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/156

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [9:57:41<2:33:59, 210.00s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3227.949436918218
INFO:root:current train perplexity3.535963535308838
INFO:root:current mean train loss 3238.3245957562713
INFO:root:current train perplexity3.563941240310669
INFO:root:current mean train loss 3231.94262003416
INFO:root:current train perplexity3.5695106983184814
INFO:root:current mean train loss 3232.632636606178
INFO:root:current train perplexity3.571119546890259
INFO:root:current mean train loss 3234.792100876503
INFO:root:current train perplexity3.575888156890869
INFO:root:current mean train loss 3230.3946200689843
INFO:root:current train perplexity3.575195074081421
INFO:root:current mean train loss 3230.727398691074
INFO:root:current train perplexity3.5769901275634766
INFO:root:current mean train loss 3233.5048883685786
INFO:root:current train perplexity3.578716993331909
INFO:root:current mean train loss 3236.699144671912
INFO:root:current train perplexity3.580782175064087
INFO:root:current mean train loss 3237.5275829923444
INFO:root:current train perplexity3.5821170806884766


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.48s/it]
INFO:root:final mean train loss: 3235.000595523465
INFO:root:final train perplexity: 3.5833616256713867
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4092.69381129488
INFO:root:eval perplexity: 5.232897758483887
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/157

 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [10:01:04<2:28:55, 207.81s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3211.9362571022725
INFO:root:current train perplexity3.594802141189575
INFO:root:current mean train loss 3205.3878606980848
INFO:root:current train perplexity3.568965196609497
INFO:root:current mean train loss 3220.3734355851716
INFO:root:current train perplexity3.5743277072906494
INFO:root:current mean train loss 3226.8129559584067
INFO:root:current train perplexity3.579371929168701
INFO:root:current mean train loss 3227.8216083233174
INFO:root:current train perplexity3.5784525871276855
INFO:root:current mean train loss 3231.4294028892173
INFO:root:current train perplexity3.5783417224884033
INFO:root:current mean train loss 3235.515522125477
INFO:root:current train perplexity3.581773519515991
INFO:root:current mean train loss 3235.9126733236753
INFO:root:current train perplexity3.5817935466766357
INFO:root:current mean train loss 3237.791124703034
INFO:root:current train perplexity3.5837974548339844
INFO:root:current mean train loss 3236.731628865347
INFO:root:current train perplexity3.5819053649902344


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.60s/it]
INFO:root:final mean train loss: 3234.4707796035273
INFO:root:final train perplexity: 3.5826125144958496
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4095.028496924867
INFO:root:eval perplexity: 5.2378411293029785
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/158

 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [10:04:29<2:24:51, 206.93s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3251.605162605407
INFO:root:current train perplexity3.5811898708343506
INFO:root:current mean train loss 3244.768115833493
INFO:root:current train perplexity3.5753650665283203
INFO:root:current mean train loss 3238.374490368049
INFO:root:current train perplexity3.572051525115967
INFO:root:current mean train loss 3229.524010524277
INFO:root:current train perplexity3.572751522064209
INFO:root:current mean train loss 3232.4171187398756
INFO:root:current train perplexity3.579055070877075
INFO:root:current mean train loss 3233.6271790526475
INFO:root:current train perplexity3.5783698558807373
INFO:root:current mean train loss 3232.495899321267
INFO:root:current train perplexity3.5813071727752686
INFO:root:current mean train loss 3236.302295049865
INFO:root:current train perplexity3.580799102783203
INFO:root:current mean train loss 3235.1328506911755
INFO:root:current train perplexity3.5806257724761963
INFO:root:current mean train loss 3234.709846599575
INFO:root:current train perplexity3.5792269706726074


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.37s/it]
INFO:root:final mean train loss: 3232.4270941826603
INFO:root:final train perplexity: 3.5797250270843506
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.93s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.93s/it]
INFO:root:eval mean loss: 4095.5665309175533
INFO:root:eval perplexity: 5.238980293273926
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/159

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [10:07:53<2:20:49, 206.09s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3231.50937706316
INFO:root:current train perplexity3.576692581176758
INFO:root:current mean train loss 3221.1577876576207
INFO:root:current train perplexity3.5607471466064453
INFO:root:current mean train loss 3224.273213178909
INFO:root:current train perplexity3.5643351078033447
INFO:root:current mean train loss 3220.8012478152377
INFO:root:current train perplexity3.569249391555786
INFO:root:current mean train loss 3226.023977097432
INFO:root:current train perplexity3.571972608566284
INFO:root:current mean train loss 3225.5287966218802
INFO:root:current train perplexity3.572920322418213
INFO:root:current mean train loss 3227.7016998154572
INFO:root:current train perplexity3.5709943771362305
INFO:root:current mean train loss 3229.5812703292195
INFO:root:current train perplexity3.571871757507324
INFO:root:current mean train loss 3231.1244200609035
INFO:root:current train perplexity3.5757124423980713
INFO:root:current mean train loss 3231.0369580128413
INFO:root:current train perplexity3.5751984119415283


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.39s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.39s/it]
INFO:root:final mean train loss: 3229.073396867321
INFO:root:final train perplexity: 3.5749921798706055
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it]
INFO:root:eval mean loss: 4096.244857463431
INFO:root:eval perplexity: 5.240417957305908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/160

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [10:11:16<2:16:41, 205.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3223.4337173655063
INFO:root:current train perplexity3.556142568588257
INFO:root:current mean train loss 3217.6953970626746
INFO:root:current train perplexity3.5675065517425537
INFO:root:current mean train loss 3223.0705767669133
INFO:root:current train perplexity3.5689706802368164
INFO:root:current mean train loss 3229.212950532858
INFO:root:current train perplexity3.568133592605591
INFO:root:current mean train loss 3228.802473414666
INFO:root:current train perplexity3.569178342819214
INFO:root:current mean train loss 3230.1959976960547
INFO:root:current train perplexity3.569166421890259
INFO:root:current mean train loss 3225.8419468111424
INFO:root:current train perplexity3.5692174434661865
INFO:root:current mean train loss 3227.2202872397506
INFO:root:current train perplexity3.5698859691619873
INFO:root:current mean train loss 3226.2376115992074
INFO:root:current train perplexity3.5709261894226074
INFO:root:current mean train loss 3228.609404177174
INFO:root:current train perplexity3.5719387531280518


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.77s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.77s/it]
INFO:root:final mean train loss: 3226.7548642312327
INFO:root:final train perplexity: 3.5717227458953857
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it]
INFO:root:eval mean loss: 4096.9797917359265
INFO:root:eval perplexity: 5.241974353790283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/161

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [10:14:40<2:13:04, 204.74s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3213.8573938128593
INFO:root:current train perplexity3.5387015342712402
INFO:root:current mean train loss 3214.2759246010196
INFO:root:current train perplexity3.5478250980377197
INFO:root:current mean train loss 3221.669562044044
INFO:root:current train perplexity3.5534040927886963
INFO:root:current mean train loss 3227.154279841933
INFO:root:current train perplexity3.5622408390045166
INFO:root:current mean train loss 3224.8967565892904
INFO:root:current train perplexity3.562896966934204
INFO:root:current mean train loss 3226.955758557815
INFO:root:current train perplexity3.563751697540283
INFO:root:current mean train loss 3225.4710012821824
INFO:root:current train perplexity3.5636518001556396
INFO:root:current mean train loss 3226.354036168798
INFO:root:current train perplexity3.565520763397217
INFO:root:current mean train loss 3226.282212249859
INFO:root:current train perplexity3.5670506954193115
INFO:root:current mean train loss 3228.6383035615345
INFO:root:current train perplexity3.5709331035614014


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.60s/it]
INFO:root:final mean train loss: 3225.8509411965647
INFO:root:final train perplexity: 3.5704498291015625
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4096.43530966035
INFO:root:eval perplexity: 5.24082088470459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/162

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [10:18:02<2:09:18, 204.18s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3222.1035593133224
INFO:root:current train perplexity3.5685198307037354
INFO:root:current mean train loss 3228.6375763721953
INFO:root:current train perplexity3.5732388496398926
INFO:root:current mean train loss 3223.918020060911
INFO:root:current train perplexity3.5622718334198
INFO:root:current mean train loss 3228.4872410255143
INFO:root:current train perplexity3.5616085529327393
INFO:root:current mean train loss 3229.6195795849117
INFO:root:current train perplexity3.564077854156494
INFO:root:current mean train loss 3228.8157694327733
INFO:root:current train perplexity3.5638511180877686
INFO:root:current mean train loss 3227.432816012815
INFO:root:current train perplexity3.5620944499969482
INFO:root:current mean train loss 3225.671005613699
INFO:root:current train perplexity3.563754081726074
INFO:root:current mean train loss 3226.813642960021
INFO:root:current train perplexity3.5662386417388916


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.15s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.15s/it]
INFO:root:final mean train loss: 3223.7136419973067
INFO:root:final train perplexity: 3.5674402713775635
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.44s/it]
INFO:root:eval mean loss: 4097.36679964539
INFO:root:eval perplexity: 5.242796421051025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/163

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [10:21:27<2:05:57, 204.27s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3210.8299153645835
INFO:root:current train perplexity3.5282328128814697
INFO:root:current mean train loss 3196.814121283374
INFO:root:current train perplexity3.5621817111968994
INFO:root:current mean train loss 3221.648110375616
INFO:root:current train perplexity3.575892210006714
INFO:root:current mean train loss 3219.0849617432445
INFO:root:current train perplexity3.5714614391326904
INFO:root:current mean train loss 3218.269038122286
INFO:root:current train perplexity3.572352886199951
INFO:root:current mean train loss 3220.162772389103
INFO:root:current train perplexity3.5709495544433594
INFO:root:current mean train loss 3221.668997541589
INFO:root:current train perplexity3.5672607421875
INFO:root:current mean train loss 3220.6207113903583
INFO:root:current train perplexity3.5636234283447266
INFO:root:current mean train loss 3220.843934245602
INFO:root:current train perplexity3.564103364944458
INFO:root:current mean train loss 3225.2380038543397
INFO:root:current train perplexity3.5660455226898193


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.01s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.01s/it]
INFO:root:final mean train loss: 3222.6979816805933
INFO:root:final train perplexity: 3.5660111904144287
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.66s/it]
INFO:root:eval mean loss: 4097.9727930380095
INFO:root:eval perplexity: 5.244080066680908
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/164

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [10:24:50<2:02:15, 203.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3241.306551846591
INFO:root:current train perplexity3.555114984512329
INFO:root:current mean train loss 3217.7690253730293
INFO:root:current train perplexity3.5416438579559326
INFO:root:current mean train loss 3220.197178845157
INFO:root:current train perplexity3.5485005378723145
INFO:root:current mean train loss 3222.8256616132435
INFO:root:current train perplexity3.559659004211426
INFO:root:current mean train loss 3227.4713820854245
INFO:root:current train perplexity3.5658535957336426
INFO:root:current mean train loss 3224.364467553663
INFO:root:current train perplexity3.566467046737671
INFO:root:current mean train loss 3221.7761478205553
INFO:root:current train perplexity3.563194751739502
INFO:root:current mean train loss 3220.4774947669434
INFO:root:current train perplexity3.5618417263031006
INFO:root:current mean train loss 3221.354739639527
INFO:root:current train perplexity3.561020612716675
INFO:root:current mean train loss 3221.604997620232
INFO:root:current train perplexity3.561145067214966


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.73s/it]
INFO:root:final mean train loss: 3218.6261818793514
INFO:root:final train perplexity: 3.5602872371673584
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.55s/it]
INFO:root:eval mean loss: 4099.372821780807
INFO:root:eval perplexity: 5.247049808502197
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/165

 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [10:28:13<1:58:45, 203.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3231.738898026316
INFO:root:current train perplexity3.566837787628174
INFO:root:current mean train loss 3218.839148256959
INFO:root:current train perplexity3.558152914047241
INFO:root:current mean train loss 3221.59805534746
INFO:root:current train perplexity3.563422918319702
INFO:root:current mean train loss 3223.202482121865
INFO:root:current train perplexity3.5613865852355957
INFO:root:current mean train loss 3220.6894175818543
INFO:root:current train perplexity3.5616910457611084
INFO:root:current mean train loss 3218.450571166757
INFO:root:current train perplexity3.5608513355255127
INFO:root:current mean train loss 3220.391910386586
INFO:root:current train perplexity3.5606424808502197
INFO:root:current mean train loss 3222.307929578842
INFO:root:current train perplexity3.5602505207061768
INFO:root:current mean train loss 3222.801323486626
INFO:root:current train perplexity3.562337636947632
INFO:root:current mean train loss 3220.8990384206677
INFO:root:current train perplexity3.5592501163482666


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 3218.0036143026045
INFO:root:final train perplexity: 3.559412956237793
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.33s/it]
INFO:root:eval mean loss: 4101.369402080563
INFO:root:eval perplexity: 5.251287937164307
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/166

 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [10:31:35<1:55:05, 203.11s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3207.902660228588
INFO:root:current train perplexity3.5567805767059326
INFO:root:current mean train loss 3214.6119432824803
INFO:root:current train perplexity3.5469138622283936
INFO:root:current mean train loss 3220.914941191148
INFO:root:current train perplexity3.5577521324157715
INFO:root:current mean train loss 3215.2972281453076
INFO:root:current train perplexity3.5535993576049805
INFO:root:current mean train loss 3217.758198436585
INFO:root:current train perplexity3.5527467727661133
INFO:root:current mean train loss 3220.5570663654826
INFO:root:current train perplexity3.5521442890167236
INFO:root:current mean train loss 3220.1093189294256
INFO:root:current train perplexity3.5534799098968506
INFO:root:current mean train loss 3221.1217250902682
INFO:root:current train perplexity3.555281639099121
INFO:root:current mean train loss 3218.2563228584113
INFO:root:current train perplexity3.5557148456573486
INFO:root:current mean train loss 3219.3762394021373
INFO:root:current train perplexity3.557640552520752


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.03s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.03s/it]
INFO:root:final mean train loss: 3216.567367553711
INFO:root:final train perplexity: 3.557396411895752
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.48s/it]
INFO:root:eval mean loss: 4101.0087890625
INFO:root:eval perplexity: 5.250523567199707
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/167

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [10:34:58<1:51:45, 203.20s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3246.6112862723216
INFO:root:current train perplexity3.551460027694702
INFO:root:current mean train loss 3214.7146122685185
INFO:root:current train perplexity3.551285982131958
INFO:root:current mean train loss 3205.506148188165
INFO:root:current train perplexity3.5461764335632324
INFO:root:current mean train loss 3210.5829946944964
INFO:root:current train perplexity3.5458810329437256
INFO:root:current mean train loss 3212.2785515445403
INFO:root:current train perplexity3.5471620559692383
INFO:root:current mean train loss 3215.9664678555782
INFO:root:current train perplexity3.5486114025115967
INFO:root:current mean train loss 3215.3741268608514
INFO:root:current train perplexity3.5506319999694824
INFO:root:current mean train loss 3216.5939287042943
INFO:root:current train perplexity3.550072431564331
INFO:root:current mean train loss 3216.5076259590196
INFO:root:current train perplexity3.550208806991577
INFO:root:current mean train loss 3215.7923402510864
INFO:root:current train perplexity3.55157208442688


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.47s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.47s/it]
INFO:root:final mean train loss: 3213.2227322363083
INFO:root:final train perplexity: 3.5527052879333496
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it]
INFO:root:eval mean loss: 4101.7086553911795
INFO:root:eval perplexity: 5.252007961273193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/168

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [10:38:22<1:48:26, 203.33s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3226.8499443586484
INFO:root:current train perplexity3.538743257522583
INFO:root:current mean train loss 3233.9364261227056
INFO:root:current train perplexity3.5637638568878174
INFO:root:current mean train loss 3217.7936559606483
INFO:root:current train perplexity3.5581750869750977
INFO:root:current mean train loss 3212.29983315871
INFO:root:current train perplexity3.551158905029297
INFO:root:current mean train loss 3213.340677575656
INFO:root:current train perplexity3.551760196685791
INFO:root:current mean train loss 3211.055470278689
INFO:root:current train perplexity3.5470046997070312
INFO:root:current mean train loss 3216.2292028637735
INFO:root:current train perplexity3.5489490032196045
INFO:root:current mean train loss 3218.0299004905155
INFO:root:current train perplexity3.553997755050659
INFO:root:current mean train loss 3219.329391751001
INFO:root:current train perplexity3.5549964904785156
INFO:root:current mean train loss 3217.6353890509013
INFO:root:current train perplexity3.554590940475464


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.75s/it]
INFO:root:final mean train loss: 3214.316128823065
INFO:root:final train perplexity: 3.5542378425598145
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.37s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.37s/it]
INFO:root:eval mean loss: 4102.207933358267
INFO:root:eval perplexity: 5.253068447113037
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/169

 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [10:41:47<1:45:18, 203.83s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3217.9820149739585
INFO:root:current train perplexity3.5338940620422363
INFO:root:current mean train loss 3203.099853515625
INFO:root:current train perplexity3.531877040863037
INFO:root:current mean train loss 3207.050538082047
INFO:root:current train perplexity3.5448315143585205
INFO:root:current mean train loss 3207.5238026676016
INFO:root:current train perplexity3.544527769088745
INFO:root:current mean train loss 3203.0771771280833
INFO:root:current train perplexity3.541079521179199
INFO:root:current mean train loss 3203.80143480249
INFO:root:current train perplexity3.542738676071167
INFO:root:current mean train loss 3207.9946022795457
INFO:root:current train perplexity3.5452871322631836
INFO:root:current mean train loss 3211.286944230609
INFO:root:current train perplexity3.54887318611145
INFO:root:current mean train loss 3212.78807532269
INFO:root:current train perplexity3.548370361328125
INFO:root:current mean train loss 3213.134822103378
INFO:root:current train perplexity3.5489554405212402


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.34s/it]
INFO:root:final mean train loss: 3211.5499949301443
INFO:root:final train perplexity: 3.5503616333007812
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.53s/it]
INFO:root:eval mean loss: 4102.950474083001
INFO:root:eval perplexity: 5.254646301269531
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/170

 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [10:45:10<1:41:53, 203.80s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3207.259877350371
INFO:root:current train perplexity3.551858425140381
INFO:root:current mean train loss 3193.3517129766115
INFO:root:current train perplexity3.52662992477417
INFO:root:current mean train loss 3192.1484959429295
INFO:root:current train perplexity3.5336363315582275
INFO:root:current mean train loss 3202.8117770989293
INFO:root:current train perplexity3.5448215007781982
INFO:root:current mean train loss 3203.0038929525804
INFO:root:current train perplexity3.5438010692596436
INFO:root:current mean train loss 3205.4672353672854
INFO:root:current train perplexity3.546698570251465
INFO:root:current mean train loss 3208.85694174412
INFO:root:current train perplexity3.5480992794036865
INFO:root:current mean train loss 3210.992859771286
INFO:root:current train perplexity3.548722267150879
INFO:root:current mean train loss 3211.5908399233303
INFO:root:current train perplexity3.5490291118621826
INFO:root:current mean train loss 3212.1225458648332
INFO:root:current train perplexity3.5489392280578613


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.13s/it]
INFO:root:final mean train loss: 3210.361261367798
INFO:root:final train perplexity: 3.548696994781494
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4102.325313054078
INFO:root:eval perplexity: 5.253319263458252
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/171

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [10:48:35<1:38:35, 203.98s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3238.640322557136
INFO:root:current train perplexity3.5408666133880615
INFO:root:current mean train loss 3221.9456268127806
INFO:root:current train perplexity3.548813581466675
INFO:root:current mean train loss 3213.7729528762875
INFO:root:current train perplexity3.5525476932525635
INFO:root:current mean train loss 3213.4813834458023
INFO:root:current train perplexity3.5532774925231934
INFO:root:current mean train loss 3206.2983346158994
INFO:root:current train perplexity3.5487124919891357
INFO:root:current mean train loss 3204.6248480041613
INFO:root:current train perplexity3.544523239135742
INFO:root:current mean train loss 3205.0173420369892
INFO:root:current train perplexity3.5448873043060303
INFO:root:current mean train loss 3207.3092163563397
INFO:root:current train perplexity3.545293092727661
INFO:root:current mean train loss 3210.29960306733
INFO:root:current train perplexity3.5459511280059814
INFO:root:current mean train loss 3210.4277144296957
INFO:root:current train perplexity3.5458836555480957


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.99s/it]
INFO:root:final mean train loss: 3208.261736962103
INFO:root:final train perplexity: 3.5457584857940674
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4104.427536984707
INFO:root:eval perplexity: 5.257785320281982
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/172

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [10:51:58<1:35:05, 203.75s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3200.343359375
INFO:root:current train perplexity3.551401138305664
INFO:root:current mean train loss 3199.069086216518
INFO:root:current train perplexity3.5378360748291016
INFO:root:current mean train loss 3209.4565562855114
INFO:root:current train perplexity3.5423152446746826
INFO:root:current mean train loss 3205.505185546875
INFO:root:current train perplexity3.53913950920105
INFO:root:current mean train loss 3205.8542367393093
INFO:root:current train perplexity3.539716958999634
INFO:root:current mean train loss 3206.927377292799
INFO:root:current train perplexity3.5393807888031006
INFO:root:current mean train loss 3205.3350611255787
INFO:root:current train perplexity3.540614604949951
INFO:root:current mean train loss 3207.909107862903
INFO:root:current train perplexity3.5427956581115723
INFO:root:current mean train loss 3209.7638727678573
INFO:root:current train perplexity3.543267011642456
INFO:root:current mean train loss 3209.32762895633
INFO:root:current train perplexity3.543069839477539


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.16s/it]
INFO:root:final mean train loss: 3206.4740796858264
INFO:root:final train perplexity: 3.5432586669921875
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.76s/it]
INFO:root:eval mean loss: 4104.071278673538
INFO:root:eval perplexity: 5.257028579711914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/173

 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [10:55:23<1:31:49, 204.06s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3207.0764483716116
INFO:root:current train perplexity3.5583808422088623
INFO:root:current mean train loss 3210.539452057719
INFO:root:current train perplexity3.5402348041534424
INFO:root:current mean train loss 3207.003458515073
INFO:root:current train perplexity3.540921211242676
INFO:root:current mean train loss 3207.489794539409
INFO:root:current train perplexity3.536198616027832
INFO:root:current mean train loss 3216.391757246377
INFO:root:current train perplexity3.543238401412964
INFO:root:current mean train loss 3210.062421690743
INFO:root:current train perplexity3.541306495666504
INFO:root:current mean train loss 3208.404668269011
INFO:root:current train perplexity3.5405356884002686
INFO:root:current mean train loss 3209.5029028725653
INFO:root:current train perplexity3.541905164718628
INFO:root:current mean train loss 3211.083968338583
INFO:root:current train perplexity3.5439705848693848
INFO:root:current mean train loss 3209.071972308542
INFO:root:current train perplexity3.543306350708008


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:12<00:00, 192.16s/it]
INFO:root:final mean train loss: 3206.7918995888003
INFO:root:final train perplexity: 3.543703079223633
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.82s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.82s/it]
INFO:root:eval mean loss: 4105.479563178746
INFO:root:eval perplexity: 5.26002311706543
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/174

 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [10:58:49<1:28:39, 204.60s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3191.05033321171
INFO:root:current train perplexity3.5354177951812744
INFO:root:current mean train loss 3189.42824694249
INFO:root:current train perplexity3.5260818004608154
INFO:root:current mean train loss 3197.686280135846
INFO:root:current train perplexity3.5312936305999756
INFO:root:current mean train loss 3203.344529251918
INFO:root:current train perplexity3.535745620727539
INFO:root:current mean train loss 3199.4132549961814
INFO:root:current train perplexity3.5306596755981445
INFO:root:current mean train loss 3201.2910738717483
INFO:root:current train perplexity3.531386137008667
INFO:root:current mean train loss 3203.400335861184
INFO:root:current train perplexity3.533662796020508
INFO:root:current mean train loss 3203.644695450774
INFO:root:current train perplexity3.5345096588134766
INFO:root:current mean train loss 3206.89360702292
INFO:root:current train perplexity3.5386600494384766
INFO:root:current mean train loss 3207.5245283725403
INFO:root:current train perplexity3.5412352085113525


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.66s/it]
INFO:root:final mean train loss: 3204.986272196616
INFO:root:final train perplexity: 3.5411794185638428
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.86s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.86s/it]
INFO:root:eval mean loss: 4105.854630707004
INFO:root:eval perplexity: 5.260820388793945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/175

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [11:02:13<1:25:13, 204.54s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3197.3852761008525
INFO:root:current train perplexity3.537982225418091
INFO:root:current mean train loss 3197.422712929884
INFO:root:current train perplexity3.5292601585388184
INFO:root:current mean train loss 3199.8402030204848
INFO:root:current train perplexity3.5321896076202393
INFO:root:current mean train loss 3200.5055399729795
INFO:root:current train perplexity3.5324368476867676
INFO:root:current mean train loss 3200.742467845848
INFO:root:current train perplexity3.5324573516845703
INFO:root:current mean train loss 3202.9807397909276
INFO:root:current train perplexity3.53701114654541
INFO:root:current mean train loss 3204.4396770078906
INFO:root:current train perplexity3.536168336868286
INFO:root:current mean train loss 3202.9425991473718
INFO:root:current train perplexity3.5354299545288086
INFO:root:current mean train loss 3203.339420373766
INFO:root:current train perplexity3.536208391189575


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.16s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.16s/it]
INFO:root:final mean train loss: 3202.9395312647666
INFO:root:final train perplexity: 3.538320541381836
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.46s/it]
INFO:root:eval mean loss: 4106.383750969637
INFO:root:eval perplexity: 5.261947154998779
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/176

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [11:05:37<1:21:41, 204.21s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3188.3128487723216
INFO:root:current train perplexity3.449803113937378
INFO:root:current mean train loss 3214.7181248174647
INFO:root:current train perplexity3.532369375228882
INFO:root:current mean train loss 3205.7476895097375
INFO:root:current train perplexity3.52713942527771
INFO:root:current mean train loss 3201.15543407726
INFO:root:current train perplexity3.5269949436187744
INFO:root:current mean train loss 3201.8734919667536
INFO:root:current train perplexity3.5247645378112793
INFO:root:current mean train loss 3200.4712930111255
INFO:root:current train perplexity3.5270700454711914
INFO:root:current mean train loss 3199.8766237161503
INFO:root:current train perplexity3.5296261310577393
INFO:root:current mean train loss 3201.7356481916327
INFO:root:current train perplexity3.534071445465088
INFO:root:current mean train loss 3202.563581237415
INFO:root:current train perplexity3.5348849296569824
INFO:root:current mean train loss 3205.275877291207
INFO:root:current train perplexity3.5386769771575928


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it]
INFO:root:final mean train loss: 3202.1499519963418
INFO:root:final train perplexity: 3.537219524383545
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it]
INFO:root:eval mean loss: 4106.321960882092
INFO:root:eval perplexity: 5.261814594268799
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/177

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [11:09:00<1:18:11, 203.96s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3206.9909505208334
INFO:root:current train perplexity3.4994306564331055
INFO:root:current mean train loss 3192.271686056386
INFO:root:current train perplexity3.5060172080993652
INFO:root:current mean train loss 3195.449615052689
INFO:root:current train perplexity3.529576539993286
INFO:root:current mean train loss 3194.3213960193452
INFO:root:current train perplexity3.532081365585327
INFO:root:current mean train loss 3198.040686182229
INFO:root:current train perplexity3.530381202697754
INFO:root:current mean train loss 3197.1476387097996
INFO:root:current train perplexity3.5280401706695557
INFO:root:current mean train loss 3200.8547966685724
INFO:root:current train perplexity3.530303716659546
INFO:root:current mean train loss 3203.8363171984265
INFO:root:current train perplexity3.5334811210632324
INFO:root:current mean train loss 3203.9921296851035
INFO:root:current train perplexity3.5336737632751465
INFO:root:current mean train loss 3202.9481864220456
INFO:root:current train perplexity3.535252809524536


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.49s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.49s/it]
INFO:root:final mean train loss: 3201.1636997345954
INFO:root:final train perplexity: 3.5358428955078125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.56s/it]
INFO:root:eval mean loss: 4107.369833222518
INFO:root:eval perplexity: 5.264045238494873
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/178

 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [11:12:24<1:14:46, 203.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3224.7723335597825
INFO:root:current train perplexity3.539445638656616
INFO:root:current mean train loss 3216.615293921494
INFO:root:current train perplexity3.530721664428711
INFO:root:current mean train loss 3209.42251545859
INFO:root:current train perplexity3.5249903202056885
INFO:root:current mean train loss 3209.3773083760643
INFO:root:current train perplexity3.5276873111724854
INFO:root:current mean train loss 3211.214059960476
INFO:root:current train perplexity3.5306992530822754
INFO:root:current mean train loss 3205.6926110816503
INFO:root:current train perplexity3.530947685241699
INFO:root:current mean train loss 3204.2303402136836
INFO:root:current train perplexity3.53025484085083
INFO:root:current mean train loss 3201.135039818897
INFO:root:current train perplexity3.530986785888672
INFO:root:current mean train loss 3199.752338173033
INFO:root:current train perplexity3.5297999382019043
INFO:root:current mean train loss 3200.219540878081
INFO:root:current train perplexity3.5327017307281494


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.19s/it]
INFO:root:final mean train loss: 3199.3369998931885
INFO:root:final train perplexity: 3.533295154571533
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/it]
INFO:root:eval mean loss: 4107.851247368129
INFO:root:eval perplexity: 5.265069961547852
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/179

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [11:15:47<1:11:15, 203.58s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3185.7565366683466
INFO:root:current train perplexity3.5335912704467773
INFO:root:current mean train loss 3191.543033978411
INFO:root:current train perplexity3.5258119106292725
INFO:root:current mean train loss 3190.819073829816
INFO:root:current train perplexity3.532994270324707
INFO:root:current mean train loss 3191.5437852565615
INFO:root:current train perplexity3.532531499862671
INFO:root:current mean train loss 3193.9209669781394
INFO:root:current train perplexity3.5309901237487793
INFO:root:current mean train loss 3195.902790191708
INFO:root:current train perplexity3.5283985137939453
INFO:root:current mean train loss 3197.4070617385596
INFO:root:current train perplexity3.5306556224823
INFO:root:current mean train loss 3199.794003759298
INFO:root:current train perplexity3.532461643218994
INFO:root:current mean train loss 3200.035375418359
INFO:root:current train perplexity3.530806303024292
INFO:root:current mean train loss 3198.9254922672194
INFO:root:current train perplexity3.529796600341797


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.09s/it]
INFO:root:final mean train loss: 3198.1798550390426
INFO:root:final train perplexity: 3.5316829681396484
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4108.758563968307
INFO:root:eval perplexity: 5.267001628875732
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/180

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [11:19:09<1:07:44, 203.23s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3210.650547125401
INFO:root:current train perplexity3.554436206817627
INFO:root:current mean train loss 3188.762229864546
INFO:root:current train perplexity3.5249183177948
INFO:root:current mean train loss 3187.4813441831197
INFO:root:current train perplexity3.522152900695801
INFO:root:current mean train loss 3193.970158669801
INFO:root:current train perplexity3.5225765705108643
INFO:root:current mean train loss 3193.270704682161
INFO:root:current train perplexity3.524228096008301
INFO:root:current mean train loss 3193.1003508558965
INFO:root:current train perplexity3.523115873336792
INFO:root:current mean train loss 3192.664961120892
INFO:root:current train perplexity3.5224106311798096
INFO:root:current mean train loss 3196.231501805121
INFO:root:current train perplexity3.5254478454589844
INFO:root:current mean train loss 3198.645805204298
INFO:root:current train perplexity3.5292892456054688
INFO:root:current mean train loss 3198.656372200313
INFO:root:current train perplexity3.5283291339874268


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.29s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.29s/it]
INFO:root:final mean train loss: 3196.2412150598343
INFO:root:final train perplexity: 3.528982400894165
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.43s/it]
INFO:root:eval mean loss: 4108.577409893063
INFO:root:eval perplexity: 5.2666168212890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/181

 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [11:22:33<1:04:23, 203.32s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3199.7162462599736
INFO:root:current train perplexity3.503401041030884
INFO:root:current mean train loss 3193.568482275723
INFO:root:current train perplexity3.528775930404663
INFO:root:current mean train loss 3197.2083507954835
INFO:root:current train perplexity3.526799440383911
INFO:root:current mean train loss 3204.6981063974695
INFO:root:current train perplexity3.533172845840454
INFO:root:current mean train loss 3200.848128692149
INFO:root:current train perplexity3.5289862155914307
INFO:root:current mean train loss 3199.386773648166
INFO:root:current train perplexity3.530301094055176
INFO:root:current mean train loss 3201.6317457526325
INFO:root:current train perplexity3.530485153198242
INFO:root:current mean train loss 3200.2944839252846
INFO:root:current train perplexity3.530580520629883
INFO:root:current mean train loss 3200.4769167489117
INFO:root:current train perplexity3.530909776687622
INFO:root:current mean train loss 3200.177512921149
INFO:root:current train perplexity3.530733346939087


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.88s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.89s/it]
INFO:root:final mean train loss: 3196.666176826723
INFO:root:final train perplexity: 3.529574394226074
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it]
INFO:root:eval mean loss: 4109.466152759309
INFO:root:eval perplexity: 5.268509864807129
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/182

 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [11:25:56<1:00:58, 203.25s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3193.5887650923296
INFO:root:current train perplexity3.5466701984405518
INFO:root:current mean train loss 3189.393871282762
INFO:root:current train perplexity3.538710355758667
INFO:root:current mean train loss 3187.7914972043504
INFO:root:current train perplexity3.5236828327178955
INFO:root:current mean train loss 3195.0246520136443
INFO:root:current train perplexity3.526658058166504
INFO:root:current mean train loss 3196.656425995879
INFO:root:current train perplexity3.524395227432251
INFO:root:current mean train loss 3194.485549074465
INFO:root:current train perplexity3.5271360874176025
INFO:root:current mean train loss 3195.880066197519
INFO:root:current train perplexity3.5255672931671143
INFO:root:current mean train loss 3197.5214863151905
INFO:root:current train perplexity3.5289227962493896
INFO:root:current mean train loss 3198.439790638706
INFO:root:current train perplexity3.528324604034424
INFO:root:current mean train loss 3198.502595559964
INFO:root:current train perplexity3.5275814533233643


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.81s/it]
INFO:root:final mean train loss: 3195.208869441863
INFO:root:final train perplexity: 3.5275461673736572
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it]
INFO:root:eval mean loss: 4108.985448526152
INFO:root:eval perplexity: 5.267486095428467
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/183

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [11:29:20<57:38, 203.45s/it]  

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3189.012757316468
INFO:root:current train perplexity3.5364882946014404
INFO:root:current mean train loss 3201.1014486675613
INFO:root:current train perplexity3.524260997772217
INFO:root:current mean train loss 3193.685855067728
INFO:root:current train perplexity3.522702932357788
INFO:root:current mean train loss 3195.677734375
INFO:root:current train perplexity3.5274131298065186
INFO:root:current mean train loss 3196.0498621633706
INFO:root:current train perplexity3.5274713039398193
INFO:root:current mean train loss 3194.0550235727965
INFO:root:current train perplexity3.5257606506347656
INFO:root:current mean train loss 3196.4806267233457
INFO:root:current train perplexity3.526651382446289
INFO:root:current mean train loss 3198.2439416007946
INFO:root:current train perplexity3.5261402130126953
INFO:root:current mean train loss 3195.087591036446
INFO:root:current train perplexity3.5247714519500732
INFO:root:current mean train loss 3195.4088528990624
INFO:root:current train perplexity3.5258913040161133


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 3194.1220914163896
INFO:root:final train perplexity: 3.526033401489258
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.13s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.13s/it]
INFO:root:eval mean loss: 4110.1167130707
INFO:root:eval perplexity: 5.269896030426025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/184

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [11:32:43<54:14, 203.39s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3199.2957608934857
INFO:root:current train perplexity3.525514841079712
INFO:root:current mean train loss 3201.831660042032
INFO:root:current train perplexity3.520313024520874
INFO:root:current mean train loss 3198.834140228609
INFO:root:current train perplexity3.518782615661621
INFO:root:current mean train loss 3194.795774722035
INFO:root:current train perplexity3.5166568756103516
INFO:root:current mean train loss 3194.4198902559383
INFO:root:current train perplexity3.5173234939575195
INFO:root:current mean train loss 3195.5261003858363
INFO:root:current train perplexity3.5199732780456543
INFO:root:current mean train loss 3195.771561874185
INFO:root:current train perplexity3.5225512981414795
INFO:root:current mean train loss 3194.645465380796
INFO:root:current train perplexity3.5234367847442627
INFO:root:current mean train loss 3198.181857296301
INFO:root:current train perplexity3.524822235107422
INFO:root:current mean train loss 3197.286182344635
INFO:root:current train perplexity3.526139259338379


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.55s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.55s/it]
INFO:root:final mean train loss: 3193.970481749504
INFO:root:final train perplexity: 3.525822401046753
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it]
INFO:root:eval mean loss: 4110.132038522274
INFO:root:eval perplexity: 5.269928455352783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/185

 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [11:36:07<50:52, 203.53s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3186.363930231408
INFO:root:current train perplexity3.511110305786133
INFO:root:current mean train loss 3178.788687423621
INFO:root:current train perplexity3.5154521465301514
INFO:root:current mean train loss 3182.856272926467
INFO:root:current train perplexity3.524170160293579
INFO:root:current mean train loss 3187.3331952661197
INFO:root:current train perplexity3.5208263397216797
INFO:root:current mean train loss 3190.538380027564
INFO:root:current train perplexity3.5218114852905273
INFO:root:current mean train loss 3193.6936857391515
INFO:root:current train perplexity3.522169589996338
INFO:root:current mean train loss 3194.929099620881
INFO:root:current train perplexity3.523374080657959
INFO:root:current mean train loss 3195.2393092105262
INFO:root:current train perplexity3.524604082107544
INFO:root:current mean train loss 3196.6003917915423
INFO:root:current train perplexity3.524916410446167
INFO:root:current mean train loss 3195.709909565724
INFO:root:current train perplexity3.5241785049438477


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.19s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.19s/it]
INFO:root:final mean train loss: 3192.6909630067885
INFO:root:final train perplexity: 3.524043321609497
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.71s/it]
INFO:root:eval mean loss: 4110.255902662345
INFO:root:eval perplexity: 5.270192623138428
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/186

 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [11:39:29<47:26, 203.30s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3194.6772180316093
INFO:root:current train perplexity3.5323634147644043
INFO:root:current mean train loss 3190.490785323362
INFO:root:current train perplexity3.5241310596466064
INFO:root:current mean train loss 3194.155072680749
INFO:root:current train perplexity3.5187556743621826
INFO:root:current mean train loss 3195.785890564438
INFO:root:current train perplexity3.5187435150146484
INFO:root:current mean train loss 3196.990113056661
INFO:root:current train perplexity3.5190982818603516
INFO:root:current mean train loss 3196.11552634556
INFO:root:current train perplexity3.5216617584228516
INFO:root:current mean train loss 3197.1339489799398
INFO:root:current train perplexity3.522260904312134
INFO:root:current mean train loss 3195.210262158017
INFO:root:current train perplexity3.5219790935516357
INFO:root:current mean train loss 3193.95107581516
INFO:root:current train perplexity3.5216336250305176
INFO:root:current mean train loss 3192.9131979898843
INFO:root:current train perplexity3.5208921432495117


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.44s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.44s/it]
INFO:root:final mean train loss: 3189.9892917140837
INFO:root:final train perplexity: 3.520289182662964
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.35s/it]
INFO:root:eval mean loss: 4111.1965366661125
INFO:root:eval perplexity: 5.2721967697143555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/187

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [11:42:53<44:04, 203.40s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3194.5849326685857
INFO:root:current train perplexity3.5064406394958496
INFO:root:current mean train loss 3191.2728728465545
INFO:root:current train perplexity3.5154874324798584
INFO:root:current mean train loss 3190.7539848715573
INFO:root:current train perplexity3.5170724391937256
INFO:root:current mean train loss 3189.0413289903086
INFO:root:current train perplexity3.5167055130004883
INFO:root:current mean train loss 3187.1680299084596
INFO:root:current train perplexity3.5147311687469482
INFO:root:current mean train loss 3190.3990098969275
INFO:root:current train perplexity3.51893949508667
INFO:root:current mean train loss 3193.7851418474597
INFO:root:current train perplexity3.5213303565979004
INFO:root:current mean train loss 3192.8752042182587
INFO:root:current train perplexity3.5200676918029785
INFO:root:current mean train loss 3192.3631874127095
INFO:root:current train perplexity3.5207130908966064


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.56s/it]
INFO:root:final mean train loss: 3189.8129113105038
INFO:root:final train perplexity: 3.5200440883636475
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it]
INFO:root:eval mean loss: 4110.328206380208
INFO:root:eval perplexity: 5.270346164703369
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/188

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [11:46:17<40:41, 203.49s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3178.88134765625
INFO:root:current train perplexity3.4893798828125
INFO:root:current mean train loss 3185.948386775637
INFO:root:current train perplexity3.4970595836639404
INFO:root:current mean train loss 3184.7983085745072
INFO:root:current train perplexity3.5068700313568115
INFO:root:current mean train loss 3189.4804260455344
INFO:root:current train perplexity3.514853000640869
INFO:root:current mean train loss 3187.4756513647644
INFO:root:current train perplexity3.5158658027648926
INFO:root:current mean train loss 3188.3703705501366
INFO:root:current train perplexity3.5184898376464844
INFO:root:current mean train loss 3185.1403391570793
INFO:root:current train perplexity3.517646312713623
INFO:root:current mean train loss 3187.4435142636466
INFO:root:current train perplexity3.5155560970306396
INFO:root:current mean train loss 3187.753669710266
INFO:root:current train perplexity3.5159332752227783
INFO:root:current mean train loss 3192.529468016767
INFO:root:current train perplexity3.518558979034424


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it]
INFO:root:final mean train loss: 3189.502511916622
INFO:root:final train perplexity: 3.519613265991211
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.67s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.67s/it]
INFO:root:eval mean loss: 4110.697857795878
INFO:root:eval perplexity: 5.271134376525879
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/189

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [11:49:39<37:14, 203.14s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3172.307062322443
INFO:root:current train perplexity3.5258615016937256
INFO:root:current mean train loss 3174.0972823409347
INFO:root:current train perplexity3.5065674781799316
INFO:root:current mean train loss 3189.8475150881222
INFO:root:current train perplexity3.514991283416748
INFO:root:current mean train loss 3192.300474307928
INFO:root:current train perplexity3.518308162689209
INFO:root:current mean train loss 3188.609992776764
INFO:root:current train perplexity3.5198562145233154
INFO:root:current mean train loss 3187.767273785317
INFO:root:current train perplexity3.519314765930176
INFO:root:current mean train loss 3185.739443215528
INFO:root:current train perplexity3.5165469646453857
INFO:root:current mean train loss 3187.8666253928227
INFO:root:current train perplexity3.5185484886169434
INFO:root:current mean train loss 3187.754671785893
INFO:root:current train perplexity3.516965389251709
INFO:root:current mean train loss 3191.0610169328006
INFO:root:current train perplexity3.5190556049346924


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 3189.980467765562
INFO:root:final train perplexity: 3.5202770233154297
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it]
INFO:root:eval mean loss: 4111.716937125997
INFO:root:eval perplexity: 5.2733073234558105
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/190

 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [11:53:01<33:48, 202.87s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3171.652587890625
INFO:root:current train perplexity3.5234265327453613
INFO:root:current mean train loss 3207.839821182379
INFO:root:current train perplexity3.5260472297668457
INFO:root:current mean train loss 3203.010605067423
INFO:root:current train perplexity3.5213561058044434
INFO:root:current mean train loss 3203.4574468247943
INFO:root:current train perplexity3.5235939025878906
INFO:root:current mean train loss 3195.1215237637975
INFO:root:current train perplexity3.520796060562134
INFO:root:current mean train loss 3194.2101288723807
INFO:root:current train perplexity3.518754482269287
INFO:root:current mean train loss 3191.3769393206026
INFO:root:current train perplexity3.5180723667144775
INFO:root:current mean train loss 3190.8731956004435
INFO:root:current train perplexity3.517791748046875
INFO:root:current mean train loss 3188.020029368418
INFO:root:current train perplexity3.516296863555908
INFO:root:current mean train loss 3188.71360577904
INFO:root:current train perplexity3.515968084335327


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.60s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.60s/it]
INFO:root:final mean train loss: 3187.6519742781115
INFO:root:final train perplexity: 3.5170443058013916
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.38s/it]
INFO:root:eval mean loss: 4111.593904102948
INFO:root:eval perplexity: 5.273044586181641
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/191

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [11:56:24<30:25, 202.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3235.8040183738426
INFO:root:current train perplexity3.523799180984497
INFO:root:current mean train loss 3208.0630324956937
INFO:root:current train perplexity3.517141103744507
INFO:root:current mean train loss 3198.5843995216132
INFO:root:current train perplexity3.517542839050293
INFO:root:current mean train loss 3195.971731950019
INFO:root:current train perplexity3.5151710510253906
INFO:root:current mean train loss 3192.9539254610654
INFO:root:current train perplexity3.515476942062378
INFO:root:current mean train loss 3195.947300369871
INFO:root:current train perplexity3.5164706707000732
INFO:root:current mean train loss 3191.6269067889007
INFO:root:current train perplexity3.5153210163116455
INFO:root:current mean train loss 3189.718075003224
INFO:root:current train perplexity3.5145785808563232
INFO:root:current mean train loss 3188.529304255309
INFO:root:current train perplexity3.5151631832122803
INFO:root:current mean train loss 3189.110145346632
INFO:root:current train perplexity3.516033172607422


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.32s/it]
INFO:root:final mean train loss: 3187.3494848435926
INFO:root:final train perplexity: 3.5166242122650146
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.40s/it]
INFO:root:eval mean loss: 4111.116983183732
INFO:root:eval perplexity: 5.272027492523193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/192

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [11:59:47<27:02, 202.76s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3177.4126325334823
INFO:root:current train perplexity3.5227303504943848
INFO:root:current mean train loss 3190.3804814091436
INFO:root:current train perplexity3.517239570617676
INFO:root:current mean train loss 3185.4318193151594
INFO:root:current train perplexity3.51657772064209
INFO:root:current mean train loss 3187.628091476213
INFO:root:current train perplexity3.5176568031311035
INFO:root:current mean train loss 3191.477625493894
INFO:root:current train perplexity3.5171804428100586
INFO:root:current mean train loss 3185.9819828782124
INFO:root:current train perplexity3.5123369693756104
INFO:root:current mean train loss 3188.152465628076
INFO:root:current train perplexity3.5154433250427246
INFO:root:current mean train loss 3186.67755035608
INFO:root:current train perplexity3.5154178142547607
INFO:root:current mean train loss 3186.518000619854
INFO:root:current train perplexity3.514047861099243
INFO:root:current mean train loss 3187.4537939714237
INFO:root:current train perplexity3.5145316123962402


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.98s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.98s/it]
INFO:root:final mean train loss: 3186.6841225777903
INFO:root:final train perplexity: 3.5157010555267334
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.51s/it]
INFO:root:eval mean loss: 4111.76299659242
INFO:root:eval perplexity: 5.2734055519104
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/193

 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [12:03:10<23:40, 202.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3188.648204714753
INFO:root:current train perplexity3.5349225997924805
INFO:root:current mean train loss 3190.1993143575173
INFO:root:current train perplexity3.5298736095428467
INFO:root:current mean train loss 3188.258389194316
INFO:root:current train perplexity3.517838478088379
INFO:root:current mean train loss 3184.332050468067
INFO:root:current train perplexity3.5184645652770996
INFO:root:current mean train loss 3187.8855283577877
INFO:root:current train perplexity3.5148885250091553
INFO:root:current mean train loss 3188.4097583412467
INFO:root:current train perplexity3.5137486457824707
INFO:root:current mean train loss 3187.9830282398184
INFO:root:current train perplexity3.5147125720977783
INFO:root:current mean train loss 3187.100337853823
INFO:root:current train perplexity3.5131447315216064
INFO:root:current mean train loss 3189.4536630940465
INFO:root:current train perplexity3.5141117572784424
INFO:root:current mean train loss 3189.8070481301365
INFO:root:current train perplexity3.5146498680114746


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 3185.971960252331
INFO:root:final train perplexity: 3.514714241027832
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.05s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.05s/it]
INFO:root:eval mean loss: 4112.031341769171
INFO:root:eval perplexity: 5.2739787101745605
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/194

 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [12:06:33<20:17, 202.86s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3158.4575243183212
INFO:root:current train perplexity3.483456611633301
INFO:root:current mean train loss 3175.964756441432
INFO:root:current train perplexity3.5021250247955322
INFO:root:current mean train loss 3180.890483962587
INFO:root:current train perplexity3.512143135070801
INFO:root:current mean train loss 3180.597022597267
INFO:root:current train perplexity3.5109219551086426
INFO:root:current mean train loss 3181.9219323811667
INFO:root:current train perplexity3.5096778869628906
INFO:root:current mean train loss 3184.3241429822197
INFO:root:current train perplexity3.5125412940979004
INFO:root:current mean train loss 3187.9277850032404
INFO:root:current train perplexity3.515273332595825
INFO:root:current mean train loss 3185.9937807532665
INFO:root:current train perplexity3.5139832496643066
INFO:root:current mean train loss 3187.2635803581265
INFO:root:current train perplexity3.5149619579315186
INFO:root:current mean train loss 3187.7558432016463
INFO:root:current train perplexity3.5149343013763428


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it]
INFO:root:final mean train loss: 3185.243691844325
INFO:root:final train perplexity: 3.513704299926758
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.45s/it]
INFO:root:eval mean loss: 4111.810243863586
INFO:root:eval perplexity: 5.273506164550781
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/195

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [12:09:55<16:53, 202.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3201.9000430349574
INFO:root:current train perplexity3.534435510635376
INFO:root:current mean train loss 3198.9088971599845
INFO:root:current train perplexity3.5234181880950928
INFO:root:current mean train loss 3197.889965160473
INFO:root:current train perplexity3.517786741256714
INFO:root:current mean train loss 3193.720565753395
INFO:root:current train perplexity3.5141961574554443
INFO:root:current mean train loss 3190.0258549708947
INFO:root:current train perplexity3.512660264968872
INFO:root:current mean train loss 3185.2198698149596
INFO:root:current train perplexity3.5114996433258057
INFO:root:current mean train loss 3184.064495729206
INFO:root:current train perplexity3.511732816696167
INFO:root:current mean train loss 3185.155038946702
INFO:root:current train perplexity3.5126593112945557
INFO:root:current mean train loss 3186.378506928023
INFO:root:current train perplexity3.5115649700164795
INFO:root:current mean train loss 3186.63687175158
INFO:root:current train perplexity3.512270450592041


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.18s/it]
INFO:root:final mean train loss: 3184.33494617093
INFO:root:final train perplexity: 3.512444496154785
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.26s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.26s/it]
INFO:root:eval mean loss: 4112.07012549867
INFO:root:eval perplexity: 5.274059772491455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/196

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [12:13:17<13:30, 202.57s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3201.5539951609144
INFO:root:current train perplexity3.524700403213501
INFO:root:current mean train loss 3188.842963487088
INFO:root:current train perplexity3.517929792404175
INFO:root:current mean train loss 3192.53522117129
INFO:root:current train perplexity3.5168559551239014
INFO:root:current mean train loss 3192.219028067524
INFO:root:current train perplexity3.5153119564056396
INFO:root:current mean train loss 3192.919527695062
INFO:root:current train perplexity3.5162129402160645
INFO:root:current mean train loss 3191.8966703869046
INFO:root:current train perplexity3.5160629749298096
INFO:root:current mean train loss 3188.9199039396317
INFO:root:current train perplexity3.514228343963623
INFO:root:current mean train loss 3189.3540586548647
INFO:root:current train perplexity3.5156798362731934
INFO:root:current mean train loss 3187.577356815888
INFO:root:current train perplexity3.5139739513397217
INFO:root:current mean train loss 3187.04279378676
INFO:root:current train perplexity3.5135886669158936


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.73s/it]
INFO:root:final mean train loss: 3184.587684200656
INFO:root:final train perplexity: 3.5127947330474854
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.36s/it]
INFO:root:eval mean loss: 4112.3299932818045
INFO:root:eval perplexity: 5.274614334106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/197

 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [12:16:40<10:08, 202.69s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3184.1836360677084
INFO:root:current train perplexity3.5086450576782227
INFO:root:current mean train loss 3184.250537109375
INFO:root:current train perplexity3.5058276653289795
INFO:root:current mean train loss 3181.5723366477273
INFO:root:current train perplexity3.5068554878234863
INFO:root:current mean train loss 3178.75636328125
INFO:root:current train perplexity3.505770683288574
INFO:root:current mean train loss 3181.7897234786183
INFO:root:current train perplexity3.50429368019104
INFO:root:current mean train loss 3183.881254245924
INFO:root:current train perplexity3.505192279815674
INFO:root:current mean train loss 3185.491265914352
INFO:root:current train perplexity3.507697105407715
INFO:root:current mean train loss 3184.0895955141127
INFO:root:current train perplexity3.5079424381256104
INFO:root:current mean train loss 3186.123439453125
INFO:root:current train perplexity3.5089168548583984
INFO:root:current mean train loss 3185.218424979968
INFO:root:current train perplexity3.5104968547821045


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.31s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:10<00:00, 190.31s/it]
INFO:root:final mean train loss: 3182.9924101060437
INFO:root:final train perplexity: 3.510584592819214
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.34s/it]
INFO:root:eval mean loss: 4112.358670282026
INFO:root:eval perplexity: 5.274675369262695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/198

 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [12:20:04<06:45, 202.94s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3181.334887401167
INFO:root:current train perplexity3.5170135498046875
INFO:root:current mean train loss 3193.3269189719945
INFO:root:current train perplexity3.517702579498291
INFO:root:current mean train loss 3194.9670677589443
INFO:root:current train perplexity3.522909641265869
INFO:root:current mean train loss 3189.3542136249594
INFO:root:current train perplexity3.517091989517212
INFO:root:current mean train loss 3182.1375987682777
INFO:root:current train perplexity3.514632225036621
INFO:root:current mean train loss 3185.4191513454116
INFO:root:current train perplexity3.516947031021118
INFO:root:current mean train loss 3186.330270434892
INFO:root:current train perplexity3.5135459899902344
INFO:root:current mean train loss 3186.0771499965076
INFO:root:current train perplexity3.511228084564209
INFO:root:current mean train loss 3186.1077743996852
INFO:root:current train perplexity3.509427785873413
INFO:root:current mean train loss 3186.35795014266
INFO:root:current train perplexity3.5112078189849854


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.83s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.84s/it]
INFO:root:final mean train loss: 3183.4094576681814
INFO:root:final train perplexity: 3.511162281036377
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.59s/it]
INFO:root:eval mean loss: 4112.498432998116
INFO:root:eval perplexity: 5.2749738693237305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [12:23:27<03:23, 203.04s/it]

  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 3182.378833812672
INFO:root:current train perplexity3.5188751220703125
INFO:root:current mean train loss 3179.0593900830336
INFO:root:current train perplexity3.5020906925201416
INFO:root:current mean train loss 3177.596236710696
INFO:root:current train perplexity3.5058155059814453
INFO:root:current mean train loss 3180.523552389706
INFO:root:current train perplexity3.5054819583892822
INFO:root:current mean train loss 3183.2498786755345
INFO:root:current train perplexity3.5083117485046387
INFO:root:current mean train loss 3183.552377871854
INFO:root:current train perplexity3.511539936065674
INFO:root:current mean train loss 3182.662275433023
INFO:root:current train perplexity3.5100786685943604
INFO:root:current mean train loss 3184.229460705397
INFO:root:current train perplexity3.5097625255584717
INFO:root:current mean train loss 3185.862841468066
INFO:root:current train perplexity3.5114243030548096
INFO:root:current mean train loss 3185.2545677208154
INFO:root:current train perplexity3.510220527648926


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.80s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.80s/it]
INFO:root:final mean train loss: 3182.720561612037
INFO:root:final train perplexity: 3.5102081298828125
INFO:root:epoch finished
INFO:root:start evaluating


  0%|          | 0/1 [00:00<?, ?it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.73s/it]
INFO:root:eval mean loss: 4112.445104720745
INFO:root:eval perplexity: 5.274859428405762
INFO:root:evalaution complete
INFO:root:checkpoint. save model: small_val_230/200

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [12:26:50<00:00, 203.13s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [12:26:50<00:00, 224.05s/it]
INFO:root:evaluating final model
INFO:root:start evaluating

  0%|          | 0/1 [00:00<?, ?it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.27s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:13<00:00, 13.27s/it]
INFO:root:eval mean loss: 4112.445104720745
INFO:root:eval perplexity: 5.274859428405762
INFO:root:evalaution complete
INFO:root:save model final: small_val_230/final
Fatal error condition occurred in /opt/vcpkg/buildtrees/aws-c-io/src/9e6648842a-364b708815.clean/source/event_loop.c:72: aws_thread_launch(&cleanup_thread, s_event_loop_destroy_async_thread_fn, el_group, &thread_options) == AWS_OP_SUCCESS
Exiting Application
################################################################################
Stack trace:
################################################################################
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200af06) [0x153f26bd8f06]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x20028e5) [0x153f26bd08e5]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f27e09) [0x153f26af5e09]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x153f26bd9a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1f25948) [0x153f26af3948]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x200ba3d) [0x153f26bd9a3d]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x1ee0b46) [0x153f26aaeb46]
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/pyarrow/libarrow.so.900(+0x194546a) [0x153f2651346a]
/lib/x86_64-linux-gnu/libc.so.6(+0x49a27) [0x154022d2fa27]
/lib/x86_64-linux-gnu/libc.so.6(on_exit+0) [0x154022d2fbe0]
python(+0x24a989) [0x562be15c1989]
python(+0x24a9bd) [0x562be15c19bd]
python(+0x24aa14) [0x562be15c1a14]
python(+0x108f75) [0x562be147ff75]
python(Py_RunMain+0x313) [0x562be15c4983]
python(Py_BytesMain+0x39) [0x562be15c4bc9]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x154022d0d0b3]
python(+0x1d6e13) [0x562be154de13]
/opt/slurm/data/slurmd/job26146341/slurm_script: line 134: 716502 Aborted                 singularity exec --nv --overlay /scratch/zw2374/overlay-50G-10M.ext3:ro /scratch/work/public/singularity/cuda11.3.0-cudnn8-devel-ubuntu20.04.sif /bin/bash -c "
source /ext3/env.sh
conda activate rblm
python train_script.py --model_path sentence-transformers/multi-qa-MiniLM-L6-cos-v1 --data_config data_config.json --data_folder fast_processed_data_230_final  --output small_val_230 --batch_size 128 --epochs 200 --save_head  --save_epochs 1 --external_embedding
"
