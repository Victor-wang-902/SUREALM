INFO:root:Output: allmini_l12_baseline
INFO:root:Steps per epochs:1983
INFO:root:Total steps:396600
Some weights of RetrievalGenerationModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L12-v1 and are newly initialized: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/ext3/miniconda3/envs/rblm/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:root:started training
  0%|          | 0/200 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 12304.494742345329
INFO:root:current train perplexity15121.8876953125
INFO:root:current mean train loss 10064.82067809752
INFO:root:current train perplexity2649.046875
INFO:root:current mean train loss 8753.16809449467
INFO:root:current train perplexity954.5439453125
INFO:root:current mean train loss 7855.280818011826
INFO:root:current train perplexity474.69683837890625
INFO:root:current mean train loss 7200.508806675852
INFO:root:current train perplexity285.94403076171875
INFO:root:current mean train loss 6707.638966963168
INFO:root:current train perplexity194.15989685058594
INFO:root:current mean train loss 6310.336086638837
INFO:root:current train perplexity143.3419952392578
INFO:root:current mean train loss 5995.346737437911
INFO:root:current train perplexity112.27803802490234
INFO:root:current mean train loss 5737.341484842099
INFO:root:current train perplexity91.60231018066406
INFO:root:current mean train loss 5520.692088084178
INFO:root:current train perplexity77.11485290527344
INFO:root:current mean train loss 5333.871321895971
INFO:root:current train perplexity66.56454467773438
INFO:root:current mean train loss 5170.857568888788
INFO:root:current train perplexity58.63551330566406
INFO:root:current mean train loss 5026.717629095699
INFO:root:current train perplexity52.43688201904297
INFO:root:current mean train loss 4901.428863808971
INFO:root:current train perplexity47.518463134765625
INFO:root:current mean train loss 4788.770484196496
INFO:root:current train perplexity43.53959274291992
INFO:root:current mean train loss 4685.9478962071025
INFO:root:current train perplexity40.20949935913086
INFO:root:current mean train loss 4594.384230067549
INFO:root:current train perplexity37.42829513549805
INFO:root:current mean train loss 4511.948517541212
INFO:root:current train perplexity35.02438735961914
INFO:root:current mean train loss 4436.179708327162
INFO:root:current train perplexity33.01128005981445

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:11<00:00, 191.53s/it]
INFO:root:final mean train loss: 4375.259796404202
INFO:root:final train perplexity: 31.51907730102539
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2833.2315907579787
INFO:root:eval perplexity: 9.888291358947754
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/1
  0%|          | 1/200 [03:21<11:06:59, 201.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2990.7129974365234
INFO:root:current train perplexity10.69295883178711
INFO:root:current mean train loss 2976.690926387392
INFO:root:current train perplexity10.60719108581543
INFO:root:current mean train loss 2966.5561319986978
INFO:root:current train perplexity10.516441345214844
INFO:root:current mean train loss 2958.1319031534317
INFO:root:current train perplexity10.421958923339844
INFO:root:current mean train loss 2947.638597341684
INFO:root:current train perplexity10.285807609558105
INFO:root:current mean train loss 2942.3521354734435
INFO:root:current train perplexity10.183423042297363
INFO:root:current mean train loss 2929.669189453125
INFO:root:current train perplexity10.097383499145508
INFO:root:current mean train loss 2913.7433993376835
INFO:root:current train perplexity9.999025344848633
INFO:root:current mean train loss 2907.5116221110025
INFO:root:current train perplexity9.927388191223145
INFO:root:current mean train loss 2897.601564898762
INFO:root:current train perplexity9.848211288452148
INFO:root:current mean train loss 2889.98807892086
INFO:root:current train perplexity9.769161224365234
INFO:root:current mean train loss 2880.311307954959
INFO:root:current train perplexity9.692052841186523
INFO:root:current mean train loss 2874.466211218583
INFO:root:current train perplexity9.630209922790527
INFO:root:current mean train loss 2865.594599112189
INFO:root:current train perplexity9.566289901733398
INFO:root:current mean train loss 2858.255642476055
INFO:root:current train perplexity9.520962715148926
INFO:root:current mean train loss 2851.2436235171194
INFO:root:current train perplexity9.466133117675781
INFO:root:current mean train loss 2842.596962806022
INFO:root:current train perplexity9.400362014770508
INFO:root:current mean train loss 2834.8069308621066
INFO:root:current train perplexity9.344714164733887
INFO:root:current mean train loss 2827.7177523306286
INFO:root:current train perplexity9.286882400512695
INFO:root:current mean train loss 2821.05215231113
INFO:root:current train perplexity9.24621295928955

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.80s/it]
INFO:root:final mean train loss: 2815.4838907816047
INFO:root:final train perplexity: 9.211564064025879
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2527.4681764426805
INFO:root:eval perplexity: 7.721957206726074
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/2
  1%|          | 2/200 [06:39<10:58:16, 199.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2657.44638523911
INFO:root:current train perplexity8.20614242553711
INFO:root:current mean train loss 2684.0942437881813
INFO:root:current train perplexity8.200165748596191
INFO:root:current mean train loss 2668.3299094269714
INFO:root:current train perplexity8.126367568969727
INFO:root:current mean train loss 2655.4438740498313
INFO:root:current train perplexity8.041423797607422
INFO:root:current mean train loss 2647.5449720563292
INFO:root:current train perplexity8.018194198608398
INFO:root:current mean train loss 2642.047708192865
INFO:root:current train perplexity7.997654438018799
INFO:root:current mean train loss 2638.59517858906
INFO:root:current train perplexity7.975720405578613
INFO:root:current mean train loss 2632.0035918315143
INFO:root:current train perplexity7.946495532989502
INFO:root:current mean train loss 2628.817518511311
INFO:root:current train perplexity7.933902740478516
INFO:root:current mean train loss 2629.0225488595256
INFO:root:current train perplexity7.915920734405518
INFO:root:current mean train loss 2625.7360660224317
INFO:root:current train perplexity7.903547763824463
INFO:root:current mean train loss 2621.9188283060043
INFO:root:current train perplexity7.888047218322754
INFO:root:current mean train loss 2617.2053893894463
INFO:root:current train perplexity7.8623151779174805
INFO:root:current mean train loss 2612.9135134125327
INFO:root:current train perplexity7.844731330871582
INFO:root:current mean train loss 2609.196768910421
INFO:root:current train perplexity7.826979160308838
INFO:root:current mean train loss 2606.3751996283586
INFO:root:current train perplexity7.807530879974365
INFO:root:current mean train loss 2603.838787725931
INFO:root:current train perplexity7.785297393798828
INFO:root:current mean train loss 2600.4743330438773
INFO:root:current train perplexity7.76088285446167
INFO:root:current mean train loss 2595.5906031432205
INFO:root:current train perplexity7.739029884338379
INFO:root:current mean train loss 2592.53421284162
INFO:root:current train perplexity7.72158145904541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.03s/it]
INFO:root:final mean train loss: 2590.127741688138
INFO:root:final train perplexity: 7.711637496948242
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2401.824138668412
INFO:root:eval perplexity: 6.9758501052856445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/3
  2%|â–         | 3/200 [09:58<10:53:37, 199.07s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2488.1505151367187
INFO:root:current train perplexity7.138172149658203
INFO:root:current mean train loss 2509.261101074219
INFO:root:current train perplexity7.233515739440918
INFO:root:current mean train loss 2502.433036621094
INFO:root:current train perplexity7.206965923309326
INFO:root:current mean train loss 2511.367393624442
INFO:root:current train perplexity7.208231449127197
INFO:root:current mean train loss 2503.035153266059
INFO:root:current train perplexity7.188543319702148
INFO:root:current mean train loss 2497.4487746360087
INFO:root:current train perplexity7.164811611175537
INFO:root:current mean train loss 2497.4916877629207
INFO:root:current train perplexity7.156467437744141
INFO:root:current mean train loss 2495.2173911132813
INFO:root:current train perplexity7.154842376708984
INFO:root:current mean train loss 2494.635771627987
INFO:root:current train perplexity7.14499044418335
INFO:root:current mean train loss 2493.8654585988897
INFO:root:current train perplexity7.1447930335998535
INFO:root:current mean train loss 2490.9392125883555
INFO:root:current train perplexity7.135283470153809
INFO:root:current mean train loss 2489.2553999660327
INFO:root:current train perplexity7.121542930603027
INFO:root:current mean train loss 2488.87771015625
INFO:root:current train perplexity7.1158928871154785
INFO:root:current mean train loss 2488.5616505714697
INFO:root:current train perplexity7.103504180908203
INFO:root:current mean train loss 2486.80339725889
INFO:root:current train perplexity7.095630168914795
INFO:root:current mean train loss 2482.2296091387348
INFO:root:current train perplexity7.077024936676025
INFO:root:current mean train loss 2479.021182454427
INFO:root:current train perplexity7.053594589233398
INFO:root:current mean train loss 2477.3580293666296
INFO:root:current train perplexity7.046172142028809
INFO:root:current mean train loss 2475.521513605891
INFO:root:current train perplexity7.041096210479736
INFO:root:current mean train loss 2473.881414513221
INFO:root:current train perplexity7.030741214752197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it]
INFO:root:final mean train loss: 2472.458731862432
INFO:root:final train perplexity: 7.028192520141602
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2331.2468131856717
INFO:root:eval perplexity: 6.588829040527344
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/4
  2%|â–         | 4/200 [13:16<10:49:42, 198.89s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2435.1596406395756
INFO:root:current train perplexity6.823568820953369
INFO:root:current mean train loss 2423.2061731030126
INFO:root:current train perplexity6.751260757446289
INFO:root:current mean train loss 2419.7000398671585
INFO:root:current train perplexity6.725189208984375
INFO:root:current mean train loss 2422.405717480735
INFO:root:current train perplexity6.731324195861816
INFO:root:current mean train loss 2414.0760100730226
INFO:root:current train perplexity6.712582111358643
INFO:root:current mean train loss 2412.162152648603
INFO:root:current train perplexity6.690927982330322
INFO:root:current mean train loss 2409.7764310593725
INFO:root:current train perplexity6.686020851135254
INFO:root:current mean train loss 2407.2193886807877
INFO:root:current train perplexity6.678953647613525
INFO:root:current mean train loss 2406.334154034431
INFO:root:current train perplexity6.680360317230225
INFO:root:current mean train loss 2405.861583248166
INFO:root:current train perplexity6.677491188049316
INFO:root:current mean train loss 2406.092623566732
INFO:root:current train perplexity6.67529821395874
INFO:root:current mean train loss 2404.711395865132
INFO:root:current train perplexity6.666147708892822
INFO:root:current mean train loss 2402.8954611810627
INFO:root:current train perplexity6.654682159423828
INFO:root:current mean train loss 2402.657456951239
INFO:root:current train perplexity6.649807453155518
INFO:root:current mean train loss 2402.0136526532942
INFO:root:current train perplexity6.645173072814941
INFO:root:current mean train loss 2398.7989255787083
INFO:root:current train perplexity6.6354265213012695
INFO:root:current mean train loss 2395.8299186354134
INFO:root:current train perplexity6.623743057250977
INFO:root:current mean train loss 2395.8541323322324
INFO:root:current train perplexity6.621113300323486
INFO:root:current mean train loss 2395.3984240310742
INFO:root:current train perplexity6.616668701171875
INFO:root:current mean train loss 2395.4626829751446
INFO:root:current train perplexity6.614192008972168

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 2394.853553544976
INFO:root:final train perplexity: 6.6109395027160645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2281.0798712114915
INFO:root:eval perplexity: 6.3268561363220215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/5
  2%|â–Ž         | 5/200 [16:35<10:45:48, 198.71s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2349.2068394252233
INFO:root:current train perplexity6.275267124176025
INFO:root:current mean train loss 2343.2557777736497
INFO:root:current train perplexity6.3509955406188965
INFO:root:current mean train loss 2339.155731630997
INFO:root:current train perplexity6.325253486633301
INFO:root:current mean train loss 2338.457704861959
INFO:root:current train perplexity6.333391189575195
INFO:root:current mean train loss 2343.2120605973173
INFO:root:current train perplexity6.348803520202637
INFO:root:current mean train loss 2344.8754188851135
INFO:root:current train perplexity6.36223840713501
INFO:root:current mean train loss 2343.5849436263593
INFO:root:current train perplexity6.349477291107178
INFO:root:current mean train loss 2342.460400639748
INFO:root:current train perplexity6.346854209899902
INFO:root:current mean train loss 2346.1002556295957
INFO:root:current train perplexity6.360631942749023
INFO:root:current mean train loss 2341.7268173093717
INFO:root:current train perplexity6.3450493812561035
INFO:root:current mean train loss 2341.871570995373
INFO:root:current train perplexity6.340683460235596
INFO:root:current mean train loss 2341.822258304905
INFO:root:current train perplexity6.33907413482666
INFO:root:current mean train loss 2339.9543107172412
INFO:root:current train perplexity6.3326497077941895
INFO:root:current mean train loss 2338.9533721394623
INFO:root:current train perplexity6.328405380249023
INFO:root:current mean train loss 2339.242263835074
INFO:root:current train perplexity6.330251693725586
INFO:root:current mean train loss 2339.3837836679786
INFO:root:current train perplexity6.330777645111084
INFO:root:current mean train loss 2340.407732530897
INFO:root:current train perplexity6.3285698890686035
INFO:root:current mean train loss 2338.7656481276713
INFO:root:current train perplexity6.322855472564697
INFO:root:current mean train loss 2338.2806650473563
INFO:root:current train perplexity6.318544864654541

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.97s/it]
INFO:root:final mean train loss: 2337.517266270132
INFO:root:final train perplexity: 6.318656921386719
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2245.9912663453015
INFO:root:eval perplexity: 6.149838924407959
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/6
  3%|â–Ž         | 6/200 [19:53<10:42:16, 198.64s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2224.47607421875
INFO:root:current train perplexity5.64691686630249
INFO:root:current mean train loss 2309.07674355082
INFO:root:current train perplexity6.187950611114502
INFO:root:current mean train loss 2296.2009933243935
INFO:root:current train perplexity6.136610507965088
INFO:root:current mean train loss 2290.503262237853
INFO:root:current train perplexity6.122199058532715
INFO:root:current mean train loss 2291.829958185591
INFO:root:current train perplexity6.108856201171875
INFO:root:current mean train loss 2290.5493770759263
INFO:root:current train perplexity6.1034770011901855
INFO:root:current mean train loss 2296.6366750809198
INFO:root:current train perplexity6.110692024230957
INFO:root:current mean train loss 2297.409763047767
INFO:root:current train perplexity6.113835334777832
INFO:root:current mean train loss 2296.1594351055323
INFO:root:current train perplexity6.11420202255249
INFO:root:current mean train loss 2298.3825140306344
INFO:root:current train perplexity6.126441478729248
INFO:root:current mean train loss 2298.134502338482
INFO:root:current train perplexity6.130524158477783
INFO:root:current mean train loss 2295.319290258146
INFO:root:current train perplexity6.127890110015869
INFO:root:current mean train loss 2294.5291810047615
INFO:root:current train perplexity6.122067928314209
INFO:root:current mean train loss 2294.448113643051
INFO:root:current train perplexity6.118438720703125
INFO:root:current mean train loss 2293.7198865347295
INFO:root:current train perplexity6.11258602142334
INFO:root:current mean train loss 2295.457274740017
INFO:root:current train perplexity6.1107659339904785
INFO:root:current mean train loss 2295.068713691516
INFO:root:current train perplexity6.108072757720947
INFO:root:current mean train loss 2292.7582811179545
INFO:root:current train perplexity6.100547790527344
INFO:root:current mean train loss 2293.364918862893
INFO:root:current train perplexity6.101482391357422
INFO:root:current mean train loss 2293.1172063146246
INFO:root:current train perplexity6.1015777587890625

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it]
INFO:root:final mean train loss: 2292.7070487325864
INFO:root:final train perplexity: 6.099255084991455
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2218.026109627798
INFO:root:eval perplexity: 6.0123114585876465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/7
  4%|â–Ž         | 7/200 [23:11<10:38:33, 198.52s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2225.2730170355903
INFO:root:current train perplexity5.795176029205322
INFO:root:current mean train loss 2256.6618879932466
INFO:root:current train perplexity5.911426544189453
INFO:root:current mean train loss 2262.2250836573608
INFO:root:current train perplexity5.964511394500732
INFO:root:current mean train loss 2264.345640554368
INFO:root:current train perplexity5.958261966705322
INFO:root:current mean train loss 2263.2772724936453
INFO:root:current train perplexity5.950800895690918
INFO:root:current mean train loss 2265.107978968086
INFO:root:current train perplexity5.958422660827637
INFO:root:current mean train loss 2262.765412858389
INFO:root:current train perplexity5.948646068572998
INFO:root:current mean train loss 2260.2433055144497
INFO:root:current train perplexity5.948792934417725
INFO:root:current mean train loss 2260.686086192982
INFO:root:current train perplexity5.944265365600586
INFO:root:current mean train loss 2257.4475973956205
INFO:root:current train perplexity5.934068202972412
INFO:root:current mean train loss 2257.300739880395
INFO:root:current train perplexity5.924703598022461
INFO:root:current mean train loss 2255.437487989504
INFO:root:current train perplexity5.925318241119385
INFO:root:current mean train loss 2254.7798668371242
INFO:root:current train perplexity5.922377586364746
INFO:root:current mean train loss 2257.0147359613584
INFO:root:current train perplexity5.924394607543945
INFO:root:current mean train loss 2257.655441908305
INFO:root:current train perplexity5.924346446990967
INFO:root:current mean train loss 2257.6559671796513
INFO:root:current train perplexity5.9275665283203125
INFO:root:current mean train loss 2257.344590610273
INFO:root:current train perplexity5.930046558380127
INFO:root:current mean train loss 2257.3104213230545
INFO:root:current train perplexity5.929342269897461
INFO:root:current mean train loss 2255.8617540953314
INFO:root:current train perplexity5.923448085784912
INFO:root:current mean train loss 2255.3703177952293
INFO:root:current train perplexity5.920475959777832

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 2254.544113028368
INFO:root:final train perplexity: 5.918417930603027
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2196.6220980164007
INFO:root:eval perplexity: 5.909132480621338
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/8
  4%|â–         | 8/200 [26:30<10:35:02, 198.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2199.0512590680805
INFO:root:current train perplexity5.69595193862915
INFO:root:current mean train loss 2228.4666567201966
INFO:root:current train perplexity5.752511501312256
INFO:root:current mean train loss 2225.0121176861703
INFO:root:current train perplexity5.768198013305664
INFO:root:current mean train loss 2222.85708554396
INFO:root:current train perplexity5.770087242126465
INFO:root:current mean train loss 2225.8372334096625
INFO:root:current train perplexity5.778764724731445
INFO:root:current mean train loss 2226.3106333509786
INFO:root:current train perplexity5.779380798339844
INFO:root:current mean train loss 2222.761891763041
INFO:root:current train perplexity5.762956142425537
INFO:root:current mean train loss 2224.2203711269663
INFO:root:current train perplexity5.768898963928223
INFO:root:current mean train loss 2223.1558175640907
INFO:root:current train perplexity5.7719550132751465
INFO:root:current mean train loss 2219.9637009890957
INFO:root:current train perplexity5.760509490966797
INFO:root:current mean train loss 2219.697361512115
INFO:root:current train perplexity5.765167713165283
INFO:root:current mean train loss 2221.145224308232
INFO:root:current train perplexity5.77529239654541
INFO:root:current mean train loss 2219.962647571641
INFO:root:current train perplexity5.768370628356934
INFO:root:current mean train loss 2221.5316292866337
INFO:root:current train perplexity5.770743370056152
INFO:root:current mean train loss 2222.724857513747
INFO:root:current train perplexity5.773284435272217
INFO:root:current mean train loss 2223.4384989884466
INFO:root:current train perplexity5.768733024597168
INFO:root:current mean train loss 2222.2505993764335
INFO:root:current train perplexity5.765855312347412
INFO:root:current mean train loss 2223.2710269807053
INFO:root:current train perplexity5.767212390899658
INFO:root:current mean train loss 2223.4215079907826
INFO:root:current train perplexity5.768105983734131
INFO:root:current mean train loss 2223.1823305272933
INFO:root:current train perplexity5.77137565612793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.57s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.57s/it]
INFO:root:final mean train loss: 2222.9042369786744
INFO:root:final train perplexity: 5.772562503814697
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.36s/it]
INFO:root:eval mean loss: 2178.3326610808676
INFO:root:eval perplexity: 5.8223700523376465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/9
  4%|â–         | 9/200 [29:48<10:31:30, 198.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2165.1900728665864
INFO:root:current train perplexity5.659592628479004
INFO:root:current mean train loss 2178.2916115208677
INFO:root:current train perplexity5.6367034912109375
INFO:root:current mean train loss 2186.374290829613
INFO:root:current train perplexity5.62332820892334
INFO:root:current mean train loss 2190.090187766335
INFO:root:current train perplexity5.641611576080322
INFO:root:current mean train loss 2190.85375193368
INFO:root:current train perplexity5.637553691864014
INFO:root:current mean train loss 2189.1431234608526
INFO:root:current train perplexity5.635339260101318
INFO:root:current mean train loss 2192.3267844732554
INFO:root:current train perplexity5.6485915184021
INFO:root:current mean train loss 2193.593737338452
INFO:root:current train perplexity5.643274307250977
INFO:root:current mean train loss 2192.6677961036075
INFO:root:current train perplexity5.642635822296143
INFO:root:current mean train loss 2192.840950460995
INFO:root:current train perplexity5.643318176269531
INFO:root:current mean train loss 2193.9902304297616
INFO:root:current train perplexity5.644387722015381
INFO:root:current mean train loss 2194.7110457950166
INFO:root:current train perplexity5.644924640655518
INFO:root:current mean train loss 2193.3881079335565
INFO:root:current train perplexity5.642496585845947
INFO:root:current mean train loss 2192.3979464198005
INFO:root:current train perplexity5.641659259796143
INFO:root:current mean train loss 2193.310984713972
INFO:root:current train perplexity5.643543243408203
INFO:root:current mean train loss 2193.572627431339
INFO:root:current train perplexity5.644825458526611
INFO:root:current mean train loss 2193.0358973172906
INFO:root:current train perplexity5.644341945648193
INFO:root:current mean train loss 2193.795536755427
INFO:root:current train perplexity5.645870208740234
INFO:root:current mean train loss 2194.2238503243957
INFO:root:current train perplexity5.646097183227539
INFO:root:current mean train loss 2193.957161074779
INFO:root:current train perplexity5.643259525299072

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.15s/it]
INFO:root:final mean train loss: 2194.567748284737
INFO:root:final train perplexity: 5.6449875831604
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2162.822421026568
INFO:root:eval perplexity: 5.74979305267334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/10
  5%|â–Œ         | 10/200 [33:06<10:27:30, 198.16s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2160.438904693161
INFO:root:current train perplexity5.545893669128418
INFO:root:current mean train loss 2163.3149262377497
INFO:root:current train perplexity5.5333571434021
INFO:root:current mean train loss 2160.523050868378
INFO:root:current train perplexity5.534776210784912
INFO:root:current mean train loss 2163.1935684493565
INFO:root:current train perplexity5.542372703552246
INFO:root:current mean train loss 2166.668758953558
INFO:root:current train perplexity5.544475078582764
INFO:root:current mean train loss 2168.9393153936458
INFO:root:current train perplexity5.547768592834473
INFO:root:current mean train loss 2167.4342143197055
INFO:root:current train perplexity5.541481018066406
INFO:root:current mean train loss 2168.9933986787833
INFO:root:current train perplexity5.545598030090332
INFO:root:current mean train loss 2170.587972239329
INFO:root:current train perplexity5.553264141082764
INFO:root:current mean train loss 2170.279118997517
INFO:root:current train perplexity5.5487236976623535
INFO:root:current mean train loss 2171.093520704221
INFO:root:current train perplexity5.548244953155518
INFO:root:current mean train loss 2170.668077767456
INFO:root:current train perplexity5.548670768737793
INFO:root:current mean train loss 2169.619804556676
INFO:root:current train perplexity5.54284143447876
INFO:root:current mean train loss 2172.088418942459
INFO:root:current train perplexity5.545668601989746
INFO:root:current mean train loss 2172.65403860779
INFO:root:current train perplexity5.544807434082031
INFO:root:current mean train loss 2171.6211493001615
INFO:root:current train perplexity5.541254997253418
INFO:root:current mean train loss 2171.92726723115
INFO:root:current train perplexity5.540905475616455
INFO:root:current mean train loss 2172.1649809600676
INFO:root:current train perplexity5.539709568023682
INFO:root:current mean train loss 2171.5686229789494
INFO:root:current train perplexity5.5394368171691895
INFO:root:current mean train loss 2170.9798979519464
INFO:root:current train perplexity5.53890323638916

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it]
INFO:root:final mean train loss: 2170.4680337064265
INFO:root:final train perplexity: 5.53870964050293
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2149.8837020549367
INFO:root:eval perplexity: 5.689940452575684
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/11
  6%|â–Œ         | 11/200 [36:24<10:24:06, 198.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2134.6777726994005
INFO:root:current train perplexity5.396783828735352
INFO:root:current mean train loss 2144.374060189852
INFO:root:current train perplexity5.432750225067139
INFO:root:current mean train loss 2147.6483209783382
INFO:root:current train perplexity5.43376350402832
INFO:root:current mean train loss 2142.5819015898237
INFO:root:current train perplexity5.408328533172607
INFO:root:current mean train loss 2144.6188578036586
INFO:root:current train perplexity5.420112133026123
INFO:root:current mean train loss 2139.9378939163157
INFO:root:current train perplexity5.409674644470215
INFO:root:current mean train loss 2144.9832163997016
INFO:root:current train perplexity5.436734199523926
INFO:root:current mean train loss 2146.309829362476
INFO:root:current train perplexity5.436647415161133
INFO:root:current mean train loss 2146.3324218474445
INFO:root:current train perplexity5.4360432624816895
INFO:root:current mean train loss 2148.0127933649214
INFO:root:current train perplexity5.441946983337402
INFO:root:current mean train loss 2148.365008556162
INFO:root:current train perplexity5.441382884979248
INFO:root:current mean train loss 2148.0130021350983
INFO:root:current train perplexity5.4387006759643555
INFO:root:current mean train loss 2147.8119459374698
INFO:root:current train perplexity5.440390110015869
INFO:root:current mean train loss 2147.822253558887
INFO:root:current train perplexity5.441769123077393
INFO:root:current mean train loss 2149.005101734023
INFO:root:current train perplexity5.4463300704956055
INFO:root:current mean train loss 2148.894637234124
INFO:root:current train perplexity5.446972370147705
INFO:root:current mean train loss 2147.114920004124
INFO:root:current train perplexity5.441577434539795
INFO:root:current mean train loss 2147.6196078549306
INFO:root:current train perplexity5.441630840301514
INFO:root:current mean train loss 2148.5224354360666
INFO:root:current train perplexity5.442483901977539

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it]
INFO:root:final mean train loss: 2147.97440743266
INFO:root:final train perplexity: 5.441319942474365
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2138.309290676252
INFO:root:eval perplexity: 5.636927604675293
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/12
  6%|â–Œ         | 12/200 [39:42<10:20:44, 198.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2060.676961263021
INFO:root:current train perplexity5.392106533050537
INFO:root:current mean train loss 2121.4846274366655
INFO:root:current train perplexity5.282254695892334
INFO:root:current mean train loss 2128.9158484548184
INFO:root:current train perplexity5.325345516204834
INFO:root:current mean train loss 2128.4248949308994
INFO:root:current train perplexity5.327013969421387
INFO:root:current mean train loss 2129.0921306752093
INFO:root:current train perplexity5.332731246948242
INFO:root:current mean train loss 2130.832188024199
INFO:root:current train perplexity5.3379807472229
INFO:root:current mean train loss 2132.229215859181
INFO:root:current train perplexity5.342672348022461
INFO:root:current mean train loss 2131.0549972772938
INFO:root:current train perplexity5.346085071563721
INFO:root:current mean train loss 2128.9850621205637
INFO:root:current train perplexity5.345470905303955
INFO:root:current mean train loss 2126.7033953661407
INFO:root:current train perplexity5.340560436248779
INFO:root:current mean train loss 2126.758858069347
INFO:root:current train perplexity5.335195064544678
INFO:root:current mean train loss 2128.010097749214
INFO:root:current train perplexity5.342771530151367
INFO:root:current mean train loss 2130.2413586801226
INFO:root:current train perplexity5.348514556884766
INFO:root:current mean train loss 2130.904807453053
INFO:root:current train perplexity5.350405693054199
INFO:root:current mean train loss 2130.5967934051755
INFO:root:current train perplexity5.351845741271973
INFO:root:current mean train loss 2129.434379694387
INFO:root:current train perplexity5.350388050079346
INFO:root:current mean train loss 2128.681235120063
INFO:root:current train perplexity5.350613117218018
INFO:root:current mean train loss 2128.7660096325876
INFO:root:current train perplexity5.351485252380371
INFO:root:current mean train loss 2128.709950985012
INFO:root:current train perplexity5.35272216796875
INFO:root:current mean train loss 2128.5943196828403
INFO:root:current train perplexity5.3544921875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 2127.9655232932073
INFO:root:final train perplexity: 5.3561296463012695
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2129.7968066059952
INFO:root:eval perplexity: 5.598252773284912
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/13
  6%|â–‹         | 13/200 [43:00<10:17:47, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2064.354071044922
INFO:root:current train perplexity5.0437469482421875
INFO:root:current mean train loss 2090.5260447184246
INFO:root:current train perplexity5.237249851226807
INFO:root:current mean train loss 2102.183925559304
INFO:root:current train perplexity5.247328758239746
INFO:root:current mean train loss 2101.300163650513
INFO:root:current train perplexity5.252141952514648
INFO:root:current mean train loss 2107.398348563058
INFO:root:current train perplexity5.260344505310059
INFO:root:current mean train loss 2109.9442749023438
INFO:root:current train perplexity5.269469261169434
INFO:root:current mean train loss 2110.549317390688
INFO:root:current train perplexity5.2694830894470215
INFO:root:current mean train loss 2110.6256774902345
INFO:root:current train perplexity5.272202491760254
INFO:root:current mean train loss 2113.2661600252477
INFO:root:current train perplexity5.283980846405029
INFO:root:current mean train loss 2114.773403532609
INFO:root:current train perplexity5.284923076629639
INFO:root:current mean train loss 2112.769014485677
INFO:root:current train perplexity5.277045726776123
INFO:root:current mean train loss 2110.690983036586
INFO:root:current train perplexity5.27569580078125
INFO:root:current mean train loss 2111.1508714019274
INFO:root:current train perplexity5.281940460205078
INFO:root:current mean train loss 2111.441652979995
INFO:root:current train perplexity5.285855770111084
INFO:root:current mean train loss 2113.0079043965943
INFO:root:current train perplexity5.288565158843994
INFO:root:current mean train loss 2112.140472893966
INFO:root:current train perplexity5.2835164070129395
INFO:root:current mean train loss 2110.817327654803
INFO:root:current train perplexity5.27844762802124
INFO:root:current mean train loss 2109.581188609988
INFO:root:current train perplexity5.2727370262146
INFO:root:current mean train loss 2108.2892621050823
INFO:root:current train perplexity5.27134370803833
INFO:root:current mean train loss 2109.6118942896524
INFO:root:current train perplexity5.275110721588135

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 2109.1119033267146
INFO:root:final train perplexity: 5.277077674865723
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2121.02666846742
INFO:root:eval perplexity: 5.558686256408691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/14
  7%|â–‹         | 14/200 [46:19<10:14:48, 198.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2078.874627190667
INFO:root:current train perplexity5.150747299194336
INFO:root:current mean train loss 2073.5283933764827
INFO:root:current train perplexity5.169391632080078
INFO:root:current mean train loss 2078.4800134328852
INFO:root:current train perplexity5.160374164581299
INFO:root:current mean train loss 2078.7510442988455
INFO:root:current train perplexity5.161729335784912
INFO:root:current mean train loss 2073.8497350766947
INFO:root:current train perplexity5.150585651397705
INFO:root:current mean train loss 2076.0870638657325
INFO:root:current train perplexity5.163890361785889
INFO:root:current mean train loss 2081.161367563101
INFO:root:current train perplexity5.173550605773926
INFO:root:current mean train loss 2084.0799500919584
INFO:root:current train perplexity5.178807258605957
INFO:root:current mean train loss 2085.5650280367945
INFO:root:current train perplexity5.186348915100098
INFO:root:current mean train loss 2086.4095296137107
INFO:root:current train perplexity5.187091827392578
INFO:root:current mean train loss 2087.8615017544225
INFO:root:current train perplexity5.192231178283691
INFO:root:current mean train loss 2087.075238149839
INFO:root:current train perplexity5.187143802642822
INFO:root:current mean train loss 2088.257861249179
INFO:root:current train perplexity5.1908769607543945
INFO:root:current mean train loss 2089.1288111136755
INFO:root:current train perplexity5.19483757019043
INFO:root:current mean train loss 2089.343512740165
INFO:root:current train perplexity5.194046497344971
INFO:root:current mean train loss 2089.8300971066556
INFO:root:current train perplexity5.194511413574219
INFO:root:current mean train loss 2089.179475722549
INFO:root:current train perplexity5.193713188171387
INFO:root:current mean train loss 2091.108826983709
INFO:root:current train perplexity5.200748443603516
INFO:root:current mean train loss 2091.1821831301886
INFO:root:current train perplexity5.2027907371521
INFO:root:current mean train loss 2091.4757968664294
INFO:root:current train perplexity5.201289653778076

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it]
INFO:root:final mean train loss: 2090.951530391137
INFO:root:final train perplexity: 5.202036380767822
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2112.6070807707224
INFO:root:eval perplexity: 5.520965099334717
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/15
  8%|â–Š         | 15/200 [49:37<10:11:40, 198.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2055.3459314416955
INFO:root:current train perplexity5.071508884429932
INFO:root:current mean train loss 2074.2186318930094
INFO:root:current train perplexity5.1324567794799805
INFO:root:current mean train loss 2074.4518149068035
INFO:root:current train perplexity5.121756553649902
INFO:root:current mean train loss 2077.1102894928495
INFO:root:current train perplexity5.135246276855469
INFO:root:current mean train loss 2078.139981576525
INFO:root:current train perplexity5.144041061401367
INFO:root:current mean train loss 2077.5121678101027
INFO:root:current train perplexity5.13812255859375
INFO:root:current mean train loss 2074.7252557503703
INFO:root:current train perplexity5.128346920013428
INFO:root:current mean train loss 2075.687548407193
INFO:root:current train perplexity5.133706569671631
INFO:root:current mean train loss 2075.367840304587
INFO:root:current train perplexity5.136212348937988
INFO:root:current mean train loss 2076.5027792108885
INFO:root:current train perplexity5.139946460723877
INFO:root:current mean train loss 2074.2950338693
INFO:root:current train perplexity5.1342034339904785
INFO:root:current mean train loss 2074.0256237644876
INFO:root:current train perplexity5.132387161254883
INFO:root:current mean train loss 2072.600437097382
INFO:root:current train perplexity5.133510589599609
INFO:root:current mean train loss 2072.2597525524775
INFO:root:current train perplexity5.133325576782227
INFO:root:current mean train loss 2072.5333266225443
INFO:root:current train perplexity5.132686138153076
INFO:root:current mean train loss 2075.3106738155566
INFO:root:current train perplexity5.1366472244262695
INFO:root:current mean train loss 2075.3998815312784
INFO:root:current train perplexity5.139007091522217
INFO:root:current mean train loss 2075.2042532804485
INFO:root:current train perplexity5.138341426849365
INFO:root:current mean train loss 2074.519004188052
INFO:root:current train perplexity5.133762836456299
INFO:root:current mean train loss 2075.287612462117
INFO:root:current train perplexity5.135843276977539

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it]
INFO:root:final mean train loss: 2074.211144767142
INFO:root:final train perplexity: 5.133808135986328
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2107.2351186765845
INFO:root:eval perplexity: 5.497030258178711
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/16
  8%|â–Š         | 16/200 [52:55<10:08:07, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2055.91191509408
INFO:root:current train perplexity5.059985637664795
INFO:root:current mean train loss 2057.1917460480627
INFO:root:current train perplexity5.060492515563965
INFO:root:current mean train loss 2049.9064806273063
INFO:root:current train perplexity5.052504062652588
INFO:root:current mean train loss 2051.622345711022
INFO:root:current train perplexity5.050024032592773
INFO:root:current mean train loss 2049.0463926797206
INFO:root:current train perplexity5.048098087310791
INFO:root:current mean train loss 2055.237317514503
INFO:root:current train perplexity5.065847396850586
INFO:root:current mean train loss 2055.340451918487
INFO:root:current train perplexity5.0697760581970215
INFO:root:current mean train loss 2056.7072693741893
INFO:root:current train perplexity5.069526672363281
INFO:root:current mean train loss 2056.1474556118146
INFO:root:current train perplexity5.064131736755371
INFO:root:current mean train loss 2056.7916734972405
INFO:root:current train perplexity5.0678629875183105
INFO:root:current mean train loss 2056.9609127667995
INFO:root:current train perplexity5.066812515258789
INFO:root:current mean train loss 2057.553297086825
INFO:root:current train perplexity5.070555210113525
INFO:root:current mean train loss 2057.7832151303414
INFO:root:current train perplexity5.072160720825195
INFO:root:current mean train loss 2057.5982357946127
INFO:root:current train perplexity5.071407794952393
INFO:root:current mean train loss 2059.1947189943066
INFO:root:current train perplexity5.073736667633057
INFO:root:current mean train loss 2060.1813389846734
INFO:root:current train perplexity5.076667308807373
INFO:root:current mean train loss 2059.0212555022954
INFO:root:current train perplexity5.074619770050049
INFO:root:current mean train loss 2059.2549214118085
INFO:root:current train perplexity5.074403762817383
INFO:root:current mean train loss 2060.3640807420416
INFO:root:current train perplexity5.0771164894104
INFO:root:current mean train loss 2060.551238502723
INFO:root:current train perplexity5.076271057128906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it]
INFO:root:final mean train loss: 2059.88851231562
INFO:root:final train perplexity: 5.076144218444824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2100.741058133173
INFO:root:eval perplexity: 5.468236446380615
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/17
  8%|â–Š         | 17/200 [56:14<10:04:50, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2017.4915688254616
INFO:root:current train perplexity4.927967548370361
INFO:root:current mean train loss 2015.8231512840757
INFO:root:current train perplexity4.904923439025879
INFO:root:current mean train loss 2030.977387746175
INFO:root:current train perplexity4.9602179527282715
INFO:root:current mean train loss 2033.416097739308
INFO:root:current train perplexity4.975053787231445
INFO:root:current mean train loss 2035.2410100718014
INFO:root:current train perplexity4.980672836303711
INFO:root:current mean train loss 2033.9241685932186
INFO:root:current train perplexity4.98634147644043
INFO:root:current mean train loss 2042.0038918783498
INFO:root:current train perplexity5.001998424530029
INFO:root:current mean train loss 2042.9896331632199
INFO:root:current train perplexity4.9977569580078125
INFO:root:current mean train loss 2043.34813971992
INFO:root:current train perplexity5.001855850219727
INFO:root:current mean train loss 2042.57847119536
INFO:root:current train perplexity5.001835823059082
INFO:root:current mean train loss 2041.849500768325
INFO:root:current train perplexity5.009169578552246
INFO:root:current mean train loss 2040.5855864964751
INFO:root:current train perplexity5.005337238311768
INFO:root:current mean train loss 2043.7191239824947
INFO:root:current train perplexity5.00895881652832
INFO:root:current mean train loss 2046.0035856835093
INFO:root:current train perplexity5.01352596282959
INFO:root:current mean train loss 2046.3405079995432
INFO:root:current train perplexity5.0137152671813965
INFO:root:current mean train loss 2044.517941030507
INFO:root:current train perplexity5.011790752410889
INFO:root:current mean train loss 2044.2818059695276
INFO:root:current train perplexity5.0146684646606445
INFO:root:current mean train loss 2044.5763159971643
INFO:root:current train perplexity5.016708850860596
INFO:root:current mean train loss 2045.5265703362934
INFO:root:current train perplexity5.016479969024658

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it]
INFO:root:final mean train loss: 2045.6173792544243
INFO:root:final train perplexity: 5.0193328857421875
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2096.5398464338155
INFO:root:eval perplexity: 5.4496870040893555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/18
  9%|â–‰         | 18/200 [59:32<10:01:26, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2119.7612548828124
INFO:root:current train perplexity4.853932857513428
INFO:root:current mean train loss 2033.429353841146
INFO:root:current train perplexity4.926340103149414
INFO:root:current mean train loss 2022.4160519483612
INFO:root:current train perplexity4.908621788024902
INFO:root:current mean train loss 2027.6035580494365
INFO:root:current train perplexity4.932805061340332
INFO:root:current mean train loss 2026.3919472776813
INFO:root:current train perplexity4.929022312164307
INFO:root:current mean train loss 2027.6731887666306
INFO:root:current train perplexity4.928188800811768
INFO:root:current mean train loss 2028.554739959969
INFO:root:current train perplexity4.931967735290527
INFO:root:current mean train loss 2029.586292629377
INFO:root:current train perplexity4.940841197967529
INFO:root:current mean train loss 2030.9213014969914
INFO:root:current train perplexity4.948216438293457
INFO:root:current mean train loss 2030.4370770027624
INFO:root:current train perplexity4.951038360595703
INFO:root:current mean train loss 2029.8776468001788
INFO:root:current train perplexity4.949560642242432
INFO:root:current mean train loss 2030.2459907911482
INFO:root:current train perplexity4.952306747436523
INFO:root:current mean train loss 2032.8731126207533
INFO:root:current train perplexity4.953517436981201
INFO:root:current mean train loss 2033.390732197378
INFO:root:current train perplexity4.956112861633301
INFO:root:current mean train loss 2033.7893222795262
INFO:root:current train perplexity4.961855411529541
INFO:root:current mean train loss 2033.4741245003634
INFO:root:current train perplexity4.963609218597412
INFO:root:current mean train loss 2033.911148327979
INFO:root:current train perplexity4.9662089347839355
INFO:root:current mean train loss 2033.7505838612308
INFO:root:current train perplexity4.966716289520264
INFO:root:current mean train loss 2033.7297229375865
INFO:root:current train perplexity4.966236591339111
INFO:root:current mean train loss 2032.8574007289617
INFO:root:current train perplexity4.964972019195557

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.73s/it]
INFO:root:final mean train loss: 2032.2623439679166
INFO:root:final train perplexity: 4.966743469238281
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2091.4210503760805
INFO:root:eval perplexity: 5.427173137664795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/19
 10%|â–‰         | 19/200 [1:02:50<9:58:06, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1998.600791237571
INFO:root:current train perplexity4.769224166870117
INFO:root:current mean train loss 2018.984411020748
INFO:root:current train perplexity4.877723217010498
INFO:root:current mean train loss 2012.7799132750915
INFO:root:current train perplexity4.880915641784668
INFO:root:current mean train loss 2013.049493067013
INFO:root:current train perplexity4.897393226623535
INFO:root:current mean train loss 2017.6825833433613
INFO:root:current train perplexity4.895751476287842
INFO:root:current mean train loss 2014.9300936994882
INFO:root:current train perplexity4.89426326751709
INFO:root:current mean train loss 2011.462566020021
INFO:root:current train perplexity4.893004894256592
INFO:root:current mean train loss 2011.8194514139868
INFO:root:current train perplexity4.890570640563965
INFO:root:current mean train loss 2014.215541867444
INFO:root:current train perplexity4.8987717628479
INFO:root:current mean train loss 2012.7248990602968
INFO:root:current train perplexity4.8965678215026855
INFO:root:current mean train loss 2013.2392959146816
INFO:root:current train perplexity4.898539066314697
INFO:root:current mean train loss 2016.6944283062123
INFO:root:current train perplexity4.901013374328613
INFO:root:current mean train loss 2018.5564864887544
INFO:root:current train perplexity4.9030232429504395
INFO:root:current mean train loss 2018.6835497973004
INFO:root:current train perplexity4.902687072753906
INFO:root:current mean train loss 2019.3655087327488
INFO:root:current train perplexity4.908608913421631
INFO:root:current mean train loss 2019.6743759175324
INFO:root:current train perplexity4.912118434906006
INFO:root:current mean train loss 2019.876118275446
INFO:root:current train perplexity4.9168381690979
INFO:root:current mean train loss 2019.365803469349
INFO:root:current train perplexity4.914872646331787
INFO:root:current mean train loss 2020.8128489254595
INFO:root:current train perplexity4.917186260223389
INFO:root:current mean train loss 2020.0330702576255
INFO:root:current train perplexity4.915341854095459

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 2018.9447067037593
INFO:root:final train perplexity: 4.914850234985352
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2087.233295413619
INFO:root:eval perplexity: 5.4088239669799805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/20
 10%|â–ˆ         | 20/200 [1:06:09<9:54:56, 198.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 2021.1266088241186
INFO:root:current train perplexity4.916810989379883
INFO:root:current mean train loss 1993.6733495039905
INFO:root:current train perplexity4.826145172119141
INFO:root:current mean train loss 1991.7885440842376
INFO:root:current train perplexity4.802186965942383
INFO:root:current mean train loss 1994.355738817063
INFO:root:current train perplexity4.82471227645874
INFO:root:current mean train loss 1998.8376684514699
INFO:root:current train perplexity4.82016134262085
INFO:root:current mean train loss 1999.1062734175703
INFO:root:current train perplexity4.8258056640625
INFO:root:current mean train loss 2003.0555461949214
INFO:root:current train perplexity4.834567070007324
INFO:root:current mean train loss 2002.3511792752029
INFO:root:current train perplexity4.83435583114624
INFO:root:current mean train loss 2004.002590102149
INFO:root:current train perplexity4.833245277404785
INFO:root:current mean train loss 2003.6684843313199
INFO:root:current train perplexity4.840023040771484
INFO:root:current mean train loss 2003.904719362819
INFO:root:current train perplexity4.844513416290283
INFO:root:current mean train loss 2005.6303865266955
INFO:root:current train perplexity4.852252960205078
INFO:root:current mean train loss 2006.0862209514037
INFO:root:current train perplexity4.852919578552246
INFO:root:current mean train loss 2006.7466432943195
INFO:root:current train perplexity4.8600969314575195
INFO:root:current mean train loss 2007.4922352592675
INFO:root:current train perplexity4.863681793212891
INFO:root:current mean train loss 2006.9584205037513
INFO:root:current train perplexity4.863925933837891
INFO:root:current mean train loss 2006.8478896657166
INFO:root:current train perplexity4.865739345550537
INFO:root:current mean train loss 2007.6911112174967
INFO:root:current train perplexity4.865921974182129
INFO:root:current mean train loss 2006.7546708655136
INFO:root:current train perplexity4.866045951843262
INFO:root:current mean train loss 2007.244728060836
INFO:root:current train perplexity4.867744445800781

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.49s/it]
INFO:root:final mean train loss: 2006.520545886372
INFO:root:final train perplexity: 4.866926670074463
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2084.684643468113
INFO:root:eval perplexity: 5.397686958312988
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/21
 10%|â–ˆ         | 21/200 [1:09:27<9:51:25, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1992.5220031738281
INFO:root:current train perplexity4.831088542938232
INFO:root:current mean train loss 2001.6847189878806
INFO:root:current train perplexity4.831052780151367
INFO:root:current mean train loss 2001.5192427635193
INFO:root:current train perplexity4.81038236618042
INFO:root:current mean train loss 1998.1477818864116
INFO:root:current train perplexity4.820990562438965
INFO:root:current mean train loss 1996.2017913282964
INFO:root:current train perplexity4.815681457519531
INFO:root:current mean train loss 1997.8814701656643
INFO:root:current train perplexity4.816568851470947
INFO:root:current mean train loss 1997.7221934620927
INFO:root:current train perplexity4.809625148773193
INFO:root:current mean train loss 1995.4171375093006
INFO:root:current train perplexity4.809568405151367
INFO:root:current mean train loss 1994.652456123138
INFO:root:current train perplexity4.813551902770996
INFO:root:current mean train loss 1995.4018908384953
INFO:root:current train perplexity4.815495014190674
INFO:root:current mean train loss 1995.2632450912938
INFO:root:current train perplexity4.817658424377441
INFO:root:current mean train loss 1994.3526751772342
INFO:root:current train perplexity4.818983554840088
INFO:root:current mean train loss 1995.2185574671266
INFO:root:current train perplexity4.8188090324401855
INFO:root:current mean train loss 1994.962226530092
INFO:root:current train perplexity4.815677642822266
INFO:root:current mean train loss 1995.0002333253294
INFO:root:current train perplexity4.818371772766113
INFO:root:current mean train loss 1994.0800917755362
INFO:root:current train perplexity4.816608905792236
INFO:root:current mean train loss 1995.7408481174045
INFO:root:current train perplexity4.820287704467773
INFO:root:current mean train loss 1996.0526286409765
INFO:root:current train perplexity4.824576377868652
INFO:root:current mean train loss 1995.5129380719416
INFO:root:current train perplexity4.824659824371338
INFO:root:current mean train loss 1995.1159373402352
INFO:root:current train perplexity4.823103427886963

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it]
INFO:root:final mean train loss: 1995.0937034618476
INFO:root:final train perplexity: 4.823264122009277
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2081.0423670559067
INFO:root:eval perplexity: 5.381811141967773
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/22
 11%|â–ˆ         | 22/200 [1:12:45<9:48:03, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1970.964328713613
INFO:root:current train perplexity4.7289347648620605
INFO:root:current mean train loss 1969.5217179314939
INFO:root:current train perplexity4.742735385894775
INFO:root:current mean train loss 1974.0082462296818
INFO:root:current train perplexity4.7502522468566895
INFO:root:current mean train loss 1977.3606058483788
INFO:root:current train perplexity4.745439052581787
INFO:root:current mean train loss 1980.1283722891617
INFO:root:current train perplexity4.759687900543213
INFO:root:current mean train loss 1981.3516739184527
INFO:root:current train perplexity4.75862455368042
INFO:root:current mean train loss 1980.1220354870914
INFO:root:current train perplexity4.758481979370117
INFO:root:current mean train loss 1983.4826712269062
INFO:root:current train perplexity4.771967887878418
INFO:root:current mean train loss 1981.240368610395
INFO:root:current train perplexity4.76735258102417
INFO:root:current mean train loss 1981.1253322119092
INFO:root:current train perplexity4.770016193389893
INFO:root:current mean train loss 1981.6314987367487
INFO:root:current train perplexity4.7685041427612305
INFO:root:current mean train loss 1982.274509179521
INFO:root:current train perplexity4.774564266204834
INFO:root:current mean train loss 1981.878987182713
INFO:root:current train perplexity4.775963306427002
INFO:root:current mean train loss 1980.8142864230074
INFO:root:current train perplexity4.776296138763428
INFO:root:current mean train loss 1980.4503342058088
INFO:root:current train perplexity4.777659893035889
INFO:root:current mean train loss 1981.0971614500556
INFO:root:current train perplexity4.777363300323486
INFO:root:current mean train loss 1983.7109878457954
INFO:root:current train perplexity4.780297756195068
INFO:root:current mean train loss 1983.314488720235
INFO:root:current train perplexity4.778550148010254
INFO:root:current mean train loss 1983.788816273817
INFO:root:current train perplexity4.779426097869873
INFO:root:current mean train loss 1983.7188825882818
INFO:root:current train perplexity4.778820514678955

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it]
INFO:root:final mean train loss: 1983.2250141214977
INFO:root:final train perplexity: 4.778327465057373
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2078.79584822418
INFO:root:eval perplexity: 5.372040748596191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/23
 12%|â–ˆâ–        | 23/200 [1:16:03<9:44:47, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1954.9647583007813
INFO:root:current train perplexity4.6748762130737305
INFO:root:current mean train loss 1963.255845883018
INFO:root:current train perplexity4.692085266113281
INFO:root:current mean train loss 1960.1489943931842
INFO:root:current train perplexity4.6925129890441895
INFO:root:current mean train loss 1966.5614721053687
INFO:root:current train perplexity4.709777355194092
INFO:root:current mean train loss 1962.7500480807557
INFO:root:current train perplexity4.712427139282227
INFO:root:current mean train loss 1963.0272988529528
INFO:root:current train perplexity4.712270736694336
INFO:root:current mean train loss 1966.2178116508153
INFO:root:current train perplexity4.72410774230957
INFO:root:current mean train loss 1968.1587754647944
INFO:root:current train perplexity4.726632595062256
INFO:root:current mean train loss 1968.0207215041257
INFO:root:current train perplexity4.727993965148926
INFO:root:current mean train loss 1967.7576301343513
INFO:root:current train perplexity4.724701881408691
INFO:root:current mean train loss 1967.931950616399
INFO:root:current train perplexity4.727327346801758
INFO:root:current mean train loss 1967.7986681000525
INFO:root:current train perplexity4.727452754974365
INFO:root:current mean train loss 1969.9764301152193
INFO:root:current train perplexity4.731884956359863
INFO:root:current mean train loss 1970.7069329790074
INFO:root:current train perplexity4.733334541320801
INFO:root:current mean train loss 1969.591130977349
INFO:root:current train perplexity4.731858730316162
INFO:root:current mean train loss 1970.8156316025452
INFO:root:current train perplexity4.733950138092041
INFO:root:current mean train loss 1971.9208389191938
INFO:root:current train perplexity4.733240604400635
INFO:root:current mean train loss 1972.0891800012002
INFO:root:current train perplexity4.733565330505371
INFO:root:current mean train loss 1972.2302939117271
INFO:root:current train perplexity4.735876560211182

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.63s/it]
INFO:root:final mean train loss: 1972.4505889169268
INFO:root:final train perplexity: 4.73789644241333
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2075.509213711353
INFO:root:eval perplexity: 5.357782363891602
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/24
 12%|â–ˆâ–        | 24/200 [1:19:21<9:41:27, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1891.4235316685267
INFO:root:current train perplexity4.571828365325928
INFO:root:current mean train loss 1959.6123583071699
INFO:root:current train perplexity4.665903091430664
INFO:root:current mean train loss 1957.9943788685084
INFO:root:current train perplexity4.658810615539551
INFO:root:current mean train loss 1959.4217747989617
INFO:root:current train perplexity4.6719231605529785
INFO:root:current mean train loss 1961.432554502745
INFO:root:current train perplexity4.683309078216553
INFO:root:current mean train loss 1962.0595469578248
INFO:root:current train perplexity4.688268184661865
INFO:root:current mean train loss 1961.8545784612463
INFO:root:current train perplexity4.686336517333984
INFO:root:current mean train loss 1961.4263888390094
INFO:root:current train perplexity4.684020519256592
INFO:root:current mean train loss 1959.115061328609
INFO:root:current train perplexity4.679078578948975
INFO:root:current mean train loss 1962.2431223405629
INFO:root:current train perplexity4.6874098777771
INFO:root:current mean train loss 1962.402457456011
INFO:root:current train perplexity4.685275077819824
INFO:root:current mean train loss 1962.5444333732075
INFO:root:current train perplexity4.685781955718994
INFO:root:current mean train loss 1963.6139185035536
INFO:root:current train perplexity4.695599555969238
INFO:root:current mean train loss 1963.0791385478433
INFO:root:current train perplexity4.6943583488464355
INFO:root:current mean train loss 1962.8364898963607
INFO:root:current train perplexity4.693790435791016
INFO:root:current mean train loss 1962.8002719081785
INFO:root:current train perplexity4.692896842956543
INFO:root:current mean train loss 1961.5317179235376
INFO:root:current train perplexity4.691887855529785
INFO:root:current mean train loss 1961.0637893542673
INFO:root:current train perplexity4.691069602966309
INFO:root:current mean train loss 1962.0467334740981
INFO:root:current train perplexity4.6957011222839355
INFO:root:current mean train loss 1962.2267112631914
INFO:root:current train perplexity4.695800304412842

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1961.4248471627977
INFO:root:final train perplexity: 4.6968770027160645
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2075.8255433427526
INFO:root:eval perplexity: 5.359152317047119
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/25
 12%|â–ˆâ–Ž        | 25/200 [1:22:40<9:38:21, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1961.015640258789
INFO:root:current train perplexity4.67880916595459
INFO:root:current mean train loss 1950.1157009986139
INFO:root:current train perplexity4.640727519989014
INFO:root:current mean train loss 1940.6063063485283
INFO:root:current train perplexity4.642066955566406
INFO:root:current mean train loss 1944.771476086275
INFO:root:current train perplexity4.640622138977051
INFO:root:current mean train loss 1946.4105771622567
INFO:root:current train perplexity4.6435322761535645
INFO:root:current mean train loss 1948.348048552302
INFO:root:current train perplexity4.644153594970703
INFO:root:current mean train loss 1950.9639444595728
INFO:root:current train perplexity4.642695426940918
INFO:root:current mean train loss 1950.0588913385382
INFO:root:current train perplexity4.647618770599365
INFO:root:current mean train loss 1948.2496782321375
INFO:root:current train perplexity4.646998882293701
INFO:root:current mean train loss 1949.2012056953463
INFO:root:current train perplexity4.650003910064697
INFO:root:current mean train loss 1951.5847653150558
INFO:root:current train perplexity4.656348705291748
INFO:root:current mean train loss 1950.2858676027997
INFO:root:current train perplexity4.655102252960205
INFO:root:current mean train loss 1950.5858378690832
INFO:root:current train perplexity4.655472278594971
INFO:root:current mean train loss 1951.141518676389
INFO:root:current train perplexity4.654811382293701
INFO:root:current mean train loss 1950.7091450209027
INFO:root:current train perplexity4.653634548187256
INFO:root:current mean train loss 1950.59238047362
INFO:root:current train perplexity4.6549577713012695
INFO:root:current mean train loss 1950.9813803686884
INFO:root:current train perplexity4.657154560089111
INFO:root:current mean train loss 1951.0583549198584
INFO:root:current train perplexity4.657458305358887
INFO:root:current mean train loss 1951.6110400149696
INFO:root:current train perplexity4.65858793258667
INFO:root:current mean train loss 1951.6048546551172
INFO:root:current train perplexity4.6586761474609375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1950.6833090423877
INFO:root:final train perplexity: 4.657255172729492
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2074.558134471271
INFO:root:eval perplexity: 5.35366153717041
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/26
 13%|â–ˆâ–Ž        | 26/200 [1:25:58<9:35:11, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1933.3298458936738
INFO:root:current train perplexity4.5670390129089355
INFO:root:current mean train loss 1923.0921509654809
INFO:root:current train perplexity4.5634446144104
INFO:root:current mean train loss 1931.3244345257392
INFO:root:current train perplexity4.57196569442749
INFO:root:current mean train loss 1929.7867209694602
INFO:root:current train perplexity4.572970390319824
INFO:root:current mean train loss 1930.8490950742275
INFO:root:current train perplexity4.575357437133789
INFO:root:current mean train loss 1929.5765380859375
INFO:root:current train perplexity4.579635143280029
INFO:root:current mean train loss 1934.037637648084
INFO:root:current train perplexity4.594677925109863
INFO:root:current mean train loss 1935.0021809566358
INFO:root:current train perplexity4.598498821258545
INFO:root:current mean train loss 1933.8021511663012
INFO:root:current train perplexity4.59813117980957
INFO:root:current mean train loss 1935.4043114040915
INFO:root:current train perplexity4.60506534576416
INFO:root:current mean train loss 1937.0903692034776
INFO:root:current train perplexity4.60676383972168
INFO:root:current mean train loss 1937.5496539451071
INFO:root:current train perplexity4.611536026000977
INFO:root:current mean train loss 1937.2499664577142
INFO:root:current train perplexity4.614357948303223
INFO:root:current mean train loss 1937.8304538940156
INFO:root:current train perplexity4.615180969238281
INFO:root:current mean train loss 1937.7386180657963
INFO:root:current train perplexity4.615091323852539
INFO:root:current mean train loss 1939.022267494474
INFO:root:current train perplexity4.619229793548584
INFO:root:current mean train loss 1939.832788368611
INFO:root:current train perplexity4.618971347808838
INFO:root:current mean train loss 1940.3951957303857
INFO:root:current train perplexity4.6175665855407715
INFO:root:current mean train loss 1941.2650604662463
INFO:root:current train perplexity4.621095657348633
INFO:root:current mean train loss 1942.001394343585
INFO:root:current train perplexity4.622087478637695

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it]
INFO:root:final mean train loss: 1941.2031952689167
INFO:root:final train perplexity: 4.622564315795898
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2072.87103228197
INFO:root:eval perplexity: 5.346362590789795
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/27
 14%|â–ˆâ–Ž        | 27/200 [1:29:16<9:31:52, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1910.6141778353988
INFO:root:current train perplexity4.53240442276001
INFO:root:current mean train loss 1921.197046980073
INFO:root:current train perplexity4.548577308654785
INFO:root:current mean train loss 1918.542704264323
INFO:root:current train perplexity4.556461334228516
INFO:root:current mean train loss 1920.7124896342527
INFO:root:current train perplexity4.560283184051514
INFO:root:current mean train loss 1919.4667952758257
INFO:root:current train perplexity4.5573410987854
INFO:root:current mean train loss 1923.5959774550572
INFO:root:current train perplexity4.564016342163086
INFO:root:current mean train loss 1923.8310849268023
INFO:root:current train perplexity4.5684099197387695
INFO:root:current mean train loss 1921.8146281783372
INFO:root:current train perplexity4.565720558166504
INFO:root:current mean train loss 1923.16719421529
INFO:root:current train perplexity4.56824254989624
INFO:root:current mean train loss 1924.380541202171
INFO:root:current train perplexity4.566030502319336
INFO:root:current mean train loss 1924.458914571088
INFO:root:current train perplexity4.568549156188965
INFO:root:current mean train loss 1925.6077426521683
INFO:root:current train perplexity4.568112850189209
INFO:root:current mean train loss 1927.8930792148997
INFO:root:current train perplexity4.577407360076904
INFO:root:current mean train loss 1928.002927260476
INFO:root:current train perplexity4.578866958618164
INFO:root:current mean train loss 1928.9234258288054
INFO:root:current train perplexity4.579124450683594
INFO:root:current mean train loss 1929.338450518744
INFO:root:current train perplexity4.581750392913818
INFO:root:current mean train loss 1929.9188742643387
INFO:root:current train perplexity4.5818328857421875
INFO:root:current mean train loss 1930.878651416071
INFO:root:current train perplexity4.583164691925049
INFO:root:current mean train loss 1931.2927384063426
INFO:root:current train perplexity4.58584451675415
INFO:root:current mean train loss 1931.6978493555087
INFO:root:current train perplexity4.586405277252197

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.51s/it]
INFO:root:final mean train loss: 1931.4223787939675
INFO:root:final train perplexity: 4.587043762207031
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2070.9599172172816
INFO:root:eval perplexity: 5.338105201721191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/28
 14%|â–ˆâ–        | 28/200 [1:32:35<9:28:20, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1937.670654296875
INFO:root:current train perplexity4.562490463256836
INFO:root:current mean train loss 1919.383408203125
INFO:root:current train perplexity4.525781154632568
INFO:root:current mean train loss 1915.804656427557
INFO:root:current train perplexity4.532829284667969
INFO:root:current mean train loss 1920.1027522786458
INFO:root:current train perplexity4.539584159851074
INFO:root:current mean train loss 1918.9092161800986
INFO:root:current train perplexity4.538228511810303
INFO:root:current mean train loss 1919.0769860309103
INFO:root:current train perplexity4.53849983215332
INFO:root:current mean train loss 1920.2324618417244
INFO:root:current train perplexity4.537335395812988
INFO:root:current mean train loss 1923.3805832598287
INFO:root:current train perplexity4.540800094604492
INFO:root:current mean train loss 1920.2895970982142
INFO:root:current train perplexity4.5432047843933105
INFO:root:current mean train loss 1917.712415990585
INFO:root:current train perplexity4.537330627441406
INFO:root:current mean train loss 1916.416208893532
INFO:root:current train perplexity4.536888122558594
INFO:root:current mean train loss 1917.7507026055519
INFO:root:current train perplexity4.5411553382873535
INFO:root:current mean train loss 1918.735765356924
INFO:root:current train perplexity4.5445122718811035
INFO:root:current mean train loss 1918.83826171875
INFO:root:current train perplexity4.54439640045166
INFO:root:current mean train loss 1919.982011884269
INFO:root:current train perplexity4.547362804412842
INFO:root:current mean train loss 1920.801166217138
INFO:root:current train perplexity4.547425746917725
INFO:root:current mean train loss 1921.652966199277
INFO:root:current train perplexity4.549562931060791
INFO:root:current mean train loss 1921.345067259023
INFO:root:current train perplexity4.5491509437561035
INFO:root:current mean train loss 1922.1372217447918
INFO:root:current train perplexity4.552670478820801
INFO:root:current mean train loss 1922.4628056393394
INFO:root:current train perplexity4.553170204162598

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 1922.0060855097922
INFO:root:final train perplexity: 4.553105354309082
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2070.315748282358
INFO:root:eval perplexity: 5.335325717926025
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/29
 14%|â–ˆâ–        | 29/200 [1:35:53<9:25:07, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1891.6491553265116
INFO:root:current train perplexity4.486919403076172
INFO:root:current mean train loss 1892.8636004130046
INFO:root:current train perplexity4.486565589904785
INFO:root:current mean train loss 1891.5942541671127
INFO:root:current train perplexity4.48472785949707
INFO:root:current mean train loss 1897.520903918208
INFO:root:current train perplexity4.492830276489258
INFO:root:current mean train loss 1899.9127522290237
INFO:root:current train perplexity4.500424385070801
INFO:root:current mean train loss 1905.4171474559887
INFO:root:current train perplexity4.509816646575928
INFO:root:current mean train loss 1905.680829351348
INFO:root:current train perplexity4.507601737976074
INFO:root:current mean train loss 1907.153361619121
INFO:root:current train perplexity4.512509822845459
INFO:root:current mean train loss 1905.4981310378275
INFO:root:current train perplexity4.506109237670898
INFO:root:current mean train loss 1910.6904707877866
INFO:root:current train perplexity4.511343002319336
INFO:root:current mean train loss 1911.1749665536286
INFO:root:current train perplexity4.512104034423828
INFO:root:current mean train loss 1911.7139690834404
INFO:root:current train perplexity4.5133867263793945
INFO:root:current mean train loss 1911.7041922648993
INFO:root:current train perplexity4.512171745300293
INFO:root:current mean train loss 1911.0938946954136
INFO:root:current train perplexity4.511951446533203
INFO:root:current mean train loss 1910.852783203125
INFO:root:current train perplexity4.511277198791504
INFO:root:current mean train loss 1911.8120303513417
INFO:root:current train perplexity4.513325214385986
INFO:root:current mean train loss 1912.4690903828218
INFO:root:current train perplexity4.517634391784668
INFO:root:current mean train loss 1912.3001218523298
INFO:root:current train perplexity4.518254280090332
INFO:root:current mean train loss 1913.0394996820494
INFO:root:current train perplexity4.522036075592041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it]
INFO:root:final mean train loss: 1913.11614811715
INFO:root:final train perplexity: 4.521295547485352
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2070.0926158715647
INFO:root:eval perplexity: 5.334362506866455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/30
 15%|â–ˆâ–Œ        | 30/200 [1:39:11<9:21:45, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1820.210205078125
INFO:root:current train perplexity4.402276039123535
INFO:root:current mean train loss 1881.918719090453
INFO:root:current train perplexity4.419865608215332
INFO:root:current mean train loss 1881.9864513634495
INFO:root:current train perplexity4.437405109405518
INFO:root:current mean train loss 1884.9114954679915
INFO:root:current train perplexity4.431737422943115
INFO:root:current mean train loss 1888.1691963177147
INFO:root:current train perplexity4.44244909286499
INFO:root:current mean train loss 1892.7437444360878
INFO:root:current train perplexity4.445216655731201
INFO:root:current mean train loss 1893.6791795752515
INFO:root:current train perplexity4.450843811035156
INFO:root:current mean train loss 1894.8565959634498
INFO:root:current train perplexity4.454061985015869
INFO:root:current mean train loss 1893.85507326633
INFO:root:current train perplexity4.457458972930908
INFO:root:current mean train loss 1894.5098374705635
INFO:root:current train perplexity4.460261344909668
INFO:root:current mean train loss 1896.0967553009245
INFO:root:current train perplexity4.463914394378662
INFO:root:current mean train loss 1898.7384484500042
INFO:root:current train perplexity4.47377347946167
INFO:root:current mean train loss 1898.6624007686491
INFO:root:current train perplexity4.472586154937744
INFO:root:current mean train loss 1899.2634651294823
INFO:root:current train perplexity4.47358512878418
INFO:root:current mean train loss 1899.629327821427
INFO:root:current train perplexity4.475940704345703
INFO:root:current mean train loss 1901.6195727652316
INFO:root:current train perplexity4.480684757232666
INFO:root:current mean train loss 1902.1838018537087
INFO:root:current train perplexity4.483179569244385
INFO:root:current mean train loss 1904.2392276699184
INFO:root:current train perplexity4.483835697174072
INFO:root:current mean train loss 1904.3481568125087
INFO:root:current train perplexity4.486756324768066
INFO:root:current mean train loss 1904.4788821556606
INFO:root:current train perplexity4.488829612731934

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it]
INFO:root:final mean train loss: 1903.976420484765
INFO:root:final train perplexity: 4.488822937011719
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2072.1463389295213
INFO:root:eval perplexity: 5.343230724334717
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/31
 16%|â–ˆâ–Œ        | 31/200 [1:42:29<9:18:23, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1886.0903648963342
INFO:root:current train perplexity4.374789237976074
INFO:root:current mean train loss 1893.155502077133
INFO:root:current train perplexity4.440043926239014
INFO:root:current mean train loss 1881.0544320165584
INFO:root:current train perplexity4.421456336975098
INFO:root:current mean train loss 1886.4895888252493
INFO:root:current train perplexity4.4218244552612305
INFO:root:current mean train loss 1885.6951354120818
INFO:root:current train perplexity4.430883407592773
INFO:root:current mean train loss 1887.6942008711087
INFO:root:current train perplexity4.43387508392334
INFO:root:current mean train loss 1887.9389724487694
INFO:root:current train perplexity4.440079689025879
INFO:root:current mean train loss 1891.211617630047
INFO:root:current train perplexity4.440634250640869
INFO:root:current mean train loss 1893.6760873124906
INFO:root:current train perplexity4.445508003234863
INFO:root:current mean train loss 1894.5840828485675
INFO:root:current train perplexity4.445460796356201
INFO:root:current mean train loss 1892.213854813901
INFO:root:current train perplexity4.440232276916504
INFO:root:current mean train loss 1892.5468938634408
INFO:root:current train perplexity4.440189361572266
INFO:root:current mean train loss 1891.2030651596592
INFO:root:current train perplexity4.439533710479736
INFO:root:current mean train loss 1891.9686811398237
INFO:root:current train perplexity4.44229793548584
INFO:root:current mean train loss 1893.030877197608
INFO:root:current train perplexity4.446550369262695
INFO:root:current mean train loss 1894.0952458012932
INFO:root:current train perplexity4.452730178833008
INFO:root:current mean train loss 1895.6118642283834
INFO:root:current train perplexity4.455423831939697
INFO:root:current mean train loss 1895.5970824629517
INFO:root:current train perplexity4.455658435821533
INFO:root:current mean train loss 1896.3137747857552
INFO:root:current train perplexity4.457637786865234
INFO:root:current mean train loss 1896.0497682438709
INFO:root:current train perplexity4.4578094482421875

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1895.183724284593
INFO:root:final train perplexity: 4.457802772521973
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2068.618930248504
INFO:root:eval perplexity: 5.328008651733398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/32
 16%|â–ˆâ–Œ        | 32/200 [1:45:48<9:15:12, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1868.9015091297238
INFO:root:current train perplexity4.432534217834473
INFO:root:current mean train loss 1886.6080535675262
INFO:root:current train perplexity4.428016185760498
INFO:root:current mean train loss 1877.2813107839827
INFO:root:current train perplexity4.404717445373535
INFO:root:current mean train loss 1875.0790723795099
INFO:root:current train perplexity4.403331279754639
INFO:root:current mean train loss 1877.1399702842832
INFO:root:current train perplexity4.404386520385742
INFO:root:current mean train loss 1877.6969585383574
INFO:root:current train perplexity4.404976844787598
INFO:root:current mean train loss 1878.2398664554578
INFO:root:current train perplexity4.409053325653076
INFO:root:current mean train loss 1875.896980213766
INFO:root:current train perplexity4.406616687774658
INFO:root:current mean train loss 1877.6192224396223
INFO:root:current train perplexity4.410160541534424
INFO:root:current mean train loss 1878.2326796077596
INFO:root:current train perplexity4.409511089324951
INFO:root:current mean train loss 1879.4830757645839
INFO:root:current train perplexity4.411501884460449
INFO:root:current mean train loss 1880.046040479071
INFO:root:current train perplexity4.410995006561279
INFO:root:current mean train loss 1881.0137070328212
INFO:root:current train perplexity4.4145660400390625
INFO:root:current mean train loss 1882.5023793258156
INFO:root:current train perplexity4.414861679077148
INFO:root:current mean train loss 1882.6809688576045
INFO:root:current train perplexity4.416975021362305
INFO:root:current mean train loss 1882.6028691982187
INFO:root:current train perplexity4.41873025894165
INFO:root:current mean train loss 1884.8135964038963
INFO:root:current train perplexity4.423052787780762
INFO:root:current mean train loss 1885.4324563880523
INFO:root:current train perplexity4.4243574142456055
INFO:root:current mean train loss 1886.5574812741625
INFO:root:current train perplexity4.425934791564941
INFO:root:current mean train loss 1887.1332302908277
INFO:root:current train perplexity4.427896022796631

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 1886.488377096433
INFO:root:final train perplexity: 4.427336692810059
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2069.1563093036625
INFO:root:eval perplexity: 5.330325603485107
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/33
 16%|â–ˆâ–‹        | 33/200 [1:49:06<9:12:01, 198.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1853.8184427897136
INFO:root:current train perplexity4.3148369789123535
INFO:root:current mean train loss 1863.004132080078
INFO:root:current train perplexity4.347218036651611
INFO:root:current mean train loss 1855.0258540226862
INFO:root:current train perplexity4.3456501960754395
INFO:root:current mean train loss 1862.7474609375
INFO:root:current train perplexity4.344700813293457
INFO:root:current mean train loss 1862.1468983525815
INFO:root:current train perplexity4.3483428955078125
INFO:root:current mean train loss 1866.3878655569895
INFO:root:current train perplexity4.360970497131348
INFO:root:current mean train loss 1867.2906777121805
INFO:root:current train perplexity4.362678050994873
INFO:root:current mean train loss 1871.0483844957853
INFO:root:current train perplexity4.36744499206543
INFO:root:current mean train loss 1870.2925985646802
INFO:root:current train perplexity4.368653297424316
INFO:root:current mean train loss 1870.8492811838785
INFO:root:current train perplexity4.373968601226807
INFO:root:current mean train loss 1872.1724224738355
INFO:root:current train perplexity4.380396842956543
INFO:root:current mean train loss 1871.63208754967
INFO:root:current train perplexity4.38560676574707
INFO:root:current mean train loss 1871.9040709480407
INFO:root:current train perplexity4.387722492218018
INFO:root:current mean train loss 1871.9306966445026
INFO:root:current train perplexity4.388530731201172
INFO:root:current mean train loss 1874.564653036039
INFO:root:current train perplexity4.391071796417236
INFO:root:current mean train loss 1876.2763142121144
INFO:root:current train perplexity4.393342018127441
INFO:root:current mean train loss 1876.9936281502964
INFO:root:current train perplexity4.395327091217041
INFO:root:current mean train loss 1877.003195398504
INFO:root:current train perplexity4.3979878425598145
INFO:root:current mean train loss 1877.8996391050277
INFO:root:current train perplexity4.400233268737793
INFO:root:current mean train loss 1879.1321211211534
INFO:root:current train perplexity4.400224208831787

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1878.3602811396872
INFO:root:final train perplexity: 4.399046897888184
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2069.9538552575077
INFO:root:eval perplexity: 5.333763599395752
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/34
 17%|â–ˆâ–‹        | 34/200 [1:52:25<9:08:53, 198.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1847.90894188819
INFO:root:current train perplexity4.304168701171875
INFO:root:current mean train loss 1854.2037629380739
INFO:root:current train perplexity4.314353942871094
INFO:root:current mean train loss 1857.7153346753723
INFO:root:current train perplexity4.325812339782715
INFO:root:current mean train loss 1860.6858577171752
INFO:root:current train perplexity4.338472843170166
INFO:root:current mean train loss 1856.307610789685
INFO:root:current train perplexity4.331035614013672
INFO:root:current mean train loss 1859.6729349172715
INFO:root:current train perplexity4.339218616485596
INFO:root:current mean train loss 1857.9064562753877
INFO:root:current train perplexity4.3380022048950195
INFO:root:current mean train loss 1860.1763081789936
INFO:root:current train perplexity4.342181205749512
INFO:root:current mean train loss 1862.8500720451468
INFO:root:current train perplexity4.344167232513428
INFO:root:current mean train loss 1865.460912386251
INFO:root:current train perplexity4.352685451507568
INFO:root:current mean train loss 1865.0771636254497
INFO:root:current train perplexity4.354471206665039
INFO:root:current mean train loss 1867.0035234449674
INFO:root:current train perplexity4.359716892242432
INFO:root:current mean train loss 1869.1659922050887
INFO:root:current train perplexity4.363696575164795
INFO:root:current mean train loss 1870.3121046233887
INFO:root:current train perplexity4.363239765167236
INFO:root:current mean train loss 1870.007171072989
INFO:root:current train perplexity4.364422798156738
INFO:root:current mean train loss 1870.787825541475
INFO:root:current train perplexity4.36604118347168
INFO:root:current mean train loss 1870.8391887776256
INFO:root:current train perplexity4.3691725730896
INFO:root:current mean train loss 1871.138252425744
INFO:root:current train perplexity4.370800971984863
INFO:root:current mean train loss 1870.640826347729
INFO:root:current train perplexity4.370683193206787
INFO:root:current mean train loss 1870.9648974683469
INFO:root:current train perplexity4.37172269821167

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.45s/it]
INFO:root:final mean train loss: 1870.5866296011213
INFO:root:final train perplexity: 4.372159957885742
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2068.6841374390515
INFO:root:eval perplexity: 5.32828950881958
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/35
 18%|â–ˆâ–Š        | 35/200 [1:55:43<9:05:15, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1845.5879412711936
INFO:root:current train perplexity4.290865421295166
INFO:root:current mean train loss 1853.048462543291
INFO:root:current train perplexity4.302846908569336
INFO:root:current mean train loss 1856.763014190051
INFO:root:current train perplexity4.313155174255371
INFO:root:current mean train loss 1863.0930519684923
INFO:root:current train perplexity4.3142547607421875
INFO:root:current mean train loss 1859.1182164489499
INFO:root:current train perplexity4.3103837966918945
INFO:root:current mean train loss 1857.4055722426083
INFO:root:current train perplexity4.315962791442871
INFO:root:current mean train loss 1857.0219427543002
INFO:root:current train perplexity4.314929962158203
INFO:root:current mean train loss 1859.610498385105
INFO:root:current train perplexity4.322705268859863
INFO:root:current mean train loss 1860.9709530004718
INFO:root:current train perplexity4.327508926391602
INFO:root:current mean train loss 1859.6452386192152
INFO:root:current train perplexity4.330012798309326
INFO:root:current mean train loss 1858.690477221278
INFO:root:current train perplexity4.333973407745361
INFO:root:current mean train loss 1859.2243059372383
INFO:root:current train perplexity4.334617614746094
INFO:root:current mean train loss 1860.5825282101284
INFO:root:current train perplexity4.336801052093506
INFO:root:current mean train loss 1860.815589674918
INFO:root:current train perplexity4.338571548461914
INFO:root:current mean train loss 1861.9121469602367
INFO:root:current train perplexity4.338991165161133
INFO:root:current mean train loss 1862.1122676222358
INFO:root:current train perplexity4.336963176727295
INFO:root:current mean train loss 1861.9821267876744
INFO:root:current train perplexity4.339584827423096
INFO:root:current mean train loss 1862.720624058276
INFO:root:current train perplexity4.3413615226745605
INFO:root:current mean train loss 1863.0322935916051
INFO:root:current train perplexity4.342905521392822

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.62s/it]
INFO:root:final mean train loss: 1862.2447628573343
INFO:root:final train perplexity: 4.343490123748779
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2071.551376883865
INFO:root:eval perplexity: 5.340659141540527
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/36
 18%|â–ˆâ–Š        | 36/200 [1:59:01<9:01:51, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1814.0499933416193
INFO:root:current train perplexity4.1289873123168945
INFO:root:current mean train loss 1840.2355220210445
INFO:root:current train perplexity4.243528842926025
INFO:root:current mean train loss 1836.2632518605599
INFO:root:current train perplexity4.2404255867004395
INFO:root:current mean train loss 1839.9092825248695
INFO:root:current train perplexity4.25863790512085
INFO:root:current mean train loss 1839.9950441235173
INFO:root:current train perplexity4.263558387756348
INFO:root:current mean train loss 1842.7318994331733
INFO:root:current train perplexity4.277912139892578
INFO:root:current mean train loss 1845.4464832561887
INFO:root:current train perplexity4.2837653160095215
INFO:root:current mean train loss 1845.806084698598
INFO:root:current train perplexity4.292922496795654
INFO:root:current mean train loss 1846.9083891655748
INFO:root:current train perplexity4.29934549331665
INFO:root:current mean train loss 1848.4563279052466
INFO:root:current train perplexity4.301251411437988
INFO:root:current mean train loss 1850.5219704828912
INFO:root:current train perplexity4.301588535308838
INFO:root:current mean train loss 1850.8662851026313
INFO:root:current train perplexity4.30424690246582
INFO:root:current mean train loss 1850.1868814835364
INFO:root:current train perplexity4.303487777709961
INFO:root:current mean train loss 1850.7069747307935
INFO:root:current train perplexity4.303583145141602
INFO:root:current mean train loss 1852.126078994286
INFO:root:current train perplexity4.308322906494141
INFO:root:current mean train loss 1853.6872964956206
INFO:root:current train perplexity4.31392765045166
INFO:root:current mean train loss 1853.8264899700787
INFO:root:current train perplexity4.313525676727295
INFO:root:current mean train loss 1853.8544354686815
INFO:root:current train perplexity4.3125457763671875
INFO:root:current mean train loss 1854.6311201646406
INFO:root:current train perplexity4.3146586418151855
INFO:root:current mean train loss 1854.9146267318526
INFO:root:current train perplexity4.315292835235596

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it]
INFO:root:final mean train loss: 1854.8131177077917
INFO:root:final train perplexity: 4.318108081817627
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2070.100188992548
INFO:root:eval perplexity: 5.334394931793213
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/37
 18%|â–ˆâ–Š        | 37/200 [2:02:19<8:58:42, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1847.6343514578682
INFO:root:current train perplexity4.268572807312012
INFO:root:current mean train loss 1842.656123161316
INFO:root:current train perplexity4.285794734954834
INFO:root:current mean train loss 1827.7832994962994
INFO:root:current train perplexity4.2600016593933105
INFO:root:current mean train loss 1832.9580148836462
INFO:root:current train perplexity4.252201557159424
INFO:root:current mean train loss 1832.8751748343495
INFO:root:current train perplexity4.252987384796143
INFO:root:current mean train loss 1835.9251343698213
INFO:root:current train perplexity4.256689548492432
INFO:root:current mean train loss 1836.9979113925035
INFO:root:current train perplexity4.258552551269531
INFO:root:current mean train loss 1839.5862725226434
INFO:root:current train perplexity4.261008262634277
INFO:root:current mean train loss 1840.582227918837
INFO:root:current train perplexity4.26622200012207
INFO:root:current mean train loss 1842.9645560692097
INFO:root:current train perplexity4.273222923278809
INFO:root:current mean train loss 1842.0599797467778
INFO:root:current train perplexity4.273558139801025
INFO:root:current mean train loss 1843.5268362058816
INFO:root:current train perplexity4.277454853057861
INFO:root:current mean train loss 1844.89510551726
INFO:root:current train perplexity4.281558990478516
INFO:root:current mean train loss 1845.9806124216102
INFO:root:current train perplexity4.283439636230469
INFO:root:current mean train loss 1845.0763215564546
INFO:root:current train perplexity4.284007549285889
INFO:root:current mean train loss 1845.9634089444944
INFO:root:current train perplexity4.286181926727295
INFO:root:current mean train loss 1845.6544137715707
INFO:root:current train perplexity4.285900592803955
INFO:root:current mean train loss 1845.6045634658249
INFO:root:current train perplexity4.284378528594971
INFO:root:current mean train loss 1846.1151111694603
INFO:root:current train perplexity4.2867817878723145
INFO:root:current mean train loss 1847.2277794675708
INFO:root:current train perplexity4.289044380187988

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.52s/it]
INFO:root:final mean train loss: 1846.5271642149187
INFO:root:final train perplexity: 4.289981842041016
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2071.0966606410684
INFO:root:eval perplexity: 5.338695526123047
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/38
 19%|â–ˆâ–‰        | 38/200 [2:05:37<8:55:14, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1809.3286051432292
INFO:root:current train perplexity4.200445175170898
INFO:root:current mean train loss 1811.6524136247306
INFO:root:current train perplexity4.203697204589844
INFO:root:current mean train loss 1829.9911576251595
INFO:root:current train perplexity4.233954429626465
INFO:root:current mean train loss 1833.642477284307
INFO:root:current train perplexity4.244879722595215
INFO:root:current mean train loss 1836.978925726387
INFO:root:current train perplexity4.2551774978637695
INFO:root:current mean train loss 1838.0975800960437
INFO:root:current train perplexity4.25518798828125
INFO:root:current mean train loss 1838.0163248697916
INFO:root:current train perplexity4.260865688323975
INFO:root:current mean train loss 1838.418284330432
INFO:root:current train perplexity4.262019634246826
INFO:root:current mean train loss 1836.9053309333394
INFO:root:current train perplexity4.256165981292725
INFO:root:current mean train loss 1838.1861284205522
INFO:root:current train perplexity4.255891799926758
INFO:root:current mean train loss 1837.7809678949238
INFO:root:current train perplexity4.255034923553467
INFO:root:current mean train loss 1836.9253894522722
INFO:root:current train perplexity4.253719329833984
INFO:root:current mean train loss 1837.4649044419866
INFO:root:current train perplexity4.254941940307617
INFO:root:current mean train loss 1836.9180962658284
INFO:root:current train perplexity4.253436088562012
INFO:root:current mean train loss 1837.9040828929228
INFO:root:current train perplexity4.25682258605957
INFO:root:current mean train loss 1837.9851592523767
INFO:root:current train perplexity4.259774684906006
INFO:root:current mean train loss 1837.7431228034764
INFO:root:current train perplexity4.258098602294922
INFO:root:current mean train loss 1838.4437278244761
INFO:root:current train perplexity4.258434772491455
INFO:root:current mean train loss 1839.3254202659214
INFO:root:current train perplexity4.261562347412109
INFO:root:current mean train loss 1840.2128106046152
INFO:root:current train perplexity4.264959812164307

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.35s/it]
INFO:root:final mean train loss: 1839.216654398077
INFO:root:final train perplexity: 4.265318393707275
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2071.2991856784683
INFO:root:eval perplexity: 5.339570045471191
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/39
 20%|â–ˆâ–‰        | 39/200 [2:08:55<8:51:38, 198.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1813.9934830204134
INFO:root:current train perplexity4.22548770904541
INFO:root:current mean train loss 1827.6532577703028
INFO:root:current train perplexity4.22235107421875
INFO:root:current mean train loss 1823.0901820059041
INFO:root:current train perplexity4.20835542678833
INFO:root:current mean train loss 1823.3965201193457
INFO:root:current train perplexity4.2072834968566895
INFO:root:current mean train loss 1825.300384653595
INFO:root:current train perplexity4.209669589996338
INFO:root:current mean train loss 1826.387565639944
INFO:root:current train perplexity4.21438455581665
INFO:root:current mean train loss 1826.9998078591154
INFO:root:current train perplexity4.2207489013671875
INFO:root:current mean train loss 1826.9725210435122
INFO:root:current train perplexity4.22048807144165
INFO:root:current mean train loss 1826.1874157403295
INFO:root:current train perplexity4.219707012176514
INFO:root:current mean train loss 1826.5557778848183
INFO:root:current train perplexity4.221989631652832
INFO:root:current mean train loss 1828.4214600528926
INFO:root:current train perplexity4.229012489318848
INFO:root:current mean train loss 1826.967190924692
INFO:root:current train perplexity4.225431442260742
INFO:root:current mean train loss 1827.7713469249888
INFO:root:current train perplexity4.230621337890625
INFO:root:current mean train loss 1828.571326167573
INFO:root:current train perplexity4.234612941741943
INFO:root:current mean train loss 1829.903369908783
INFO:root:current train perplexity4.236449718475342
INFO:root:current mean train loss 1830.3365519153629
INFO:root:current train perplexity4.238802433013916
INFO:root:current mean train loss 1831.252045816176
INFO:root:current train perplexity4.242276191711426
INFO:root:current mean train loss 1832.0491125169597
INFO:root:current train perplexity4.242814540863037
INFO:root:current mean train loss 1832.5412598311837
INFO:root:current train perplexity4.2415595054626465
INFO:root:current mean train loss 1833.230977562954
INFO:root:current train perplexity4.242459297180176

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.08s/it]
INFO:root:final mean train loss: 1832.3971470512047
INFO:root:final train perplexity: 4.242440223693848
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2073.6359607885915
INFO:root:eval perplexity: 5.349671363830566
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/40
 20%|â–ˆâ–ˆ        | 40/200 [2:12:14<8:48:43, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1815.322664285008
INFO:root:current train perplexity4.185688018798828
INFO:root:current mean train loss 1818.7667024921438
INFO:root:current train perplexity4.186270236968994
INFO:root:current mean train loss 1806.7050024326556
INFO:root:current train perplexity4.176276683807373
INFO:root:current mean train loss 1811.6545625953372
INFO:root:current train perplexity4.184181213378906
INFO:root:current mean train loss 1814.2121161538523
INFO:root:current train perplexity4.187399387359619
INFO:root:current mean train loss 1812.7077910797173
INFO:root:current train perplexity4.1847381591796875
INFO:root:current mean train loss 1814.0324120949927
INFO:root:current train perplexity4.1921210289001465
INFO:root:current mean train loss 1815.0823209907032
INFO:root:current train perplexity4.189670085906982
INFO:root:current mean train loss 1818.0957995036085
INFO:root:current train perplexity4.195201873779297
INFO:root:current mean train loss 1818.2061758849911
INFO:root:current train perplexity4.197094917297363
INFO:root:current mean train loss 1818.9859363460453
INFO:root:current train perplexity4.196688175201416
INFO:root:current mean train loss 1818.752054177269
INFO:root:current train perplexity4.1974711418151855
INFO:root:current mean train loss 1818.9072772421998
INFO:root:current train perplexity4.19911527633667
INFO:root:current mean train loss 1820.7806355764765
INFO:root:current train perplexity4.2024431228637695
INFO:root:current mean train loss 1821.4228574225353
INFO:root:current train perplexity4.205568790435791
INFO:root:current mean train loss 1822.8607274370152
INFO:root:current train perplexity4.208766460418701
INFO:root:current mean train loss 1823.6041859575082
INFO:root:current train perplexity4.213879585266113
INFO:root:current mean train loss 1823.6187329691672
INFO:root:current train perplexity4.2141008377075195
INFO:root:current mean train loss 1824.3802143967869
INFO:root:current train perplexity4.215853691101074
INFO:root:current mean train loss 1825.3005319280176
INFO:root:current train perplexity4.2164626121521

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it]
INFO:root:final mean train loss: 1824.6866457540461
INFO:root:final train perplexity: 4.216720104217529
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2075.168723248421
INFO:root:eval perplexity: 5.356306552886963
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/41
 20%|â–ˆâ–ˆ        | 41/200 [2:15:32<8:45:38, 198.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1799.443728129069
INFO:root:current train perplexity4.148143291473389
INFO:root:current mean train loss 1803.5760591468033
INFO:root:current train perplexity4.133002758026123
INFO:root:current mean train loss 1805.8391851476722
INFO:root:current train perplexity4.13634729385376
INFO:root:current mean train loss 1811.0115831163193
INFO:root:current train perplexity4.147254943847656
INFO:root:current mean train loss 1810.7025956184634
INFO:root:current train perplexity4.149311065673828
INFO:root:current mean train loss 1811.4591259028286
INFO:root:current train perplexity4.156473159790039
INFO:root:current mean train loss 1812.453541722791
INFO:root:current train perplexity4.158683776855469
INFO:root:current mean train loss 1813.2784885425663
INFO:root:current train perplexity4.16424036026001
INFO:root:current mean train loss 1812.686473437718
INFO:root:current train perplexity4.168173313140869
INFO:root:current mean train loss 1814.07708654442
INFO:root:current train perplexity4.173806667327881
INFO:root:current mean train loss 1813.0472555787023
INFO:root:current train perplexity4.176418781280518
INFO:root:current mean train loss 1813.4387596921379
INFO:root:current train perplexity4.178373336791992
INFO:root:current mean train loss 1815.3399018652644
INFO:root:current train perplexity4.183955669403076
INFO:root:current mean train loss 1815.5459573740263
INFO:root:current train perplexity4.184870719909668
INFO:root:current mean train loss 1815.3705506350268
INFO:root:current train perplexity4.185688018798828
INFO:root:current mean train loss 1815.527233458402
INFO:root:current train perplexity4.18894100189209
INFO:root:current mean train loss 1816.5342484960015
INFO:root:current train perplexity4.190362453460693
INFO:root:current mean train loss 1816.6196882422093
INFO:root:current train perplexity4.189324855804443
INFO:root:current mean train loss 1818.2983337273577
INFO:root:current train perplexity4.192337512969971

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it]
INFO:root:final mean train loss: 1817.792290745753
INFO:root:final train perplexity: 4.193855285644531
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2074.0307651817375
INFO:root:eval perplexity: 5.35137939453125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/42
 21%|â–ˆâ–ˆ        | 42/200 [2:18:51<8:42:17, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1834.296367938702
INFO:root:current train perplexity4.2460618019104
INFO:root:current mean train loss 1806.6320152620299
INFO:root:current train perplexity4.165931224822998
INFO:root:current mean train loss 1802.3586844144293
INFO:root:current train perplexity4.164495468139648
INFO:root:current mean train loss 1806.5649940563849
INFO:root:current train perplexity4.170015335083008
INFO:root:current mean train loss 1809.39819956634
INFO:root:current train perplexity4.165388107299805
INFO:root:current mean train loss 1810.9154043216222
INFO:root:current train perplexity4.159072399139404
INFO:root:current mean train loss 1810.5557690071244
INFO:root:current train perplexity4.16004753112793
INFO:root:current mean train loss 1810.068211623696
INFO:root:current train perplexity4.160118103027344
INFO:root:current mean train loss 1811.3156998037266
INFO:root:current train perplexity4.163458347320557
INFO:root:current mean train loss 1810.5403603494233
INFO:root:current train perplexity4.165168762207031
INFO:root:current mean train loss 1809.8849342097653
INFO:root:current train perplexity4.166171073913574
INFO:root:current mean train loss 1810.4547133398612
INFO:root:current train perplexity4.165881156921387
INFO:root:current mean train loss 1809.8772866295471
INFO:root:current train perplexity4.167492866516113
INFO:root:current mean train loss 1810.2112599218156
INFO:root:current train perplexity4.164360046386719
INFO:root:current mean train loss 1810.2402551952018
INFO:root:current train perplexity4.164876937866211
INFO:root:current mean train loss 1809.8317652448313
INFO:root:current train perplexity4.165588855743408
INFO:root:current mean train loss 1810.145522267199
INFO:root:current train perplexity4.167062759399414
INFO:root:current mean train loss 1809.6785928578106
INFO:root:current train perplexity4.168548583984375
INFO:root:current mean train loss 1811.5757479617691
INFO:root:current train perplexity4.172979354858398
INFO:root:current mean train loss 1810.9105149950585
INFO:root:current train perplexity4.170305252075195

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it]
INFO:root:final mean train loss: 1810.3245992117077
INFO:root:final train perplexity: 4.1692280769348145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2076.701719027039
INFO:root:eval perplexity: 5.362951278686523
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/43
 22%|â–ˆâ–ˆâ–       | 43/200 [2:22:09<8:39:08, 198.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1779.8363403320313
INFO:root:current train perplexity4.086049556732178
INFO:root:current mean train loss 1785.6523681640624
INFO:root:current train perplexity4.0953450202941895
INFO:root:current mean train loss 1783.4391283118207
INFO:root:current train perplexity4.0984110832214355
INFO:root:current mean train loss 1789.019591915246
INFO:root:current train perplexity4.112570762634277
INFO:root:current mean train loss 1790.4844067950582
INFO:root:current train perplexity4.119076728820801
INFO:root:current mean train loss 1788.7203456662735
INFO:root:current train perplexity4.109476566314697
INFO:root:current mean train loss 1790.9411725725447
INFO:root:current train perplexity4.11558198928833
INFO:root:current mean train loss 1790.544924884953
INFO:root:current train perplexity4.1134562492370605
INFO:root:current mean train loss 1791.9888220361916
INFO:root:current train perplexity4.11991548538208
INFO:root:current mean train loss 1794.0956599409863
INFO:root:current train perplexity4.122361660003662
INFO:root:current mean train loss 1796.328775646617
INFO:root:current train perplexity4.1261982917785645
INFO:root:current mean train loss 1796.3428502445727
INFO:root:current train perplexity4.12787389755249
INFO:root:current mean train loss 1797.521788161363
INFO:root:current train perplexity4.132419109344482
INFO:root:current mean train loss 1800.2701541757226
INFO:root:current train perplexity4.13442850112915
INFO:root:current mean train loss 1800.964567341838
INFO:root:current train perplexity4.137838363647461
INFO:root:current mean train loss 1802.5072783426522
INFO:root:current train perplexity4.140169143676758
INFO:root:current mean train loss 1802.1231855708397
INFO:root:current train perplexity4.141113758087158
INFO:root:current mean train loss 1803.4936541077718
INFO:root:current train perplexity4.14531946182251
INFO:root:current mean train loss 1803.954692702997
INFO:root:current train perplexity4.14544153213501
INFO:root:current mean train loss 1804.5729511794648
INFO:root:current train perplexity4.147738456726074

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 1804.1910480108756
INFO:root:final train perplexity: 4.149108409881592
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2077.777838957225
INFO:root:eval perplexity: 5.367620944976807
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/44
 22%|â–ˆâ–ˆâ–       | 44/200 [2:25:28<8:35:59, 198.46s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1812.5156561668882
INFO:root:current train perplexity4.097025394439697
INFO:root:current mean train loss 1779.6857063137754
INFO:root:current train perplexity4.076080322265625
INFO:root:current mean train loss 1784.4849754673266
INFO:root:current train perplexity4.0823259353637695
INFO:root:current mean train loss 1785.9206194698982
INFO:root:current train perplexity4.108959197998047
INFO:root:current mean train loss 1784.8899988093365
INFO:root:current train perplexity4.106779098510742
INFO:root:current mean train loss 1789.5824137518566
INFO:root:current train perplexity4.102919101715088
INFO:root:current mean train loss 1789.664167401227
INFO:root:current train perplexity4.103989124298096
INFO:root:current mean train loss 1788.9502897658342
INFO:root:current train perplexity4.10108757019043
INFO:root:current mean train loss 1789.9270387039276
INFO:root:current train perplexity4.102929592132568
INFO:root:current mean train loss 1791.7534233826393
INFO:root:current train perplexity4.108593463897705
INFO:root:current mean train loss 1793.1161842382626
INFO:root:current train perplexity4.109936237335205
INFO:root:current mean train loss 1793.4455367390135
INFO:root:current train perplexity4.110180377960205
INFO:root:current mean train loss 1793.2293806894359
INFO:root:current train perplexity4.10986852645874
INFO:root:current mean train loss 1793.1793743946328
INFO:root:current train perplexity4.1110334396362305
INFO:root:current mean train loss 1793.7795071025182
INFO:root:current train perplexity4.1140570640563965
INFO:root:current mean train loss 1794.1473409187895
INFO:root:current train perplexity4.11580228805542
INFO:root:current mean train loss 1794.0997232183515
INFO:root:current train perplexity4.115614414215088
INFO:root:current mean train loss 1795.1903363355036
INFO:root:current train perplexity4.1170806884765625
INFO:root:current mean train loss 1796.226584310072
INFO:root:current train perplexity4.120449542999268
INFO:root:current mean train loss 1796.8019679088623
INFO:root:current train perplexity4.123220443725586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it]
INFO:root:final mean train loss: 1796.782106585173
INFO:root:final train perplexity: 4.124936103820801
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2078.2108028763573
INFO:root:eval perplexity: 5.369500637054443
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/45
 22%|â–ˆâ–ˆâ–Ž       | 45/200 [2:28:46<8:32:27, 198.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1799.3151664733887
INFO:root:current train perplexity4.077812194824219
INFO:root:current mean train loss 1781.2791547077459
INFO:root:current train perplexity4.067410469055176
INFO:root:current mean train loss 1775.3495733087714
INFO:root:current train perplexity4.053746223449707
INFO:root:current mean train loss 1779.1927667974117
INFO:root:current train perplexity4.055901050567627
INFO:root:current mean train loss 1774.5494324256633
INFO:root:current train perplexity4.053370475769043
INFO:root:current mean train loss 1777.10985072792
INFO:root:current train perplexity4.064404010772705
INFO:root:current mean train loss 1779.0337276228938
INFO:root:current train perplexity4.065642833709717
INFO:root:current mean train loss 1780.2974626631012
INFO:root:current train perplexity4.071275234222412
INFO:root:current mean train loss 1780.2158077381275
INFO:root:current train perplexity4.07078218460083
INFO:root:current mean train loss 1783.4167198086182
INFO:root:current train perplexity4.074758052825928
INFO:root:current mean train loss 1784.1567747646704
INFO:root:current train perplexity4.076900005340576
INFO:root:current mean train loss 1784.6862904132436
INFO:root:current train perplexity4.079573154449463
INFO:root:current mean train loss 1784.3372429956364
INFO:root:current train perplexity4.081664562225342
INFO:root:current mean train loss 1786.889396689854
INFO:root:current train perplexity4.087010383605957
INFO:root:current mean train loss 1786.7738023768356
INFO:root:current train perplexity4.089351177215576
INFO:root:current mean train loss 1787.9569083211368
INFO:root:current train perplexity4.0937275886535645
INFO:root:current mean train loss 1788.7281469198374
INFO:root:current train perplexity4.097050189971924
INFO:root:current mean train loss 1788.5867197464922
INFO:root:current train perplexity4.09674072265625
INFO:root:current mean train loss 1788.8129971876676
INFO:root:current train perplexity4.098659515380859
INFO:root:current mean train loss 1791.1266076113202
INFO:root:current train perplexity4.104091167449951

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it]
INFO:root:final mean train loss: 1790.5430538852709
INFO:root:final train perplexity: 4.104689121246338
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2079.139758387356
INFO:root:eval perplexity: 5.37353515625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/46
 23%|â–ˆâ–ˆâ–Ž       | 46/200 [2:32:04<8:29:03, 198.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1747.1541311005014
INFO:root:current train perplexity4.039622783660889
INFO:root:current mean train loss 1758.05104832096
INFO:root:current train perplexity4.026153087615967
INFO:root:current mean train loss 1768.045869331767
INFO:root:current train perplexity4.035765171051025
INFO:root:current mean train loss 1771.4312942785227
INFO:root:current train perplexity4.045669078826904
INFO:root:current mean train loss 1771.124694951111
INFO:root:current train perplexity4.042817115783691
INFO:root:current mean train loss 1773.1463688179056
INFO:root:current train perplexity4.046694278717041
INFO:root:current mean train loss 1774.2580225828285
INFO:root:current train perplexity4.048829555511475
INFO:root:current mean train loss 1777.011358478463
INFO:root:current train perplexity4.0568928718566895
INFO:root:current mean train loss 1778.2122291452363
INFO:root:current train perplexity4.0630340576171875
INFO:root:current mean train loss 1778.0349012835675
INFO:root:current train perplexity4.065597057342529
INFO:root:current mean train loss 1778.359272352531
INFO:root:current train perplexity4.0665764808654785
INFO:root:current mean train loss 1779.0011807020996
INFO:root:current train perplexity4.069891452789307
INFO:root:current mean train loss 1779.9587374708785
INFO:root:current train perplexity4.0720906257629395
INFO:root:current mean train loss 1779.7653679540415
INFO:root:current train perplexity4.072504043579102
INFO:root:current mean train loss 1779.5277887585194
INFO:root:current train perplexity4.072113513946533
INFO:root:current mean train loss 1779.722626909887
INFO:root:current train perplexity4.072218894958496
INFO:root:current mean train loss 1781.3942813725787
INFO:root:current train perplexity4.075461387634277
INFO:root:current mean train loss 1781.8941511939177
INFO:root:current train perplexity4.077746391296387
INFO:root:current mean train loss 1782.3462038663777
INFO:root:current train perplexity4.079012393951416
INFO:root:current mean train loss 1783.6887317948242
INFO:root:current train perplexity4.080821514129639

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it]
INFO:root:final mean train loss: 1783.1180040722115
INFO:root:final train perplexity: 4.080722808837891
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2081.6214274954286
INFO:root:eval perplexity: 5.384332180023193
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/47
 24%|â–ˆâ–ˆâ–Ž       | 47/200 [2:35:22<8:25:33, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1778.0668995137116
INFO:root:current train perplexity4.018554210662842
INFO:root:current mean train loss 1782.904319686119
INFO:root:current train perplexity4.039437770843506
INFO:root:current mean train loss 1775.1936153949507
INFO:root:current train perplexity4.040877342224121
INFO:root:current mean train loss 1774.0978651190524
INFO:root:current train perplexity4.0396223068237305
INFO:root:current mean train loss 1772.082938443226
INFO:root:current train perplexity4.039006233215332
INFO:root:current mean train loss 1770.9956203703098
INFO:root:current train perplexity4.0440192222595215
INFO:root:current mean train loss 1772.8139092300546
INFO:root:current train perplexity4.044841289520264
INFO:root:current mean train loss 1774.124879612361
INFO:root:current train perplexity4.046538352966309
INFO:root:current mean train loss 1774.1626154638345
INFO:root:current train perplexity4.04896879196167
INFO:root:current mean train loss 1773.9694364314566
INFO:root:current train perplexity4.046940803527832
INFO:root:current mean train loss 1775.2067665419727
INFO:root:current train perplexity4.053517818450928
INFO:root:current mean train loss 1774.6408049467211
INFO:root:current train perplexity4.050695419311523
INFO:root:current mean train loss 1775.693909067586
INFO:root:current train perplexity4.052666664123535
INFO:root:current mean train loss 1775.7981088881158
INFO:root:current train perplexity4.054026126861572
INFO:root:current mean train loss 1776.5961525360638
INFO:root:current train perplexity4.055285930633545
INFO:root:current mean train loss 1777.5483721564797
INFO:root:current train perplexity4.056449890136719
INFO:root:current mean train loss 1777.0823712208526
INFO:root:current train perplexity4.056188106536865
INFO:root:current mean train loss 1777.0298165224815
INFO:root:current train perplexity4.057746887207031
INFO:root:current mean train loss 1777.643717212094
INFO:root:current train perplexity4.06021785736084

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1777.1850765072452
INFO:root:final train perplexity: 4.061673641204834
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2082.009635330092
INFO:root:eval perplexity: 5.386023044586182
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/48
 24%|â–ˆâ–ˆâ–       | 48/200 [2:38:41<8:22:23, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1795.8630208333334
INFO:root:current train perplexity4.149048805236816
INFO:root:current mean train loss 1749.0717019786005
INFO:root:current train perplexity4.004638195037842
INFO:root:current mean train loss 1761.666829237827
INFO:root:current train perplexity4.015486240386963
INFO:root:current mean train loss 1763.857417999752
INFO:root:current train perplexity4.022421360015869
INFO:root:current mean train loss 1766.6937920627824
INFO:root:current train perplexity4.019364356994629
INFO:root:current mean train loss 1762.3285791489684
INFO:root:current train perplexity4.01391077041626
INFO:root:current mean train loss 1763.3735333698553
INFO:root:current train perplexity4.012917518615723
INFO:root:current mean train loss 1764.6239749508304
INFO:root:current train perplexity4.013339996337891
INFO:root:current mean train loss 1763.762539691574
INFO:root:current train perplexity4.014966011047363
INFO:root:current mean train loss 1765.759348584785
INFO:root:current train perplexity4.020281791687012
INFO:root:current mean train loss 1765.115886218442
INFO:root:current train perplexity4.0211029052734375
INFO:root:current mean train loss 1766.6958094301779
INFO:root:current train perplexity4.0251946449279785
INFO:root:current mean train loss 1768.2292040412808
INFO:root:current train perplexity4.028414726257324
INFO:root:current mean train loss 1769.857650884476
INFO:root:current train perplexity4.029636383056641
INFO:root:current mean train loss 1770.1950995024017
INFO:root:current train perplexity4.034584045410156
INFO:root:current mean train loss 1770.3698621693225
INFO:root:current train perplexity4.03643798828125
INFO:root:current mean train loss 1771.2832892166941
INFO:root:current train perplexity4.038261413574219
INFO:root:current mean train loss 1770.2892858566418
INFO:root:current train perplexity4.037399768829346
INFO:root:current mean train loss 1770.626788481405
INFO:root:current train perplexity4.038645267486572
INFO:root:current mean train loss 1770.8144788139482
INFO:root:current train perplexity4.0390849113464355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it]
INFO:root:final mean train loss: 1770.8417694199525
INFO:root:final train perplexity: 4.041404724121094
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2083.234158130402
INFO:root:eval perplexity: 5.391358375549316
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/49
 24%|â–ˆâ–ˆâ–       | 49/200 [2:41:59<8:19:01, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1752.2025756835938
INFO:root:current train perplexity3.9458394050598145
INFO:root:current mean train loss 1744.1292595140862
INFO:root:current train perplexity3.949286460876465
INFO:root:current mean train loss 1748.620447093043
INFO:root:current train perplexity3.971601724624634
INFO:root:current mean train loss 1755.8689957584243
INFO:root:current train perplexity3.9769866466522217
INFO:root:current mean train loss 1758.7211015489365
INFO:root:current train perplexity3.9861104488372803
INFO:root:current mean train loss 1758.6206690279164
INFO:root:current train perplexity3.9863505363464355
INFO:root:current mean train loss 1762.244669467588
INFO:root:current train perplexity3.9908018112182617
INFO:root:current mean train loss 1761.3288455817217
INFO:root:current train perplexity3.994417667388916
INFO:root:current mean train loss 1762.5338941720815
INFO:root:current train perplexity3.999131679534912
INFO:root:current mean train loss 1763.8690977956092
INFO:root:current train perplexity4.004776477813721
INFO:root:current mean train loss 1764.3494160970051
INFO:root:current train perplexity4.008999824523926
INFO:root:current mean train loss 1765.5882808833578
INFO:root:current train perplexity4.012146472930908
INFO:root:current mean train loss 1765.1191594507786
INFO:root:current train perplexity4.011378288269043
INFO:root:current mean train loss 1764.8813137478298
INFO:root:current train perplexity4.011289596557617
INFO:root:current mean train loss 1764.3509516369697
INFO:root:current train perplexity4.012749195098877
INFO:root:current mean train loss 1764.4935554524316
INFO:root:current train perplexity4.015134334564209
INFO:root:current mean train loss 1765.6452974805645
INFO:root:current train perplexity4.0169148445129395
INFO:root:current mean train loss 1765.0373927021687
INFO:root:current train perplexity4.020051956176758
INFO:root:current mean train loss 1764.545607321127
INFO:root:current train perplexity4.018988609313965
INFO:root:current mean train loss 1765.0287800980404
INFO:root:current train perplexity4.0214056968688965

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1764.9401786215547
INFO:root:final train perplexity: 4.022638320922852
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2086.5027002645725
INFO:root:eval perplexity: 5.4056291580200195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/50
 25%|â–ˆâ–ˆâ–Œ       | 50/200 [2:45:17<8:15:49, 198.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1747.3536800462373
INFO:root:current train perplexity3.943704843521118
INFO:root:current mean train loss 1745.789700706533
INFO:root:current train perplexity3.948711633682251
INFO:root:current mean train loss 1744.937119081796
INFO:root:current train perplexity3.9604079723358154
INFO:root:current mean train loss 1745.8073684998433
INFO:root:current train perplexity3.971923589706421
INFO:root:current mean train loss 1751.4748211629142
INFO:root:current train perplexity3.978736400604248
INFO:root:current mean train loss 1752.3846886917977
INFO:root:current train perplexity3.9840102195739746
INFO:root:current mean train loss 1754.391452595339
INFO:root:current train perplexity3.9836783409118652
INFO:root:current mean train loss 1754.0142893975822
INFO:root:current train perplexity3.986081600189209
INFO:root:current mean train loss 1754.7973901683506
INFO:root:current train perplexity3.98602557182312
INFO:root:current mean train loss 1754.5151241129643
INFO:root:current train perplexity3.9867289066314697
INFO:root:current mean train loss 1754.0413151569203
INFO:root:current train perplexity3.98602557182312
INFO:root:current mean train loss 1755.0147460725018
INFO:root:current train perplexity3.98901104927063
INFO:root:current mean train loss 1755.7159554792272
INFO:root:current train perplexity3.9905741214752197
INFO:root:current mean train loss 1756.531900709872
INFO:root:current train perplexity3.993037700653076
INFO:root:current mean train loss 1755.9283900501155
INFO:root:current train perplexity3.9926154613494873
INFO:root:current mean train loss 1756.2554651091682
INFO:root:current train perplexity3.996525764465332
INFO:root:current mean train loss 1756.9443772444947
INFO:root:current train perplexity3.999593734741211
INFO:root:current mean train loss 1756.6578957506968
INFO:root:current train perplexity4.000147819519043
INFO:root:current mean train loss 1757.479123533836
INFO:root:current train perplexity4.0008039474487305
INFO:root:current mean train loss 1758.3018074172653
INFO:root:current train perplexity4.001931190490723

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1758.5519589238015
INFO:root:final train perplexity: 4.002422332763672
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2087.7340637639904
INFO:root:eval perplexity: 5.411015510559082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/51
 26%|â–ˆâ–ˆâ–Œ       | 51/200 [2:48:36<8:12:35, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1754.911552660393
INFO:root:current train perplexity3.94848895072937
INFO:root:current mean train loss 1748.2264132212445
INFO:root:current train perplexity3.938863754272461
INFO:root:current mean train loss 1746.3404981570136
INFO:root:current train perplexity3.943743944168091
INFO:root:current mean train loss 1745.2212250923199
INFO:root:current train perplexity3.946100950241089
INFO:root:current mean train loss 1744.6276905239909
INFO:root:current train perplexity3.9500370025634766
INFO:root:current mean train loss 1746.493544292113
INFO:root:current train perplexity3.950531482696533
INFO:root:current mean train loss 1747.4824981231232
INFO:root:current train perplexity3.9523091316223145
INFO:root:current mean train loss 1747.5894759454552
INFO:root:current train perplexity3.9512224197387695
INFO:root:current mean train loss 1746.7158981217524
INFO:root:current train perplexity3.957824945449829
INFO:root:current mean train loss 1745.959868310648
INFO:root:current train perplexity3.958143949508667
INFO:root:current mean train loss 1746.6148020903568
INFO:root:current train perplexity3.960587978363037
INFO:root:current mean train loss 1748.7873425230155
INFO:root:current train perplexity3.9659740924835205
INFO:root:current mean train loss 1749.8880071414026
INFO:root:current train perplexity3.9685287475585938
INFO:root:current mean train loss 1751.9049887259218
INFO:root:current train perplexity3.9734349250793457
INFO:root:current mean train loss 1751.9135374144676
INFO:root:current train perplexity3.9757261276245117
INFO:root:current mean train loss 1750.9560033961427
INFO:root:current train perplexity3.976609706878662
INFO:root:current mean train loss 1750.736440083846
INFO:root:current train perplexity3.9784066677093506
INFO:root:current mean train loss 1751.2374145231322
INFO:root:current train perplexity3.9807255268096924
INFO:root:current mean train loss 1751.7850899159716
INFO:root:current train perplexity3.9806900024414062
INFO:root:current mean train loss 1752.5627240232388
INFO:root:current train perplexity3.982638120651245

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.39s/it]
INFO:root:final mean train loss: 1752.2571059634333
INFO:root:final train perplexity: 3.98260235786438
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2090.0907592773438
INFO:root:eval perplexity: 5.421337604522705
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/52
 26%|â–ˆâ–ˆâ–Œ       | 52/200 [2:51:54<8:08:59, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1735.8258939076618
INFO:root:current train perplexity3.942089796066284
INFO:root:current mean train loss 1727.8614248473787
INFO:root:current train perplexity3.9180405139923096
INFO:root:current mean train loss 1728.2691059449537
INFO:root:current train perplexity3.9150238037109375
INFO:root:current mean train loss 1732.031626728745
INFO:root:current train perplexity3.914370059967041
INFO:root:current mean train loss 1734.562519207751
INFO:root:current train perplexity3.9257893562316895
INFO:root:current mean train loss 1738.470552787977
INFO:root:current train perplexity3.929652452468872
INFO:root:current mean train loss 1741.2293440230944
INFO:root:current train perplexity3.930908679962158
INFO:root:current mean train loss 1743.6592172595886
INFO:root:current train perplexity3.9354350566864014
INFO:root:current mean train loss 1743.6831072659347
INFO:root:current train perplexity3.938714027404785
INFO:root:current mean train loss 1743.920485410176
INFO:root:current train perplexity3.94345760345459
INFO:root:current mean train loss 1744.7354207694916
INFO:root:current train perplexity3.945918321609497
INFO:root:current mean train loss 1745.5352689302886
INFO:root:current train perplexity3.9497082233428955
INFO:root:current mean train loss 1745.6543227542868
INFO:root:current train perplexity3.9509594440460205
INFO:root:current mean train loss 1746.0538853488792
INFO:root:current train perplexity3.956742763519287
INFO:root:current mean train loss 1747.1370152417503
INFO:root:current train perplexity3.959599018096924
INFO:root:current mean train loss 1746.7011993273254
INFO:root:current train perplexity3.9602460861206055
INFO:root:current mean train loss 1745.988723328761
INFO:root:current train perplexity3.960090398788452
INFO:root:current mean train loss 1745.4366385245416
INFO:root:current train perplexity3.961254596710205
INFO:root:current mean train loss 1746.2269678019616
INFO:root:current train perplexity3.963107109069824
INFO:root:current mean train loss 1746.2644913404563
INFO:root:current train perplexity3.9638237953186035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it]
INFO:root:final mean train loss: 1746.2644913404563
INFO:root:final train perplexity: 3.9638237953186035
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2093.1053873697915
INFO:root:eval perplexity: 5.434572219848633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/53
 26%|â–ˆâ–ˆâ–‹       | 53/200 [2:55:12<8:05:48, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1736.2418811035157
INFO:root:current train perplexity3.921415090560913
INFO:root:current mean train loss 1730.7502410888671
INFO:root:current train perplexity3.908778190612793
INFO:root:current mean train loss 1730.125458984375
INFO:root:current train perplexity3.896683931350708
INFO:root:current mean train loss 1728.7247177124023
INFO:root:current train perplexity3.9041800498962402
INFO:root:current mean train loss 1727.4910854492186
INFO:root:current train perplexity3.9092042446136475
INFO:root:current mean train loss 1726.352369181315
INFO:root:current train perplexity3.9052987098693848
INFO:root:current mean train loss 1726.9681255231585
INFO:root:current train perplexity3.9109060764312744
INFO:root:current mean train loss 1728.848325805664
INFO:root:current train perplexity3.918745279312134
INFO:root:current mean train loss 1731.1760331217447
INFO:root:current train perplexity3.9221878051757812
INFO:root:current mean train loss 1731.4857891845704
INFO:root:current train perplexity3.924349546432495
INFO:root:current mean train loss 1732.6902373712712
INFO:root:current train perplexity3.9293134212493896
INFO:root:current mean train loss 1732.4702124023438
INFO:root:current train perplexity3.9328627586364746
INFO:root:current mean train loss 1734.3711247370793
INFO:root:current train perplexity3.9354145526885986
INFO:root:current mean train loss 1735.7460518101284
INFO:root:current train perplexity3.9348220825195312
INFO:root:current mean train loss 1736.2607819010416
INFO:root:current train perplexity3.9361460208892822
INFO:root:current mean train loss 1737.8352855682374
INFO:root:current train perplexity3.9379870891571045
INFO:root:current mean train loss 1738.3932385971966
INFO:root:current train perplexity3.938714027404785
INFO:root:current mean train loss 1739.4639238145617
INFO:root:current train perplexity3.940932512283325
INFO:root:current mean train loss 1740.322974917763
INFO:root:current train perplexity3.94352388381958

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.55s/it]
INFO:root:final mean train loss: 1739.9454221662943
INFO:root:final train perplexity: 3.944119691848755
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2094.3883801182956
INFO:root:eval perplexity: 5.440213680267334
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/54
 27%|â–ˆâ–ˆâ–‹       | 54/200 [2:58:30<8:02:20, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1745.6108829273896
INFO:root:current train perplexity3.9879379272460938
INFO:root:current mean train loss 1720.9889364650107
INFO:root:current train perplexity3.8941428661346436
INFO:root:current mean train loss 1724.2595754878312
INFO:root:current train perplexity3.8999884128570557
INFO:root:current mean train loss 1725.9935218016808
INFO:root:current train perplexity3.9094040393829346
INFO:root:current mean train loss 1727.0103218206684
INFO:root:current train perplexity3.9103641510009766
INFO:root:current mean train loss 1730.0231895815703
INFO:root:current train perplexity3.9074831008911133
INFO:root:current mean train loss 1729.0893794079848
INFO:root:current train perplexity3.908883571624756
INFO:root:current mean train loss 1728.9373578400127
INFO:root:current train perplexity3.9109694957733154
INFO:root:current mean train loss 1728.2031299306245
INFO:root:current train perplexity3.912485361099243
INFO:root:current mean train loss 1729.0362741850975
INFO:root:current train perplexity3.9129998683929443
INFO:root:current mean train loss 1728.8093827059135
INFO:root:current train perplexity3.912543773651123
INFO:root:current mean train loss 1730.1767685223394
INFO:root:current train perplexity3.9135749340057373
INFO:root:current mean train loss 1731.0066343258911
INFO:root:current train perplexity3.9160983562469482
INFO:root:current mean train loss 1730.403174792082
INFO:root:current train perplexity3.9160685539245605
INFO:root:current mean train loss 1730.681144159784
INFO:root:current train perplexity3.9172282218933105
INFO:root:current mean train loss 1731.4289092916993
INFO:root:current train perplexity3.9168832302093506
INFO:root:current mean train loss 1732.6745699078638
INFO:root:current train perplexity3.918670654296875
INFO:root:current mean train loss 1733.6661974863043
INFO:root:current train perplexity3.920984983444214
INFO:root:current mean train loss 1734.491621690329
INFO:root:current train perplexity3.9239444732666016
INFO:root:current mean train loss 1734.6767677462344
INFO:root:current train perplexity3.9256787300109863

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it]
INFO:root:final mean train loss: 1734.5326928058419
INFO:root:final train perplexity: 3.9273180961608887
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2096.110477961547
INFO:root:eval perplexity: 5.447795391082764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/55
 28%|â–ˆâ–ˆâ–Š       | 55/200 [3:01:49<7:59:04, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1761.4424582088695
INFO:root:current train perplexity3.9301340579986572
INFO:root:current mean train loss 1733.5133666992188
INFO:root:current train perplexity3.9017579555511475
INFO:root:current mean train loss 1719.660129123264
INFO:root:current train perplexity3.8892011642456055
INFO:root:current mean train loss 1721.4894591919676
INFO:root:current train perplexity3.8822007179260254
INFO:root:current mean train loss 1722.5997981058288
INFO:root:current train perplexity3.889469623565674
INFO:root:current mean train loss 1722.9620418477148
INFO:root:current train perplexity3.8871333599090576
INFO:root:current mean train loss 1719.3751625036969
INFO:root:current train perplexity3.8852524757385254
INFO:root:current mean train loss 1720.7862053229308
INFO:root:current train perplexity3.8875014781951904
INFO:root:current mean train loss 1720.997331578097
INFO:root:current train perplexity3.8895363807678223
INFO:root:current mean train loss 1723.5196950124214
INFO:root:current train perplexity3.893686056137085
INFO:root:current mean train loss 1722.564911537982
INFO:root:current train perplexity3.8970837593078613
INFO:root:current mean train loss 1724.4294683331955
INFO:root:current train perplexity3.898125171661377
INFO:root:current mean train loss 1724.8495599137714
INFO:root:current train perplexity3.8986172676086426
INFO:root:current mean train loss 1724.854247314819
INFO:root:current train perplexity3.8993473052978516
INFO:root:current mean train loss 1724.6788349657045
INFO:root:current train perplexity3.90041446685791
INFO:root:current mean train loss 1725.9969640779061
INFO:root:current train perplexity3.9038710594177246
INFO:root:current mean train loss 1726.5965615766286
INFO:root:current train perplexity3.905736207962036
INFO:root:current mean train loss 1727.897281140742
INFO:root:current train perplexity3.9070351123809814
INFO:root:current mean train loss 1729.0023676582096
INFO:root:current train perplexity3.90895676612854
INFO:root:current mean train loss 1728.9359728587326
INFO:root:current train perplexity3.909428834915161

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1728.998575353887
INFO:root:final train perplexity: 3.91021466255188
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2097.2273728390956
INFO:root:eval perplexity: 5.452719211578369
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/56
 28%|â–ˆâ–ˆâ–Š       | 56/200 [3:05:07<7:55:54, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1707.4807224647673
INFO:root:current train perplexity3.8419077396392822
INFO:root:current mean train loss 1710.5977249650766
INFO:root:current train perplexity3.8593459129333496
INFO:root:current mean train loss 1719.3699411339019
INFO:root:current train perplexity3.8616902828216553
INFO:root:current mean train loss 1718.5642799312234
INFO:root:current train perplexity3.864161252975464
INFO:root:current mean train loss 1716.4939470988948
INFO:root:current train perplexity3.86006236076355
INFO:root:current mean train loss 1720.615834978661
INFO:root:current train perplexity3.8644795417785645
INFO:root:current mean train loss 1717.5378832370272
INFO:root:current train perplexity3.863630533218384
INFO:root:current mean train loss 1717.2676246124959
INFO:root:current train perplexity3.86383056640625
INFO:root:current mean train loss 1720.3795709666017
INFO:root:current train perplexity3.868858575820923
INFO:root:current mean train loss 1720.45687670462
INFO:root:current train perplexity3.873067855834961
INFO:root:current mean train loss 1719.6964624697089
INFO:root:current train perplexity3.8733279705047607
INFO:root:current mean train loss 1720.1463376997244
INFO:root:current train perplexity3.8748795986175537
INFO:root:current mean train loss 1722.195816881651
INFO:root:current train perplexity3.8808417320251465
INFO:root:current mean train loss 1721.8371450112186
INFO:root:current train perplexity3.881932258605957
INFO:root:current mean train loss 1722.4721378507818
INFO:root:current train perplexity3.884486675262451
INFO:root:current mean train loss 1722.4502187663704
INFO:root:current train perplexity3.8838095664978027
INFO:root:current mean train loss 1721.8113323098453
INFO:root:current train perplexity3.8842287063598633
INFO:root:current mean train loss 1721.7925128163372
INFO:root:current train perplexity3.8858954906463623
INFO:root:current mean train loss 1722.7479881810086
INFO:root:current train perplexity3.887965202331543
INFO:root:current mean train loss 1723.6288448081514
INFO:root:current train perplexity3.891357183456421

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it]
INFO:root:final mean train loss: 1723.1184449534915
INFO:root:final train perplexity: 3.8921234607696533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2099.594024008893
INFO:root:eval perplexity: 5.463165283203125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/57
 28%|â–ˆâ–ˆâ–Š       | 57/200 [3:08:25<7:52:22, 198.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1692.8705426384422
INFO:root:current train perplexity3.838277816772461
INFO:root:current mean train loss 1699.3550640287854
INFO:root:current train perplexity3.8241066932678223
INFO:root:current mean train loss 1707.959092325239
INFO:root:current train perplexity3.8249542713165283
INFO:root:current mean train loss 1708.9905232968538
INFO:root:current train perplexity3.8302135467529297
INFO:root:current mean train loss 1707.8443084455962
INFO:root:current train perplexity3.837340831756592
INFO:root:current mean train loss 1709.2546878868425
INFO:root:current train perplexity3.8453369140625
INFO:root:current mean train loss 1713.3576777109843
INFO:root:current train perplexity3.849196434020996
INFO:root:current mean train loss 1712.434463818868
INFO:root:current train perplexity3.852851152420044
INFO:root:current mean train loss 1709.7901819466447
INFO:root:current train perplexity3.8519294261932373
INFO:root:current mean train loss 1710.0048512860762
INFO:root:current train perplexity3.8536999225616455
INFO:root:current mean train loss 1710.914943280738
INFO:root:current train perplexity3.8563907146453857
INFO:root:current mean train loss 1713.0231474784955
INFO:root:current train perplexity3.8616414070129395
INFO:root:current mean train loss 1713.9179639365018
INFO:root:current train perplexity3.8641533851623535
INFO:root:current mean train loss 1715.2207655878792
INFO:root:current train perplexity3.8659286499023438
INFO:root:current mean train loss 1715.3624592710905
INFO:root:current train perplexity3.8679518699645996
INFO:root:current mean train loss 1716.1128551716706
INFO:root:current train perplexity3.868093490600586
INFO:root:current mean train loss 1715.7224195009228
INFO:root:current train perplexity3.8659439086914062
INFO:root:current mean train loss 1716.292309998387
INFO:root:current train perplexity3.869230270385742
INFO:root:current mean train loss 1716.996105120575
INFO:root:current train perplexity3.869983196258545
INFO:root:current mean train loss 1717.559710805009
INFO:root:current train perplexity3.8726768493652344

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.99s/it]
INFO:root:final mean train loss: 1717.0481866248858
INFO:root:final train perplexity: 3.873534679412842
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2101.876968708444
INFO:root:eval perplexity: 5.473260402679443
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/58
 29%|â–ˆâ–ˆâ–‰       | 58/200 [3:11:43<7:49:18, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1686.398713235294
INFO:root:current train perplexity3.8191423416137695
INFO:root:current mean train loss 1700.009314954603
INFO:root:current train perplexity3.8236026763916016
INFO:root:current mean train loss 1705.1693517852248
INFO:root:current train perplexity3.830155611038208
INFO:root:current mean train loss 1705.7346841391031
INFO:root:current train perplexity3.8354785442352295
INFO:root:current mean train loss 1703.7059263249034
INFO:root:current train perplexity3.836276054382324
INFO:root:current mean train loss 1705.438670831664
INFO:root:current train perplexity3.836862802505493
INFO:root:current mean train loss 1704.0956977788549
INFO:root:current train perplexity3.835186243057251
INFO:root:current mean train loss 1705.2859717107883
INFO:root:current train perplexity3.8375556468963623
INFO:root:current mean train loss 1705.0346871413753
INFO:root:current train perplexity3.8362882137298584
INFO:root:current mean train loss 1708.320783555084
INFO:root:current train perplexity3.8410532474517822
INFO:root:current mean train loss 1709.1366453953053
INFO:root:current train perplexity3.8439629077911377
INFO:root:current mean train loss 1709.1339265847507
INFO:root:current train perplexity3.8462398052215576
INFO:root:current mean train loss 1709.058309140929
INFO:root:current train perplexity3.8460562229156494
INFO:root:current mean train loss 1709.511167009815
INFO:root:current train perplexity3.8485159873962402
INFO:root:current mean train loss 1710.4152202362163
INFO:root:current train perplexity3.8514935970306396
INFO:root:current mean train loss 1711.3065764706969
INFO:root:current train perplexity3.852313756942749
INFO:root:current mean train loss 1712.2390766268313
INFO:root:current train perplexity3.854545831680298
INFO:root:current mean train loss 1712.1436439321822
INFO:root:current train perplexity3.856184720993042
INFO:root:current mean train loss 1712.175437316085
INFO:root:current train perplexity3.856997013092041

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.92s/it]
INFO:root:final mean train loss: 1712.194602577721
INFO:root:final train perplexity: 3.858736038208008
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2102.8594979360596
INFO:root:eval perplexity: 5.477612495422363
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/59
 30%|â–ˆâ–ˆâ–‰       | 59/200 [3:15:02<7:46:06, 198.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.1719360351562
INFO:root:current train perplexity3.850409746170044
INFO:root:current mean train loss 1688.1508657418046
INFO:root:current train perplexity3.7951390743255615
INFO:root:current mean train loss 1684.2432208674968
INFO:root:current train perplexity3.812514305114746
INFO:root:current mean train loss 1689.9545247665303
INFO:root:current train perplexity3.81040620803833
INFO:root:current mean train loss 1692.5861382176033
INFO:root:current train perplexity3.8156893253326416
INFO:root:current mean train loss 1694.2685162669634
INFO:root:current train perplexity3.812466621398926
INFO:root:current mean train loss 1697.294395066575
INFO:root:current train perplexity3.81656813621521
INFO:root:current mean train loss 1698.8939577629762
INFO:root:current train perplexity3.8158817291259766
INFO:root:current mean train loss 1700.6366391455445
INFO:root:current train perplexity3.823124647140503
INFO:root:current mean train loss 1701.3589454532462
INFO:root:current train perplexity3.8229682445526123
INFO:root:current mean train loss 1702.7503032265547
INFO:root:current train perplexity3.8256962299346924
INFO:root:current mean train loss 1703.446784433133
INFO:root:current train perplexity3.8259353637695312
INFO:root:current mean train loss 1705.1587589409903
INFO:root:current train perplexity3.8291876316070557
INFO:root:current mean train loss 1704.7299061202416
INFO:root:current train perplexity3.8320157527923584
INFO:root:current mean train loss 1705.4177642256318
INFO:root:current train perplexity3.834566831588745
INFO:root:current mean train loss 1705.3118183730287
INFO:root:current train perplexity3.8370800018310547
INFO:root:current mean train loss 1706.7094623694259
INFO:root:current train perplexity3.8391013145446777
INFO:root:current mean train loss 1707.682488590514
INFO:root:current train perplexity3.840691089630127
INFO:root:current mean train loss 1707.3085573050344
INFO:root:current train perplexity3.8402860164642334
INFO:root:current mean train loss 1706.6931637544362
INFO:root:current train perplexity3.840303421020508

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1706.1558771099758
INFO:root:final train perplexity: 3.840402364730835
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2105.403204302416
INFO:root:eval perplexity: 5.488893032073975
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/60
 30%|â–ˆâ–ˆâ–ˆ       | 60/200 [3:18:21<7:43:03, 198.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1650.3451120476973
INFO:root:current train perplexity3.820666551589966
INFO:root:current mean train loss 1689.969463957458
INFO:root:current train perplexity3.809978723526001
INFO:root:current mean train loss 1695.9143417567423
INFO:root:current train perplexity3.8051319122314453
INFO:root:current mean train loss 1699.422262257543
INFO:root:current train perplexity3.8049538135528564
INFO:root:current mean train loss 1697.4496726534533
INFO:root:current train perplexity3.7985363006591797
INFO:root:current mean train loss 1699.9744459089745
INFO:root:current train perplexity3.808964252471924
INFO:root:current mean train loss 1696.3737660840948
INFO:root:current train perplexity3.8113274574279785
INFO:root:current mean train loss 1694.9141315996067
INFO:root:current train perplexity3.8085083961486816
INFO:root:current mean train loss 1695.7876077915141
INFO:root:current train perplexity3.808450698852539
INFO:root:current mean train loss 1696.1043163212391
INFO:root:current train perplexity3.8108816146850586
INFO:root:current mean train loss 1696.2870056332035
INFO:root:current train perplexity3.8127665519714355
INFO:root:current mean train loss 1697.4511080580805
INFO:root:current train perplexity3.817568778991699
INFO:root:current mean train loss 1698.158850528257
INFO:root:current train perplexity3.8188798427581787
INFO:root:current mean train loss 1697.7489147865926
INFO:root:current train perplexity3.819654703140259
INFO:root:current mean train loss 1698.9018029071144
INFO:root:current train perplexity3.821835994720459
INFO:root:current mean train loss 1698.6601227389267
INFO:root:current train perplexity3.821319341659546
INFO:root:current mean train loss 1699.0834335129227
INFO:root:current train perplexity3.8208537101745605
INFO:root:current mean train loss 1699.2332297830542
INFO:root:current train perplexity3.819807291030884
INFO:root:current mean train loss 1699.9623388591344
INFO:root:current train perplexity3.8216655254364014
INFO:root:current mean train loss 1700.0609873840745
INFO:root:current train perplexity3.823056221008301

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.34s/it]
INFO:root:final mean train loss: 1700.689952732996
INFO:root:final train perplexity: 3.823883056640625
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2109.493375737616
INFO:root:eval perplexity: 5.507079601287842
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/61
 30%|â–ˆâ–ˆâ–ˆ       | 61/200 [3:21:39<7:39:21, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1676.6918233235676
INFO:root:current train perplexity3.732006549835205
INFO:root:current mean train loss 1689.6747239056756
INFO:root:current train perplexity3.7604265213012695
INFO:root:current mean train loss 1688.1103510452529
INFO:root:current train perplexity3.7665317058563232
INFO:root:current mean train loss 1689.7180800664992
INFO:root:current train perplexity3.777538537979126
INFO:root:current mean train loss 1686.5030685564793
INFO:root:current train perplexity3.7768352031707764
INFO:root:current mean train loss 1688.0687734119929
INFO:root:current train perplexity3.778085231781006
INFO:root:current mean train loss 1690.6234232584636
INFO:root:current train perplexity3.780250310897827
INFO:root:current mean train loss 1690.930889627208
INFO:root:current train perplexity3.7827811241149902
INFO:root:current mean train loss 1694.3547272750636
INFO:root:current train perplexity3.788475275039673
INFO:root:current mean train loss 1696.1558960482605
INFO:root:current train perplexity3.795369863510132
INFO:root:current mean train loss 1696.6075345190336
INFO:root:current train perplexity3.7992148399353027
INFO:root:current mean train loss 1695.6868971703757
INFO:root:current train perplexity3.798353433609009
INFO:root:current mean train loss 1695.167755620765
INFO:root:current train perplexity3.8004150390625
INFO:root:current mean train loss 1695.2108767389536
INFO:root:current train perplexity3.8009941577911377
INFO:root:current mean train loss 1694.8156489210207
INFO:root:current train perplexity3.8013625144958496
INFO:root:current mean train loss 1694.671552658081
INFO:root:current train perplexity3.7996065616607666
INFO:root:current mean train loss 1695.1587379660757
INFO:root:current train perplexity3.803147077560425
INFO:root:current mean train loss 1695.0628434282294
INFO:root:current train perplexity3.8039214611053467
INFO:root:current mean train loss 1695.3586340677764
INFO:root:current train perplexity3.8067870140075684
INFO:root:current mean train loss 1696.1801301309886
INFO:root:current train perplexity3.8092589378356934

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.35s/it]
INFO:root:final mean train loss: 1695.6799305017946
INFO:root:final train perplexity: 3.8088037967681885
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2110.4132448020555
INFO:root:eval perplexity: 5.511178016662598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/62
 31%|â–ˆâ–ˆâ–ˆ       | 62/200 [3:24:56<7:35:49, 198.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1691.8773861291274
INFO:root:current train perplexity3.762413263320923
INFO:root:current mean train loss 1689.6488428053513
INFO:root:current train perplexity3.7645277976989746
INFO:root:current mean train loss 1682.530887166502
INFO:root:current train perplexity3.763103723526001
INFO:root:current mean train loss 1683.8798790086094
INFO:root:current train perplexity3.762052297592163
INFO:root:current mean train loss 1685.9588957190776
INFO:root:current train perplexity3.7649128437042236
INFO:root:current mean train loss 1681.6319165083212
INFO:root:current train perplexity3.7650744915008545
INFO:root:current mean train loss 1681.8044328908643
INFO:root:current train perplexity3.768822193145752
INFO:root:current mean train loss 1680.396370086062
INFO:root:current train perplexity3.769098997116089
INFO:root:current mean train loss 1681.6966700134635
INFO:root:current train perplexity3.7694170475006104
INFO:root:current mean train loss 1683.4103488982162
INFO:root:current train perplexity3.7731058597564697
INFO:root:current mean train loss 1684.0611332298345
INFO:root:current train perplexity3.7736897468566895
INFO:root:current mean train loss 1684.6637727285818
INFO:root:current train perplexity3.777055263519287
INFO:root:current mean train loss 1685.4057818851943
INFO:root:current train perplexity3.7803332805633545
INFO:root:current mean train loss 1686.8740570902912
INFO:root:current train perplexity3.7846710681915283
INFO:root:current mean train loss 1687.547056467223
INFO:root:current train perplexity3.786181926727295
INFO:root:current mean train loss 1688.389870883723
INFO:root:current train perplexity3.7878098487854004
INFO:root:current mean train loss 1690.2092951262855
INFO:root:current train perplexity3.7908895015716553
INFO:root:current mean train loss 1689.4800189769546
INFO:root:current train perplexity3.7896058559417725
INFO:root:current mean train loss 1689.9828670067332
INFO:root:current train perplexity3.790832996368408
INFO:root:current mean train loss 1690.499491217438
INFO:root:current train perplexity3.792055368423462

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.95s/it]
INFO:root:final mean train loss: 1690.3536966067038
INFO:root:final train perplexity: 3.7928383350372314
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2112.356767803219
INFO:root:eval perplexity: 5.519847393035889
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/63
 32%|â–ˆâ–ˆâ–ˆâ–      | 63/200 [3:28:15<7:32:44, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1662.8415475027903
INFO:root:current train perplexity3.720898151397705
INFO:root:current mean train loss 1670.4927009133733
INFO:root:current train perplexity3.7342677116394043
INFO:root:current mean train loss 1671.1639616789641
INFO:root:current train perplexity3.7332653999328613
INFO:root:current mean train loss 1671.8958350929054
INFO:root:current train perplexity3.743816614151001
INFO:root:current mean train loss 1672.5667168799866
INFO:root:current train perplexity3.7461464405059814
INFO:root:current mean train loss 1673.564112827234
INFO:root:current train perplexity3.7483415603637695
INFO:root:current mean train loss 1675.853491757521
INFO:root:current train perplexity3.7547895908355713
INFO:root:current mean train loss 1677.5807726575183
INFO:root:current train perplexity3.759141206741333
INFO:root:current mean train loss 1679.2358520507812
INFO:root:current train perplexity3.7608776092529297
INFO:root:current mean train loss 1681.2193728102852
INFO:root:current train perplexity3.7623937129974365
INFO:root:current mean train loss 1680.306787679797
INFO:root:current train perplexity3.761674642562866
INFO:root:current mean train loss 1680.8739337106035
INFO:root:current train perplexity3.762697219848633
INFO:root:current mean train loss 1680.941794183686
INFO:root:current train perplexity3.764045476913452
INFO:root:current mean train loss 1681.759941334968
INFO:root:current train perplexity3.7641682624816895
INFO:root:current mean train loss 1681.8829407983897
INFO:root:current train perplexity3.7651782035827637
INFO:root:current mean train loss 1683.1948994047323
INFO:root:current train perplexity3.76741361618042
INFO:root:current mean train loss 1683.7055877502808
INFO:root:current train perplexity3.769165515899658
INFO:root:current mean train loss 1683.9443345581744
INFO:root:current train perplexity3.77099871635437
INFO:root:current mean train loss 1684.8817583869486
INFO:root:current train perplexity3.773855686187744
INFO:root:current mean train loss 1685.424781822553
INFO:root:current train perplexity3.7765743732452393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 1684.9906706086206
INFO:root:final train perplexity: 3.776829719543457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2115.3292219013188
INFO:root:eval perplexity: 5.533132553100586
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/64
 32%|â–ˆâ–ˆâ–ˆâ–      | 64/200 [3:31:33<7:29:25, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1644.6129823882004
INFO:root:current train perplexity3.7132320404052734
INFO:root:current mean train loss 1665.1227361505682
INFO:root:current train perplexity3.7228140830993652
INFO:root:current mean train loss 1670.4574969886487
INFO:root:current train perplexity3.729688882827759
INFO:root:current mean train loss 1671.7299012965318
INFO:root:current train perplexity3.73117995262146
INFO:root:current mean train loss 1674.0515698192055
INFO:root:current train perplexity3.730664014816284
INFO:root:current mean train loss 1676.2782373379605
INFO:root:current train perplexity3.736384153366089
INFO:root:current mean train loss 1674.9859466330627
INFO:root:current train perplexity3.741823434829712
INFO:root:current mean train loss 1675.3520292211822
INFO:root:current train perplexity3.7442731857299805
INFO:root:current mean train loss 1677.1877807479566
INFO:root:current train perplexity3.7528793811798096
INFO:root:current mean train loss 1677.6003132272274
INFO:root:current train perplexity3.7577240467071533
INFO:root:current mean train loss 1678.182510951515
INFO:root:current train perplexity3.7570879459381104
INFO:root:current mean train loss 1677.453703162845
INFO:root:current train perplexity3.7558014392852783
INFO:root:current mean train loss 1678.8269748643283
INFO:root:current train perplexity3.757456064224243
INFO:root:current mean train loss 1679.0527051555741
INFO:root:current train perplexity3.757594585418701
INFO:root:current mean train loss 1679.6294786752112
INFO:root:current train perplexity3.7582077980041504
INFO:root:current mean train loss 1679.1470095004086
INFO:root:current train perplexity3.7584264278411865
INFO:root:current mean train loss 1679.6702971308628
INFO:root:current train perplexity3.758246660232544
INFO:root:current mean train loss 1679.8591031254373
INFO:root:current train perplexity3.758206367492676
INFO:root:current mean train loss 1680.3843533546758
INFO:root:current train perplexity3.7586710453033447

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.33s/it]
INFO:root:final mean train loss: 1679.8655641741423
INFO:root:final train perplexity: 3.7615954875946045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2118.698320537594
INFO:root:eval perplexity: 5.5482282638549805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/65
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 65/200 [3:34:51<7:25:50, 198.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1666.81787109375
INFO:root:current train perplexity3.633173942565918
INFO:root:current mean train loss 1649.5068699763372
INFO:root:current train perplexity3.711716651916504
INFO:root:current mean train loss 1652.0214921539905
INFO:root:current train perplexity3.7129526138305664
INFO:root:current mean train loss 1657.4799595883019
INFO:root:current train perplexity3.7190237045288086
INFO:root:current mean train loss 1658.8583878621016
INFO:root:current train perplexity3.71868634223938
INFO:root:current mean train loss 1658.8152359250992
INFO:root:current train perplexity3.7251291275024414
INFO:root:current mean train loss 1661.6177317638271
INFO:root:current train perplexity3.727036952972412
INFO:root:current mean train loss 1663.4892983870072
INFO:root:current train perplexity3.7284939289093018
INFO:root:current mean train loss 1663.6105281393327
INFO:root:current train perplexity3.728743314743042
INFO:root:current mean train loss 1665.382045779608
INFO:root:current train perplexity3.7284131050109863
INFO:root:current mean train loss 1667.297840619942
INFO:root:current train perplexity3.7328407764434814
INFO:root:current mean train loss 1669.3330507140229
INFO:root:current train perplexity3.7364206314086914
INFO:root:current mean train loss 1670.168602724804
INFO:root:current train perplexity3.7368264198303223
INFO:root:current mean train loss 1670.5018170128571
INFO:root:current train perplexity3.736495018005371
INFO:root:current mean train loss 1671.1633132108595
INFO:root:current train perplexity3.7386667728424072
INFO:root:current mean train loss 1672.2579786422405
INFO:root:current train perplexity3.7412872314453125
INFO:root:current mean train loss 1672.986183908515
INFO:root:current train perplexity3.7432501316070557
INFO:root:current mean train loss 1673.370737353401
INFO:root:current train perplexity3.74481201171875
INFO:root:current mean train loss 1674.8810809150239
INFO:root:current train perplexity3.745962142944336
INFO:root:current mean train loss 1675.9832295650194
INFO:root:current train perplexity3.7468464374542236

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.71s/it]
INFO:root:final mean train loss: 1675.67471373496
INFO:root:final train perplexity: 3.74918270111084
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2118.42536828873
INFO:root:eval perplexity: 5.5470051765441895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/66
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/200 [3:38:10<7:22:43, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1680.5028541201636
INFO:root:current train perplexity3.7414398193359375
INFO:root:current mean train loss 1665.960293856534
INFO:root:current train perplexity3.7086293697357178
INFO:root:current mean train loss 1664.2599230459912
INFO:root:current train perplexity3.7101643085479736
INFO:root:current mean train loss 1667.7673138294635
INFO:root:current train perplexity3.7190206050872803
INFO:root:current mean train loss 1668.3468951227546
INFO:root:current train perplexity3.7178773880004883
INFO:root:current mean train loss 1666.4764966616933
INFO:root:current train perplexity3.719590663909912
INFO:root:current mean train loss 1665.7638890854594
INFO:root:current train perplexity3.713467836380005
INFO:root:current mean train loss 1666.9475686844444
INFO:root:current train perplexity3.7189159393310547
INFO:root:current mean train loss 1667.099745719064
INFO:root:current train perplexity3.720226764678955
INFO:root:current mean train loss 1666.8513545430833
INFO:root:current train perplexity3.716132402420044
INFO:root:current mean train loss 1666.2799515831598
INFO:root:current train perplexity3.7168209552764893
INFO:root:current mean train loss 1666.4878734415072
INFO:root:current train perplexity3.719221830368042
INFO:root:current mean train loss 1667.7806094557804
INFO:root:current train perplexity3.7201685905456543
INFO:root:current mean train loss 1668.2806413117726
INFO:root:current train perplexity3.721383810043335
INFO:root:current mean train loss 1668.3199273900698
INFO:root:current train perplexity3.7227213382720947
INFO:root:current mean train loss 1667.8062079615847
INFO:root:current train perplexity3.7228832244873047
INFO:root:current mean train loss 1667.5363514998458
INFO:root:current train perplexity3.724196672439575
INFO:root:current mean train loss 1668.7113755628995
INFO:root:current train perplexity3.7260689735412598
INFO:root:current mean train loss 1669.7075607576323
INFO:root:current train perplexity3.728085994720459
INFO:root:current mean train loss 1669.5063707866996
INFO:root:current train perplexity3.729393720626831

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.55s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.55s/it]
INFO:root:final mean train loss: 1669.2656808642505
INFO:root:final train perplexity: 3.7302799224853516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2120.42655695922
INFO:root:eval perplexity: 5.5559892654418945
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/67
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/200 [3:41:28<7:19:20, 198.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1637.2628366570723
INFO:root:current train perplexity3.688201904296875
INFO:root:current mean train loss 1655.57544476053
INFO:root:current train perplexity3.676607608795166
INFO:root:current mean train loss 1655.6900947634913
INFO:root:current train perplexity3.6676390171051025
INFO:root:current mean train loss 1659.3810883471247
INFO:root:current train perplexity3.6891794204711914
INFO:root:current mean train loss 1662.5113500307684
INFO:root:current train perplexity3.694425344467163
INFO:root:current mean train loss 1661.2110613854845
INFO:root:current train perplexity3.698244333267212
INFO:root:current mean train loss 1661.9052474162422
INFO:root:current train perplexity3.6973042488098145
INFO:root:current mean train loss 1662.339018038618
INFO:root:current train perplexity3.700752019882202
INFO:root:current mean train loss 1661.262627867924
INFO:root:current train perplexity3.7006051540374756
INFO:root:current mean train loss 1661.531799576684
INFO:root:current train perplexity3.704540491104126
INFO:root:current mean train loss 1663.5786536185499
INFO:root:current train perplexity3.7056097984313965
INFO:root:current mean train loss 1663.4635849669444
INFO:root:current train perplexity3.7096707820892334
INFO:root:current mean train loss 1663.4770534435265
INFO:root:current train perplexity3.7097432613372803
INFO:root:current mean train loss 1664.221254995942
INFO:root:current train perplexity3.712541103363037
INFO:root:current mean train loss 1664.2998950093445
INFO:root:current train perplexity3.713939905166626
INFO:root:current mean train loss 1665.0182439558528
INFO:root:current train perplexity3.7160863876342773
INFO:root:current mean train loss 1665.4283054524144
INFO:root:current train perplexity3.715447187423706
INFO:root:current mean train loss 1665.1476303469326
INFO:root:current train perplexity3.716073513031006
INFO:root:current mean train loss 1665.87003051625
INFO:root:current train perplexity3.717642068862915
INFO:root:current mean train loss 1665.314708036535
INFO:root:current train perplexity3.718550205230713

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1665.325461460254
INFO:root:final train perplexity: 3.7187063694000244
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2121.8006055033798
INFO:root:eval perplexity: 5.562166690826416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/68
 34%|â–ˆâ–ˆâ–ˆâ–      | 68/200 [3:44:46<7:16:02, 198.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1646.1410089666192
INFO:root:current train perplexity3.6638565063476562
INFO:root:current mean train loss 1646.4012002268146
INFO:root:current train perplexity3.6576240062713623
INFO:root:current mean train loss 1648.9798842486214
INFO:root:current train perplexity3.6660666465759277
INFO:root:current mean train loss 1648.3724647199604
INFO:root:current train perplexity3.6768882274627686
INFO:root:current mean train loss 1650.270538665436
INFO:root:current train perplexity3.678231954574585
INFO:root:current mean train loss 1650.4886133692287
INFO:root:current train perplexity3.682166337966919
INFO:root:current mean train loss 1649.7940172501192
INFO:root:current train perplexity3.685462474822998
INFO:root:current mean train loss 1650.41309806369
INFO:root:current train perplexity3.683490037918091
INFO:root:current mean train loss 1652.0410250479715
INFO:root:current train perplexity3.6859190464019775
INFO:root:current mean train loss 1651.9917500920321
INFO:root:current train perplexity3.6885602474212646
INFO:root:current mean train loss 1652.8335379794876
INFO:root:current train perplexity3.688314437866211
INFO:root:current mean train loss 1654.1751494436553
INFO:root:current train perplexity3.6916158199310303
INFO:root:current mean train loss 1655.087377637886
INFO:root:current train perplexity3.6932053565979004
INFO:root:current mean train loss 1655.9245113583947
INFO:root:current train perplexity3.6971349716186523
INFO:root:current mean train loss 1656.6570110307937
INFO:root:current train perplexity3.6969242095947266
INFO:root:current mean train loss 1657.382222401904
INFO:root:current train perplexity3.696550130844116
INFO:root:current mean train loss 1657.7499290443259
INFO:root:current train perplexity3.696552276611328
INFO:root:current mean train loss 1658.6484193459535
INFO:root:current train perplexity3.698136329650879
INFO:root:current mean train loss 1659.0724815348087
INFO:root:current train perplexity3.6999857425689697
INFO:root:current mean train loss 1660.1141636528932
INFO:root:current train perplexity3.702331304550171

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.00s/it]
INFO:root:final mean train loss: 1659.9711642589943
INFO:root:final train perplexity: 3.7030365467071533
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2125.169212828291
INFO:root:eval perplexity: 5.577340126037598
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/69
 34%|â–ˆâ–ˆâ–ˆâ–      | 69/200 [3:48:04<7:12:58, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1622.6446923149956
INFO:root:current train perplexity3.633315324783325
INFO:root:current mean train loss 1624.7546195096747
INFO:root:current train perplexity3.6443629264831543
INFO:root:current mean train loss 1633.407662784352
INFO:root:current train perplexity3.6468770503997803
INFO:root:current mean train loss 1636.891080794796
INFO:root:current train perplexity3.647392511367798
INFO:root:current mean train loss 1642.4745010117354
INFO:root:current train perplexity3.656075954437256
INFO:root:current mean train loss 1645.439793086552
INFO:root:current train perplexity3.664458751678467
INFO:root:current mean train loss 1646.0831698463076
INFO:root:current train perplexity3.6659841537475586
INFO:root:current mean train loss 1647.0572897164932
INFO:root:current train perplexity3.668553352355957
INFO:root:current mean train loss 1647.301639101921
INFO:root:current train perplexity3.669898748397827
INFO:root:current mean train loss 1648.293707953559
INFO:root:current train perplexity3.673003673553467
INFO:root:current mean train loss 1649.4086935698097
INFO:root:current train perplexity3.6734683513641357
INFO:root:current mean train loss 1649.1311366370921
INFO:root:current train perplexity3.672715187072754
INFO:root:current mean train loss 1650.2307916797183
INFO:root:current train perplexity3.6743433475494385
INFO:root:current mean train loss 1651.4283209708967
INFO:root:current train perplexity3.677476167678833
INFO:root:current mean train loss 1651.405152362326
INFO:root:current train perplexity3.6805100440979004
INFO:root:current mean train loss 1652.364280021221
INFO:root:current train perplexity3.6810097694396973
INFO:root:current mean train loss 1653.0932657342207
INFO:root:current train perplexity3.6827149391174316
INFO:root:current mean train loss 1653.648394582503
INFO:root:current train perplexity3.6851062774658203
INFO:root:current mean train loss 1654.5430846907134
INFO:root:current train perplexity3.686265230178833
INFO:root:current mean train loss 1655.5246812924893
INFO:root:current train perplexity3.6892783641815186

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 1655.1207802330553
INFO:root:final train perplexity: 3.6888980865478516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2127.74293810256
INFO:root:eval perplexity: 5.588961601257324
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/70
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 70/200 [3:51:23<7:09:48, 198.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1614.8571256144662
INFO:root:current train perplexity3.5868353843688965
INFO:root:current mean train loss 1621.6498293599125
INFO:root:current train perplexity3.6033413410186768
INFO:root:current mean train loss 1624.9297348075258
INFO:root:current train perplexity3.6005935668945312
INFO:root:current mean train loss 1630.1701233380863
INFO:root:current train perplexity3.620347738265991
INFO:root:current mean train loss 1632.7550066302404
INFO:root:current train perplexity3.6325299739837646
INFO:root:current mean train loss 1634.106902091734
INFO:root:current train perplexity3.6362483501434326
INFO:root:current mean train loss 1636.067597542861
INFO:root:current train perplexity3.6391220092773438
INFO:root:current mean train loss 1640.1254864246673
INFO:root:current train perplexity3.643984079360962
INFO:root:current mean train loss 1642.1537592987645
INFO:root:current train perplexity3.645981550216675
INFO:root:current mean train loss 1643.3733099001122
INFO:root:current train perplexity3.6469483375549316
INFO:root:current mean train loss 1643.729006148129
INFO:root:current train perplexity3.650935649871826
INFO:root:current mean train loss 1643.7788462723074
INFO:root:current train perplexity3.65380597114563
INFO:root:current mean train loss 1643.386371384673
INFO:root:current train perplexity3.656806468963623
INFO:root:current mean train loss 1644.0080248267639
INFO:root:current train perplexity3.6593496799468994
INFO:root:current mean train loss 1644.4608522393385
INFO:root:current train perplexity3.6617467403411865
INFO:root:current mean train loss 1646.2707955880762
INFO:root:current train perplexity3.6651766300201416
INFO:root:current mean train loss 1647.934733506559
INFO:root:current train perplexity3.6676766872406006
INFO:root:current mean train loss 1648.2399235016944
INFO:root:current train perplexity3.668283224105835
INFO:root:current mean train loss 1648.9584305027668
INFO:root:current train perplexity3.6707236766815186

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it]
INFO:root:final mean train loss: 1650.1661477601112
INFO:root:final train perplexity: 3.6745119094848633
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2128.8881723390405
INFO:root:eval perplexity: 5.594141483306885
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/71
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [3:54:41<7:06:13, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1619.444091796875
INFO:root:current train perplexity3.5732712745666504
INFO:root:current mean train loss 1629.1133779849647
INFO:root:current train perplexity3.63065767288208
INFO:root:current mean train loss 1633.0699119197511
INFO:root:current train perplexity3.641326427459717
INFO:root:current mean train loss 1634.2920879289215
INFO:root:current train perplexity3.6392595767974854
INFO:root:current mean train loss 1631.90478515625
INFO:root:current train perplexity3.6417956352233887
INFO:root:current mean train loss 1633.9498409226007
INFO:root:current train perplexity3.640982151031494
INFO:root:current mean train loss 1634.6023213100118
INFO:root:current train perplexity3.6381547451019287
INFO:root:current mean train loss 1636.651990333968
INFO:root:current train perplexity3.6409244537353516
INFO:root:current mean train loss 1638.5599836250096
INFO:root:current train perplexity3.6410930156707764
INFO:root:current mean train loss 1639.1074641819296
INFO:root:current train perplexity3.644296884536743
INFO:root:current mean train loss 1639.118295354824
INFO:root:current train perplexity3.6447930335998535
INFO:root:current mean train loss 1640.518935798521
INFO:root:current train perplexity3.645435094833374
INFO:root:current mean train loss 1641.1207255146792
INFO:root:current train perplexity3.6457502841949463
INFO:root:current mean train loss 1641.9300281004737
INFO:root:current train perplexity3.645541191101074
INFO:root:current mean train loss 1643.0614841944123
INFO:root:current train perplexity3.6483914852142334
INFO:root:current mean train loss 1643.4212126104956
INFO:root:current train perplexity3.649989366531372
INFO:root:current mean train loss 1643.9766129699176
INFO:root:current train perplexity3.651949167251587
INFO:root:current mean train loss 1645.0652379956084
INFO:root:current train perplexity3.6547882556915283
INFO:root:current mean train loss 1645.1065033736288
INFO:root:current train perplexity3.656162738800049
INFO:root:current mean train loss 1644.8769721464496
INFO:root:current train perplexity3.6572396755218506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.45s/it]
INFO:root:final mean train loss: 1645.3271811865702
INFO:root:final train perplexity: 3.660515546798706
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2131.687735483156
INFO:root:eval perplexity: 5.6068196296691895
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/72
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/200 [3:57:59<7:02:46, 198.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.6708135190217
INFO:root:current train perplexity3.5820531845092773
INFO:root:current mean train loss 1618.645998078633
INFO:root:current train perplexity3.6075069904327393
INFO:root:current mean train loss 1622.3498080813831
INFO:root:current train perplexity3.6057045459747314
INFO:root:current mean train loss 1623.4818372224506
INFO:root:current train perplexity3.6169850826263428
INFO:root:current mean train loss 1628.8330987159243
INFO:root:current train perplexity3.621232032775879
INFO:root:current mean train loss 1632.0329008667693
INFO:root:current train perplexity3.626018762588501
INFO:root:current mean train loss 1635.7280195061696
INFO:root:current train perplexity3.631943702697754
INFO:root:current mean train loss 1636.045953310047
INFO:root:current train perplexity3.6353323459625244
INFO:root:current mean train loss 1636.4320984999147
INFO:root:current train perplexity3.634930372238159
INFO:root:current mean train loss 1637.0294945945202
INFO:root:current train perplexity3.636676549911499
INFO:root:current mean train loss 1637.9518714822748
INFO:root:current train perplexity3.638791084289551
INFO:root:current mean train loss 1638.1053550496022
INFO:root:current train perplexity3.640902280807495
INFO:root:current mean train loss 1637.3936895337733
INFO:root:current train perplexity3.6398978233337402
INFO:root:current mean train loss 1637.2143264043898
INFO:root:current train perplexity3.6403772830963135
INFO:root:current mean train loss 1636.7385088343563
INFO:root:current train perplexity3.6406874656677246
INFO:root:current mean train loss 1638.165712573226
INFO:root:current train perplexity3.642303228378296
INFO:root:current mean train loss 1639.062208776186
INFO:root:current train perplexity3.6428208351135254
INFO:root:current mean train loss 1640.238497760084
INFO:root:current train perplexity3.6452412605285645
INFO:root:current mean train loss 1640.7996255662242
INFO:root:current train perplexity3.6466870307922363
INFO:root:current mean train loss 1640.939774583171
INFO:root:current train perplexity3.646170139312744

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it]
INFO:root:final mean train loss: 1640.9642965881378
INFO:root:final train perplexity: 3.647942304611206
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2134.3850379023993
INFO:root:eval perplexity: 5.619065761566162
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/73
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/200 [4:01:17<6:59:33, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1632.0049438476562
INFO:root:current train perplexity3.563206672668457
INFO:root:current mean train loss 1627.8626752580915
INFO:root:current train perplexity3.5788979530334473
INFO:root:current mean train loss 1625.3209437052408
INFO:root:current train perplexity3.5866401195526123
INFO:root:current mean train loss 1625.5554554658777
INFO:root:current train perplexity3.6042962074279785
INFO:root:current mean train loss 1627.569911887429
INFO:root:current train perplexity3.6107332706451416
INFO:root:current mean train loss 1627.636016619647
INFO:root:current train perplexity3.6146483421325684
INFO:root:current mean train loss 1630.2247554779053
INFO:root:current train perplexity3.6215481758117676
INFO:root:current mean train loss 1629.4313300055426
INFO:root:current train perplexity3.622607707977295
INFO:root:current mean train loss 1629.5846056256976
INFO:root:current train perplexity3.619567394256592
INFO:root:current mean train loss 1630.7350391144448
INFO:root:current train perplexity3.621699333190918
INFO:root:current mean train loss 1631.4997003408578
INFO:root:current train perplexity3.6250498294830322
INFO:root:current mean train loss 1631.3211908708538
INFO:root:current train perplexity3.6249988079071045
INFO:root:current mean train loss 1630.2052264798072
INFO:root:current train perplexity3.625534772872925
INFO:root:current mean train loss 1631.0890148561393
INFO:root:current train perplexity3.624783992767334
INFO:root:current mean train loss 1632.2565896775986
INFO:root:current train perplexity3.6263530254364014
INFO:root:current mean train loss 1632.3863542036577
INFO:root:current train perplexity3.6269075870513916
INFO:root:current mean train loss 1634.2937472459746
INFO:root:current train perplexity3.630021095275879
INFO:root:current mean train loss 1635.8962672441855
INFO:root:current train perplexity3.6329753398895264
INFO:root:current mean train loss 1635.987225474482
INFO:root:current train perplexity3.634307622909546
INFO:root:current mean train loss 1636.754688632611
INFO:root:current train perplexity3.635143995285034

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 1636.3613596429022
INFO:root:final train perplexity: 3.63472318649292
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2137.4620660010805
INFO:root:eval perplexity: 5.633066177368164
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/74
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/200 [4:04:36<6:56:20, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1613.5857854475055
INFO:root:current train perplexity3.573132276535034
INFO:root:current mean train loss 1614.9876724534734
INFO:root:current train perplexity3.581461191177368
INFO:root:current mean train loss 1610.7865759704828
INFO:root:current train perplexity3.576329231262207
INFO:root:current mean train loss 1615.9619113270307
INFO:root:current train perplexity3.579461097717285
INFO:root:current mean train loss 1621.4956498093886
INFO:root:current train perplexity3.5833637714385986
INFO:root:current mean train loss 1624.951220089486
INFO:root:current train perplexity3.5874879360198975
INFO:root:current mean train loss 1625.4165092944372
INFO:root:current train perplexity3.5964839458465576
INFO:root:current mean train loss 1626.669759168335
INFO:root:current train perplexity3.600203037261963
INFO:root:current mean train loss 1627.4453407029428
INFO:root:current train perplexity3.602344512939453
INFO:root:current mean train loss 1628.0546230846314
INFO:root:current train perplexity3.602724552154541
INFO:root:current mean train loss 1630.0211931154654
INFO:root:current train perplexity3.606328248977661
INFO:root:current mean train loss 1629.9587011971964
INFO:root:current train perplexity3.609635353088379
INFO:root:current mean train loss 1629.828254547969
INFO:root:current train perplexity3.611872911453247
INFO:root:current mean train loss 1630.3806170334954
INFO:root:current train perplexity3.6133430004119873
INFO:root:current mean train loss 1631.1597287944514
INFO:root:current train perplexity3.6141512393951416
INFO:root:current mean train loss 1631.4850264336715
INFO:root:current train perplexity3.614809989929199
INFO:root:current mean train loss 1631.509197633383
INFO:root:current train perplexity3.6176397800445557
INFO:root:current mean train loss 1631.9962343705536
INFO:root:current train perplexity3.618025779724121
INFO:root:current mean train loss 1631.6030099896507
INFO:root:current train perplexity3.6194307804107666
INFO:root:current mean train loss 1631.8007765717816
INFO:root:current train perplexity3.620375871658325

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it]
INFO:root:final mean train loss: 1631.25630604277
INFO:root:final train perplexity: 3.6201186180114746
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2139.150872413148
INFO:root:eval perplexity: 5.640765190124512
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/75
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 75/200 [4:07:54<6:52:58, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1629.585041767842
INFO:root:current train perplexity3.586336851119995
INFO:root:current mean train loss 1619.7347180596714
INFO:root:current train perplexity3.5717248916625977
INFO:root:current mean train loss 1614.5564797951358
INFO:root:current train perplexity3.5682904720306396
INFO:root:current mean train loss 1616.6055803757938
INFO:root:current train perplexity3.5780110359191895
INFO:root:current mean train loss 1615.5953299606904
INFO:root:current train perplexity3.5781569480895996
INFO:root:current mean train loss 1618.6896298504873
INFO:root:current train perplexity3.5825936794281006
INFO:root:current mean train loss 1620.5576894516762
INFO:root:current train perplexity3.590787649154663
INFO:root:current mean train loss 1621.3348316123627
INFO:root:current train perplexity3.5914134979248047
INFO:root:current mean train loss 1621.8211393378147
INFO:root:current train perplexity3.590611219406128
INFO:root:current mean train loss 1619.8942700646496
INFO:root:current train perplexity3.586125612258911
INFO:root:current mean train loss 1621.1514429983924
INFO:root:current train perplexity3.5893192291259766
INFO:root:current mean train loss 1621.8024278475032
INFO:root:current train perplexity3.592486619949341
INFO:root:current mean train loss 1623.2579717471424
INFO:root:current train perplexity3.594879627227783
INFO:root:current mean train loss 1623.756275782245
INFO:root:current train perplexity3.59706449508667
INFO:root:current mean train loss 1624.4096246561494
INFO:root:current train perplexity3.6004481315612793
INFO:root:current mean train loss 1624.7654550011912
INFO:root:current train perplexity3.6010451316833496
INFO:root:current mean train loss 1625.6453556256768
INFO:root:current train perplexity3.6044487953186035
INFO:root:current mean train loss 1626.4334709915797
INFO:root:current train perplexity3.6054046154022217
INFO:root:current mean train loss 1626.7262013047584
INFO:root:current train perplexity3.6061296463012695
INFO:root:current mean train loss 1627.7387792399832
INFO:root:current train perplexity3.6085081100463867

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 1627.238953283082
INFO:root:final train perplexity: 3.6086678504943848
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2141.4398634890294
INFO:root:eval perplexity: 5.651217937469482
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/76
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/200 [4:11:12<6:49:42, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1594.6322289770776
INFO:root:current train perplexity3.531834125518799
INFO:root:current mean train loss 1603.7106754642507
INFO:root:current train perplexity3.545914888381958
INFO:root:current mean train loss 1610.0249245764874
INFO:root:current train perplexity3.5559606552124023
INFO:root:current mean train loss 1614.5916922879037
INFO:root:current train perplexity3.5702810287475586
INFO:root:current mean train loss 1614.4411270545602
INFO:root:current train perplexity3.5704915523529053
INFO:root:current mean train loss 1614.587093966463
INFO:root:current train perplexity3.571868896484375
INFO:root:current mean train loss 1614.4218864827355
INFO:root:current train perplexity3.577415704727173
INFO:root:current mean train loss 1614.7843775309143
INFO:root:current train perplexity3.577410936355591
INFO:root:current mean train loss 1617.2723655829125
INFO:root:current train perplexity3.5807254314422607
INFO:root:current mean train loss 1618.293247873439
INFO:root:current train perplexity3.5834522247314453
INFO:root:current mean train loss 1618.6151550460802
INFO:root:current train perplexity3.5837936401367188
INFO:root:current mean train loss 1618.7143683829895
INFO:root:current train perplexity3.5833427906036377
INFO:root:current mean train loss 1618.3928459988927
INFO:root:current train perplexity3.582420825958252
INFO:root:current mean train loss 1618.700145027605
INFO:root:current train perplexity3.5832412242889404
INFO:root:current mean train loss 1619.1405538537213
INFO:root:current train perplexity3.5846469402313232
INFO:root:current mean train loss 1619.4535579621454
INFO:root:current train perplexity3.5851857662200928
INFO:root:current mean train loss 1620.7708655774136
INFO:root:current train perplexity3.5885045528411865
INFO:root:current mean train loss 1622.2202800706004
INFO:root:current train perplexity3.5905120372772217
INFO:root:current mean train loss 1622.0481722633526
INFO:root:current train perplexity3.5921854972839355

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it]
INFO:root:final mean train loss: 1622.318840607813
INFO:root:final train perplexity: 3.594691514968872
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2143.3435703575187
INFO:root:eval perplexity: 5.6599225997924805
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/77
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/200 [4:14:30<6:46:26, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1592.9710845947266
INFO:root:current train perplexity3.4746081829071045
INFO:root:current mean train loss 1598.2518287941261
INFO:root:current train perplexity3.5578720569610596
INFO:root:current mean train loss 1600.7288483839768
INFO:root:current train perplexity3.5517678260803223
INFO:root:current mean train loss 1602.705130044516
INFO:root:current train perplexity3.5459542274475098
INFO:root:current mean train loss 1604.8843979181029
INFO:root:current train perplexity3.55019211769104
INFO:root:current mean train loss 1606.4002884992465
INFO:root:current train perplexity3.555269718170166
INFO:root:current mean train loss 1608.8083186902497
INFO:root:current train perplexity3.556462526321411
INFO:root:current mean train loss 1609.1733753613833
INFO:root:current train perplexity3.561349630355835
INFO:root:current mean train loss 1611.3239637318225
INFO:root:current train perplexity3.5641794204711914
INFO:root:current mean train loss 1612.429615306434
INFO:root:current train perplexity3.566732168197632
INFO:root:current mean train loss 1613.7277575296068
INFO:root:current train perplexity3.567488670349121
INFO:root:current mean train loss 1613.235294603699
INFO:root:current train perplexity3.5691399574279785
INFO:root:current mean train loss 1614.3806417098897
INFO:root:current train perplexity3.5717313289642334
INFO:root:current mean train loss 1614.932518915299
INFO:root:current train perplexity3.57228684425354
INFO:root:current mean train loss 1615.5613035722213
INFO:root:current train perplexity3.574300527572632
INFO:root:current mean train loss 1615.498180601894
INFO:root:current train perplexity3.575598955154419
INFO:root:current mean train loss 1615.7145646128488
INFO:root:current train perplexity3.5774238109588623
INFO:root:current mean train loss 1617.1207352577942
INFO:root:current train perplexity3.581372022628784
INFO:root:current mean train loss 1617.963350684242
INFO:root:current train perplexity3.582653522491455
INFO:root:current mean train loss 1618.353791498788
INFO:root:current train perplexity3.582080841064453

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it]
INFO:root:final mean train loss: 1618.3712975698713
INFO:root:final train perplexity: 3.5835180282592773
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2146.580211450022
INFO:root:eval perplexity: 5.6747589111328125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/78
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/200 [4:17:49<6:43:06, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1627.0443212890625
INFO:root:current train perplexity3.5226211547851562
INFO:root:current mean train loss 1609.0231416015624
INFO:root:current train perplexity3.5249016284942627
INFO:root:current mean train loss 1600.2196359592015
INFO:root:current train perplexity3.5246365070343018
INFO:root:current mean train loss 1603.9443216646634
INFO:root:current train perplexity3.531132459640503
INFO:root:current mean train loss 1603.3717917049632
INFO:root:current train perplexity3.5318994522094727
INFO:root:current mean train loss 1604.0015185546874
INFO:root:current train perplexity3.532390832901001
INFO:root:current mean train loss 1604.501635546875
INFO:root:current train perplexity3.535374164581299
INFO:root:current mean train loss 1605.2034400255927
INFO:root:current train perplexity3.5396006107330322
INFO:root:current mean train loss 1605.8452019708807
INFO:root:current train perplexity3.545433282852173
INFO:root:current mean train loss 1606.234059860642
INFO:root:current train perplexity3.5492312908172607
INFO:root:current mean train loss 1608.1008533012575
INFO:root:current train perplexity3.555405855178833
INFO:root:current mean train loss 1608.853755859375
INFO:root:current train perplexity3.558549642562866
INFO:root:current mean train loss 1610.0486778539541
INFO:root:current train perplexity3.559588670730591
INFO:root:current mean train loss 1611.2862105689858
INFO:root:current train perplexity3.559727430343628
INFO:root:current mean train loss 1612.0791187808388
INFO:root:current train perplexity3.5617728233337402
INFO:root:current mean train loss 1611.851346935835
INFO:root:current train perplexity3.5639467239379883
INFO:root:current mean train loss 1612.3780561899039
INFO:root:current train perplexity3.5659172534942627
INFO:root:current mean train loss 1612.049862219769
INFO:root:current train perplexity3.5654525756835938
INFO:root:current mean train loss 1613.285303135702
INFO:root:current train perplexity3.5680413246154785
INFO:root:current mean train loss 1614.2706728135147
INFO:root:current train perplexity3.570585250854492

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it]
INFO:root:final mean train loss: 1613.982475553927
INFO:root:final train perplexity: 3.5711357593536377
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2146.8180048516456
INFO:root:eval perplexity: 5.675849437713623
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/79
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 79/200 [4:21:07<6:39:44, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.7032935732886
INFO:root:current train perplexity3.5363104343414307
INFO:root:current mean train loss 1604.972735337808
INFO:root:current train perplexity3.5188772678375244
INFO:root:current mean train loss 1604.6890879229081
INFO:root:current train perplexity3.5346217155456543
INFO:root:current mean train loss 1605.0314159727934
INFO:root:current train perplexity3.538560152053833
INFO:root:current mean train loss 1605.5745034886702
INFO:root:current train perplexity3.5392563343048096
INFO:root:current mean train loss 1605.7542605241727
INFO:root:current train perplexity3.5438876152038574
INFO:root:current mean train loss 1606.7706464250512
INFO:root:current train perplexity3.543686628341675
INFO:root:current mean train loss 1606.908299530934
INFO:root:current train perplexity3.5481622219085693
INFO:root:current mean train loss 1605.8086907393576
INFO:root:current train perplexity3.5479884147644043
INFO:root:current mean train loss 1605.9519219206145
INFO:root:current train perplexity3.5445425510406494
INFO:root:current mean train loss 1606.4663937618088
INFO:root:current train perplexity3.547853469848633
INFO:root:current mean train loss 1607.0546779866395
INFO:root:current train perplexity3.5489304065704346
INFO:root:current mean train loss 1607.8931856262896
INFO:root:current train perplexity3.547642469406128
INFO:root:current mean train loss 1609.1842855120913
INFO:root:current train perplexity3.5491416454315186
INFO:root:current mean train loss 1608.503899816336
INFO:root:current train perplexity3.550564765930176
INFO:root:current mean train loss 1608.7022456504337
INFO:root:current train perplexity3.551445960998535
INFO:root:current mean train loss 1609.1360898128235
INFO:root:current train perplexity3.5522730350494385
INFO:root:current mean train loss 1608.672392922893
INFO:root:current train perplexity3.554365396499634
INFO:root:current mean train loss 1609.6560979091382
INFO:root:current train perplexity3.5572450160980225
INFO:root:current mean train loss 1609.6837201575172
INFO:root:current train perplexity3.557781457901001

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.30s/it]
INFO:root:final mean train loss: 1609.3182036937997
INFO:root:final train perplexity: 3.558023452758789
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2151.2946734056404
INFO:root:eval perplexity: 5.696436882019043
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/80
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 80/200 [4:24:25<6:36:13, 198.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1584.4168411513506
INFO:root:current train perplexity3.4785218238830566
INFO:root:current mean train loss 1590.2130349597091
INFO:root:current train perplexity3.515798330307007
INFO:root:current mean train loss 1590.2698192228222
INFO:root:current train perplexity3.512354850769043
INFO:root:current mean train loss 1595.2246971024113
INFO:root:current train perplexity3.5158424377441406
INFO:root:current mean train loss 1595.965460484324
INFO:root:current train perplexity3.5155205726623535
INFO:root:current mean train loss 1597.1765973086008
INFO:root:current train perplexity3.5168018341064453
INFO:root:current mean train loss 1596.765829314954
INFO:root:current train perplexity3.5202815532684326
INFO:root:current mean train loss 1598.250984764853
INFO:root:current train perplexity3.5240979194641113
INFO:root:current mean train loss 1598.7744719002383
INFO:root:current train perplexity3.523573160171509
INFO:root:current mean train loss 1598.8979956792964
INFO:root:current train perplexity3.5278728008270264
INFO:root:current mean train loss 1598.582460628578
INFO:root:current train perplexity3.529193639755249
INFO:root:current mean train loss 1601.1494408147514
INFO:root:current train perplexity3.5343270301818848
INFO:root:current mean train loss 1602.2655783631292
INFO:root:current train perplexity3.536911725997925
INFO:root:current mean train loss 1602.8885022879874
INFO:root:current train perplexity3.5386035442352295
INFO:root:current mean train loss 1603.7841104111335
INFO:root:current train perplexity3.538921594619751
INFO:root:current mean train loss 1604.7676069395445
INFO:root:current train perplexity3.5416460037231445
INFO:root:current mean train loss 1604.6775819894563
INFO:root:current train perplexity3.543574571609497
INFO:root:current mean train loss 1605.6221011250177
INFO:root:current train perplexity3.5446343421936035
INFO:root:current mean train loss 1605.857179178994
INFO:root:current train perplexity3.547886848449707
INFO:root:current mean train loss 1605.7684698801006
INFO:root:current train perplexity3.5472652912139893

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1605.7984533266654
INFO:root:final train perplexity: 3.5481600761413574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2152.9956478903478
INFO:root:eval perplexity: 5.704278469085693
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/81
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [4:27:43<6:33:04, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1598.150560880962
INFO:root:current train perplexity3.5033164024353027
INFO:root:current mean train loss 1589.4923102638938
INFO:root:current train perplexity3.49275279045105
INFO:root:current mean train loss 1591.2028441498246
INFO:root:current train perplexity3.4919402599334717
INFO:root:current mean train loss 1587.4426435105345
INFO:root:current train perplexity3.495466947555542
INFO:root:current mean train loss 1587.6267500164129
INFO:root:current train perplexity3.5024802684783936
INFO:root:current mean train loss 1587.7560746934678
INFO:root:current train perplexity3.5048701763153076
INFO:root:current mean train loss 1589.4513482991056
INFO:root:current train perplexity3.507476329803467
INFO:root:current mean train loss 1593.0950605254811
INFO:root:current train perplexity3.514011859893799
INFO:root:current mean train loss 1594.4989713207228
INFO:root:current train perplexity3.514547109603882
INFO:root:current mean train loss 1595.2237871513992
INFO:root:current train perplexity3.515209197998047
INFO:root:current mean train loss 1595.7499347672588
INFO:root:current train perplexity3.516350030899048
INFO:root:current mean train loss 1597.0208602178664
INFO:root:current train perplexity3.5201919078826904
INFO:root:current mean train loss 1596.9972050107758
INFO:root:current train perplexity3.5208852291107178
INFO:root:current mean train loss 1597.5743770155796
INFO:root:current train perplexity3.5228466987609863
INFO:root:current mean train loss 1597.5769530919185
INFO:root:current train perplexity3.5241386890411377
INFO:root:current mean train loss 1597.992814349644
INFO:root:current train perplexity3.5273563861846924
INFO:root:current mean train loss 1599.0573218443512
INFO:root:current train perplexity3.529799222946167
INFO:root:current mean train loss 1599.6814791705158
INFO:root:current train perplexity3.5316333770751953
INFO:root:current mean train loss 1600.9063044631405
INFO:root:current train perplexity3.5326106548309326
INFO:root:current mean train loss 1601.5628360022417
INFO:root:current train perplexity3.5347580909729004

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it]
INFO:root:final mean train loss: 1601.0936310691661
INFO:root:final train perplexity: 3.5350189208984375
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2157.6424512930794
INFO:root:eval perplexity: 5.725754737854004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/82
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/200 [4:31:01<6:29:55, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.3935953776042
INFO:root:current train perplexity3.482578754425049
INFO:root:current mean train loss 1583.5476896454015
INFO:root:current train perplexity3.485546827316284
INFO:root:current mean train loss 1576.6564849749361
INFO:root:current train perplexity3.4788498878479004
INFO:root:current mean train loss 1577.9110327956028
INFO:root:current train perplexity3.480487108230591
INFO:root:current mean train loss 1579.7855043856173
INFO:root:current train perplexity3.4807798862457275
INFO:root:current mean train loss 1582.4482333358585
INFO:root:current train perplexity3.4830715656280518
INFO:root:current mean train loss 1584.4111625814508
INFO:root:current train perplexity3.489084482192993
INFO:root:current mean train loss 1585.8877164015703
INFO:root:current train perplexity3.494317054748535
INFO:root:current mean train loss 1587.3764685345657
INFO:root:current train perplexity3.4955921173095703
INFO:root:current mean train loss 1587.5445648838747
INFO:root:current train perplexity3.499418258666992
INFO:root:current mean train loss 1588.0323547754174
INFO:root:current train perplexity3.5018625259399414
INFO:root:current mean train loss 1589.1136260756102
INFO:root:current train perplexity3.5059237480163574
INFO:root:current mean train loss 1589.717765507178
INFO:root:current train perplexity3.5096395015716553
INFO:root:current mean train loss 1591.9515706847576
INFO:root:current train perplexity3.5133750438690186
INFO:root:current mean train loss 1592.2647908664915
INFO:root:current train perplexity3.51235032081604
INFO:root:current mean train loss 1593.26305363063
INFO:root:current train perplexity3.5128324031829834
INFO:root:current mean train loss 1594.735491009626
INFO:root:current train perplexity3.5153253078460693
INFO:root:current mean train loss 1595.001972800583
INFO:root:current train perplexity3.5174076557159424
INFO:root:current mean train loss 1596.3552583350872
INFO:root:current train perplexity3.520014762878418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1596.4794132080694
INFO:root:final train perplexity: 3.5221784114837646
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2156.8027586159133
INFO:root:eval perplexity: 5.721869468688965
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/83
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/200 [4:34:20<6:26:36, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1587.4531005859376
INFO:root:current train perplexity3.4134674072265625
INFO:root:current mean train loss 1591.7745294744318
INFO:root:current train perplexity3.4988150596618652
INFO:root:current mean train loss 1587.2362008231028
INFO:root:current train perplexity3.4896836280822754
INFO:root:current mean train loss 1590.4983528383316
INFO:root:current train perplexity3.4889345169067383
INFO:root:current mean train loss 1588.0091698623285
INFO:root:current train perplexity3.4865453243255615
INFO:root:current mean train loss 1586.82860251034
INFO:root:current train perplexity3.4895687103271484
INFO:root:current mean train loss 1588.0222085921491
INFO:root:current train perplexity3.490826368331909
INFO:root:current mean train loss 1587.4391859457526
INFO:root:current train perplexity3.4938576221466064
INFO:root:current mean train loss 1587.6432046019002
INFO:root:current train perplexity3.495440721511841
INFO:root:current mean train loss 1588.3331072126116
INFO:root:current train perplexity3.497771978378296
INFO:root:current mean train loss 1588.3100672957921
INFO:root:current train perplexity3.501148223876953
INFO:root:current mean train loss 1588.2851163297087
INFO:root:current train perplexity3.5011181831359863
INFO:root:current mean train loss 1588.6182900673102
INFO:root:current train perplexity3.5032758712768555
INFO:root:current mean train loss 1589.8427622554866
INFO:root:current train perplexity3.5053553581237793
INFO:root:current mean train loss 1590.5305344601895
INFO:root:current train perplexity3.506669759750366
INFO:root:current mean train loss 1590.7172990609479
INFO:root:current train perplexity3.507132053375244
INFO:root:current mean train loss 1591.31787094211
INFO:root:current train perplexity3.5082690715789795
INFO:root:current mean train loss 1592.2845356188323
INFO:root:current train perplexity3.5095114707946777
INFO:root:current mean train loss 1592.7991879963743
INFO:root:current train perplexity3.5106189250946045
INFO:root:current mean train loss 1593.2879968453453
INFO:root:current train perplexity3.511768341064453

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it]
INFO:root:final mean train loss: 1593.086082254584
INFO:root:final train perplexity: 3.512765407562256
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2160.6604739791114
INFO:root:eval perplexity: 5.739748001098633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/84
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/200 [4:37:38<6:23:22, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1564.0821849681713
INFO:root:current train perplexity3.468843936920166
INFO:root:current mean train loss 1561.8114494263657
INFO:root:current train perplexity3.4507391452789307
INFO:root:current mean train loss 1565.757517810435
INFO:root:current train perplexity3.4630582332611084
INFO:root:current mean train loss 1572.2637517619935
INFO:root:current train perplexity3.4648454189300537
INFO:root:current mean train loss 1576.5925207205064
INFO:root:current train perplexity3.468336582183838
INFO:root:current mean train loss 1577.9229117869427
INFO:root:current train perplexity3.4742367267608643
INFO:root:current mean train loss 1578.2344799376372
INFO:root:current train perplexity3.478400707244873
INFO:root:current mean train loss 1579.5813129325352
INFO:root:current train perplexity3.4826643466949463
INFO:root:current mean train loss 1580.9439647079523
INFO:root:current train perplexity3.4862308502197266
INFO:root:current mean train loss 1582.681660640844
INFO:root:current train perplexity3.4873015880584717
INFO:root:current mean train loss 1584.280551334667
INFO:root:current train perplexity3.489173173904419
INFO:root:current mean train loss 1584.7136957258276
INFO:root:current train perplexity3.4902567863464355
INFO:root:current mean train loss 1584.055730718661
INFO:root:current train perplexity3.488837480545044
INFO:root:current mean train loss 1584.7572375644663
INFO:root:current train perplexity3.4897501468658447
INFO:root:current mean train loss 1585.015622604787
INFO:root:current train perplexity3.490205764770508
INFO:root:current mean train loss 1586.3508479849686
INFO:root:current train perplexity3.494020462036133
INFO:root:current mean train loss 1586.917869563182
INFO:root:current train perplexity3.494962453842163
INFO:root:current mean train loss 1587.4504202272276
INFO:root:current train perplexity3.496508836746216
INFO:root:current mean train loss 1588.51164558799
INFO:root:current train perplexity3.499336004257202
INFO:root:current mean train loss 1588.9778148134446
INFO:root:current train perplexity3.500239372253418

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it]
INFO:root:final mean train loss: 1588.674825925149
INFO:root:final train perplexity: 3.50056529045105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2162.79490932167
INFO:root:eval perplexity: 5.749664783477783
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/85
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 85/200 [4:40:56<6:20:02, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1576.3183122114701
INFO:root:current train perplexity3.4346280097961426
INFO:root:current mean train loss 1581.5365609063042
INFO:root:current train perplexity3.460693120956421
INFO:root:current mean train loss 1574.5604888415728
INFO:root:current train perplexity3.4544644355773926
INFO:root:current mean train loss 1574.7415317269258
INFO:root:current train perplexity3.459402561187744
INFO:root:current mean train loss 1573.6223900597374
INFO:root:current train perplexity3.461691379547119
INFO:root:current mean train loss 1574.1950975305895
INFO:root:current train perplexity3.464500904083252
INFO:root:current mean train loss 1575.2936319860612
INFO:root:current train perplexity3.4658203125
INFO:root:current mean train loss 1577.6464527089108
INFO:root:current train perplexity3.4679644107818604
INFO:root:current mean train loss 1580.2606195386552
INFO:root:current train perplexity3.472071647644043
INFO:root:current mean train loss 1580.506646754378
INFO:root:current train perplexity3.4758358001708984
INFO:root:current mean train loss 1580.6656754884684
INFO:root:current train perplexity3.47517728805542
INFO:root:current mean train loss 1582.5333600877882
INFO:root:current train perplexity3.4771668910980225
INFO:root:current mean train loss 1583.0373441935353
INFO:root:current train perplexity3.4776551723480225
INFO:root:current mean train loss 1582.9232175917853
INFO:root:current train perplexity3.478602647781372
INFO:root:current mean train loss 1583.7184789768546
INFO:root:current train perplexity3.4805288314819336
INFO:root:current mean train loss 1582.8089903994546
INFO:root:current train perplexity3.480527639389038
INFO:root:current mean train loss 1583.248948517217
INFO:root:current train perplexity3.483120918273926
INFO:root:current mean train loss 1584.2578404277835
INFO:root:current train perplexity3.4853551387786865
INFO:root:current mean train loss 1584.4088085778624
INFO:root:current train perplexity3.486815929412842
INFO:root:current mean train loss 1584.5320715884613
INFO:root:current train perplexity3.4878597259521484

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.65s/it]
INFO:root:final mean train loss: 1584.2943808689781
INFO:root:final train perplexity: 3.4884934425354004
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2164.977853761497
INFO:root:eval perplexity: 5.759824275970459
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/86
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/200 [4:44:14<6:16:40, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1551.0943443423412
INFO:root:current train perplexity3.3712871074676514
INFO:root:current mean train loss 1567.0770695846272
INFO:root:current train perplexity3.432737350463867
INFO:root:current mean train loss 1568.8157032933727
INFO:root:current train perplexity3.443232297897339
INFO:root:current mean train loss 1567.678703160167
INFO:root:current train perplexity3.4437882900238037
INFO:root:current mean train loss 1569.2595625275387
INFO:root:current train perplexity3.4506008625030518
INFO:root:current mean train loss 1568.2033415061692
INFO:root:current train perplexity3.4545116424560547
INFO:root:current mean train loss 1568.4731955016075
INFO:root:current train perplexity3.454099416732788
INFO:root:current mean train loss 1569.3800541279977
INFO:root:current train perplexity3.4557371139526367
INFO:root:current mean train loss 1571.1621095167773
INFO:root:current train perplexity3.4562458992004395
INFO:root:current mean train loss 1572.2831792444395
INFO:root:current train perplexity3.455636501312256
INFO:root:current mean train loss 1574.5611794316242
INFO:root:current train perplexity3.4595940113067627
INFO:root:current mean train loss 1574.6221547418377
INFO:root:current train perplexity3.4622113704681396
INFO:root:current mean train loss 1575.5721707567159
INFO:root:current train perplexity3.4641380310058594
INFO:root:current mean train loss 1576.6461506324335
INFO:root:current train perplexity3.4652717113494873
INFO:root:current mean train loss 1577.6087727530373
INFO:root:current train perplexity3.4668190479278564
INFO:root:current mean train loss 1578.628621836338
INFO:root:current train perplexity3.470099687576294
INFO:root:current mean train loss 1579.549993047651
INFO:root:current train perplexity3.473097562789917
INFO:root:current mean train loss 1580.1343331453409
INFO:root:current train perplexity3.475945234298706
INFO:root:current mean train loss 1580.4078478026556
INFO:root:current train perplexity3.477477788925171
INFO:root:current mean train loss 1580.9395712113758
INFO:root:current train perplexity3.4783244132995605

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.66s/it]
INFO:root:final mean train loss: 1580.5795424130008
INFO:root:final train perplexity: 3.4782874584198
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2166.702879560755
INFO:root:eval perplexity: 5.767865180969238
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/87
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/200 [4:47:33<6:13:21, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1583.6897379557292
INFO:root:current train perplexity3.4453811645507812
INFO:root:current mean train loss 1567.1723673959796
INFO:root:current train perplexity3.4378502368927
INFO:root:current mean train loss 1569.6290796952283
INFO:root:current train perplexity3.4381790161132812
INFO:root:current mean train loss 1572.9373598451969
INFO:root:current train perplexity3.4504244327545166
INFO:root:current mean train loss 1569.988574678429
INFO:root:current train perplexity3.448011636734009
INFO:root:current mean train loss 1569.9425914724804
INFO:root:current train perplexity3.4512686729431152
INFO:root:current mean train loss 1570.1949135209254
INFO:root:current train perplexity3.451606512069702
INFO:root:current mean train loss 1571.4757379762311
INFO:root:current train perplexity3.454702377319336
INFO:root:current mean train loss 1572.2192245170577
INFO:root:current train perplexity3.4540464878082275
INFO:root:current mean train loss 1574.7386834080235
INFO:root:current train perplexity3.4558610916137695
INFO:root:current mean train loss 1575.798485467518
INFO:root:current train perplexity3.4585037231445312
INFO:root:current mean train loss 1575.4687645075073
INFO:root:current train perplexity3.459890127182007
INFO:root:current mean train loss 1576.1723016729936
INFO:root:current train perplexity3.4615468978881836
INFO:root:current mean train loss 1575.6410060932396
INFO:root:current train perplexity3.462329626083374
INFO:root:current mean train loss 1576.0828446941866
INFO:root:current train perplexity3.4640755653381348
INFO:root:current mean train loss 1576.524637239212
INFO:root:current train perplexity3.466270685195923
INFO:root:current mean train loss 1576.591624827152
INFO:root:current train perplexity3.465543270111084
INFO:root:current mean train loss 1576.5872412768472
INFO:root:current train perplexity3.467607021331787
INFO:root:current mean train loss 1576.497706209128
INFO:root:current train perplexity3.4672815799713135
INFO:root:current mean train loss 1577.184813589129
INFO:root:current train perplexity3.467756509780884

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 1576.7507660327628
INFO:root:final train perplexity: 3.4677999019622803
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2171.714667137633
INFO:root:eval perplexity: 5.791290760040283
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/88
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/200 [4:50:51<6:10:13, 198.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1563.0875950863488
INFO:root:current train perplexity3.459902763366699
INFO:root:current mean train loss 1559.9532157702324
INFO:root:current train perplexity3.442633867263794
INFO:root:current mean train loss 1556.8179886122882
INFO:root:current train perplexity3.4389102458953857
INFO:root:current mean train loss 1560.7637395544898
INFO:root:current train perplexity3.4388089179992676
INFO:root:current mean train loss 1560.6374911221592
INFO:root:current train perplexity3.4348304271698
INFO:root:current mean train loss 1561.7466076762737
INFO:root:current train perplexity3.438505172729492
INFO:root:current mean train loss 1565.575082902428
INFO:root:current train perplexity3.440366744995117
INFO:root:current mean train loss 1567.75758693863
INFO:root:current train perplexity3.444748878479004
INFO:root:current mean train loss 1567.4351843466306
INFO:root:current train perplexity3.4459123611450195
INFO:root:current mean train loss 1567.9023362662924
INFO:root:current train perplexity3.448789358139038
INFO:root:current mean train loss 1568.2666114841966
INFO:root:current train perplexity3.4497311115264893
INFO:root:current mean train loss 1569.6833739212866
INFO:root:current train perplexity3.4495604038238525
INFO:root:current mean train loss 1570.2008993612753
INFO:root:current train perplexity3.4524030685424805
INFO:root:current mean train loss 1570.786352451557
INFO:root:current train perplexity3.452674627304077
INFO:root:current mean train loss 1571.5270449022785
INFO:root:current train perplexity3.4533066749572754
INFO:root:current mean train loss 1570.8667259288059
INFO:root:current train perplexity3.4528377056121826
INFO:root:current mean train loss 1571.3397000743225
INFO:root:current train perplexity3.452984571456909
INFO:root:current mean train loss 1572.3661045765145
INFO:root:current train perplexity3.4542300701141357
INFO:root:current mean train loss 1573.3321718724233
INFO:root:current train perplexity3.4568004608154297

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it]
INFO:root:final mean train loss: 1572.909520474817
INFO:root:final train perplexity: 3.4573111534118652
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2173.3119705957724
INFO:root:eval perplexity: 5.798777103424072
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/89
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/200 [4:54:09<6:06:42, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1526.9196268717449
INFO:root:current train perplexity3.4065608978271484
INFO:root:current mean train loss 1552.7947954450335
INFO:root:current train perplexity3.40618896484375
INFO:root:current mean train loss 1551.9817873396964
INFO:root:current train perplexity3.404179573059082
INFO:root:current mean train loss 1554.028044089293
INFO:root:current train perplexity3.411238193511963
INFO:root:current mean train loss 1555.535414908696
INFO:root:current train perplexity3.4119837284088135
INFO:root:current mean train loss 1556.786465883255
INFO:root:current train perplexity3.4145398139953613
INFO:root:current mean train loss 1559.3843088586345
INFO:root:current train perplexity3.418405532836914
INFO:root:current mean train loss 1562.1333172401685
INFO:root:current train perplexity3.4226646423339844
INFO:root:current mean train loss 1562.2931610257756
INFO:root:current train perplexity3.4226644039154053
INFO:root:current mean train loss 1562.226163496051
INFO:root:current train perplexity3.424607276916504
INFO:root:current mean train loss 1564.277816350281
INFO:root:current train perplexity3.4296655654907227
INFO:root:current mean train loss 1564.0193129066083
INFO:root:current train perplexity3.4309206008911133
INFO:root:current mean train loss 1564.1912779351665
INFO:root:current train perplexity3.4337360858917236
INFO:root:current mean train loss 1565.1959790485662
INFO:root:current train perplexity3.4345486164093018
INFO:root:current mean train loss 1565.9134807640703
INFO:root:current train perplexity3.4382104873657227
INFO:root:current mean train loss 1566.6368652020813
INFO:root:current train perplexity3.438591718673706
INFO:root:current mean train loss 1567.055661563542
INFO:root:current train perplexity3.4403748512268066
INFO:root:current mean train loss 1567.8895856910776
INFO:root:current train perplexity3.442310094833374
INFO:root:current mean train loss 1568.3344550732745
INFO:root:current train perplexity3.4440367221832275
INFO:root:current mean train loss 1568.5991502705976
INFO:root:current train perplexity3.445240020751953

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it]
INFO:root:final mean train loss: 1568.8689438227866
INFO:root:final train perplexity: 3.4463107585906982
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2174.9828257459276
INFO:root:eval perplexity: 5.806619167327881
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/90
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 90/200 [4:57:28<6:03:37, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1585.6998248922414
INFO:root:current train perplexity3.4290661811828613
INFO:root:current mean train loss 1559.0861882645954
INFO:root:current train perplexity3.4203014373779297
INFO:root:current mean train loss 1561.3394178365515
INFO:root:current train perplexity3.410078763961792
INFO:root:current mean train loss 1560.2680196559174
INFO:root:current train perplexity3.414501190185547
INFO:root:current mean train loss 1559.747177301865
INFO:root:current train perplexity3.4104247093200684
INFO:root:current mean train loss 1558.468845533288
INFO:root:current train perplexity3.412003517150879
INFO:root:current mean train loss 1558.8253030215992
INFO:root:current train perplexity3.4107165336608887
INFO:root:current mean train loss 1561.0996457114304
INFO:root:current train perplexity3.415086507797241
INFO:root:current mean train loss 1563.2905869800306
INFO:root:current train perplexity3.4216346740722656
INFO:root:current mean train loss 1563.7019479215724
INFO:root:current train perplexity3.4229514598846436
INFO:root:current mean train loss 1565.2876290932109
INFO:root:current train perplexity3.4263739585876465
INFO:root:current mean train loss 1565.0284797932006
INFO:root:current train perplexity3.4254403114318848
INFO:root:current mean train loss 1566.1023464715024
INFO:root:current train perplexity3.4287679195404053
INFO:root:current mean train loss 1564.9005524669042
INFO:root:current train perplexity3.4296135902404785
INFO:root:current mean train loss 1565.4702699419665
INFO:root:current train perplexity3.430516481399536
INFO:root:current mean train loss 1566.075136712363
INFO:root:current train perplexity3.430774211883545
INFO:root:current mean train loss 1565.3424422964865
INFO:root:current train perplexity3.4314703941345215
INFO:root:current mean train loss 1565.4567898628407
INFO:root:current train perplexity3.431767702102661
INFO:root:current mean train loss 1565.605935807434
INFO:root:current train perplexity3.434384822845459
INFO:root:current mean train loss 1565.3273656834217
INFO:root:current train perplexity3.434706687927246

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it]
INFO:root:final mean train loss: 1564.9732047353677
INFO:root:final train perplexity: 3.4357385635375977
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2177.919337062971
INFO:root:eval perplexity: 5.8204240798950195
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/91
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [5:00:46<6:00:17, 198.33s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1539.6182516346807
INFO:root:current train perplexity3.3862757682800293
INFO:root:current mean train loss 1555.4700852485553
INFO:root:current train perplexity3.399890899658203
INFO:root:current mean train loss 1552.952846620141
INFO:root:current train perplexity3.3890416622161865
INFO:root:current mean train loss 1553.5060851697976
INFO:root:current train perplexity3.3895115852355957
INFO:root:current mean train loss 1553.3503215430562
INFO:root:current train perplexity3.3964717388153076
INFO:root:current mean train loss 1555.750743824047
INFO:root:current train perplexity3.402857780456543
INFO:root:current mean train loss 1556.1309574469694
INFO:root:current train perplexity3.4043893814086914
INFO:root:current mean train loss 1555.5464403904155
INFO:root:current train perplexity3.4060475826263428
INFO:root:current mean train loss 1554.404040181045
INFO:root:current train perplexity3.4094929695129395
INFO:root:current mean train loss 1555.1334912419068
INFO:root:current train perplexity3.4112093448638916
INFO:root:current mean train loss 1554.9811163295067
INFO:root:current train perplexity3.4124417304992676
INFO:root:current mean train loss 1556.7170172619778
INFO:root:current train perplexity3.4165539741516113
INFO:root:current mean train loss 1556.6514908253287
INFO:root:current train perplexity3.4167609214782715
INFO:root:current mean train loss 1558.3473946059796
INFO:root:current train perplexity3.418649435043335
INFO:root:current mean train loss 1560.023652684804
INFO:root:current train perplexity3.421607732772827
INFO:root:current mean train loss 1561.3156581153228
INFO:root:current train perplexity3.422856569290161
INFO:root:current mean train loss 1562.1784033885413
INFO:root:current train perplexity3.424732208251953
INFO:root:current mean train loss 1562.0543324753455
INFO:root:current train perplexity3.425790786743164
INFO:root:current mean train loss 1561.8427233132873
INFO:root:current train perplexity3.4269020557403564
INFO:root:current mean train loss 1562.5853021196324
INFO:root:current train perplexity3.4273593425750732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 189.00s/it]
INFO:root:final mean train loss: 1561.8423763550236
INFO:root:final train perplexity: 3.4272665977478027
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2181.1777118655805
INFO:root:eval perplexity: 5.8357834815979
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/92
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/200 [5:04:05<5:57:06, 198.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1535.0416492280506
INFO:root:current train perplexity3.3799383640289307
INFO:root:current mean train loss 1547.8673560019652
INFO:root:current train perplexity3.380753993988037
INFO:root:current mean train loss 1552.4590577100166
INFO:root:current train perplexity3.3917698860168457
INFO:root:current mean train loss 1549.7440626076102
INFO:root:current train perplexity3.3865911960601807
INFO:root:current mean train loss 1551.7804100086055
INFO:root:current train perplexity3.3893678188323975
INFO:root:current mean train loss 1552.514374158734
INFO:root:current train perplexity3.3965208530426025
INFO:root:current mean train loss 1553.231760706837
INFO:root:current train perplexity3.4015400409698486
INFO:root:current mean train loss 1554.1285589175643
INFO:root:current train perplexity3.4022209644317627
INFO:root:current mean train loss 1553.776612035369
INFO:root:current train perplexity3.4007222652435303
INFO:root:current mean train loss 1555.421557465075
INFO:root:current train perplexity3.405653476715088
INFO:root:current mean train loss 1555.7718625288467
INFO:root:current train perplexity3.4061508178710938
INFO:root:current mean train loss 1555.915133527918
INFO:root:current train perplexity3.409024238586426
INFO:root:current mean train loss 1556.3195359085819
INFO:root:current train perplexity3.4088399410247803
INFO:root:current mean train loss 1555.908619758231
INFO:root:current train perplexity3.410736083984375
INFO:root:current mean train loss 1555.865147765988
INFO:root:current train perplexity3.4105656147003174
INFO:root:current mean train loss 1555.489591143334
INFO:root:current train perplexity3.410629987716675
INFO:root:current mean train loss 1556.532874129726
INFO:root:current train perplexity3.412701368331909
INFO:root:current mean train loss 1556.3915814136283
INFO:root:current train perplexity3.4134583473205566
INFO:root:current mean train loss 1556.808208733813
INFO:root:current train perplexity3.4144086837768555
INFO:root:current mean train loss 1557.697940152091
INFO:root:current train perplexity3.414600133895874

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it]
INFO:root:final mean train loss: 1557.26802633173
INFO:root:final train perplexity: 3.414924383163452
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2183.258908968445
INFO:root:eval perplexity: 5.845613956451416
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/93
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/200 [5:07:23<5:53:34, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1542.667057800293
INFO:root:current train perplexity3.380160093307495
INFO:root:current mean train loss 1545.4284227159287
INFO:root:current train perplexity3.378572702407837
INFO:root:current mean train loss 1548.2960000174387
INFO:root:current train perplexity3.384894371032715
INFO:root:current mean train loss 1551.2546149002878
INFO:root:current train perplexity3.390421152114868
INFO:root:current mean train loss 1551.8325991312663
INFO:root:current train perplexity3.393178939819336
INFO:root:current mean train loss 1550.4143003266433
INFO:root:current train perplexity3.390180826187134
INFO:root:current mean train loss 1551.536881570255
INFO:root:current train perplexity3.3910114765167236
INFO:root:current mean train loss 1550.2434083596254
INFO:root:current train perplexity3.3894917964935303
INFO:root:current mean train loss 1550.8226554177024
INFO:root:current train perplexity3.3899953365325928
INFO:root:current mean train loss 1551.1156003368144
INFO:root:current train perplexity3.3941383361816406
INFO:root:current mean train loss 1552.2646296748408
INFO:root:current train perplexity3.3960957527160645
INFO:root:current mean train loss 1552.3574667720472
INFO:root:current train perplexity3.396306276321411
INFO:root:current mean train loss 1552.507518863678
INFO:root:current train perplexity3.3986172676086426
INFO:root:current mean train loss 1553.000310837013
INFO:root:current train perplexity3.400207042694092
INFO:root:current mean train loss 1552.8031212059227
INFO:root:current train perplexity3.400607109069824
INFO:root:current mean train loss 1552.9488502985314
INFO:root:current train perplexity3.4009764194488525
INFO:root:current mean train loss 1553.4605020432246
INFO:root:current train perplexity3.4027693271636963
INFO:root:current mean train loss 1554.0766567958874
INFO:root:current train perplexity3.403721809387207
INFO:root:current mean train loss 1554.219651372382
INFO:root:current train perplexity3.404761552810669
INFO:root:current mean train loss 1554.5319691667653
INFO:root:current train perplexity3.4062514305114746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it]
INFO:root:final mean train loss: 1554.0775924890377
INFO:root:final train perplexity: 3.4063425064086914
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2187.1495140562665
INFO:root:eval perplexity: 5.864035606384277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/94
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/200 [5:10:41<5:50:09, 198.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1532.0248494885632
INFO:root:current train perplexity3.3482167720794678
INFO:root:current mean train loss 1542.5849373909423
INFO:root:current train perplexity3.3666746616363525
INFO:root:current mean train loss 1541.8631858954125
INFO:root:current train perplexity3.3747663497924805
INFO:root:current mean train loss 1544.2317767779832
INFO:root:current train perplexity3.379997730255127
INFO:root:current mean train loss 1543.763240330656
INFO:root:current train perplexity3.379429578781128
INFO:root:current mean train loss 1544.2051292432213
INFO:root:current train perplexity3.37758731842041
INFO:root:current mean train loss 1545.8753546519122
INFO:root:current train perplexity3.380618095397949
INFO:root:current mean train loss 1544.9261643087848
INFO:root:current train perplexity3.383333921432495
INFO:root:current mean train loss 1544.9071660036493
INFO:root:current train perplexity3.386848211288452
INFO:root:current mean train loss 1547.0707382156234
INFO:root:current train perplexity3.3902361392974854
INFO:root:current mean train loss 1547.2143704910766
INFO:root:current train perplexity3.3900272846221924
INFO:root:current mean train loss 1547.0718316992024
INFO:root:current train perplexity3.3908755779266357
INFO:root:current mean train loss 1547.2286643305465
INFO:root:current train perplexity3.3914265632629395
INFO:root:current mean train loss 1547.3934659964711
INFO:root:current train perplexity3.3925845623016357
INFO:root:current mean train loss 1548.7716261787898
INFO:root:current train perplexity3.395223379135132
INFO:root:current mean train loss 1549.363182722365
INFO:root:current train perplexity3.3953678607940674
INFO:root:current mean train loss 1549.9303978384419
INFO:root:current train perplexity3.395343065261841
INFO:root:current mean train loss 1550.8919976626626
INFO:root:current train perplexity3.3964297771453857
INFO:root:current mean train loss 1551.0619059287692
INFO:root:current train perplexity3.398135185241699

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it]
INFO:root:final mean train loss: 1551.370512361666
INFO:root:final train perplexity: 3.3990774154663086
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2187.2231670406695
INFO:root:eval perplexity: 5.864384651184082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/95
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 95/200 [5:13:59<5:46:55, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1522.6924700055804
INFO:root:current train perplexity3.325679063796997
INFO:root:current mean train loss 1530.1442753306605
INFO:root:current train perplexity3.328965663909912
INFO:root:current mean train loss 1533.7902683721525
INFO:root:current train perplexity3.3431074619293213
INFO:root:current mean train loss 1533.049496012888
INFO:root:current train perplexity3.3392016887664795
INFO:root:current mean train loss 1534.764436731016
INFO:root:current train perplexity3.358592987060547
INFO:root:current mean train loss 1537.604809712807
INFO:root:current train perplexity3.3621890544891357
INFO:root:current mean train loss 1537.2080086077463
INFO:root:current train perplexity3.363474130630493
INFO:root:current mean train loss 1537.72051848083
INFO:root:current train perplexity3.366819381713867
INFO:root:current mean train loss 1539.3330802448845
INFO:root:current train perplexity3.368884563446045
INFO:root:current mean train loss 1541.4847639154814
INFO:root:current train perplexity3.371960401535034
INFO:root:current mean train loss 1542.1259365947053
INFO:root:current train perplexity3.373638868331909
INFO:root:current mean train loss 1543.8546628010338
INFO:root:current train perplexity3.376612663269043
INFO:root:current mean train loss 1544.5511007041869
INFO:root:current train perplexity3.380098819732666
INFO:root:current mean train loss 1545.4106619964089
INFO:root:current train perplexity3.381908893585205
INFO:root:current mean train loss 1545.8332772477513
INFO:root:current train perplexity3.3827624320983887
INFO:root:current mean train loss 1545.9497966086062
INFO:root:current train perplexity3.384434223175049
INFO:root:current mean train loss 1546.668350919324
INFO:root:current train perplexity3.38450288772583
INFO:root:current mean train loss 1546.3356697144836
INFO:root:current train perplexity3.385624647140503
INFO:root:current mean train loss 1547.1574118886524
INFO:root:current train perplexity3.386333465576172
INFO:root:current mean train loss 1547.2953055609978
INFO:root:current train perplexity3.386927843093872

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it]
INFO:root:final mean train loss: 1547.255382235821
INFO:root:final train perplexity: 3.388063669204712
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2189.5496072972073
INFO:root:eval perplexity: 5.875430107116699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/96
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/200 [5:17:17<5:43:42, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1520.8743542086693
INFO:root:current train perplexity3.318419933319092
INFO:root:current mean train loss 1530.8055652880487
INFO:root:current train perplexity3.3463518619537354
INFO:root:current mean train loss 1529.7021695752164
INFO:root:current train perplexity3.3447341918945312
INFO:root:current mean train loss 1528.9374273478804
INFO:root:current train perplexity3.347430944442749
INFO:root:current mean train loss 1533.4235732217953
INFO:root:current train perplexity3.3519771099090576
INFO:root:current mean train loss 1534.2144609871557
INFO:root:current train perplexity3.3581416606903076
INFO:root:current mean train loss 1535.2392986315745
INFO:root:current train perplexity3.360260009765625
INFO:root:current mean train loss 1535.4191104664521
INFO:root:current train perplexity3.36008620262146
INFO:root:current mean train loss 1536.724457925551
INFO:root:current train perplexity3.364307165145874
INFO:root:current mean train loss 1537.2825557983267
INFO:root:current train perplexity3.3674628734588623
INFO:root:current mean train loss 1537.7201501973964
INFO:root:current train perplexity3.371155261993408
INFO:root:current mean train loss 1537.6797529063742
INFO:root:current train perplexity3.3715882301330566
INFO:root:current mean train loss 1539.1732772715588
INFO:root:current train perplexity3.3722312450408936
INFO:root:current mean train loss 1540.723366568986
INFO:root:current train perplexity3.374464750289917
INFO:root:current mean train loss 1541.8035508215137
INFO:root:current train perplexity3.3765833377838135
INFO:root:current mean train loss 1542.681357893891
INFO:root:current train perplexity3.3777568340301514
INFO:root:current mean train loss 1543.0328456707925
INFO:root:current train perplexity3.37717866897583
INFO:root:current mean train loss 1543.774159061778
INFO:root:current train perplexity3.376377582550049
INFO:root:current mean train loss 1543.9613317117737
INFO:root:current train perplexity3.377603530883789
INFO:root:current mean train loss 1544.0742619898208
INFO:root:current train perplexity3.3781723976135254

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.89s/it]
INFO:root:final mean train loss: 1543.6679064836758
INFO:root:final train perplexity: 3.3784918785095215
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2191.204298519919
INFO:root:eval perplexity: 5.883298873901367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/97
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/200 [5:20:36<5:40:28, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1521.250249226888
INFO:root:current train perplexity3.3383872509002686
INFO:root:current mean train loss 1529.63882528769
INFO:root:current train perplexity3.3323721885681152
INFO:root:current mean train loss 1528.4315731909967
INFO:root:current train perplexity3.342564582824707
INFO:root:current mean train loss 1528.7404557151356
INFO:root:current train perplexity3.3469107151031494
INFO:root:current mean train loss 1530.538737160819
INFO:root:current train perplexity3.352559804916382
INFO:root:current mean train loss 1531.2768129223455
INFO:root:current train perplexity3.3564538955688477
INFO:root:current mean train loss 1535.4618530273438
INFO:root:current train perplexity3.360436201095581
INFO:root:current mean train loss 1536.8235024844898
INFO:root:current train perplexity3.3627586364746094
INFO:root:current mean train loss 1536.147324903956
INFO:root:current train perplexity3.361279249191284
INFO:root:current mean train loss 1537.2238587970976
INFO:root:current train perplexity3.3629345893859863
INFO:root:current mean train loss 1537.150784441533
INFO:root:current train perplexity3.362610340118408
INFO:root:current mean train loss 1538.9741746855946
INFO:root:current train perplexity3.36256742477417
INFO:root:current mean train loss 1539.1289358872634
INFO:root:current train perplexity3.3643691539764404
INFO:root:current mean train loss 1539.3570152758136
INFO:root:current train perplexity3.365595579147339
INFO:root:current mean train loss 1540.342072713441
INFO:root:current train perplexity3.3677775859832764
INFO:root:current mean train loss 1540.226038023483
INFO:root:current train perplexity3.366464376449585
INFO:root:current mean train loss 1539.4809952522944
INFO:root:current train perplexity3.3670334815979004
INFO:root:current mean train loss 1540.3775476940164
INFO:root:current train perplexity3.367835521697998
INFO:root:current mean train loss 1540.4562836353914
INFO:root:current train perplexity3.3680763244628906
INFO:root:current mean train loss 1540.4858579537706
INFO:root:current train perplexity3.3688437938690186

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.33s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.33s/it]
INFO:root:final mean train loss: 1540.4670832754684
INFO:root:final train perplexity: 3.36997389793396
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2192.3254004945147
INFO:root:eval perplexity: 5.888634204864502
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/98
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/200 [5:23:54<5:36:56, 198.20s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1510.3158747746395
INFO:root:current train perplexity3.320544481277466
INFO:root:current mean train loss 1514.6370324337122
INFO:root:current train perplexity3.321276903152466
INFO:root:current mean train loss 1525.1753542342276
INFO:root:current train perplexity3.3352372646331787
INFO:root:current mean train loss 1531.7335235177654
INFO:root:current train perplexity3.3408567905426025
INFO:root:current mean train loss 1531.1893956338206
INFO:root:current train perplexity3.340240478515625
INFO:root:current mean train loss 1531.3662971429064
INFO:root:current train perplexity3.344902515411377
INFO:root:current mean train loss 1532.2453648158482
INFO:root:current train perplexity3.3462235927581787
INFO:root:current mean train loss 1533.128853113511
INFO:root:current train perplexity3.3499112129211426
INFO:root:current mean train loss 1533.3015874785494
INFO:root:current train perplexity3.3482959270477295
INFO:root:current mean train loss 1534.0365014268943
INFO:root:current train perplexity3.3495070934295654
INFO:root:current mean train loss 1533.8993409349325
INFO:root:current train perplexity3.3537914752960205
INFO:root:current mean train loss 1533.6658652637138
INFO:root:current train perplexity3.35478138923645
INFO:root:current mean train loss 1534.7369678120367
INFO:root:current train perplexity3.3542375564575195
INFO:root:current mean train loss 1535.118109868647
INFO:root:current train perplexity3.3543624877929688
INFO:root:current mean train loss 1535.2759536482774
INFO:root:current train perplexity3.356060743331909
INFO:root:current mean train loss 1535.3992501060802
INFO:root:current train perplexity3.3572142124176025
INFO:root:current mean train loss 1536.2745082729214
INFO:root:current train perplexity3.358952522277832
INFO:root:current mean train loss 1537.2878146855082
INFO:root:current train perplexity3.358985662460327
INFO:root:current mean train loss 1537.0314267237768
INFO:root:current train perplexity3.3596091270446777
INFO:root:current mean train loss 1537.3574536194935
INFO:root:current train perplexity3.3605661392211914

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 1537.0302264376596
INFO:root:final train perplexity: 3.360851287841797
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2197.199026987062
INFO:root:eval perplexity: 5.911890029907227
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/99
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/200 [5:27:12<5:33:42, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1516.8301525581173
INFO:root:current train perplexity3.3205037117004395
INFO:root:current mean train loss 1514.4237456269316
INFO:root:current train perplexity3.3094708919525146
INFO:root:current mean train loss 1521.4018944273603
INFO:root:current train perplexity3.3230559825897217
INFO:root:current mean train loss 1519.2978569949485
INFO:root:current train perplexity3.3205132484436035
INFO:root:current mean train loss 1520.1979463822613
INFO:root:current train perplexity3.318419933319092
INFO:root:current mean train loss 1522.5860564241705
INFO:root:current train perplexity3.32334303855896
INFO:root:current mean train loss 1523.4653928874175
INFO:root:current train perplexity3.325138807296753
INFO:root:current mean train loss 1522.675679316606
INFO:root:current train perplexity3.325162172317505
INFO:root:current mean train loss 1524.1785238183816
INFO:root:current train perplexity3.326160430908203
INFO:root:current mean train loss 1524.7284933738943
INFO:root:current train perplexity3.3280677795410156
INFO:root:current mean train loss 1525.8764180238059
INFO:root:current train perplexity3.3320553302764893
INFO:root:current mean train loss 1527.1048673833081
INFO:root:current train perplexity3.333988666534424
INFO:root:current mean train loss 1528.6048547801288
INFO:root:current train perplexity3.3375864028930664
INFO:root:current mean train loss 1529.119876403367
INFO:root:current train perplexity3.3391525745391846
INFO:root:current mean train loss 1529.262905681986
INFO:root:current train perplexity3.340285301208496
INFO:root:current mean train loss 1529.6689777977097
INFO:root:current train perplexity3.3425045013427734
INFO:root:current mean train loss 1530.8282618755109
INFO:root:current train perplexity3.3441085815429688
INFO:root:current mean train loss 1531.6799397923462
INFO:root:current train perplexity3.3472347259521484
INFO:root:current mean train loss 1532.6584790480124
INFO:root:current train perplexity3.3484678268432617
INFO:root:current mean train loss 1533.2435945728353
INFO:root:current train perplexity3.3496460914611816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 1532.7854888808288
INFO:root:final train perplexity: 3.3496196269989014
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2197.783407008394
INFO:root:eval perplexity: 5.914685249328613
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/100
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 100/200 [5:30:30<5:30:26, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1515.7033272174874
INFO:root:current train perplexity3.3149197101593018
INFO:root:current mean train loss 1517.252029802332
INFO:root:current train perplexity3.317427158355713
INFO:root:current mean train loss 1514.6099616723714
INFO:root:current train perplexity3.313917398452759
INFO:root:current mean train loss 1516.876139934798
INFO:root:current train perplexity3.315671682357788
INFO:root:current mean train loss 1518.2235038925508
INFO:root:current train perplexity3.316298246383667
INFO:root:current mean train loss 1521.6833469601029
INFO:root:current train perplexity3.319239377975464
INFO:root:current mean train loss 1523.7348624080719
INFO:root:current train perplexity3.3233914375305176
INFO:root:current mean train loss 1524.6807627576463
INFO:root:current train perplexity3.323848247528076
INFO:root:current mean train loss 1524.791160914471
INFO:root:current train perplexity3.3263919353485107
INFO:root:current mean train loss 1524.7503241767158
INFO:root:current train perplexity3.3287458419799805
INFO:root:current mean train loss 1526.002957233849
INFO:root:current train perplexity3.3304033279418945
INFO:root:current mean train loss 1526.3903186534026
INFO:root:current train perplexity3.3317975997924805
INFO:root:current mean train loss 1527.2540601769992
INFO:root:current train perplexity3.3333847522735596
INFO:root:current mean train loss 1528.4256029872063
INFO:root:current train perplexity3.335017442703247
INFO:root:current mean train loss 1529.7136175907635
INFO:root:current train perplexity3.336115598678589
INFO:root:current mean train loss 1529.2741303768958
INFO:root:current train perplexity3.3371129035949707
INFO:root:current mean train loss 1529.3736486047628
INFO:root:current train perplexity3.3377163410186768
INFO:root:current mean train loss 1530.0992472353348
INFO:root:current train perplexity3.3393537998199463
INFO:root:current mean train loss 1530.644666947962
INFO:root:current train perplexity3.3411548137664795

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.14s/it]
INFO:root:final mean train loss: 1530.2585445525247
INFO:root:final train perplexity: 3.3429508209228516
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2199.53589559785
INFO:root:eval perplexity: 5.9230732917785645
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/101
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [5:33:49<5:27:22, 198.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1526.0816955566406
INFO:root:current train perplexity3.314793586730957
INFO:root:current mean train loss 1523.2856834674703
INFO:root:current train perplexity3.3274965286254883
INFO:root:current mean train loss 1520.6910886411313
INFO:root:current train perplexity3.3205738067626953
INFO:root:current mean train loss 1519.87144180491
INFO:root:current train perplexity3.3251521587371826
INFO:root:current mean train loss 1517.988310300387
INFO:root:current train perplexity3.318920850753784
INFO:root:current mean train loss 1522.6654802662458
INFO:root:current train perplexity3.3253495693206787
INFO:root:current mean train loss 1523.4700630485238
INFO:root:current train perplexity3.3219778537750244
INFO:root:current mean train loss 1522.3166133944549
INFO:root:current train perplexity3.3214540481567383
INFO:root:current mean train loss 1521.3610338697247
INFO:root:current train perplexity3.322373390197754
INFO:root:current mean train loss 1520.867996415717
INFO:root:current train perplexity3.3241050243377686
INFO:root:current mean train loss 1521.8606586606484
INFO:root:current train perplexity3.3244431018829346
INFO:root:current mean train loss 1523.6309211758303
INFO:root:current train perplexity3.328073740005493
INFO:root:current mean train loss 1524.1863695445813
INFO:root:current train perplexity3.3290140628814697
INFO:root:current mean train loss 1523.651182505135
INFO:root:current train perplexity3.330947160720825
INFO:root:current mean train loss 1524.5144268833312
INFO:root:current train perplexity3.3304224014282227
INFO:root:current mean train loss 1524.3200428341183
INFO:root:current train perplexity3.33103346824646
INFO:root:current mean train loss 1524.7504066996055
INFO:root:current train perplexity3.330454111099243
INFO:root:current mean train loss 1525.9872258682074
INFO:root:current train perplexity3.331859827041626
INFO:root:current mean train loss 1526.9116976565726
INFO:root:current train perplexity3.333848476409912
INFO:root:current mean train loss 1527.6486845076208
INFO:root:current train perplexity3.334559440612793

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 1527.085190088654
INFO:root:final train perplexity: 3.334595203399658
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2203.9478751108154
INFO:root:eval perplexity: 5.944246292114258
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/102
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/200 [5:37:07<5:24:02, 198.40s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1505.2582748875473
INFO:root:current train perplexity3.3463385105133057
INFO:root:current mean train loss 1519.8321267034773
INFO:root:current train perplexity3.2971997261047363
INFO:root:current mean train loss 1514.7228783865344
INFO:root:current train perplexity3.301210880279541
INFO:root:current mean train loss 1513.7534388636684
INFO:root:current train perplexity3.299971103668213
INFO:root:current mean train loss 1517.5725238615041
INFO:root:current train perplexity3.3025522232055664
INFO:root:current mean train loss 1519.6454207830088
INFO:root:current train perplexity3.303406238555908
INFO:root:current mean train loss 1519.245979007763
INFO:root:current train perplexity3.30625581741333
INFO:root:current mean train loss 1520.3993236338783
INFO:root:current train perplexity3.310882806777954
INFO:root:current mean train loss 1519.9987458850728
INFO:root:current train perplexity3.3105270862579346
INFO:root:current mean train loss 1519.7384382536175
INFO:root:current train perplexity3.310478925704956
INFO:root:current mean train loss 1519.8020696649246
INFO:root:current train perplexity3.3134419918060303
INFO:root:current mean train loss 1521.6049375879165
INFO:root:current train perplexity3.31642484664917
INFO:root:current mean train loss 1522.1260513095283
INFO:root:current train perplexity3.316642999649048
INFO:root:current mean train loss 1522.5334152141552
INFO:root:current train perplexity3.317631244659424
INFO:root:current mean train loss 1522.2598242323795
INFO:root:current train perplexity3.3180410861968994
INFO:root:current mean train loss 1522.700418271975
INFO:root:current train perplexity3.319216012954712
INFO:root:current mean train loss 1523.1601833102898
INFO:root:current train perplexity3.32016658782959
INFO:root:current mean train loss 1523.0400948499712
INFO:root:current train perplexity3.3229830265045166
INFO:root:current mean train loss 1524.5836576554402
INFO:root:current train perplexity3.3248040676116943
INFO:root:current mean train loss 1524.2491623690507
INFO:root:current train perplexity3.3249318599700928

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.65s/it]
INFO:root:final mean train loss: 1523.4453932646243
INFO:root:final train perplexity: 3.3250367641448975
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2206.1533428219195
INFO:root:eval perplexity: 5.9548563957214355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/103
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 103/200 [5:40:26<5:20:38, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1512.63318359375
INFO:root:current train perplexity3.313206672668457
INFO:root:current mean train loss 1508.8465470377605
INFO:root:current train perplexity3.2925381660461426
INFO:root:current mean train loss 1505.7728671875
INFO:root:current train perplexity3.286090612411499
INFO:root:current mean train loss 1508.8377835518972
INFO:root:current train perplexity3.2883496284484863
INFO:root:current mean train loss 1509.415391438802
INFO:root:current train perplexity3.2927093505859375
INFO:root:current mean train loss 1511.660146262429
INFO:root:current train perplexity3.298377752304077
INFO:root:current mean train loss 1512.2113758263222
INFO:root:current train perplexity3.3009049892425537
INFO:root:current mean train loss 1513.24633203125
INFO:root:current train perplexity3.3030388355255127
INFO:root:current mean train loss 1512.9430131261488
INFO:root:current train perplexity3.3035571575164795
INFO:root:current mean train loss 1512.8502113743832
INFO:root:current train perplexity3.302354335784912
INFO:root:current mean train loss 1514.643515625
INFO:root:current train perplexity3.3051021099090576
INFO:root:current mean train loss 1514.544343686311
INFO:root:current train perplexity3.3049325942993164
INFO:root:current mean train loss 1515.645446484375
INFO:root:current train perplexity3.3059284687042236
INFO:root:current mean train loss 1515.9163333695024
INFO:root:current train perplexity3.3054921627044678
INFO:root:current mean train loss 1517.411954724542
INFO:root:current train perplexity3.308310031890869
INFO:root:current mean train loss 1518.284931955645
INFO:root:current train perplexity3.3100385665893555
INFO:root:current mean train loss 1519.3580564186789
INFO:root:current train perplexity3.3125810623168945
INFO:root:current mean train loss 1519.7041337890626
INFO:root:current train perplexity3.314577102661133
INFO:root:current mean train loss 1519.9719366949957
INFO:root:current train perplexity3.3162295818328857
INFO:root:current mean train loss 1520.4029517853567
INFO:root:current train perplexity3.316551685333252

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1520.066261772429
INFO:root:final train perplexity: 3.3161871433258057
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2207.290532105358
INFO:root:eval perplexity: 5.960337162017822
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/104
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/200 [5:43:44<5:17:26, 198.41s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1509.5037732480178
INFO:root:current train perplexity3.3045032024383545
INFO:root:current mean train loss 1506.2278716492797
INFO:root:current train perplexity3.2987616062164307
INFO:root:current mean train loss 1508.0943905262466
INFO:root:current train perplexity3.295267105102539
INFO:root:current mean train loss 1509.424427500213
INFO:root:current train perplexity3.2942070960998535
INFO:root:current mean train loss 1513.9658976846895
INFO:root:current train perplexity3.293194532394409
INFO:root:current mean train loss 1512.8696491436563
INFO:root:current train perplexity3.293490171432495
INFO:root:current mean train loss 1513.1033046099021
INFO:root:current train perplexity3.2894623279571533
INFO:root:current mean train loss 1514.2626743043106
INFO:root:current train perplexity3.2923810482025146
INFO:root:current mean train loss 1513.809726596291
INFO:root:current train perplexity3.2926602363586426
INFO:root:current mean train loss 1513.7892979303338
INFO:root:current train perplexity3.2925941944122314
INFO:root:current mean train loss 1514.5612667123066
INFO:root:current train perplexity3.295231342315674
INFO:root:current mean train loss 1514.477330277287
INFO:root:current train perplexity3.294459104537964
INFO:root:current mean train loss 1513.9146960709352
INFO:root:current train perplexity3.2955944538116455
INFO:root:current mean train loss 1513.5521604570056
INFO:root:current train perplexity3.297217607498169
INFO:root:current mean train loss 1514.1714053579797
INFO:root:current train perplexity3.2993040084838867
INFO:root:current mean train loss 1514.7443984761387
INFO:root:current train perplexity3.3010976314544678
INFO:root:current mean train loss 1515.7805931489484
INFO:root:current train perplexity3.3020877838134766
INFO:root:current mean train loss 1515.8829459690683
INFO:root:current train perplexity3.3033835887908936
INFO:root:current mean train loss 1516.6714170565244
INFO:root:current train perplexity3.305856227874756
INFO:root:current mean train loss 1517.028476080921
INFO:root:current train perplexity3.306285858154297

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it]
INFO:root:final mean train loss: 1516.4646703399796
INFO:root:final train perplexity: 3.30678129196167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2210.8756783126937
INFO:root:eval perplexity: 5.9776434898376465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/105
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 105/200 [5:47:02<5:14:02, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1501.9094790504091
INFO:root:current train perplexity3.277311086654663
INFO:root:current mean train loss 1504.6533813476562
INFO:root:current train perplexity3.26824951171875
INFO:root:current mean train loss 1503.5331180196413
INFO:root:current train perplexity3.278958559036255
INFO:root:current mean train loss 1505.5871063868206
INFO:root:current train perplexity3.275745153427124
INFO:root:current mean train loss 1504.6308702200897
INFO:root:current train perplexity3.276639938354492
INFO:root:current mean train loss 1505.233632335924
INFO:root:current train perplexity3.275921583175659
INFO:root:current mean train loss 1505.7606011998585
INFO:root:current train perplexity3.2810442447662354
INFO:root:current mean train loss 1508.3243570133131
INFO:root:current train perplexity3.2837817668914795
INFO:root:current mean train loss 1508.618714759792
INFO:root:current train perplexity3.2849373817443848
INFO:root:current mean train loss 1509.6611800775295
INFO:root:current train perplexity3.286717414855957
INFO:root:current mean train loss 1509.8993363609172
INFO:root:current train perplexity3.288911819458008
INFO:root:current mean train loss 1511.2715269552696
INFO:root:current train perplexity3.291888952255249
INFO:root:current mean train loss 1512.3055269710742
INFO:root:current train perplexity3.2928600311279297
INFO:root:current mean train loss 1513.2774061963737
INFO:root:current train perplexity3.294658660888672
INFO:root:current mean train loss 1512.73784487331
INFO:root:current train perplexity3.2964227199554443
INFO:root:current mean train loss 1512.8498293173434
INFO:root:current train perplexity3.2981207370758057
INFO:root:current mean train loss 1513.3865713087794
INFO:root:current train perplexity3.2993760108947754
INFO:root:current mean train loss 1514.6845090036436
INFO:root:current train perplexity3.300992012023926
INFO:root:current mean train loss 1514.8946419167164
INFO:root:current train perplexity3.3012025356292725

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it]
INFO:root:final mean train loss: 1514.38342642195
INFO:root:final train perplexity: 3.3013579845428467
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2211.3804355918937
INFO:root:eval perplexity: 5.98008394241333
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/106
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/200 [5:50:21<5:10:46, 198.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1556.7476806640625
INFO:root:current train perplexity3.474317789077759
INFO:root:current mean train loss 1498.5476726871907
INFO:root:current train perplexity3.2615966796875
INFO:root:current mean train loss 1497.893259532416
INFO:root:current train perplexity3.2640697956085205
INFO:root:current mean train loss 1498.0255897496627
INFO:root:current train perplexity3.2657783031463623
INFO:root:current mean train loss 1502.4330725919576
INFO:root:current train perplexity3.268449306488037
INFO:root:current mean train loss 1502.6992350747723
INFO:root:current train perplexity3.266092300415039
INFO:root:current mean train loss 1505.1908269745736
INFO:root:current train perplexity3.27203369140625
INFO:root:current mean train loss 1504.5066564018477
INFO:root:current train perplexity3.268097162246704
INFO:root:current mean train loss 1504.9469045955739
INFO:root:current train perplexity3.2714829444885254
INFO:root:current mean train loss 1504.9703079206697
INFO:root:current train perplexity3.2745046615600586
INFO:root:current mean train loss 1504.7141507174467
INFO:root:current train perplexity3.2754979133605957
INFO:root:current mean train loss 1504.8132864166453
INFO:root:current train perplexity3.276672840118408
INFO:root:current mean train loss 1505.3048660824638
INFO:root:current train perplexity3.2787630558013916
INFO:root:current mean train loss 1505.9123048188592
INFO:root:current train perplexity3.28190016746521
INFO:root:current mean train loss 1507.0777012827054
INFO:root:current train perplexity3.2841222286224365
INFO:root:current mean train loss 1507.8649406255204
INFO:root:current train perplexity3.2844786643981934
INFO:root:current mean train loss 1508.516181597927
INFO:root:current train perplexity3.2848165035247803
INFO:root:current mean train loss 1509.8271289177321
INFO:root:current train perplexity3.2884225845336914
INFO:root:current mean train loss 1510.4688584466962
INFO:root:current train perplexity3.28908634185791
INFO:root:current mean train loss 1510.381552112786
INFO:root:current train perplexity3.2900969982147217

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it]
INFO:root:final mean train loss: 1510.0328760036482
INFO:root:final train perplexity: 3.2900502681732178
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2215.3963640361812
INFO:root:eval perplexity: 5.999537944793701
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/107
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/200 [5:53:39<5:07:17, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1512.8331773546006
INFO:root:current train perplexity3.3050217628479004
INFO:root:current mean train loss 1492.6792147361625
INFO:root:current train perplexity3.2608988285064697
INFO:root:current mean train loss 1495.9086331708716
INFO:root:current train perplexity3.2614645957946777
INFO:root:current mean train loss 1497.785310181432
INFO:root:current train perplexity3.2665507793426514
INFO:root:current mean train loss 1498.460724899073
INFO:root:current train perplexity3.2666890621185303
INFO:root:current mean train loss 1498.6032512178751
INFO:root:current train perplexity3.2656893730163574
INFO:root:current mean train loss 1499.612321674631
INFO:root:current train perplexity3.262622117996216
INFO:root:current mean train loss 1499.3316652090768
INFO:root:current train perplexity3.266801357269287
INFO:root:current mean train loss 1499.6207846942332
INFO:root:current train perplexity3.2667109966278076
INFO:root:current mean train loss 1501.7742530307478
INFO:root:current train perplexity3.2687156200408936
INFO:root:current mean train loss 1502.820075434177
INFO:root:current train perplexity3.2700870037078857
INFO:root:current mean train loss 1503.7221031120723
INFO:root:current train perplexity3.2704153060913086
INFO:root:current mean train loss 1505.0828612880362
INFO:root:current train perplexity3.2717909812927246
INFO:root:current mean train loss 1505.3397101950754
INFO:root:current train perplexity3.2739100456237793
INFO:root:current mean train loss 1506.270016862576
INFO:root:current train perplexity3.276608943939209
INFO:root:current mean train loss 1506.2497377659492
INFO:root:current train perplexity3.2782046794891357
INFO:root:current mean train loss 1506.9833012640993
INFO:root:current train perplexity3.2790415287017822
INFO:root:current mean train loss 1507.2797058602846
INFO:root:current train perplexity3.2813172340393066
INFO:root:current mean train loss 1508.1336620905743
INFO:root:current train perplexity3.283897876739502
INFO:root:current mean train loss 1508.5580206305192
INFO:root:current train perplexity3.2858617305755615

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it]
INFO:root:final mean train loss: 1508.7160404145689
INFO:root:final train perplexity: 3.286635160446167
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2216.0495207225176
INFO:root:eval perplexity: 6.0027079582214355
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/108
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 108/200 [5:56:57<5:03:58, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1478.9172293526785
INFO:root:current train perplexity3.2489755153656006
INFO:root:current mean train loss 1492.7809660734954
INFO:root:current train perplexity3.234422206878662
INFO:root:current mean train loss 1490.8426778590426
INFO:root:current train perplexity3.242560625076294
INFO:root:current mean train loss 1492.4600462045244
INFO:root:current train perplexity3.2576582431793213
INFO:root:current mean train loss 1491.1598913433909
INFO:root:current train perplexity3.2531118392944336
INFO:root:current mean train loss 1492.1717520170123
INFO:root:current train perplexity3.2528116703033447
INFO:root:current mean train loss 1496.020654873585
INFO:root:current train perplexity3.2539117336273193
INFO:root:current mean train loss 1496.0101635576104
INFO:root:current train perplexity3.256687879562378
INFO:root:current mean train loss 1497.1213121608346
INFO:root:current train perplexity3.259061574935913
INFO:root:current mean train loss 1497.2777736725018
INFO:root:current train perplexity3.2596921920776367
INFO:root:current mean train loss 1498.744788600166
INFO:root:current train perplexity3.2605538368225098
INFO:root:current mean train loss 1499.7069377882365
INFO:root:current train perplexity3.2616360187530518
INFO:root:current mean train loss 1499.917774524766
INFO:root:current train perplexity3.262619972229004
INFO:root:current mean train loss 1500.2833624107561
INFO:root:current train perplexity3.264873504638672
INFO:root:current mean train loss 1501.3129117214721
INFO:root:current train perplexity3.2662110328674316
INFO:root:current mean train loss 1501.9119143805985
INFO:root:current train perplexity3.265829086303711
INFO:root:current mean train loss 1502.21765465226
INFO:root:current train perplexity3.2698421478271484
INFO:root:current mean train loss 1503.123345683425
INFO:root:current train perplexity3.271949291229248
INFO:root:current mean train loss 1504.6214128624192
INFO:root:current train perplexity3.273463487625122
INFO:root:current mean train loss 1504.2900005173005
INFO:root:current train perplexity3.273383378982544

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it]
INFO:root:final mean train loss: 1503.9042894879917
INFO:root:final train perplexity: 3.274186372756958
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2217.945641483821
INFO:root:eval perplexity: 6.0119194984436035
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/109
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/200 [6:00:15<5:00:42, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1484.7679889385518
INFO:root:current train perplexity3.218210220336914
INFO:root:current mean train loss 1485.987299868935
INFO:root:current train perplexity3.2367353439331055
INFO:root:current mean train loss 1488.2671746148003
INFO:root:current train perplexity3.2403604984283447
INFO:root:current mean train loss 1491.930947043679
INFO:root:current train perplexity3.241804361343384
INFO:root:current mean train loss 1489.761662305984
INFO:root:current train perplexity3.242605209350586
INFO:root:current mean train loss 1492.6685114321501
INFO:root:current train perplexity3.2474164962768555
INFO:root:current mean train loss 1493.5883369679832
INFO:root:current train perplexity3.2507731914520264
INFO:root:current mean train loss 1497.7252197265625
INFO:root:current train perplexity3.2547786235809326
INFO:root:current mean train loss 1498.4158767915108
INFO:root:current train perplexity3.255492687225342
INFO:root:current mean train loss 1498.5512691465747
INFO:root:current train perplexity3.2567577362060547
INFO:root:current mean train loss 1499.1240920150235
INFO:root:current train perplexity3.258248805999756
INFO:root:current mean train loss 1498.9505455229018
INFO:root:current train perplexity3.2576675415039062
INFO:root:current mean train loss 1499.87399993994
INFO:root:current train perplexity3.2599897384643555
INFO:root:current mean train loss 1500.4587591950005
INFO:root:current train perplexity3.260504722595215
INFO:root:current mean train loss 1500.6132214759007
INFO:root:current train perplexity3.263572931289673
INFO:root:current mean train loss 1500.4805609319628
INFO:root:current train perplexity3.2644691467285156
INFO:root:current mean train loss 1501.0613786822082
INFO:root:current train perplexity3.2649753093719482
INFO:root:current mean train loss 1501.3544792279806
INFO:root:current train perplexity3.2651264667510986
INFO:root:current mean train loss 1501.9400344090636
INFO:root:current train perplexity3.267385482788086
INFO:root:current mean train loss 1502.133007925065
INFO:root:current train perplexity3.267946481704712

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it]
INFO:root:final mean train loss: 1501.5778050809813
INFO:root:final train perplexity: 3.2681844234466553
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2221.09925918038
INFO:root:eval perplexity: 6.0272722244262695
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/110
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 110/200 [6:03:34<4:57:25, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1487.0495463937953
INFO:root:current train perplexity3.257894277572632
INFO:root:current mean train loss 1483.7115153476332
INFO:root:current train perplexity3.237196683883667
INFO:root:current mean train loss 1483.4596142396608
INFO:root:current train perplexity3.239976406097412
INFO:root:current mean train loss 1487.1998959259613
INFO:root:current train perplexity3.2430825233459473
INFO:root:current mean train loss 1488.628597039912
INFO:root:current train perplexity3.2455458641052246
INFO:root:current mean train loss 1490.8143846883925
INFO:root:current train perplexity3.245612382888794
INFO:root:current mean train loss 1492.6604283080508
INFO:root:current train perplexity3.248419761657715
INFO:root:current mean train loss 1493.6480984334362
INFO:root:current train perplexity3.2500853538513184
INFO:root:current mean train loss 1493.7384926606103
INFO:root:current train perplexity3.2495031356811523
INFO:root:current mean train loss 1494.6643160887916
INFO:root:current train perplexity3.2486934661865234
INFO:root:current mean train loss 1495.9286530425998
INFO:root:current train perplexity3.250509262084961
INFO:root:current mean train loss 1497.5278902992006
INFO:root:current train perplexity3.2520647048950195
INFO:root:current mean train loss 1497.356948119059
INFO:root:current train perplexity3.2514641284942627
INFO:root:current mean train loss 1497.7544484775897
INFO:root:current train perplexity3.2528839111328125
INFO:root:current mean train loss 1497.1418506889784
INFO:root:current train perplexity3.253788709640503
INFO:root:current mean train loss 1497.6401309614503
INFO:root:current train perplexity3.255047082901001
INFO:root:current mean train loss 1498.5739342362099
INFO:root:current train perplexity3.2580325603485107
INFO:root:current mean train loss 1499.0503020084395
INFO:root:current train perplexity3.2593603134155273
INFO:root:current mean train loss 1499.1849571623989
INFO:root:current train perplexity3.260272741317749
INFO:root:current mean train loss 1499.5317011455886
INFO:root:current train perplexity3.2614388465881348

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.42s/it]
INFO:root:final mean train loss: 1499.1664324677238
INFO:root:final train perplexity: 3.261975049972534
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2222.903416843279
INFO:root:eval perplexity: 6.036073207855225
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/111
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [6:06:52<4:53:58, 198.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1473.5688675281613
INFO:root:current train perplexity3.2169430255889893
INFO:root:current mean train loss 1478.4742339759744
INFO:root:current train perplexity3.2299017906188965
INFO:root:current mean train loss 1484.5778065928212
INFO:root:current train perplexity3.2320783138275146
INFO:root:current mean train loss 1486.4987321764695
INFO:root:current train perplexity3.2346391677856445
INFO:root:current mean train loss 1488.2231553317097
INFO:root:current train perplexity3.2356817722320557
INFO:root:current mean train loss 1490.5567843596682
INFO:root:current train perplexity3.237593173980713
INFO:root:current mean train loss 1491.596280200836
INFO:root:current train perplexity3.240313768386841
INFO:root:current mean train loss 1491.266001150505
INFO:root:current train perplexity3.242525100708008
INFO:root:current mean train loss 1492.9624053748412
INFO:root:current train perplexity3.2432751655578613
INFO:root:current mean train loss 1491.9002357467434
INFO:root:current train perplexity3.2431328296661377
INFO:root:current mean train loss 1490.9483180599318
INFO:root:current train perplexity3.2442824840545654
INFO:root:current mean train loss 1492.084334014841
INFO:root:current train perplexity3.246426820755005
INFO:root:current mean train loss 1493.0297421563655
INFO:root:current train perplexity3.24639630317688
INFO:root:current mean train loss 1493.9807471513404
INFO:root:current train perplexity3.245370864868164
INFO:root:current mean train loss 1494.1435582198174
INFO:root:current train perplexity3.24739408493042
INFO:root:current mean train loss 1494.3804920095513
INFO:root:current train perplexity3.2492854595184326
INFO:root:current mean train loss 1494.4734315340488
INFO:root:current train perplexity3.2481751441955566
INFO:root:current mean train loss 1494.6983943994742
INFO:root:current train perplexity3.251129388809204
INFO:root:current mean train loss 1495.433313946203
INFO:root:current train perplexity3.250885486602783

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.73s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.73s/it]
INFO:root:final mean train loss: 1495.6755295684707
INFO:root:final train perplexity: 3.25300669670105
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2225.349519337323
INFO:root:eval perplexity: 6.048027038574219
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/112
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/200 [6:10:10<4:50:42, 198.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1480.2953287760417
INFO:root:current train perplexity3.3525490760803223
INFO:root:current mean train loss 1481.3112555939017
INFO:root:current train perplexity3.2219064235687256
INFO:root:current mean train loss 1480.783168247768
INFO:root:current train perplexity3.223346710205078
INFO:root:current mean train loss 1481.5211044664036
INFO:root:current train perplexity3.22281813621521
INFO:root:current mean train loss 1480.7931719985847
INFO:root:current train perplexity3.224130630493164
INFO:root:current mean train loss 1482.8324843419948
INFO:root:current train perplexity3.2223544120788574
INFO:root:current mean train loss 1485.7240293891869
INFO:root:current train perplexity3.22438907623291
INFO:root:current mean train loss 1486.3797973806454
INFO:root:current train perplexity3.224055767059326
INFO:root:current mean train loss 1488.959730630497
INFO:root:current train perplexity3.2273569107055664
INFO:root:current mean train loss 1488.5392633820425
INFO:root:current train perplexity3.229032039642334
INFO:root:current mean train loss 1487.7782345103362
INFO:root:current train perplexity3.2295215129852295
INFO:root:current mean train loss 1488.5372923144619
INFO:root:current train perplexity3.233091115951538
INFO:root:current mean train loss 1489.3803807335503
INFO:root:current train perplexity3.2370331287384033
INFO:root:current mean train loss 1490.0100406813604
INFO:root:current train perplexity3.2375032901763916
INFO:root:current mean train loss 1490.478748715782
INFO:root:current train perplexity3.239332437515259
INFO:root:current mean train loss 1491.531860107909
INFO:root:current train perplexity3.2402515411376953
INFO:root:current mean train loss 1491.658231833988
INFO:root:current train perplexity3.240232229232788
INFO:root:current mean train loss 1492.2957354381515
INFO:root:current train perplexity3.242734909057617
INFO:root:current mean train loss 1492.8655833891214
INFO:root:current train perplexity3.244739294052124
INFO:root:current mean train loss 1493.308940781209
INFO:root:current train perplexity3.246671438217163

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it]
INFO:root:final mean train loss: 1493.334857734838
INFO:root:final train perplexity: 3.247007131576538
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2228.152914277205
INFO:root:eval perplexity: 6.06175422668457
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/113
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 113/200 [6:13:28<4:47:22, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1454.608251953125
INFO:root:current train perplexity3.148405075073242
INFO:root:current mean train loss 1483.7824198404949
INFO:root:current train perplexity3.2123167514801025
INFO:root:current mean train loss 1486.2852322665128
INFO:root:current train perplexity3.219196081161499
INFO:root:current mean train loss 1482.7251926422118
INFO:root:current train perplexity3.2117440700531006
INFO:root:current mean train loss 1486.0753345307849
INFO:root:current train perplexity3.213809013366699
INFO:root:current mean train loss 1487.0372474083533
INFO:root:current train perplexity3.218219518661499
INFO:root:current mean train loss 1487.8490114273563
INFO:root:current train perplexity3.218449354171753
INFO:root:current mean train loss 1488.4508480495876
INFO:root:current train perplexity3.222316265106201
INFO:root:current mean train loss 1487.8824038621856
INFO:root:current train perplexity3.223886251449585
INFO:root:current mean train loss 1488.7062462848166
INFO:root:current train perplexity3.225590705871582
INFO:root:current mean train loss 1489.285555970435
INFO:root:current train perplexity3.227036476135254
INFO:root:current mean train loss 1489.6151311601911
INFO:root:current train perplexity3.2289986610412598
INFO:root:current mean train loss 1489.29745713531
INFO:root:current train perplexity3.228991746902466
INFO:root:current mean train loss 1489.7712461529356
INFO:root:current train perplexity3.2307982444763184
INFO:root:current mean train loss 1489.972947413485
INFO:root:current train perplexity3.231783151626587
INFO:root:current mean train loss 1490.402112539191
INFO:root:current train perplexity3.23386549949646
INFO:root:current mean train loss 1490.476641619647
INFO:root:current train perplexity3.2344954013824463
INFO:root:current mean train loss 1490.6994121462799
INFO:root:current train perplexity3.2360544204711914
INFO:root:current mean train loss 1490.5309777563746
INFO:root:current train perplexity3.236953020095825
INFO:root:current mean train loss 1490.397918510437
INFO:root:current train perplexity3.2384111881256104

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it]
INFO:root:final mean train loss: 1490.0868553357839
INFO:root:final train perplexity: 3.2387001514434814
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2230.307440575133
INFO:root:eval perplexity: 6.072325706481934
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/114
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/200 [6:16:46<4:44:04, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1464.6653326910896
INFO:root:current train perplexity3.1943588256835938
INFO:root:current mean train loss 1476.4822035740763
INFO:root:current train perplexity3.2134592533111572
INFO:root:current mean train loss 1478.5157573716046
INFO:root:current train perplexity3.2216053009033203
INFO:root:current mean train loss 1476.074261854947
INFO:root:current train perplexity3.214487314224243
INFO:root:current mean train loss 1477.9999659208738
INFO:root:current train perplexity3.2134687900543213
INFO:root:current mean train loss 1476.6205318173882
INFO:root:current train perplexity3.2128164768218994
INFO:root:current mean train loss 1478.6807796172857
INFO:root:current train perplexity3.2127315998077393
INFO:root:current mean train loss 1480.414593514141
INFO:root:current train perplexity3.2180569171905518
INFO:root:current mean train loss 1481.2110069211096
INFO:root:current train perplexity3.218034505844116
INFO:root:current mean train loss 1481.7944018059632
INFO:root:current train perplexity3.217278242111206
INFO:root:current mean train loss 1481.3197241611167
INFO:root:current train perplexity3.2190520763397217
INFO:root:current mean train loss 1482.4204621193383
INFO:root:current train perplexity3.220409631729126
INFO:root:current mean train loss 1483.5909368565897
INFO:root:current train perplexity3.221759080886841
INFO:root:current mean train loss 1484.2934413273654
INFO:root:current train perplexity3.223231315612793
INFO:root:current mean train loss 1484.1627971142136
INFO:root:current train perplexity3.222198486328125
INFO:root:current mean train loss 1485.0560591827373
INFO:root:current train perplexity3.223658323287964
INFO:root:current mean train loss 1485.9019316788047
INFO:root:current train perplexity3.2257206439971924
INFO:root:current mean train loss 1486.816690377964
INFO:root:current train perplexity3.226710319519043
INFO:root:current mean train loss 1487.2035284234444
INFO:root:current train perplexity3.2299604415893555
INFO:root:current mean train loss 1488.0633681675915
INFO:root:current train perplexity3.232370138168335

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it]
INFO:root:final mean train loss: 1487.6497916925211
INFO:root:final train perplexity: 3.2324814796447754
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2229.5493835016346
INFO:root:eval perplexity: 6.068604946136475
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/115
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 115/200 [6:20:04<4:40:45, 198.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1482.8079359266494
INFO:root:current train perplexity3.182253360748291
INFO:root:current mean train loss 1479.6846646395597
INFO:root:current train perplexity3.1873912811279297
INFO:root:current mean train loss 1481.5759469580462
INFO:root:current train perplexity3.1956050395965576
INFO:root:current mean train loss 1484.8835121628929
INFO:root:current train perplexity3.2073612213134766
INFO:root:current mean train loss 1483.480813988505
INFO:root:current train perplexity3.21199107170105
INFO:root:current mean train loss 1482.702382001636
INFO:root:current train perplexity3.2092015743255615
INFO:root:current mean train loss 1483.5205339437596
INFO:root:current train perplexity3.211001396179199
INFO:root:current mean train loss 1481.6720776950017
INFO:root:current train perplexity3.210681676864624
INFO:root:current mean train loss 1481.0891370572306
INFO:root:current train perplexity3.2118515968322754
INFO:root:current mean train loss 1481.190744076135
INFO:root:current train perplexity3.213099241256714
INFO:root:current mean train loss 1481.1862352867054
INFO:root:current train perplexity3.215449333190918
INFO:root:current mean train loss 1482.0525985532659
INFO:root:current train perplexity3.2141616344451904
INFO:root:current mean train loss 1482.5722453772926
INFO:root:current train perplexity3.216193437576294
INFO:root:current mean train loss 1482.943836296679
INFO:root:current train perplexity3.2186477184295654
INFO:root:current mean train loss 1482.3696809582402
INFO:root:current train perplexity3.2201435565948486
INFO:root:current mean train loss 1482.7777340765012
INFO:root:current train perplexity3.2207186222076416
INFO:root:current mean train loss 1483.304066889808
INFO:root:current train perplexity3.222917079925537
INFO:root:current mean train loss 1483.799522826174
INFO:root:current train perplexity3.2241060733795166
INFO:root:current mean train loss 1483.9415445831858
INFO:root:current train perplexity3.224003791809082
INFO:root:current mean train loss 1484.7904088343423
INFO:root:current train perplexity3.2241621017456055

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.78s/it]
INFO:root:final mean train loss: 1484.635819843425
INFO:root:final train perplexity: 3.2248072624206543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2232.666817306627
INFO:root:eval perplexity: 6.08392333984375
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/116
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/200 [6:23:23<4:37:31, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1473.892352896677
INFO:root:current train perplexity3.17673397064209
INFO:root:current mean train loss 1474.9303763763248
INFO:root:current train perplexity3.1999504566192627
INFO:root:current mean train loss 1473.972849040014
INFO:root:current train perplexity3.201613187789917
INFO:root:current mean train loss 1474.1212171364346
INFO:root:current train perplexity3.204373836517334
INFO:root:current mean train loss 1473.5974108135117
INFO:root:current train perplexity3.202026128768921
INFO:root:current mean train loss 1474.6232057160546
INFO:root:current train perplexity3.201230764389038
INFO:root:current mean train loss 1475.8630070920851
INFO:root:current train perplexity3.1995925903320312
INFO:root:current mean train loss 1475.810478794281
INFO:root:current train perplexity3.2021703720092773
INFO:root:current mean train loss 1477.3677996735075
INFO:root:current train perplexity3.2025110721588135
INFO:root:current mean train loss 1478.1205662453335
INFO:root:current train perplexity3.2037291526794434
INFO:root:current mean train loss 1478.2068068275487
INFO:root:current train perplexity3.20717191696167
INFO:root:current mean train loss 1479.1595343272977
INFO:root:current train perplexity3.2088394165039062
INFO:root:current mean train loss 1478.802835123826
INFO:root:current train perplexity3.2093098163604736
INFO:root:current mean train loss 1479.2780978970072
INFO:root:current train perplexity3.209789276123047
INFO:root:current mean train loss 1479.9379179933135
INFO:root:current train perplexity3.2109358310699463
INFO:root:current mean train loss 1480.7277190210257
INFO:root:current train perplexity3.2132973670959473
INFO:root:current mean train loss 1480.4523396152426
INFO:root:current train perplexity3.2137105464935303
INFO:root:current mean train loss 1480.615853204625
INFO:root:current train perplexity3.2146689891815186
INFO:root:current mean train loss 1480.810712005925
INFO:root:current train perplexity3.214383125305176
INFO:root:current mean train loss 1481.8425800201555
INFO:root:current train perplexity3.2164154052734375

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it]
INFO:root:final mean train loss: 1481.2744402863796
INFO:root:final train perplexity: 3.2162692546844482
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2235.605609866744
INFO:root:eval perplexity: 6.098400115966797
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/117
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/200 [6:26:41<4:34:11, 198.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1475.6784459894354
INFO:root:current train perplexity3.1975278854370117
INFO:root:current mean train loss 1470.0552855146693
INFO:root:current train perplexity3.188370943069458
INFO:root:current mean train loss 1471.8180185953777
INFO:root:current train perplexity3.1892640590667725
INFO:root:current mean train loss 1469.2327758159834
INFO:root:current train perplexity3.186947822570801
INFO:root:current mean train loss 1470.692466860912
INFO:root:current train perplexity3.1911604404449463
INFO:root:current mean train loss 1470.0284386459662
INFO:root:current train perplexity3.1930627822875977
INFO:root:current mean train loss 1472.3432638478835
INFO:root:current train perplexity3.19814395904541
INFO:root:current mean train loss 1472.6710657419892
INFO:root:current train perplexity3.199089527130127
INFO:root:current mean train loss 1473.0646760957736
INFO:root:current train perplexity3.1978793144226074
INFO:root:current mean train loss 1472.9234835358284
INFO:root:current train perplexity3.1975903511047363
INFO:root:current mean train loss 1473.5655034009148
INFO:root:current train perplexity3.201151132583618
INFO:root:current mean train loss 1475.446485217573
INFO:root:current train perplexity3.202305316925049
INFO:root:current mean train loss 1475.7543952894507
INFO:root:current train perplexity3.203378915786743
INFO:root:current mean train loss 1476.771786912374
INFO:root:current train perplexity3.2058420181274414
INFO:root:current mean train loss 1477.2554202336137
INFO:root:current train perplexity3.2067904472351074
INFO:root:current mean train loss 1477.3244625062725
INFO:root:current train perplexity3.2083799839019775
INFO:root:current mean train loss 1477.99155183765
INFO:root:current train perplexity3.208894729614258
INFO:root:current mean train loss 1478.5253867334968
INFO:root:current train perplexity3.209239959716797
INFO:root:current mean train loss 1478.8444851374222
INFO:root:current train perplexity3.210054874420166

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it]
INFO:root:final mean train loss: 1479.1953018811755
INFO:root:final train perplexity: 3.2110002040863037
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2237.253857768174
INFO:root:eval perplexity: 6.1065354347229
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/118
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 118/200 [6:29:59<4:30:46, 198.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1482.1436279296875
INFO:root:current train perplexity3.2303225994110107
INFO:root:current mean train loss 1459.8472749255952
INFO:root:current train perplexity3.179738759994507
INFO:root:current mean train loss 1467.0134313071646
INFO:root:current train perplexity3.1826581954956055
INFO:root:current mean train loss 1468.7155789734886
INFO:root:current train perplexity3.182729959487915
INFO:root:current mean train loss 1469.931125518422
INFO:root:current train perplexity3.1801092624664307
INFO:root:current mean train loss 1469.683460802135
INFO:root:current train perplexity3.179896831512451
INFO:root:current mean train loss 1472.4976665402246
INFO:root:current train perplexity3.184974431991577
INFO:root:current mean train loss 1473.1765311599622
INFO:root:current train perplexity3.1854069232940674
INFO:root:current mean train loss 1473.4213430463897
INFO:root:current train perplexity3.1871557235717773
INFO:root:current mean train loss 1473.3361623521669
INFO:root:current train perplexity3.1881775856018066
INFO:root:current mean train loss 1473.7763749611318
INFO:root:current train perplexity3.1906216144561768
INFO:root:current mean train loss 1474.5057366418623
INFO:root:current train perplexity3.193035125732422
INFO:root:current mean train loss 1475.0591334932571
INFO:root:current train perplexity3.1939444541931152
INFO:root:current mean train loss 1475.3044697377873
INFO:root:current train perplexity3.1949970722198486
INFO:root:current mean train loss 1476.2414299690001
INFO:root:current train perplexity3.1968612670898438
INFO:root:current mean train loss 1476.4776106013808
INFO:root:current train perplexity3.1970019340515137
INFO:root:current mean train loss 1475.971015335986
INFO:root:current train perplexity3.1986794471740723
INFO:root:current mean train loss 1476.3680733510128
INFO:root:current train perplexity3.201916217803955
INFO:root:current mean train loss 1476.2242146246322
INFO:root:current train perplexity3.2027297019958496
INFO:root:current mean train loss 1476.6410253649935
INFO:root:current train perplexity3.203190326690674

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.86s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it]
INFO:root:final mean train loss: 1476.2428638421702
INFO:root:final train perplexity: 3.2035317420959473
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2239.2356355274824
INFO:root:eval perplexity: 6.116329193115234
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/119
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/200 [6:33:17<4:27:35, 198.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1490.459883256392
INFO:root:current train perplexity3.1690897941589355
INFO:root:current mean train loss 1477.0256767898309
INFO:root:current train perplexity3.176018476486206
INFO:root:current mean train loss 1475.612362973325
INFO:root:current train perplexity3.176600694656372
INFO:root:current mean train loss 1470.5948126182793
INFO:root:current train perplexity3.1766648292541504
INFO:root:current mean train loss 1468.9947726715232
INFO:root:current train perplexity3.1759350299835205
INFO:root:current mean train loss 1468.7741081851652
INFO:root:current train perplexity3.1782760620117188
INFO:root:current mean train loss 1471.3812628742967
INFO:root:current train perplexity3.1795663833618164
INFO:root:current mean train loss 1472.0444613216325
INFO:root:current train perplexity3.177934408187866
INFO:root:current mean train loss 1472.4183404555865
INFO:root:current train perplexity3.182816505432129
INFO:root:current mean train loss 1471.4047850238528
INFO:root:current train perplexity3.1841423511505127
INFO:root:current mean train loss 1471.9369343199608
INFO:root:current train perplexity3.1861023902893066
INFO:root:current mean train loss 1472.6351206298393
INFO:root:current train perplexity3.1869277954101562
INFO:root:current mean train loss 1473.9599472520395
INFO:root:current train perplexity3.18924617767334
INFO:root:current mean train loss 1474.0229063741017
INFO:root:current train perplexity3.1925394535064697
INFO:root:current mean train loss 1474.0955388193895
INFO:root:current train perplexity3.193117618560791
INFO:root:current mean train loss 1473.2166069522011
INFO:root:current train perplexity3.193458318710327
INFO:root:current mean train loss 1474.191598010269
INFO:root:current train perplexity3.193373441696167
INFO:root:current mean train loss 1474.478874747069
INFO:root:current train perplexity3.1945412158966064
INFO:root:current mean train loss 1473.9409867756715
INFO:root:current train perplexity3.195626974105835
INFO:root:current mean train loss 1474.8626660080035
INFO:root:current train perplexity3.1977908611297607

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.34s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.34s/it]
INFO:root:final mean train loss: 1474.1951945848796
INFO:root:final train perplexity: 3.1983630657196045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2241.874665821698
INFO:root:eval perplexity: 6.129397869110107
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/120
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 120/200 [6:36:35<4:24:08, 198.11s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1445.2012720352564
INFO:root:current train perplexity3.1934926509857178
INFO:root:current mean train loss 1457.9823841122416
INFO:root:current train perplexity3.1655473709106445
INFO:root:current mean train loss 1458.6113638778113
INFO:root:current train perplexity3.17101788520813
INFO:root:current mean train loss 1461.3490224292495
INFO:root:current train perplexity3.1738901138305664
INFO:root:current mean train loss 1459.9128448555844
INFO:root:current train perplexity3.173384666442871
INFO:root:current mean train loss 1461.6306362965995
INFO:root:current train perplexity3.174105405807495
INFO:root:current mean train loss 1462.2985971656763
INFO:root:current train perplexity3.176636219024658
INFO:root:current mean train loss 1465.3094043034823
INFO:root:current train perplexity3.1792097091674805
INFO:root:current mean train loss 1467.241615413625
INFO:root:current train perplexity3.181946039199829
INFO:root:current mean train loss 1466.2770049691328
INFO:root:current train perplexity3.1830856800079346
INFO:root:current mean train loss 1466.4335250193622
INFO:root:current train perplexity3.183987855911255
INFO:root:current mean train loss 1466.1009457180435
INFO:root:current train perplexity3.184587001800537
INFO:root:current mean train loss 1465.8303459112062
INFO:root:current train perplexity3.1858410835266113
INFO:root:current mean train loss 1466.6017276003256
INFO:root:current train perplexity3.18737530708313
INFO:root:current mean train loss 1467.3929764864924
INFO:root:current train perplexity3.188310146331787
INFO:root:current mean train loss 1467.8367934992284
INFO:root:current train perplexity3.1889944076538086
INFO:root:current mean train loss 1468.9048174799325
INFO:root:current train perplexity3.1892635822296143
INFO:root:current mean train loss 1469.8079136239173
INFO:root:current train perplexity3.1897213459014893
INFO:root:current mean train loss 1470.4225134961043
INFO:root:current train perplexity3.19002628326416
INFO:root:current mean train loss 1470.849688320937
INFO:root:current train perplexity3.190624475479126

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.63s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.63s/it]
INFO:root:final mean train loss: 1471.2208903548817
INFO:root:final train perplexity: 3.190869092941284
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2244.169195946227
INFO:root:eval perplexity: 6.140782833099365
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/121
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [6:39:53<4:20:52, 198.13s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1450.5641305106026
INFO:root:current train perplexity3.1608784198760986
INFO:root:current mean train loss 1459.6078694661458
INFO:root:current train perplexity3.1680290699005127
INFO:root:current mean train loss 1454.8251156806946
INFO:root:current train perplexity3.166304111480713
INFO:root:current mean train loss 1459.833765265647
INFO:root:current train perplexity3.1672515869140625
INFO:root:current mean train loss 1461.7116179884526
INFO:root:current train perplexity3.1711199283599854
INFO:root:current mean train loss 1461.477123891707
INFO:root:current train perplexity3.173417329788208
INFO:root:current mean train loss 1463.7411474832675
INFO:root:current train perplexity3.1745717525482178
INFO:root:current mean train loss 1464.8863321940105
INFO:root:current train perplexity3.175877809524536
INFO:root:current mean train loss 1464.9081033011464
INFO:root:current train perplexity3.176513910293579
INFO:root:current mean train loss 1464.7803522213733
INFO:root:current train perplexity3.175938844680786
INFO:root:current mean train loss 1465.4731430284905
INFO:root:current train perplexity3.1746504306793213
INFO:root:current mean train loss 1465.8552808926593
INFO:root:current train perplexity3.1743717193603516
INFO:root:current mean train loss 1466.4962435193881
INFO:root:current train perplexity3.1762678623199463
INFO:root:current mean train loss 1466.9977924718266
INFO:root:current train perplexity3.1767163276672363
INFO:root:current mean train loss 1466.7836296165383
INFO:root:current train perplexity3.1788856983184814
INFO:root:current mean train loss 1467.5509495281622
INFO:root:current train perplexity3.1817970275878906
INFO:root:current mean train loss 1467.9456088301065
INFO:root:current train perplexity3.1818103790283203
INFO:root:current mean train loss 1468.256829750565
INFO:root:current train perplexity3.1834683418273926
INFO:root:current mean train loss 1468.6253080696895
INFO:root:current train perplexity3.183880567550659
INFO:root:current mean train loss 1469.515194820724
INFO:root:current train perplexity3.185931444168091

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.38s/it]
INFO:root:final mean train loss: 1469.244129913838
INFO:root:final train perplexity: 3.1858983039855957
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2245.7489563421154
INFO:root:eval perplexity: 6.148634910583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/122
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/200 [6:43:11<4:17:29, 198.06s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1466.890161801691
INFO:root:current train perplexity3.1553351879119873
INFO:root:current mean train loss 1463.424143532108
INFO:root:current train perplexity3.1611809730529785
INFO:root:current mean train loss 1461.869585533282
INFO:root:current train perplexity3.161191940307617
INFO:root:current mean train loss 1461.3795735458905
INFO:root:current train perplexity3.1601200103759766
INFO:root:current mean train loss 1460.8203052738504
INFO:root:current train perplexity3.1593406200408936
INFO:root:current mean train loss 1460.2889504424356
INFO:root:current train perplexity3.1622374057769775
INFO:root:current mean train loss 1459.5390416410312
INFO:root:current train perplexity3.1635701656341553
INFO:root:current mean train loss 1458.782920136643
INFO:root:current train perplexity3.16428279876709
INFO:root:current mean train loss 1460.7586488144777
INFO:root:current train perplexity3.165731906890869
INFO:root:current mean train loss 1461.3227306965812
INFO:root:current train perplexity3.168090343475342
INFO:root:current mean train loss 1461.5505456417827
INFO:root:current train perplexity3.1680116653442383
INFO:root:current mean train loss 1461.9107814831095
INFO:root:current train perplexity3.1685073375701904
INFO:root:current mean train loss 1462.3672867480545
INFO:root:current train perplexity3.1701138019561768
INFO:root:current mean train loss 1463.420024118889
INFO:root:current train perplexity3.1712632179260254
INFO:root:current mean train loss 1464.0205576185135
INFO:root:current train perplexity3.172499895095825
INFO:root:current mean train loss 1464.5460732781955
INFO:root:current train perplexity3.1752634048461914
INFO:root:current mean train loss 1465.2500070046324
INFO:root:current train perplexity3.1755383014678955
INFO:root:current mean train loss 1465.4597203082037
INFO:root:current train perplexity3.1769113540649414
INFO:root:current mean train loss 1466.1398685942088
INFO:root:current train perplexity3.1772449016571045
INFO:root:current mean train loss 1466.240841014338
INFO:root:current train perplexity3.1772074699401855

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.68s/it]
INFO:root:final mean train loss: 1465.852693020065
INFO:root:final train perplexity: 3.1773881912231445
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2247.2192348182625
INFO:root:eval perplexity: 6.155949115753174
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/123
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 123/200 [6:46:29<4:14:15, 198.12s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1457.0085055881077
INFO:root:current train perplexity3.165245771408081
INFO:root:current mean train loss 1453.220796926398
INFO:root:current train perplexity3.1560070514678955
INFO:root:current mean train loss 1455.5459232724945
INFO:root:current train perplexity3.155156373977661
INFO:root:current mean train loss 1455.2601024138621
INFO:root:current train perplexity3.1540496349334717
INFO:root:current mean train loss 1455.5105187240913
INFO:root:current train perplexity3.1554553508758545
INFO:root:current mean train loss 1455.6474677651615
INFO:root:current train perplexity3.1570723056793213
INFO:root:current mean train loss 1454.270167077106
INFO:root:current train perplexity3.156404733657837
INFO:root:current mean train loss 1455.4903782325455
INFO:root:current train perplexity3.1559598445892334
INFO:root:current mean train loss 1458.5367149095857
INFO:root:current train perplexity3.1593828201293945
INFO:root:current mean train loss 1458.8826604669744
INFO:root:current train perplexity3.1604909896850586
INFO:root:current mean train loss 1459.0830379381093
INFO:root:current train perplexity3.160677194595337
INFO:root:current mean train loss 1459.8507510914521
INFO:root:current train perplexity3.163637399673462
INFO:root:current mean train loss 1460.221548343629
INFO:root:current train perplexity3.164469003677368
INFO:root:current mean train loss 1461.1523099391582
INFO:root:current train perplexity3.1662962436676025
INFO:root:current mean train loss 1461.4381325536126
INFO:root:current train perplexity3.1672534942626953
INFO:root:current mean train loss 1461.711642897504
INFO:root:current train perplexity3.1665894985198975
INFO:root:current mean train loss 1462.7981464092547
INFO:root:current train perplexity3.1674396991729736
INFO:root:current mean train loss 1463.5285703861514
INFO:root:current train perplexity3.168789863586426
INFO:root:current mean train loss 1463.6848844013516
INFO:root:current train perplexity3.1707983016967773

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.87s/it]
INFO:root:final mean train loss: 1463.6097281297768
INFO:root:final train perplexity: 3.1717727184295654
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2248.4635213216147
INFO:root:eval perplexity: 6.16214656829834
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/124
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/200 [6:49:48<4:11:04, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.182163783482
INFO:root:current train perplexity3.0452513694763184
INFO:root:current mean train loss 1457.225801557024
INFO:root:current train perplexity3.1342122554779053
INFO:root:current mean train loss 1452.1285011180933
INFO:root:current train perplexity3.135303020477295
INFO:root:current mean train loss 1455.7085867518322
INFO:root:current train perplexity3.1361706256866455
INFO:root:current mean train loss 1455.5558944064803
INFO:root:current train perplexity3.139591932296753
INFO:root:current mean train loss 1456.9492577547153
INFO:root:current train perplexity3.1433868408203125
INFO:root:current mean train loss 1455.6388335628603
INFO:root:current train perplexity3.1470162868499756
INFO:root:current mean train loss 1457.3408522545196
INFO:root:current train perplexity3.1479175090789795
INFO:root:current mean train loss 1456.8963825741075
INFO:root:current train perplexity3.1488776206970215
INFO:root:current mean train loss 1457.6865721579554
INFO:root:current train perplexity3.151226758956909
INFO:root:current mean train loss 1458.8455523251303
INFO:root:current train perplexity3.153129816055298
INFO:root:current mean train loss 1459.2525518980776
INFO:root:current train perplexity3.1556034088134766
INFO:root:current mean train loss 1460.4639650864747
INFO:root:current train perplexity3.1576130390167236
INFO:root:current mean train loss 1460.5963974407637
INFO:root:current train perplexity3.1568167209625244
INFO:root:current mean train loss 1460.4464399368949
INFO:root:current train perplexity3.159029483795166
INFO:root:current mean train loss 1459.8283171372138
INFO:root:current train perplexity3.160564422607422
INFO:root:current mean train loss 1459.8548152522412
INFO:root:current train perplexity3.162088394165039
INFO:root:current mean train loss 1460.3501914079663
INFO:root:current train perplexity3.162996292114258
INFO:root:current mean train loss 1460.9906293639967
INFO:root:current train perplexity3.1641392707824707
INFO:root:current mean train loss 1461.1501703607391
INFO:root:current train perplexity3.1653850078582764

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it]
INFO:root:final mean train loss: 1461.1280270113346
INFO:root:final train perplexity: 3.1655712127685547
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2251.569542418135
INFO:root:eval perplexity: 6.177645683288574
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/125
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 125/200 [6:53:06<4:07:48, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1450.1608632405598
INFO:root:current train perplexity3.191084146499634
INFO:root:current mean train loss 1450.379849341608
INFO:root:current train perplexity3.1425952911376953
INFO:root:current mean train loss 1451.0081062316895
INFO:root:current train perplexity3.1388137340545654
INFO:root:current mean train loss 1451.108388641734
INFO:root:current train perplexity3.137050151824951
INFO:root:current mean train loss 1452.291539030255
INFO:root:current train perplexity3.1382594108581543
INFO:root:current mean train loss 1449.6774054374403
INFO:root:current train perplexity3.136847734451294
INFO:root:current mean train loss 1451.0269178732847
INFO:root:current train perplexity3.140272617340088
INFO:root:current mean train loss 1452.537087287692
INFO:root:current train perplexity3.1436567306518555
INFO:root:current mean train loss 1453.2452925895025
INFO:root:current train perplexity3.1450486183166504
INFO:root:current mean train loss 1454.178721638469
INFO:root:current train perplexity3.1472256183624268
INFO:root:current mean train loss 1454.4942294359207
INFO:root:current train perplexity3.1497700214385986
INFO:root:current mean train loss 1455.0348257696078
INFO:root:current train perplexity3.150923013687134
INFO:root:current mean train loss 1456.1593178144467
INFO:root:current train perplexity3.1516592502593994
INFO:root:current mean train loss 1456.9058071724237
INFO:root:current train perplexity3.1538124084472656
INFO:root:current mean train loss 1458.2631557336015
INFO:root:current train perplexity3.1546409130096436
INFO:root:current mean train loss 1459.5442094777825
INFO:root:current train perplexity3.156407594680786
INFO:root:current mean train loss 1458.7663241231382
INFO:root:current train perplexity3.1578612327575684
INFO:root:current mean train loss 1459.690436201693
INFO:root:current train perplexity3.1587727069854736
INFO:root:current mean train loss 1460.0761337280273
INFO:root:current train perplexity3.1604762077331543
INFO:root:current mean train loss 1460.1589507739411
INFO:root:current train perplexity3.161506414413452

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it]
INFO:root:final mean train loss: 1459.5585793453338
INFO:root:final train perplexity: 3.1616549491882324
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2253.207865830009
INFO:root:eval perplexity: 6.1858367919921875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/126
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/200 [6:56:24<4:04:30, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1442.316415181974
INFO:root:current train perplexity3.1112618446350098
INFO:root:current mean train loss 1448.1169061322585
INFO:root:current train perplexity3.1249427795410156
INFO:root:current mean train loss 1447.7887190822744
INFO:root:current train perplexity3.1378698348999023
INFO:root:current mean train loss 1449.656111104747
INFO:root:current train perplexity3.1397995948791504
INFO:root:current mean train loss 1451.8291630128615
INFO:root:current train perplexity3.1393961906433105
INFO:root:current mean train loss 1454.157241680265
INFO:root:current train perplexity3.138892889022827
INFO:root:current mean train loss 1453.3638717960828
INFO:root:current train perplexity3.1418614387512207
INFO:root:current mean train loss 1454.411311387694
INFO:root:current train perplexity3.144684076309204
INFO:root:current mean train loss 1454.3866312251505
INFO:root:current train perplexity3.1449997425079346
INFO:root:current mean train loss 1454.6114888530735
INFO:root:current train perplexity3.146195650100708
INFO:root:current mean train loss 1454.7947437531896
INFO:root:current train perplexity3.1458284854888916
INFO:root:current mean train loss 1455.168950982725
INFO:root:current train perplexity3.1452274322509766
INFO:root:current mean train loss 1454.3620396936064
INFO:root:current train perplexity3.1465208530426025
INFO:root:current mean train loss 1454.2379066643655
INFO:root:current train perplexity3.1474380493164062
INFO:root:current mean train loss 1455.1576265736142
INFO:root:current train perplexity3.1479227542877197
INFO:root:current mean train loss 1455.3456745283856
INFO:root:current train perplexity3.1491167545318604
INFO:root:current mean train loss 1456.1445464994906
INFO:root:current train perplexity3.149120569229126
INFO:root:current mean train loss 1456.8795830706356
INFO:root:current train perplexity3.1505887508392334
INFO:root:current mean train loss 1457.1725228810556
INFO:root:current train perplexity3.152770757675171
INFO:root:current mean train loss 1456.880380904656
INFO:root:current train perplexity3.154155731201172

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 1456.5005516864048
INFO:root:final train perplexity: 3.1540396213531494
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2256.1568512612203
INFO:root:eval perplexity: 6.200607776641846
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/127
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/200 [6:59:43<4:01:14, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1449.863314924569
INFO:root:current train perplexity3.1071739196777344
INFO:root:current mean train loss 1448.2642992236947
INFO:root:current train perplexity3.1237595081329346
INFO:root:current mean train loss 1447.5111405720083
INFO:root:current train perplexity3.12886643409729
INFO:root:current mean train loss 1448.62788570127
INFO:root:current train perplexity3.12835955619812
INFO:root:current mean train loss 1447.3838381038483
INFO:root:current train perplexity3.1259894371032715
INFO:root:current mean train loss 1447.6715052888385
INFO:root:current train perplexity3.1296396255493164
INFO:root:current mean train loss 1448.480445560351
INFO:root:current train perplexity3.1305341720581055
INFO:root:current mean train loss 1449.169492052224
INFO:root:current train perplexity3.1335532665252686
INFO:root:current mean train loss 1448.563072364647
INFO:root:current train perplexity3.134692430496216
INFO:root:current mean train loss 1448.3583821274792
INFO:root:current train perplexity3.134866237640381
INFO:root:current mean train loss 1448.2112389836734
INFO:root:current train perplexity3.135016441345215
INFO:root:current mean train loss 1448.493229946736
INFO:root:current train perplexity3.136376142501831
INFO:root:current mean train loss 1448.6538201409417
INFO:root:current train perplexity3.136253595352173
INFO:root:current mean train loss 1449.1007115135137
INFO:root:current train perplexity3.1368160247802734
INFO:root:current mean train loss 1450.136286229263
INFO:root:current train perplexity3.1388418674468994
INFO:root:current mean train loss 1450.5901043442614
INFO:root:current train perplexity3.1403090953826904
INFO:root:current mean train loss 1451.3663000237957
INFO:root:current train perplexity3.1417224407196045
INFO:root:current mean train loss 1452.2547274124092
INFO:root:current train perplexity3.141820192337036
INFO:root:current mean train loss 1452.6418692893694
INFO:root:current train perplexity3.143256187438965
INFO:root:current mean train loss 1453.3841303980266
INFO:root:current train perplexity3.145625114440918

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.69s/it]
INFO:root:final mean train loss: 1453.1541439639277
INFO:root:final train perplexity: 3.145726442337036
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2256.8960930574026
INFO:root:eval perplexity: 6.204315185546875
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/128
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 128/200 [7:03:01<3:57:55, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1436.8677685546875
INFO:root:current train perplexity3.1149814128875732
INFO:root:current mean train loss 1438.8874993024554
INFO:root:current train perplexity3.118821859359741
INFO:root:current mean train loss 1440.4315012428976
INFO:root:current train perplexity3.1132822036743164
INFO:root:current mean train loss 1443.250838216146
INFO:root:current train perplexity3.1207146644592285
INFO:root:current mean train loss 1445.3907984683387
INFO:root:current train perplexity3.1278529167175293
INFO:root:current mean train loss 1446.1149900220787
INFO:root:current train perplexity3.129077911376953
INFO:root:current mean train loss 1446.4699669053819
INFO:root:current train perplexity3.131157159805298
INFO:root:current mean train loss 1446.3757922757056
INFO:root:current train perplexity3.133358955383301
INFO:root:current mean train loss 1447.4362433035715
INFO:root:current train perplexity3.1357743740081787
INFO:root:current mean train loss 1447.5587208283252
INFO:root:current train perplexity3.13468337059021
INFO:root:current mean train loss 1446.686302461846
INFO:root:current train perplexity3.1351258754730225
INFO:root:current mean train loss 1447.4748163231384
INFO:root:current train perplexity3.136870861053467
INFO:root:current mean train loss 1447.9433653109681
INFO:root:current train perplexity3.137685775756836
INFO:root:current mean train loss 1448.5287453835226
INFO:root:current train perplexity3.1383771896362305
INFO:root:current mean train loss 1449.4863506355932
INFO:root:current train perplexity3.1385810375213623
INFO:root:current mean train loss 1450.2069582403274
INFO:root:current train perplexity3.1401116847991943
INFO:root:current mean train loss 1450.363491065182
INFO:root:current train perplexity3.140860080718994
INFO:root:current mean train loss 1450.9238234485035
INFO:root:current train perplexity3.1411685943603516
INFO:root:current mean train loss 1451.7402680989583
INFO:root:current train perplexity3.1417877674102783
INFO:root:current mean train loss 1452.24015872231
INFO:root:current train perplexity3.1423447132110596

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.98s/it]
INFO:root:final mean train loss: 1451.816470455414
INFO:root:final train perplexity: 3.142409563064575
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2259.811813029837
INFO:root:eval perplexity: 6.218961715698242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/129
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/200 [7:06:20<3:54:43, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1450.9900048297384
INFO:root:current train perplexity3.1290671825408936
INFO:root:current mean train loss 1442.9360631306965
INFO:root:current train perplexity3.1243338584899902
INFO:root:current mean train loss 1438.8182076232074
INFO:root:current train perplexity3.1126139163970947
INFO:root:current mean train loss 1441.878798815669
INFO:root:current train perplexity3.1127779483795166
INFO:root:current mean train loss 1444.6307762580188
INFO:root:current train perplexity3.119844436645508
INFO:root:current mean train loss 1445.194595336914
INFO:root:current train perplexity3.121924638748169
INFO:root:current mean train loss 1444.8255841029172
INFO:root:current train perplexity3.1218104362487793
INFO:root:current mean train loss 1446.4205490266434
INFO:root:current train perplexity3.1239006519317627
INFO:root:current mean train loss 1445.7553193644023
INFO:root:current train perplexity3.1257762908935547
INFO:root:current mean train loss 1445.982105255127
INFO:root:current train perplexity3.125568389892578
INFO:root:current mean train loss 1445.9613061702296
INFO:root:current train perplexity3.126765012741089
INFO:root:current mean train loss 1446.147787209325
INFO:root:current train perplexity3.127901315689087
INFO:root:current mean train loss 1447.4982964010799
INFO:root:current train perplexity3.1293177604675293
INFO:root:current mean train loss 1448.03784776008
INFO:root:current train perplexity3.1307640075683594
INFO:root:current mean train loss 1448.9422249883494
INFO:root:current train perplexity3.132232189178467
INFO:root:current mean train loss 1448.8925378694007
INFO:root:current train perplexity3.132500648498535
INFO:root:current mean train loss 1449.7976502763463
INFO:root:current train perplexity3.1342368125915527
INFO:root:current mean train loss 1449.7580349104744
INFO:root:current train perplexity3.1347970962524414
INFO:root:current mean train loss 1449.9582787931092
INFO:root:current train perplexity3.1364331245422363

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it]
INFO:root:final mean train loss: 1449.9876522462414
INFO:root:final train perplexity: 3.137880325317383
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2261.3426985503934
INFO:root:eval perplexity: 6.2266669273376465
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/130
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 130/200 [7:09:38<3:51:20, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1428.2755398220486
INFO:root:current train perplexity3.0015931129455566
INFO:root:current mean train loss 1442.1912595416427
INFO:root:current train perplexity3.126502513885498
INFO:root:current mean train loss 1444.3200461647727
INFO:root:current train perplexity3.1258668899536133
INFO:root:current mean train loss 1443.059010132231
INFO:root:current train perplexity3.126312017440796
INFO:root:current mean train loss 1441.838955531785
INFO:root:current train perplexity3.1219232082366943
INFO:root:current mean train loss 1441.8294922354648
INFO:root:current train perplexity3.12394380569458
INFO:root:current mean train loss 1441.4769597797363
INFO:root:current train perplexity3.125617027282715
INFO:root:current mean train loss 1441.2161656905632
INFO:root:current train perplexity3.1270477771759033
INFO:root:current mean train loss 1441.4119145755274
INFO:root:current train perplexity3.126551389694214
INFO:root:current mean train loss 1442.5058025700032
INFO:root:current train perplexity3.1279079914093018
INFO:root:current mean train loss 1442.8806087013752
INFO:root:current train perplexity3.1278486251831055
INFO:root:current mean train loss 1444.1962910438035
INFO:root:current train perplexity3.1303420066833496
INFO:root:current mean train loss 1444.0117916488964
INFO:root:current train perplexity3.130382537841797
INFO:root:current mean train loss 1444.698745109727
INFO:root:current train perplexity3.130033016204834
INFO:root:current mean train loss 1444.4586480535286
INFO:root:current train perplexity3.1300437450408936
INFO:root:current mean train loss 1444.7446091679092
INFO:root:current train perplexity3.1304023265838623
INFO:root:current mean train loss 1445.178854478228
INFO:root:current train perplexity3.129640817642212
INFO:root:current mean train loss 1446.0367386641037
INFO:root:current train perplexity3.130488157272339
INFO:root:current mean train loss 1447.0544057733252
INFO:root:current train perplexity3.1311182975769043
INFO:root:current mean train loss 1447.6683350888268
INFO:root:current train perplexity3.1313321590423584

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.51s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.51s/it]
INFO:root:final mean train loss: 1447.5588247171268
INFO:root:final train perplexity: 3.1318752765655518
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2264.7120846146386
INFO:root:eval perplexity: 6.243656635284424
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/131
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [7:12:56<3:47:57, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1438.3497924804688
INFO:root:current train perplexity3.105292558670044
INFO:root:current mean train loss 1435.572510734437
INFO:root:current train perplexity3.0853819847106934
INFO:root:current mean train loss 1440.4085066803789
INFO:root:current train perplexity3.1047234535217285
INFO:root:current mean train loss 1439.967876410923
INFO:root:current train perplexity3.110748767852783
INFO:root:current mean train loss 1440.3396635100316
INFO:root:current train perplexity3.110339403152466
INFO:root:current mean train loss 1440.4846096256388
INFO:root:current train perplexity3.1124470233917236
INFO:root:current mean train loss 1441.271743335663
INFO:root:current train perplexity3.114776134490967
INFO:root:current mean train loss 1442.0342933507661
INFO:root:current train perplexity3.1172068119049072
INFO:root:current mean train loss 1441.6770576680256
INFO:root:current train perplexity3.1171913146972656
INFO:root:current mean train loss 1442.223466185205
INFO:root:current train perplexity3.1192269325256348
INFO:root:current mean train loss 1442.2953550699394
INFO:root:current train perplexity3.119047164916992
INFO:root:current mean train loss 1441.7855694027185
INFO:root:current train perplexity3.1206858158111572
INFO:root:current mean train loss 1441.7578499375509
INFO:root:current train perplexity3.1205673217773438
INFO:root:current mean train loss 1442.257742443056
INFO:root:current train perplexity3.121324062347412
INFO:root:current mean train loss 1443.4620898060846
INFO:root:current train perplexity3.1224546432495117
INFO:root:current mean train loss 1444.4052731175254
INFO:root:current train perplexity3.1233108043670654
INFO:root:current mean train loss 1444.1656374022236
INFO:root:current train perplexity3.12272310256958
INFO:root:current mean train loss 1444.4254859049101
INFO:root:current train perplexity3.12310528755188
INFO:root:current mean train loss 1444.7628225303558
INFO:root:current train perplexity3.123194694519043
INFO:root:current mean train loss 1445.3771274206176
INFO:root:current train perplexity3.125063896179199

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.20s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.20s/it]
INFO:root:final mean train loss: 1444.9119300245939
INFO:root:final train perplexity: 3.1253440380096436
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2266.331955064273
INFO:root:eval perplexity: 6.251842975616455
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/132
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/200 [7:16:14<3:44:30, 198.09s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1455.101128156795
INFO:root:current train perplexity3.1519434452056885
INFO:root:current mean train loss 1443.6006422776443
INFO:root:current train perplexity3.1308159828186035
INFO:root:current mean train loss 1441.5752425331148
INFO:root:current train perplexity3.1056129932403564
INFO:root:current mean train loss 1440.1794906927614
INFO:root:current train perplexity3.1000728607177734
INFO:root:current mean train loss 1441.882754633712
INFO:root:current train perplexity3.0996742248535156
INFO:root:current mean train loss 1440.860110569176
INFO:root:current train perplexity3.102827787399292
INFO:root:current mean train loss 1441.4855801358378
INFO:root:current train perplexity3.1025805473327637
INFO:root:current mean train loss 1440.0740666139175
INFO:root:current train perplexity3.104135751724243
INFO:root:current mean train loss 1440.9273027123647
INFO:root:current train perplexity3.1064445972442627
INFO:root:current mean train loss 1441.4862043718517
INFO:root:current train perplexity3.1069154739379883
INFO:root:current mean train loss 1441.0031515909636
INFO:root:current train perplexity3.107372760772705
INFO:root:current mean train loss 1441.8801753326977
INFO:root:current train perplexity3.1092302799224854
INFO:root:current mean train loss 1442.0416978635044
INFO:root:current train perplexity3.110255241394043
INFO:root:current mean train loss 1442.431189791977
INFO:root:current train perplexity3.1109957695007324
INFO:root:current mean train loss 1441.7542169667413
INFO:root:current train perplexity3.1124799251556396
INFO:root:current mean train loss 1441.9419071892214
INFO:root:current train perplexity3.1122167110443115
INFO:root:current mean train loss 1442.146527615975
INFO:root:current train perplexity3.1138226985931396
INFO:root:current mean train loss 1442.1889646336463
INFO:root:current train perplexity3.1155645847320557
INFO:root:current mean train loss 1442.5552222779181
INFO:root:current train perplexity3.1167187690734863
INFO:root:current mean train loss 1443.0052527929788
INFO:root:current train perplexity3.1185081005096436

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.05s/it]
INFO:root:final mean train loss: 1442.659545959996
INFO:root:final train perplexity: 3.1197972297668457
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it]
INFO:root:eval mean loss: 2266.518063376136
INFO:root:eval perplexity: 6.252782821655273
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/133
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 133/200 [7:19:32<3:41:22, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1437.758455403646
INFO:root:current train perplexity3.0920498371124268
INFO:root:current mean train loss 1433.9380722045898
INFO:root:current train perplexity3.1053719520568848
INFO:root:current mean train loss 1433.2964106633112
INFO:root:current train perplexity3.106442451477051
INFO:root:current mean train loss 1431.8792588975693
INFO:root:current train perplexity3.10862135887146
INFO:root:current mean train loss 1431.9606225055197
INFO:root:current train perplexity3.10662579536438
INFO:root:current mean train loss 1430.9267547607421
INFO:root:current train perplexity3.1085054874420166
INFO:root:current mean train loss 1430.826396965258
INFO:root:current train perplexity3.1074929237365723
INFO:root:current mean train loss 1433.6079747250205
INFO:root:current train perplexity3.107691764831543
INFO:root:current mean train loss 1435.0963766408522
INFO:root:current train perplexity3.107975482940674
INFO:root:current mean train loss 1435.3887428283692
INFO:root:current train perplexity3.1075711250305176
INFO:root:current mean train loss 1437.725725397074
INFO:root:current train perplexity3.107743978500366
INFO:root:current mean train loss 1438.460644952182
INFO:root:current train perplexity3.109250783920288
INFO:root:current mean train loss 1437.9150370279947
INFO:root:current train perplexity3.1083760261535645
INFO:root:current mean train loss 1438.3078786513386
INFO:root:current train perplexity3.1108906269073486
INFO:root:current mean train loss 1438.0200464536065
INFO:root:current train perplexity3.111215353012085
INFO:root:current mean train loss 1438.468158584986
INFO:root:current train perplexity3.1118314266204834
INFO:root:current mean train loss 1438.882461216938
INFO:root:current train perplexity3.111973524093628
INFO:root:current mean train loss 1439.5090443004262
INFO:root:current train perplexity3.1129374504089355
INFO:root:current mean train loss 1440.0395338489163
INFO:root:current train perplexity3.1134512424468994
INFO:root:current mean train loss 1440.5714649433992
INFO:root:current train perplexity3.1141698360443115

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1440.4250630788952
INFO:root:final train perplexity: 3.114304304122925
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2270.085760021886
INFO:root:eval perplexity: 6.270851135253906
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/134
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/200 [7:22:50<3:38:03, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1426.1450084339488
INFO:root:current train perplexity3.0984604358673096
INFO:root:current mean train loss 1430.4920019807116
INFO:root:current train perplexity3.101498603820801
INFO:root:current mean train loss 1431.3728203618568
INFO:root:current train perplexity3.0996346473693848
INFO:root:current mean train loss 1431.9254014397172
INFO:root:current train perplexity3.0984561443328857
INFO:root:current mean train loss 1431.7919919315873
INFO:root:current train perplexity3.0969860553741455
INFO:root:current mean train loss 1433.3181689706998
INFO:root:current train perplexity3.095604419708252
INFO:root:current mean train loss 1435.4060512976598
INFO:root:current train perplexity3.095458745956421
INFO:root:current mean train loss 1434.4516482162967
INFO:root:current train perplexity3.095951795578003
INFO:root:current mean train loss 1434.5301832418756
INFO:root:current train perplexity3.0999181270599365
INFO:root:current mean train loss 1434.5394347082506
INFO:root:current train perplexity3.1016387939453125
INFO:root:current mean train loss 1434.609428044481
INFO:root:current train perplexity3.1019253730773926
INFO:root:current mean train loss 1434.4021945068566
INFO:root:current train perplexity3.1017019748687744
INFO:root:current mean train loss 1435.3739534645408
INFO:root:current train perplexity3.1023240089416504
INFO:root:current mean train loss 1436.010307716475
INFO:root:current train perplexity3.103605270385742
INFO:root:current mean train loss 1437.0583682050556
INFO:root:current train perplexity3.105076313018799
INFO:root:current mean train loss 1437.8036853158935
INFO:root:current train perplexity3.1058406829833984
INFO:root:current mean train loss 1437.87465912029
INFO:root:current train perplexity3.1070661544799805
INFO:root:current mean train loss 1438.4860908538355
INFO:root:current train perplexity3.108842372894287
INFO:root:current mean train loss 1438.7707613831703
INFO:root:current train perplexity3.1092007160186768
INFO:root:current mean train loss 1438.9635819245543
INFO:root:current train perplexity3.1096644401550293

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1438.4988852696652
INFO:root:final train perplexity: 3.109576940536499
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.24s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2270.8220760264294
INFO:root:eval perplexity: 6.274586200714111
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/135
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 135/200 [7:26:09<3:34:48, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1430.3643409242022
INFO:root:current train perplexity3.0847458839416504
INFO:root:current mean train loss 1434.1062099810729
INFO:root:current train perplexity3.0766444206237793
INFO:root:current mean train loss 1432.2834613825999
INFO:root:current train perplexity3.0765416622161865
INFO:root:current mean train loss 1431.154915281964
INFO:root:current train perplexity3.0841729640960693
INFO:root:current mean train loss 1432.2636071332554
INFO:root:current train perplexity3.084179162979126
INFO:root:current mean train loss 1432.1051156914193
INFO:root:current train perplexity3.0857598781585693
INFO:root:current mean train loss 1432.119102104253
INFO:root:current train perplexity3.0888497829437256
INFO:root:current mean train loss 1433.0892107985183
INFO:root:current train perplexity3.088730812072754
INFO:root:current mean train loss 1433.0719197591145
INFO:root:current train perplexity3.0897834300994873
INFO:root:current mean train loss 1433.1634968502422
INFO:root:current train perplexity3.092280626296997
INFO:root:current mean train loss 1433.2897179305444
INFO:root:current train perplexity3.092116117477417
INFO:root:current mean train loss 1435.2084306624267
INFO:root:current train perplexity3.093581438064575
INFO:root:current mean train loss 1435.572291755971
INFO:root:current train perplexity3.0948922634124756
INFO:root:current mean train loss 1435.3044328511703
INFO:root:current train perplexity3.0970802307128906
INFO:root:current mean train loss 1434.9430385931748
INFO:root:current train perplexity3.098219633102417
INFO:root:current mean train loss 1435.6832607752706
INFO:root:current train perplexity3.100114583969116
INFO:root:current mean train loss 1435.8322903791875
INFO:root:current train perplexity3.1026644706726074
INFO:root:current mean train loss 1436.2053177067003
INFO:root:current train perplexity3.1031739711761475
INFO:root:current mean train loss 1436.9687884772843
INFO:root:current train perplexity3.1049134731292725

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it]
INFO:root:final mean train loss: 1436.3842725421946
INFO:root:final train perplexity: 3.1043953895568848
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2272.4389964435118
INFO:root:eval perplexity: 6.282797336578369
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/136
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/200 [7:29:27<3:31:25, 198.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1457.0458318536932
INFO:root:current train perplexity3.091381311416626
INFO:root:current mean train loss 1424.996630419482
INFO:root:current train perplexity3.0755608081817627
INFO:root:current mean train loss 1428.8451297995039
INFO:root:current train perplexity3.0898380279541016
INFO:root:current mean train loss 1428.4852656030196
INFO:root:current train perplexity3.0882647037506104
INFO:root:current mean train loss 1425.744953239051
INFO:root:current train perplexity3.081336259841919
INFO:root:current mean train loss 1427.4642448649247
INFO:root:current train perplexity3.0851824283599854
INFO:root:current mean train loss 1427.486649183907
INFO:root:current train perplexity3.086212635040283
INFO:root:current mean train loss 1428.9332654821553
INFO:root:current train perplexity3.0875656604766846
INFO:root:current mean train loss 1429.0042742671567
INFO:root:current train perplexity3.089768409729004
INFO:root:current mean train loss 1428.9840125409492
INFO:root:current train perplexity3.0904715061187744
INFO:root:current mean train loss 1428.7417962954378
INFO:root:current train perplexity3.089998960494995
INFO:root:current mean train loss 1429.4430731525301
INFO:root:current train perplexity3.0916569232940674
INFO:root:current mean train loss 1430.4729330502296
INFO:root:current train perplexity3.094259262084961
INFO:root:current mean train loss 1431.2854295347956
INFO:root:current train perplexity3.095299243927002
INFO:root:current mean train loss 1432.7168665182328
INFO:root:current train perplexity3.0942814350128174
INFO:root:current mean train loss 1432.9850801156106
INFO:root:current train perplexity3.0951855182647705
INFO:root:current mean train loss 1433.219916752745
INFO:root:current train perplexity3.0962777137756348
INFO:root:current mean train loss 1433.8978578265405
INFO:root:current train perplexity3.097478151321411
INFO:root:current mean train loss 1433.3408097973322
INFO:root:current train perplexity3.0975277423858643
INFO:root:current mean train loss 1433.8264816819155
INFO:root:current train perplexity3.0986618995666504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1434.007829336223
INFO:root:final train perplexity: 3.0985827445983887
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2273.6785009938776
INFO:root:eval perplexity: 6.289096832275391
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/137
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/200 [7:32:45<3:28:07, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1411.0941336495537
INFO:root:current train perplexity3.10764217376709
INFO:root:current mean train loss 1427.0172986984253
INFO:root:current train perplexity3.101872444152832
INFO:root:current mean train loss 1428.990892912212
INFO:root:current train perplexity3.0829944610595703
INFO:root:current mean train loss 1429.4561454959032
INFO:root:current train perplexity3.082005262374878
INFO:root:current mean train loss 1428.9740175621532
INFO:root:current train perplexity3.0812253952026367
INFO:root:current mean train loss 1429.6617454760003
INFO:root:current train perplexity3.0861144065856934
INFO:root:current mean train loss 1430.7093793541003
INFO:root:current train perplexity3.0863285064697266
INFO:root:current mean train loss 1430.304704603258
INFO:root:current train perplexity3.0875308513641357
INFO:root:current mean train loss 1429.7752610358639
INFO:root:current train perplexity3.087284803390503
INFO:root:current mean train loss 1429.5886589576457
INFO:root:current train perplexity3.0880987644195557
INFO:root:current mean train loss 1429.3848534966257
INFO:root:current train perplexity3.0870914459228516
INFO:root:current mean train loss 1430.4491209206005
INFO:root:current train perplexity3.087662935256958
INFO:root:current mean train loss 1431.6443757793413
INFO:root:current train perplexity3.0895678997039795
INFO:root:current mean train loss 1431.7507353633282
INFO:root:current train perplexity3.0921125411987305
INFO:root:current mean train loss 1431.92018242844
INFO:root:current train perplexity3.093651056289673
INFO:root:current mean train loss 1432.9362002068165
INFO:root:current train perplexity3.0931267738342285
INFO:root:current mean train loss 1433.0971312276854
INFO:root:current train perplexity3.0945262908935547
INFO:root:current mean train loss 1432.886063752351
INFO:root:current train perplexity3.095613956451416
INFO:root:current mean train loss 1432.695811198629
INFO:root:current train perplexity3.095351219177246
INFO:root:current mean train loss 1433.0070509534653
INFO:root:current train perplexity3.096086025238037

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.83s/it]
INFO:root:final mean train loss: 1432.958275037526
INFO:root:final train perplexity: 3.0960192680358887
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.31s/it]
INFO:root:eval mean loss: 2274.000986951463
INFO:root:eval perplexity: 6.290739059448242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/138
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 138/200 [7:36:04<3:24:54, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1417.135164388021
INFO:root:current train perplexity3.026381015777588
INFO:root:current mean train loss 1411.0304620150862
INFO:root:current train perplexity3.0608038902282715
INFO:root:current mean train loss 1420.5734165736608
INFO:root:current train perplexity3.0689427852630615
INFO:root:current mean train loss 1423.8098894644475
INFO:root:current train perplexity3.0715925693511963
INFO:root:current mean train loss 1423.7521843728055
INFO:root:current train perplexity3.0747487545013428
INFO:root:current mean train loss 1423.3907163847477
INFO:root:current train perplexity3.072972536087036
INFO:root:current mean train loss 1423.8468443404797
INFO:root:current train perplexity3.07627534866333
INFO:root:current mean train loss 1424.676431581638
INFO:root:current train perplexity3.0769248008728027
INFO:root:current mean train loss 1426.3962961411335
INFO:root:current train perplexity3.0795090198516846
INFO:root:current mean train loss 1426.7515404110864
INFO:root:current train perplexity3.081143379211426
INFO:root:current mean train loss 1427.8959840619393
INFO:root:current train perplexity3.082219362258911
INFO:root:current mean train loss 1428.7678220524017
INFO:root:current train perplexity3.082566738128662
INFO:root:current mean train loss 1428.7441112104668
INFO:root:current train perplexity3.0852036476135254
INFO:root:current mean train loss 1429.6715643333237
INFO:root:current train perplexity3.086451768875122
INFO:root:current mean train loss 1429.831914586262
INFO:root:current train perplexity3.086050510406494
INFO:root:current mean train loss 1429.288357573574
INFO:root:current train perplexity3.086282253265381
INFO:root:current mean train loss 1429.6181144923062
INFO:root:current train perplexity3.0871381759643555
INFO:root:current mean train loss 1430.1699898006582
INFO:root:current train perplexity3.089092969894409
INFO:root:current mean train loss 1430.6661997559918
INFO:root:current train perplexity3.0897159576416016
INFO:root:current mean train loss 1430.4672190060653
INFO:root:current train perplexity3.089116096496582

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it]
INFO:root:final mean train loss: 1430.1171137530337
INFO:root:final train perplexity: 3.0890896320343018
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2277.4907507930243
INFO:root:eval perplexity: 6.308518886566162
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/139
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/200 [7:39:22<3:21:35, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1423.5035498834425
INFO:root:current train perplexity3.043205738067627
INFO:root:current mean train loss 1426.4341144326293
INFO:root:current train perplexity3.0706846714019775
INFO:root:current mean train loss 1422.9252692069715
INFO:root:current train perplexity3.065966844558716
INFO:root:current mean train loss 1420.7032143608642
INFO:root:current train perplexity3.069796085357666
INFO:root:current mean train loss 1420.197891301407
INFO:root:current train perplexity3.067732810974121
INFO:root:current mean train loss 1420.4150981427936
INFO:root:current train perplexity3.0680956840515137
INFO:root:current mean train loss 1420.8561657677965
INFO:root:current train perplexity3.066950559616089
INFO:root:current mean train loss 1422.3654464761728
INFO:root:current train perplexity3.069876194000244
INFO:root:current mean train loss 1423.397464761048
INFO:root:current train perplexity3.0710997581481934
INFO:root:current mean train loss 1425.0598006218734
INFO:root:current train perplexity3.0746724605560303
INFO:root:current mean train loss 1426.0885931614878
INFO:root:current train perplexity3.0763514041900635
INFO:root:current mean train loss 1426.8532076128174
INFO:root:current train perplexity3.0770514011383057
INFO:root:current mean train loss 1426.5697869785993
INFO:root:current train perplexity3.0765531063079834
INFO:root:current mean train loss 1427.145265285139
INFO:root:current train perplexity3.0774805545806885
INFO:root:current mean train loss 1426.7972906402297
INFO:root:current train perplexity3.078367233276367
INFO:root:current mean train loss 1427.1763555744087
INFO:root:current train perplexity3.080155849456787
INFO:root:current mean train loss 1427.7237914598375
INFO:root:current train perplexity3.0810041427612305
INFO:root:current mean train loss 1427.4775674670564
INFO:root:current train perplexity3.081695079803467
INFO:root:current mean train loss 1427.5695620494805
INFO:root:current train perplexity3.082423686981201
INFO:root:current mean train loss 1427.9848388298572
INFO:root:current train perplexity3.0834975242614746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.40s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.40s/it]
INFO:root:final mean train loss: 1427.8681437790062
INFO:root:final train perplexity: 3.083615303039551
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2277.1754427429632
INFO:root:eval perplexity: 6.306910037994385
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/140
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 140/200 [7:42:40<3:18:10, 198.18s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1418.0661559285998
INFO:root:current train perplexity3.0432865619659424
INFO:root:current mean train loss 1420.2838912196667
INFO:root:current train perplexity3.045574426651001
INFO:root:current mean train loss 1416.9215809811828
INFO:root:current train perplexity3.0539700984954834
INFO:root:current mean train loss 1417.2046758405138
INFO:root:current train perplexity3.0549845695495605
INFO:root:current mean train loss 1418.6587222423832
INFO:root:current train perplexity3.0602073669433594
INFO:root:current mean train loss 1418.736669879709
INFO:root:current train perplexity3.063241958618164
INFO:root:current mean train loss 1419.080531529018
INFO:root:current train perplexity3.0657103061676025
INFO:root:current mean train loss 1419.038118374669
INFO:root:current train perplexity3.06575083732605
INFO:root:current mean train loss 1420.9642109008373
INFO:root:current train perplexity3.0676655769348145
INFO:root:current mean train loss 1421.7013292322363
INFO:root:current train perplexity3.0697436332702637
INFO:root:current mean train loss 1422.7281443230856
INFO:root:current train perplexity3.0714080333709717
INFO:root:current mean train loss 1423.462845586335
INFO:root:current train perplexity3.0733437538146973
INFO:root:current mean train loss 1423.835183508234
INFO:root:current train perplexity3.072535991668701
INFO:root:current mean train loss 1424.2009516350163
INFO:root:current train perplexity3.074873685836792
INFO:root:current mean train loss 1424.2290064648569
INFO:root:current train perplexity3.0743401050567627
INFO:root:current mean train loss 1423.9063737711022
INFO:root:current train perplexity3.074734926223755
INFO:root:current mean train loss 1424.5418597204903
INFO:root:current train perplexity3.0765647888183594
INFO:root:current mean train loss 1425.4433025598123
INFO:root:current train perplexity3.0772979259490967
INFO:root:current mean train loss 1426.054291664761
INFO:root:current train perplexity3.0798306465148926
INFO:root:current mean train loss 1426.403510233921
INFO:root:current train perplexity3.0792620182037354

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.07s/it]
INFO:root:final mean train loss: 1426.1662890058662
INFO:root:final train perplexity: 3.079479217529297
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2281.903231573443
INFO:root:eval perplexity: 6.3310699462890625
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/141
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [7:45:58<3:15:00, 198.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1423.8477210998535
INFO:root:current train perplexity3.0759379863739014
INFO:root:current mean train loss 1420.4099401357223
INFO:root:current train perplexity3.0633738040924072
INFO:root:current mean train loss 1421.256722939981
INFO:root:current train perplexity3.0609118938446045
INFO:root:current mean train loss 1422.3713280263573
INFO:root:current train perplexity3.0625486373901367
INFO:root:current mean train loss 1421.9280245381017
INFO:root:current train perplexity3.0625288486480713
INFO:root:current mean train loss 1422.9210108814623
INFO:root:current train perplexity3.0605390071868896
INFO:root:current mean train loss 1423.1185067713946
INFO:root:current train perplexity3.0630061626434326
INFO:root:current mean train loss 1423.6425203102917
INFO:root:current train perplexity3.0658931732177734
INFO:root:current mean train loss 1423.439032963344
INFO:root:current train perplexity3.066770315170288
INFO:root:current mean train loss 1421.8256399621926
INFO:root:current train perplexity3.0680553913116455
INFO:root:current mean train loss 1422.3056352155922
INFO:root:current train perplexity3.068284273147583
INFO:root:current mean train loss 1422.3435061655714
INFO:root:current train perplexity3.067958354949951
INFO:root:current mean train loss 1423.2561240490572
INFO:root:current train perplexity3.0699548721313477
INFO:root:current mean train loss 1422.958881804458
INFO:root:current train perplexity3.0708017349243164
INFO:root:current mean train loss 1422.7335969649534
INFO:root:current train perplexity3.0717427730560303
INFO:root:current mean train loss 1423.1556185385339
INFO:root:current train perplexity3.072786808013916
INFO:root:current mean train loss 1423.5679357996528
INFO:root:current train perplexity3.0740623474121094
INFO:root:current mean train loss 1423.8961279514372
INFO:root:current train perplexity3.0740180015563965
INFO:root:current mean train loss 1424.6602131646393
INFO:root:current train perplexity3.074329137802124

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.02s/it]
INFO:root:final mean train loss: 1424.1303979639927
INFO:root:final train perplexity: 3.0745387077331543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2281.45228522551
INFO:root:eval perplexity: 6.328763961791992
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/142
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/200 [7:49:17<3:11:46, 198.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1413.7602914663462
INFO:root:current train perplexity3.0061604976654053
INFO:root:current mean train loss 1427.3594970703125
INFO:root:current train perplexity3.038463592529297
INFO:root:current mean train loss 1420.6148034037558
INFO:root:current train perplexity3.0568597316741943
INFO:root:current mean train loss 1417.92726949381
INFO:root:current train perplexity3.052921772003174
INFO:root:current mean train loss 1417.611419456057
INFO:root:current train perplexity3.0541129112243652
INFO:root:current mean train loss 1419.0257254260325
INFO:root:current train perplexity3.055438756942749
INFO:root:current mean train loss 1419.2084925093036
INFO:root:current train perplexity3.059129238128662
INFO:root:current mean train loss 1420.8067474579243
INFO:root:current train perplexity3.0616021156311035
INFO:root:current mean train loss 1420.7813698180928
INFO:root:current train perplexity3.06105637550354
INFO:root:current mean train loss 1420.5540183193712
INFO:root:current train perplexity3.059180974960327
INFO:root:current mean train loss 1420.2281337485733
INFO:root:current train perplexity3.0589184761047363
INFO:root:current mean train loss 1420.7509236982676
INFO:root:current train perplexity3.0618395805358887
INFO:root:current mean train loss 1420.2527865844627
INFO:root:current train perplexity3.061457633972168
INFO:root:current mean train loss 1420.9686959841192
INFO:root:current train perplexity3.0633819103240967
INFO:root:current mean train loss 1421.271320491502
INFO:root:current train perplexity3.0644888877868652
INFO:root:current mean train loss 1421.0332195032377
INFO:root:current train perplexity3.0667810440063477
INFO:root:current mean train loss 1421.2101900936725
INFO:root:current train perplexity3.067854166030884
INFO:root:current mean train loss 1421.1892680598503
INFO:root:current train perplexity3.069152355194092
INFO:root:current mean train loss 1421.7987449044228
INFO:root:current train perplexity3.070207357406616
INFO:root:current mean train loss 1422.0560028985478
INFO:root:current train perplexity3.070307970046997

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1422.4835666458353
INFO:root:final train perplexity: 3.0705480575561523
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2283.705872014905
INFO:root:eval perplexity: 6.340307712554932
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/143
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 143/200 [7:52:35<3:08:25, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1381.2089599609376
INFO:root:current train perplexity3.0120317935943604
INFO:root:current mean train loss 1407.3762150691107
INFO:root:current train perplexity3.045982837677002
INFO:root:current mean train loss 1410.7363934060802
INFO:root:current train perplexity3.044973373413086
INFO:root:current mean train loss 1411.737706039891
INFO:root:current train perplexity3.0476551055908203
INFO:root:current mean train loss 1410.5603206190951
INFO:root:current train perplexity3.050405502319336
INFO:root:current mean train loss 1410.3053125921285
INFO:root:current train perplexity3.052241086959839
INFO:root:current mean train loss 1412.0158472454737
INFO:root:current train perplexity3.0532636642456055
INFO:root:current mean train loss 1414.0337868886452
INFO:root:current train perplexity3.052318811416626
INFO:root:current mean train loss 1415.1037893272307
INFO:root:current train perplexity3.0517897605895996
INFO:root:current mean train loss 1415.5015987273187
INFO:root:current train perplexity3.0539417266845703
INFO:root:current mean train loss 1416.2106250948118
INFO:root:current train perplexity3.055054187774658
INFO:root:current mean train loss 1416.8217630842091
INFO:root:current train perplexity3.056178569793701
INFO:root:current mean train loss 1416.9069042174797
INFO:root:current train perplexity3.055893659591675
INFO:root:current mean train loss 1417.5633354932743
INFO:root:current train perplexity3.05849289894104
INFO:root:current mean train loss 1417.8894666978529
INFO:root:current train perplexity3.0597198009490967
INFO:root:current mean train loss 1418.2573167190053
INFO:root:current train perplexity3.060657262802124
INFO:root:current mean train loss 1419.0639888833637
INFO:root:current train perplexity3.062462091445923
INFO:root:current mean train loss 1420.218507552836
INFO:root:current train perplexity3.063725471496582
INFO:root:current mean train loss 1420.6680517311304
INFO:root:current train perplexity3.0641801357269287
INFO:root:current mean train loss 1420.56670257094
INFO:root:current train perplexity3.0647037029266357

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 1420.614003730193
INFO:root:final train perplexity: 3.0660240650177
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.28s/it]
INFO:root:eval mean loss: 2285.0921016179077
INFO:root:eval perplexity: 6.347418785095215
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/144
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/200 [7:55:54<3:05:08, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1406.9858294547873
INFO:root:current train perplexity3.03798770904541
INFO:root:current mean train loss 1415.1664374734269
INFO:root:current train perplexity3.0474658012390137
INFO:root:current mean train loss 1418.9991880100265
INFO:root:current train perplexity3.0477771759033203
INFO:root:current mean train loss 1418.583042287689
INFO:root:current train perplexity3.0490469932556152
INFO:root:current mean train loss 1418.4876597018317
INFO:root:current train perplexity3.052377939224243
INFO:root:current mean train loss 1418.4782621115173
INFO:root:current train perplexity3.0542213916778564
INFO:root:current mean train loss 1418.2982524889514
INFO:root:current train perplexity3.0556857585906982
INFO:root:current mean train loss 1417.435183932344
INFO:root:current train perplexity3.055445432662964
INFO:root:current mean train loss 1417.9453261914755
INFO:root:current train perplexity3.0551979541778564
INFO:root:current mean train loss 1416.8360598281167
INFO:root:current train perplexity3.0558242797851562
INFO:root:current mean train loss 1416.469425642274
INFO:root:current train perplexity3.057403087615967
INFO:root:current mean train loss 1416.3512282167815
INFO:root:current train perplexity3.059074878692627
INFO:root:current mean train loss 1416.8615891029094
INFO:root:current train perplexity3.057882070541382
INFO:root:current mean train loss 1417.1661591731627
INFO:root:current train perplexity3.057130813598633
INFO:root:current mean train loss 1417.4696859005162
INFO:root:current train perplexity3.0588860511779785
INFO:root:current mean train loss 1417.6963079214559
INFO:root:current train perplexity3.058645725250244
INFO:root:current mean train loss 1418.8115981471994
INFO:root:current train perplexity3.0614755153656006
INFO:root:current mean train loss 1418.7514832905517
INFO:root:current train perplexity3.0617871284484863
INFO:root:current mean train loss 1419.0534062574022
INFO:root:current train perplexity3.06183123588562
INFO:root:current mean train loss 1419.4551569973437
INFO:root:current train perplexity3.062101364135742

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.85s/it]
INFO:root:final mean train loss: 1419.0815930465105
INFO:root:final train perplexity: 3.062321186065674
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2286.133166590481
INFO:root:eval perplexity: 6.352766036987305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/145
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 145/200 [7:59:12<3:01:50, 198.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1417.4595355987549
INFO:root:current train perplexity3.052326202392578
INFO:root:current mean train loss 1410.3073663478945
INFO:root:current train perplexity3.0518877506256104
INFO:root:current mean train loss 1408.9588216145833
INFO:root:current train perplexity3.056849479675293
INFO:root:current mean train loss 1408.1793870192307
INFO:root:current train perplexity3.0533065795898438
INFO:root:current mean train loss 1410.0197169862945
INFO:root:current train perplexity3.0506088733673096
INFO:root:current mean train loss 1411.126244944038
INFO:root:current train perplexity3.0516347885131836
INFO:root:current mean train loss 1413.160157353045
INFO:root:current train perplexity3.0519022941589355
INFO:root:current mean train loss 1414.003143150769
INFO:root:current train perplexity3.0512521266937256
INFO:root:current mean train loss 1415.5781278257016
INFO:root:current train perplexity3.053429126739502
INFO:root:current mean train loss 1415.7179579358872
INFO:root:current train perplexity3.052227258682251
INFO:root:current mean train loss 1417.7964939891845
INFO:root:current train perplexity3.0541255474090576
INFO:root:current mean train loss 1417.7486176900438
INFO:root:current train perplexity3.0547869205474854
INFO:root:current mean train loss 1417.5664207361922
INFO:root:current train perplexity3.0547075271606445
INFO:root:current mean train loss 1417.1901639787334
INFO:root:current train perplexity3.056000232696533
INFO:root:current mean train loss 1417.2846416202399
INFO:root:current train perplexity3.056309700012207
INFO:root:current mean train loss 1417.4751949066397
INFO:root:current train perplexity3.056621789932251
INFO:root:current mean train loss 1417.3647384643555
INFO:root:current train perplexity3.0585105419158936
INFO:root:current mean train loss 1417.9268643818205
INFO:root:current train perplexity3.0591585636138916
INFO:root:current mean train loss 1417.6703176866786
INFO:root:current train perplexity3.0592260360717773
INFO:root:current mean train loss 1418.1204733916552
INFO:root:current train perplexity3.058708906173706

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 1417.7364676163404
INFO:root:final train perplexity: 3.0590741634368896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2290.31117194426
INFO:root:eval perplexity: 6.374267578125
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/146
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/200 [8:02:30<2:58:31, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1409.6136896580826
INFO:root:current train perplexity3.040982961654663
INFO:root:current mean train loss 1409.2400266531422
INFO:root:current train perplexity3.03749942779541
INFO:root:current mean train loss 1410.4304959443116
INFO:root:current train perplexity3.032594919204712
INFO:root:current mean train loss 1411.4643955180652
INFO:root:current train perplexity3.0360257625579834
INFO:root:current mean train loss 1412.8886302543528
INFO:root:current train perplexity3.037590980529785
INFO:root:current mean train loss 1412.6860664617175
INFO:root:current train perplexity3.038900136947632
INFO:root:current mean train loss 1413.1205796565253
INFO:root:current train perplexity3.0415055751800537
INFO:root:current mean train loss 1412.6535528869238
INFO:root:current train perplexity3.0408337116241455
INFO:root:current mean train loss 1412.730996520511
INFO:root:current train perplexity3.041391134262085
INFO:root:current mean train loss 1413.7493734719435
INFO:root:current train perplexity3.044919490814209
INFO:root:current mean train loss 1414.3284943727958
INFO:root:current train perplexity3.0449209213256836
INFO:root:current mean train loss 1414.6559169682075
INFO:root:current train perplexity3.0461840629577637
INFO:root:current mean train loss 1414.3446763430975
INFO:root:current train perplexity3.047621250152588
INFO:root:current mean train loss 1414.4464783996539
INFO:root:current train perplexity3.0492632389068604
INFO:root:current mean train loss 1414.3579338120094
INFO:root:current train perplexity3.0509510040283203
INFO:root:current mean train loss 1414.8615447785717
INFO:root:current train perplexity3.051379442214966
INFO:root:current mean train loss 1415.4649251544142
INFO:root:current train perplexity3.052313804626465
INFO:root:current mean train loss 1415.0478341532585
INFO:root:current train perplexity3.052762508392334
INFO:root:current mean train loss 1415.8262870662836
INFO:root:current train perplexity3.053647994995117
INFO:root:current mean train loss 1415.9628473057523
INFO:root:current train perplexity3.053922414779663

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it]
INFO:root:final mean train loss: 1415.6132312030186
INFO:root:final train perplexity: 3.0539560317993164
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2290.707472348044
INFO:root:eval perplexity: 6.376311302185059
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/147
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/200 [8:05:49<2:55:10, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1411.2737376933196
INFO:root:current train perplexity3.0304174423217773
INFO:root:current mean train loss 1409.3232952079388
INFO:root:current train perplexity3.041898250579834
INFO:root:current mean train loss 1407.8403254771392
INFO:root:current train perplexity3.042837142944336
INFO:root:current mean train loss 1410.4486826220948
INFO:root:current train perplexity3.042738914489746
INFO:root:current mean train loss 1411.7400127855171
INFO:root:current train perplexity3.046654462814331
INFO:root:current mean train loss 1413.143459562474
INFO:root:current train perplexity3.0465087890625
INFO:root:current mean train loss 1413.6091354064067
INFO:root:current train perplexity3.045464515686035
INFO:root:current mean train loss 1414.3812731597059
INFO:root:current train perplexity3.0439116954803467
INFO:root:current mean train loss 1414.3394276506385
INFO:root:current train perplexity3.0440282821655273
INFO:root:current mean train loss 1414.5543444065866
INFO:root:current train perplexity3.042870044708252
INFO:root:current mean train loss 1414.4809740410476
INFO:root:current train perplexity3.043729543685913
INFO:root:current mean train loss 1414.7342234820078
INFO:root:current train perplexity3.044598340988159
INFO:root:current mean train loss 1414.4653089902433
INFO:root:current train perplexity3.0456807613372803
INFO:root:current mean train loss 1414.3883074977368
INFO:root:current train perplexity3.0457334518432617
INFO:root:current mean train loss 1414.588942401718
INFO:root:current train perplexity3.047503709793091
INFO:root:current mean train loss 1414.7962588428406
INFO:root:current train perplexity3.049053907394409
INFO:root:current mean train loss 1414.9727672491533
INFO:root:current train perplexity3.0505590438842773
INFO:root:current mean train loss 1414.7697559055418
INFO:root:current train perplexity3.0495870113372803
INFO:root:current mean train loss 1414.5997179391136
INFO:root:current train perplexity3.050861120223999

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it]
INFO:root:final mean train loss: 1414.6149644721834
INFO:root:final train perplexity: 3.0515522956848145
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2291.2043924534573
INFO:root:eval perplexity: 6.378873825073242
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/148
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 148/200 [8:09:07<2:51:49, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1408.9251708984375
INFO:root:current train perplexity2.973585367202759
INFO:root:current mean train loss 1401.8074611497962
INFO:root:current train perplexity3.015354633331299
INFO:root:current mean train loss 1402.4769588026888
INFO:root:current train perplexity3.031620740890503
INFO:root:current mean train loss 1405.423738606771
INFO:root:current train perplexity3.0335183143615723
INFO:root:current mean train loss 1408.4024890577937
INFO:root:current train perplexity3.0358169078826904
INFO:root:current mean train loss 1408.5499170395935
INFO:root:current train perplexity3.035613775253296
INFO:root:current mean train loss 1409.596168580094
INFO:root:current train perplexity3.037928581237793
INFO:root:current mean train loss 1409.2771335841892
INFO:root:current train perplexity3.034219980239868
INFO:root:current mean train loss 1408.929633579371
INFO:root:current train perplexity3.0344669818878174
INFO:root:current mean train loss 1409.3787134722934
INFO:root:current train perplexity3.0373458862304688
INFO:root:current mean train loss 1409.9254914081744
INFO:root:current train perplexity3.0398354530334473
INFO:root:current mean train loss 1408.9774481940162
INFO:root:current train perplexity3.0390048027038574
INFO:root:current mean train loss 1410.5063721707818
INFO:root:current train perplexity3.0413482189178467
INFO:root:current mean train loss 1410.5933020994237
INFO:root:current train perplexity3.040651321411133
INFO:root:current mean train loss 1410.8451946568573
INFO:root:current train perplexity3.0409648418426514
INFO:root:current mean train loss 1411.6086275912749
INFO:root:current train perplexity3.041440486907959
INFO:root:current mean train loss 1412.3380269809404
INFO:root:current train perplexity3.0429553985595703
INFO:root:current mean train loss 1412.5438123519498
INFO:root:current train perplexity3.0432748794555664
INFO:root:current mean train loss 1412.505001183712
INFO:root:current train perplexity3.0442988872528076
INFO:root:current mean train loss 1412.3306360787574
INFO:root:current train perplexity3.0445168018341064

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.81s/it]
INFO:root:final mean train loss: 1411.9074869853225
INFO:root:final train perplexity: 3.045043468475342
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2292.29453402039
INFO:root:eval perplexity: 6.384500503540039
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/149
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/200 [8:12:25<2:48:32, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1391.597770690918
INFO:root:current train perplexity3.0246739387512207
INFO:root:current mean train loss 1401.5736768317945
INFO:root:current train perplexity3.0204970836639404
INFO:root:current mean train loss 1403.302738058156
INFO:root:current train perplexity3.0151565074920654
INFO:root:current mean train loss 1405.0520276908414
INFO:root:current train perplexity3.0266342163085938
INFO:root:current mean train loss 1403.7401719269928
INFO:root:current train perplexity3.0307111740112305
INFO:root:current mean train loss 1402.8757014453859
INFO:root:current train perplexity3.0295214653015137
INFO:root:current mean train loss 1404.355357302895
INFO:root:current train perplexity3.030632495880127
INFO:root:current mean train loss 1404.076240914767
INFO:root:current train perplexity3.0310471057891846
INFO:root:current mean train loss 1405.4563380021316
INFO:root:current train perplexity3.0337135791778564
INFO:root:current mean train loss 1405.3205955407138
INFO:root:current train perplexity3.0333051681518555
INFO:root:current mean train loss 1406.8638917198477
INFO:root:current train perplexity3.0359747409820557
INFO:root:current mean train loss 1407.4698541324467
INFO:root:current train perplexity3.037611961364746
INFO:root:current mean train loss 1408.3075753744547
INFO:root:current train perplexity3.039547920227051
INFO:root:current mean train loss 1408.8464448946017
INFO:root:current train perplexity3.041008472442627
INFO:root:current mean train loss 1409.10410807519
INFO:root:current train perplexity3.0415115356445312
INFO:root:current mean train loss 1409.9329655500367
INFO:root:current train perplexity3.0405349731445312
INFO:root:current mean train loss 1410.4109757367303
INFO:root:current train perplexity3.040466785430908
INFO:root:current mean train loss 1410.3428626644143
INFO:root:current train perplexity3.0411646366119385
INFO:root:current mean train loss 1410.6301050311092
INFO:root:current train perplexity3.0409247875213623
INFO:root:current mean train loss 1410.5072300123131
INFO:root:current train perplexity3.040558099746704

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it]
INFO:root:final mean train loss: 1410.0736423786757
INFO:root:final train perplexity: 3.040642738342285
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2294.122518336519
INFO:root:eval perplexity: 6.393947124481201
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/150
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 150/200 [8:15:43<2:45:15, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1422.0528414979274
INFO:root:current train perplexity3.0255048274993896
INFO:root:current mean train loss 1413.2831605232802
INFO:root:current train perplexity3.030317783355713
INFO:root:current mean train loss 1409.8132662485882
INFO:root:current train perplexity3.0323729515075684
INFO:root:current mean train loss 1408.2654077917935
INFO:root:current train perplexity3.0316760540008545
INFO:root:current mean train loss 1408.9368205930714
INFO:root:current train perplexity3.0373191833496094
INFO:root:current mean train loss 1409.009881247154
INFO:root:current train perplexity3.039336681365967
INFO:root:current mean train loss 1409.3467712872568
INFO:root:current train perplexity3.036008596420288
INFO:root:current mean train loss 1409.0373986604536
INFO:root:current train perplexity3.0349531173706055
INFO:root:current mean train loss 1409.3880770518165
INFO:root:current train perplexity3.0349347591400146
INFO:root:current mean train loss 1408.3417533979025
INFO:root:current train perplexity3.0328900814056396
INFO:root:current mean train loss 1407.6976252029463
INFO:root:current train perplexity3.0347201824188232
INFO:root:current mean train loss 1408.5318370848972
INFO:root:current train perplexity3.0348825454711914
INFO:root:current mean train loss 1408.1496014194167
INFO:root:current train perplexity3.0343525409698486
INFO:root:current mean train loss 1407.6812971269228
INFO:root:current train perplexity3.033747434616089
INFO:root:current mean train loss 1407.5621803762833
INFO:root:current train perplexity3.034379005432129
INFO:root:current mean train loss 1408.2141190511015
INFO:root:current train perplexity3.036757707595825
INFO:root:current mean train loss 1408.6844981362995
INFO:root:current train perplexity3.037074327468872
INFO:root:current mean train loss 1408.742468841012
INFO:root:current train perplexity3.037531614303589
INFO:root:current mean train loss 1408.9367062458805
INFO:root:current train perplexity3.037667751312256
INFO:root:current mean train loss 1409.0439166895483
INFO:root:current train perplexity3.0375852584838867

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.72s/it]
INFO:root:final mean train loss: 1408.9914862266767
INFO:root:final train perplexity: 3.038048505783081
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2295.722815547429
INFO:root:eval perplexity: 6.402226448059082
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/151
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [8:19:02<2:41:57, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1398.4547026663115
INFO:root:current train perplexity3.0022099018096924
INFO:root:current mean train loss 1409.4838536274003
INFO:root:current train perplexity3.0115854740142822
INFO:root:current mean train loss 1406.7574559261925
INFO:root:current train perplexity3.016206741333008
INFO:root:current mean train loss 1406.2503018405268
INFO:root:current train perplexity3.0136353969573975
INFO:root:current mean train loss 1405.7616983176301
INFO:root:current train perplexity3.0170042514801025
INFO:root:current mean train loss 1403.3722941799635
INFO:root:current train perplexity3.0211122035980225
INFO:root:current mean train loss 1405.029258384361
INFO:root:current train perplexity3.022408962249756
INFO:root:current mean train loss 1405.1544077900621
INFO:root:current train perplexity3.025282621383667
INFO:root:current mean train loss 1404.643596411302
INFO:root:current train perplexity3.0259575843811035
INFO:root:current mean train loss 1404.524447296964
INFO:root:current train perplexity3.028657913208008
INFO:root:current mean train loss 1406.4732937410222
INFO:root:current train perplexity3.03135085105896
INFO:root:current mean train loss 1405.5482026978586
INFO:root:current train perplexity3.0305185317993164
INFO:root:current mean train loss 1405.812574630665
INFO:root:current train perplexity3.0310659408569336
INFO:root:current mean train loss 1405.772185072892
INFO:root:current train perplexity3.0322036743164062
INFO:root:current mean train loss 1405.9704464942338
INFO:root:current train perplexity3.0321426391601562
INFO:root:current mean train loss 1406.1660516380807
INFO:root:current train perplexity3.031630516052246
INFO:root:current mean train loss 1406.6297195636066
INFO:root:current train perplexity3.0311639308929443
INFO:root:current mean train loss 1406.5256990495382
INFO:root:current train perplexity3.0322017669677734
INFO:root:current mean train loss 1406.955903833191
INFO:root:current train perplexity3.0330328941345215
INFO:root:current mean train loss 1406.997213245287
INFO:root:current train perplexity3.0325510501861572

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1406.6495541078662
INFO:root:final train perplexity: 3.032442569732666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2296.3736351500165
INFO:root:eval perplexity: 6.405597686767578
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/152
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/200 [8:22:20<2:38:40, 198.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1403.4414959643261
INFO:root:current train perplexity2.997479200363159
INFO:root:current mean train loss 1397.9594753244535
INFO:root:current train perplexity3.0047860145568848
INFO:root:current mean train loss 1395.051844081272
INFO:root:current train perplexity3.0125107765197754
INFO:root:current mean train loss 1398.7482725297814
INFO:root:current train perplexity3.02018141746521
INFO:root:current mean train loss 1399.331732266191
INFO:root:current train perplexity3.024120807647705
INFO:root:current mean train loss 1399.120424143037
INFO:root:current train perplexity3.024822950363159
INFO:root:current mean train loss 1400.2985360856287
INFO:root:current train perplexity3.024444341659546
INFO:root:current mean train loss 1401.4326376105025
INFO:root:current train perplexity3.0266354084014893
INFO:root:current mean train loss 1402.6543765041054
INFO:root:current train perplexity3.025724172592163
INFO:root:current mean train loss 1402.9200132725077
INFO:root:current train perplexity3.0253381729125977
INFO:root:current mean train loss 1403.3288930398055
INFO:root:current train perplexity3.0245020389556885
INFO:root:current mean train loss 1403.4467905516958
INFO:root:current train perplexity3.0259768962860107
INFO:root:current mean train loss 1403.4120356380615
INFO:root:current train perplexity3.025996446609497
INFO:root:current mean train loss 1403.2294751523805
INFO:root:current train perplexity3.0261330604553223
INFO:root:current mean train loss 1404.072650603322
INFO:root:current train perplexity3.0257699489593506
INFO:root:current mean train loss 1404.060135475684
INFO:root:current train perplexity3.0256166458129883
INFO:root:current mean train loss 1404.774690842246
INFO:root:current train perplexity3.0265591144561768
INFO:root:current mean train loss 1404.8296298948621
INFO:root:current train perplexity3.0285396575927734
INFO:root:current mean train loss 1405.1531761359865
INFO:root:current train perplexity3.0289666652679443
INFO:root:current mean train loss 1405.2629798969956
INFO:root:current train perplexity3.0291285514831543

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.35s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.36s/it]
INFO:root:final mean train loss: 1405.2629798969956
INFO:root:final train perplexity: 3.0291285514831543
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2299.0420817923036
INFO:root:eval perplexity: 6.419436454772949
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/153
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 153/200 [8:25:38<2:35:16, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1396.672568359375
INFO:root:current train perplexity3.0146865844726562
INFO:root:current mean train loss 1397.140482788086
INFO:root:current train perplexity3.012688636779785
INFO:root:current mean train loss 1399.1750162760416
INFO:root:current train perplexity3.0175528526306152
INFO:root:current mean train loss 1399.4306317138671
INFO:root:current train perplexity3.0159904956817627
INFO:root:current mean train loss 1400.04498828125
INFO:root:current train perplexity3.0184271335601807
INFO:root:current mean train loss 1399.2927402750652
INFO:root:current train perplexity3.016641139984131
INFO:root:current mean train loss 1399.600489153181
INFO:root:current train perplexity3.015932321548462
INFO:root:current mean train loss 1400.332226715088
INFO:root:current train perplexity3.015244245529175
INFO:root:current mean train loss 1401.373963080512
INFO:root:current train perplexity3.0171966552734375
INFO:root:current mean train loss 1400.718639404297
INFO:root:current train perplexity3.0166287422180176
INFO:root:current mean train loss 1400.7474873490767
INFO:root:current train perplexity3.0192179679870605
INFO:root:current mean train loss 1401.7730517578125
INFO:root:current train perplexity3.019784688949585
INFO:root:current mean train loss 1402.5627193509615
INFO:root:current train perplexity3.020613670349121
INFO:root:current mean train loss 1402.4201218959263
INFO:root:current train perplexity3.023153781890869
INFO:root:current mean train loss 1403.0204479166666
INFO:root:current train perplexity3.024092435836792
INFO:root:current mean train loss 1403.809994430542
INFO:root:current train perplexity3.0248095989227295
INFO:root:current mean train loss 1403.716060001149
INFO:root:current train perplexity3.0243585109710693
INFO:root:current mean train loss 1403.8710892062718
INFO:root:current train perplexity3.025003433227539
INFO:root:current mean train loss 1404.014782265111
INFO:root:current train perplexity3.0260353088378906

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.44s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.44s/it]
INFO:root:final mean train loss: 1404.0517969944233
INFO:root:final train perplexity: 3.026236057281494
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2301.3596169762577
INFO:root:eval perplexity: 6.431478023529053
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/154
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/200 [8:28:56<2:31:54, 198.14s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1393.3739516314338
INFO:root:current train perplexity2.9852993488311768
INFO:root:current mean train loss 1401.7558458116318
INFO:root:current train perplexity3.006803512573242
INFO:root:current mean train loss 1397.081518217166
INFO:root:current train perplexity3.0025134086608887
INFO:root:current mean train loss 1396.9743764016907
INFO:root:current train perplexity2.9985222816467285
INFO:root:current mean train loss 1395.1823150854316
INFO:root:current train perplexity2.996917486190796
INFO:root:current mean train loss 1397.2677757514055
INFO:root:current train perplexity2.999966859817505
INFO:root:current mean train loss 1399.038239465154
INFO:root:current train perplexity3.004432201385498
INFO:root:current mean train loss 1399.879930823418
INFO:root:current train perplexity3.0093038082122803
INFO:root:current mean train loss 1400.1439537692684
INFO:root:current train perplexity3.009770631790161
INFO:root:current mean train loss 1401.0391937555378
INFO:root:current train perplexity3.0108847618103027
INFO:root:current mean train loss 1400.8696791987386
INFO:root:current train perplexity3.012233018875122
INFO:root:current mean train loss 1401.9710104318208
INFO:root:current train perplexity3.014737606048584
INFO:root:current mean train loss 1402.1710885141165
INFO:root:current train perplexity3.016533136367798
INFO:root:current mean train loss 1402.3329956703506
INFO:root:current train perplexity3.0169074535369873
INFO:root:current mean train loss 1402.4470107159989
INFO:root:current train perplexity3.018749713897705
INFO:root:current mean train loss 1402.210270820673
INFO:root:current train perplexity3.019923686981201
INFO:root:current mean train loss 1402.6224999365868
INFO:root:current train perplexity3.020050048828125
INFO:root:current mean train loss 1402.4602280418471
INFO:root:current train perplexity3.021608591079712
INFO:root:current mean train loss 1402.5910210533332
INFO:root:current train perplexity3.0223894119262695
INFO:root:current mean train loss 1402.4485485413202
INFO:root:current train perplexity3.0217108726501465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.25s/it]
INFO:root:final mean train loss: 1402.3869223236377
INFO:root:final train perplexity: 3.022265672683716
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2302.500984354222
INFO:root:eval perplexity: 6.437417984008789
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/155
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 155/200 [8:32:15<2:28:45, 198.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.9595552332262
INFO:root:current train perplexity2.994471549987793
INFO:root:current mean train loss 1392.4969045155085
INFO:root:current train perplexity3.0049540996551514
INFO:root:current mean train loss 1392.3113387670273
INFO:root:current train perplexity3.010798454284668
INFO:root:current mean train loss 1393.647151741439
INFO:root:current train perplexity3.0100860595703125
INFO:root:current mean train loss 1397.179701282132
INFO:root:current train perplexity3.0131351947784424
INFO:root:current mean train loss 1397.9657505906923
INFO:root:current train perplexity3.012829065322876
INFO:root:current mean train loss 1398.3172332089782
INFO:root:current train perplexity3.0117175579071045
INFO:root:current mean train loss 1397.400763654579
INFO:root:current train perplexity3.010868787765503
INFO:root:current mean train loss 1397.9496462888283
INFO:root:current train perplexity3.0096709728240967
INFO:root:current mean train loss 1398.643869142716
INFO:root:current train perplexity3.009652614593506
INFO:root:current mean train loss 1400.4533711475838
INFO:root:current train perplexity3.0133941173553467
INFO:root:current mean train loss 1400.621704855083
INFO:root:current train perplexity3.0153276920318604
INFO:root:current mean train loss 1399.6877196078556
INFO:root:current train perplexity3.0166285037994385
INFO:root:current mean train loss 1399.810517501259
INFO:root:current train perplexity3.0173330307006836
INFO:root:current mean train loss 1400.2547596355528
INFO:root:current train perplexity3.0180776119232178
INFO:root:current mean train loss 1400.5370912315636
INFO:root:current train perplexity3.018768072128296
INFO:root:current mean train loss 1400.8076763549955
INFO:root:current train perplexity3.0199167728424072
INFO:root:current mean train loss 1401.4954764994234
INFO:root:current train perplexity3.0204763412475586
INFO:root:current mean train loss 1401.4341292220088
INFO:root:current train perplexity3.0198256969451904
INFO:root:current mean train loss 1401.4703390600762
INFO:root:current train perplexity3.019368886947632

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1401.3502816543155
INFO:root:final train perplexity: 3.0197956562042236
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2301.749967101618
INFO:root:eval perplexity: 6.4335103034973145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/156
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/200 [8:35:33<2:25:27, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1388.675204407935
INFO:root:current train perplexity3.0255260467529297
INFO:root:current mean train loss 1389.7977375763141
INFO:root:current train perplexity3.0004711151123047
INFO:root:current mean train loss 1390.6712296322523
INFO:root:current train perplexity3.0013675689697266
INFO:root:current mean train loss 1391.8181482733485
INFO:root:current train perplexity3.005539894104004
INFO:root:current mean train loss 1394.4439626892495
INFO:root:current train perplexity3.0061328411102295
INFO:root:current mean train loss 1396.0164119215065
INFO:root:current train perplexity3.005080223083496
INFO:root:current mean train loss 1396.7427554738504
INFO:root:current train perplexity3.006955146789551
INFO:root:current mean train loss 1397.166411093802
INFO:root:current train perplexity3.0069639682769775
INFO:root:current mean train loss 1397.435816978876
INFO:root:current train perplexity3.0067338943481445
INFO:root:current mean train loss 1397.5722237796563
INFO:root:current train perplexity3.0049562454223633
INFO:root:current mean train loss 1398.5309819331292
INFO:root:current train perplexity3.0054550170898438
INFO:root:current mean train loss 1398.109912597232
INFO:root:current train perplexity3.0069377422332764
INFO:root:current mean train loss 1398.4784247797074
INFO:root:current train perplexity3.007265090942383
INFO:root:current mean train loss 1398.9256597218207
INFO:root:current train perplexity3.007159471511841
INFO:root:current mean train loss 1398.901468898739
INFO:root:current train perplexity3.007565498352051
INFO:root:current mean train loss 1399.2618441258915
INFO:root:current train perplexity3.009730100631714
INFO:root:current mean train loss 1399.5041282094658
INFO:root:current train perplexity3.010035514831543
INFO:root:current mean train loss 1399.964311687964
INFO:root:current train perplexity3.012057065963745
INFO:root:current mean train loss 1399.5933426373201
INFO:root:current train perplexity3.012739896774292
INFO:root:current mean train loss 1399.8303736340138
INFO:root:current train perplexity3.014909267425537

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.65s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.65s/it]
INFO:root:final mean train loss: 1399.2643615137852
INFO:root:final train perplexity: 3.014832019805908
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2301.600558666473
INFO:root:eval perplexity: 6.432732582092285
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/157
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/200 [8:38:51<2:22:07, 198.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.9147446576287
INFO:root:current train perplexity2.995053768157959
INFO:root:current mean train loss 1385.1168329148065
INFO:root:current train perplexity3.0065760612487793
INFO:root:current mean train loss 1390.0550719303872
INFO:root:current train perplexity3.00281023979187
INFO:root:current mean train loss 1391.729108064071
INFO:root:current train perplexity3.0029218196868896
INFO:root:current mean train loss 1392.718769301716
INFO:root:current train perplexity2.9981017112731934
INFO:root:current mean train loss 1393.1001204369772
INFO:root:current train perplexity3.0003974437713623
INFO:root:current mean train loss 1394.0216435415302
INFO:root:current train perplexity2.9998624324798584
INFO:root:current mean train loss 1394.7932381629944
INFO:root:current train perplexity3.001012086868286
INFO:root:current mean train loss 1395.2383881318403
INFO:root:current train perplexity3.003080129623413
INFO:root:current mean train loss 1394.2687148417324
INFO:root:current train perplexity3.001638412475586
INFO:root:current mean train loss 1394.0096150944742
INFO:root:current train perplexity3.0025620460510254
INFO:root:current mean train loss 1395.4164587569564
INFO:root:current train perplexity3.005751848220825
INFO:root:current mean train loss 1396.4998309499458
INFO:root:current train perplexity3.0088038444519043
INFO:root:current mean train loss 1396.2613702963667
INFO:root:current train perplexity3.007767677307129
INFO:root:current mean train loss 1397.1228863043098
INFO:root:current train perplexity3.008369207382202
INFO:root:current mean train loss 1396.9512745604222
INFO:root:current train perplexity3.009260416030884
INFO:root:current mean train loss 1397.1103711757157
INFO:root:current train perplexity3.00864315032959
INFO:root:current mean train loss 1397.2354497434867
INFO:root:current train perplexity3.009233236312866
INFO:root:current mean train loss 1397.6344822754952
INFO:root:current train perplexity3.010219097137451
INFO:root:current mean train loss 1398.354751772997
INFO:root:current train perplexity3.0118422508239746

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.53s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.53s/it]
INFO:root:final mean train loss: 1397.9488661742487
INFO:root:final train perplexity: 3.0117053985595703
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2303.9473565284243
INFO:root:eval perplexity: 6.444952011108398
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/158
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 158/200 [8:42:10<2:18:46, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1392.9176585477942
INFO:root:current train perplexity3.002495527267456
INFO:root:current mean train loss 1393.8927034945102
INFO:root:current train perplexity2.9840505123138428
INFO:root:current mean train loss 1396.1284055475603
INFO:root:current train perplexity2.9953339099884033
INFO:root:current mean train loss 1396.205085734578
INFO:root:current train perplexity2.9917337894439697
INFO:root:current mean train loss 1395.9246008174935
INFO:root:current train perplexity2.993666410446167
INFO:root:current mean train loss 1395.4237110627002
INFO:root:current train perplexity2.9951412677764893
INFO:root:current mean train loss 1396.5692739222172
INFO:root:current train perplexity2.9991462230682373
INFO:root:current mean train loss 1396.9912369065983
INFO:root:current train perplexity3.0002410411834717
INFO:root:current mean train loss 1396.976364015051
INFO:root:current train perplexity3.0039279460906982
INFO:root:current mean train loss 1397.4361936617624
INFO:root:current train perplexity3.005570650100708
INFO:root:current mean train loss 1396.3142113470262
INFO:root:current train perplexity3.0046138763427734
INFO:root:current mean train loss 1396.7674789235562
INFO:root:current train perplexity3.0045340061187744
INFO:root:current mean train loss 1396.7185974358586
INFO:root:current train perplexity3.0053133964538574
INFO:root:current mean train loss 1397.2318399918208
INFO:root:current train perplexity3.0064263343811035
INFO:root:current mean train loss 1397.9203443122633
INFO:root:current train perplexity3.0079033374786377
INFO:root:current mean train loss 1397.4279111266512
INFO:root:current train perplexity3.0078678131103516
INFO:root:current mean train loss 1396.9909622328219
INFO:root:current train perplexity3.008409023284912
INFO:root:current mean train loss 1397.4595223734025
INFO:root:current train perplexity3.009077787399292
INFO:root:current mean train loss 1397.306387353386
INFO:root:current train perplexity3.0092101097106934

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it]
INFO:root:final mean train loss: 1396.7444308113102
INFO:root:final train perplexity: 3.0088460445404053
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2304.8834501225897
INFO:root:eval perplexity: 6.44983434677124
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/159
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/200 [8:45:28<2:15:27, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1383.1768188476562
INFO:root:current train perplexity3.2020792961120605
INFO:root:current mean train loss 1390.114595301011
INFO:root:current train perplexity2.994354486465454
INFO:root:current mean train loss 1393.6182341622834
INFO:root:current train perplexity2.9914839267730713
INFO:root:current mean train loss 1391.2770737401697
INFO:root:current train perplexity2.9979963302612305
INFO:root:current mean train loss 1393.558571583003
INFO:root:current train perplexity2.995296001434326
INFO:root:current mean train loss 1391.4317160070655
INFO:root:current train perplexity2.993570327758789
INFO:root:current mean train loss 1391.2215140206474
INFO:root:current train perplexity2.992866039276123
INFO:root:current mean train loss 1391.0368163714722
INFO:root:current train perplexity2.9960126876831055
INFO:root:current mean train loss 1392.098716983177
INFO:root:current train perplexity2.996666669845581
INFO:root:current mean train loss 1392.865506123545
INFO:root:current train perplexity2.997836112976074
INFO:root:current mean train loss 1392.960987570757
INFO:root:current train perplexity2.9993300437927246
INFO:root:current mean train loss 1392.9027804116804
INFO:root:current train perplexity2.999530553817749
INFO:root:current mean train loss 1392.443375827072
INFO:root:current train perplexity2.9994893074035645
INFO:root:current mean train loss 1392.671595513363
INFO:root:current train perplexity3.0009350776672363
INFO:root:current mean train loss 1393.164844899307
INFO:root:current train perplexity3.0010159015655518
INFO:root:current mean train loss 1393.590997485124
INFO:root:current train perplexity3.0012874603271484
INFO:root:current mean train loss 1393.9141507380912
INFO:root:current train perplexity3.0020856857299805
INFO:root:current mean train loss 1394.0287646283555
INFO:root:current train perplexity3.0020382404327393
INFO:root:current mean train loss 1394.7941833563834
INFO:root:current train perplexity3.0032501220703125
INFO:root:current mean train loss 1395.17017840236
INFO:root:current train perplexity3.0043630599975586

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.74s/it]
INFO:root:final mean train loss: 1394.9583718996726
INFO:root:final train perplexity: 3.004611015319824
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2307.517447830092
INFO:root:eval perplexity: 6.463587284088135
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/160
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 160/200 [8:48:46<2:12:10, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1407.1165964226973
INFO:root:current train perplexity3.0404977798461914
INFO:root:current mean train loss 1386.4525782480962
INFO:root:current train perplexity2.9908530712127686
INFO:root:current mean train loss 1386.767558616046
INFO:root:current train perplexity2.987325668334961
INFO:root:current mean train loss 1388.3085432381465
INFO:root:current train perplexity2.9927403926849365
INFO:root:current mean train loss 1391.2178299569287
INFO:root:current train perplexity2.991283416748047
INFO:root:current mean train loss 1392.4608946930696
INFO:root:current train perplexity2.994978189468384
INFO:root:current mean train loss 1392.0014983687147
INFO:root:current train perplexity2.9959089756011963
INFO:root:current mean train loss 1391.8308127539879
INFO:root:current train perplexity2.997781753540039
INFO:root:current mean train loss 1391.2378192011981
INFO:root:current train perplexity2.9987618923187256
INFO:root:current mean train loss 1391.9076963804491
INFO:root:current train perplexity2.9983153343200684
INFO:root:current mean train loss 1392.362817766154
INFO:root:current train perplexity2.99726939201355
INFO:root:current mean train loss 1392.6298371043133
INFO:root:current train perplexity2.9975767135620117
INFO:root:current mean train loss 1392.857129266753
INFO:root:current train perplexity2.9978835582733154
INFO:root:current mean train loss 1393.518948662724
INFO:root:current train perplexity2.997576951980591
INFO:root:current mean train loss 1393.7087219969499
INFO:root:current train perplexity2.9969892501831055
INFO:root:current mean train loss 1393.8931090786239
INFO:root:current train perplexity2.9978322982788086
INFO:root:current mean train loss 1394.5915614052126
INFO:root:current train perplexity2.99939227104187
INFO:root:current mean train loss 1393.9883013465087
INFO:root:current train perplexity2.998223066329956
INFO:root:current mean train loss 1393.9918801431934
INFO:root:current train perplexity2.9996137619018555
INFO:root:current mean train loss 1393.8599525916818
INFO:root:current train perplexity3.0001723766326904

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.67s/it]
INFO:root:final mean train loss: 1393.4788006404044
INFO:root:final train perplexity: 3.0011072158813477
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2307.467766944398
INFO:root:eval perplexity: 6.463327407836914
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/161
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [8:52:04<2:08:51, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1392.487572564019
INFO:root:current train perplexity3.001871109008789
INFO:root:current mean train loss 1388.1591375014361
INFO:root:current train perplexity2.993398666381836
INFO:root:current mean train loss 1387.3167440123477
INFO:root:current train perplexity2.9846343994140625
INFO:root:current mean train loss 1388.4169613066174
INFO:root:current train perplexity2.983152151107788
INFO:root:current mean train loss 1387.4156944904853
INFO:root:current train perplexity2.986402750015259
INFO:root:current mean train loss 1388.029518469056
INFO:root:current train perplexity2.9894635677337646
INFO:root:current mean train loss 1388.4141540527344
INFO:root:current train perplexity2.988522529602051
INFO:root:current mean train loss 1388.2855183145275
INFO:root:current train perplexity2.9886906147003174
INFO:root:current mean train loss 1389.189877158717
INFO:root:current train perplexity2.989112615585327
INFO:root:current mean train loss 1390.9646749904014
INFO:root:current train perplexity2.9905269145965576
INFO:root:current mean train loss 1390.8257521699295
INFO:root:current train perplexity2.9905998706817627
INFO:root:current mean train loss 1391.3686923174791
INFO:root:current train perplexity2.9912519454956055
INFO:root:current mean train loss 1391.102456497143
INFO:root:current train perplexity2.9906256198883057
INFO:root:current mean train loss 1391.3110931761964
INFO:root:current train perplexity2.992994785308838
INFO:root:current mean train loss 1390.762389286647
INFO:root:current train perplexity2.993499279022217
INFO:root:current mean train loss 1391.403294801712
INFO:root:current train perplexity2.995572328567505
INFO:root:current mean train loss 1391.7569245056302
INFO:root:current train perplexity2.996608018875122
INFO:root:current mean train loss 1391.8733180876702
INFO:root:current train perplexity2.9962241649627686
INFO:root:current mean train loss 1392.6048970274417
INFO:root:current train perplexity2.997629165649414
INFO:root:current mean train loss 1392.3048016887067
INFO:root:current train perplexity2.998290777206421

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1392.2543773872349
INFO:root:final train perplexity: 2.9982104301452637
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2308.768975440492
INFO:root:eval perplexity: 6.470133304595947
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/162
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/200 [8:55:23<2:05:35, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1382.6257001768868
INFO:root:current train perplexity2.975170612335205
INFO:root:current mean train loss 1388.1794816559436
INFO:root:current train perplexity2.9793033599853516
INFO:root:current mean train loss 1387.8297109490798
INFO:root:current train perplexity2.992202043533325
INFO:root:current mean train loss 1384.9926650611942
INFO:root:current train perplexity2.9863951206207275
INFO:root:current mean train loss 1388.5840635994412
INFO:root:current train perplexity2.9897661209106445
INFO:root:current mean train loss 1388.6064528177271
INFO:root:current train perplexity2.989065647125244
INFO:root:current mean train loss 1388.1176499838487
INFO:root:current train perplexity2.9905905723571777
INFO:root:current mean train loss 1389.2692649000353
INFO:root:current train perplexity2.991588592529297
INFO:root:current mean train loss 1388.836134415299
INFO:root:current train perplexity2.990487575531006
INFO:root:current mean train loss 1389.2404941426744
INFO:root:current train perplexity2.9911727905273438
INFO:root:current mean train loss 1389.928947774773
INFO:root:current train perplexity2.9922595024108887
INFO:root:current mean train loss 1389.5855536084534
INFO:root:current train perplexity2.9929726123809814
INFO:root:current mean train loss 1390.5332914871497
INFO:root:current train perplexity2.9927875995635986
INFO:root:current mean train loss 1390.6884443532601
INFO:root:current train perplexity2.9928741455078125
INFO:root:current mean train loss 1391.3142107486397
INFO:root:current train perplexity2.9935107231140137
INFO:root:current mean train loss 1391.8694867135937
INFO:root:current train perplexity2.9947328567504883
INFO:root:current mean train loss 1391.394979875014
INFO:root:current train perplexity2.994575023651123
INFO:root:current mean train loss 1391.3530414796733
INFO:root:current train perplexity2.9946229457855225
INFO:root:current mean train loss 1391.501252390022
INFO:root:current train perplexity2.9953534603118896
INFO:root:current mean train loss 1391.697518516185
INFO:root:current train perplexity2.9952926635742188

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it]
INFO:root:final mean train loss: 1391.1420688215555
INFO:root:final train perplexity: 2.9955811500549316
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2310.1838097538507
INFO:root:eval perplexity: 6.477540969848633
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/163
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 163/200 [8:58:41<2:02:17, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1388.6446219308036
INFO:root:current train perplexity2.9770407676696777
INFO:root:current mean train loss 1394.8742532169117
INFO:root:current train perplexity2.982820749282837
INFO:root:current mean train loss 1390.8989289460358
INFO:root:current train perplexity2.9831905364990234
INFO:root:current mean train loss 1389.0292124155405
INFO:root:current train perplexity2.980835437774658
INFO:root:current mean train loss 1387.5877093375998
INFO:root:current train perplexity2.984464645385742
INFO:root:current mean train loss 1386.2628856993558
INFO:root:current train perplexity2.9829206466674805
INFO:root:current mean train loss 1387.226724106518
INFO:root:current train perplexity2.9840025901794434
INFO:root:current mean train loss 1386.0067011845576
INFO:root:current train perplexity2.9847517013549805
INFO:root:current mean train loss 1387.103760186557
INFO:root:current train perplexity2.9865946769714355
INFO:root:current mean train loss 1386.5630595099067
INFO:root:current train perplexity2.9873578548431396
INFO:root:current mean train loss 1386.5580878997519
INFO:root:current train perplexity2.9877350330352783
INFO:root:current mean train loss 1385.9564652402178
INFO:root:current train perplexity2.9882724285125732
INFO:root:current mean train loss 1386.54422367126
INFO:root:current train perplexity2.9897243976593018
INFO:root:current mean train loss 1387.1693507285013
INFO:root:current train perplexity2.9890434741973877
INFO:root:current mean train loss 1387.4808686755953
INFO:root:current train perplexity2.990245819091797
INFO:root:current mean train loss 1388.3772796047722
INFO:root:current train perplexity2.9909839630126953
INFO:root:current mean train loss 1389.4392703119152
INFO:root:current train perplexity2.9915058612823486
INFO:root:current mean train loss 1389.387729312875
INFO:root:current train perplexity2.9910669326782227
INFO:root:current mean train loss 1389.1962207161807
INFO:root:current train perplexity2.9906604290008545
INFO:root:current mean train loss 1389.875701625456
INFO:root:current train perplexity2.9921910762786865

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.61s/it]
INFO:root:final mean train loss: 1389.6969975381564
INFO:root:final train perplexity: 2.9921693801879883
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2311.1191008006426
INFO:root:eval perplexity: 6.482443809509277
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/164
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/200 [9:01:59<1:58:57, 198.26s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1390.9250740840516
INFO:root:current train perplexity2.9729256629943848
INFO:root:current mean train loss 1387.9248144792364
INFO:root:current train perplexity2.9917352199554443
INFO:root:current mean train loss 1383.0781564745753
INFO:root:current train perplexity2.9875261783599854
INFO:root:current mean train loss 1384.906374909157
INFO:root:current train perplexity2.9895241260528564
INFO:root:current mean train loss 1385.4010759232226
INFO:root:current train perplexity2.988060235977173
INFO:root:current mean train loss 1386.5139155997126
INFO:root:current train perplexity2.9882876873016357
INFO:root:current mean train loss 1387.6727820872566
INFO:root:current train perplexity2.9868133068084717
INFO:root:current mean train loss 1386.9517406575108
INFO:root:current train perplexity2.9848835468292236
INFO:root:current mean train loss 1387.069133771447
INFO:root:current train perplexity2.983220100402832
INFO:root:current mean train loss 1387.9975134512333
INFO:root:current train perplexity2.98327374458313
INFO:root:current mean train loss 1388.124464103467
INFO:root:current train perplexity2.984151601791382
INFO:root:current mean train loss 1388.6798970866023
INFO:root:current train perplexity2.9844343662261963
INFO:root:current mean train loss 1389.34489131474
INFO:root:current train perplexity2.985938310623169
INFO:root:current mean train loss 1388.8811094123164
INFO:root:current train perplexity2.9871017932891846
INFO:root:current mean train loss 1388.7018758110657
INFO:root:current train perplexity2.986720085144043
INFO:root:current mean train loss 1388.8138002372796
INFO:root:current train perplexity2.987323522567749
INFO:root:current mean train loss 1389.2213156473447
INFO:root:current train perplexity2.9881017208099365
INFO:root:current mean train loss 1389.29184969244
INFO:root:current train perplexity2.9889421463012695
INFO:root:current mean train loss 1389.0504189851617
INFO:root:current train perplexity2.989283323287964

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.56s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.56s/it]
INFO:root:final mean train loss: 1388.8373368148305
INFO:root:final train perplexity: 2.9901413917541504
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2312.891653507314
INFO:root:eval perplexity: 6.491740703582764
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/165
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 165/200 [9:05:17<1:55:37, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1348.4135437011719
INFO:root:current train perplexity2.9177327156066895
INFO:root:current mean train loss 1376.894555898813
INFO:root:current train perplexity2.9779434204101562
INFO:root:current mean train loss 1379.9367807425704
INFO:root:current train perplexity2.987581491470337
INFO:root:current mean train loss 1380.3509429128546
INFO:root:current train perplexity2.988873243331909
INFO:root:current mean train loss 1383.1292836406444
INFO:root:current train perplexity2.9884214401245117
INFO:root:current mean train loss 1384.4788273402623
INFO:root:current train perplexity2.990018129348755
INFO:root:current mean train loss 1386.4729965917322
INFO:root:current train perplexity2.986412763595581
INFO:root:current mean train loss 1386.4486465454102
INFO:root:current train perplexity2.985004425048828
INFO:root:current mean train loss 1385.3998161050217
INFO:root:current train perplexity2.982607364654541
INFO:root:current mean train loss 1385.1259067501642
INFO:root:current train perplexity2.981793165206909
INFO:root:current mean train loss 1386.2020718395947
INFO:root:current train perplexity2.9822568893432617
INFO:root:current mean train loss 1386.012451061304
INFO:root:current train perplexity2.982745409011841
INFO:root:current mean train loss 1387.402769069735
INFO:root:current train perplexity2.984405517578125
INFO:root:current mean train loss 1387.5636928066886
INFO:root:current train perplexity2.9836695194244385
INFO:root:current mean train loss 1387.7544833886998
INFO:root:current train perplexity2.9840750694274902
INFO:root:current mean train loss 1387.3780286261376
INFO:root:current train perplexity2.984194040298462
INFO:root:current mean train loss 1387.4559066658305
INFO:root:current train perplexity2.985149621963501
INFO:root:current mean train loss 1387.3836115447568
INFO:root:current train perplexity2.985731601715088
INFO:root:current mean train loss 1387.7015045910348
INFO:root:current train perplexity2.986510753631592
INFO:root:current mean train loss 1387.7962741370961
INFO:root:current train perplexity2.9866855144500732

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.49s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.49s/it]
INFO:root:final mean train loss: 1387.4392628479773
INFO:root:final train perplexity: 2.9868462085723877
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2313.3091365733044
INFO:root:eval perplexity: 6.4939351081848145
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/166
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/200 [9:08:35<1:52:17, 198.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1402.6432698567708
INFO:root:current train perplexity2.9622745513916016
INFO:root:current mean train loss 1389.1622586841427
INFO:root:current train perplexity2.969172954559326
INFO:root:current mean train loss 1385.9450749876273
INFO:root:current train perplexity2.976416826248169
INFO:root:current mean train loss 1385.918286284925
INFO:root:current train perplexity2.976942777633667
INFO:root:current mean train loss 1385.5528944291864
INFO:root:current train perplexity2.9775078296661377
INFO:root:current mean train loss 1385.9642809613424
INFO:root:current train perplexity2.9786949157714844
INFO:root:current mean train loss 1385.418493789943
INFO:root:current train perplexity2.9787988662719727
INFO:root:current mean train loss 1385.5954645715044
INFO:root:current train perplexity2.9801230430603027
INFO:root:current mean train loss 1385.7044435377968
INFO:root:current train perplexity2.982208728790283
INFO:root:current mean train loss 1385.0151482498218
INFO:root:current train perplexity2.9832496643066406
INFO:root:current mean train loss 1384.3486013683353
INFO:root:current train perplexity2.982032060623169
INFO:root:current mean train loss 1384.4221733698985
INFO:root:current train perplexity2.980508327484131
INFO:root:current mean train loss 1384.4517414364827
INFO:root:current train perplexity2.9798598289489746
INFO:root:current mean train loss 1384.4595341996476
INFO:root:current train perplexity2.979922294616699
INFO:root:current mean train loss 1385.5585578419136
INFO:root:current train perplexity2.9831175804138184
INFO:root:current mean train loss 1385.4179586376665
INFO:root:current train perplexity2.983747959136963
INFO:root:current mean train loss 1385.3002019996336
INFO:root:current train perplexity2.9832072257995605
INFO:root:current mean train loss 1385.9579763196361
INFO:root:current train perplexity2.9827356338500977
INFO:root:current mean train loss 1386.0517395790432
INFO:root:current train perplexity2.9829952716827393
INFO:root:current mean train loss 1386.7149041179318
INFO:root:current train perplexity2.983638286590576

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it]
INFO:root:final mean train loss: 1386.410137290012
INFO:root:final train perplexity: 2.9844233989715576
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2314.6977729526816
INFO:root:eval perplexity: 6.501231670379639
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/167
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/200 [9:11:54<1:49:01, 198.22s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1377.2484291478206
INFO:root:current train perplexity2.943875551223755
INFO:root:current mean train loss 1382.858662923177
INFO:root:current train perplexity2.960413694381714
INFO:root:current mean train loss 1381.3765812721574
INFO:root:current train perplexity2.9660375118255615
INFO:root:current mean train loss 1380.0654300486547
INFO:root:current train perplexity2.962888717651367
INFO:root:current mean train loss 1380.2193553349744
INFO:root:current train perplexity2.9613845348358154
INFO:root:current mean train loss 1379.387394447752
INFO:root:current train perplexity2.964709758758545
INFO:root:current mean train loss 1378.997990814496
INFO:root:current train perplexity2.9672791957855225
INFO:root:current mean train loss 1379.238701052782
INFO:root:current train perplexity2.968230724334717
INFO:root:current mean train loss 1381.6349587816042
INFO:root:current train perplexity2.968494415283203
INFO:root:current mean train loss 1383.896991786672
INFO:root:current train perplexity2.971118927001953
INFO:root:current mean train loss 1383.6434495517974
INFO:root:current train perplexity2.9710257053375244
INFO:root:current mean train loss 1384.933028772552
INFO:root:current train perplexity2.972943067550659
INFO:root:current mean train loss 1385.0980223623346
INFO:root:current train perplexity2.9739081859588623
INFO:root:current mean train loss 1384.7413257091391
INFO:root:current train perplexity2.9737472534179688
INFO:root:current mean train loss 1384.086211606425
INFO:root:current train perplexity2.9740664958953857
INFO:root:current mean train loss 1384.8929218743651
INFO:root:current train perplexity2.975857734680176
INFO:root:current mean train loss 1384.977066878434
INFO:root:current train perplexity2.9762206077575684
INFO:root:current mean train loss 1385.2376385898392
INFO:root:current train perplexity2.9776835441589355
INFO:root:current mean train loss 1385.4425451965665
INFO:root:current train perplexity2.979550361633301
INFO:root:current mean train loss 1385.2913142500524
INFO:root:current train perplexity2.9803271293640137

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.62s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.62s/it]
INFO:root:final mean train loss: 1384.7721592089893
INFO:root:final train perplexity: 2.9805703163146973
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2315.7341572646556
INFO:root:eval perplexity: 6.506684303283691
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/168
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 168/200 [9:15:12<1:45:42, 198.21s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1363.2258833451704
INFO:root:current train perplexity2.968474864959717
INFO:root:current mean train loss 1378.8243565713205
INFO:root:current train perplexity2.9752228260040283
INFO:root:current mean train loss 1378.8329077627145
INFO:root:current train perplexity2.976134777069092
INFO:root:current mean train loss 1378.1736847353652
INFO:root:current train perplexity2.9826183319091797
INFO:root:current mean train loss 1378.3398751395089
INFO:root:current train perplexity2.983351230621338
INFO:root:current mean train loss 1378.3834514446087
INFO:root:current train perplexity2.97743558883667
INFO:root:current mean train loss 1380.4140123673067
INFO:root:current train perplexity2.977032423019409
INFO:root:current mean train loss 1380.6544790912149
INFO:root:current train perplexity2.9747653007507324
INFO:root:current mean train loss 1381.258117604395
INFO:root:current train perplexity2.974513053894043
INFO:root:current mean train loss 1381.98317168071
INFO:root:current train perplexity2.973771095275879
INFO:root:current mean train loss 1382.298139208753
INFO:root:current train perplexity2.9755451679229736
INFO:root:current mean train loss 1381.7446204511634
INFO:root:current train perplexity2.9758121967315674
INFO:root:current mean train loss 1382.0195760901706
INFO:root:current train perplexity2.974484443664551
INFO:root:current mean train loss 1381.9941835072648
INFO:root:current train perplexity2.974909543991089
INFO:root:current mean train loss 1381.8639355636544
INFO:root:current train perplexity2.9742884635925293
INFO:root:current mean train loss 1381.7955836452472
INFO:root:current train perplexity2.974099636077881
INFO:root:current mean train loss 1382.0838980775586
INFO:root:current train perplexity2.974674940109253
INFO:root:current mean train loss 1382.8929594990875
INFO:root:current train perplexity2.9765076637268066
INFO:root:current mean train loss 1383.9308400938132
INFO:root:current train perplexity2.9776153564453125
INFO:root:current mean train loss 1384.124656080163
INFO:root:current train perplexity2.978128671646118

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.58s/it]
INFO:root:final mean train loss: 1383.8219417354644
INFO:root:final train perplexity: 2.978337287902832
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2316.018453395113
INFO:root:eval perplexity: 6.508179187774658
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/169
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/200 [9:18:30<1:42:23, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1389.1212768554688
INFO:root:current train perplexity2.9820926189422607
INFO:root:current mean train loss 1380.9119752395984
INFO:root:current train perplexity2.9825544357299805
INFO:root:current mean train loss 1380.7486617144416
INFO:root:current train perplexity2.9730594158172607
INFO:root:current mean train loss 1379.3580722603747
INFO:root:current train perplexity2.9718146324157715
INFO:root:current mean train loss 1382.7863966085142
INFO:root:current train perplexity2.970914840698242
INFO:root:current mean train loss 1380.763292432665
INFO:root:current train perplexity2.9689676761627197
INFO:root:current mean train loss 1380.5173328944616
INFO:root:current train perplexity2.971656560897827
INFO:root:current mean train loss 1380.7422562831423
INFO:root:current train perplexity2.9699933528900146
INFO:root:current mean train loss 1382.2150131365574
INFO:root:current train perplexity2.970580577850342
INFO:root:current mean train loss 1382.4973190998344
INFO:root:current train perplexity2.9720423221588135
INFO:root:current mean train loss 1382.7960635512623
INFO:root:current train perplexity2.971510171890259
INFO:root:current mean train loss 1382.428852172435
INFO:root:current train perplexity2.9734413623809814
INFO:root:current mean train loss 1382.2967110879765
INFO:root:current train perplexity2.9753525257110596
INFO:root:current mean train loss 1382.5569746278813
INFO:root:current train perplexity2.9759931564331055
INFO:root:current mean train loss 1382.8689164700716
INFO:root:current train perplexity2.976372480392456
INFO:root:current mean train loss 1383.5787844281767
INFO:root:current train perplexity2.9771876335144043
INFO:root:current mean train loss 1383.8584378621224
INFO:root:current train perplexity2.9769513607025146
INFO:root:current mean train loss 1383.652620268191
INFO:root:current train perplexity2.976605176925659
INFO:root:current mean train loss 1383.7324370685806
INFO:root:current train perplexity2.977203607559204
INFO:root:current mean train loss 1383.5889115091754
INFO:root:current train perplexity2.9767889976501465

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.36s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.36s/it]
INFO:root:final mean train loss: 1383.075796399523
INFO:root:final train perplexity: 2.9765853881835938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2318.2228852400544
INFO:root:eval perplexity: 6.519791603088379
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/170
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 170/200 [9:21:48<1:39:03, 198.10s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1372.0736509173105
INFO:root:current train perplexity2.9678030014038086
INFO:root:current mean train loss 1377.0468207465278
INFO:root:current train perplexity2.9677679538726807
INFO:root:current mean train loss 1377.822835849643
INFO:root:current train perplexity2.9677774906158447
INFO:root:current mean train loss 1376.969536396409
INFO:root:current train perplexity2.9662818908691406
INFO:root:current mean train loss 1378.9182191314385
INFO:root:current train perplexity2.9730777740478516
INFO:root:current mean train loss 1378.4665802986392
INFO:root:current train perplexity2.9744997024536133
INFO:root:current mean train loss 1379.7155130992608
INFO:root:current train perplexity2.9727628231048584
INFO:root:current mean train loss 1380.3109562514853
INFO:root:current train perplexity2.9696946144104004
INFO:root:current mean train loss 1381.049485437245
INFO:root:current train perplexity2.9715287685394287
INFO:root:current mean train loss 1381.9985189871793
INFO:root:current train perplexity2.97147274017334
INFO:root:current mean train loss 1382.4063181531221
INFO:root:current train perplexity2.9709088802337646
INFO:root:current mean train loss 1381.9230785578413
INFO:root:current train perplexity2.9704830646514893
INFO:root:current mean train loss 1381.4903676200968
INFO:root:current train perplexity2.969776153564453
INFO:root:current mean train loss 1381.4406397292903
INFO:root:current train perplexity2.970522403717041
INFO:root:current mean train loss 1382.0163591434846
INFO:root:current train perplexity2.971508502960205
INFO:root:current mean train loss 1381.9184382866583
INFO:root:current train perplexity2.9713401794433594
INFO:root:current mean train loss 1381.826097866711
INFO:root:current train perplexity2.9723236560821533
INFO:root:current mean train loss 1382.3046494937596
INFO:root:current train perplexity2.972548007965088
INFO:root:current mean train loss 1382.6988440736252
INFO:root:current train perplexity2.9725632667541504

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.77s/it]
INFO:root:final mean train loss: 1381.5821733575726
INFO:root:final train perplexity: 2.97308087348938
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2317.9882587405805
INFO:root:eval perplexity: 6.518556118011475
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/171
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [9:25:06<1:35:46, 198.17s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.9749348958333
INFO:root:current train perplexity2.9736948013305664
INFO:root:current mean train loss 1372.8941477649616
INFO:root:current train perplexity2.965564012527466
INFO:root:current mean train loss 1377.734447886643
INFO:root:current train perplexity2.9641530513763428
INFO:root:current mean train loss 1375.817620969286
INFO:root:current train perplexity2.963524103164673
INFO:root:current mean train loss 1377.635003752309
INFO:root:current train perplexity2.965268611907959
INFO:root:current mean train loss 1377.2590315144053
INFO:root:current train perplexity2.967024803161621
INFO:root:current mean train loss 1377.0988104791925
INFO:root:current train perplexity2.9682633876800537
INFO:root:current mean train loss 1377.039287448267
INFO:root:current train perplexity2.965904474258423
INFO:root:current mean train loss 1377.248292681597
INFO:root:current train perplexity2.9665279388427734
INFO:root:current mean train loss 1377.905271281733
INFO:root:current train perplexity2.9675638675689697
INFO:root:current mean train loss 1379.1869441311117
INFO:root:current train perplexity2.967081069946289
INFO:root:current mean train loss 1379.4149167714456
INFO:root:current train perplexity2.969421625137329
INFO:root:current mean train loss 1379.2093479542393
INFO:root:current train perplexity2.9697322845458984
INFO:root:current mean train loss 1379.8724149321342
INFO:root:current train perplexity2.969498634338379
INFO:root:current mean train loss 1379.459852411262
INFO:root:current train perplexity2.970510482788086
INFO:root:current mean train loss 1379.1453599663844
INFO:root:current train perplexity2.9704244136810303
INFO:root:current mean train loss 1379.1444860246975
INFO:root:current train perplexity2.9711849689483643
INFO:root:current mean train loss 1380.4282258761586
INFO:root:current train perplexity2.971606492996216
INFO:root:current mean train loss 1380.94830470967
INFO:root:current train perplexity2.971973180770874
INFO:root:current mean train loss 1381.140472892449
INFO:root:current train perplexity2.9723267555236816

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1381.1056022701755
INFO:root:final train perplexity: 2.971964120864868
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2320.2888763644173
INFO:root:eval perplexity: 6.530695915222168
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/172
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/200 [9:28:25<1:32:30, 198.23s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.0075046705163
INFO:root:current train perplexity2.975116729736328
INFO:root:current mean train loss 1377.530366727007
INFO:root:current train perplexity2.968949794769287
INFO:root:current mean train loss 1378.1105020976388
INFO:root:current train perplexity2.9730756282806396
INFO:root:current mean train loss 1379.8831560353376
INFO:root:current train perplexity2.9690239429473877
INFO:root:current mean train loss 1378.0983912691156
INFO:root:current train perplexity2.9650659561157227
INFO:root:current mean train loss 1380.0308359412345
INFO:root:current train perplexity2.9665017127990723
INFO:root:current mean train loss 1379.8969904867452
INFO:root:current train perplexity2.9684102535247803
INFO:root:current mean train loss 1380.4535904880381
INFO:root:current train perplexity2.967792510986328
INFO:root:current mean train loss 1381.2772974730408
INFO:root:current train perplexity2.970339059829712
INFO:root:current mean train loss 1381.4321664663462
INFO:root:current train perplexity2.9711432456970215
INFO:root:current mean train loss 1381.7069131174396
INFO:root:current train perplexity2.9700706005096436
INFO:root:current mean train loss 1381.1266776787204
INFO:root:current train perplexity2.969728708267212
INFO:root:current mean train loss 1380.9509603729623
INFO:root:current train perplexity2.969862699508667
INFO:root:current mean train loss 1380.788732919383
INFO:root:current train perplexity2.9711992740631104
INFO:root:current mean train loss 1380.7636711887299
INFO:root:current train perplexity2.970160484313965
INFO:root:current mean train loss 1380.9231168630479
INFO:root:current train perplexity2.9693336486816406
INFO:root:current mean train loss 1380.4221180124337
INFO:root:current train perplexity2.9688451290130615
INFO:root:current mean train loss 1380.3128054237477
INFO:root:current train perplexity2.970064640045166
INFO:root:current mean train loss 1379.9793918126243
INFO:root:current train perplexity2.970048666000366
INFO:root:current mean train loss 1380.175951056597
INFO:root:current train perplexity2.9693968296051025

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it]
INFO:root:final mean train loss: 1380.009793141606
INFO:root:final train perplexity: 2.9693965911865234
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2321.3031776374114
INFO:root:eval perplexity: 6.536055088043213
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/173
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 173/200 [9:31:43<1:29:12, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1387.2038696289062
INFO:root:current train perplexity2.9506964683532715
INFO:root:current mean train loss 1380.8196611676897
INFO:root:current train perplexity2.9700050354003906
INFO:root:current mean train loss 1378.377107747396
INFO:root:current train perplexity2.9635071754455566
INFO:root:current mean train loss 1378.144677734375
INFO:root:current train perplexity2.967149019241333
INFO:root:current mean train loss 1378.892512928356
INFO:root:current train perplexity2.9646050930023193
INFO:root:current mean train loss 1378.0875122070313
INFO:root:current train perplexity2.9650702476501465
INFO:root:current mean train loss 1377.9417688369751
INFO:root:current train perplexity2.967714309692383
INFO:root:current mean train loss 1378.3502977525866
INFO:root:current train perplexity2.9696719646453857
INFO:root:current mean train loss 1378.1504683721632
INFO:root:current train perplexity2.9660727977752686
INFO:root:current mean train loss 1377.409513822515
INFO:root:current train perplexity2.9671175479888916
INFO:root:current mean train loss 1377.7417776254508
INFO:root:current train perplexity2.967074394226074
INFO:root:current mean train loss 1379.1007955986156
INFO:root:current train perplexity2.968485116958618
INFO:root:current mean train loss 1378.556398453251
INFO:root:current train perplexity2.9689416885375977
INFO:root:current mean train loss 1377.44047040797
INFO:root:current train perplexity2.968026876449585
INFO:root:current mean train loss 1378.5505879720051
INFO:root:current train perplexity2.968569040298462
INFO:root:current mean train loss 1378.7729936079545
INFO:root:current train perplexity2.967891216278076
INFO:root:current mean train loss 1378.5605186648486
INFO:root:current train perplexity2.9673593044281006
INFO:root:current mean train loss 1378.557378027905
INFO:root:current train perplexity2.9668397903442383
INFO:root:current mean train loss 1378.8779908553413
INFO:root:current train perplexity2.9666171073913574
INFO:root:current mean train loss 1378.9599465281692
INFO:root:current train perplexity2.966468572616577

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1378.618322267595
INFO:root:final train perplexity: 2.966140031814575
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2320.986376606826
INFO:root:eval perplexity: 6.534379959106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/174
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/200 [9:35:01<1:25:56, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1381.3480395936128
INFO:root:current train perplexity2.927741050720215
INFO:root:current mean train loss 1376.7828353590266
INFO:root:current train perplexity2.9526138305664062
INFO:root:current mean train loss 1375.0668988060859
INFO:root:current train perplexity2.9541897773742676
INFO:root:current mean train loss 1374.1935481223738
INFO:root:current train perplexity2.95625638961792
INFO:root:current mean train loss 1374.6471000688082
INFO:root:current train perplexity2.959050416946411
INFO:root:current mean train loss 1373.2765913848602
INFO:root:current train perplexity2.958983898162842
INFO:root:current mean train loss 1375.488090991248
INFO:root:current train perplexity2.9603781700134277
INFO:root:current mean train loss 1375.6126128465055
INFO:root:current train perplexity2.9600930213928223
INFO:root:current mean train loss 1374.579113812263
INFO:root:current train perplexity2.958468437194824
INFO:root:current mean train loss 1375.4627541409516
INFO:root:current train perplexity2.96036958694458
INFO:root:current mean train loss 1375.3661116182298
INFO:root:current train perplexity2.9614062309265137
INFO:root:current mean train loss 1376.3938531636575
INFO:root:current train perplexity2.9624009132385254
INFO:root:current mean train loss 1376.566368570381
INFO:root:current train perplexity2.962284564971924
INFO:root:current mean train loss 1376.703757210874
INFO:root:current train perplexity2.9613518714904785
INFO:root:current mean train loss 1376.9078286196486
INFO:root:current train perplexity2.9603025913238525
INFO:root:current mean train loss 1377.066762660816
INFO:root:current train perplexity2.9609529972076416
INFO:root:current mean train loss 1377.7187416753497
INFO:root:current train perplexity2.961733341217041
INFO:root:current mean train loss 1377.559491526083
INFO:root:current train perplexity2.9617722034454346
INFO:root:current mean train loss 1377.876690841453
INFO:root:current train perplexity2.9633500576019287
INFO:root:current mean train loss 1377.8137736605574
INFO:root:current train perplexity2.963270664215088

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.75s/it]
INFO:root:final mean train loss: 1377.5120565517348
INFO:root:final train perplexity: 2.9635531902313232
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2322.8910314681684
INFO:root:eval perplexity: 6.5444536209106445
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/175
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 175/200 [9:38:20<1:22:37, 198.31s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1364.9443788270692
INFO:root:current train perplexity2.9506802558898926
INFO:root:current mean train loss 1366.6700250033675
INFO:root:current train perplexity2.9423816204071045
INFO:root:current mean train loss 1365.6014796347513
INFO:root:current train perplexity2.9459664821624756
INFO:root:current mean train loss 1367.6757864722595
INFO:root:current train perplexity2.94918155670166
INFO:root:current mean train loss 1371.6378652838212
INFO:root:current train perplexity2.9516961574554443
INFO:root:current mean train loss 1370.07589732313
INFO:root:current train perplexity2.9484469890594482
INFO:root:current mean train loss 1370.5569193582508
INFO:root:current train perplexity2.95086669921875
INFO:root:current mean train loss 1371.2758986204478
INFO:root:current train perplexity2.951712131500244
INFO:root:current mean train loss 1372.2443895143556
INFO:root:current train perplexity2.9527392387390137
INFO:root:current mean train loss 1373.562401992829
INFO:root:current train perplexity2.953484058380127
INFO:root:current mean train loss 1373.569935036771
INFO:root:current train perplexity2.955793857574463
INFO:root:current mean train loss 1374.7333609013988
INFO:root:current train perplexity2.9567761421203613
INFO:root:current mean train loss 1374.295750496713
INFO:root:current train perplexity2.9575533866882324
INFO:root:current mean train loss 1375.1826974127491
INFO:root:current train perplexity2.958648443222046
INFO:root:current mean train loss 1375.8442328154151
INFO:root:current train perplexity2.958366870880127
INFO:root:current mean train loss 1376.6489681258438
INFO:root:current train perplexity2.958822011947632
INFO:root:current mean train loss 1377.1406395113454
INFO:root:current train perplexity2.959620237350464
INFO:root:current mean train loss 1377.443514474484
INFO:root:current train perplexity2.9600894451141357
INFO:root:current mean train loss 1377.1170740280234
INFO:root:current train perplexity2.9610562324523926
INFO:root:current mean train loss 1377.0052893425072
INFO:root:current train perplexity2.9613823890686035

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.85s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.85s/it]
INFO:root:final mean train loss: 1376.5231617491352
INFO:root:final train perplexity: 2.961242437362671
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2322.950478844609
INFO:root:eval perplexity: 6.544768810272217
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/176
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/200 [9:41:38<1:19:20, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1375.5089755215488
INFO:root:current train perplexity2.945211172103882
INFO:root:current mean train loss 1371.451199356798
INFO:root:current train perplexity2.941951274871826
INFO:root:current mean train loss 1374.5046793619792
INFO:root:current train perplexity2.9517505168914795
INFO:root:current mean train loss 1376.8841693224504
INFO:root:current train perplexity2.950206995010376
INFO:root:current mean train loss 1373.0028851852883
INFO:root:current train perplexity2.9472899436950684
INFO:root:current mean train loss 1371.9003142019617
INFO:root:current train perplexity2.9495809078216553
INFO:root:current mean train loss 1372.7190701033376
INFO:root:current train perplexity2.951637029647827
INFO:root:current mean train loss 1372.2167965663518
INFO:root:current train perplexity2.952655076980591
INFO:root:current mean train loss 1373.066667927101
INFO:root:current train perplexity2.952862024307251
INFO:root:current mean train loss 1373.939413461387
INFO:root:current train perplexity2.9515883922576904
INFO:root:current mean train loss 1374.682984964876
INFO:root:current train perplexity2.952404022216797
INFO:root:current mean train loss 1375.6939167371825
INFO:root:current train perplexity2.9543728828430176
INFO:root:current mean train loss 1375.9953193457711
INFO:root:current train perplexity2.9544119834899902
INFO:root:current mean train loss 1376.05939295513
INFO:root:current train perplexity2.9561290740966797
INFO:root:current mean train loss 1376.1423447914046
INFO:root:current train perplexity2.956692695617676
INFO:root:current mean train loss 1376.1627357621976
INFO:root:current train perplexity2.957216501235962
INFO:root:current mean train loss 1375.8272414159521
INFO:root:current train perplexity2.9570937156677246
INFO:root:current mean train loss 1376.256158178043
INFO:root:current train perplexity2.957934617996216
INFO:root:current mean train loss 1376.1799601731889
INFO:root:current train perplexity2.958995819091797

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.79s/it]
INFO:root:final mean train loss: 1375.7598621485754
INFO:root:final train perplexity: 2.959460735321045
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2323.805991314827
INFO:root:eval perplexity: 6.549299240112305
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/177
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/200 [9:44:57<1:16:02, 198.35s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1381.1414642333984
INFO:root:current train perplexity2.9260969161987305
INFO:root:current mean train loss 1373.4328274197048
INFO:root:current train perplexity2.9590258598327637
INFO:root:current mean train loss 1373.9075499314529
INFO:root:current train perplexity2.953580617904663
INFO:root:current mean train loss 1374.1073703518161
INFO:root:current train perplexity2.9544074535369873
INFO:root:current mean train loss 1373.700507369696
INFO:root:current train perplexity2.953076124191284
INFO:root:current mean train loss 1373.1053555706355
INFO:root:current train perplexity2.9540531635284424
INFO:root:current mean train loss 1373.2157737330388
INFO:root:current train perplexity2.9559950828552246
INFO:root:current mean train loss 1374.671790516309
INFO:root:current train perplexity2.9569149017333984
INFO:root:current mean train loss 1373.8085603619566
INFO:root:current train perplexity2.9560179710388184
INFO:root:current mean train loss 1374.4125955052314
INFO:root:current train perplexity2.957503318786621
INFO:root:current mean train loss 1374.6269118293883
INFO:root:current train perplexity2.957735776901245
INFO:root:current mean train loss 1374.6571629933933
INFO:root:current train perplexity2.956275224685669
INFO:root:current mean train loss 1374.8519825714313
INFO:root:current train perplexity2.9557371139526367
INFO:root:current mean train loss 1374.50328759243
INFO:root:current train perplexity2.955167531967163
INFO:root:current mean train loss 1374.9199517856944
INFO:root:current train perplexity2.9556567668914795
INFO:root:current mean train loss 1375.1952826300098
INFO:root:current train perplexity2.9559335708618164
INFO:root:current mean train loss 1375.7145009966039
INFO:root:current train perplexity2.955585479736328
INFO:root:current mean train loss 1375.6083434058016
INFO:root:current train perplexity2.9569499492645264
INFO:root:current mean train loss 1375.3052442432504
INFO:root:current train perplexity2.9570116996765137
INFO:root:current mean train loss 1375.2896148873574
INFO:root:current train perplexity2.9569454193115234

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.23s/it]
INFO:root:final mean train loss: 1374.6634468544148
INFO:root:final train perplexity: 2.956902265548706
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2324.6681470938606
INFO:root:eval perplexity: 6.5538649559021
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/178
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 178/200 [9:48:15<1:12:46, 198.48s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1366.0873828125
INFO:root:current train perplexity2.949052572250366
INFO:root:current mean train loss 1365.535216796875
INFO:root:current train perplexity2.9338321685791016
INFO:root:current mean train loss 1365.9941004774305
INFO:root:current train perplexity2.9462687969207764
INFO:root:current mean train loss 1369.4221912560097
INFO:root:current train perplexity2.950767993927002
INFO:root:current mean train loss 1370.659757008272
INFO:root:current train perplexity2.9537220001220703
INFO:root:current mean train loss 1369.5887218656994
INFO:root:current train perplexity2.953671455383301
INFO:root:current mean train loss 1370.70478671875
INFO:root:current train perplexity2.9534566402435303
INFO:root:current mean train loss 1370.4424536974677
INFO:root:current train perplexity2.9503049850463867
INFO:root:current mean train loss 1371.223351089015
INFO:root:current train perplexity2.9492201805114746
INFO:root:current mean train loss 1372.0885693359376
INFO:root:current train perplexity2.952392578125
INFO:root:current mean train loss 1372.6614010099086
INFO:root:current train perplexity2.953892469406128
INFO:root:current mean train loss 1373.0727544487847
INFO:root:current train perplexity2.952702522277832
INFO:root:current mean train loss 1373.5217050980548
INFO:root:current train perplexity2.9535865783691406
INFO:root:current mean train loss 1373.279349111881
INFO:root:current train perplexity2.9541501998901367
INFO:root:current mean train loss 1373.3936658785635
INFO:root:current train perplexity2.9549667835235596
INFO:root:current mean train loss 1374.1097234407018
INFO:root:current train perplexity2.9544925689697266
INFO:root:current mean train loss 1374.0012471454327
INFO:root:current train perplexity2.953963279724121
INFO:root:current mean train loss 1373.9825343212183
INFO:root:current train perplexity2.954584836959839
INFO:root:current mean train loss 1374.5176691593535
INFO:root:current train perplexity2.9552547931671143
INFO:root:current mean train loss 1374.096624898539
INFO:root:current train perplexity2.954554557800293

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.59s/it]
INFO:root:final mean train loss: 1373.8319454068076
INFO:root:final train perplexity: 2.9549641609191895
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2326.1828821060503
INFO:root:eval perplexity: 6.561900615692139
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/179
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/200 [9:51:33<1:09:26, 198.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1368.8923397972471
INFO:root:current train perplexity2.925485849380493
INFO:root:current mean train loss 1366.889998315086
INFO:root:current train perplexity2.942871332168579
INFO:root:current mean train loss 1368.7562992316632
INFO:root:current train perplexity2.937650680541992
INFO:root:current mean train loss 1367.650353861134
INFO:root:current train perplexity2.9415581226348877
INFO:root:current mean train loss 1366.4513516663426
INFO:root:current train perplexity2.940439462661743
INFO:root:current mean train loss 1367.718134693554
INFO:root:current train perplexity2.942582368850708
INFO:root:current mean train loss 1367.5255336107866
INFO:root:current train perplexity2.9434592723846436
INFO:root:current mean train loss 1370.0596456604826
INFO:root:current train perplexity2.9474594593048096
INFO:root:current mean train loss 1371.3474996752523
INFO:root:current train perplexity2.948775291442871
INFO:root:current mean train loss 1371.8821501843236
INFO:root:current train perplexity2.9526867866516113
INFO:root:current mean train loss 1372.0606263027082
INFO:root:current train perplexity2.951375961303711
INFO:root:current mean train loss 1372.1108374921328
INFO:root:current train perplexity2.9515128135681152
INFO:root:current mean train loss 1372.4806419286558
INFO:root:current train perplexity2.9517431259155273
INFO:root:current mean train loss 1372.0173808295397
INFO:root:current train perplexity2.9512417316436768
INFO:root:current mean train loss 1372.1524671747682
INFO:root:current train perplexity2.9515233039855957
INFO:root:current mean train loss 1372.3307039926333
INFO:root:current train perplexity2.9509050846099854
INFO:root:current mean train loss 1371.533277839168
INFO:root:current train perplexity2.9498589038848877
INFO:root:current mean train loss 1371.5032969495596
INFO:root:current train perplexity2.9500441551208496
INFO:root:current mean train loss 1371.8051011606356
INFO:root:current train perplexity2.9506828784942627
INFO:root:current mean train loss 1372.6235497393152
INFO:root:current train perplexity2.9521028995513916

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.70s/it]
INFO:root:final mean train loss: 1372.5941013445834
INFO:root:final train perplexity: 2.952080726623535
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2325.4515844033963
INFO:root:eval perplexity: 6.55802059173584
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/180
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 180/200 [9:54:52<1:06:07, 198.36s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1357.001607603946
INFO:root:current train perplexity2.9465901851654053
INFO:root:current mean train loss 1364.643558526189
INFO:root:current train perplexity2.943854570388794
INFO:root:current mean train loss 1369.2328050532396
INFO:root:current train perplexity2.9464855194091797
INFO:root:current mean train loss 1372.7337197646457
INFO:root:current train perplexity2.946345806121826
INFO:root:current mean train loss 1369.416622253285
INFO:root:current train perplexity2.942638397216797
INFO:root:current mean train loss 1370.662015256387
INFO:root:current train perplexity2.943816900253296
INFO:root:current mean train loss 1372.4923921854254
INFO:root:current train perplexity2.9467990398406982
INFO:root:current mean train loss 1371.839115992208
INFO:root:current train perplexity2.9471499919891357
INFO:root:current mean train loss 1372.2297242489906
INFO:root:current train perplexity2.9476613998413086
INFO:root:current mean train loss 1372.7471286109392
INFO:root:current train perplexity2.949343681335449
INFO:root:current mean train loss 1372.9018883205338
INFO:root:current train perplexity2.9506466388702393
INFO:root:current mean train loss 1372.294486360986
INFO:root:current train perplexity2.94973087310791
INFO:root:current mean train loss 1372.9785933854373
INFO:root:current train perplexity2.9493155479431152
INFO:root:current mean train loss 1373.499769961685
INFO:root:current train perplexity2.949551582336426
INFO:root:current mean train loss 1373.3428230520958
INFO:root:current train perplexity2.949230909347534
INFO:root:current mean train loss 1373.076531821906
INFO:root:current train perplexity2.9491422176361084
INFO:root:current mean train loss 1372.8077525023311
INFO:root:current train perplexity2.9494221210479736
INFO:root:current mean train loss 1372.1976719477286
INFO:root:current train perplexity2.950978994369507
INFO:root:current mean train loss 1372.8080958817325
INFO:root:current train perplexity2.9515380859375
INFO:root:current mean train loss 1372.9978578186815
INFO:root:current train perplexity2.952225685119629

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.39s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.39s/it]
INFO:root:final mean train loss: 1372.686736398794
INFO:root:final train perplexity: 2.952296495437622
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2326.6635763831173
INFO:root:eval perplexity: 6.564451217651367
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/181
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [9:58:10<1:02:46, 198.24s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.3199960809004
INFO:root:current train perplexity2.92071795463562
INFO:root:current mean train loss 1371.9291444258256
INFO:root:current train perplexity2.933849573135376
INFO:root:current mean train loss 1369.6358562967052
INFO:root:current train perplexity2.9354352951049805
INFO:root:current mean train loss 1367.883661797706
INFO:root:current train perplexity2.936565399169922
INFO:root:current mean train loss 1369.209463936942
INFO:root:current train perplexity2.9373302459716797
INFO:root:current mean train loss 1371.434782240126
INFO:root:current train perplexity2.943114757537842
INFO:root:current mean train loss 1371.4897016717132
INFO:root:current train perplexity2.9428658485412598
INFO:root:current mean train loss 1371.2390709316608
INFO:root:current train perplexity2.942260265350342
INFO:root:current mean train loss 1372.4650509629619
INFO:root:current train perplexity2.9428117275238037
INFO:root:current mean train loss 1372.3120177222079
INFO:root:current train perplexity2.945956230163574
INFO:root:current mean train loss 1372.0718484758002
INFO:root:current train perplexity2.947181224822998
INFO:root:current mean train loss 1372.3348101142312
INFO:root:current train perplexity2.946607828140259
INFO:root:current mean train loss 1371.2464494376347
INFO:root:current train perplexity2.946155548095703
INFO:root:current mean train loss 1371.9960323599882
INFO:root:current train perplexity2.9463412761688232
INFO:root:current mean train loss 1371.6265266232374
INFO:root:current train perplexity2.9471547603607178
INFO:root:current mean train loss 1372.249158055649
INFO:root:current train perplexity2.9472875595092773
INFO:root:current mean train loss 1372.044945910324
INFO:root:current train perplexity2.9471523761749268
INFO:root:current mean train loss 1371.8632395288967
INFO:root:current train perplexity2.94815731048584
INFO:root:current mean train loss 1372.1082478016933
INFO:root:current train perplexity2.949282169342041
INFO:root:current mean train loss 1371.8137969352938
INFO:root:current train perplexity2.949331283569336

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1371.3903267803182
INFO:root:final train perplexity: 2.949279308319092
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2327.293727144282
INFO:root:eval perplexity: 6.567795753479004
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/182
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/200 [10:01:28<59:29, 198.31s/it] 
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.9146111601142
INFO:root:current train perplexity2.938796043395996
INFO:root:current mean train loss 1369.313105291653
INFO:root:current train perplexity2.9363996982574463
INFO:root:current mean train loss 1371.627562643318
INFO:root:current train perplexity2.939321756362915
INFO:root:current mean train loss 1368.404530765446
INFO:root:current train perplexity2.938549041748047
INFO:root:current mean train loss 1366.6526492971918
INFO:root:current train perplexity2.94008469581604
INFO:root:current mean train loss 1367.2294100525005
INFO:root:current train perplexity2.940781831741333
INFO:root:current mean train loss 1367.122746895574
INFO:root:current train perplexity2.9403023719787598
INFO:root:current mean train loss 1365.750411929579
INFO:root:current train perplexity2.9380407333374023
INFO:root:current mean train loss 1366.0788745089849
INFO:root:current train perplexity2.9408063888549805
INFO:root:current mean train loss 1366.3185196768002
INFO:root:current train perplexity2.940760612487793
INFO:root:current mean train loss 1366.7162410250958
INFO:root:current train perplexity2.940472364425659
INFO:root:current mean train loss 1366.9860514459347
INFO:root:current train perplexity2.941821575164795
INFO:root:current mean train loss 1367.388538947699
INFO:root:current train perplexity2.942823648452759
INFO:root:current mean train loss 1367.745977813874
INFO:root:current train perplexity2.944488763809204
INFO:root:current mean train loss 1368.538344304672
INFO:root:current train perplexity2.9459049701690674
INFO:root:current mean train loss 1369.758883852818
INFO:root:current train perplexity2.9455602169036865
INFO:root:current mean train loss 1370.0348838450143
INFO:root:current train perplexity2.9452919960021973
INFO:root:current mean train loss 1369.8327175806305
INFO:root:current train perplexity2.945570707321167
INFO:root:current mean train loss 1370.021676734082
INFO:root:current train perplexity2.945765733718872

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.84s/it]
INFO:root:final mean train loss: 1370.4224143131657
INFO:root:final train perplexity: 2.9470291137695312
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2325.967495532746
INFO:root:eval perplexity: 6.560756206512451
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/183
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 183/200 [10:04:47<56:11, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1384.5177368164063
INFO:root:current train perplexity2.9032580852508545
INFO:root:current mean train loss 1372.6121315696023
INFO:root:current train perplexity2.931751251220703
INFO:root:current mean train loss 1374.3093128022692
INFO:root:current train perplexity2.936415672302246
INFO:root:current mean train loss 1373.9299517231602
INFO:root:current train perplexity2.9369895458221436
INFO:root:current mean train loss 1371.734151105183
INFO:root:current train perplexity2.9337868690490723
INFO:root:current mean train loss 1370.5068227730546
INFO:root:current train perplexity2.938793897628784
INFO:root:current mean train loss 1370.197150758837
INFO:root:current train perplexity2.9431793689727783
INFO:root:current mean train loss 1370.4645844795334
INFO:root:current train perplexity2.9421043395996094
INFO:root:current mean train loss 1368.916266396605
INFO:root:current train perplexity2.941575050354004
INFO:root:current mean train loss 1369.7715514466004
INFO:root:current train perplexity2.9408488273620605
INFO:root:current mean train loss 1370.568792543317
INFO:root:current train perplexity2.9417402744293213
INFO:root:current mean train loss 1370.8928534980294
INFO:root:current train perplexity2.9419772624969482
INFO:root:current mean train loss 1371.1295476740056
INFO:root:current train perplexity2.942371368408203
INFO:root:current mean train loss 1370.7307812872734
INFO:root:current train perplexity2.9433064460754395
INFO:root:current mean train loss 1370.9731678198416
INFO:root:current train perplexity2.943620443344116
INFO:root:current mean train loss 1370.6535983256156
INFO:root:current train perplexity2.944009304046631
INFO:root:current mean train loss 1370.8823321040372
INFO:root:current train perplexity2.944382429122925
INFO:root:current mean train loss 1370.637986853527
INFO:root:current train perplexity2.9447243213653564
INFO:root:current mean train loss 1370.388675584319
INFO:root:current train perplexity2.9455811977386475
INFO:root:current mean train loss 1370.3863775283255
INFO:root:current train perplexity2.946086883544922

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.90s/it]
INFO:root:final mean train loss: 1370.2540661171718
INFO:root:final train perplexity: 2.9466376304626465
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2328.1394986632868
INFO:root:eval perplexity: 6.572289943695068
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/184
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/200 [10:08:05<52:53, 198.37s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1365.0611572265625
INFO:root:current train perplexity2.9303839206695557
INFO:root:current mean train loss 1359.452904888964
INFO:root:current train perplexity2.927722930908203
INFO:root:current mean train loss 1362.2851368908316
INFO:root:current train perplexity2.9238674640655518
INFO:root:current mean train loss 1361.2309197008792
INFO:root:current train perplexity2.9327688217163086
INFO:root:current mean train loss 1363.294361266375
INFO:root:current train perplexity2.9329941272735596
INFO:root:current mean train loss 1365.6735619792903
INFO:root:current train perplexity2.936305522918701
INFO:root:current mean train loss 1365.529024309709
INFO:root:current train perplexity2.938992500305176
INFO:root:current mean train loss 1365.436968565971
INFO:root:current train perplexity2.9367361068725586
INFO:root:current mean train loss 1365.5213200893195
INFO:root:current train perplexity2.93745756149292
INFO:root:current mean train loss 1366.165235533812
INFO:root:current train perplexity2.9367153644561768
INFO:root:current mean train loss 1366.329444476669
INFO:root:current train perplexity2.937657117843628
INFO:root:current mean train loss 1366.5697027983238
INFO:root:current train perplexity2.9377353191375732
INFO:root:current mean train loss 1366.2196326469539
INFO:root:current train perplexity2.9377923011779785
INFO:root:current mean train loss 1366.9741203578326
INFO:root:current train perplexity2.9385221004486084
INFO:root:current mean train loss 1367.7174931975956
INFO:root:current train perplexity2.93942928314209
INFO:root:current mean train loss 1367.5111534853113
INFO:root:current train perplexity2.939887523651123
INFO:root:current mean train loss 1367.4531953010958
INFO:root:current train perplexity2.9398856163024902
INFO:root:current mean train loss 1367.9974339788334
INFO:root:current train perplexity2.940744400024414
INFO:root:current mean train loss 1368.3100035385023
INFO:root:current train perplexity2.941741704940796
INFO:root:current mean train loss 1368.6821483538815
INFO:root:current train perplexity2.9424328804016113

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.83s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.83s/it]
INFO:root:final mean train loss: 1368.6385030510808
INFO:root:final train perplexity: 2.942885637283325
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2328.032758996842
INFO:root:eval perplexity: 6.5717244148254395
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/185
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 185/200 [10:11:23<49:35, 198.38s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1370.751708984375
INFO:root:current train perplexity2.921783924102783
INFO:root:current mean train loss 1366.323757595486
INFO:root:current train perplexity2.939134359359741
INFO:root:current mean train loss 1368.6057384053215
INFO:root:current train perplexity2.9405176639556885
INFO:root:current mean train loss 1367.359673788381
INFO:root:current train perplexity2.935472011566162
INFO:root:current mean train loss 1366.0024449803807
INFO:root:current train perplexity2.9372692108154297
INFO:root:current mean train loss 1365.7684673982508
INFO:root:current train perplexity2.938382148742676
INFO:root:current mean train loss 1364.6605867184467
INFO:root:current train perplexity2.9373042583465576
INFO:root:current mean train loss 1365.4529036655222
INFO:root:current train perplexity2.939319133758545
INFO:root:current mean train loss 1366.1438721570923
INFO:root:current train perplexity2.9400553703308105
INFO:root:current mean train loss 1367.6596382270425
INFO:root:current train perplexity2.9412002563476562
INFO:root:current mean train loss 1367.9730813914332
INFO:root:current train perplexity2.942061424255371
INFO:root:current mean train loss 1368.305963476221
INFO:root:current train perplexity2.941924571990967
INFO:root:current mean train loss 1368.4823390555919
INFO:root:current train perplexity2.9422669410705566
INFO:root:current mean train loss 1368.2751825423468
INFO:root:current train perplexity2.9411303997039795
INFO:root:current mean train loss 1368.0070881936028
INFO:root:current train perplexity2.940925359725952
INFO:root:current mean train loss 1368.0482651310263
INFO:root:current train perplexity2.941293716430664
INFO:root:current mean train loss 1368.0666716267012
INFO:root:current train perplexity2.940378427505493
INFO:root:current mean train loss 1367.7388344860951
INFO:root:current train perplexity2.940505027770996
INFO:root:current mean train loss 1367.8295797153564
INFO:root:current train perplexity2.93949818611145
INFO:root:current mean train loss 1368.2554031811624
INFO:root:current train perplexity2.9408090114593506

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:09<00:00, 189.06s/it]
INFO:root:final mean train loss: 1367.836147998958
INFO:root:final train perplexity: 2.9410243034362793
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.27s/it]
INFO:root:eval mean loss: 2328.7680157600566
INFO:root:eval perplexity: 6.575632572174072
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/186
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/200 [10:14:42<46:18, 198.45s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1360.9140885149845
INFO:root:current train perplexity2.9348976612091064
INFO:root:current mean train loss 1370.8093527088995
INFO:root:current train perplexity2.9276115894317627
INFO:root:current mean train loss 1369.1657055383441
INFO:root:current train perplexity2.9370734691619873
INFO:root:current mean train loss 1367.852133626753
INFO:root:current train perplexity2.939145565032959
INFO:root:current mean train loss 1369.5371218203463
INFO:root:current train perplexity2.940279722213745
INFO:root:current mean train loss 1368.8540115220449
INFO:root:current train perplexity2.941908597946167
INFO:root:current mean train loss 1371.2846504246052
INFO:root:current train perplexity2.9427225589752197
INFO:root:current mean train loss 1369.4562957803773
INFO:root:current train perplexity2.9406354427337646
INFO:root:current mean train loss 1368.4575894274917
INFO:root:current train perplexity2.9425268173217773
INFO:root:current mean train loss 1368.1763622589588
INFO:root:current train perplexity2.940582513809204
INFO:root:current mean train loss 1369.2763208214906
INFO:root:current train perplexity2.940850257873535
INFO:root:current mean train loss 1369.0028299073738
INFO:root:current train perplexity2.940882921218872
INFO:root:current mean train loss 1368.6310572431355
INFO:root:current train perplexity2.9401628971099854
INFO:root:current mean train loss 1368.8029079283099
INFO:root:current train perplexity2.939816951751709
INFO:root:current mean train loss 1369.1421759864552
INFO:root:current train perplexity2.940411329269409
INFO:root:current mean train loss 1369.204803486347
INFO:root:current train perplexity2.942002773284912
INFO:root:current mean train loss 1368.8129484485532
INFO:root:current train perplexity2.9426660537719727
INFO:root:current mean train loss 1368.9912895449584
INFO:root:current train perplexity2.942368745803833
INFO:root:current mean train loss 1369.1968722581735
INFO:root:current train perplexity2.942272186279297
INFO:root:current mean train loss 1368.900499685269
INFO:root:current train perplexity2.9428443908691406

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.30s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.31s/it]
INFO:root:final mean train loss: 1368.5369194673278
INFO:root:final train perplexity: 2.942650079727173
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2329.2999999134254
INFO:root:eval perplexity: 6.57846212387085
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/187
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/200 [10:18:00<42:57, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1369.9006958007812
INFO:root:current train perplexity2.9242870807647705
INFO:root:current mean train loss 1367.1549051691977
INFO:root:current train perplexity2.927483081817627
INFO:root:current mean train loss 1369.1267954874381
INFO:root:current train perplexity2.9258339405059814
INFO:root:current mean train loss 1371.159529105696
INFO:root:current train perplexity2.9286279678344727
INFO:root:current mean train loss 1372.703470014628
INFO:root:current train perplexity2.9318270683288574
INFO:root:current mean train loss 1370.9793230208559
INFO:root:current train perplexity2.9353458881378174
INFO:root:current mean train loss 1370.1392085882767
INFO:root:current train perplexity2.937164306640625
INFO:root:current mean train loss 1368.6993685920932
INFO:root:current train perplexity2.935586452484131
INFO:root:current mean train loss 1368.430713140883
INFO:root:current train perplexity2.9355900287628174
INFO:root:current mean train loss 1367.8062472041156
INFO:root:current train perplexity2.93562388420105
INFO:root:current mean train loss 1368.3072310467155
INFO:root:current train perplexity2.937577486038208
INFO:root:current mean train loss 1367.6916351577422
INFO:root:current train perplexity2.9378442764282227
INFO:root:current mean train loss 1366.7111728530908
INFO:root:current train perplexity2.9367730617523193
INFO:root:current mean train loss 1367.0898285133571
INFO:root:current train perplexity2.9378437995910645
INFO:root:current mean train loss 1367.4808005202608
INFO:root:current train perplexity2.9389595985412598
INFO:root:current mean train loss 1367.587229062698
INFO:root:current train perplexity2.938375473022461
INFO:root:current mean train loss 1367.6528620759693
INFO:root:current train perplexity2.9384355545043945
INFO:root:current mean train loss 1367.6683984677086
INFO:root:current train perplexity2.93766188621521
INFO:root:current mean train loss 1367.6339433728951
INFO:root:current train perplexity2.938565731048584
INFO:root:current mean train loss 1367.5590217983759
INFO:root:current train perplexity2.9396274089813232

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.82s/it]
INFO:root:final mean train loss: 1367.2256752895214
INFO:root:final train perplexity: 2.939608573913574
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2330.3675017661235
INFO:root:eval perplexity: 6.584144592285156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/188
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 188/200 [10:21:18<39:39, 198.30s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1366.0485120271383
INFO:root:current train perplexity2.934950351715088
INFO:root:current mean train loss 1363.5822159204727
INFO:root:current train perplexity2.929184675216675
INFO:root:current mean train loss 1364.6616608183263
INFO:root:current train perplexity2.936290979385376
INFO:root:current mean train loss 1364.111772831784
INFO:root:current train perplexity2.936572790145874
INFO:root:current mean train loss 1362.6023622455018
INFO:root:current train perplexity2.933885335922241
INFO:root:current mean train loss 1363.3180643546482
INFO:root:current train perplexity2.9373769760131836
INFO:root:current mean train loss 1363.3874469564973
INFO:root:current train perplexity2.93355393409729
INFO:root:current mean train loss 1362.7239494275748
INFO:root:current train perplexity2.9318602085113525
INFO:root:current mean train loss 1362.5572361098987
INFO:root:current train perplexity2.932058334350586
INFO:root:current mean train loss 1363.355011507734
INFO:root:current train perplexity2.9320995807647705
INFO:root:current mean train loss 1363.3814728479952
INFO:root:current train perplexity2.932199001312256
INFO:root:current mean train loss 1364.4849805504707
INFO:root:current train perplexity2.9347331523895264
INFO:root:current mean train loss 1364.4036096050013
INFO:root:current train perplexity2.935882329940796
INFO:root:current mean train loss 1365.078477560064
INFO:root:current train perplexity2.9374799728393555
INFO:root:current mean train loss 1364.6072025566993
INFO:root:current train perplexity2.9366214275360107
INFO:root:current mean train loss 1364.7469807687598
INFO:root:current train perplexity2.9359467029571533
INFO:root:current mean train loss 1364.9599224799501
INFO:root:current train perplexity2.935408353805542
INFO:root:current mean train loss 1365.8287877159862
INFO:root:current train perplexity2.935241460800171
INFO:root:current mean train loss 1366.018543350099
INFO:root:current train perplexity2.9357666969299316

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.64s/it]
INFO:root:final mean train loss: 1365.8872800395156
INFO:root:final train perplexity: 2.9365074634552
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2330.6060059459496
INFO:root:eval perplexity: 6.585414409637451
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/189
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/200 [10:24:37<36:21, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.8118998209636
INFO:root:current train perplexity2.915381669998169
INFO:root:current mean train loss 1354.7855824061803
INFO:root:current train perplexity2.917632818222046
INFO:root:current mean train loss 1360.6760374824955
INFO:root:current train perplexity2.9363033771514893
INFO:root:current mean train loss 1364.0324006691958
INFO:root:current train perplexity2.9345667362213135
INFO:root:current mean train loss 1364.717041015625
INFO:root:current train perplexity2.9318599700927734
INFO:root:current mean train loss 1364.3355400562286
INFO:root:current train perplexity2.933000087738037
INFO:root:current mean train loss 1364.3467321458206
INFO:root:current train perplexity2.9329240322113037
INFO:root:current mean train loss 1364.8924512541696
INFO:root:current train perplexity2.9324705600738525
INFO:root:current mean train loss 1364.605699961996
INFO:root:current train perplexity2.9316725730895996
INFO:root:current mean train loss 1364.4379548189934
INFO:root:current train perplexity2.9328625202178955
INFO:root:current mean train loss 1365.5147445739021
INFO:root:current train perplexity2.9328620433807373
INFO:root:current mean train loss 1365.5666725652682
INFO:root:current train perplexity2.933460235595703
INFO:root:current mean train loss 1367.1049808716223
INFO:root:current train perplexity2.9339640140533447
INFO:root:current mean train loss 1366.9596197546982
INFO:root:current train perplexity2.935256242752075
INFO:root:current mean train loss 1366.7322565786562
INFO:root:current train perplexity2.936265230178833
INFO:root:current mean train loss 1367.1505100310794
INFO:root:current train perplexity2.9371182918548584
INFO:root:current mean train loss 1367.5040016647604
INFO:root:current train perplexity2.9371047019958496
INFO:root:current mean train loss 1367.0648380172586
INFO:root:current train perplexity2.9384407997131348
INFO:root:current mean train loss 1366.7374943950056
INFO:root:current train perplexity2.937631607055664
INFO:root:current mean train loss 1366.739208780073
INFO:root:current train perplexity2.9370529651641846

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.76s/it]
INFO:root:final mean train loss: 1366.160767925068
INFO:root:final train perplexity: 2.937140703201294
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2330.2506133816764
INFO:root:eval perplexity: 6.583522319793701
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/190
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 190/200 [10:27:55<33:03, 198.32s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1355.0441641971984
INFO:root:current train perplexity2.9420995712280273
INFO:root:current mean train loss 1365.3126296405644
INFO:root:current train perplexity2.928600549697876
INFO:root:current mean train loss 1368.9258158987786
INFO:root:current train perplexity2.928600788116455
INFO:root:current mean train loss 1366.5448161302004
INFO:root:current train perplexity2.9273550510406494
INFO:root:current mean train loss 1366.032319893648
INFO:root:current train perplexity2.9328463077545166
INFO:root:current mean train loss 1365.3998481159174
INFO:root:current train perplexity2.9341769218444824
INFO:root:current mean train loss 1364.7188513047745
INFO:root:current train perplexity2.9357569217681885
INFO:root:current mean train loss 1364.4584848746679
INFO:root:current train perplexity2.9315576553344727
INFO:root:current mean train loss 1365.0121849437387
INFO:root:current train perplexity2.9339821338653564
INFO:root:current mean train loss 1365.0198434556646
INFO:root:current train perplexity2.9358198642730713
INFO:root:current mean train loss 1364.675735340174
INFO:root:current train perplexity2.935763359069824
INFO:root:current mean train loss 1364.7923852128197
INFO:root:current train perplexity2.936032772064209
INFO:root:current mean train loss 1364.0142107126285
INFO:root:current train perplexity2.9353740215301514
INFO:root:current mean train loss 1364.1956286520058
INFO:root:current train perplexity2.934615135192871
INFO:root:current mean train loss 1365.1062960774852
INFO:root:current train perplexity2.934628963470459
INFO:root:current mean train loss 1365.3081647075805
INFO:root:current train perplexity2.9337575435638428
INFO:root:current mean train loss 1365.5229724488279
INFO:root:current train perplexity2.9340555667877197
INFO:root:current mean train loss 1365.5418223904487
INFO:root:current train perplexity2.934889554977417
INFO:root:current mean train loss 1365.6086113430752
INFO:root:current train perplexity2.9347543716430664
INFO:root:current mean train loss 1365.507692011729
INFO:root:current train perplexity2.934488296508789

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it]
INFO:root:final mean train loss: 1365.197727928604
INFO:root:final train perplexity: 2.934910535812378
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2330.555078384724
INFO:root:eval perplexity: 6.585143566131592
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/191
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [10:31:13<29:44, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1371.0418409264605
INFO:root:current train perplexity2.939260244369507
INFO:root:current mean train loss 1359.756784099422
INFO:root:current train perplexity2.934110403060913
INFO:root:current mean train loss 1361.0817350061927
INFO:root:current train perplexity2.9356324672698975
INFO:root:current mean train loss 1360.3214446492277
INFO:root:current train perplexity2.937579870223999
INFO:root:current mean train loss 1363.1873338639468
INFO:root:current train perplexity2.934962749481201
INFO:root:current mean train loss 1363.3815473060467
INFO:root:current train perplexity2.938203811645508
INFO:root:current mean train loss 1364.6207914086688
INFO:root:current train perplexity2.935511350631714
INFO:root:current mean train loss 1365.1974241854998
INFO:root:current train perplexity2.933154821395874
INFO:root:current mean train loss 1363.7507924469933
INFO:root:current train perplexity2.9314677715301514
INFO:root:current mean train loss 1364.759133982104
INFO:root:current train perplexity2.9304072856903076
INFO:root:current mean train loss 1364.8451877688815
INFO:root:current train perplexity2.9307782649993896
INFO:root:current mean train loss 1365.61308759919
INFO:root:current train perplexity2.931304931640625
INFO:root:current mean train loss 1366.008111895566
INFO:root:current train perplexity2.9311630725860596
INFO:root:current mean train loss 1365.882527911114
INFO:root:current train perplexity2.931884288787842
INFO:root:current mean train loss 1366.2070339514178
INFO:root:current train perplexity2.932436943054199
INFO:root:current mean train loss 1365.996737501137
INFO:root:current train perplexity2.9319636821746826
INFO:root:current mean train loss 1365.922910891935
INFO:root:current train perplexity2.9311203956604004
INFO:root:current mean train loss 1365.9295022971032
INFO:root:current train perplexity2.931532621383667
INFO:root:current mean train loss 1365.7747843071802
INFO:root:current train perplexity2.931962728500366
INFO:root:current mean train loss 1365.3347507959033
INFO:root:current train perplexity2.9335272312164307

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.60s/it]
INFO:root:final mean train loss: 1364.6831845712975
INFO:root:final train perplexity: 2.933720111846924
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2330.548148080812
INFO:root:eval perplexity: 6.585105895996094
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/192
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/200 [10:34:31<26:26, 198.25s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1376.6171487475199
INFO:root:current train perplexity2.94130539894104
INFO:root:current mean train loss 1369.4993956396186
INFO:root:current train perplexity2.944002151489258
INFO:root:current mean train loss 1371.0085825176748
INFO:root:current train perplexity2.93915057182312
INFO:root:current mean train loss 1368.4868900519757
INFO:root:current train perplexity2.9380745887756348
INFO:root:current mean train loss 1367.5896378387386
INFO:root:current train perplexity2.935126781463623
INFO:root:current mean train loss 1366.3492845769038
INFO:root:current train perplexity2.9341373443603516
INFO:root:current mean train loss 1365.7238274253511
INFO:root:current train perplexity2.9332187175750732
INFO:root:current mean train loss 1367.7844723042779
INFO:root:current train perplexity2.933880090713501
INFO:root:current mean train loss 1367.9429942956529
INFO:root:current train perplexity2.9337568283081055
INFO:root:current mean train loss 1367.1251500843719
INFO:root:current train perplexity2.9345908164978027
INFO:root:current mean train loss 1366.2178604829346
INFO:root:current train perplexity2.936689853668213
INFO:root:current mean train loss 1365.9877293620352
INFO:root:current train perplexity2.93609881401062
INFO:root:current mean train loss 1366.0766875085053
INFO:root:current train perplexity2.93512225151062
INFO:root:current mean train loss 1365.5173508216594
INFO:root:current train perplexity2.934399366378784
INFO:root:current mean train loss 1365.292582847611
INFO:root:current train perplexity2.9329278469085693
INFO:root:current mean train loss 1365.3006516039868
INFO:root:current train perplexity2.9323019981384277
INFO:root:current mean train loss 1365.5667702588273
INFO:root:current train perplexity2.932633399963379
INFO:root:current mean train loss 1365.1874426691895
INFO:root:current train perplexity2.9323933124542236
INFO:root:current mean train loss 1364.4450382840805
INFO:root:current train perplexity2.9314939975738525
INFO:root:current mean train loss 1364.104771338612
INFO:root:current train perplexity2.93174147605896

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.45s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.46s/it]
INFO:root:final mean train loss: 1363.8829706127572
INFO:root:final train perplexity: 2.9318692684173584
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2331.5131377091643
INFO:root:eval perplexity: 6.590248107910156
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/193
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 193/200 [10:37:49<23:07, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1363.9620315551758
INFO:root:current train perplexity2.9189789295196533
INFO:root:current mean train loss 1362.240840657552
INFO:root:current train perplexity2.924596071243286
INFO:root:current mean train loss 1366.009323992048
INFO:root:current train perplexity2.92344331741333
INFO:root:current mean train loss 1365.0711441843134
INFO:root:current train perplexity2.9235167503356934
INFO:root:current mean train loss 1366.9635485331216
INFO:root:current train perplexity2.931428909301758
INFO:root:current mean train loss 1366.825942887931
INFO:root:current train perplexity2.9321401119232178
INFO:root:current mean train loss 1365.6898898853974
INFO:root:current train perplexity2.9304423332214355
INFO:root:current mean train loss 1364.5842815692608
INFO:root:current train perplexity2.9291915893554688
INFO:root:current mean train loss 1365.2112737482244
INFO:root:current train perplexity2.9298362731933594
INFO:root:current mean train loss 1364.9562663175623
INFO:root:current train perplexity2.9297704696655273
INFO:root:current mean train loss 1365.4196260805484
INFO:root:current train perplexity2.930579662322998
INFO:root:current mean train loss 1364.5635637703588
INFO:root:current train perplexity2.932185173034668
INFO:root:current mean train loss 1364.972685432434
INFO:root:current train perplexity2.931061029434204
INFO:root:current mean train loss 1365.026527648041
INFO:root:current train perplexity2.932076930999756
INFO:root:current mean train loss 1365.1993388407939
INFO:root:current train perplexity2.931757688522339
INFO:root:current mean train loss 1365.01464588793
INFO:root:current train perplexity2.9317336082458496
INFO:root:current mean train loss 1365.11201898484
INFO:root:current train perplexity2.9313764572143555
INFO:root:current mean train loss 1364.8883773975158
INFO:root:current train perplexity2.9318010807037354
INFO:root:current mean train loss 1364.8822898053108
INFO:root:current train perplexity2.932065725326538
INFO:root:current mean train loss 1364.379771962792
INFO:root:current train perplexity2.932028293609619

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.91s/it]
INFO:root:final mean train loss: 1363.9694252956774
INFO:root:final train perplexity: 2.9320693016052246
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2331.5155908029974
INFO:root:eval perplexity: 6.590260028839111
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/194
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/200 [10:41:08<19:49, 198.27s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1367.4614169720521
INFO:root:current train perplexity2.9275333881378174
INFO:root:current mean train loss 1367.5661143966133
INFO:root:current train perplexity2.923696279525757
INFO:root:current mean train loss 1361.8809929536249
INFO:root:current train perplexity2.92824649810791
INFO:root:current mean train loss 1363.662391335898
INFO:root:current train perplexity2.925808906555176
INFO:root:current mean train loss 1363.0315587371888
INFO:root:current train perplexity2.925018548965454
INFO:root:current mean train loss 1361.870166874411
INFO:root:current train perplexity2.9243156909942627
INFO:root:current mean train loss 1362.9200245051336
INFO:root:current train perplexity2.924579620361328
INFO:root:current mean train loss 1363.2611182008213
INFO:root:current train perplexity2.9262115955352783
INFO:root:current mean train loss 1362.5091637108503
INFO:root:current train perplexity2.926466464996338
INFO:root:current mean train loss 1363.0898848890422
INFO:root:current train perplexity2.9263110160827637
INFO:root:current mean train loss 1363.0264973809965
INFO:root:current train perplexity2.927518367767334
INFO:root:current mean train loss 1362.7513533793795
INFO:root:current train perplexity2.9281156063079834
INFO:root:current mean train loss 1362.2596860957678
INFO:root:current train perplexity2.9285683631896973
INFO:root:current mean train loss 1362.8345328612932
INFO:root:current train perplexity2.9285054206848145
INFO:root:current mean train loss 1363.2025100004696
INFO:root:current train perplexity2.9291000366210938
INFO:root:current mean train loss 1363.2093013603387
INFO:root:current train perplexity2.9283292293548584
INFO:root:current mean train loss 1363.66355292658
INFO:root:current train perplexity2.929755210876465
INFO:root:current mean train loss 1363.3670376462942
INFO:root:current train perplexity2.930461883544922
INFO:root:current mean train loss 1363.624143770386
INFO:root:current train perplexity2.9304890632629395

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1363.171574594995
INFO:root:final train perplexity: 2.930224657058716
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2331.6095096236427
INFO:root:eval perplexity: 6.590762138366699
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/195
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 195/200 [10:44:26<16:31, 198.34s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1352.6680210658483
INFO:root:current train perplexity2.927340030670166
INFO:root:current mean train loss 1364.2282158031799
INFO:root:current train perplexity2.919523239135742
INFO:root:current mean train loss 1361.9977559597692
INFO:root:current train perplexity2.921876907348633
INFO:root:current mean train loss 1360.4164716392565
INFO:root:current train perplexity2.9269704818725586
INFO:root:current mean train loss 1361.0498182508682
INFO:root:current train perplexity2.92983341217041
INFO:root:current mean train loss 1361.51728411129
INFO:root:current train perplexity2.9282102584838867
INFO:root:current mean train loss 1362.6731565394696
INFO:root:current train perplexity2.931739330291748
INFO:root:current mean train loss 1363.1251448089001
INFO:root:current train perplexity2.932565927505493
INFO:root:current mean train loss 1364.2406856152584
INFO:root:current train perplexity2.93174409866333
INFO:root:current mean train loss 1364.4297335768686
INFO:root:current train perplexity2.9331488609313965
INFO:root:current mean train loss 1364.4708114714313
INFO:root:current train perplexity2.9341228008270264
INFO:root:current mean train loss 1363.8215089863036
INFO:root:current train perplexity2.9330456256866455
INFO:root:current mean train loss 1363.5453001521958
INFO:root:current train perplexity2.931203842163086
INFO:root:current mean train loss 1363.4836779729417
INFO:root:current train perplexity2.9312031269073486
INFO:root:current mean train loss 1363.6990061197457
INFO:root:current train perplexity2.93113112449646
INFO:root:current mean train loss 1363.845107689559
INFO:root:current train perplexity2.929211378097534
INFO:root:current mean train loss 1363.6033233680394
INFO:root:current train perplexity2.930711507797241
INFO:root:current mean train loss 1363.2607538675065
INFO:root:current train perplexity2.9311420917510986
INFO:root:current mean train loss 1363.2469635850935
INFO:root:current train perplexity2.9301693439483643
INFO:root:current mean train loss 1363.0291080933255
INFO:root:current train perplexity2.930096387863159

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.96s/it]
INFO:root:final mean train loss: 1362.800447665016
INFO:root:final train perplexity: 2.9293673038482666
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2331.8457308289007
INFO:root:eval perplexity: 6.5920209884643555
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/196
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/200 [10:47:45<13:13, 198.39s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1365.3861005229335
INFO:root:current train perplexity2.9164774417877197
INFO:root:current mean train loss 1372.7040065153865
INFO:root:current train perplexity2.9365921020507812
INFO:root:current mean train loss 1367.0117789924918
INFO:root:current train perplexity2.9419491291046143
INFO:root:current mean train loss 1365.4658114614804
INFO:root:current train perplexity2.9304986000061035
INFO:root:current mean train loss 1366.4275205395338
INFO:root:current train perplexity2.9288156032562256
INFO:root:current mean train loss 1367.8829352599753
INFO:root:current train perplexity2.933694362640381
INFO:root:current mean train loss 1368.4549133010598
INFO:root:current train perplexity2.9341859817504883
INFO:root:current mean train loss 1365.6270818749465
INFO:root:current train perplexity2.934321165084839
INFO:root:current mean train loss 1364.6132180848563
INFO:root:current train perplexity2.934406042098999
INFO:root:current mean train loss 1363.2705596038786
INFO:root:current train perplexity2.9325454235076904
INFO:root:current mean train loss 1363.214363756744
INFO:root:current train perplexity2.9338276386260986
INFO:root:current mean train loss 1363.1148807920258
INFO:root:current train perplexity2.934837818145752
INFO:root:current mean train loss 1363.9184236131384
INFO:root:current train perplexity2.9350435733795166
INFO:root:current mean train loss 1364.3306063015061
INFO:root:current train perplexity2.9345593452453613
INFO:root:current mean train loss 1364.1089569241078
INFO:root:current train perplexity2.934659242630005
INFO:root:current mean train loss 1364.02836440452
INFO:root:current train perplexity2.9337034225463867
INFO:root:current mean train loss 1363.860682521986
INFO:root:current train perplexity2.931833267211914
INFO:root:current mean train loss 1364.287106836276
INFO:root:current train perplexity2.9326913356781006
INFO:root:current mean train loss 1363.9742842319558
INFO:root:current train perplexity2.9314069747924805
INFO:root:current mean train loss 1363.7373211869053
INFO:root:current train perplexity2.9304234981536865

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.50s/it]
INFO:root:final mean train loss: 1363.1472896512446
INFO:root:final train perplexity: 2.930168390274048
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2332.1042302990636
INFO:root:eval perplexity: 6.593398571014404
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/197
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/200 [10:51:03<09:54, 198.29s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1366.5490188598633
INFO:root:current train perplexity2.906822681427002
INFO:root:current mean train loss 1363.6248234929265
INFO:root:current train perplexity2.9346210956573486
INFO:root:current mean train loss 1363.0141803372292
INFO:root:current train perplexity2.93515682220459
INFO:root:current mean train loss 1364.4206188684223
INFO:root:current train perplexity2.9315378665924072
INFO:root:current mean train loss 1363.3030054909843
INFO:root:current train perplexity2.9322938919067383
INFO:root:current mean train loss 1361.8346334861142
INFO:root:current train perplexity2.928093671798706
INFO:root:current mean train loss 1360.5367324263962
INFO:root:current train perplexity2.9285073280334473
INFO:root:current mean train loss 1361.6909652954755
INFO:root:current train perplexity2.928924798965454
INFO:root:current mean train loss 1362.2079890988907
INFO:root:current train perplexity2.927973508834839
INFO:root:current mean train loss 1361.414719722442
INFO:root:current train perplexity2.9293158054351807
INFO:root:current mean train loss 1361.8035474005546
INFO:root:current train perplexity2.927954912185669
INFO:root:current mean train loss 1362.301929434002
INFO:root:current train perplexity2.927358627319336
INFO:root:current mean train loss 1361.947652376615
INFO:root:current train perplexity2.9288623332977295
INFO:root:current mean train loss 1362.2167980522358
INFO:root:current train perplexity2.9283835887908936
INFO:root:current mean train loss 1361.5777610652356
INFO:root:current train perplexity2.9276058673858643
INFO:root:current mean train loss 1361.4224351197865
INFO:root:current train perplexity2.9280765056610107
INFO:root:current mean train loss 1361.277920176682
INFO:root:current train perplexity2.9270718097686768
INFO:root:current mean train loss 1361.2401271095537
INFO:root:current train perplexity2.9273195266723633
INFO:root:current mean train loss 1362.1562857359518
INFO:root:current train perplexity2.928255796432495
INFO:root:current mean train loss 1362.4127729913293
INFO:root:current train perplexity2.927443027496338

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.41s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.41s/it]
INFO:root:final mean train loss: 1362.106264023004
INFO:root:final train perplexity: 2.9277637004852295
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.25s/it]
INFO:root:eval mean loss: 2332.14603375374
INFO:root:eval perplexity: 6.593621730804443
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/198
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 198/200 [10:54:21<06:36, 198.19s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1358.66796875
INFO:root:current train perplexity2.933257818222046
INFO:root:current mean train loss 1360.1761260061553
INFO:root:current train perplexity2.9194295406341553
INFO:root:current mean train loss 1356.413441093013
INFO:root:current train perplexity2.916393518447876
INFO:root:current mean train loss 1359.365911614405
INFO:root:current train perplexity2.923524856567383
INFO:root:current mean train loss 1359.5594374789987
INFO:root:current train perplexity2.924919843673706
INFO:root:current mean train loss 1361.095260647124
INFO:root:current train perplexity2.9258904457092285
INFO:root:current mean train loss 1360.61652611754
INFO:root:current train perplexity2.923943519592285
INFO:root:current mean train loss 1361.3602513531455
INFO:root:current train perplexity2.9269087314605713
INFO:root:current mean train loss 1361.356954903134
INFO:root:current train perplexity2.9274497032165527
INFO:root:current mean train loss 1361.9162383875082
INFO:root:current train perplexity2.927076578140259
INFO:root:current mean train loss 1361.2246309235622
INFO:root:current train perplexity2.9272265434265137
INFO:root:current mean train loss 1361.877547549792
INFO:root:current train perplexity2.9274418354034424
INFO:root:current mean train loss 1361.4918628798173
INFO:root:current train perplexity2.927276849746704
INFO:root:current mean train loss 1361.735093918126
INFO:root:current train perplexity2.927011251449585
INFO:root:current mean train loss 1362.132375463284
INFO:root:current train perplexity2.9267375469207764
INFO:root:current mean train loss 1362.347967626797
INFO:root:current train perplexity2.9258360862731934
INFO:root:current mean train loss 1362.343437089433
INFO:root:current train perplexity2.9266977310180664
INFO:root:current mean train loss 1362.012233520162
INFO:root:current train perplexity2.926011800765991
INFO:root:current mean train loss 1362.2321293644227
INFO:root:current train perplexity2.926117181777954
INFO:root:current mean train loss 1361.8484677535585
INFO:root:current train perplexity2.9256222248077393

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.88s/it]
INFO:root:final mean train loss: 1361.3293124923669
INFO:root:final train perplexity: 2.9259703159332275
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2332.2070035460993
INFO:root:eval perplexity: 6.593947410583496
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/199
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/200 [10:57:39<03:18, 198.28s/it]
  0%|          | 0/1 [00:00<?, ?it/s][AINFO:root:current mean train loss 1360.7816802234183
INFO:root:current train perplexity2.884571075439453
INFO:root:current mean train loss 1358.0103571965144
INFO:root:current train perplexity2.9042277336120605
INFO:root:current mean train loss 1357.574825638575
INFO:root:current train perplexity2.9134347438812256
INFO:root:current mean train loss 1360.6633131416681
INFO:root:current train perplexity2.919436454772949
INFO:root:current mean train loss 1359.8390667547328
INFO:root:current train perplexity2.9239249229431152
INFO:root:current mean train loss 1360.4091467578796
INFO:root:current train perplexity2.92516827583313
INFO:root:current mean train loss 1360.5085254121036
INFO:root:current train perplexity2.925194025039673
INFO:root:current mean train loss 1360.8152996560802
INFO:root:current train perplexity2.9275550842285156
INFO:root:current mean train loss 1361.5778180249965
INFO:root:current train perplexity2.9277005195617676
INFO:root:current mean train loss 1360.9840638574417
INFO:root:current train perplexity2.9272806644439697
INFO:root:current mean train loss 1360.6785769083583
INFO:root:current train perplexity2.926527738571167
INFO:root:current mean train loss 1360.8706184813213
INFO:root:current train perplexity2.926995277404785
INFO:root:current mean train loss 1360.8860290813
INFO:root:current train perplexity2.9270951747894287
INFO:root:current mean train loss 1360.765469011453
INFO:root:current train perplexity2.9257662296295166
INFO:root:current mean train loss 1361.4554035634646
INFO:root:current train perplexity2.9262940883636475
INFO:root:current mean train loss 1361.0511485412057
INFO:root:current train perplexity2.925651788711548
INFO:root:current mean train loss 1361.7362170860117
INFO:root:current train perplexity2.9252114295959473
INFO:root:current mean train loss 1361.6704021415326
INFO:root:current train perplexity2.9257442951202393
INFO:root:current mean train loss 1361.5285141850632
INFO:root:current train perplexity2.926187753677368
INFO:root:current mean train loss 1361.4561057451635
INFO:root:current train perplexity2.9254143238067627

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.48s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [03:08<00:00, 188.48s/it]
INFO:root:final mean train loss: 1361.0852041239698
INFO:root:final train perplexity: 2.9254071712493896
INFO:root:epoch finished
INFO:root:start evaluating on validation

  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2332.1671198851673
INFO:root:eval perplexity: 6.593734264373779
INFO:root:evalaution complete
INFO:root:checkpoint. save model: allmini_l12_baseline/200
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:00:57<00:00, 198.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [11:00:57<00:00, 198.29s/it]
INFO:root:evaluating final model
INFO:root:start evaluating on validation
  0%|          | 0/1 [00:00<?, ?it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.26s/it]
INFO:root:eval mean loss: 2332.1671198851673
INFO:root:eval perplexity: 6.593734264373779
INFO:root:evalaution complete
INFO:root:save model final: allmini_l12_baseline/final
